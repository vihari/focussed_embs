sem no . with tem you can see the atomic structure but only very local . it does not give you the idea of the whole sample . besides , tem is a much complex technique than xrd and involves a specimen preparation which may be very ( very ! ) difficult and alter you sample . powder xrd is a simple technique , most of the times non destructive . you do not need to do a fourier transform . . . powder xrd gives you a diffraction pattern with diffraction peaks . in a general way , sharper peaks means a more " crystalline " sample and well organized ( closer to a single crystal ) . however this is not always straightforward ( it depends on type of sample , material , size of particles . . . ) . it would be helpful if you could say what type of sample are you trying to analyse .
i will take a stab , though it needs a solid state physics background , in which i am weak . a ) in the case of crystals , as the two surfaces touch the crystal symmetry will take over with maybe some dislocation plane . i.e. the van der waals forces will make the two one and you will not be able to easily separate them . b ) if amorphous , 1 ) inorganic example glass on glass , the attraction is not strong enough to create one body . the two will be separable . 2 ) organic surfaces with strong static fields example two cling film layers , would adhere as one body , depending on the material . etc . so it will depend on the specific materials under study .
any optical assembly , whether a telescope or a camera or whatever , will generally have a minimum distance at which it can focus . that said , the scale for distances is set by the focal lengths of the optics . as you point out , they are $360~\mathrm{mm}$ and either $6~\mathrm{mm}$ or $20~\mathrm{mm}$ . ( the $360~\mathrm{mm}$ is the really important one . ) distances long compared to these are all much the same to your telescope , so the building half a kilometer away and the stars thousands of light years away are all " at infinity " as far as it is concerned . the $360~\mathrm{mm}$ actually means that parallel rays " from infinity " need to travel $360~\mathrm{mm}$ beyond the lens to converge to a point . diverging rays from " closer than infinity " objects need to travel further . thus the minimum distance at which you can focus is set by how much you can extend the optical path , and the maximum distance you can focus is set by how short the path can be made . any telescope , whether for terrestrial or astronomical use , should be able to focus at infinity - that is after all where its targets of interest are . note though that i mentioned changing the optical path . in order to focus , assuming you do not have some multimillion dollar deformable optics on hand , you need to turn some knob somewhere to move something . while there are systems built that are fixed to focus at a single distance , i am guessing that is not what you have , since it seems designed for changing eyepieces ( doing so requires refocusing ) and personal use ( while contacts are fine , people who wear glasses are best served by removing them and changing the focus to correct for nearsightedness , since you generally need to get your eye close to the eyepiece ) .
working in the frequency space helps simplify the differential equation you need to solve . now it should be possible to find a bunch of solutions to the new differential equation . however , in the end what you want to solve is still the time-dependent one . so you need to come back to the initial or boundary conditions of the original time-dependent equation to fix the uncertainty . to be more specific , you can try to build the time-dependent wave function with those solutions you obtained . certainly there will be unknown coefficients to be determined at the last step .
you definitely do not want to do the integral from $-\tau/2$ to $\tau/2$ ; you need to go all the way to $\pm \infty$ for the integral to be meaningful . in this particular case , as a comment notes , the integral will diverge if you take the lower limit to $-\infty$ ; physically , you want to start the integral at $t=0$ because that is when the field comes into existence . here is a hint towards the easiest way to do this integral ( and many fourier integrals , for that matter ) : $\cos ( x ) = \frac{1}{2} ( e^{ix} + e^{-ix} ) $ . the integral should be easy from there . you can also use integration by parts , but that is a little more tedious and error-prone .
in mathematics , there is a complete symmetry between $+i$ and $-i$ . both the imaginary unit and the minus imaginary unit obey $$ i^2 = ( -i ) ^2 = -1 $$ the exchange of $i$ and $-i$ is known as the ${\mathbb z}_2$ automorphism group of the complex numbers ${\mathbb c}$ . when you introduce the complex numbers for the first time , it is a complete convention whether you call a square root of $ ( -1 ) $ as $+i$ or $-i$ . however , in physics , we have to break the symmetry between $+i$ and $-i$ because we must know whether a wave in a particular situation is $\exp ( i\omega t ) $ or $\exp ( -i\omega t ) $ , for example . in particular , $xp-px=i\hbar$ and not $-i\hbar$ . also , and the following choice of the sign is actually not independent from the previous one in the commutator , schrödinger 's equation was chosen to be $$ h |\psi \rangle = i\hbar\frac{d}{dt}|\psi\rangle $$ where $h$ is the hamiltonian that may be replaced by $h=e$ when acting on an energy eigenstate $|\psi\rangle$ . this equation is totally universal everywhere in quantum mechanics where a hamiltonian is well-defined ( it may be even quantum field theory or some descriptions of string theory ) . the equation above , with $h=e$ , is solved by $$|\psi ( t ) \rangle = \exp ( et/i\hbar ) |\psi ( 0 ) \rangle = \exp ( -iet/ \hbar ) |\psi ( 0 ) \rangle = \exp ( -i\omega t ) |\psi ( 0 ) \rangle $$ all the forms are equivalent because $1/i = -i$ – this equation is equivalent to $i^2=-1$ – and because $e=\hbar\omega$ without a minus sign . so your sign is wrong ; the sign you denounced is the right one and the sign you wanted is the incorrect one . just to be sure , in quantum field theory , we work with various objects – quantum fields – that are expanded into terms that depend on time as $\exp ( -i\omega t ) $ while there must also be terms that depend on time via $\exp ( +i\omega t ) $ . but these are terms in operators , not the time dependence of the wave function . one must be careful about the precise statements and objects . i have not made any statement of the sort that only the expression $\exp ( -i\omega t ) $ and not $\exp ( +i\omega t ) $ appears in quantum theory papers and books . of course , both of them may appear somewhere – in quantum field theory , both of them have to appear because there are both creation and annihilation operators , both particles and antiparticles . but when we are asking how an energy $e$ wave function ( and i mean the ket vector ) depends on time , it is always via $\exp ( -iet/\hbar ) $ . the bra vector has the opposite sign ( plus ) in the exponent .
as understood by einstein 's general theory of relativity completed in 1915-16 , gravity is indeed a manifestation of ( nothing else than ) the curvature of space and i have some doubts about your implicit claim that you have made this discovery " independently " of einstein . according to the precise equations of general relativity , the so-called einstein 's equations $$ g_{\mu\nu} = \frac{8\pi g}{c^2} t_{\mu\nu} , $$ what influences the curvature of spacetime is the stress-energy tensor that knows about the density of energy and momentum and the flux of energy and momentum . terms like " flux of momentum " may sound obscure but they are described by well-defined mathematical formulae . in particular , " flux of momentum " is nothing else than the component of pressure . so pressure also influences the curvature of spacetime – and therefore the gravitational field and the behavior of objects in this field – according to general relativity . on the other hand , it is irrelevant for the curvature and gravity whether the same stress energy tensor – the density of mass , energy , momentum , and components of pressure and stress – are achieved by the electromagnetic field , one material , or another material . however , it is still impossible to " create " curvature of space without any material ( or energetic ) carrier . the equations explicitly show that the ricci tensor is zero if there is no energy/momentum density in the space . so one can not create a " black hole out of nothing " . nevertheless , black holes may suck all the material and make the spacetime around ricci-flat ; the ricci ( or einstein ) tensor is equal to zero almost everywhere in the space . this ricci-flatness is still importantly violated at the black hole singularity which is the reason why the black holes still carry a nonzero mass/energy . the question is getting increasingly impenetrable as one continues to read it so what you exactly wanted to do with the frame-dragging effect remained unknown to me ( and i guess that not only me ) . frame-dragging is a particular new gravitational effect that occurs in the gravitational field induced by rotating bodies .
quoting from my copy of the 2nd edition of jackson 's book on classical electrodynamics , section 1.2: assume that the force varies as $1/r^{2+\epsilon}$ and quote a value or limit for $\epsilon$ . [ . . . ] the original experiment with concentric spheres by cavendish in 1772 gave an upper limit on $\epsilon$ of $\left| \epsilon \right| \le 0.02$ . followed a bit later by williams , fakker , and hill [ . . . gave ] a limit of $\epsilon \le ( 2.7 \pm 3.1 ) \times 10^{-16}$ . that book was first published in 1975 , so presumably there has been some progress in the mean time .
scale bars may be helpful . also , spr 's decay exponentially . if you google " surface plasmon image , " you might notice that there is a lack of images and mostly cartoons . imaging a surface plasmon does not really make sense , as the light is traped along the interface between a metal and a dielectric . and while you may be able to image light near a spr , it would be really hard , your objective lens would have to be ridiculously close to the metal , most likely less than a micron if your objective is in air . imaging the light that is coupled out of a spr makes more sense . to get outcoupling of light you need to use a type of grating , or create some kind of rough patch for the light to escape via . in order to get this kind of out coupling , you need to do angled imaging , which it seems you perform . so maybe what you have is just a bump in your metal film ? the wavelengths of light that are emitted from film of course are affected by the permittivity/refractive index of your surounding materials , but they are also very much modified by the size/shape/geometry of your nanostructure . this is very important if your looking to gain insight about what you are seeing . so in order to gain some kind of quantitative insight from this blimp in your image , it sounds like you are going to have to make that same identical rough patch . or actually create a meaningful outcoupling grating . as for really knowing if it is a spr , i am not sure what to tell you . you had probably need a high intensity laser to create that kind of resonance , since you are in the uv ( blue color ) . you would need something higher than the color that is being emitted , as a great deal of loss is typically seen with spr 's . i am not sure what you imaged with ( you sounded like the tem imaging and the other imaging were separate ? ) , and i can not speak for tem ( not familiar ) . i would suggest googling to see if you can find tem 's producing spr 's . from a quick google search , i think they can , but it looks like advanced filtering might be needed . . . good luck
in short , the answer goes like this : all fermion masses are assumed to unify at some high scale ( e . g . ~$10^{16} gev$ ) in msugra . so , the mass differences between them at low energies are due to the running of the masses from that high scale down to the observed scale ( e . g . ~1tev at the lhc ) . the $\beta$ function for the stop mass has a positive contribution due to the top yukawa coupling , $y_t$ , which is large due to the fact that the top mass is not small compared to the higgs vacuum expectation value , $v$ , ( $m_t = y_tv$ ) . this implies that the stop mass drops more rapidly as the renormalization scale is lowered than the 1st and 2nd generation squarks , which have negligible yukawa couplings . this leads to a mass spectrum where the stop is light and the first two generation squarks are nearly degenerate ( ditto for the stau ) . the supersymmetry primer by martin has a good discussion of this ( http://arxiv.org/abs/hep-ph/9709356 ) , particularly on pg . 46 ( where you can find the $\beta$ functions ) and 75 ( which discusses the squark and slepton spectrum ) .
as a matter of fact one could also discuss commutation relations at different time : $$ [ \phi ( x ) , \phi ( y ) ] = i e ( x , y ) i\quad ( 1 ) $$ for free fields $e$ is the so-called causal propagator or advanced minus retarded fundamental solution that depends on the free field equation satisfied by $\phi$ . the point is that , passing to considering interacting fields , at least formally , equal time commutation relations remain unchanged with respect to the free case , whereas the corresponding of ( 1 ) changes into an , in practice , unknown form as they include the full dynamics . actually even this idea does not work completely , as interacting fields $\phi$ are affected by a renormalization constant $z^{1/2}$: $$\phi ( t , \vec x ) \to z^{1/2} \phi ( t , \vec x ) \quad \mbox{in weak sense as } t \to \pm \infty$$ and , dealing with naively with renormalization procedure , it arises $z=0$ . so canonical commutation relations seem to be untenable for fields $\phi ( x ) , \partial_t \phi ( y ) = \pi ( y ) $ . however all that sounds a bit academic as the renormalization procedure , in a sense , solves the problem . i would like to stress that the fact that commutation relations are taken at equal time is not in contradiction with relativistic invariance : covariance ( i.e. . , the use of tensors and taking space and time on the same footing ) is just one way to make explicit relativistic invariance , but it is by no means the unique one ! hamiltonian formalism is not covariant , though it is relativistically invariant : all equations ( including ccr ) take the same form in every inertial reference frame .
the electric field from your potential is : $$e ( r ) = {2\over r^3}$$ using gauss 's law , the total charge in a sphere of radius r is : $$q ( r ) = \oint e \cdot ds = 4\pi r^2 {2\over r^3} = {8\pi\over r}$$ the total charge is decreasing with r , so there is a negative charge cloud of density $$ \rho ( r ) = {1\over 4\pi r^2} {dq\over dr} = - {4\over r^4}$$ but the total charge at infinity is zero , so there is a positive charge at the origin , cancelling the negative charge cloud , of a divergent magnitude . if you assume this charge is a sphere of infinitesimal radius $\epsilon$ , the positive charge at the origin is $$q_0 = \int_\epsilon^\infty 4\pi r^2 {4\over r^4} = {16\pi \over \epsilon}$$ this is not a distribution in the mathematical sense , but it is certainly ok to work with , so long as you keep the $\epsilon$ around and take the limit $\epsilon$ goes to zero at the end of the day . mathematicians have not had the last word on the class of appropriate generalized solutions yet .
the hopping term is given by $$ t_{ij}=\int\limits_{\mathbf{r}}d\mathbf{r}\phi^*\left ( \mathbf{r}_i\right ) \left [ -\frac{\hbar^2}{2m}\nabla^2+u ( \mathbf{r} ) \right ] \phi\left ( \mathbf{r}_j\right ) $$ where $i$ and $j$ are the sites whose hopping you want to find , $\phi\left ( \mathbf{r}_j\right ) $ are the atomic orbitals , and $u ( \mathbf{r} ) $ is the potential of the crystal lattice . so the sign depends on your choice for $u ( \mathbf{r} ) $ . if $u ( \mathbf{r} ) $ is taken as negative ( coulomb potential ) you are more likely to end up with a negative $t$ which can be a nuisance , so people just redefine it as $-|t|$ .
none of them . all of them ( and the rho and eta , too ) . it does not matter and does not have a unique answer as the meson exchange idea is an effective theory . if you insist on trying to write down rules , then charged pion exchanges can only occur between protons and neutrons resulting in the nucleons exchanging types . on the other hand neutral pions can be exchanged between any two nucleons . the heavier mesons can also participate , but their contribution will be smaller at all ranges and they range out sooner . welecka spends a few sections on computation using this model in his nuclear physics text .
the diffusion is determined from the broadening of the usual elastic scattering peaks . this is known as quasielastic scattering . there is no requirement for the neutron speed to be related to the diffusion speed . have a look at this book for an introduction to the subject .
when two black holes collide , they coalesce to form a larger black hole . the event will produce gravitational waves that will have a particular signature that depends on the details of the collision . black hole merger events are one of the main gravitational wave signals that are being sought at ligo . there are a few misconceptions in your question : 1 ) black holes do not have infinite mass . they in fact have a finite mass . the orbit of the earth around a black hole with the same mass as the sun 's would in fact be identical to the earth 's current orbit . so , the idea of black holes sucking up the space around them is inaccurate . 2 ) the gravity at the event horizon of a black hole is finite and ( generally ) inversely proprortional to the mass of the black hole--a sufficiently large black hole would not exert any detectable local strains on objects at all ! 3 ) the geometry of spacetime will be well-defined everywhere except at the central singularity of the black hole . every observer will report only one direction of time . this direction , in an absolute sense , will be observer dependent , just as it is in special relativity ( the rate of aging will depend on speed and position , but will always be well-defined and knowable ) .
not knowing your situation in detail it is hard to offer detailed advice , but here is one course of action that should be available to you and will give a sufficient energy calibration : use the co-60 and cs-137 source as you have been ( 3 points ) use only the 356 kev line from the ba-133 source ( 1 point ) use a banana ( suitably wrapped in shrink wrap and ziplock bags to avoid contaminating the detector ) as a k-40 source source ( 1 point ) that gives you five points which is sufficient for a full linear fit with uncertainties . now that i see the op 's data with it is very good signal to noise ratios ( i was using a rather aged and low activity barium source ) , i would modify this as several of the barium lines are usable , and there appears to be a k-40 background in the detector visible in the co-60 data which means that the banana is not actually needed .
the moon orbits around the sun , but so does the earth . they orbit together with the moon 's orbit perturbed by the nearby earth . if fact , despite their different masses they experience the same acceleration , so it should not be surprising that they are bound to the same orbit since they are bound to each other ( i.e. . at basically the same distance from the sun ) . the moon experiences motion relative to the earth and is bound to it by the earth 's gravity , and once bound , unless the tidal forces due to the sun pull them apart , they will stay bound together - accelerating towards the sun at the same rate , as essentially one object . this is the key : outside of the hill sphere the difference in gravitational force ( the so-called " tidal force" ) is great enough to break the gravitational binding . you may be interested to know that the earth-moon roche limit vis-à-vis the sun is about 33.6 million kilometers and that the earth is roughly 150 million kilometers from the sun . so we are quite safe from the danger of having our moon stolen .
it turns out my approach was correct , but i had different errors preventing acceptance of my solution . to summarize : calculate the maximum constant deceleration $a$ that can be applied for time $t$ that results in non-negative final velocity : $a_{max}=\frac{v_0}{t}$ . this is needed because the next equation does not take into account decleration that stops at velocity 0 ( deceleration due to friction ) calculate the slowing ( might be stopping ) decleration that places the vehicle at distance $d$ at time $t$: $a_{slow} = 2\frac{v_0t-d}{t^2}$ if $a_{slow} &gt ; a_{max}$ , then applying $a_{slow}$ causes the vehicle to exceed distance $d$ before time $t$ ( and to go " in reverse " back to $d$ ) -- use $a_{stop} = \frac{v_0^2}{2d}$ , else use $a_{slow}$ thanks to all who commented/answered
before ewsb , chiral symmetry is still broken by the yukawas , that is there is a higgs quarks vertex that breaks explicitly $su ( 2 ) _l \times su ( 2 ) _r$ . therefore , the pions would still be massive and hence mediating a short range residual nuclear force . moreover , it does not make much sense thinking about ' before ' the ewsb since you are looking at pions interacting with nucleons , that is at scales well below the higgs vev . as for the last part of your question , i do not think i understand what negative beta function you are referring to .
the crab is one of the youngest snr 's we know of in the galaxy , so i would not have expected to find one younger , but i did . i present to you snr 0540 , as observed by chandra x-ray observatory : unfortunately , this is really the best we can do because it is 160,000 light-years away ! ( for reference , crab is about 6000 light-years away ) . this remnant is estimated to be about 750 years old , so it is about 250 years younger than crab . in the general case , there are other snrs that have compact objects located in the vicinity . first up is ic 443 : source ( with description ) and puppis a : source ( also with description )
i think $\cos ( 90^\circ-\theta ) =\frac{g}{a}$ is wrong . angle between $\vec{g}$ and $\vec{a}$ is that , but that is not the same thing . think about what happens when $\theta=0$ in the acceleration infinite ? to solve this problem you need to find the projection of $\vec{g}$ along the incline . see picture below that might help you .
as you note , the algebraic structure on $t x \oplus t^* x$ studied in generalized complex geometry is that of the standard courant lie 2-algebroid . courant lie 2-algebroids ( standard or non-standard ) play a role in various guises in 2-dimensional qft , thanks to the fact that they are in a precise sense the next higher analog of symplectic manifolds ( see symplectic lie n-algebroid ) and thus the direct generalization of hamiltonian mechanics from point particles to strings . this higher symplectic geometry aspect of courant lie 2-algebroids -- gence of gcg -- is recently receiving more attention . related to this is the courant sigma-model , which is a 3d tft generalizing chern-simons theory , being the direct higher dimensional analog of the poisson sigma-model . it has a courant lie 2-algebroid as its target space . hence in particular every generalized complex geometry forms the target space of such a 3d sigma-model .
there does not appear to be anything wrong with user35915 's calculation . however , in order to get the desired answer , the canonical momenta needs to be different . starting from user35915 's action , $$ s=\int d^{4} x\left ( -\frac{1}{4}f_{\mu\lambda}f^{\mu\lambda}-\frac{1}{2}a^{\mu}_{ , \mu}a^{\lambda}_{ , \lambda}\right ) $$ change the second term by integrating by parts and chuck the surface term away to get , $$ s=-\frac{1}{4}\int d^{4} x ( f_{\mu\lambda}f^{\mu\lambda}-2a^{\mu}a^{\lambda}_{ , \lambda\mu} ) \ . $$ now expand the electromagnetic field tensor $f_{\mu\lambda}=a_{\lambda , \mu}-a_{\lambda , \mu}$ and do a bit of swopping dummy indices to get , $$ s=-\frac{1}{2}\int d^{4}x \eta^{\mu\rho}\eta^{\lambda\sigma} ( a_{\lambda , \mu}a_{\sigma , \rho}-a_{\lambda , \mu}a_{\rho , \sigma}-a_{\rho}a_{\lambda , \sigma\mu} ) \ . $$ the last two terms can be combined into a surface integral which vanishes at infinity and the final form of the action is only the first term in the last line . the lagrangian is now , $$ l=-\frac{1}{2}\int d^{3}x \eta^{\mu\rho}\eta^{\lambda\sigma}a_{\lambda , \mu}a_{\sigma , \rho}=-\frac{1}{2}\int d^{3}x a_{\mu , \lambda}a^{\mu , \lambda}\ . $$ the reason for getting the lagrangian in this form is because it looks like the lagrangian for four scalar fields . the canonical momenta are now , $$ \pi^{\mu}=-a^{\mu}_{ , 0}=-\frac{\partial a^{\mu}}{\partial t} $$ which look like the momenta for four scalar fields . now , it is straightforward to go over to the desired hamiltonian , $$ h=-\frac{1}{2}\int d^{3}x ( \pi^{\mu}\pi_{\mu}+a^{\mu}_{ , r}a_{\mu , r} ) $$
the wave equation for a spherical surface is given ( in spherical coordinates ) by $$\frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\left ( \sin\theta\frac{\partial f}{\partial\theta}\right ) +\frac{1}{\sin^2\theta}\frac{\partial^2f}{\partial \phi^2}=\frac{r^2}{v^2}\frac{\partial^2 f}{\partial t^2}$$ with $\theta\in [ 0 , \pi ] $ , $\phi\in [ 0,2\pi ] $ , $r$ being the radius and $v$ the wave velocity . in the case of time-independence , the solutions are given by legendre functions .
how can i prove thevenin 's and norton 's theorem ? here 's an outline - you can fill in the dots . measure the voltage ( with an ideal voltmeter ) between any two nodes of an arbitrary linear circuit . call this voltage the open circuit voltage $v_{oc}$ since there is zero current through an ideal voltmeter . then , place an ideal ammeter across the same two nodes and measure the current . call this current the short circuit current $i_{sc}$ since there is zero voltage across an ideal ammeter . now , you have the voltage when there is zero current and the current when there is zero voltage . so , you have two points on the current-voltage ( iv ) plot for these two nodes . as you know , it takes two points to uniquely identify a line in this plane and the equation for this line is $$v ( i ) = v_{oc} - \frac{v_{oc}}{i_{sc}}i$$ can you take it from here ? update 1 to respond to a comment . @alfredcentauri , how do we prove that the characteristic is a straight line ? if the circuit is linear , the superposition theorem holds and , thus , any circuit voltage or current can be expressed as a sum of terms , each term involving one source and equal to the circuit voltage or current due to that source alone , i.e. , the result obtained by zeroing all other sources . in the previous section , we measured the voltage across two terminals of a circuit and labelled that voltage $v_{oc}$ . by superposition , and assuming a dc circuit , if we connect a current test source across these terminals such that $i = i_s$ , the voltage across the terminals is given by $$v = v_{oc} - r_{eq}i_s$$ where $r_{eq}$ is the equivalent resistance between the terminals when all the circuit sources are zeroed ( with all circuit sources zeroed , there is just a resistor network between the terminals ) . thus , by superposition , we know that for a linear dc circuit , there is , between any two terminals , an equivalent circuit with identical terminal characteristics : a voltage source with voltage $v_{oc}$ in series with a resistor with resistance $r_{eq}$ and , since there is just one line between the two measured points in the i-v plane found in the previous section , it follows that $$r_{eq} = \frac{v_{oc}}{i_{sc}} $$ update 2: to verify the validity of my arguments above , i found a formal proof of thevenin 's theorem in one of my undergrad textbooks : " fundamentals of circuits , electronics , and signal analysis " by kendall l . su . i will excerpt and paraphrase this proof found in the appendix a . 1 on page 568 to simplify the proof , we shall assume that the network in question is excited by an independent current source [ $i_{sn}$ ] at its terminal pair ( terminals a and b in figure a . 1 ) . furthermore we shall assume that the network contains $n-1$ independent current sources and $m$ independent voltage sources , and a number of lti elements , including lti controlled sources . since the network is lti , the superposition property prevails . that is to say , $v_{ab}$ is a linear combination of all the strengths of the independent sources . this fact can be expressed analytically as $$v_{ab} = \sum_{j=1}^{n-1}z_{nj}i_{sj} + \sum_{k=1}^{m}h_{nk}e_{sk} + z_{nn}i_{sn} = v_{oc} + z_{nn}i_{sn}$$ $$v_{oc} = \sum_{j=1}^{n-1}z_{nj}i_{sj} + \sum_{k=1}^{m}h_{nk}e{sk}$$ $$z_{nn} = \frac{v_{ab}}{i_{sn}} , e_{sk}=0 , i_{sj}=0$$ . . . the circuit of figure a . 2 [ a voltage source with voltage $v_{oc}$ in series with $z_{nn}$ connected between terminals a and b where the source $i_{sn}$ is connected ] has exactly the relationship described by $$v_{ab} = v_{oc} + i_{sn}z_{nn}$$ the current source $i_{sn}$ cannot tell the difference between [ the actual circuit and the equivalent circuit ] . hence the two circuits are equivalent electrically .
assuming the train does not accelerate during the ball 's fall , it will land in the spot you aimed at . think about it this way . before you drop the ball , it is moving along with the train ( i.e. . it has some horizontal speed ) . when you drop it , the ball still has this speed , and since an object in motion tends to stay in motion unless you exert a force on it ( newton 's first law ) , the ball will continue to move at this horizontal speed as it falls . thus it will land exactly where it would if the train were at rest and so the observer will not be able to figure out he is in a moving reference frame . this actually speaks to something much deeper : namely that physics behaves the same in any inertial reference frame ( a reference frame moving with constant velocity ) . there is thus no concept of " absolute motion . " the train is moving with respect to the earth , but that is no different from the train being at rest and the earth moving underneath it . of course this all assumes that the train does not accelerate during the ball 's fall . if the train accelerates , then the ball will still move as it would have if the train had not accelerated . thus in this case the ball may land elsewhere than where you aimed it , and an observer can figure out in this case that the train is accelerating .
the radiation from an ultrarelativistic ( $v \approx c$ ) particle on a circular path is called synchotron radiation . the total power radiated from such a particle is $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c}\gamma^4$$ where $a$ is the acceleration and $\gamma$ is the lorentz factor , $\gamma^2 = 1/ ( 1-v^2/c^2 ) $ .
if you are analyzing the inductor over such short time scales , the answer is that not only is the electric field important , but the whole electromagnetic field . disturbances propagate at the speed of light through the circuit as electromagnetic waves - they cannot move faster than this ( otherwise there would be faster than light signaling , which is impossible as far as everyone knows because it would gainsay special relativity , which is one of the most thoroughly experimentally checked frameworks of thought that there is ) . at the short timescales you speak of , inductor actually looks from a circuit standpoint more like a distributed $lc$ ladder network like the one i have drawn as part of the system at the end of this answer . a complex system of bi-directionally running waves bounce back and forth through the system and eventually all these transients die down to establish the inductor 's steady state behavior . actually , it will not take as long for the waves to propagate through the system and establish steady state as you think . the windings of the coil lie near to one-another , so the propagating waves will jump directly across from one bight of wire through its insulating , dielectric coating to the neigbouring loop . this direct coupling means that the propagation delay is more like the length of the wound coil , not the total length of the wire . so your inductor will work as an inductor at higher frequencies than you think . if you see my answer describing how circuits work " so fast": this is how that answer ties in with this one . each electron shoves its neighbours in the circuit , but , at the subatomic level , the " shove " between each neighbour and the next is being transmitted by an electromagnetic wave arising from the shift in each charge . quantum field theory would see this as an exchange of virtual photons and the whole process is constrained by qft so that the momentum is not transmitted at greater than the speed of light . in the coil , as above , neighbouring charges are also on the neighbouring bight of wire in the loops of the coil . lastly , my comments on special relativity should not be too mysterious . you may ask " how does special relativity step in and prevent faster than light signaling ? " the answer is that maxwell 's equations , which wholly determine your system 's behavior , are well known to fulfill all the principles of special relativity . indeed , maxwell 's equations historically were the first known " relativistic " system and early researchers ( notably h . a . lorentz ) originally derived the transformation laws of special relativity directly from maxwell 's equations before einstein gave his more general interpretation of the michelson morley experiment .
surely someone has mulled over why the universe might exhibit such a non-intuitive and thus interesting asymmetry ? oh yes , definitely . i have for one ( though i have not made a significant contribution to the question ) ! : ) there are a number of " left-right symmetric " models out there which usually involve a group like $su ( 2 ) _l \times su ( 2 ) _r$ where the $su ( 2 ) _r$ gets spontaneously broken by a higgs mechanism . you will find a number of highly cited papers in inspirehep . i have always thought these models sounded very interesting but have not yet had the opportunity to work on them ! if you find anything good let me know . : )
the spin-statistics thing is not a problem , it is a theorem ( a demonstrably valid proposition ) , and it should not be addressed , it should be understood and celebrated . the higgs field gives us interactions between chiral fermions and the higgs , $yh\cdot \chi_\alpha\eta^\alpha$ which produces mass terms $m \chi_\alpha\eta^\alpha$ if the higgs field has a vacuum expectation value $h=v$ . we have $m=yv$ . however , in more general quantum field theories , one may write down similar ( bilinear ) mass terms manually without any higgs field ( and without cubic yukawa terms ) and none of these issues affects the fact that the spin-statistics relationship holds . the field $h$ is spin-zero field and bosonic , the fields $\chi , \eta , \psi$ ( the latter is the 4-component dirac spinor combining the previous two ) are spin-1/2 and fermionic fields .
ferromagnetism is not a property of a single atom or molecule , but rather of the crystalline structure of a given material . it emerges due to alignment of magnetic moments of neighboring atoms in the crystalline structure . this is why ferromagnetic properties disappear above curie temperature when alignment of magnetic moments of the atoms begins to decrease . properties of the crystalline structure are also the reason why some materials with high concentration of iron are ferromagnetic while others are not . for example , stainless steel with austenite crystalline structure is a nonmagnetic material , while steel with martensite or ferrite structure is ferromagnetic .
what is colloquially called ''empty space'' is not really empty - it is filled by the electromagnetic field and the gravitational field ; it is called empty only because it does not contain ( nonzero ) matter fields . the electromagnetic is the medium that carries electromagnetic waves , such as the air density field ( colloquially just called ''air'' ) carries sound waves and the water density field ( colloquially just called ''water'' ) carries water waves . indeed , electromagnetic waves are nothing else than propagating high-frequency oscillations in the electric fields , in precisely the same way as sound waves are propagating ohigh-frequency scillations in the pressure field of air ( or any other mechanical medium ) , and water waves are propagating low frequency oscillations in the mass density field of water .
no , it does not have to be numbered unless , like @dmckee comments , you have received specific instructions to do the numbering . i think your question can best be split up in 2 parts for the answer : writing a report and writing a research paper . writing a report for a report the main goal is typically to show exactly what you have done and how you have done it . quite often the report will be used by other students to continue your work . for this purpose it is convenient to have a numbered list for the methods , because it immediately attracts attention and makes it easy to follow the ' recipe ' . writing a research paper when you write a research paper your goal is in general to resolve a particular issue that exists in the scientific community . your focus will be on the research question and the conclusions you can draw from the experiments/simulations that you did . in this case a numbered list for the method will draw way too much attention to it , much more than it deserves . it is , after all , easy to spot because it disrupts the flow of the paper . in some journals it is not even allowed to use numbered lists if they are not inline ( i.e. . 1 ) . . . 2 ) . . . ) . so in conclusion , if you do not have specific instructions from someone ' higher up': use the numbered list if you want to focus on the method , use the paragraph if you want to focus on a different part of the report/paper
are you asking about the formula for the ellipse , or the formula for how angles in the sky are distorted ? in the former case : it is easy to work out the shape and size of the ellipse by considering lorentz contraction . suppose that in the rest frame of the source , the sphere has a radius $r$ . since the lorentz transform has no effect on components of length perpendicular to the motion , then the transverse radius of the ellipsoid remains the same - but that is just the semiminor axis . in the forward direction of motion , the length of the path taken from the source to the sphere is given by $$r_1&#39 ; = \gamma\biggl ( r + \frac{vr}{c}\biggr ) $$ and from the sphere back to the source , $$r_2&#39 ; = \gamma\biggl ( r - \frac{vr}{c}\biggr ) $$ adding these up you get $r_1&#39 ; + r_2&#39 ; = 2\gamma r$ , but this is the total travel distance which is just twice the semimajor axis of the ellipsoid . so we have an ellipsoid with semimajor axis $\gamma r$ and semiminor axis $r$ . to figure out the formula for the angular distortion - relativistic aberration - consider a photon arriving at the center of the sphere coming at an angle $\theta$ relative to the direction of motion ( precisely speaking : relative to an axis which points along the direction of motion in the external observer 's frame ) . from the stationary observer 's reference frame , the component of this photon 's velocity parallel to the direction of motion is subject to the lorentz transformation , $$u_{\parallel}&#39 ; = \frac{u_\parallel - v_\parallel}{1 + \frac{uv}{c^2}}$$ so if you plug in $u_{\parallel}&#39 ; = c\cos\theta&#39 ; $ , $u_\parallel = c\cos\theta$ , and $v_\parallel = v$ ( the relative speed of the source with respect to the observer ) , you get $$\cos\theta&#39 ; = \frac{c\cos\theta - v}{c + v\cos\theta}$$ this gives you the angle of an incoming light ray as seen by the external observer , $\theta&#39 ; $ , in terms of the angle as seen by the center of the sphere , $\theta$ . ( i might have mixed up the signs - i will have to come back to this and recheck it later . )
the most general lorentz transformation that is connected to the identity is given by the conjugation by $\exp ( -a ) $ where $$ a = \frac 12 \omega_{\mu\nu} \gamma^\mu \gamma^\nu $$ and $\omega_{\mu\nu}$ is an antisymmetric tensor containing $d ( d-1 ) /2$ parameters . the group of all such transformations is isomorphic to $spin ( d-1,1 ) $ . if $\omega$ only contains one component $0\mu$ , then it is a boost , and the nonzero numerical value of $\omega$ is the rapidity - the " hyperbolic angle " $\eta$ such that $v/c=\tanh\eta$ . if only one doubly spatial component of $\omega$ is nonzero , then this component $\omega_{\mu\nu} = -\omega_{\nu\mu}$ is obviously the angle itself . note that the spatial-spatial terms in $a$ are anti-hermitean , producing unitary transformations ; the mixed temporal-spatial terms in $a$ are hermitean and they do not product unitary transformations on the 4-component space of spinors ( but they become unitary if they are promoted to transformations of the full hilbert space of quantum field theory ) . in 4 dimensions , a general antisymmetric matrix $4\times 4$ contains 6 independent parameters and has eigenvalues $\pm i a , \pm i b$ , so in 3+1 dimensions , one can always represent a general lorentz transformation as a rotation around an axis in the 4-dimensional space followed by a boost in the complementary transverse 2-plane . this is the counterpart of the statement that any $su ( 2 ) $ rotations in 3 dimensions is a rotation around a particular axis by an angle . if you allowed $a$ to contain something else than $\gamma^{\mu\nu}$ matrices which generate the lorentz group , you could get other groups . only for a properly chosen subset of allowed values of $\omega$ , you would get a closed group from the resulting exponentials ( under multiplication ) . in particular , if you allowed $a$ to be an arbitrary complex combination of any products of gamma matrices , well , then you would allow $a$ to be any complex $4\times 4$ matrix , and its exponentials would produce the full group $gl ( 4 , c ) $ - surprising , carl ? ; - ) it is not a terribly useful groups in physics because actions are usually not invariant under this " full group " , are they ? also , there are not too many groups in between $spin ( 3,1 ) $ and $gl ( 4 , c ) $ - i guess that there is no proper group of $gl ( 4 , c ) $ that has a proper $spin ( 3,1 ) $ subgroup . obviously , there are many subgroups of $spin ( 3,1 ) $ - such as $spin ( 3 ) $ , $spin ( 1,1 ) \times spin ( 2 ) $ , and others .
how do i calculate $m\left ( ^{10}_4\mathrm{be}\right ) $ and $m\left ( ^{11}_5 \mathrm{b}\right ) $ ? the masses and various other properties of isotopes are available freely at wolfram alpha . they are , $m\left ( ^{10}_4\mathrm{be}\right ) =10.013533818u$ $m\left ( ^{11}_5 \mathrm{b}\right ) =11.009305406u$ where $u$ denotes unified atomic mass units . notice you are already given the mass number in the superscript of the isotope . as john rennie noted , the reaction should probably be with $^{9}_4\mathrm{be}$ , $$^{9}_4\mathrm{be} + ^{2}_1 \mathrm{h} \to ^{10}_{5}\mathrm{b} + n$$ in which case the mass is $9.012182201u$ .
the heat equation is an example of a convection-diffusion equation . your problem is one-dimensional in space ( only $x$ ) , which simplifies it a bit . the term on the left hand side is the time-rate of change of the internal energy $u$ ( often a multiple of the temperature ) . the second term is a diffusion term , as , in time , it diffuses or " smooths " peaks . the last term is a convective term , i.e. your medium is moving at constant velocity $v_0$ to the right .
i ) op is asking for the composition formula for so-called single mode squeezing operators , see eq . ( 8 ) below . we will not here prove the composition formula ( 8 ) , but only give partial hints and references . the key is to realize that one can identify $$\tag{1} \sigma_{+}~:=~\frac{1}{2} a^{\dagger}a^{\dagger} , \qquad \sigma_{-}~:=~\frac{1}{2} aa , \qquad\sigma_{3}~:=~a^{\dagger}a+\frac{1}{2} , $$ with generators of a $su ( 1,1 ) \cong so ( 2,1 ; \mathbb{r} ) \cong sl ( 2 , \mathbb{r} ) $ lie algebra $$\tag{2} [ \sigma_{+} , \sigma_{-} ] ~=~\sigma_{3} , \qquad [ \sigma_{3} , \sigma_{\pm} ] ~=~\pm2 \sigma_{\pm} . $$ here the single mode creation and annihilation operators satisfy $$\tag{3} [ a , a^{\dagger} ] ~=~1 . $$ the squeezing operator $$ \tag{4} s ( z ) ~:=~\exp\left [ -z\sigma_{+} + z^{*} \sigma_{-}\right ] , \qquad z~\in~ \mathbb{c} , $$ may be written in normal-ordered form $$\tag{5}s ( z ) ~=~\exp\left [ -t\sigma_{+}\right ] \exp\left [ \ln ( 1+|t|^2 ) \frac{\sigma_{3}}{2}\right ] \exp\left [ t^{*}\sigma_{-}\right ] , $$ cf . e.g. ref . 1 eq . ( 1.207 ) , or ref . 2 eqs . ( 2.16 ) and ( 3.4 ) . here $$\tag{6} z~=~re^{i\theta}~\in~ \mathbb{c} , \qquad r~\geq~ 0 , \qquad\theta~\in~ \mathbb{r} , $$ and $$\tag{7} t~:=~e^{i\theta}\tanh ( r ) ~\in~ \mathbb{c} . $$ the composition formula reads $$\tag{8} s ( z_1 ) s ( z_2 ) ~=~s ( z_3 ) \exp\left [ \ln\frac{1+t_1 t_2^{*}}{1+t_1^{*}t_2} \frac{\sigma_{3}}{2}\right ] , \qquad t_3~:=~\frac{t_1+t_2}{1+t_1^{*}t_2} , $$ cf . e.g. ref . 2 exercise 3.8 . ii ) the squeezing operators ( 4 ) may be viewed as elements of $sl ( 2 , \mathbb{c} ) $ . we may use the exponential map $$\tag{9}\exp : sl ( 2 , \mathbb{c} ) ~\longrightarrow~ sl ( 2 , \mathbb{c} ) $$ to generalized to operators of the form $$\tag{10} r ( \vec{\alpha} ) ~:=~\exp\left [ \vec{\alpha} \cdot \vec{\sigma} \right ] ~=~\exp\left [ \alpha^{+} \sigma_{+}+\alpha^{3} \sigma_{3}+ \alpha^{-} \sigma_{-}\right ] , $$ $$ \vec{\alpha}~=~ ( \alpha^{+} , \alpha^{3} , \alpha^{-} ) \in \mathbb{c}^3 . $$ the composition formula ( 8 ) generalizes to the baker-campbell-hausdorff ( bch ) formula $$\tag{11} \vec{\gamma}~=~ \vec{f} ( \vec{\alpha} , \vec{\beta} ) , $$ where $$\tag{12} r ( \vec{\alpha} ) r ( \vec{\beta} ) ~=~r ( \vec{\gamma} ) . $$ [ see also this phys . se post for the corresponding bch formula for $su ( 2 ) $ and $so ( 3 ; \mathbb{r} ) $ . ] note however that the exponential map ( 9 ) is not surjective $$\tag{13}{\rm im} ( \exp ) ~=~\left\{m\in sl ( 2 , \mathbb{c} ) \mid {\rm tr} ( m ) \neq -2\right\} ~\cup~ \left\{-{\bf 1}\right\} \subsetneq sl ( 2 , \mathbb{c} ) , $$ which means that the bch map ( 12 ) has singularities . references : p . kok and b.w. lovett , introduction to optical quantum information processing , 2010 . g.s. agarwal , quantum optics , 2012 . [ note that ref . 2 has the opposite sign convention $z\to -z$ in eq . ( 4 ) , see ref . 2 . eqs . ( 2.14 ) and ( 3.2 ) . ] d.r. truax , baker-campbell-hausdorff relations and unitarity of $su ( 2 ) $ and $su ( 1,1 ) $ squeeze operators , phys . rev . d 31 ( 1985 ) 1988 .
in a $d$-dimensional euclidean space ( with positive definite norm ) , one has $$ \vec{\nabla} \cdot \frac{\vec{r}}{r^d} ~=~{\rm vol} ( s^{d-1} ) ~\delta^d ( \vec{r} ) , $$ cf . the divergence theorem and arguments involving either test functions and integration by part , or $\epsilon$-regularization , similar to methods applied in this phys . se answer . here ${\rm vol} ( s^{d-1} ) $ is the surface area of the $ ( d-1 ) $-dimensional units sphere $s^{d-1}$ . by similar arguments one may show that the identity $$ \vec{\nabla} ( r^{2-d} ) ~=~ ( 2-d ) \frac{\vec{r}}{r^d} , \qquad d\neq 2 , $$ contains no distributional contributions in $d$-dimensional euclidean space . for the related questions in minkowski space , one suggestion is to introduce an $\epsilon$-regularization in the euclidean formulation , and then perform a wick rotation , and at the end of the calculation , let $\epsilon\to 0^+$ .
your problem lies in assuming that $$ \nabla^2 = \frac{\partial^2}{\partial r^2} + \cdots $$ this is not the case , you need to use $$ \nabla^2 = \frac{1}{r^2}\frac{\partial}{\partial r}\left ( r^2\frac{\partial}{\partial r}\right ) +\cdots $$ then will you obtain the correct answer of $-1/2$ .
in the double slit experiment , if you decrease the amplitude of the output light gradually , you will see a transition from continuous bright and dark fringe on the screen to a single dots at a time . if you can measure the dots very accurately , you always see there is one and only one dots there . it is the proof of the existence of the smallest unit of each measurement which is called single photon : you either get a single bright dot , or not . so , probably you may ask why it is not a single photon composite of two " sub-photon " , each of them passing through the slit separately and then interference with " itself " at the screen so that we only get one dot . however , the same thing occurs for three slits , four slits , etc . . . but the final results is still a single dot . it means that the photon must be able to split into infinitely many " sub-photon " . if you get to this point , then congratulation , you basically discover the path-integral formalism of quantum mechanics .
you have a counter weight $w$ hanging a distance $d_1$ from the pivot on the left . a payload and bracket of combined weight $f$ hanging a distance $d_2$ from the pivot on the right . a link of weight $w_1$ on the left with its center of gravity $\frac{d_1}{2}$ from the pivot . two links of weight $w_2$ each on the right with their combined center of gravity $\frac{d_2}{2}$ from the pivot . all together you balance the moments with $$ d_1 w + \frac{d_1}{2} w_1 = d_2 f + 2 \frac{d_2}{2} w_2 $$ which is solved for $$ w = \frac{d_2 ( f+w_2 ) }{d_1} - \frac{w_1}{2} $$
for simplicity of notation say $p = \frac{x - n}{x + n}$ given $\delta x$ is the uncertainty in x and $\delta n$ is the uncertainty in n then $\delta ( x - n ) $ = $\delta ( x + n ) = \sqrt {\delta ^2x + \delta ^2n}$ and therefore : $\delta p = p \sqrt{ ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x - n} ) ^2 + ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x + n} ) ^2}$ this is based upon equations 1b and 2b of the following reference : http://www.rit.edu/~w-uphysi/uncertainties/uncertaintiespart2.html
try to look at introduction to the replica theory of disordered statistical systems by v . dotsenko . in the following , i have written a possible answer to your question : \begin{equation} f=-\lim_{n\rightarrow\infty}\frac{1}{\beta n}\mathbb{e}\left [ \ln z_{j}\right ] \end{equation} where : $\mathbb{e}\left [ \mathcal{o}\right ] =\left ( \prod_{\left\{ i , j\right\} }\int dj_{ij}\right ) p\left [ j\right ] \mathcal{o} $ $z_{j}=\sum_{\sigma}e^{-\beta h\left [ j , \sigma\right ] } $ then labelling with $a$ the replicas : \begin{equation} z_{j}^{n}=\left ( \prod_{a=1}^{n}\sum_{\sigma^{a}}\right ) e^{-\beta\sum_{a=1}^{n}h\left [ j , \sigma_{a}\right ] } \end{equation} thus , remember that $\ln x=\lim_{n\rightarrow0}\frac{1}{n}\left ( x^{n}-1\right ) $: \begin{equation} f=-\lim_{n\rightarrow\infty}\frac{1}{\beta n}\mathbb{e}\left [ \ln\left ( z_{j}\right ) \right ] =-\lim_{n\rightarrow\infty}\lim_{n\rightarrow0}\frac{1}{\beta n}\mathbb{e}\left [ \frac{\left ( z_{j}^{n}-1\right ) }{n}\right ] =-\lim_{n\rightarrow\infty}\lim_{n\rightarrow0}\frac{1}{\beta nn}\mathbb{e}\left [ z_{j}^{n}\right ] \end{equation} but in general there are many issues concerning the commutation of the two limits .
david h is correct . if someone answered " well , space is xyzzy " then you might , understandably , scratch your head and follow-up with " but . . . what is xyzzy " . ultimately , as has been mentioned here numerous times , we get to the level of fundamental constituents of the world such as , for example , electric charge . when we say that electric charge is a fundamental constituent of the world , we mean that electric charge cannot be explained in terms somehow " more " fundamental . if that were not the case , electric charge would not be fundamental . similarly for spacetime or perhaps , the geometry of spacetime . classically , at least , spacetime is fundamental so asking " what is spacetime " presumes that there spacetime is not fundamental . by the way , what is mass and what is energy ?
first of all , this is just a change of basis , which is up to us to make . furthermore we should always choose a basis that makes our calculations easier , and hopefully makes things more intuitive . for a simpler example - just try finding the volume of a sphere in cartesian coordinates , its just a bad choice . second of all , you do not have to use a fourier basis , to my knowledge everything -loops renormalization etc can be done in a position basis . now as to why the fourier basis is a convenient choice : ( 1 ) it simplifies derivative terms in the lagrangian - as usual the fourier basis turns derivative expressions into algebraic ones , which are much easier to manipulate . ( 2 ) it it more intuitive - written in terms of a fourier basis the feynman rules are in terms of momentum . so for example at the vertices momentum is conserved - its just a nice tidy way to think about whats happening at the vertex . ( 3 ) even if you start in position space , one method for doing the integrals you will encounter when writing for your loop expressions will be going to momentum space - so you sort of cut this step out from the outset . ( 4 ) ( following up on vibert 's comment ) plane waves are the basis we do the experiment in . that is , we send in wave packets highly localized in p space , i.e. this is the exact solution we perturb around .
no , nothing in physics depends on the validity of the axiom of choice because physics deals with the explanation of observable phenomena . infinite collections of sets – and they are the issue of the axiom of choice – are obviously not observable ( we only observe a finite number of objects ) , so experimental physics may say nothing about the validity of the axiom of choice . if it could say something , it would be very paradoxical because axiom of choice is about pure maths and moreover , maths may prove that both systems with ac or non-ac are equally consistent . theoretical physics is no different because it deals with various well-defined , " constructible " objects such as spaces of real or complex functions or functionals . for a physicist , just like for an open-minded evidence-based mathematician , the axiom of choice is a matter of personal preferences and " beliefs " . a physicist could say that any non-contractible object , like a particular selected " set of elements " postulated to exist by the axiom of choice , is " unphysical " . in mathematics , the axiom of choice may simplify some proofs but if i were deciding , i would choose a stronger framework in which the axiom of choice is invalid . a particular advantage of this choice is that one can not prove the existence of unmeasurable sets in the lebesgue theory of measure . consequently , one may add a very convenient and elegant extra axiom that all subsets of real numbers are measurable – an advantage that physicists are more likely to appreciate because they use measures often , even if they do not speak about them .
up and anti-up . or down and anti-down . funny thing is , both of those have the exact same quantum numbers - parity , spin , baryon number and the rest . so a neutral pion can be a mixture of ( u + anti-u ) and ( d + anti-d ) . there actually result two types of neutral " pion " that decay differently . one is actually heavier , and we call it the eta meson . oops i did not mention yet the strange and anti-strange quark combination , which also gets tangled into the mixes . . . but it is not important to the neutral pion .
notice that $$\frac{l_{\bigodot}}{\pi r_{\bigodot}^2} = \sigma t_{\bigodot}^4$$ so that , dividing through the relation for an arbitrary star and that for the sun gives : $$\frac{l/l_{\bigodot}}{r^2/r_{\bigodot}^2} = t^4/t_{\bigodot}^4$$ using the other relations $$\frac{ ( m/m_{\bigodot} ) ^{3.5}}{m/m_{\bigodot}} = ( t/t_{\bigodot} ) ^4$$ or $$\left ( \frac{m}{m_{\bigodot}} \right ) ^{2.5} = \left ( \frac{t}{t_{\bigodot}}\right ) ^4$$
calculating the energy eigenvalue , will give you $\langle v^2 \rangle$ . this is how it is done : $$\langle v^2 \rangle = \frac{2}{m}\langle t \rangle=\frac{2}{m}\langle \psi|\hat t|\psi\rangle=\frac{2}{m}\int\psi^* ( x ) \hat t \psi ( x ) dx$$ where you should write $t$ ( the kinematic energy ) as an operator . this can be done by writing it as a function of $x$ and $p$ , and then replacing $p$ with its operator . however this is not what we call the expectation value of speed . to calculate the expectation value of speed , we calculate the expectation value of its momentum : $$\langle v \rangle = \frac{\langle p\rangle}{m}=\frac{1}{m}\int \psi^* ( x ) p\psi ( x ) dx$$ which can be calculated either by transforming $\psi$ to the momentum space , or replacing $p$ with its operator $-\mathfrak i \hbar\frac{\partial}{\partial x}$ . an important thing to note is : $\langle v^2 \rangle \ne \langle v\rangle^2$ in general . e.g. consider the symmetric harmonic potential where $\langle v \rangle=0$ but $\langle v^2 \rangle &gt ; 0$ . the difference is called variance $\sigma_v^2=\langle v^2 \rangle-\langle v \rangle^2$ .
i agree that this might be better for chemistry stackexchange . however i will give you an short answer after doing a little searching . you should look at henry 's law which states that that the concentration , $c$ , of a gas in a liquid at a specific temperature is proportional to the partial pressure , $p$ , of that gas in the atmosphere above the liquid , $$ p=k_hc . $$ so the concentration will be the higher if $k_h$ is smaller . wikipedia also gives a table for a few common gasses of these $k_h$ constants at room temperature $ ( 298.15 k ) $ . for $co_2$ it is equal to $29.41\frac{atm}{mol/l}$ . however the constant of $o_2$ is equal to $769.23\frac{atm}{mol/l}$ , so you would need roughly 26 times higher partial pressure of $o_2$ to get the same concentration . and for nitrogen it is even roughly 56 times higher $ ( k_{h , n_2}=1639.34\frac{atm}{mol/l} ) $ . i do not know how many other gasses also have a relatively low henry 's law constant , $k_h$ . but i believe , if i did the conversion correctly , that ammonia has an even lower value of roughly ( the literature values deviate quite a bit ) $0.02\frac{atm}{mol/l}$ . edit : apparently ozonated water is being used medically . ozone has a henry 's law constant of about $100.30\frac{atm}{mol/l}$ , so it will escape the water eventually and therefore it has to be made before consumption . but i was not able to find a source that ozonated water is fizzy , so i do not know why it does for $co_2$ , it probably has something to do with the fact that it also forms carbonic acid .
hints to the question ( v2 ) : first note that the operator norm $||a||=||ua||=||au||$ of an operator $a$ is invariant if we compose with an unitary operator $u$ from either left or right . therefore $\dot{\rho} ( t ) $ is not the zero-operator : $|| \dot{\rho} ( t ) || = || [ h , \rho ( 0 ) || \neq 0 . $
molten solder has a low contact angle on ( clean ) copper . so if you looked at a cross section of the pipe joint as the solder was flowing in you had see something like : the solder is drawn into the joint in exactly the same way as water rises in a capillary tube . both are correctly described as capillary action .
a voltage or current given as a complex constant is a phasor . a voltage given as the complex constant $v_z$ represents the real voltage $$v ( t ) = \operatorname{re} \left ( v_z e^{i\omega t} \right ) \ \ , $$ where $\omega$ is the voltage 's angular frequency and $t$ is time . currents represented as phasors work the same way .
the square brackets mean antisymmetrization . that is : $$ x_{ [ a_1a_2\dots a_n ] } = \frac{1}{n ! }\sum_{p\in s ( n ) } \text{sign} ( p ) x_{a_{p ( 1 ) }a_{p ( 2 ) }\dots a_{p ( n}} $$ where $s ( n ) $ is the set of permutations of $n$ elements , and $\text{sign} ( p ) $ is the sign of the permutation $p$ , that is , $\text{sign} ( p ) =-1$ if you need an odd number of element exchanges , and $\text{sign} ( p ) =+1$ if you need an even number of element exchanges . in particular , $r_{ [ abc ] }{}^d = \frac{1}{6}\left ( r_{abc}{}^d+r_{bca}{}^d+r_{cab}{}^d-r_{bac}{}^d-r_{acb}{}^d-r_{cba}{}^d\right ) $ $\nabla_{ [ a}r_{bc ] d}{}^{e} = \frac{1}{6}\left ( \nabla_{a}r_{bcd}{}^{e}+\nabla_{b}r_{cad}{}^{e}+\nabla_{c}r_{abd}{}^{e}-\nabla_{b}r_{ac d}{}^{e}-\nabla_{a}r_{cbd}{}^{e}-\nabla_{c}r_{bad}{}^{e}\right ) $ you can " move " indices up and down using the metric tensor . that is , $$r_{abcd} = g_{de}r_{abc}{}^e , \quad r_{abc}{}^d = g^{de}r_{abce} . $$ the square brackets just affects the indices ; the $r$ is inside because the antisymmetrization affects indices both from $\nabla$ and from $r$ .
i find this sort of thing becomes much more intuitive if you can think of an analogy in terms of water . in this case , we can think of it like this : here we have water flowing through a hole in a bath tub , into another tub underneath . the stick figure has been given the task of keeping the water level constant , by lifting water back up into the top tub using a bucket . water plays the role of charge in this analogy , so a rate of water flow is analogous to electrical current . in this situation the water flows through a small hole , which represents the resistor . a small hole tends to resist the flow of water , so it is analogous to a high resistance . a large hole would represent a low resistance . if the stick figure lifts $m\:\mathrm{kg}$ of water in her bucket , she must do an amount of work equal to $mgh$ . if she lifts $i\:\mathrm{kg/s}$ then she must expend power at a rate $p=igh$ watts . if we compare this to $p=iv$ we can see that the height difference between the two tubs ( multiplied by $g$ ) plays the role of $v$ in this analogy , and by maintaining the height difference , the stick figure plays the role of the voltage source . now we have everything we need to see intuitively that the power must increase as $r$ decreases . first imagine that the hole in the top tub is very small , so that the water just drips gently through it . this represents a large resistor , and you can imagine that the stick figure does not have to do a lot of work to keep the water level constant , because the water is being drained very slowly . however , if you make the hole bigger then the flow rate of water will increase , and now the stick figure will have to work harder to keep lifting buckets of water to counter it . the bigger the hole ( i.e. . the lower the resistance ) , the faster the flow , so the more buckets of water she has to lift per second to maintain the water level , which means she has to spend more power to do it . hopefully in this analogy you can see that the low resistance does not cancel out the faster current . instead the faster flow is caused by the low resistance . the power itself does not directly depend on the resistance , but only on the voltage ( or height difference ) and current ( or rate of water flow ) . i have calculated the power that the voltage source ( stick figure ) puts into the system , rather than the power dissipated by the resistor . however , by the conservation of energy , these have to be equal . in the water system , the stick figure puts energy into the system in the form of gravitational potential . this is dissipated by viscous friction as the fluid flows through the hole and settles in the lower container . in the electrical case the battery puts energy into the system in the form of electric potential , which gets dissipated in the resistor . but the principle is the same : in both cases the power you put in has to equal the power that comes out as heat .
the question is clear enough to answer . the question does involve mass defect . the best place to start is with ordinary problems . we can consider a nonrelativistic problem of a particle in a potential well , $v ( r ) ~=~-r^n$ for some power of $n$ . for mathematical reasons $n~=~\pm 2$ have nice properties with closed form solutions . this is input into a schrodinger equation $$ i\hbar\frac{\partial}{\partial t}\psi ( r , t ) ~=~-\frac{\hbar^2\nabla^2}{2m}\psi ( r , t ) ~+~v\psi ( r , t ) $$ so if we consider a stationary phase $\psi ( r , t ) ~=~e^{-i\omega t}\psi ( r ) $ and the frequency determines energy $e~=~\hbar\omega$ $$ \hbar\omega~=~\frac{p^2}{2m}~+~v $$ where the potential energy is negative . if the potential energy were “turned off” so the particles are free then energy is larger . the energy of system is then lower , and if this energy $e~=~mc^2$ is some appreciable fraction of the mass-energy of the system $e’~=~mc^2$ , say $m/m~~\simeq~ . 01$ to $ . 1$ the system is not highly relativistic but the mass equivalence is measurable . for atomic physics the energy levels of electrons are on the order of electron volts , while the mass of electrons are $ . 51$mev . so the mass defect is pretty small . for nuclear physics of nucleons and mesons the energy levels are on the order of $10mev$ while the collection of nucleons has mass-energy in multiples of $1gev$ . the energy levels are determined by $\pi^0 , ~\pi^\pm$ mesons , which are the intermediary gauge bosons between the nucleons ${p , ~n}$ . this in fact forms a doublet which has energy level splitting due to the electric charge difference . the above model may be made more exact if the mesons are considered to be similar to a photon , with a gauge potential $$ {\vec a} ( k ) ~=~{\vec n} ( a_ke^{-ikr}~+~a^\dagger_ke^{ikr} ) $$ which interacts with a dipole formed from the nucleon doublet ${\vec{\cal p}}~=~p{vec\sigma}$ in an interaction hamiltonian $$ h_{int}~=~-{\vec{\cal p}}\cdot{\vec a} ( k ) . $$ we are only considering interactions with one momentum or wave number . now expand that out and keep terms $a ( k ) \sigma^+$ and $a^\dagger\sigma^-$ , which is the rotating wave approximation in atomic interactions with photons . this makes the above schrodinger equation and potential more exact . this interaction hamiltonian will then reproduce the mass-defect . this may be further improved of course . the nucleon doublet is su ( 2 ) , and the meson potential may be extended from this naïve u ( 1 ) approximation to su ( 2 ) as well . the momentum operator may be made covariant with respect of the gauge potential and the theory refined further . in fact the yang-mills theory was derived to understand this isopin theory of nuclear physics as understood in the 1950s .
it is all in what you want to describe mathematically . you can have an n dimensional space and yes , you could " visualize " the analogue of two dimensions going into three . these are euclidean spaces , i.e. the metric is ds* 2=dx *2+dy* 2+dz *2+ . . . . up to n terms . time is the fourth dimension in current physics because we are attempting to describe and predict motions and interactions of matter and light in a mathematical manner , and the equations are such that they simplify when time is assumed to be the fourth dimension in what is called a pseudo euclidean space . , and in our case dt**2 has a negative sign . it is what the physics comes out with that makes time the fourth dimension .
it is a standard terminology – and set of insights – not only in string theory but in quantum field theories or anything that can be approximated by ( other ) quantum field theories at . . . low energies . such a low-energy action becomes very accurate for the calculation of interaction of particles ( quanta of the fields ) of low energies , in this case $e\ll m_{\rm string}$ . equivalently , the frequencies of the quanta must be much smaller than the characteristic frequency of string theory . the previous sentence may also be applied in the classical theory : the low-energy effective action becomes accurate for calculations of interactions of waves whose frequency is much lower than the stringy frequency or , equivalently , whose wavelength is much longer than the string scale , $\lambda\gg l_{\rm string}$ . low-energy effective actions may completely neglect particles whose mass is ( equal to or ) higher than the characteristic energy scale , in this case $m_{\rm string}$ , because such heavy particles can not be produced by the scattering of low-energy particles at all – so they may be consistently removed from the spectrum in this approximation . the scattering of the light and massless particles that are kept may be approximately calculated from the low-energy effective action and this approximation only creates errors that are proportional to positive powers of $ ( e/m_{\rm string} ) $ so these errors may be ignored for $e\ll m_{\rm string}$ . you may imagine that there are corrections in the action proportional to $\alpha'$ or its higher powers that would make the effective action more accurate at higher energies but become negligible for low-energy processes . there are lots of insights – conceptual ones as well as calculations – surrounding similar approximations and they are a part of the " renormalization group " pioneered mainly by ken wilson in the 1970s . in particular , by " low-energy effective actions " , we usually mean the wilsonian effective actions . but they are pretty much interchangeable concepts to the 1pi ( one-particle-irreducible ) effective actions , up to a different treatment of massless particles . it is impossible to teach everything about the renormalization group and effective theories in a single stack exchange answer . this is a topic for numerous chapters of quantum field theory textbooks – and for whole graduate courses . so i just conclude with a sentence relevant for your stringy example : string theory may be approximated by quantum field theories for all processes in which only particles much lighter than the string mass are participating and in which they have energies much smaller than the string scale , too . if that is the case , predictions of string theory for the amplitudes are equal to the predictions of a quantum field theory , the low-energy effective field theory , up to corrections proportional to powers $ ( e/e_{\rm string} ) $ .
an ' energy saving light bulb ' is a fluorescent lamp . those lamps are filled with a gas containing low pressure mercury vapor and noble gas . to generate light , an electrical discharge is sent through an ionized gas . the excited atoms produce ultraviolet radiation . the inner surface of its tube is coated of various metallic elements such as rare earth . those rare earth elements when crossed by uv radiation emits red , blue and green lights , which together are percieved as white . regular bulbs are incandescents light which emit light by heating the filament present inside the bulb . as there is no uv radiation , there is no reason to use rare earth ! references : wikipedia - fluorescent lamp what is inside a low-energy light bulb ? ( reference given by luboš motl )
neutrinos are leptons , they have leptons number just like the charged leptons ( electron , muon , tau ) . weak interaction conserve not only the global lepton number , but the lepton flavor numbers as well . and that is how we identify their flavors : electron neutrinos participate in reactions that involve electrons and muon neutrinos participate in reaction that involve muons . we know that they are not the same because we have intense sources of muon neutrinos and muon anti-neutrinos ( from cosmic rays and accellerator created muons and anti-muons ) and we have intense sources of electron anti-neutrinos ( reactors ) . and when we put detectors in front of these sources the two flavor behave differently ; the most diagnostic interaction ( and a common one ) is quasi-elastic scattering in which a charged lepton out---a muon or an electron depending on the flavor of the beam . we do not have any intense sources for tau neutrinos , but they have been identified in the oscillated input to both opera ( which was designed for that measurement ) and icecube .
the whystringtheory page is written in a popularized style that makes it impossible to tell what they really have in mind . their statement does not make sense if interpreted according to the standard technical definitions of the terms . gr does not describe gravity as a force . in the system of units normally used in gr , with g=c=1 , force and power are unitless , so there is also no natural motivation for defining something like a planck force by analogy with the planck length , etc . possibly their " force " really means curvature , in which case this could be interpreted as a correct statement that gr breaks down when the riemann tensor corresponds to a radius of curvature comparable to the planck length .
the dimension of the string is a special case of the concept of dimension for a much more general class of objects called manifolds . manifolds are a mathematical abstraction and generalization of the concept of a surface ( like the surface of a sphere ) . the dimension of a ( real ) manifold is , roughly speaking , the number of coordinates ( real numbers ) necessary to specify a point on the manifold . for example , the surface of a sphere is two-dimensional because any point on the surface can be uniquely identified by a particular lattitude and longitude , each of which is a real number . a string is a one-dimensional manifold ( or manifold with boundary in the case of open strings which have endpoints ) because it takes precisely one real coordinate to specify each point along the string . in the case of open strings , one can take the coordinate $t$ along the string to be some real number in the closed interval $ [ 0,1 ] $ where $0$ is the coordinate for one end of the string , $1$ is the coordinate for the other end , and any point in between has some coordinate between $0$ and $1$ . for open strings , one can take the coordinate $\phi$ to be an angle because closed strings are just loops . simply choose one of the points along the string to correspond to the angle $0$ , then going once around the string corresponds to going once around the unit circle in the complex plane , and one gets back to the original point after a total angle of $2\pi$ radians .
at low velocities like this you can ignore special relativity and simply add the two velocities . this is really easy to see if you imagine yourself standing still and the earth moving under you . relative to you the gun should fire just like you were standing still . this is called an inertial frame of reference . you see the bullet leave at $400\: \mathrm{m/s}$ ( relative to you ) and the earth sees the bullet leave at $800\: \mathrm{m/s}$ .
presuming that there are not nonlocal constraints , a differential operator that is polynomial in differential operators is local , it does not have to be quadratic . my understanding is that irrational or transcendental functions of differential operators are generally nonlocal ( though that is perhaps a question for math . se ) . a given space of solutions implies a particular nonlocal choice of boundary conditions , unless the equations are on a compact manifold ( which , however , is itself a nonlocal structure ) . there is always an element of nonlocality when we discuss solutions in contrast to equations . [ for the anti -locality of the operator $ ( -\nabla^2+m^2 ) ^\lambda$ for odd dimension and non-integer $\lambda$ , one can see i.e. segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ( for a review of this paper , see here ) . ] edit : sorry , i should have gone straight to hegerfeldt 's theorem . schrodinger 's equation is enough like the heat equation to be nonlocal in hegerfeldt 's sense . there are two theorems , from 1974 in prd and from 1994 in prl , but in arxiv:quant-ph/9809030 we have , of course with references to the originals , theorem 1 . consider a free relativistic particle of positive or zero mass and arbitrary spin . assume that at time $t=0$ the particle is localized with probability 1 in a bounded region v . then there is a nonzero probability of finding the particle arbitrarily far away at any later time . theorem 2 . let the operator $h$ be self-adjoint and bounded from below . let $\mathcal{o}$ be any operator satisfying $$0\le \mathcal{o} \le \mathrm{const . }$$ let $\psi_0$ be any vector and define $$\psi_t \equiv \mathrm{e}^{-\mathrm{i}ht}\psi_0 . $$ then one of the following two alternatives holds . ( i ) $\left&lt ; \psi_t , \mathcal{o}\psi_t\right&gt ; \not=0$ for almost all $t$ ( and the set of such t 's is dense and open ) ( ii ) $\left&lt ; \psi_t , \mathcal{o}\psi_t\right&gt ; \equiv 0$ for all $t$ . exactly how to understand hegerfeldt 's theorem is another question . it seems almost as if it is not mentioned because it is so inconvenient ( the second theorem , in particular , has a rather simple statement with rather general conditions ) , but a lot depends on how we define local and nonlocal . i usually take hegerfeldt 's theorem to be a non-relativistic cognate of the reeh-schlieder theorem in axiomatic qft , although that is perhaps heterodox , where microcausality is close to the only definition of local . microcausality is one of the axioms that leads to the reeh-schlieder theorem , so , no nonlocality .
i will make this an answer , even though it is more of a drawn out comment . as i mentioned as a comment , computing the potential energy is trivial . if you want speed , you will probably want to look at fast methods for long-range interactions . the link takes you to state-of-the-art libraries and methods , but any introductory book on computational statistical mechanics ( or molecular dynamics ) will explain ewald sums . this is to say that i do not think you will be able to analytically calculate the stuff you want , but these methods should cope well with thousands of particles ( or maybe hundreds if you want real-time performance ) . the entropy is tricky for several reasons . first of all , it is difficult to compute in general from simulations . also , i do not know if you even can in general compute absolute entropies . so what we are left is the relative entropy between two states . what are the two states ? herein lies the next problem : entropy is an equilibrium quantity . so , say you want to compare the entropies between t = 1 and t = 2 . this is to say that you want systems that are equilibriated with the charges that would occur at t = 1 , and t = 2 . to this effect you can do , for example , thermodynamic integration . this is an expensive computation .
the variable $r$ represents the position in space at which you wish to evaluate the wave function $\psi$ . this is indicated by $\psi ( r ) $ . the variable $r'$ is a spatial integration variable . it represents the points at which you are integrating the function $gv\psi$ . an analogy to electrostatics might help . when you want to evaluate the electric potential $v$ at a point $r$ due to some charge distribution $\rho$ , you do $$v ( r ) =k\int \frac{\rho ( r' ) }{\left|\vec r-\vec r'\right|}dr ' . $$ the variable $r'$ represents points where there is some contribution from your charge distribution to your electric potential . likewise , in your qm example , the variable $r'$ represents points where there is a contribution from the potential ( and other causes ) to the wave function itself .
the " lift " produced by a sail is ( primarily ) directed horizontally . the term lift is used since the mechanism is the same as the one that produces lift on an aircraft wing . the key concept is that both wings and sails are airfoils ; the only difference is that wings are ( typically ) oriented horizontally , and sails are ( typically ) oriented vertically . due to the way it modifies the airflow around it , a horizontally oriented wing experiences a vertical lift force ; the sail modifies the airflow around it in exactly the same way ; however , because it is oriented vertially , the corresponding " lift " force is oriented horizontally . consider the usual type of picture of how a wing provides lift : imagine taking an airplane 's wing , and rotating it ninety degrees so that it is sticking straight up out of the ground . also imagine that there is wind blowing onto the wing . now , consider the above picture as though you were suspended above this vertical wing . due to the motion of the air across the wing , this upward pointing wing experiences a horizontal " lift " force . the only difference between the wing and the sail is that the base ( flatter ) side of the sail is not filled in ; what is going on is that there is a ( relatively ) stationary pocket of air sitting inside of the curve of sail , so that as the wind blows by it , it passes by the sail just like in the diagram above ( i.e. . imagine that the grey area is a stationary pocket of air ) . " the physics of sailing " page provides some useful diagrams spelling out the vectoral forces on sailboat . this link correctly describes the lift as being related to the change in momentum of the air in some places , but also refers to bernouli 's princple in others . its easy to conflate bernouli 's principle with the equal times fallacy , c.f. this question for more details on the mechanisms of lift .
this question involving " randomness " and quantum mechanics introduces some subtleties . firstly we have the definition of " randomness " to consider . it turns out that there are various ways to define this term : the two i shall consider here are ( the digit sequence of a ) normal number ( mentioned in the question ) and martin-lof randomness . as explained in the wikipedia article a " normal number " is " finite state machine random " . so it looks random to a fsm . however such numbers can be computable by a turing machine ( as the link shows that turing proved ) . a martin-lof random sequence is based on the familiar notion of incompressibility and cannot be computed by a turing machine . phrased alternatively because it is an infinite sequence it will not have a finite compression onto a turing machine - so there can be no program for it ( which has to be finite ) . the logical link between the two is that every martin-lof random sequence is normal ( but not conversely - as shown by turing ) . also note that every finite sequence can be generated by an algorithm ( and also it will be the solution of a polynomial ) . the reason why the random sequences work is that although the first n digits can be replicated via a program p , a different program ( in general ) is required for the first n+1 digits of the sequence ie p will fail to " predict " n+1 . to get the entire sequence requires an infinite series of programs so we are back where we started with the random sequence . now that we have some definitions available we can examine connection to physics . the problem in a experimental based theory is that we only ever have a finite amount of data . thus the claim that a given series is " random " is strictly not empirically provable . so this puts the copenhagen claim that " qm sequences are random " into an awkward semi-scientific status . such claims cannot be proved experimentally , yet copenhagen asserts this . so where is the proof ? indeed what constitutes a proof ? also we have seen several definitions of randomness ( there are more ) - so which type of randomness does qm have exactly ? to return to specific points in the op question : is there any way to differentiate if they came from a truly random or from a formula/algorithm ? how ? no , because one only ever has a finite amount of stream data to analyse , which can always be explained algorithmically ( as discussed above ) . if there is no way to decide this , then , i can not find any basis , to keep denying that behind the " truly random " of quantum mechanics can be a hidden algorithm . the " truly random " of quantum mechanics might be martin-lof randomness , for which there is no algorithm ; however if it is really normal number randomness then there might be an algorithm . i suspect that most physicists take the view that qm is as " random as it gets " , hence would prefer the martin-lof option . i know i am talking about the possibility of a " hidden variables " theory , but i can not find any other explanation . the link between algorithmic underlying structure and " hidden variables " would appear to be close . if its a non-algorithmic type of randomness then we have to decide whether it belongs to one of " oracle classes " associated with martin-lof randomness , and what that would mean in terms of " hidden variables " . some readers might recall that in his book " the emperor 's new mind " in 1989 roger penrose proposed that aspects of quantum mechanics were " non-computable " . although that argument was not formulated as i have here , it is consistent ( i believe ) with the idea of martin-lof randomness too .
the second law of thermodynamics - about the increasing entropy - which is apparently what you are talking about - holds for any system . permanent magnets are no exception . a ferromagnet may look " more ordered " than a non-magnetic material because the spins are oriented in the same direction , rather than random directions . but physical systems may only try to maximize their entropy among configurations that conserve energy ( much like the momentum , charge , and other conserved quantities ) . for ferromagnets , the configuration with spins oriented in random directions would have a much higher energy - because one reduces the energy by orienting the spins , elementary magnets , in the same direction . so the spontaneous disappearance of the uniform electrons ' spin would violate the energy conservation . among the configurations with the same energy , the magnet still tries to maximize its entropy . in particular , the heat is flowing from warmer pieces of the material to colder ones , and so on . more generally , the entropy never goes down , and that is the only general statement that follows from the second law of thermodynamics . ferromagnets are not special among physical objects that could have a higher entropy if you allowed the energy to increase . for example , any object would raise its entropy - the amount of disorder - if its temperature increased . but a higher temperature requires a higher energy , too . one can not violate the first law of thermodynamics ( energy conservation ) just because it would make it more straightforward to satisfy the second law . both of them hold in nature .
i will give it a shot . spoiler : i did this in the body frame so that the moment of inertia is time independent , before you get excited . . . starting with euler 's equations : $$ i_i\dot{\omega}_i+ ( i_j - i_k ) \omega_j \omega_k = 0 $$ and taking cyclic permutations of $i , j , k$ to get the three of them ; and in the absence of torques ( i ignore air friction ) . it is a symmetric top so $i=i_1=i_2 \neq i_3$ so write $$ \dot{\omega}_1 = -\frac{ ( i_3-i ) }{i}\omega_2 \omega_3 $$ $$ \dot{\omega}_2 = -\frac{ ( i - i_3 ) }{i}\omega_1\omega_3 $$ $$ \dot{\omega}_3=0 \implies \omega_3=k_1 $$ now for this problem the coin is spinning about one of the first two symmetric axies . i chose 1 . then consider small variations on the other two angular velocities from zero : $\omega_2 = \delta\omega_2$ , $\omega_3 = \delta\omega_3$ , and $\omega_1 \rightarrow \omega_1$ . so we make small changes in how the coin is rotating about a line through its center perpendicular to the coin , and about the other symmetric axis . in other words , it was spinning ideally like a coin would , then we changed the ideal to a little weird spinning . making the changes , and ignoring second order in perturbations : $$ \dot{\omega}_1=0 \implies \omega_1 = k_1 $$ $$ \frac{d}{dt} ( \delta\omega_2 ) =-\frac{ ( i-i_3 ) }{i}\omega_1 ( \delta\omega_3 ) $$ $$ \frac{d}{dt} ( \delta\omega_3 ) =0 \implies \delta\omega_3 = k_2 $$ then we can write $$ \frac{d}{dt} ( \delta\omega_2 ) =-\frac{ ( i-i_3 ) }{i}k_1 k_2 $$ everything on the r.h. s is a number so $$ \delta\omega_2 = -\left ( \frac{ ( i-i_3 ) }{i}k_1 k_2 \right ) t $$ so depending on how big $i$ is compared to $i_3$ will determine how $\delta\omega_2$ changes during the flip . if one uses a radius of $r=0.014$ m and $h=0.0015$ m for the hight of the coin , one gets a moment of inertia tensor like the following : $$ i=m ( 0.0000491875 ) \quad i_3 = m ( 0.000098 ) $$ which tells me that the variations are unstable . . . which i do not really believe since i have seen a coin in real life . so look this over . but i can not find anything wrong so i am going with it , and thinking that i can not really see a coin in real life up close while it is spinning . . . hope this helps .
in practice , the apparatus measuring the spin should be localized somewhere in space ( it cannot fill the whole universe ! ) and this fact implies that you always make a measurement of position ( actually very rough in general ) , even if you are measuring the spin . suppose that $\omega \subset r^3$ is the bounded region in $r^3$ where the apparatus is localized . the simplest ( naive ) mathematical model of the apparatus i could imagine is the following . the yes-no observable associated with the apparatus measuring , say , if the spin is directed along z+ , has the form of the orthogonal projector : $$p_{\omega} \otimes p_{z+}$$ here $p_{z^+} = |z+\rangle \langle z+|$ is the obvious projector in $c^2$ along the states with spin $z+$-directed , whereas $p_\omega$ is the operator ( orthogonal projector in $l^2 ( r^3 ) $ ) $$ ( p_\omega \psi ) ( x ) = \chi_\omega ( x ) \psi ( x ) \: . $$ this observable admits two values ( its eigenvalues ) $0=$ no and $1=$yes . yes means that the particle is found in $\omega$ and the spin is found to be directed along $z+$ . no means that the the particle is not found in $\omega$ or the spin is not along $z+$ . there is another elementary yes-no observable associated with the spin detected along the direction $-z$ , with analogous meaning . it is the orthogonal projector : $$p_{\omega} \otimes p_{z-}\: . $$ the observable associated with the spin along $z$ -- referring to this experiment -- is not the standard operator $s_z = \sigma_z/2$ ( i am assuming $\hbar =1$ ) . it is instead constructed , via spectral decomposition , taking the above elementary observables ( projectors ) into account and combining them with the corresponding values of the spin ( which turn out to be the eigenvalues of the overall observable ) . $$\gamma_{z , \omega} = \frac{1}{2}p_{\omega} \otimes p_{z+} - \frac{1}{2} p_{\omega} \otimes p_{z-} = p_\omega \otimes s_z\: . $$ you see that it includes a rough measurement of the position : it just checks if the position of the particle is in $\omega$ . to measure the spin of the particle the supports of the components $\phi_i$ must have a non-negligible intersection with $\omega$ . in general the measurement procedure of the spin , for instance along $z+$ , even affects the surviving component $\phi_{+1/2}$ . you see that , only if the support of $\phi_{+1/2}$ is completely included in $\omega$ , the wavefunction is not affected by the measurement of the spin , otherwise , after the procedure ( supposing to have found spin $+1/2$ ) , the state , up to a normalization constant , is described by : $$p_\omega \phi_{+1/2} \otimes |z+\rangle \: . $$ it is questionable if we have defined an observable $\gamma_{z , \omega}$ in that way . the point is that $\gamma_{z , \omega}$ admits a third eigenvalue , $0$ , associated with the projector $p_{r^3-\omega} \otimes i$ . actually , there is no real measurement in the region $r^3-\omega$ , since we are not assuming that there are detectors therein . we are simply using the argument : " if the particle is not found in $\omega$ it must be found outside it " . for several reasons i am always a bit suspicious to this sort of formal arguments . a more physically safe interpretation could be that we are performing a conditioned measurement of the spin . $p_\omega$ is nothing but a filter : only the particles which pass through it are measured .
in the frame of reference of the body , is the centripetal force felt or is only the centrifugal force felt ? it depends on what you mean exactly . consider , for example , the amusement park ride dumbo at disneyland : . on this ride , passengers sit in mini dumbo replicas and are swung around in a circle . what forces do they feel ? well , firstly , they feel a centrifugal force radially outward . but this is not all . if that were the only force they felt , then in the frame that is stationary with respect to dumbo , they would accelerate radially outward . instead , they also feel a normal force of dumbo pushing them inward that is precisely equal to the centrifugal force , and as a result , as measured in the dumbo frame , they remain stationary with respect to dumbo . now , we know that if we were to analyze the same situation from the frame of reference of a person watching the ride from the ground , then we would say that there is only one force on the passengers , namely the normal force of dumbo on them , and this force causes the passengers to accelerate , namely to move in a circle . as a result , the convention is to call the normal force the " centripetal " force . i personally think this is terrible terminology that confuses students because it leads them to believe that " centripetal force " is somehow an independent thing that does not need to be comprised of real physical interactions with objects . . . by anywho . now , going back to the accelerated frame , we had noticed that there were two forces acting on the passengers , the ( fictitious ) centrifugal force , and the normal force . would you now call the normal force a " centripetal " ? if we are doing the analysis in the accelerating frame , then that would be extremely non-standard because in that frame , no circular motion is occurring . does a body only feel the effect of pseudo forces in an accelerated reference frame ? no ! just look at the above example ! the passengers feel the centrifugal force , but they also feel a normal force due to their interaction with dumbo ! in general , there can be all sorts of forces that an object feels in an accelerated frame that are not pseudo forces like friction , gravitational forces , electromagnetic forces etc .
" total energy of the earth " is somewhat of an odd concept , but there is no reason we can not really entertain it . it brings up some genuinely difficult questions . the right way to approach this is to define the system correctly and then identify forms of energy content and flows . things to " count " in the earth 's energy : heat content nuclear energy rotational energy gravitational energy as i look at this list , i believe that all of them are steadily decreasing . nuclear decays in the earth 's core continue over time , and this converts nuclear energy into heat content . i have heard that nuclear decay comprises a large fraction of the geothermal energy conducting through the crust , so it follows that the nuclear energy is declining at a similar rate as the heat content . the rotational energy is constantly being transferred to the moon slowly , and this is similar to the maximum theoretical tidal energy that could be extracted . heat energy , of course , is lost by blackbody radiation to space . global warming " blankets " our planet a little more , so it would initially decrease this . however , the heat content of the oceans and biosphere ( which have the capability to absorb this energy ) are small compared to the total earth . there has always been a deficit between earth 's radiated energy and the sun 's incoming energy which is from the nuclear and thermal energy of the planet . earth has always been on-net losing energy to space by radiation . an increased greenhouse gas could theoretically change this , and cause the earth to keep more of its heat . however , it is small compared to earth 's natural flows . perhaps after some past super-volcano the earth temporarily gained energy . however , that is certainly not the case today . i will point out a slight fallacy in the question : many years ego earth was hot and over time has lost energy and has got colder . if normal heat content was the only store of energy , this would be a logical connection . losing energy would mean lowering temperature . however , other stores of energy are present , notably rotational , nuclear , and gravitational . the conversion of these into thermal energy is strange . rotational turns into tidal heating , and then contributes to the radiation deficit . nonetheless , if the earth is taken as a whole , only the large center should matter significantly . that has almost certainly gotten cooler over time , in addition to releasing stored nuclear energy . if nuclear heat production was large enough , or if the mantle was insulating enough , it could have increased temperature because the heat from nuclear decay was not dispelled to the surface fast enough . venus , for instance , may have had cycles where the planet stored up extra energy , and then an event where the mantle went through a massive volcanic shift .
http://www.antenna-theory.com/antennas/shortdipole.php is a website with useful info . , including formulas . to oversimplify , it seems to say that once the antenna is a tenth or less of the wavelength , the exact ratios do not matter so much . the antenna is inefficient , but it works for both sending and receiving . if you can detect the signal , of course you can amplify it as much as you want .
firstly , note that they postulate those commutation relations in the beginning of section 3.5 in order to show that they are wrong , which they demonstrate in the ensuing pages . the ultimate point is to show that one needs to impose anti-commutation relations on fermionic fields . in fact , the correct relations are postulated in equation 3.96 ; \begin{align} \{\psi_a ( \mathbf x ) , \psi_b^\dagger ( \mathbf y ) \} and = \delta^{ ( 3 ) } ( \mathbf x - \mathbf y ) \delta_{ab} \end{align} you could then ask , are these equivalent to the anti-commutation relations of the mode operators that they write in ( 3.97 ) ? namely , \begin{align} \{a^r_\mathbf p , {a^s_\mathbf q}^\dagger\} = \{b^r_\mathbf p , {b^s_\mathbf q}^\dagger\} = ( 2\pi ) ^3\delta^{ ( 3 ) } ( \mathbf p - \mathbf q ) \delta^{rs} \end{align} and the answer is yes . to show that the second set implies the first , write the fields in their integral mode expansions , compute the anti-commutator of these integral expressions , and apply the anti-commutators between modes . to show that the first set implies the second , invert the integral expressions for the fields in terms of the modes to obtain integral expressions for the modes in terms of the fields , and do the analogous thing . main point . the commutators/anti-commutators between fields are equivalent to the commutators/anti-commutators between modes .
the tl ; dr version : even if we could form a synthetic event horizon , it would not help us learn about the black hole interior . the long version : the phenomena you describe where light essentially orbits a black hole is called the " photon sphere " and it does not happen at the event horizon . the radius of a black hole , $r$ is where the event horizon is and the photon sphere where photons can orbit ( unstable orbits ) is $\frac{3}{2} r$ . you do not actually have to form an event horizon to form the photon sphere although i am pretty sure the density needed to form the photon sphere is greater than the quark degeneracy pressure and so you had still get a collapse into a black hole . unfortunately there are not any magic tricks and everything we know about general relativity and quantum mechanics says that even if we were infinitely advanced technologically we would not be able to learn anything about what happens beyond the event horizon . other than the possibility of a firewall at the horizon , there is nothing special or interesting about either the photon sphere or horizon . the only way we are going to learn about what is inside of a black hole is with a good theory of quantum gravity . no amount of making one to run experiments will help us .
you can analyze this by imagining a tiny tiny gap between the two masses . physically we have exactly that , as the electrons at the surface of the first block are certainly not in contact with the electrons at the surface of the second block . then we have a series of two collisions : the first is the initial impulse , the second is the collision between the two objects . the answer will differ , of course , depending on the degree of elasticity of the collision .
the presence or absence of inversion symmetry in a medium has a direct impact on the types of nonlinear interactions that it can support ; specifically , media which do have inversion symmetry cannot support nonlinear effects of even order . the reason for this is that adding an even harmonic to the fundamental will yield an asymmetric dependence of the electric field , and this is only possible if the medium itself is asymmetric . " inversion symmetry " is the property that the material remain the same when you change the position $\mathbf r_j$ of each particle $j$ to its ' inverse ' , $-\mathbf r_j$ . because we can typically move materials around , this is equivalent to saying that the medium is identical to its mirror image . this is true , for example , for a gas ( if the $\mathbf r_j$ are random , then the $-\mathbf r_j$ will also be random ) , or for crystals like body-centred cubic lattices : on the other hand , certain lattices do not have this symmetry , like you get when you displace the atom in the centre towards one of the faces of the cubic unit cell : here the symmetry is broken , and if you invert all the coordinates with respect to ( say ) the middle atom , you no longer recover the original lattice . it is fairly easy to see why this asymmetry is necessary for second-harmonic generation . suppose that the second harmonic has a maximum in the $+z$ direction at the same time as the fundamental , so that they add constructively . if you wait for half a period of the fundamental , its electric field will be in the $-z$ direction , but the second harmonic will have undergone a full period and will be pointing towards $+z$ , so that the two interfere destructively . this means that the maximum of the total field is stronger in the $+z$ direction than it is in the $-z$ direction . this is actually quite remarkable ! in particular , the medium itself needs to be asymmetric to " know " which direction the stronger fields need to go towards . if the medium has inversion symmetry , then the $+z$ and $-z$ directions are equivalent , and an asymmetric output like this is impossible . consider , on the other hand , a process with odd order like third harmonic generation . here a half-integral period of the fundamental is also a half-integral period of the harmonic , which means that they add in the same direction in each half-cycle , and the output is symmetric . in fact , this selection rule - the forbiddenness of even harmonics in inversion-symmetric media - goes all the way up the harmonic scale , including phenomena where the field is strong enough to break out of the perturbative treatment in david 's answer . the nicest example is high-harmonic generation , which you get in gas jets when the driving laser is intense enough that the laser 's electric field roughly equals the internal electric fields of the atom . in that case , you get a reasonable response at very high harmonic orders ( the record is around 5000 ( doi ) ) , and you get a very flat plateau in which the response does not really drop with the harmonic order : note , in particular , that all the even harmonics are missing . in here i am plotting the response of a single , symmetrical atom , which means that even orders cannot appear . for inversion-symmetric media , then , this relation holds all the way up the even-integers scale .
the ability to " avoid the singularity " is generally regarded as a special property of the very special , stationary , exact solutions we know for black hole space-times . it has to do with the analytic continuation of the solution of einstein 's equations beyond the region that one can predict based on causal principles . ( basically , if you only know that there is a black hole , and you only know what goes on outside of the black hole , you cannot predict what happens inside the black hole ; if you actually see the black hole form , on the other hand , you may have a chance at making this prediction . ) so one should not take that possibility too seriously . ( i for one will not bet my life on it and jump into a charged black hole . ) in fact , one interpretation of the strong cosmic censorship conjecture is precisely that for generic black holes , the singularity is unavoidable once you entered the event horizon . the bound on the charge-to-mass ratio is , at the present day , more of an imprecise conjecture than a stated fact . there are several problems with that statement : in a dynamical space-time , mass-energy can radiate . so the definition of the " mass " of a black hole is already problematic . ( this is also related to the fact that mass in general relativity cannot be defined locally ; though there are a lot of work put into definitions of quasilocal mass . ) similarly , the charge of a black hole , in a general dynamical space-time , is not well-defined . what we do know is that we have a three parameter family of exact , stationary solutions to einstein 's equation depending on $m , a , q$ . because these solutions are all stationary , the mass and charge are well-defined . because these solutions are all axisymmetric , angular momentum is well-defined . and within this family we know that were the charge $q$ to exceed the mass $m$ , the formula that gives the expression of the metric tensor still makes sense as a solution to einstein 's equations , but the formula will lead to solutions with no event horizons . so we conjecture that this is a general fact , despite not knowing how to define the mass and the charge of a generic black hole . now , there are some cases where this conjecture is known to be true for dynamical solutions . for example , in spherical symmetry , if we also assume that we have , in addition to the electromagnetic field , some other " good " uncharged matter fields ( this makes the electromagnetic field non-dynamical , but the gravitational field is still dynamical ) , then we can prove such a statement using the hawking mass of the black hole . these types of statements are related to penrose-type inequalities , most of which are conjectural and only a few have been proven to hold generally . ( remark : there is however evidence that one cannot start with a subextremal black hole and then ``supercharge'' it . ) the horizon area of charged black-holes are smaller than that of uncharged ones . ( again , because of the difficulty in definition , interpret the above in terms of the known stationary black holes . ) your last statement in the third paragraph is incorrect . the answer to your general question about the interplay and the mechanism is : " no one really knows " . the problem is that general relativity , unlike classical newtonian mechanics coupled to electro-magnetism , or even special relativistic mechanics coupled to electro-magnetism , is a highly nonlinear theory . in classical electrodynamics , the linearity of the system allows you to pin-point the contributors to the dynamics : you can say that the total force acting on this particle is a sum of the gravitational force plus the lorentz force plus this-and-that . in gr , because of the non-linear feedback , it is in general impossible to disentangle the sum of the parts from the whole . ( while the equivalence principle tells you that locally in inertial frames stuff behave as it were linear , the sort of questions you were asking necessarily involve long range effects of gravity and electromagnetism . ) to summarise : there is still too much we do not know , even with regards to the basic definitions , in general relativity to be able to answer your questions . we do not have a completely satisfactory definition of black holes that can be used locally ( as oppose to teleologically ) , and we do not know what the local definitions of mass or charge should be . we certainly do not have a general description of how black holes should behave under the influence of electric charge . what we do have is a rather limited zoo of examples . this dearth of data points means that a lot of different conjectures can be made to fit those data points it is hard enough to know even which of those conjectures are right , nevermind to try and understand the principles behind such behaviour .
given that the universe is expanding because space itself is expanding , is that expansion occuring in all places and on all scales ? oversimplifying a little , the answer is that expansion can occur on any scale , but it does not occur for tightly bound systems . see this question : can the hubble constant be measured locally ? if that photon is unchanging , would not that mean that its apparent wavelength is decreasing relative to the expanding space it is travelling through ? you have this backwards . if you like , you can interpret cosmological redshifts as expansions of the space occupied by electromagnetic wave-packets . in this description , cosmological expansion does not decrease the wavelength , it increases it .
no , the cornea is mostly fibrous - not crystalline . as for the physics of optical transparency - here 's an elementary introduction in order to go through a material evenly , light has to avoid two things : being absorbed being scattered off into another direction . see also why glass is transparent ? why is air invisible ? how can a body be transparent ? why is the atmosphere transparent in the visible spectrum ? why does paper become transparent when smeared with oil but not ( so much ) with water ? why wet is dark ? transparency of materials why isn&#39 ; t light scattered through transparency ?
yes , at least for functional enough lens or mirrors . as long as the answer is " yes " , the lenses or mirrors will produce an image that is totally sharp . if the answer were " no " , a point-like source of light would always look like a fuzzy disk . the lenses and mirrors and telescopes may be optimized at least for a whole " two-dimensional locus " where the light source may be located to produce sharp images . the shape of the lenses may be constructed so that this condition is satisfied exactly : a real condition ( the light ray gets to the right point ) has to be satisfied for each distance $y$ from the axis - but one has at least one variable , $x ( y ) $ ( the thickness of the lens ) , to adjust for each $y$ , too . in the first subleading approximation , the shape is always the same : as a function of the vertical coordinate $y$ , the thickness of the glass in the horizontal direction goes like $a+by^2$ . that approximates a circle , parabola , hyperbola , or anything else , up to errors of order $o ( y^4 ) $ . if you want to totally neglect those terms , it is like neglecting $o ( \theta^4 ) $ terms , quartic in the angle . in this approximation , the angle between the light ray and the horizontal line that is needed for convergence is small and linearly depends on $y$ . the linear dependence of the angles on $y$ - how much the direction of the light ray is changed when switching from the air to the glass or back - is equivalent to the quadratic shape of the mirror sketched above ( the angle change is a derivative of the shape because this derivative determines the slope of the glass at a given point ) . this explains why it is not a " miraculous conspiracy": the change of the angle is a linear function of $y$ so the shape of the glass must be a quadratic function of $y$ . in reality , where the $o ( \theta^4 ) $ terms in the shape can not be neglected , the light sources may be away from the plane where the convergence was guaranteed , and in that case , the answer will be " no " and the image will inevitably be fuzzy . however , it is important to note that e.g. telescopes that observe stars " at infinity " may always be adjusted so that all the images are sharp . because the shape of the lens actually has two functions , $x_{left} ( y ) $ and $x_{right} ( y ) $ to be adjusted , one may actually guarantee that the condition " light rays converge " is exactly satisfied in a whole region of 3d space , at least in some situations .
one possibility is that your approach is hugely sensitive to measurement uncertainty : integrating noisy signals can be a huge problem . you might think about ways to average the measurements over time , so that they are ( hopefully ! ) more stable . another possibility is that your instrument does not output the data in quite the format that you think . have you verified that your inputs are sensible ?
the voltage through a and b at the start instant should be equal to the original voltage of the battery 240v . to be sure , it is the voltage across a and b , not through - voltage is across , current is through . however , how can we determine whether vab is +240v or -240v ? kirchoff 's voltage law . the kvl equation for this series circuit , assuming the switch is closed , can be written as : $$v_{ab} = 240v - i\cdot10\mathrm k \omega$$ and , initially , $i = 0$ . if the potential different at the start instant is 240v , why should the current equals 0a ? ( 1 ) to satisfy the above kvl equation . ( 2 ) we also have $v_{ab} = l \frac{di}{dt} = 2.5mh \frac{di}{dt}$ . so the voltage across the inductor is proportional to the rate of change of current , not the instantaneous current . when the switch is closed , the instantaneous current is initially zero since the current must be continuous ( else the derivative in the above equation does not exist ) . however , $\frac{di}{dt}$ is not continuous ; the rate of change ' jumps ' from zero , before the switch is closed , to $\frac{240v}{2.5mh}$ the instant the switch is closed .
an aspect of this that is not covered by the other answers is the following : you say " we are assuming that an externally applied force is pulling the wall out . it is not the gas that is pushing the wall out . " but imagine for a moment that the wall is not moving . the gas is pushing on the wall , so in order for it to remain stationary there must be another force pushing in the other direction . now , if we suddenly removed this pushing force and started pulling on the wall , it would move very rapidly , because our pull would be added to the gas 's push . if we want the wall to move slowly then we should not stop the force that is pushing the wall inwards , but just reduce it very slightly , and allow the gas to do the work of pushing the wall outwards . so if the wall is moving slowly then it really is the gas that is doing the work of moving it , and not a pulling force from somewhere else . that is why energy can be lost from the gas in this situation without violating the conservation of energy . update to address a comment russ comments comments that if the wall was not moving then no work would be done . the question , then , is what makes the case of a moving wall different ? the short answer is that work $=$ force $\times$ distance , so if the wall does not move then distance $=0$ and so no work is done , whereas if it does move then the distance is greater than $0$ and so the work is positive . however , that might not be very enlightening , so let 's do a little thought experiment . let 's replace the wall with a piston . we will not assume this piston has an infinite mass . let 's say that the piston is rigid ( held together by electromagnetic forces , as russ says ) . we will say that it is not moving right now and is held in place just by being a very tight fit to its mantle . ( "mantle " being the technical term for the cylindrical sleeve that a piston fits into . ) outside of the piston is a vacuum , just so that there is nothing else pushing on it . this is the same situation as the box - the piston is held in place by its rigidity and its connection to the sides - it is just that making it into a piston makes the next bit easier to explain . now let 's imagine that the gas starts to push the piston outwards very slowly . in this case the pushing force of the gas is almost , but not completely , balanced by the friction of the piston against the mantle walls . we know there must be friction because if there was not then the gas would push the piston out very rapidly . now , since there is friction there must be heat generated as the piston rubs against the mantle . heat is a form of energy , so the question to ask yourself is , where is that energy coming from ? there is nothing touching the piston other than the mantle and the gas , so the only place it really can come from is the kinetic energy of the gas molecules . the total amount of heat produced is equal to the force the gas exerts on the piston , multiplied by the distance it moves . if you understand this example it might help you to see why the case where you remove a sequence of stationary walls is different . in this case there is nothing moving against a frictional force , so there is no heat being generated , and there is nowhere else that the molecules ' kinetic energy can go . this is why the energy of a gas does not change when it expands into a vacuum but does change when it pushes against a moving wall . finally , it is worth commenting on the difference in approach between this answer and the other ones . in the other answers , the wall was assumed to move slowly not because there was an opposing force but because the wall was infinitely heavy . if there is no opposing force and the wall is not very heavy then the pressure of the gas will accelerate it until it is not travelling slowly any more . the " heavy wall " approach allows you to do the kinematic calculations to see how the energy is lost from individual particles , whereas the " opposing force " picture allows you to see what is happening to the energy on a more macroscopic level . but the same amount of energy will be lost from the gas in either case ( the gas can not " know " whether there is a heavy wall or an opposing force ) , and understanding both cases helps you to get a more complete picture of what is going on .
actually the pressure at those two points does not only depend on the height from the surface but also the distance of those points from the right wall . this is because there is an acceleration towards the right , and if you move from right to left in the same horizontal level in the container , you would not get a pressure difference corresponding to gravity , but you would get a pressure difference corresponding to the rightward acceleration . a better way to look at this model is to view from the frame of the container and apply a pseudo-force on the liquid corresponding to the acceleration . it can be said that a net force ( vector sum on gravity and pseudo-force ) will act on the liquid , and the surface of the liquid will align itself perpendicular to the direction of the force . now the condition of equal pressure will be found when the perpendicular distance of those two points is same from the surface ( can be regarded as the apparent depth of the point ) . applying that condition you should get your answer . p.s. i have not calculated the answer but i do not think $a=2g$ is right . . .
what you are asking is to take the inverse of the function $v ( x ) $ . using the $z= ( r/x ) ^6$ as suggested , you will get the quadratic equation : $$z^2-2z-v/e=0$$ and the solution are $$ z_{\pm} = 1 \pm \sqrt{1+v/e}$$ and so $$x_{\pm} ( v ) = \frac{r}{ ( 1\pm\sqrt{1+v/e} ) ^{1/6}} \tag{1}$$ note that there are at most two real solutions ( not twelve ) as shown in the figure . the equation ( 1 ) needs the condition $z_{\pm}&gt ; 0$ hold which means $ x_{+} $ exists for $v&gt ; -e$ and $x_{-}$ exists for $-e&gt ; v&gt ; 0$
yes , op is right . the heisenberg evolution equation in the interaction picture applies to an operator $a_i ( t ) $ , that depends on a single moment of time $t$ . on the other hand , the evolution operator $\hat{u} ( t_f , t_i ) $ depends in principle on the whole intermediate time interval $ [ t_i , t_f ] $ , and is in particular not a local operator of a single time .
mechanical efficiency is usually used as a metric to account for frictional losses in systems . for example , the transmission of a car transmits mechanical work from the engine to the wheels so the mechanical efficiency of the system will be $w_{transmitted}/w_{received/ideal}$ . even within an engine , there is friction between the piston and cylinder walls , bearings of crankshafts etc . mechanical efficiency is : $w_{output}/w_{obtained\ ; from\ ; gas}$ . the denominator is the work obtained from the work-fluid , the thermodynamic cycle . therefor it is a way to quantify the frictional loses in the system . what i have mentioned above is one of the common use of this term , but there could be other definitions too . you need to look into the context of usage
your argument is correct . indeed ( in $d$ dimensions ) when $c = 0$ , unitarity implies that $t_{\mu \nu} \equiv 0$ , but by assumption any local cft must have a stress tensor $t_{\mu \nu}$ satisfying $$ t_{\mu \nu} ( x ) \times \mathcal{o} ( y ) \sim \delta \mathcal{o} ( y ) $$ ( a ward identity ) where $\delta$ is the dimension $\mathcal{o} . $ so unless $\delta = 0$ ( the unit operator ) , you fail to satisfy this condition .
add any instant in time , light of different wavelengths can be said to interfere . however , because of the extreme frequencies of optical light , any cross interference will get time-averaged away very quickly unless the two waves are very close in frequency .
yes , or at least probably . glueballs , if they exist , are bound states of two massless gluons . however glueballs ( are calculated to ) have a non-zero mass because their binding energy contributes to their mass , so they would travel on timelike trajectories . i think the glueball is a special case because the strong force is confining . i am not sure what would happen for a hypothetical massless system bound by the electromagnetic force .
start with your $\hat{h} = \hbar \omega \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) $ . i will omit hat notation from this point . the commutator then reads as \begin{equation} \left [ h , a \right ] = \hbar \omega \left [ \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) a - a \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) \right ] = \hbar \omega \left ( a^\dagger a a - a a^\dagger a \right ) , \end{equation} which is nothing but \begin{equation} \left [ h , a \right ] = \hbar \omega ( a^\dagger a - a a^\dagger ) a = \hbar \omega \left [ a^\dagger , a \right ] a , \end{equation} but we know that \begin{equation} \left [ a^\dagger , a \right ] = -1 , \end{equation} therefore \begin{equation} \left [ h , a \right ] = -\hbar \omega a , \end{equation} qed . proof of the second relation is done in the same way .
the origin of your problem was already explained in the previous answers , let me just do so in a bit more detail . it is better to think of some normalizable wave function rather than the $\delta$-function itself . as you probably know , you can get arbitrarily close to a $\delta$-function by making a wave packet narrow and taking a suitable limit ( see below for a concrete example ) . now you are right that you can localize your particle to an arbitrarily narrow region around $x=0$ . you can even make it " static " in the sense that the average ( quantum expectation value ) of its momentum will be , and stay , zero . however , the uncertainty principle tells you that once the spread ( dispersion ) of coordinate $\delta x$ is very small , the spread of momentum $\delta p$ will be very large . therefore , even if you initially localize the particle to a very narrow wave packet , it will broaden quickly with time . in fact , this broadening will be the faster , the sharper was the initial localization of the particle . if the wave function is initially gaussian , then for a free particle you can solve the schroedinger equation exactly and see that the wave function remains gaussian , just its width grows ( asymptotically as $\sqrt t$ ) . so after some time , the particle is no longer localized , as a consequence of the very same uncertainty principle . even if the particle is not free but moves in some potential , making the initial wave packet at $t=0$ very narrow will result in the same spreading since the average kinetic energy ( which is proportional to $\delta p^2$ if the average momentum is zero ) is much higher than the variation of the potential energy within the size of the wave packet . there is one caveat in my above argument though . if you , simultaneously with narrowing down the wave packet , confine the particle by an increasingly strong potential , then you can keep it localized . consider as a model the harmonic oscillator , defined by the hamiltonian $$ h=\frac{p^2}{2m}+\frac12m\omega^2x^2 . $$ the coherent states are gaussian wave packets which thanks to the special form of the potential remain localized : both $\delta x$ and $\delta p$ are time-independent and given explicitly by $$ \delta x=\sqrt{\frac{\hbar}{2m\omega}} , \quad \delta p=\sqrt{\frac{m\hbar\omega}2} . $$ if you now take the limit $\omega\to\infty$ , the gaussian packet goes asymptotically to the $\delta$-function . this corresponds to your particle localized infinitely close to $x=0$ . the prize for this is that you have to make the potential infinitely strong , which simultaneously makes the particle oscillate with infinite frequency around the origin . so you cannot quite say that its momentum is zero .
to name an simple example , a 1d simple gravity pendulum with lagrangian $$l ( \theta , \dot{\theta} ) = \frac{m}{2}\ell^2 \dot{\theta}^2 + mg\ell\cos ( \theta ) $$ has one degree of freedom ( d . o.f. ) , $\theta$ , although its solution $\theta=\theta ( t ) $ has two integration constants . here , $\theta$ is the angle of the pendulum ; $\dot{\theta}$ is the ( angular ) velocity ; and $p_{\theta}:=\frac{\partial l}{\partial\dot{\theta}}=m\ell^2\dot{\theta}$ is the ( angular ) momentum . furthermore , the configuration space with coordinates $ ( \theta , \dot{\theta} ) $ and the phase space with coordinates $ ( \theta , p_{\theta} ) $ are both two dimensional spaces . in other words , it takes two coordinates to fully describe the instantaneous state of the pendulum at a given instant $t$ . so , to answer the main question : no , the corresponding velocity ( or momentum in the hamiltonian formulation ) is not counted as a separate d . o.f. . references : landau and lifshitz , mechanics : see e.g. first page of chapter 1 or first page of chapter 2 ; h . goldstein , classical mechanics : see e.g. page 13 or first page of chapter 8 in both 2nd and 3rd edition ; j.v. jose and e.j. saletan , classical dynamics : a contemporary approach : see p . 18 ; or wikipedia , either here or here .
the drag of a fluid acting on an object inside is the flow of momentum through the boundary of the object . the momentum conservation law is the entire content of the navier stokes equation , which can be written in integral form : $$ {\partial\over \partial t} \int_r \rho v^i = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p \hat{n} + \nu ( \rho ) \nabla v^i ) \cdot \hat{n} $$ where $\hat{n}$ is the normal to the boundary of $r$ , $p$ is the pressure , $\nu$ is the viscosity ( as a function of the density $\rho$ ) , and v is the velocity . the left hand side says that you are looking at the flow of total i-component of momentum out of region r . the first term on the right is the physical amount of momentum flowing out of the boundary of r by the flow of the fluid . the last term is the flow of momentum through the boundary of r due to forces at the edge . using the divergence theorem , you learn that $$ \int_r {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p - \nabla\cdot ( \nu \nabla v^i ) d^dx = 0$$ and you conclude that the ns equations are satisfied . $$ {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p -\nabla \cdot ( \nu \nabla v^i ) $$ if you expand this out , and use the continuity equation , you will recover the more standard forms , but this is the form in which it is most transparently a continuity equation for the momentum flow . so you see that the flow of the i-component of momentum into any region r due to the fluid , which is the i-th component of the force exerted by the fluid on whatever is inside r , is given by the boundary integral $$ f^i_r = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p\hat{n} + \nu \nabla v^i ) \cdot \hat{n}$$ for the case where you have a solid object that the fluid cannot penetrate , the velocity is perpendicular to the object 's surface , and the first term is zero ( obviously-- the first term describes the momentum carried along with the fluid , and this is not entering r ) . so the drag is the integral of two terms across the surface , the pressure across the object , which tells you how much the object is pushing to get the water to go around , and the gradient of the velocity , which describes how the viscosity pulls the object . for a moving object , this works at one instant to tell you how much momentum is entering or leaving the object , which is the instantaneous drag force .
short answer : give up now . conservation of baryon number , lepton number , strangeness , charm , bottomness , and most importantly charge are making your job hard . long answer : i am ignoring charge conservation issues from time to time ; as its no fun if you take that into account well , not baryons or leptons . and we have conservation of charge in the way as well . baryons we have conservation of baryon number $b=\frac{1}{3} ( \text{number of quarks-number of antiquarks} ) $ , which has opposite values for particle-antiparticle pairs . the only way to convert a baryon to its antibaryon would be to bombard it with a different antibaryon with ( negative ) twice the baryon number ( and twice the negative charge as well ) . and that would require energy ( to create the antibaryon ) . leptons for leptons , we have conservation of lepton numbers $l_e , l_{\mu} , l_{\tau}$ . i doubt any reaction exists where a lepton becomes an antilepton , as leptons can only have lepton number $\pm1$ , and particle reactions involve two reactants ( iirc ) . i am not considering neutrino oscillations here ; they make it possible to do stuff like this . the only naturally abundant neutrino is the electron one ( maybe not naturally abundant , but easy to generate passively ) ; unfortunately , it is got a very low energy . and the entire neutrino oscillation thing is still debated . anyways , your neutrino source would be a beta-decaying substance ; and there are different , easier ways to get energy from that . mesons it is easier for mesons , though there still are restrictions ( conservation of charmness , topness , bottomness ) . due to these restrictions , the only conversions possible would be for these mesons ( and their antimesons ) : $u\bar{d} ( \pi^\pm ) , d\bar{t} , u\bar{t}$ . the last one is anyways in a superposition with its antiparticle , so we get a total of two pairs of particle-antiparticle conversion reactions . guage bosons ( i am not sure if the gauge bosons can do such reactions ) analysis of feasability of mesons anyways , the antimatter=limitless energy is something rather overhyped . over here , we have two possible candidates . $d\bar{t}$ is not an everyday particle ( dunno if it is even been synthesized ; top quarks are pretty hard to create , and wikipedia has no data in its list of mesons ) , and anyways is pretty unstable . you had have to pump in a bunch of energy to create it , and that defeats the purpose . feasibility of pi-meson $u\bar{d}/\bar{u}d ( \pi^\pm ) $ is interesting , as it is a common particle in nuclei . but it is bound inside ( not exactly--its part of the virtual particle " sea " , but that makes it worse ) , and decays pretty fast . so you had have to break apart the atom ( yes , you had get energy from that , but not if you separate it into nucleons ) , " catch " a pi meson , convert it to an antiparticle by means of the " quantum switch " , and collide it with another pi meson ( alternatively , without the " quantum switch " , you can just find an opposite pi-meson ) . and that will give you a tiny amount of energy as compared to your efforts . you would also need to supply some oppositely-charged particle to conserve charge . making it more complicated . conclusion so nope , it is not a good energy source . it does not work for protons/neutrons/electrons ; it only will work for two particles ( one more if we consider neutrino oscillations ) . neither of them is feasible . stick to fission-fusion .
i believe you are exactly right : it is the complexity of hills , buildings , trees , asphalt , water , etc that make surface winds complicated . as you go higher in the atmosphere , these surface effects disappear and the winds become much more steady . you can see this in the winds aloft forecasts issued by the faa for use in aviation : http://aviationweather.gov/adds/winds/ in the upper left there is a drop-down box allowing you to select altitudes from the surface ( sfc ) to 48,000 feet ( fl480 ) . as you go up in altitude , the relatively chaotic surface winds blend into a much smoother ( and faster ! ) flow . i assume the effect is similar at sea , given the simpler boundary conditions . ( i admit this does not directly address your question of , " how variable are the winds at sea " ? i too am curious to hear an authoritive answer . )
what you want are fermi geodesic coordinates . from some initial point $p$ on a timelike geodesic with four-velocity $u$ , take the proper time $\tau$ as the time coordinate and select three orthonormal vectors $\{{\mathbf{e}}_\hat{\alpha}\}$ that serve as a basis for the orthogonal complement of the geodesic 's tangent vector at $p$ . parallel-transport the spatial vectors along the geodesic by solving the equation $$\nabla_u \mathbf{e}_{\hat\alpha} = 0\text{ , }$$ where the metric-compatibility of the levi-civita connection ensures that the four basis vectors stay orthonormal along the points of the geodesic . at every $\tau$ , take the $3$-manifold of spacelike geodesics going orthogonally to the given timelike geodesic . on this manifold , we can construct the usual riemann normal coordinates using our orthonormal spatial vectors : basically , pick a unit vector $\alpha{\mathbf{e}}_\hat{1} + \beta{\mathbf{e}}_\hat{2} +\gamma{\mathbf{e}}_\hat{3}$ , send out a spatial geodesic there , and then label each point on it with coordinates $ ( \tau , s\alpha , s\beta , s\gamma ) $ , where $s$ is distance along the spatial geodesic . in the geodesic case , not only does the metric minkowski form on the points of the geodesic , but the christoffel symbols also vanish there . in general , we can also use fermi-walker transport to consider an accelerated observer that is also rotating . this is described , e.g. , in mtw §13.6 . this makes the the christoffel symbols have $\gamma^{\hat{0}}_{\hat{j}\hat{0}} = \gamma^{\hat{j}}_{\hat{0}\hat{0}} = a^\hat{j}$ , the components of the acceleration four-vector , as well a term corresponding to the rotation of the observer in $\gamma^{\hat{j}}_{\hat{k}\hat{0}}$ . things you might also be interested in : tetrads and gullstrand-painlevé coordinates for the schwarzschild spacetime that correspond to the frame field of lemaître observers freely-falling from rest at infinity .
one way to see the validity of the background field method ( bfm ) lies in the proof of the equivalence of the effective action calculated with the bfm to the standard effective action . let $\gamma [ v ] $ be the effective action ( legendre transform of the connected generating function $w [ j ] $ ) where $v=v ( j ) =\frac{\delta w [ j ] }{\delta j}$ is the " classical " field generated by the sources $j$ . if we modify the classical action by splitting the quantum fields into quantum + background , then the resultant modified effective action $\gamma [ v ( j ) , v ] $ now depends on both $v ( j ) $ and the background fields $v$ . you can show ( under reasonable assumptions ) that $\gamma [ 0 , v ] = \gamma [ v ] $ . one of the cleaner and more general discussions of this is in section 3.1 of the 2007 thesis by grasso : " higher order contributions to the effective action of n = 2 and 4 supersymmetric yang-mills theories from heat kernel techniques in superspace " . here he considers the more general ( non-linear ) quantum-background splitting that is needed for $n=1$ supersymmetric gauge theories . references to the original literature can also be found in this thesis . also worth reading are the introductory papers by abbott : " introduction to the background field method " and " the background field method beyond one loop " . abbott et al also show that the s-matrix is the same when using the bfm : " the background field method and the s matrix " . this holds irrespective of the gauge fixing , while the equivalence of the effective actions only holds if the same gauge is chosen . note that the quantum-background splitting is not really a doubling of the number of fields , but only just a background dependent change of variables ( in the simple case , just a shift ) in the path integral .
you are in free field theory , right ? in this case there is a discrete $\mathbb{z}_2$ global symmetry which takes $\phi\to -\phi$ . applying this to your path integral gives that the three point function is equal to its opposite , so it must be zero . this also holds for interacting theories as long as each term in the interaction potential contains an even amount of $\phi$ 's , e.g. $v ( \phi ) =\phi^4$ . p.s. you forgot the path integral weight $\exp ( is ) $ in your formula .
after switching to the frame mentioned above , we are left with only a static electric field , perpendicular to the initial velocity of the particle . now we consider $$\frac{d u^\alpha}{d\tau}=\frac{e}{mc}f^{\alpha\beta}u_\beta$$ this decomposes as $$\frac{du^0}{d\tau}=\frac{e}{mc}f^{0\beta}u_\beta=\frac{e}{mc}f^{0i}u_i=\frac{e\gamma}{mc}\vec{e}\cdot\vec{v}$$ and $$\frac{du^i}{d\tau}=\frac{e}{mc}f^{i\beta}u_\beta=\frac{e}{mc} ( f^{i0}u_0 -f^{ij}u_j ) =\frac{e}{mc} ( \gamma c ) \vec{e}$$ since $\vec{b}$ is zero in this frame . now i write all the velocities as parallel and perpendicular to the electric field and define $\omega_e=\frac{ee}{mc}$ $$\frac{d ( \gamma c ) }{d\tau}=\omega_e ( \gamma v_{||} ) $$ $$\frac{d ( \gamma v_{||} ) }{d\tau}=\omega_e ( \gamma c ) $$ $$\frac{d ( \gamma v_{\perp} ) }{d\tau}=0$$ then differentiating the second equation by $d/d\tau$ we get $$\frac{d^2 ( \gamma v_{||} ) }{d\tau^2}=\omega_e \frac{d ( \gamma c ) }{d\tau}=\omega_{e}^2 ( \gamma v_{||} ) $$ solutions to this are $ ( \gamma v_{||} ) =a\sinh ( \omega_e \tau ) +b\cosh ( \omega_e \tau ) $ which implies $ ( \gamma c ) =a\cosh ( \omega_e \tau ) +b\sinh ( \omega_e \tau ) $ and $\gamma v_{\perp}=\text{const . }$ . now at $\tau=0$ we know that $v_{\perp}=v_0$ and $v_{||}=0$ . let $$\gamma_0=\frac{1}{\sqrt{1-\frac{v_{0}^{2}}{c^2}}}$$ then the initial conditions demand that $b=0$ , $a=\gamma_0 c$ , and $\text{const . }=\gamma_0 v_0$ . then we have $$ ( \gamma c ) =\gamma_0 c \cosh ( \omega_e \tau ) \implies \gamma=\gamma_0 \cosh ( \omega_e \tau ) $$ $$ ( \gamma v_{||} ) =\gamma_0 \cosh ( \omega_e \tau ) v_{||}= ( \gamma_0 c ) \sinh ( \omega_e \tau ) \implies v_{||}=c\tanh ( \omega_e \tau ) $$ $$\gamma v_{\perp}=\gamma_0 \cosh ( \omega_e \tau ) v_{\perp}=\gamma_0 v_0\implies v_{\perp}=\frac{v_0}{\cosh ( \omega_e \tau ) }$$ now $dt/d\tau=\gamma$ so that $$t=\int_{0}^{\tau}\gamma d\tau&#39 ; =\int_{0}^{\tau}\gamma_0 \cosh ( \omega_e \tau&#39 ; ) d\tau&#39 ; $$ then $$\frac{t \omega_e}{\gamma_0}=\sinh ( \omega_e \tau ) $$ the important one though is $$\cosh ( x ) =\sqrt{1+\sinh^2 ( x ) }\implies \cosh ( \omega_e \tau ) =\sqrt{1+\frac{t^2 \omega_{e}^{2}}{\gamma_{0}^{2}}}$$ after plugging these into the above equations for $v_{\perp}$ and $v_{||}$ we can solve for $x_{||}$ and $x_{\perp}$ from $$dx=v\ , dt\implies x_{||}=\int_{0}^{t}v_{||} ( t&#39 ; ) dt&#39 ; \quad \text{and}\quad x_{\perp}=\int_{0}^{t}v_{\perp} ( t&#39 ; ) dt&#39 ; $$ these are what i was looking for .
temperature is the measurement of kinetic energy per unit particle mass . since you have added the same amount of heat energy to each object , the finite object will have a higher temperature because its heat energy is distributed across a smaller collection of mass . taking something 's temperature is indeed a meaningful measurement ; )
the fringe pattern of a double slit is entirely classical - it even works with ocean waves . why it still works with only one photon , and why it does not work if you look at the photon is quantum .
a passive machine on a surface ( of gravitationaly attracting sphere = ideal earth ) cannot be in a stable or meta stable state with only two supports . a di-pod cannot span an area , in a way that it is center of gravity stays within if tilted . unlike the doll below , which has always a projected area around it is point of support under it is center of gravity - a two-legged system spans only a line , which is easy to cross . self-balancing system should revert to it is state after a small ( and to be defined , in the example below it would be $x_a - x_b$ ) dicsplacement . in terms of a potential this is the case for stable and meta-stable states . in the picture the state of your system is related to potential ( y-axis ) , and some abstract position is at the x-axis : e.g. your system is staying " upright " at $x_a$ , it will go back to $x_a$ if pushed with less effort than $\delta u$ , and will be " falling down " untill it reaches next stable position at $x_c$ " lying down " addendum : a bag hook could be considered stable virtually having only one support , but it is not an option , for a ) moving machine b ) on a surface upon further reflection - the hook is actually hanging on a table , which has at least three supports itself .
first , a small correction : it should be $m_\text{rel} = \gamma m$ , because $\gamma = ( 1-\frac{v^2}{c^2} ) ^{-\frac12}$ . your formula is close . the most general version , which is true all the time ( even for zero mass ) is $$e^2 = ( mc^2 ) ^2 + ( pc ) ^2 . $$ here $m$ is the rest mass and $p$ is the magnitude of the relativistic momentum , defined as $\mathbf{p} = \gamma m \mathbf{v}$ . note that the factor of $\gamma$ is already included in the momentum , and we use the rest mass , not the relativistic mass . you can get another formula for this one : if you replace the definition of $\mathbf{p}$ , you will find $$e = \gamma m c^2 = \frac{m c^2}{\sqrt{1-\frac{v^2}{c^2}}} = m_\text{rel}c^2 . $$ this formula is also valid at all times . in older texts it usually to refer to the relativistic mass $m_\text{rel}$ as simply the mass $m$ , and this is probably where the formula $e = mc^2$ comes from .
the first bullet is correct , the outer shell does not contribute . this easily follows from gauss ' law . for this you use the fact that the electric field must be radial and any cylinder inside the cylindrical shell does not enclose the charge density $-\lambda$ . you might think that close to the negatively charged shell there is an additional electric field pointing in the same direction ( towards the shell ) , but this contribution is cancelled by the electric field created by the rest of the shell . the second bullet does not assign $r_b$ as $r_0$ and $r_a$ as $r$ in equation $ ( 1 ) $ . rather , it assigns $r_a$ as $r$ to calculate $v_a$ and $r_b$ as $r$ to calculate $v_b$ , which yields $v_a=\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_0}{r_a}$ and $v_b=\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_0}{r_b}$ . then $v_{ab}\equiv v_a-v_b=\frac{\lambda}{2\pi\epsilon_0}\left ( \ln\frac{r_0}{r_a}-\ln\frac{r_0}{r_b}\right ) =\frac{\lambda}{2\pi\epsilon_0}\left ( \ln ( \frac{r_0}{r_a}/\frac{r_0}{r_b} ) \right ) =\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_b}{r_a}$ .
when the electrostatic force was originally being studied , force , mass , distance and time were all fairly well understood , but the electrostatic force and electric charge were new and exotic . in the cgs system , the charge was defined in relation to the resulting electrostatic force ( it is called a franklin ( fr ) an " electrostatic unit " ( esu or ) sometimes a statcoulomb ( statc ) ) . in that system , we express the force on one charged particle by another as $f_e=\frac{q_1 q_2}{r^2}$ where the unit of charge is the esu , the unit of force is the dyne and the unit of distance is the centimeter . in the mks system ( now called si ) , we would write $f_e = k_e\frac{q_1 q_2}{r^2}$ where the unit of charge is the coulomb , the unit of force is the newton , and the unit of distance the meter . it would seem that if things are equivalent , then $k_e$ is indeed just a conversion factor , but things are definitely not equivalent . a little history is probably useful at this point . before 1873 , when the cgs system was first standardized , it finally made a clear distinction between mass and force . before that , it was common to express both in terms of the same unit , such as the pound . so if you think of it , people still say things like " i weigh 72 kg " rather than " i weigh 705 n here on the surface of earth " and they also say $1 \mathrm { kg} = 2.2\mathrm{ lb}$ confusing mass and weight ( the cgs imperial unit of mass is actually the slug ) . this is important , because there is a direct analogy to the issue of units of charge and to your question about the units of $k_e$ . the franklin is defined as " that charge which exerts on an equal charge at a distance of one centimeter in vacuo a force of one dyne . " the value of $k_e$ is assumed to be 1 and is dimensionless in the cgs system . in cgs , the unit of charge , therefore , already implictly has this value of $k_e$ built in . however in the si units , they started with amperes and derived coulombs from that and time ( $c=it$ ) . the resulting units of $k_e$ are a result of that choice . so although the physical phenomenon is the same , it is the choice of units that either gives $k_e$ dimension or not . see this paper for perhaps a little more detail on how this works in practice .
if you need to set up a field of magnitude $e$ in a dielectric medium of relative permittivity $\epsilon_r$ then you need to originally supply an electric field of $\epsilon_r e$ , now since a larger field would be provided with more charge , and bringing new charge to an existing charge configuration would require energy , definitely more energy is required to set up an electric field in presense of a dielectric medium than it would be in perfect vacuum .
a solid material is not made of electrons separated from the nuclei . the nuclei and electrons live together in a structure that can be ordered ( like crystals ) or not ( like glass ) . in all cases , the electron arrangement around the nuclei is very important as it allows for bonding between these atoms . in that process , some electrons are stuck close to the nuclei , they are called ' bound ' electrons . others are not totally stuck and can somewhat move from atom to atom , they are called ' free ' electrons . but the atoms are always stuck in a given position . that is why the rod stays as a rod and does not evaporate as a gas . imagine that the fixed atoms form a given stable structure . the free electrons can move inside this structure . these are the ones that are attracted to one side or the other . the nuclei do not move . that is why the rod just looks the same all the time : only a tiny number of electrons actually travel along .
these are all good questions . perhaps i can answer a few of them at once . the equation describing the violation of current conservation is $$\partial^\mu j_\mu=f ( g ) \epsilon^{\mu\nu\rho\sigma}f_{\mu\nu}f_{\rho\sigma}$$ where f ( g ) is some function of the coupling constant . it is not possible to write any other candidate answer by dimensional analysis and by parity ( assuming the current is the ordinary axial current . . . ) now we integrate both sides over $\int d^4x$ , and we find on the left hand side $\delta q$ , meaning , now that the current is violated , the charge can change while the system evolves , while the right hand side is $$f ( g ) \int d^4x \epsilon^{\mu\nu\rho\sigma}f_{\mu\nu}f_{\rho\sigma}$$ the object on the right hand side is a known topological invariant of the gauge bundle , and it is an integer ( if all the charges are appropriately quantized ) . so on the left hand side we get $\delta q$ , which must be an integer ( if all fundamental particles carry integer charge ) and the right hand side is an integer too , up to the function $f ( g ) $ . this means that the function $f ( g ) $ cannot , in fact , depend on $g$ . ( more precisely , there is a scheme where it does not . ) hence , it is exact at one loop . this is the modern proof ( without any computation ) of the abj theorem about one-loop exactness of the anomaly . so you see the deep connection between one loop and instantons . . . the violation of the conservation equation is at one loop , but to lead to interesting consequences we need to have a nontrivial gauge bundle . about some of the other comments you made : any regularization scheme that respects bose symmetry will lead to the anomaly , it is totally unavoidable . this is proven in http://inspirehep.net/record/154341?ln=en. another comment : anomalies can also arise from boson loops , for example , the trace anomaly . ( it is not one-loop exact in any sense i am aware of . )
here are some wordy , math-free answers . a phonon is the minimal amount of energy which can be stored in an lattice vibration in a given mode sounds good . i.e. that when a crystal vibration interacts with matter it does so by the creation/destruction of whole phonons at a time , which may also get absorbed at more or less precise locations , e.g. the energy of a single phonon is absorbed by a localized electron . a phonon is a periodic motion of the atoms in a solid , so i would argue that it is always interacting with matter since it is matter in motion . localization of phonons is a tricky business . the textbook derivations for phonons result in vibrations ( waves ) that extend through the whole material . however , they are usually treated as localized . you can make any function by adding up waves of different wavelengths ( the waves form a basis ) , so you can build up localized phonon " packets " from lattice vibrations of different frequencies . unlike photons , phonons have non-linear dispersion relations -- meaning that waves of different frequencies travel at different speeds ( unlike light where all frequencies travel at the same speed , at least in a vacuum ) , so the packets will eventually fall apart if left alone . however , they can stick together long enough that they can be thought of as particles . if the frequencies in the packet are of a narrow range , you can think of the packet as having a frequency equal to the average frequency of its constituent waves . this localization makes sense if electrons are likewise localized . if an electrons scatters with a phonon , and that electron is localized , that means the electron is only really interacting with nearby atoms . so , any lattice vibration the electron creates should be initially localized to that region too . i should add that a major form of phonon scattering is with other phonons . it turns out that you can not have two-phonon processes ( two phonons colliding and create two other phonons ) ; you can only have three-phonon processes and higher ( e . g . two phonons merge to create a third ) . you do not have to think of these processes as being localized in space . finally i would like to understand how phonon exchange can effectively establish an attractive force between electrons the atoms in a lattice are charged , so they can pull on nearby electrons . if several atoms are pulling on one electron , then the atoms are effectively pulling on each other and are brought closer together . if the electron is moving , it can leave a wake of atoms that are closer together ( a phonon ) . atoms being closer together means more positive change in an area , and that in turn can draw in another electron -- effectively attracting the electrons together . see http://hyperphysics.phy-astr.gsu.edu/hbase/solids/coop.html
let me focus on the context of rigorous equilibrium statistical physics ( see , e.g. , georgii 's book " gibbs measures and phase transitions" ) . there , one works with probability measures on infinite systems , often on a lattice ; let me assume it is $\mathbb{z}^d$ . in this context , a macroscopic observable is defined as a function $o:\omega\to\mathbb{r}$ ( where $\omega$ is the set of all configurations $ ( \omega_i ) _{i\in\mathbb{z}^d}$ ) which does not depend on the values of any finite sets of spins $\omega_i$ ( technically , one says that such an observable is measurable with respect to the tail $\sigma$-field ) . let me give you some examples of such observables , in the simple case of ising-type systems , i.e. , with $\omega=\{-1,1\}^{\mathbb{z}^d}$ . let $\sigma_i$ denote the spin at $i$ , $\sigma_i ( \omega ) =\omega_i$ . $\circ$ averages of local observables , e.g. , $$ \lim_{\lambda\uparrow\mathbb{z}^d} \frac1{|\lambda|}\sum_{i\in\lambda} \sigma_i\ ; . $$ $\circ$ events such as " there are no infinite connected components of $-1$-spins " . in both cases , changing a finite number of spins does not modify the value of the observable $o$ . one nice thing about this definition is that one can prove very generally that such observables take deterministic values ( i.e. . , are almost surely constant ) with respect to any pure phase ( extremal gibbs measure ) . in other words , they do not fluctuate ( remember one deals here with infinite systems ) .
when you say : " however , i am pretty sure that this is a dirac mass , and not a majorana mass . " that is where you are confused . ( why did you think you were sure of this ? ) a majorana mass has that form , and so does a dirac mass . they have the exact same feynman rule arrow structure when you use 2-component notation . it is just that for a majorana mass , the 2-component fields being connected are the same , and for a dirac mass they are different ( typically with opposite charge under some gauge or global symmetry ) . the answers about the majorana-weyl condition are not relevant . in 4 dimensions , a majorana fermion is simply a 2-component weyl fermion with a mass term by itself . a dirac fermion is a pair of 2-component weyl fermions with a mass term connecting them .
the properties of spin in higher dimensions mean that you can measure d/2 spin directions simultaneously , corresponding to a set of maximally commuting rotation planes . for example , in 4d , you can measure the spin in the x-y rotation plane and the z-w rotation plane , simultaneously , but not the spin in the x-y and x-w planes . the angular momentum of objects in string theory obeys the same commutation relations as in any other quantum theory , so in string theory , the spins are restricted as above--- you can measure as many of these simultaneously as there are independent rotation planes .
no , the heavier object does not fall faster . instead , they heavy and light object fall at the same acceleration ( and hence the same speed if they are both simply dropped ) . this is an example of the equivalence principle . the more massive object has more gravitational force on it , but it also has more inertia . specifically , because the object is twice as massive , it has twice the inertial mass . the force on it is doubled , so the acceleration stays the same . if we look at $$f = ma$$ we see that when $f$ and $m$ are both multiplied by 2 , $a$ stays the same . check these questions for more : free falling of object with no air resistance why is heavier object more reluctant to get falling down ? projectile motion without air resistance
this might be more detailed than you want ; i apologize in advance . there are two forms of friction : static friction the force of friction exerted on an object when it is at rest . kinetic friction the force of friction exerted on an object when it is in motion . these two forms of friction have qualitatively properties . specifically , the force of kinetic friction depends only on the magnitude of the normal force $f_n$ exerted on the moving object and the coefficient of kinetic friction $\mu_k$ of the surface on which it is moving . in fact , as you point at the magnitude of the force of kinetic friction as given by $$ f_k = \mu_k f_n $$ the force of static friction , on the other hand , changes depending on the other external forces on the object . to understand why , think of a box sitting still on a horizontal table . the box will not feel a friction force in the absence of any other force ( if it did , then it would accelerate ) . however , if you start exerting a small enough force on the box , it still will not move , and in this case , the static friction force is exactly counterbalancing the force you exert . if you push hard enough , however , the box will eventually start sliding . this illustrates that the static friction force can have any value between zero , and some maximum which turns out to be given by $\mu_s f_n$ where $\mu_s$ is the coefficient of static friction . mathematically , this can be expressed by the following equation : $$ f_s \leq \mu_s f_n $$ where $f_s$ is the magnitude of the static friction force . having said all of this , let me reiterate that kinetic friction always has the magnitude $\mu_k f_n$ regardless of the state of motion of the object . if you continuously push an object with a force greater than this value , then it will keep accelerating forever . in order for the acceleration to halt , you would need to reduce the applied force so that it equals the force of static friction . lastly , given an applied force $f$ , the acceleration of the object will satisfy newton 's second law which says that the net applied force equals the mass of the object multiplied by is acceleration ; $$ f - f_k = ma $$ the acceleration of the object is the rate of change of its velocity , so determine the velocity as a function of time you would , in general , have to solve the following differential equation : $$ \frac{dv}{dt} = \frac{1}{m} ( f - f_k ) $$
no . electron are not considered waves . electrons are considered particles and studied in a branch of physics named particle physics . everything is found to be made of particles . waves ( e . g . electromagnetic waves ) are just collections of particles . the myth that electrons are waves or behave as waves or sometimes are waves and sometimes are not , depending of the observer , is one of the more persistent myths that surround quantum mechanics . electrons can exist in the same state . e.g. one electron in an hydrogen atom can be in the same state than another electron in another atom . what happens here is that two electrons cannot be in the same state in the same atom at the same instant , because there is only one such state available in a single atom .
this choice is closest to the the correct one . i am tempted to shrug of the entire particle exchange as a mere numerical convenience ; a discretization of the maxwell equations perhaps . i am reluctant to say " virtual particle " because i suspect that term means something different to what i think it means . and virtual exchange is a correct description , because during the interaction the exchanged particle is not on mass shell . keep in mind that in the microcosm of particles nature is quantum mechanical . the particle scattering on another particle and the momentum and energy and quantum number exchanges between them are all described by one wave function , one mathematical formula that gives the probability for the interaction to take place in the way it has been ( will be ) observed . . thus it is not a matter for " knowing " but a matter of " being " . the feynman diagrams that give rise to the " particle exchange " framework are just a mathematical algorithm for the calculations and help in understanding how to proceed with them . to see how classical fields are built up by the substructure of quantum mechanics see the essay here .
the two phenomena feynman referred to are the $q\vec v\times \vec b$ part of the force acting on an electric charge carrier ; and the $\nabla\times \vec e =-\partial_t \vec b$ dynamical maxwell 's equation . in the most general situation when both the magnetic field and the shape of the wire is changing , we have to use and add both terms . that is true in each reference frame because for a complicated space-dependent , time-dependent geometry of the wires and fields , there will not be any inertial frame in which one of the phenomena would completely vanish . i believe that it is more likely ( but not certain ) feynman only meant this thing – that there is no way to eliminate or " explain " one of the terms in the general situation . on the other hand , the fact that both terms have the same origin is a consequence of special relativity and it was indeed one of the motivations that led einstein to his new picture of spacetime . it seems somewhat plausible to me that feynman was ignorant about this history . the full theory of electrodynamics is nicely lorentz-covariant and it implies both terms . however , these terms are not not really the same . the lorentz force comes from the integral $q a_\mu dx^\mu$ over the world lines of charged particles while maxwell 's equations arise from the $-f_{\mu\nu}f^{\mu\nu}$ maxwell lagrangian . so feynman would also be right if he said that the two terms can not be transformed to each other by any symmetry transformation . still , one can make physical arguments that do involve such transformations and imply that the two phenomena are inseparable . for example , if we assume the maxwell equation , it follows from the lorentz symmetry that $\vec e$ must transform as the remaining 3 components of the antisymmetric tensor whose purely spatial components give $\vec b$ . but then it follows that the force acting on a charged particle , $q\vec e$ , must also be extended by the remaining term $q\vec v\times \vec b$ for the theory to be lorentz-invariant . or one can run the argument backwards . still , we are dealing with transformations of two different terms in the action that just happen to have a " unified , simply describable " impact on the emf in wires with magnetic flux . the simplification and unity only occurs if we assume that the two different kinds of phenomena are in the action to start with but they deal with the same fields which respect the same lorentz symmetry ; and if we study situations that are " understood " or " simplified " on both sides in which some effects , e.g. the magnetic ones , are absent . let me say it differently : if the area enclosed by a wire goes to zero , it is an objective thing that is clearly independent of the reference frame . so one should not expect the shrinking of the area is just a matter of inertial systems ; it is a frame-independent fact . on the other hand , it is not shocking that the emf ultimately only depends on one thing , the change of the flux , which has a simple form although it may have different origin . i would conclude that feynman was more right than wrong . lorentz symmetry operates in both phenomena and it is the same one which is a constraint on the most general theory ; however , the fact that both possible sources of the changing flux influence the emf in the same way is a sort of " coincidence " , at least if we use the conventional variables to describe the electromagnetic phenomena .
neither voyager 1 nor voyager 2 was aimed at any particular target outside the solar system . their trajectories were largely determined by the requirement to do fly-bys of jupiter and saturn ( and , in the case of voyager 2 's extended mission , uranus and neptune ) . they have been sending back some interesting results about the boundary between the solar system and interstellar space , but are not expected to remain operational long enough to send back any data about any other stars . voyager 1 happens to be heading " in the general direction of the solar apex ( the direction of the sun 's motion relative to nearby stars ) " , which means it should reach the heliopause somewhat sooner than if it were going in a different direction . i do not think that was deliberate . to do that intentionally , they would have had to alter its course as it passed through the saturn system ; as far as i know , its course was optimized for observations of saturn , its rings , and its moons . and voyager 2 passed through the termination shock 10 aus closer to the sun than voyager 1 did . i do not think there is any particular reason to think that either one of them will encounter the black hole at the core of the galaxy . to reach the galactic core , they had have to have enough velocity to cancel the sun 's orbital motion around the core , about 251 kilometers per second , compared to their actual sun-relative speed of about 17 kilometers per second . jpl 's web site for the voyager interstellar mission says the following : both voyagers are headed towards the outer boundary of the solar system in search of the heliopause , the region where the sun 's influence wanes and the beginning of interstellar space can be sensed . the heliopause has never been reached by any spacecraft ; the voyagers may be the first to pass through this region , which is thought to exist somewhere from 8 to 14 billion miles from the sun . this is where the million-mile-per-hour solar winds slows to about 250,000 miles per hour—the first indication that the wind is nearing the heliopause . the voyagers should cross the heliopause 10 to 20 years after reaching the termination shock . the voyagers have enough electrical power and thruster fuel to operate at least until 2020 . by that time , voyager 1 will be 12.4 billion miles ( 19.9 billion km ) from the sun and voyager 2 will be 10.5 billion miles ( 16.9 billion km ) away . eventually , the voyagers will pass other stars . in about 40,000 years , voyager 1 will drift within 1.6 light years ( 9.3 trillion miles ) of ac+79 3888 , a star in the constellation of camelopardalis . in some 296,000 years , voyager 2 will pass 4.3 light years ( 25 trillion miles ) from sirius , the brightest star in the sky . the voyagers are destined—perhaps eternally—to wander the milky way . both voyagers are , of course , still influenced by the sun 's gravity . ignoring other forces ( which is a good enough approximation for thousands of years ) , they will continue to decelerate , asymptotically approaching a speed that depends on the sun 's gravity and their kinetic energy . according to a footnote in this article , voyager 1 has an asymptotic velocity of 3.5 au/yr , voyager 2 an asymptotic velocity of 3.4 au/yr which converts to about 16.6 and 16.1 kilometers per second . that is not much slower than their current velocities . reference ( wikipedia footnote ) : mallove , eugene f . ; gregory l . matloff ( 1989 ) . the starflight handbook : a pioneer 's guide to interstellar travel
great question lucas . the velocity of an object in orbit around a massive body can be expressed roughly as $v ( r ) \sim \sqrt{ \frac{gm}{r}}$ the closer you are to the mass ( e . g . the black-hole ) , the bigger v ( r ) becomes . it turns out , for a black-hole like the one at the galactic center , with stars about 100 au away . . . . they travel at about 500 km/s---fast ! now , the effects of general relativity are only significant when you are near the event horizon . in this case , even though the stars are relatively ' close ' ( about $10^{15}$ cm ) , they are still almost about 1000 times further away than the event-horizon ! and so the effects of general relativity ( e . g . time dilation ) are very very small ( in this case , currently unobservable at all ) .
the earliest stars did not have planets primarily due to a lack of metals . metals in this sense is an element ( with some extra properties that are not relevant in this context ) heavier than helium . the very article that you linked to references this . this leads to the following : stars without metals tend to not last very long . metals in a star act to slow down the reaction speed of the fusion . without metals , the stars quickly get to a state where they will explode . short time scales do not allow for enough time to form planets . metals seem to be the initial building block of planets . this wikipedia article discusses the current leading theories for rocky and gas planets . basically , they both start with a rock forming that is big enough , leading to a chain effect which ends up to be a planet . rocks can not form from hydrogen and helium , making planet formation difficult .
malicious counter example the desired object is a sphere of radius $r$ and mass $m$ with uniform density $\rho = \frac{m}{v} = \frac{3}{4} \frac{m}{\pi r^3}$ and moment of inertia $i = \frac{2}{5} m r^2 = \frac{8}{15} \rho \pi r^5$ . now , we design a false object , also spherically symmetric but consisting of three regions of differing density $$ \rho_f ( r ) = \left\{ \begin{array}{l l} 2\rho\ , and r \in [ 0 , r_1 ) \\ \frac{1}{2}\rho\ , and r \in [ r_1 , r_2 ) \\ 2\rho\ , and r \in [ r_2 , r ) \\ \end{array} \right . $$ we have two constraints ( total mass and total moment of inertia ) and two unknowns ( $r_1$ and $r_2$ ) , so we can find a solution which perfectly mimics our desired object .
you have the right idea , but the question asks for situations where there is a force and no work . centripetal force does do no work but there is a force , so i is true . in iii you are exactly right , but it says there is a force and no work , which falls under the question . i think you have misunderstood the question . ii is false because a force in the opposite direction does negative work on the object . negative is not 0 . the answer is c .
yes , you are correct in stating that the tension will be higher . in fact , it is simply : $$t=mg+ma$$ it is important however , to make the distinction between tension and the maximum tensile strength . tension , by definition is only as large as it needs to be ( just like the normal force ) , because it is a reaction force . if it was any larger the body would undergo acceleration .
in most cases , it does not really make sense to talk about a lowered effective mass caused by sitting in a gravitational potential well , since the equivalence principle says that locally the spacetime looks flat , and hence it looks like the gravitational field vanishes . however , in certain special cases , there is a sensible notion of energy that is different from your rest mass . this is when you have a timelike killing vector , which means there is a preferred time coordinate under whose flow the metric is invariant . the existence of time translation symmetry leads to a conserved energy . if $\xi^a$ is the killing vector representing the time flow , and $p^a$ is the 4-momentum of an object , the conserved energy is $$e = -g_{ab}\xi^a p^b$$ while the rest mass is $$m = ( g_{ab}p^a p^b ) ^{1/2} . $$ the schwarzschild spacetime is a classic example of this . the metric is $$ds^2 = -\left ( 1-\frac{2gm}{r}\right ) dt^2+\frac{dr^2}{\left ( 1-\frac{2gm}{r}\right ) }+r^2d\omega^2$$ since the metric is independent of time , it has a killing vector $\xi^\alpha = ( 1,0,0,0 ) $ . let 's first consider the 4-momentum of an observer initially at rest at some radius $r_0$ . initially at rest here means they start of with $p^a \propto \xi^a$ , and the normalization of $p^a$ tells us that it is $$p^\alpha = m\left ( \left ( 1-\frac{2gm}{r_0}\right ) ^{-1/2} , 0,0,0\right ) , $$ then the energy for this particle is $$e = -g_{0\alpha}p^\alpha=m\left ( 1-\frac{2gm}{r_0}\right ) ^{1/2}$$ as long as we are outside the event horizon $r=2gm$ ( which is the only place where the energy really makes sense ) , this shows that the killing energy $e$ is less than the rest mass . and if you are far away from $r=2gm$ , you can expand to first order in $gm/r$ to get $$e\approx m - \frac{gmm}{r_0}$$ which is the rest mass minus the newtonian gravitational potential energy . so in this sense the potential energy of the particle is negative , since it is killing energy is less than its rest mass energy . finally , for a particle falling in from at rest at infinity , we use the fact that killing energy is conserved along all points along the geodesic . at infinity , the metric is asymptotically minkowski , and being initially at rest means $p^a\propto\xi^a$ , hence \begin{align} p^\alpha and = m ( 1,0,0,0 ) \\ e and = m \end{align} since $e$ is conserved , we see that in this case it is always equal to the rest mass energy . you can sort of see this as a cancellation between kinetic energy and potential energy : to get the kinetic energy you need to specify who your observer is , so lets say it is the observers at constant radius . their 4-velocity is $$u^\alpha=\left ( \left ( 1-\frac{2gm}{r}\right ) ^{-1/2} , 0,0,0\right ) $$ and they would define the total energy ( rest mass plus kinetic , but not including potential energy ) as $$t = -g_{ab}u^a p^b = \frac{m}{1-\frac{2gm}{r}} \approx m + \frac{gmm}{r} . $$ to derive this , we used the fact that $e=m$ is conserved , which means that $p^t = \dfrac{m}{\left ( 1-\frac{2gm}{r}\right ) ^{1/2}}$ . if we continue to assign the potential energy $v = \frac{gmm}{r}$ , then we get $$e=t-v = m + k-v = m \implies k=v $$ so in some sense the relations you postulated hold when there is a well-defined " effective mass " i.e. killing energy , but in a general spacetime with no timelike killing vector , you will not be able to make a sensible definition of such a thing .
first of all , earthquakes are not necessarily transverse waves . both transverse and longitudinal waves are there in seismic waves . these waves depend upon both modulus of elasticity and density of medium . longitudinal p-waves ( primary ) have properties similar to that of sound and due to their compressive and rarefactive wave motion , they reach us faster than transverse s-waves ( secondary ) . these are the before-socks as what we call . it is the effect of s-waves which cause the shear fracture of the rocks ( due to high amplitudes ) . they result of rapid sideways movement of faults thereby causing rocks to shake randomly around their hypo-center . they typically travel up to 60% of the velocity of p-waves . they necessarily require the shear-modulus of the medium . but , they are the most destructive type of all . all the answers for your questions are yes . ' cause they have already been a done-deal . . ! refer wiki for a more detailed description regarding the topic . . .
part 1: conceptual/physical intuition since there is an electrostatic attraction between the 2 particles , then when they are apart they are at a higher potential energy then when they are together . here 's an analogy : physically , this situation is like having a ball at the top of a hill overlooking a valley or well . the ball will roll down the hill and that potential energy is converted into kinetic energy . when the ball reaches the bottom of the valley it will start climbing back out of the well and turn that kinetic energy back into potential , so if the ball starts at rest it only gets back to being as high as it started . however in the real world there is friction that will steal some of this kinetic energy and so the ball will roll back and forth , but eventually come to rest at the bottom of the hill . for the electron an proton you will see something similar . the 2 particles will accelerate towards each other , pass/scatter off each other ( and then repeat ) and will slowly lose energy to " friction " i.e. to radiation . part 2: specific questions 1 ) do they collide and bounce off ? ( conserving momentum ) 2 ) does the electron get through the proton , i.e. between its quarks ? the collision between the two particles is perfectly elastic . in addition the energies ( ~13ev ) are so small relative to the strong force holding together the proton that quarks are not involved in any way , and the scattering is described by rutherford scattering . 3 ) do both charges give off brehmsstrahlung radiation while moving towards each other ? the 2 particles will radiate and lose their kinetic energies . the term brehmsstrahlung is generally reserved for much higher particle energies ( > kev ) , and much larger accelerations . suppose i can control the two particles , and i bring them very close to each other ( but they are not moving so quickly as before , so they have almost no momentum ) . then i let them go : 1 ) would an atom be spontaneously formed ? you can immediately describe the 2 particles by their center of mass description ( an atom ) plus their individual attributes ( i.e. . what the particles are doing within the atom ) . assuming the 2 particles start off at rest , then they are in a bound state already because they can not escape each other ( go off to infinite separation ) due to lack of energy . however the atom will not be in it is ground state until it has decayed into the lowest level via spontaneous emission of radiation . 2 ) if anything else happens : what kind of assumptions do we make before solving the tise for an hydrogen atom ? does the fact that the electron is bound enter in it ? this is to say : is quantum mechanics ( thus solving the schrödinger equation ) the answer to all my questions here ? the tise of the atom itself will give you energy levels etc , but you will not get spontaneous emission into the ground state unless you put it in by hand ( and it would not be time independent anymore ) or also quantize the em vacuum ( which is how you derive se ) . so trying to solve it would be like solving the ball moving on the hill while ignoring friction , it will just oscillate at constant energy forever .
this question is all about the signal to noise ratio you achieve in your experimental setup , so the details are highly dependent on the latter . here are the physical principles you would use to calculate how long it takes a fringe pattern to form . assuming the source sends unentangled photons , each photon propagates following maxwell 's equations . so the propability density of absorption as a function of time can be calculated as a classical intensity as a function of time . simply put , this means that the time taken for the pattern to reach the screen is simply the propagation delay : the propagation distance $\ell$ divided by $c$ . however , most of the time for the interference pattern to form is the time taken for each detector - each pixel , if you like - in the interference pattern to register enough photons that it can report , with the appropriate level of statistical confidence , that the number of photons it has registered is lower or higher than that of the neighbouring detectors such that the data gathered from the whole detector array bespeak what we would call " fringes " . this is probably more easily explained by a simple calculation . suppose we have an array of ccd detectors lined up along the detection screen . the fringe pattern will form fastest when the detector spacing is exactly the fringe spacing . if the fringe visibility is $\mathscr{v}$ , then the ration of light intensity in the troughs to that in the peaks is : $$i_{min} = i_{\max}\frac{1-\mathscr{v}}{1+\mathscr{v}}\tag{1}$$ if each detector 's area is $a$ then the mean number of photons arriving each second is $$\mu ( i ) =\frac{i\ , a\ , \lambda}{h\ , c}\tag{2}$$ where $\lambda$ is the light 's wavelength . photon arrivals from most cw sources like lasers follow poisson statistics , so if a detector 's light gathering time is $\delta t$ , then the number of photons actually gathered in that time will be poisson-distributed with mean : $$\mu ( i , \ , \delta t ) = \frac{i\ , a\ , \lambda\ , \delta t}{h\ , c}\tag{3}$$ so what you are looking for is a $\delta t$ such that there is " overwhelming " probability that the number of photons detected in each photon field peak will be greater than the number detected in each trough . here is where statistical confidence levels enter the calculation . in symbols , we want the probability that : $$n_p\left ( \left ( \frac{\eta\ , i_{\max}\frac{1-\mathscr{v}}{1+\mathscr{v}}\ , a\ , \lambda}{h\ , c} + \sigma_d\right ) \ , \delta t\right ) &lt ; n_p\left ( \left ( \frac{\eta\ , i_{max}\ , a\ , \lambda}{h\ , c} + \sigma_d\right ) \ , \delta t\right ) \tag{4}$$ to be greater than some " reasonable " confidence level ; say $0.9$ for a rough calculation . here $n_p ( \mu ) $ is the number of photon measurement events that poisson variable with mean $\mu$ actually assumes in a given observation . i have also added a residual constant detector noise $\sigma_d$ which will always present owing to noise in the detector electronics and so forth . it is the number of " false positive " detections per unit time and most often , with photodetectors , represents the " dark current " . i have also added a quantum efficiency $\eta$ for the detectors ; this is a probability of a " false nagative " detection and for modern detectors $\eta \approx 0.8$ is reasonable . the above is quite an involved calculation to do properly , because the difference between two poisson rvs is not a poisson rv ( poisson distributions do not have the nice self replication property under summations that normal or chi-squared distributions have ) , so we make a normal approximation for a back of the envelope calculation . re-arranging ( 4 ) in this case shows that the peak detector photon number is greater than the trough detector photon number by an approximately normal random variable whose mean $\mu$ and standard deviation $\sigma$ are : $$\mu = \frac{2\ , \eta\ , i\ , \mathscr{v}\ , a\ , \lambda}{h\ , c}\ , \delta t ; \ ; \sigma^2 = 2\ , \left ( \frac{\eta\ , i\ , a\ , \lambda}{h\ , c}+\sigma_d\right ) \delta t\tag{5}$$ ( here i have simply added the two poisson variable variances , given that a poisson rv 's variance equals its mean ) and we want to choose $\delta t$ such that the probability of this random variable 's being positive is equal to our confidence level . here i have used $i = i_{max} / ( 1+\mathscr{v} ) $ to rewrite my equation in terms of the mean fringe pattern intensity $i$ instead of the peak $i_{max}$ . let 's put some numbers for an experiment in . suppose we do our experiment with a light intensity of $10^3\mathrm{w m^{-1}}$ ; this is a reasonable laboratory intensity if a $100mw$ laser ( beware : this is at least a class 3b laser if you are doing this ) lights an interference pattern that is a centimetre across , and suppose that our pixel area is $10^{-10} m^2$ , this corresponds to very big ccd cells . then , at $\lambda = 5\times 10^{-7}m$ , a quantum efficiency of $\eta=0.8$ , a fringe visibility of 0.5 and a perfectly clean measurement ( $\sigma_d=0$ ) , we get : $$\mu \approx 2\times 10^{11}\ , \delta t ; \ ; \sigma^2 \approx 4\times 10^{11}\ , \delta t\tag{6}$$ ( here the $2\ , \sigma^2$ comes from the fact that we subtract and with a critical value of the normal distribution for $\alpha=0.9=90\%$ confidence of $\sqrt{2}\ , \mathrm{erf}^{-1} ( \alpha ) \ , \sigma \approx 1.64 \sigma$ , we need $\mu-1.64\ , \sigma\geq0$ or $$ 2\times 10^{11}\ , \delta t \geq 1.64\times \sqrt{4\times 10^{11}\ , \delta t}\ , \rightarrow\ , \delta t \geq \frac{1.64^2\times 4\times 10^{11}}{2^2\times 10^{22}}\tag{7}$$ i.e. about 30 picoseconds , at which time you have gathered about $$3\times 10^{11} \times10^{-10} \times 5\times 10^{-7}/ ( h\ , c ) \approx 15$$ photons per pixel . your electronics is not likely to be this fast , so that the electronic delay is the dominant factor . more typically , light is spread much more widely and you need to gather light for much longer to get high quality fringes : an interferometer i have used to test small lenses takes at least several microseconds to gather a fringe pattern and i have calculated it to be very near to achieving the ideal situation studied above . quantum noise ( photon arrival variation with the poisson statistics described above ) limits high speed interferometry and microscopy much more often than you might think . another , qualitative way to answer your question is to get mathematica ( or other numerical simulation ) to calculate simulated fringe patterns by assigning random photon positions according to an intensity pattern , and then increasing the total number of photons until your fringe pattern looks clear . then you need to calculate what signal acquistion time you need given the calculated intensities in your experimental setup . but you will find about 15 photons per pixel to be a pretty representative figure . in the instruments i have designed , a common " design " standard is to aim for 100 photons per pixel ; this gives you about a 10db signal to noise ratio , given the variance is then $\sqrt{100}=10$ photons per pixel .
the wok is a design that gets the maximum efficient use of energy for cooking and needs a minimum of heat source . a flat bottom equivalent pan transfers the heat to the food from the bottom plane , and the walls are just containers radiating most heat away , since the food is at the bottom . a wok focuses the heat from the walls to the food being cooked ( think of a parabola and its focal point ) . the walls partake in the cooking process effectively giving more area contact to the food than the flat bottom one .
you have a couple of mistakes . first , if you say that $g = 6\times 10^{-11} k$ , then $k$ should be $\frac{\text{m}^3}{\text{kg}\cdot \text{s}^2}$ . if we instead define $k$ as you did , then it is a dimensionless number : the conversion factor between the two sets of units . you should rather have said that $6\times 10^{-11} / k$ is the value of $g$ in the system of units you want to use . note that we are dividing by $k$ instead of multiplying ; this is because $\frac{\text{m}^3}{\text{kg}\cdot \text{s}^2} = \frac1{k} \frac{\text{au}^3}{\text{kg}\cdot \text{d}^2}$ . your other problem is that $\text{au}/\text{m}$ should have $10^{11}$ instead of $10^{-11}$ . taking that into account , we get that $k = 4.485\times 10^{23}$ , and $g = 1.488\times 10^{-34} \frac{\text{au}^3}{\text{kg}\cdot \text{d}^2}$ . wolframalpha then says that $t = 365.13 \text{d}$ , which seems pretty close to correct .
the block is accelerating , but that is due to the pseudo force , not the frictional force , which is zero . you can not just look at the horizontal forces on the block to determine whether or not static friction will hold . you have to consider what is going on with the other surface as well . here 's a simpler situation to consider first in an inertial frame . imagine two identical blocks stacked on top of each other . there is friction between the two blocks , but there is no friction between the lower block and the horizontal surface . now push on the upper block while holding the lower block in place . it will slide if $f_\text{applied} &gt ; \mu m g$ , as you describe . but if you instead push on each block with identical forces , they will move together and there will be no frictional force ( because alone they would have identical motions ) . here , the top block is accelerating even though the frictional force is zero ; it is the applied force that is causing it to accelerate . same thing with your situation . the block and surface alone would have the same motion as each other in the non-inertial frame , so there will be no frictional force . yes , the block is accelerating , but that is due to the pseudo force , not the frictional force , which is zero .
you can do the transformation to the relative coordinate $\mathbf{r} = \mathbf{r}_1-\mathbf{r}_2$ and center-of-mass coordinates $m\mathbf{r} = m_1\mathbf{r}_1+m_2\mathbf{r}_2$ and do one of the integrals trivially provided the two functions inside the integrals depend only on $\mathbf{r}$ ( or only on $\mathbf{r}$ ) . otherwise you will still be left with two integrals one over $\mathbf{r}$ and the other over $\mathbf{r}$ . in the part that you refer to in pathria 's book the two functions $$f ( \mathbf{r}_1 ) = r \frac{\partial u ( r ) }{\partial r} ; \qquad f ( \mathbf{r}_2 ) = g ( \mathbf{r}_2 - \mathbf{r}_1 ) $$ depend only on $\mathbf{r}$ . a transformation to $\mathbf{r}$ and $\mathbf{r}$ coordinates then decouples the $\mathbf{r}$ integral which has given a factor of the system volume ( equation ( 16 ) in the image ) . further if the functions inside the integrand depend only on $r=|\mathbf{r}|$ , then one can transform to spherical coordinates as is done in pathria . .
this is a tricky question and in my opinion some elder physicists are deliberately try to confuse students by using euphemisms when characterizing phase transitions . roughly speaking ( there might be counter examples , please comment . i am interested of finding all of them ) : first order phase transition : finite correlation length scales as e.g. $k^\alpha , \alpha = 2$ ( short or finite range interaction ) in fourier space ( 1d ) second order phase transition : infinite correlation length scales as e.g. $k^\alpha , \alpha = 1$ ( long or infite range interaction ) in fourier space ( 1d ) scale invariance ( i think soc goes here . . . ) note that there is a continuous transition with exponent $\alpha$ that escapes my mind . also $\alpha$ is dependent of dimension and probably something else ( see e.g. anne tanguy et al . from individual to collective pinning : effect of long range interactions , pre 1998 ) . also daniel fisher has a nice paper , collective transport in random media : from superconductors to earthquakes . also i just stumbled upon : http://www.tcm.phy.cam.ac.uk/~bds10/phase/introduction.pdf which has a nice overview . in general the purpose of these correlation lengths , roughness exponents and orders of phase transitions is just to find universality classes . the goal is to group the phenomena together and say " look , all these systems have properties of x , y and z . this simple model has the same properties . so by explaining the simple mode , i explain all these systems . " simple characterization of phase transitions can be found at : http://link.aps.org/doi/10.1103/revmodphys.76.663
the basic setup is correct , conservation of energy might be the quickest way to go . $$ ( m_1 -m_2 ) g h = \frac 1 2 i \omega^2 + \frac 1 2 ( m_1+m_2 ) v^2 , i=\frac{mr^2}{2} , \omega = v/r$$ gives me one of your options as the result . the two $m$ in your formula seem to refer to different quantities .
i cannot claim to speak for " the community " ( whoever they might be ) , but so far i have only heard positive replies from knowledgeable people . of course , people will need to read the paper in close detail , there will be discussions in seminars etc . so it'll take at least a couple of months before there will be a serious consensus . let me give a few brief comments on the proof . first of all , much of the machinery feeds off the a-theorem proof by komargodski and schwimmer ( http://arxiv.org/abs/arxiv:1107.3987 ) and its refinements ( "lpr " http://arxiv.org/abs/arxiv:1204.5221 ) . the idea is that you can turn on a " dilaton " background and then probe the theory using these dilatons . before people studied 4-pt " on-shell " ( in a technical sense ) amplitudes of these dilatons , in the new paper they look at 3-pt functions that are off-shell . these dilaton amplitudes are connected to ( essential fourier transforms of ) matrix elements of the trace $t$ of the stress tensor . if $t = 0$ then the theory is conformal invariant . the new idea is to note that $t$ has integer scaling dimension ( $\delta = 4$ ) which means that there will be logs $$e \ln \frac{-p^2}{\mu^2}$$ in these amplitudes , and in a unitary theory $e \geq 0 . $ if $e = 0$ you can show ( using unitarity ) that $t = 0$ identically so you would be done . there is a final step ( which i have not internalized yet ) where they say that in a scale-invariant theory this $e &gt ; 0$ anomaly is not allowed because unitary gives bounds on operator dimensions , which in turn control the small-distance or large-momentum behaviour of the amplitude .
when you lift a box of an ideal gas you are not doing any work on the gas , so its internal energy remains constant . however you are increasing the gravitational potential energy of the box and the gas . that is where the energy you put goes .
yes , there is a geometric approach , it is called , as one might guess , geometric quantization . if you read a literature on the subject , you will find a lot of beautiful results , i will describe the basic idea below . the geometric quantization works with a phase space rather then configuration , i.e. you start with a symplectic manifold . it is actually a more general case because once you have the configuration manifold , the corresponding symplectic manifold is just its cotangent bundle with natural symplectic structure . you should fix extra geometric data -- a polarization . it is a half-rank integrable distribution , so locally on the phase space you have a foliation , which defines what direction corresponds to " coordinates " and what -- to momenta . for example in the usual " p , q " case you take half-rank distribution , i.e. half-rank subbundle of tangent bundle , generated by ${\partial\over\partial p}$ , so it generates " momentum direction " and the transverse direction corresponds to " coordinates " . then you build a prequantization line bundle , which is a hermitean line bundle with hermitean connection , such that its curvature is proportional to the symplectic form ( the usual normalization ${i\over 2\pi}r = \omega$ , where $r$ -- curvature , $\omega$ -- symplectic structure ) , so that symplectic form is just the first chern class of this line bundle . you do not need polarization for this step . but you actually need the first chern class of the bundle to be integer , which can be considered as a general case ( which corresponds in the compact case to the condition that the phase space has integer volume in the units of $ ( 2\pi\hbar ) ^{dim/2}$ ) . then you consider the space of sections of this bundle and take it is subspace , annihilated but the connection ( covariant derivative ) , taken along the polarization . what obtained ( or , more precisely , $l^2$-closure of square-integrable sections from the obtained subspace ) is defined to be the hilbert space of the problem . in the standard example , you take functions on the phase space , annihilated by ${\partial\over\partial p}$ , which are just functions $\psi ( q ) $ depending on coordinates only , and then consider square-integrable functions of coordinates . next for each ( smooth ) function on the manifold you build operators on that space . there are different existing prescriptions for that , i will not cover it here . should also note that there exists another version of geometric quantization -- berezin-toeplitz quantization -- where you do not need polarization in a sense described above . instead you define the hilbert space either as as a kernel of the $spin^c$ dirac operator , or as a kernel of the " renormalized bochner-laplasian " operator . this approach is pretty computable , especially in the case when the symplectic manifold is actually kahler ( in such case standard geometric quantization works well too , since there is a canonical holomorphic polarization ) . also have to add , that since there is no global notion of coordinate in general case , there is no " coordinate operator " , instead you build operators , corresponding to smooth functions on the manifold . and one more comment -- it is tentative to say , that path integral could provide such a coordinate-free description . but in fact path integral is not a well-defined notion by itself , there is no well-defined and invariant notion of integration over the infinite-dimensional space of paths in general . to define path integral you need to fix some additional data , you need to " regularize " it ( discretize , mode-expand , whatever ) . in many cases that will include fixing coordinates as well . also , in general case this regularization can break coordinate invariance of the phase space . path integral can not be considered as an honest prescription . in general it is just a heuristic instrument used by physicists in cases , when they have no better tools to use . though in some particular examples path integral can be justified .
dimensional analysis is arguably the most important technique in any physicist 's bag of tricks . it is used regularly throughout physics . variations of the principle are also used in mathematics , biology and probably in other fields as well . sticking to physics , here is an example from classical mechanics : derive kepler 's third law . the independent variables for a planet are its mass $m$ , its distance $r$ from the sun , its period $t$ , and the force $f$ acting on it . all other variables , like the velocity or acceleration , are functions of those . the only dimensionless number we can construct from the variables is $$ \frac{f t^2}{m r} \ , , $$ so this number must be the same for any planet in our solar system ( assuming that there is some functional relation between the variables ) . we also have newton 's law of gravitation , $$ f = \frac{g m m_s}{r^2} \ , . $$ together , we see that $$ \frac{t^2}{r^3} = const \ , . $$ most fields outside the sort-of fluid mechanics described above are easy enough to calculate without dimensional analysis . i encourage you to pick up polchinski 's book on string theory and work through the first few chapters . then review that statement .
if the neutron decayed to a two body state ( any two body state ) the energy spectrum of the products in the neutrons rest frame would be single valued ( this is required by the conservation of energy and momentum ) . it is not . instead the electron energy spectrum is a continuum that runs from that roughly the two-body limit down to as near zero as our instruments can measure . to grab an image from the wikipedia : so , a third particle is required . that third particle is known to be uncharged ( because our detectors are sensitive to charged particles and do not see it ) . it is also known to be of very low mass because the end-point of the electron energy spectrum is almost exactly what you would expect from the two body decay . the lifetime of the neutron suggests that the interaction that is responsible for it is decay is very weak ( and going on a little further in history it obeys the principle of weak universality suggesting that it is the same interaction responsible for the decay of strange hadrons ) . the sum of these requirements constrain the properties of the third particle quite a lot , and much observation since then has shown quite conclusively that neutrinos exist .
the first assumption is that whatever vev the higgs picks up is constant in space , because this has less energy than one that increases the kinetic term in the lagrangian . so we can do one global transformation to make the vev be in the second component only . you can imagine doing this prior to symmetry breaking , if you know what it is going to be ahead of time , and since the other fields are invariant , bob 's yer uncle . stated differently , the pre-symmetry-breaking electrons and neutrinos are not the ones we observe , so we just label whatever remains as electrons and neutrinos . " without loss of generality " , we work in an electron-neutrino ( global ) basis in which the higgs starts out with only the second component of the vev being nonzero and real . if you buy that part , then it is just a matter of showing that you can perform a gauge transformation that gauges away all the other components of the higgs except the real part of the second component . this gauge transformation will of course mix $\nu$ and $e$ spatially , but you can say that when we perform the path integral we have a gauge redundancy , and so we only integrate along a slice that obeys some gauge fixing condition . the components of $l$ might as well be labeled $l_1$ and $l_2$ . it is only after we have chosen a gauge that we decide , hey , let 's name them $\nu$ and $e$ .
white dwarves used to be the interior of a star , which was the hottest part of the star . they shine white because they are still very hot from this past part of their history . as they age , they will cool , and as they cool , they will lose temperature , and their blackbody profile will shift to redder and redder colors , and eventually into the infared and radio ranges where they will not seem to shine at all to the naked eye . and yes , a white dwarf state is a stable final state of a star , so long as it does not interact further with any matter . if that happens , it is possible to have a white dwarf supernova
a vector in a ( polynimial ) $gl ( 3 ) $ representation is highest weight if it is annihilated by the raising root operators $a_jk = b_j^{\dagger}b_k$ , $k&gt ; j$ . in our case , the relevant operators are $a_{12}$ , $a_{23}$ , and $a_{13}$ . we do not need to check the third case , because $a_{13} = [ a_{12} , a_{23} ] $ is given by the commutator of the two other operators . now , the check is fairly easy : $a_{12} b_1^n | 0 \rangle = ( b_1^{\dagger} ) ^{n+1} b_2| 0 \rangle = 0$ $a_{23 }b_1^n | 0 \rangle = ( b_2^{\dagger} ) ( b_1^{\dagger} ) ^{n} b_3| 0 \rangle = 0$ of course the vectors $b_1^n | 0 \rangle$ for different $n$ will belong to distinct representations . also , please notice that these vectors do not generate all the $gl ( 3 ) $ irreducible representations , because the weights of these highest weight vectors will be $ ( n_1 , n_2 , n_3 ) = ( n , 0 , 0 ) $ where , $n_i$ is the eigenvalue of the $i$-th number operator $n_i = b_i^{\dagger}b_i$
calling it a built-in voltage is something of a misnomer . people usually think of " voltage " as " what you measure with a voltmeter " . so " voltage " is normally synonymous with " electrochemical potential of electrons " ( in stat mech terminology ) and with " difference in fermi level " ( in semiconductor terminology ) . under this definition , the built-in " voltage " is not actually a voltage . then what is it ? it is what chemists call " galvani potential " , and some physicists call " electrostatic potential " . it is the line-integral of electric field . ( maybe you should call it " built-in potential " , not " built-in voltage " . ) voltage / fermi level measures the total " happiness " of electrons , the sum of all influences on the electron . the electric field ( galvani potential ) is just one of those many influences . other influences include diffusion ( entropy ) , the kinetic energy of the electron 's wave function , etc . etc . but it is the sum of all influences that determines how the electron moves . that is why it is the voltage , not the galvani potential , that determines the most important things like current flow and energy dissipation . so to summarize : the " voltage " across a p-n junction is zero , when the word " voltage " is defined in the most common and sensible and intuitive way . after all , the junction is in equilibrium ; an electron is equally happy to be on either side . for more details see my other answer : fermi level alignment and electrochemical potential between two metals going around a loop , both the voltage differences and the galvani potential differences sum to zero . but only the former is really important . for the galvani potential differences , most of them are unobservable , like the volta potential at the junction when you solder an aluminum wire to a copper wire . it is possible to figure out the galvani potential differences everywhere in a p-n junction circuit , including at the wire contacts , at the voltmeter , and so on . if you do figure them out , and add them all up , you will get zero ! but since none of those parameters matter for the circuit behavior , people rarely think about them or try to figure them out .
it seems to me that " symmetric fission " refers to any fission process where the end products are symmetric about some point . specifically , where the end products are symmetric in their atomic mass . this website explains it quite well , but for completeness i am quoting the relevant paragraph in the article below . it is thought to be helpful for a better understanding to take the atomic numbers into consideration . the atomic numbers of the above-mentioned elements are : ru = 44 , rh = 45 , pd = 46 , ag = 47 , cd = 48 , in = 49 and sn = 50 , respectively . by noticing that the atomic number 46 of palladium is just half that of 92 of uranium , it is supposed that one uranium atom splits into two palladium isotopes . when rhodium ( atomic number 45 ) is produced with some probability ( cross section ) , silver ( atomic number 47 ) is the counter fragment . in the same way , ruthenium ( atomic number 44 ) and cadmium ( atomic number 48 ) are the pairing fragments . thus , the nuclear fission observed by nishina and kimura is highly symmetric .
the underlying space is curved . there is no ' straight ' path . it is like asking why planes have to travel on great arcs to get the shortest path between cities on the earth 's surface , rather than flying on a straight line on a mercator projection .
it is difficult to determine which will fare better . small mammals can survive a fall from arbitrary distances . here 's one article i found talking about cats . a chief contributor to small mammals ' survival is that they have a lower terminal velocity due to the way wind resistance scales . wind resistance scales with the area of the animal , while weight scales with the volume , so large animals fall faster because they have a higher volume-surface area ration . on a long fall ( hundreds of times the animal 's body length ) , body size influences impact velocity even without this effect , small animals may have an advantage . in humans , we can study the statistics of plane crash survivors . according to wikipedia " since 1970 , two-thirds of lone survivors of airline crashes have been children or flight crew . " children make up a small percentage of passengers , so it follows they have a better chance of surviving . it is no great leap to attribute this to their body size . however , a bbc news story has an expert saying there is no physiological advantage to being a child when it comes to surviving a plane crash . i have occasionally seen this sort of question addressed with dimensional analysis , but the difficulty is that it is difficult to pin down what you want to try to scale . the peak force or pressure , the energy dissipated per unit mass , the peak power dissipation per unit mass ? here 's an example argument : if we assume the two people are impacting at the same speed , they need to dissipate the same amount of energy per unit mass . assume that people can dissipate a certain amount of energy per unit mass in a given time without harm . then whoever can make the impact last for a longer time will fare better . a taller person can bend their legs through a longer distance , and therefore can make the impact take a longer time , and therefore can fare better . however , the assumptions in this argument would need to be verified before we can take it very seriously . here 's another argument : when two people hit the ground at the same speed , the time it takes them to stop is proportional to their linear dimension because this time is roughly their height divided by the speed that mechanical waves move through their body . their acceleration is inversely proportional to height . their mass is proportional to the cube of their height , so the force is proportional to the square of their height . that makes the pressure independent of height , so large and small people will fare equally well . here 's another : same as above , but the deceleration time is proportional to the square root of height because they are flexing their knees , and so the stopping distance is proportional to height . this now favors short people . another : same as above , but mass scales with the square of height , because people are not scale invariant ( the bmi uses an exponent of two ) . this favors tall people . another : same as above , but mass is independent of height because we are considering a skinny twerp and a muscular jock . this now favors light people . my conclusion is that the problem is indeterminate . it depends on whether we are talking about a scaled-down version of the same person , or a single guy who starts taking steroids to prepare for a parachute jump . it also depends on various material properties of the human body , and on what sorts of things cause injury . ultimately i think it is an empirical question , or at least one that requires extensive computer modeling .
a group $g$ by itself is not a group of linear transformations , it is an abstract algebraic object . only its representations map its elements ( injectively if the representation is faithful ) to elements $\mathrm{aut} ( v ) $ of some vector space $v$ . now , physics seems to have no need of such abstract language at first . our " vector space " is pretty much our spacetime , and its pretty much $\mathbb{r}^4$ , so your symmetries are really just matrices on that spacetime . the lorentz symmetry is just $\mathrm{so} ( 1,3 ) $ in its fundamental representation on minkowski space $\mathbb{r}^{1,3}$ , right ? or non-relativistic , rotational symmetry is just $\mathrm{so} ( 3 ) $ on $\mathbb{r}^3$ , right ? . . . and then there is angular momentum and spin . if you solve the schrödinger equation for the energy levels of a hydrogen atom , you find that the energy levels are characterized by " quantum numbers " $ ( n , l , m , s ) $ . now $n$ is boring . but $l$ and $m$ are eigenvalues of the spherical laplacian , and lead to the beloved spherical harmonics $y^l_m$ as independent solutions . turns out , if you rotate the system in space , these harmonics behave differently depending on their $l$ ! formally , the space $$h_l := \{\sum_m c_m y^l_m | m \in \{-l , -l+1 , \dots , l\}\wedge c_m \in \mathbb{r}\}$$ is a vector space , and it carries a representation of the rotation group $\mathrm{so} ( 3 ) $ ! but not the fundamental one , if $l &gt ; 1$ . so there is your non-fundamental representation arising solely by solving the equations describing a physical system . it gets even weirder for these rotation groups , since it also turns out that there are objects , the fermions , which do not transform in a representation of $\mathrm{so} ( 1,3 ) $ or $\mathrm{so} ( 3 ) $ , but in a representation of their universal covers , $\mathrm{spin} ( 1,3 ) $ or $\mathrm{su} ( 2 ) $ , respectively . you have no chance to describe the kinds of phenomena you observe for fermions without accepting that they transform that way . and that is not the end of the story . if you build a gauge theory with gauge group $g$ , you will find that the associated field strength of the gauge field must transform as an element of the adjoint representation of $g$ . non-fundmental representations pervade many aspects of ( quantum ) field theory in that way .
at tree level , the conditions for the electroweak-symmetry breaking vacuum of the mssm can be found in any of the standard review articles and are : $$ \sin ( 2\beta ) = \frac{2b_\mu}{2|\mu|^2 + m_{h_u}^2 + m_{h_d}^2} $$ $$ \frac{1}{2} m_z^2 = -|\mu|^2 - \frac{m_{h_u}^2 \tan^2\beta - m_{h_d}^2}{\tan^2\beta - 1} $$ now , the right logical way to think about this is that a top-down theory determines the values of $\mu$ and the soft-breaking parameters $m_{h_u}^2$ , $m_{h_d}^2$ , and $b_\mu$ . these are really the inputs . on the other hand , we only want to consider the slice of parameter space on which ewsb is realized as in our world , with a particular value of $m_z$ . one can choose other useful coordinates on this slice , like $\tan \beta$ , as software inputs to choose only those points on this slice of parameter space , rather than specifying the high-scale input , which in general will fail to realize correct ewsb .
energy of a fission nuclear bomb comes from the gravitational energy of the stars . protons and neutrons can coalesce into different kinds of bound states . we call these states atomic nuclei . the ones with the same number of protons are called isotopes , the ones with different number are nuclei of atoms of different kinds . there are many possible different stable states ( that is , stable nuclei ) , with different number of nucleons and different binding energies . however there are also some general tendencies for the specific binding energy per one nucleon ( proton or neutron ) in the nuclei . states of simple nuclei ( like hidrogen or helium ) have the lowest specific nucleon binding energy amongst all elements , but the higher is the atomic number , the higher the specific energy gets . however , for the very heavy nuclei the specific binding energy starts to drop again . here is a graph that sums it up : http://en.wikipedia.org/wiki/file:binding_energy_curve_-_common_isotopes.svg it means that when nucleons are in the medium-atomic number nuclei , they have the highest possible binding energy . when they sit in very light elements ( hidrogen ) or very heavy ones ( uranium ) , they have weaker binding . thus , one can say that for the low " every-day " temperatures , the very heavy elements ( like the very light ones ) are quasistable in a sense . fission bomb effectively " lets " the very heavy atomic nuclei ( plutonium , or uranium ) to resettle to the atoms with lower number of nucleons , that is , with higher bound energies . the released binding energy difference makes the notorious effect . in terms of the graph cited above , it corresponds to nucleons moving from the right end closer to the peak . yet this is not the only way to let nucleons switch to the higher binding energy state than the initial one . we can " resettle " very light elements ( like hydrogen ) and let nucleons move to the peak from the left . that would be fusion . heavy nucleons emerge in the stars . here the gravitational energy is high enough to let the nucleons " unite " into whatever nuclei they like . stars usually are formed from the very light elements and the nucleons inside , again , tend to get to the states with lower energies , and form more " medium-number " nuclei . the energy difference powers stars and we see the light emission , high temperatures and all other fun effects . however , sometimes the temperatures in the stars are so high , that nucleons form the very heavy nuclei from the medium-number nuclei . even though there is no immediate " energy " benefit . these heavy elements then disseminate everywhere with the death of the star . this stored star energy can then be released in the fission bomb .
strictly speaking vacuum is the state of lowest energy . that means no matter or radiation ( photons or any other particles ) . note that space is not a perfect vacuum . also note that , technically , a gas of planets and comets etc . has a pressure ( there is usually little reason to care about it though ) . there is also radiation pressure due to the photons . people often use the term vacuum loosely to refer to anything less than atmospheric pressure . this is the sense people use when they say space is a vacuum . edit ( re the comments ) : yes , there is a minimum energy . imagine that you start with vacuum . there is nothing there by definition . now create some particle . this necessarily takes some energy ( at least $mc^2$ where $m$ is the mass of the particle ) , so the state with a particle in it has more energy . now the value of the vacuum energy is a subtle thing . without gravity only energy differences matter , so you can always set the vacuum energy to zero . but with gravity it is tricky , because all energy gravitates . indeed , physicists now believe that empty space has an energy density , now known as dark energy . now people will tell you a big song and dance about quantum fluctuations and zero point energy , but this is only one side of the coin , and only comes in when you try to actually calculate the vacuum energy from a more basic theory ( quantum field theory ) . the basic picture is really simple though : vacuum energy is just a number - some physical constant that we could go out and measure . now if you check very carefully all the laws we know then you will find that gravity is the only place the vacuum energy comes in , so for most purposes you can forget about vacuum energy . ( people also mention the casimir effect around this point , but that is another thing entirely . ) on the other question : whether true vacuum is achievable theoretically . well , it depends what you mean " theoretically . " if you mean " in the mind of a theoretical physicist " then sure , it is possible . ; ) but if you mean there is some way to build a box and make a perfect vacuum inside of it then no , you can not , because the box will always have some finite temperature and hence blackbody radiation will fill the cavity . you can make it arbitrarily close to true vacuum by cooling the box , but you could never actually reach it .
from qmechanic 's comment , maybe something like the noncommutative geometry approach would appeal to you . see e.g. , noncommutative standard model on wikipedia and alain conne 's homepage .
first consider there is no friction . the point of contact between the ball and the table moves with the direction of the global motion . now introduce friction : you have kinematic friction slowing down this point thus make the ball roll due to the induced torque . you will have a motion in between the cases of pure sliding and pure rolling . in this case the direction of the friction force is obvious ( by definition of the friction ) . now if you do the things at the limit case , you will have a pure rolling . in that case the point of contact has zero instantaneous velocity and if the motion is horizontal , with constant and angular and linear motion , you do not need any friction , if you had friction , this would induce a torque and the angular momentum will change . if you introduce acceleration or a non horizontal surface : in that case you have static friction : the point cannot move forward , friction is directed opposite to the " accelerated " direction , you introduce a torque .
the normal force does decrease with angle . this does not mean that the coefficient of friction changes : we can , depending on the angle $\theta$ of the slope , split the gravitational force $f_g = mg$ acting upon a thing with mass $m$ resting on the slope into the normal force $f_n = mg \cos ( \theta ) $ and the force pointing down the slope , $f_s = mg\sin ( \theta ) $ . now , the coefficient of friction is a property of materials , and does not change with the angle - but it is the case that the friction force will decrease since it is $f_k = \mu_kf_n$ . the " greater propensity " of things to slide down steeper inclined slopes is due to the friction force decreasing , and due to the force pointing down the slope increasing with increasing angle .
for organic matter , such as bread and human skin , cutting is a straightforward process because cells/tissues/proteins/etc can be broken apart with relatively little energy . this is because organic matter is much more flexible and the molecules bind through weak intermolecular interactions such as hydrogen bonding and van der waals forces . for inorganic matter , however , it is much more complicated . i collaborate with a group who perform nanoindentation experiments on ceramics which involves forcing a nanoscopic tip into a material - essentially equivalent to cutting it with a knife . i have probed them on what is actually happening at the atomic level during these experiments and what ' hardness ' means in this context , but they simply do not know . much of the insight that we do have actually comes from computer simulations . for instance , here is an image taken from a molecular dynamics study where they cut copper ( blue ) with different shaped blades ( red ) : in each case the blade penetrates the right side of the block and is dragged to the left . you can see the atoms amorphise in the immediate vicinity due to the high pressure and then deform around the blade . this is a basic answer to your question . but there are some more complicated mechanisms at play . for a material to deform it must be able to generate dislocations that can then propagate through the material . here is a much larger-scale ( $10^7$ atoms ) molecular dynamics simulation of a blade being dragged ( to the left ) along the surface of copper . the blue regions show the dislocations : that blue ring that travels through the bulk along [ 10-1 ] is a dislocation loop . if these dislocations encounter a grain boundary then it takes more energy to move them which makes the material harder . for this reason , many materials ( such as metals , which are soft ) are intentionally manufactured to be grainy . there can also be some rather exotic mechanisms involved . here is an image from a recent nature paper in which a nano-tip is forced into calcite ( a very hard but brittle material ) : what is really interesting about it is that , initially , crystal twins form ( visible in stage 1 ) in order to dissipate the energy . this involves layers of the crystal changing their orientation to accommodate the strain . in short : it is complicated but very interesting !
while writing out my progress on the problem , i managed to give myself the answer . so , i thought that i may as well share the solution as i have seen many people in my class get stuck here . if i have a kinetic energy equal to $k = ( 1/2 ) mv^2$ and i later have a velocity equal to half the original $v$ what happens to $k$ ? should not it be 1/4th the original ? plugging in numbers shows that this is indeed the case . so if my $k$ is 1/4th what it originally was , my $u$ should now be 3/4ths of my original $k$ write that out using $k_i + u_i = k_f + u_f$ $k_i = \frac{1}{4}k_i + u_f$ $\frac{3}{4}k_i = u_f$ so the question , again , was at what height $h$ above the ground is the speed $\frac{1}{2}v$ the answer is $h = \frac{3v^2}{8g}$ you simply plug in the standard $k$ and $u$ equations into $\frac{3}{4}k_i = u_f$ and solve for $h$ as usual .
the current flows shown in the diagram are only temporary and flow only when the battery is first connected . when you first connect the battery holes flow to the left ( in your diagram ) and electrons flow to the right , and the resulting charge separation creates a potential difference across the depletion layer . the flow stops when the potential difference across the depletion layer becomes equal and opposite to the battery potential . at this point the net potential difference is zero so the charges stop flowing .
yes , but it should be stated that the cosmic neutrino background is expected to be very cold and very difficult to see . also , note that when temperatures approach the electroweak unification scale , the electroweak force will treat electrons and neutrinos identically , and the universe will become opaque to neutrinos .
the answer is no . there is no energy density limit ( for all three questions ) . the easiest way to see this is that the energy density is just the $t^{00}$ component of the stress energy tensor . the solution in gr depends on the full stress energy tensor , so it is not enough to just talk about the energy density . furthermore , because the energy density is just a component of a tensor , it is a coordinate system dependent quantity . so starting from a solution that does not become a blackhole , and has some energy somewhere , we can always choose the coordinate system to make the energy density arbitrarily large . more clearly stated : local lorentz symmetry alone is enough to show that the energy density is not limited in gr . and furthermore since there exist non-zero energy solutions that do not become blackholes , this also answers your second question . to make the answer to the third question more clear , let 's discuss an exact solution . consider the robertson-walker solution with a perfect fluid . here 's an example stress energy tensor for a perfect fluid in the comoving frame : $t^{ab} =\left ( \begin{matrix} \rho and 0 and 0 and 0 \\ 0 and p and 0 and 0 \\ 0 and 0 and p and 0 \\ 0 and 0 and 0 and p \end{matrix} \right ) $ now if we change to a different coordinate system , using the coordinate transformation : $\lambda^{\mu}{}_{\nu} =\left ( \begin{matrix} \gamma and -\beta \gamma and 0 and 0 \\ -\beta \gamma and \gamma and 0 and 0\\ 0 and 0 and 1 and 0\\ 0 and 0 and 0 and 1\\ \end{matrix} \right ) $ we see the energy density will transform as : $\rho&#39 ; = \gamma^2 \rho + p \beta^2 \gamma^2 = \gamma^2 ( \rho + p \beta^2 ) $ so not only can the energy density be arbitrarily large , but even over a finite volume .
the correct option is really option 3 . most of the time when a physicist says a theory is renormalizable , they mean that the theory is a relevant deformation of some conformal field theory . this is a non-perturbative definition . it contains the physically meaningful content that the other more technical definitions about counterterms in perturbation theory are attempting to capture . indeed , it implies them . ( however , the reverse implication is not always true . for example , perturbative qed is renormalizable in the sense of option 2 , but there is no underlying non-perturbative qed , so one can not even ask about option 3 . ) it is good practice to always try to think about the non-perturbative meaning of the physical formalism you are studying . perturbation theory is a sometimes useful tool for computations , but it can obscure the physics in a cloud of virtual technicalities . so what does it mean for a theory to be a relevant deformation of a cft ? it means that there is a cft whose observables are essentially the same as the observables in your qft , and that you can compute any correlation function in the qft as $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} = \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i \int\mathcal{o}_i} \rangle_{cft}$ where the $\mathcal{o}_i$ are relevant operators in the cft and $g_i$ are ( dimensionful ) coupling constants . knowing that your qft is near a cft in this sense is what allows you to study the behavior of the qft 's expectation values under changes of scale , which is the heart of the renormalization group analysis . edit : first , an easy example : the free scalar field theory is a conformal field theory . this theory is basically described by $\langle \mathcal{o} \rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int |d\phi|^2} \mathcal{d}\phi$ . in this theory , in dim > = 3 , the operator $\phi^2 ( x ) $ is relevant , so we can deform with this term and get a non-conformal field theory . the expectation value in this qft is then described by $ \langle \mathcal{o} \rangle_{qft} = \langle \mathcal{o} e^{i m^2 \int \phi^2 ( x ) dx}\rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int [ |d\phi ( x ) |^2 + m^2\phi^2 ( x ) ] dx} \mathcal{d}\phi . $ so , not surprisingly , the theory we get by deforming the free scalar cft with a mass term is the massive free scalar . second , a subtlety that i should point out . the first equality above is not exact in most situations . the problem is the deformations we want may not be integrable with respect to the cft 's path integral measure , thanks to uv singularities . this is dealt with by regularizing . so , in most qfts , what we get is a family of approximations $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} \simeq \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i ( \lambda ) \int\mathcal{o}_i ( \lambda ) } \rangle_{cft}$ where the relevant operators and the coupling constants depend on a cutoff scale $\lambda$ and the errors vanish as $\lambda \to \infty$ .
the expression you quote is for a ideal monatomic gas , and we get $c_v = 3/2$ for the three degrees of freedom . for ideal diatomic gases we do indeed have to count rotational degrees of freedom and we get $c_v = 5/2$ . see the wikipedia article on ideal gases for more info .
low pressure in itself does not create any flow , as the pressure is the same everywhere . bernoulli 's principle should always be applied between two point along a streamline . when doing this , you will find that only the pressure difference between to points play a role . then , it comes down to the specific problem your are studying . sometimes you will have information about velocities , sometimes about the pressure or the height . there is now general statement possible .
presumably , the analytical solution is using \begin{equation} p ( x , y , z ) = \lim_{t\to \infty} \frac{1}{t} \int_{-t/2}^{t/2} p ( x , y , z , t ) \ , dt \end{equation} note the limit that takes $t$ to infinity . if the solution is periodic with period $t$ , then this is precisely equivalent to writing \begin{equation} p ( x , y , z ) = \frac{1}{t} \int_{-t/2}^{t/2} p ( x , y , z , t ) \ , dt \end{equation} which is the same thing over just one cycle . the average over infinite time averages over infinitely many cycles , but in the truly periodic case , these cycles are all identical , so they do not change the average at all . now , your numerical simulation will naturally not be precisely periodic , in the sense that it lasts for only a limited amount of time . however , that finite amount of time is just a model for the infinite time of the analytical solution . so if some portion of the numerical solution is periodic , you can just use this last version of the equation over one cycle -- or several cycles to get better numerical behavior . if it is not periodic over even a single cycle , it sounds like the numerical and analytical models just do not agree very well . [ note , of course , that you may have to shift your time axis , since you probably just start your simulation with $t=0$ like most of us . just shift it so that the integration starts at -- let 's say -- the peak of the pressure , and stops at another peak . then divide by the time between those peaks to get the average . of course , using troughs or any other identifiable feature would work just as well . ]
note that area is a vector . when you take a , say , cubical box , electric field acrsos both opposite faces is same . but areas are $\vec {a} , -\vec{a}$ . and the flux is : $$\vec{e} . \vec{a}+\vec{e} . -\vec{a}=0$$ flux across rest faces is zero as area is perpendicular to electric field
they do not make any claim in the paper about interpretations of quantum theory , either for the copenhagen interpretation or against many-worlds interpretations . nor does the phys . rev . lett . 101 , 20403 ( 2008 ) that they cite as their principal theoretical source . the vienna group 's stated intention here , as , i think , in a number of papers over the last few years , has been to try to rule out contextual classical particle models for experiments . this is to me something of a straw man , but they have been hacking away at it . i was going to ask that you expand your question to say why you think this experiment supports the copenhagen interpretation over other interpretations , because i do not see that to be the case , but i finally saw the off the cuff remark to that effect from zeilinger at the very end of the new scientist article [ you should have cited this ] . that is definitely not enough to rule out many-worlds interpretations without a much more substantial argument . there is not , so far , enough of an argument to have loopholes in it .
on this class of problems , there is a secret that allows you to get the answer without having to memorize any formulas . ( by the way , i got a 990 on the physics gre so you can trust that i use this sort of thing . ) you have three things that contribute to the problem : $m$ , $r$ and $v$ . these have three different units , $ [ kg ] , [ m ] , [ m/s ] $ . therefore , anything you want can be obtained from these in only one way . centripetal acceleration is in units of $ [ m/s^2 ] $ , correct ? from this we see that mass does not enter into it . we have velocity $ [ m/s ] $ and radius $ [ m ] $ and the only way we can get $ [ m/s^2 ] $ from these is by $v^2/r$ which is what you wrote . the one caveat on using this trick is that you want to avoid things associated with the period because you will need a factor of $2\pi$ . for example , instead of computing the frequency in hz or $ [ 1/s ] $ , you will get the angular frequency ( i.e. . the rate at which the object eats up angles ) . to get this sort of thing , just recall that the distance around the circle is $2\pi r$ .
forward bias of a p-n junction as the electrons move towards the positive terminal and the holes towards the negative , they will come to the depletion layer . this is a very narrow layer around the junction ( i.e. . around the interface of the two semiconductors . ) in the depletion layer , electrons and holes can recombine , but the recombination rate is not high enough so as not to allow electrons to reach the positive terminal . this recombination effect takes place in the diodes of solar photovoltaic cells as well , and it is an interesting field of research in how to reduce its effectiveness . the recombination rate is smaller for larger energy gaps . by the way , these recombination processes are what generate the emitted light in leds , where the energy gap of the diode is arranged to be visible light of a desired colour .
what is your hilbert space ? in $l^2 ( \mathbb r ) $ your eigenfunction would have infinite norm . if you dealt instead with a bounded set $l^2 ( [ a , b ] ) $ , your operator would not be hermitian unless you impose suitable boundary conditions to discard boundary terms . these boundary conditions , however , would rule out your candidate eigenvector !
part 1 the state vector can be written in terms of the two states at time $t$ as $$ \left|\psi\left ( t\right ) \right&gt ; = c_1\left ( t\right ) \left|1\right&gt ; + c_2\left ( t\right ) \left|2\right&gt ; $$ and at time $t=0$ as $$ \left|\psi\left ( 0\right ) \right&gt ; = c_1\left ( 0\right ) \left|1\right&gt ; + c_2\left ( 0\right ) \left|2\right&gt ; . $$ we know $$ \begin{align} -\omega_0 = \left&lt ; \hat{o}\right&gt ; and = \left&lt ; \psi\left ( 0\right ) \right| \hat{o} \left|\psi\left ( 0\right ) \right&gt ; \\ and = \omega_0 \left ( c^*_1\left ( 0\right ) \left&lt ; 1\right| + c^*_2\left ( 0\right ) \left&lt ; 2\right|\right ) \left ( 3 \left|1\right&gt ; \left&lt ; 1\right|-\left|2\right&gt ; \left&lt ; 2\right|\right ) \left ( c_1\left ( 0\right ) \left|1\right&gt ; + c_2\left ( 0\right ) \left|2\right&gt ; \right ) \\ and = \omega_0 \left ( c^*_1\left ( 0\right ) \left&lt ; 1\right| + c^*_2\left ( 0\right ) \left&lt ; 2\right|\right ) \left ( 3c_1\left ( 0\right ) \left|1\right&gt ; - c_2\left ( 0\right ) \left|2\right&gt ; \right ) \\ and = \omega_0 \left ( 3 \left|c_1\left ( 0\right ) \right|^2 - \left|c_2\left ( 0\right ) \right|^2 \right ) , \end{align} $$ so $$ 3 \left|c_1\left ( 0\right ) \right|^2 - \left|c_2\left ( 0\right ) \right|^2 = -1 . $$ since the state vector must be normalized , $$ \left|c_1\left ( 0\right ) \right|^2 + \left|c_2\left ( 0\right ) \right|^2 = 1 . $$ you can finish this part . part 2 the schrödinger equation tells us $$ i \hbar \frac{d}{dt} \left|\psi\left ( t\right ) \right&gt ; = \hat{h} \left|\psi\left ( t\right ) \right&gt ; , $$ or $$ i \hbar \left ( {\dot{c}}_1\left ( t\right ) \left|1\right&gt ; + {\dot{c}}_2\left ( t\right ) \left|2\right&gt ; \right ) = e_0 \left ( \left|1\right&gt ; \left&lt ; 2\right| + \left|2\right&gt ; \left&lt ; 1\right|\right ) \left ( c_1\left ( t\right ) \left|1\right&gt ; + c_2\left ( t\right ) \left|2\right&gt ; \right ) . $$ i will let you take it from here .
nacl melts at around 800°c . molten nacl has a density of about $1.556 \frac{g}{cm^3}$ [ 1 ] , at room temperature ( solid ) it has one of $2.71\frac{g}{cm^3}$ [ 2 ] . sadly i could not find a value for the density at barely underneath melting point but i strongly assume that the density is a strictly monotonously falling function of temperature . therefore solid nacl will probably sink in liquid salt . the case that ice floats on liquid water is special and known as the anomaly of water .
patents are for inventions , which are feats of engineering , not advances in physics . edison was a great businessman as well as a great inventor : he took understood principles of physics and turned them into useful machines . these machines are codified in the patents . he did not , however , contribute to the understanding of the laws of physics . now , admittedly , tesla did not advance our understanding much either . he , too , was predominantly an engineer/inventor . however , by experimenting with high-voltage current , x-rays and radio waves , he indirectly helped our understanding of electromagnetic radiation and the electromagnetic force .
yes the free body moves outward , but there are two critical things you have to know to interpret this statement correctly . first , this is the effective potential , taking into account gravity and centrifugal force . it has this form because we went into the non-inertial frame co-rotating with the two masses . mathematically , the potential is $$ \phi_\mathrm{eff} ( \vec{r} ) = -g \left ( \underbrace{\frac{m_1}{\lvert \vec{r}-\vec{r}_1 \rvert}}_\text{potential from mass 1} + \underbrace{\frac{m_2}{\lvert \vec{r}-\vec{r}_2 \rvert}}_\text{potential from mass 2} + \underbrace{\frac{m_1+m_2}{2\lvert \vec{r}_1-\vec{r}_2 \rvert^3} \lvert \vec{r} \rvert^2}_\text{centrifugal component}\right ) , $$ and it only decreases far away because of that last term . physically , this is because placing an object " at rest " in this frame corresponds to having it move with the same angular frequency as $m_1$ and $m_2$ about the center of mass . if you initialize an object $5\ \mathrm{au}$ on a tangential path having the same angular velocity as the earth , it will be moving too fast for a circular orbit at that distance , and so it will move away from the sun . this does not mean the object will go away forever , and that brings us to the second point , explained in chay 's response : not all effective forces have been accounted for ; in particular , the coriolis force does not arise from $\phi_\mathrm{eff}$ . the coriolis force depends on velocity , so it has no scalar potential depending solely on position , and so it is not included in the analysis so far . once your test object starts moving in your rotating frame , it will experience a perpendicular deflection that will eventually force it to turn around .
the reason you encountered higher and higher pressure at the center of the rod as you cut it into more pieces is that you were essentially approximating an integral , but the integral diverges ( "is infinity " colloquially ) . when the rod has zero thickness , but still has mass , the density of the matter is infinite , and this leads to infinitely strong gravitational forces . to answer this question , we will imagine the cylinder has some small , finite radius $r$ . we want to find the force between the two halves of the cylinder . we will let one half just sit stationary in space . it will create a gravitational potential . then we will grab the other half and pull it away to some distance $d$ . the gravitational potential energy is a function of $d$ . the force between the two halves of the cylinder is the derivative of the gravitational potential energy with respect to $d$ when $d=0$ . the problem described above is too hard . it is quite difficult to calculate the gravitational potential of a cylinder at an arbitrary point . the gravitational potential of a point mass is just $-gm/r$ , but for a cylinder that extends out in three dimensions , we need to replace $m$ with the density $\rho$ and then integrate over the mass of the entire cylinder . the expression for $r$ , the distance from an arbitrary point outside the cylinder to a point inside it , is not very tractable . however , at a point on the axis of the cylinder , the gravitational potential is more accessible due to the extra symmetry . if we set up cylindrical coordinates with the axis of the cylinder along the z-axis , and then integrate over the bottom half of the cylinder , we get $v ( z ) = \int_{z&#39 ; =0}^{-l/2}\int_{r=0}^{r}\int_{\theta=0}^{2\pi} \frac{g\rho}{\sqrt{ ( z-z&#39 ; ) ^2+r^2}} r\textrm{d}\theta\textrm{d}r\textrm{d}z&#39 ; $ and doing the integral of $\theta$ it is $v ( z ) = 2\pi g\rho\int_{z&#39 ; =0}^{-l/2}\int_{r=0}^{r} \frac{1}{\sqrt{ ( z-z&#39 ; ) ^2+r^2}} r\textrm{d}r\textrm{d}z&#39 ; $ . this allows us to make an approximation . although the half of the cylinder we use to calculate the potential must have finite width , we can calculate the potential energy by assuming that the other half of the cylinder is located perfectly along the axis . as long as the radius of the cylinder is very small compared to the length , this is a valid approximation . so the potential energy comes from integrating the previous expression for $v$ along the $z$-axis for the length of the cylinder . we do not actually want the potential energy , but the derivative of the potential energy . so we imagine moving the top half up the cylinder up a little bit $dz$ , and ask how the potential energy changes . moving the entire top half of the cylinder up by $dz$ is equivalent to taking a piece of thickness $dz$ and slicing it off the bottom and moving it to the top . so we really just need to find the difference in the potential between the top and bottom of the top half of the cylinder and multiply by the mass-per-unit-length of the cylinder . the force between the two halves of the cylinder is $\frac{m}{l} [ v ( l/2 ) - v ( 0 ) ] $ that still leaves two integrals to evaluate . $v ( l/2 ) $ is easy , because it is far away from the half of the cylinder providing the gravitational potential ( compared to $r$ ) . that lets us approximate $v ( l/2 ) = \frac{-gm}{l} \int_{-l/2}^0 \frac{1}{l/2-x}dx = -\frac{gm}{l}\ln 2$ . the integral for $v ( 0 ) $ is trickier , so i put it in mathematica and got $v ( 0 ) = -\frac{gm}{l}\textrm{arcsinh}\left ( \frac{l}{2r}\right ) $ . in the regime we are interested in ( $r$ small compared to $l$ ) the $\sinh ( x ) $ is just $e^x/2$ , so this simplifies to $v ( 0 ) = -\frac{gm}{l} \ln\left ( \frac{l}{r}\right ) $ this gives a final answer for the force $f = \frac{m^2g}{l^2}\ln\left ( \frac{l}{2r}\right ) $
i will undertake a hand waving answer and maybe since the question will come to the top somebody knowledgeable will give a full answer . virtual particles are the province of quantum mechanics and led to the development of quantum field theory with creation and annihilation operators . this led to the realization that the vacuum , in the qft description is not " empty " but is like a sea where virtual particles continuously create and annihilate with no loss of energy . the vacuum is a " ground state " . interestingly enough this field theoretic description with creation and annihilation operators does not correspond one to one with particle physics . back in 1963 i sat through a field theoretical course for nuclear physics where creation and annihilation operators acted on nuclear levels . but i digress . now the curvature of space is a unique proposal of general relativity . general relativity has not been quantized in an irrefutable manner . string theorists believe that they have managed to do that , but i leave it to them to describe what a sea of virtual particles in a string universe is like . trying to naively say : suppose gravity is quantized in the classic qft manner and gravitons exist in the vacuum sea too , the answer would be : the higher the curvature the more the distribution of the particle antiparticle sea would be weighted statistically towards heavier pairs , due to energy considerations with respect to flat space .
the einstein field equations , describing the gravitational field are given by $$r_{\mu\nu}-\frac12 g_{\mu\nu}r = -\frac{8\pi g}{c^4}t_{\mu\nu}\ . $$ they relate in a complicated manner the gravitational field that can be seen as the metric $g_{\mu\nu}$ itself to the stress-energy tensor $t_{\mu\nu}$ that might also depend on $g$ . newtonian gravitation if now velocities are small compared to the speed of light $c$ , the stress-energy tensor approximately only consists of its time-component , $$t_{tt} \approx \rho c^2$$ and the metric is flat with the exception of $$g_{tt} \approx 1 + \frac{2 u}{c^2}$$ where we can find , directly from the einstein equations , that the newtonian $$\delta u = 4\pi g \rho$$ holds . this is a static approach and we see that there is no dependence on any flux term $j_i \propto t_{ti}$ . velocity matters : rotating disc of dust considering the whole theory , we find that of course the metric will depend on contributions of all components of $t$ but only have a meaningful effect if associated characteristic velocities are approaching the speed of light . a prominent analytic solution is that of a rigidly rotating disc of dust . taking this solution , you can get an idea of how relativistic effects are important for the theory calculating the multipole moments $q_n$ with respect to some relativistic parameter $\mu$ ( corresponding also to the angular frequency $\omega$ of the disc ) . in the following picture you can see that the kerr-spacetime is approached ( from above ! ) for all moments $q_n ( \mu ) $ for $\mu \rightarrow \infty$ . this means that there is some $\mu$ where the effects of rotation dominate those of the mass itself . ( picture taken from here . ) so , to conclude , there will only be some measurable time change for a person living on a rotating planet if it is extremely fastly rotating . it is hence needless to say that this person would have some other difficulties than to measure this deviation from an almost flat metric . sincerely
if two capacitors are in series the charge on them must be the same . this is because there is no source or sink for charge in between the two capacitors : that means $q_1 = q_2$ . you know $q_2$ so you now know $q_1$ and you can calculate the voltages $v_1$ and $v_2$ and the total voltage across both , $v_{12}$ . because $c_3$ is parallel with $c_1 + c_2$ you know $v_3 = v_1 + v_2$ and from this you can calculate $q_3$ . finally , if you calculate the combined capacitance of $c_1$ , $c_2$ and $c_3$ and you know the voltage across this combined capacitor $v_{123}$ you can calculate a combined charge $q_{123}$ , and because $c_4$ in series $q_4 = q_{123}$ .
let us for simplicity work in 1d with $\hbar=1$ . ( the generalization to higher dimensions is straightforward . ) moreover , let us for simplicity take an operator $\hat{f} ( \hat{x} , \hat{p} ) $ without any ordering ambiguities , i.e. , each monomial term in the symbol $f ( x , p ) $ depends only on either $x$ or $p$ , but not on both . then one possible motivation of wigner 's phase space distribution $$ \tag{1} w ( x , p ) ~:=~ \int_{\mathbb{r}}\ ! {dy\over2\pi}e^{ipy}\psi^{*} ( x+\frac{y}{2} ) \psi ( x-\frac{y}{2} ) . $$ goes as follows . the expectation value of the operator $\hat{f} ( \hat{x} , \hat{p} ) $ in the schrödinger position representation $$\tag{2} \hat{x} ~\longrightarrow~x , \qquad \hat{p} ~\longrightarrow~ -i \frac{\partial}{\partial x} , $$ reads $$\langle\psi| \hat{f} ( \hat{x} , \hat{p} ) |\psi \rangle ~\stackrel{ ( 2 ) }{=}~ \int_{\mathbb{r}} \ ! dx~ \psi^{*} ( x ) f\left ( x , -i \frac{\partial}{\partial x}\right ) \psi ( x ) \qquad\qquad $$ $$ ~\stackrel{\begin{matrix}\text{substitute}\\ x=x^{\pm}\end{matrix}}{=}~ \int_{\mathbb{r}^{2}}\ ! dx^{+}dx^{-}~ \delta ( x^{+}-x^{-} ) \psi^{*} ( x^{+} ) f\left ( \frac{x^{+}+x^{-}}{2} , -i \frac{\partial}{\partial x^{-}}\right ) \psi ( x^{-} ) $$ $$ ~\stackrel{\delta\text{-fct}}{=}~ \int_{\mathbb{r}^{3}}\ ! {dx^{+}dx^{-}dp\over2\pi}~ e^{ip ( x^{+}-x^{-} ) }\psi^{*} ( x^{+} ) f\left ( \frac{x^{+}+x^{-}}{2} , -i \frac{\partial}{\partial x^{-}}\right ) \psi ( x^{-} ) $$ $$ ~\stackrel{\text{int . by part}}{=}~\int_{\mathbb{r}^{3}}\ ! {dx^{+}dx^{-}dp\over2\pi}~ e^{ip ( x^{+}-x^{-} ) }\psi^{*} ( x^{+} ) f\left ( \frac{x^{+}+x^{-}}{2} , p\right ) \psi ( x^{-} ) $$ $$ ~\stackrel{\begin{matrix}\text{substitute}\\ x^{\pm} = x\pm \frac{y}{2}\end{matrix}}{=}~\int_{\mathbb{r}^{3}}\ ! {dxdydp\over2\pi}~ e^{ipy}\psi^{*} ( x+\frac{y}{2} ) f ( x , p ) \psi ( x-\frac{y}{2} ) $$ $$ \tag{3} ~\stackrel{ ( 1 ) }{=}~\int_{\mathbb{r}^{2}}\ ! dxdp~w ( x , p ) f ( x , p ) . $$ that is the motivation ! the above manipulations make sense for a sufficiently well-behaved function $f ( x , p ) $ . for more general operators $\hat{f} ( \hat{x} , \hat{p} ) $ , we leave it for op to show that if $f ( x , p ) $ is interpreted as the weyl-symbol of the operator $\hat{f} ( \hat{x} , \hat{p} ) $ , then the equation $$\tag{3'} \langle\psi| \hat{f} ( \hat{x} , \hat{p} ) |\psi\rangle ~=~\int_{\mathbb{r}^{2}}\ ! dxdp~w ( x , p ) f ( x , p ) $$ continues to hold [ at least for a sufficiently well-behaved function $f ( x , p ) $ ] . one important virtue from a physics perspective of weyl-ordering ( as opposed to other ordering prescriptions ) is that the operator $\hat{f} ( \hat{x} , \hat{p} ) $ formally becomes hermitian for real functions $f:\mathbb{r}^2 \to\mathbb{r}$ and two hermitian operators $\hat{x}$ and $\hat{p}$ . recall that hermitian operators correspond to physical observables in quantum mechanics . op might also find this phys . se post useful .
i ) firstly , we are talking about the direct or cartesian product $su ( 2 ) \times su ( 2 ) $ of groups , not the tensor product$^1$ $su ( 2 ) \otimes su ( 2 ) $ of groups . ii ) secondly , $su ( 2 ) \times su ( 2 ) $ is not isomorphic to the lorentz group $so ( 3,1 ) $ but rather to a compact cousin $$ [ su ( 2 ) \times su ( 2 ) ] /\mathbb{z}_2~\cong~ so ( 4 ) . $$ in particular , a $ ( \frac{1}{2} , \frac{1}{2} ) $ irrep under $su ( 2 ) \oplus su ( 2 ) $ corresponds to a 4-dimensional fundamental vector representation under $o ( 4 ) $ . iii ) thirdly , op might be thinking of the complexified lorentz group $so ( 3,1 ; \mathbb{c} ) $ , which has double cover $sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) $ , $$ [ sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) ] /\mathbb{z}_2~\cong~ so ( 3,1 ; \mathbb{c} ) . $$ cf . this phys . se post . in particular , a $ ( \frac{1}{2} , \frac{1}{2} ) $ irrep under $sl ( 2 , \mathbb{c} ) \oplus sl ( 2 , \mathbb{c} ) $ corresponds to a 4-dimensional fundamental vector representation under $o ( 3,1 ; \mathbb{c} ) $ . -- $^1$ note that there exist various abelian and non-abelian tensor product constructions for groups . e.g. for the abelian group $ ( \mathbb{r}^n , + ) $ , the tensor product is $\mathbb{r}^n\otimes\mathbb{r}^m\cong \mathbb{r}^{nm}$ , while the cartesian product is $\mathbb{r}^n\times\mathbb{r}^m\cong \mathbb{r}^{n+m}$ .
the duel gets tenser and tenser . $\epsilon^{xyab}\partial_a\partial_b= ( -\epsilon^{xyba} ) \partial_a\partial_b= ( -\epsilon^{xyba} ) \partial_b\partial_a= ( -\epsilon^{xycd} ) \partial_c\partial_d=-\epsilon^{xyab}\partial_a\partial_b$ $\longrightarrow\ \ \epsilon^{xyab}\partial_a\partial_b=0$ more abstractly , if $a^{ab}=-a^{ba}$ and $s_{ab}=s_{ba}$ , then $a^{ab}s_{ab}=0$ .
from your comment , it seems you want to know how to determine the direction of the electric field of a continuous charge distribution at points not in the charge distribution when you are applying the integral form of gauss 's law to obtain the electric field . the procedure commonly used to obtain the electric field using gauss 's law for continuous charge distributions relies on the fact that the distribution possesses some symmetry . in particular , the procedure is commonly applied to distributions that are either spherically symmetric , cylindrically symmetric , or possess 2-dimensional planar symmetry . in each of these cases , it is possible to determine the direction of the electric field before attempting to apply the integral form of gauss 's law to determine the magnitude of the field . in fact , there is a theorem on can prove which shows that if the charge density possess a certain symmetry , then the electric field must also respect that symmetry . it follows , for example , that in the case of spherically symmetry , the electric field must point radially because only radially pointing vector fields are invariant under all rotations just as the charge density is . similar statements hold for charge distributions enjoying other symmetries .
your question is interesting , and gets specifically to the kinds of questions that quantum mechanics was intended to answer in the first place . it helps to understand the motivation behind the original bohr model of the atom , and how that led to qm in the first place . the problem bohr was trying to address can be paraphrased as , " if an electron orbits a nucleus like a planet , why does not it gradually lose energy and spiral into the nucleus ? " the answer came when bohr realized that the orbital momentum was quantized , effectively meaning that since the electron had mass then by the relationship $p=mv$ , the velocity was also quantized ( note : this simple expression is more complicated when relativity is included , but the discussion can continue without including it ) . these quantized values of momentum/velocity are what one would call eigenvalues , or observables in quantum mechanics . since the electron can only change orbits by given off specific quantities of energy instead of giving off energy continuously , it remained in a stable orbit relative to the nucleus , thus preventing it from spiraling in . what is important to understand is the idea of the potential well . in the bohr model , the electrons closest to the nucleus have higher velocity than the electrons further away . in other words , they have greater kinetic energy ( $k . e . = \frac{1}{2} mass \times velocity^2$ ) , but it had less potential energy since it was closer to the nucleus ( obeying the relationship $p . e . = mass \times distance \times gravity$ ) . however , the total energy ( $k . e . + p.e. $ ) associated with orbits closer to the nucleus is less than those further away . so in order for the electron to move closer to the nucleus , energy must be given up . this is accomplished by the emission of a photon . alternatively , if one wants to cause an electron to move into a more distant orbit , then one must add energy through use of a photon . it is in contemplating how to determine the orbit of the electron that the uncertainty principle first became apparent . the only probe that we have available to determine the position of an electron in its orbit is a photon , and the photon must be of sufficient energy in order for it to be small enough to give a meaningful result , however if we use a photon small enough ( in terms of wavelength ) , it will have enough energy to shift the electron into a different orbit , and then we would have to start the process all over again . a free electron has sufficient energy to escape the nucleus . in other words , it has acquired sufficient energy to fill its potential energy deficit . if there were only one nucleus in the universe , the potential deficit would only be eliminated when the electron was at infinite distance from the nucleus . in that situation , the implication is that the electron would also have zero velocity . this situation is obviously unrealistic , first there are more than one nuclei in the universe , and second , to verify that a particle has zero velocity at infinite distance is clearly an impossible task . if we move past the bohr model and into more modern quantum mechanics , the question then is whether there are eigenstates that have eigenvalues for momentum of a particle that are equal to zero ? it is important to review some basic facts about matrix operations and linear algebra if 0 is an eigenvalue of a matrix a , then the equation a x = λ x = 0 x = 0 must have nonzero solutions , which are the eigenvectors associated with λ = 0 . but if a is square and a x = 0 has nonzero solutions , then a must be singular , that is , det a must be 0 . this observation establishes the following fact : zero is an eigenvalue of a matrix if and only if the matrix is singular . this means that the matrix in question is not of full rank . in qft this has a very specific interpretation . the annihilation operator has the power to destroy the vacuum state and map it to zero . this situation is understood to be associated with the free field vacuum state with no particles . this state is necessary because it allows us to find vacuum state solutions for the associated quantum mechanical system . by means of analogy , we can see that the solution to ground state problem is the solution to the homogeneous part of a differential equation . the schrodinger equation is a linear , homogeneous equation which governs evolution of the wave function of a particle . the solutions of the schrodinger equation can be used to understand particle motion . the exact position and momentum of a particle can only be known if h ( planck 's constant ) approaches zero , however , in quantum mechanics , planck 's constant is fundamental to the theory , so this cannot occur in the single particle case . because of this , momentum and position uncertainty establish an inverse relation to each other , and if the uncertainty of momentum is zero , then the uncertainty in position is infinite . for these reason 's it is not possible to talk about a particle " stopping " or being " stopped " in any meaningful or non- contrived sense .
[ a black hole ] has the same mass of the star that gave it origin no , not really . a stellar-mass black hole is formed after a star with mass around 20 times that of our sun collapses due to lack of core fusion and by some ( as of yet ) unknown process , rebounds and explodes . this explosion , a type ii supernova , kicks off something like 90% of its matter . thus , the black hole would be something around the 5 solar mass range . is there a way to calculate the mass of a black hole well you can not actually observe black holes in free space ( because you can not see them ) , you need to infer it by using something around it ( either a star or gas cloud ) and computing the mass from kepler 's laws .
you have many comments to the effect that " topology is needed to describe continuity , calculus concepts , the notion of " looks like " , homeomorphism and so forth " . and these are all altogether right , but i am getting that your question is about the global picture . also , the following is mainly about a toplological or differentiable manifold ; joshphysics 's link shows that there are many other concepts of manifold . we begin with the notion of " locally looks like $\mathbb{r}^n$" ; but you can have a set $\mathbb{m}$ of any kind of weird creatures whose subsets you can put into one-to-one , surjective correspondence with some open ( more about this below ) subset of $\mathbb{r}$ ( wontedly a simply connected neighbourhood of the origin ) . for one of these subsets $\mathcal{n}$ you have a " labeller " map $\lambda:\mathcal{n}\to\mathbb{r}^n$ . then you notions of open , neighbourhood and all the rest of it arise by definition : a subset $\mathcal{o}\subset\mathcal{n}$ is open iff $\lambda ( \mathcal{o} ) $ is open in $\mathbb{r}^n$ . likewise a " path " $\sigma:\mathbb{r}\to\mathcal{n}$ is $c^0 , \ , c^1\ , c^\omega$ or whatever iff $\lambda\circ \sigma:\mathbb{r}\to\mathbb{r}^n$ has the same property . all topology , neighbourhood , calculus , differentiability and so forth concepts are then defined by " fiat " , and the need for the concepts is why we want our zoo of weird creatures to " locally look like $\mathbb{r}^n$" in the first place so this is all highly intuitive and obvious . so i am guessing ( also by reading your other probing questions on this site ) that you already understand all this . so the crucial question is then that of transistion maps and how we glue all our local copies of $\mathbb{r}^n$ together . going back to our subset $\mathcal{n}\subset\mathbb{m}$:there are other " local copies " of $\mathbb{r}^n$ that bestow our topological / calculus and so forth concepts on subsets of $\mathbb{m}$ other than $\mathcal{n}$ . but these subsets must overlap , because , when we are doing calculus or toplogy or dynamics or whatever , we do not want suddenly to run into a " co-ordinate wall " and have to jump suddenly from one co-ordinate system to another . as an example , suppose we have a spacecraft in an einstein manifold ( universe that is a vacuum solution to the efes ) . for calculus , measurement and other mathematical concepts , we need always to be able to define the manifold in a neighbourhood around the spaceship : so , as the spaceship nears the boundary of one co-ordinate system , it must also be describable by another co-ordinate system wherein we can carve out a " neighbourhood": we could not do this if our co-ordinate systems did not overlap but instead partitioned the manifold $\mathbb{m}$ . otherwise put , in relativity , the boundary between co-ordinate systems is an artifact of our particular mathematical description of the physics , it does not belong to the physics . another , dramatic , example is the phenomenon of gimbal lock in euler angle charts for the unit sphere that very nighly cost the apollo 11 astronauts their lives , cost many pilots their lives in the years before then and is the reason why the software processing signals from fibre ring sagnac gyros that keep you safe in a commercial jetliner either manipulate the aeroplane 's calculated orientation in two overlapping charts covering $so ( 3 ) $ or , more recently , model the aeroplane 's orientation by unit quaternions in $so ( 3 ) $ 's double cover $su ( 2 ) $ . so , our overlap is very much needed , so many , if not all , regions in the manifold can be described by more than one local copy of $\mathbb{r}^n$ with more than one labeller . so , suppose we have two regions $\mathcal{n}_1 , \ , \mathcal{n}_2$ with labellers $\lambda_1:\mathcal{n}_1\to\mathbb{r}^n$ , $\lambda_2:\mathcal{n}_1\to\mathbb{r}^n$: we must make sure that these labellers yield consistent notions of opennes , neighbourhood , differentiability and all the rest of it in a region $\mathcal{n}_1\cap\mathcal{n}_2$ . so , a set $\mathcal{o}\subseteq\mathcal{n}_1\cap\mathcal{n}_2$ must be open as reckonned by labeller $\lambda_1$ and $\lambda_2$ and so $\lambda_1\circ\lambda_2^{-1}$ and $\lambda_2\circ\lambda_1^{-1}$ , the " transition maps " between charts , must be local homeomorphisms , analytic , diffeomorphisms , or whatever the relevant notion is for the kind of manifold in question . likewise for all other calculus and topological concepts we wish to speak of . this is most readily achieved if the charts ( ranges of the labellers $\lambda_j$ ) are open , and their intersections are open as reckonned by all local copies of $\mathbb{r}^n$ that are applicable to the overlap . so we have two axioms for manifolds further to the obvious one that every point in the manifold must belong to the preimage of at least one labeller : an intersection between two " patches " ( domains of labellers ) must be open in the topology as reckonned by each of the two labellers for the overlapping charts ; the transition maps must be local homeomorphisms , diffeomorphisms , . . . . some authors also add the axiom that the manifold should be hausdorff ( $t_2$ ) in each chart but in many fields , notably lie groups , $t_2$ is enforced by other structure ( the group laws ) so this axiom is redundant here . the easiest way to do this is to kit the manifold globally with a topology whose base is the open sets as reckonned by their images under the labellers , or , written backwards , the base for the topology is the collection of all preimages of sets open in $\mathbb{r}^n$ under the labellers . hopefully you can see that the notion of consistency as reckoned by overlapping charts , and thus the notion of the global manifold topology , is very much bound up in the physical concept of covariance and the copernican notion that nature 's behaviour cannot depend on our mere description of it .
firtree is correct - i will just try to flesh out his answer a bit . ( 1 ) your last question first - charge ( or current ) at a point is like mass at a point . for finite masses , if you want to see how much is contained in an infinitely small volume ( i.e. . , at a point ) , the answer is zero . so instead , people consider the mass density which can have non-zero values at a point . you probably understand the relationship between mass and ( mass ) density quite well . similarly for a finite current , the amount of current at a point ( i.e. . , in an infinitely small volume ) is zero . the current density is the limit of the amount of current in a small volume around a point as the volume goes to zero - just like mass density , but with current instead . so , just as one speaks of mass density at a point and not mass at a point ( for extended bodies ) , one speaks of charge density at a point and not charge at a point or current density at a point and not current at a point ( we are ignoring point particles for now - they do fit into this formalism , but you need dirac delta functions ) . ( 2 ) now , in analogy with mass flow , your picture of flow of charge is correct . mass density times velocity gives a mass current density . $\vec{j}_{m} ( t , \vec{x} ) :=\rho_{m} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a mass current density $\vec{j}_{m}$ and want to know the mass flow $\dot{m}$ through some area a , then you take \begin{equation} \dot{m} = \int \vec{j}_{m} \cdot d\vec{a} \end{equation} similarly , charge density times velocity gives a charge current density . $\vec{j}_{q} ( t , \vec{x} ) :=\rho_{q} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a charge current density $\vec{j}_{q}$ and you want to know the flow of charge $\dot{q}$ through some area a , then you take \begin{equation} \dot{q} = \int \vec{j}_{q} \cdot d\vec{a} \end{equation} so , the picture in your head is quite close - just picture charge or a fluid of charged particles flowing .
nothing can travel faster than light yet the expansion of the universe itself can . there are already parts of the universe receding from us " faster than light " . this is because every small part of the universe is expanding only a little - but if you sum it up over cosmic distances , the expansion speed , total over a large distance , becomes huge . so , in a sense , nothing actually moves faster than light . it is just that the universe is so big that this tiny space expansion actually adds up a lot over large distances . space itself is growing , and it is not subject to relativistic limitations - only things moving through space are limited by c . back to your original question : there was a time , early on , when the universe expanded much faster than today . it went from essentially nothing to a huge size in almost no time . things that were previously close together , all of a sudden found themselves separate by large distances . so , those photons emitted 12.9 billion years ago , might have arrived here a very long time ago , if it were not for this sudden " additional " distance in between . btw , this is not entirely rigorous , but it is good enough as a pop-sci explanation , and at least will get you started .
the article that describes this research does discuss their favored model for explaining these features . the article starts off by talking about earth analogues , which are always helpful . in this case , it talks about brines rising in antarctic ice shelves ( brines being just what they are in cooking -- salty water , and salt lowers the freezing point of water by up to and over 10°c ) . the model the authors suggest starts with a thermal plume that heads from the liquid interior up through the ice and melts some of the overlying ice . the melting causes the surface to collapse a bit and fracture ( volume of water is &lt ; volume of ice ) , and this confines the water to that region . the thermal plume was transient/temporary and sinks back down , and the water at the base of the " lake " re-freezes . the article then talks a lot about the morphology ( what it looks like ) in images and theory , showing that the two intersect . as the lens of water lies below the surface , the surface fractures and calves , like glaciers calving on earth . material fills in with an impurity-rich matrix and freezes . when the lake underneath refreezes , since the volume of the ice is greater than water , it creates a positive convex topographic relief ( jumbled because of the previously calved blocks ) as opposed to the concave one when there was liquid below . from what i can tell , the article is not actually suggesting that all these areas presently contain liquid water lakes underneath them . they do suggest one particular area , thera macula , to be presently active , however . their argument for this is based on its morphology : " the large concentric fracture system encircling thera macula resembles those of collapsing ice cauldrons , and , given the absence of a continuous moat , suggests that subsurface melt and ice disaggregation is forming thera macula , rather than the collapse of a dome . " they suggest , " today , a melt lens of 20,000–60,000 km 3 of liquid water probably lies below thera macula ; this equates to at least the estimated combined volume of the great lakes . " the timescale for this amount of liquid to re-freeze is 100,000-1,000,000 years . the type of terrain that this creates ( "chaos terrain" ) is spread throughout the moon , so if their model is correct , then it has had many intra-ice lakes throughout its history , and likely throughout its recent history . the authors conclude with , " our analyses suggest that ice–water dynamics are active today on europa , sustaining large liquid lakes perched in the shallow subsurface . "
i think the easiest way to do this is to avoid solving differential equations to the greatest extent possible . there is , in fact , a way to use ladder operators and only requires you to solve one , fairly easy differential equation ; first , we note that the ladder operator technique can be used to derive the entire spectrum of one-dimensional harmonic oscillator . $$ e_n = ( n+\tfrac{1}{2} ) \hbar\omega $$ the technique can also be used to show that the corresponding , properly normalized eigenvectors satisfy the following properties $$ a^\dagger|n\rangle = \sqrt{n+1}|n+1\rangle , \qquad a|0\rangle = 0 $$ the left-hand property shows that , once one has one of the eigenstates , every other eigenstate corresponding to a higher eigenvalue can be obtained by applying the raising operator . in particular , if one knows the position basis representation of the ground state , then one can obtain the position basis representation of every other eigenstate by applying the position basis representation of the raising operator . the right-hand property shows that the ground state is annihilated by the lowering operator . writing this condition in the position basis , one obtains a simple differential equation for the ground state wavefunction , and then , per the left-hand property , one generates all other wavefunctions .
different boundary conditions represent different models of cooling . the first one states that you have a constant temperature at the boundary . this can be considered as a model of an ideal cooler in a good contact having infinitely large thermal conductivity the second one states that we have a constant heat flux at the boundary . if the flux is equal zero , the boundary conditions describe the ideal heat insulator with the heat diffusion . robin boundary conditions are the mathematical formulation of the newton 's law of cooling where the heat transfer coefficient $\alpha$ is utilized . the heat transfer coefficient is determined by details of the interface structure ( sharpness , geometry ) between two media . this law describes quite well the boundary between metals and gas and is good for the convective heat transfer . http://www.ugrad.math.ubc.ca/coursedoc/math100/notes/diffeqs/cool.html the last one reflects the stefan-boltzman law and is good for describing the heat transfer due to radiation in vacuum
if i understand correctly , then i believe you’re right . we are considering a situation where the drill pipe in the ground gets stuck , but where all the rest of the equipment is functioning properly , right ? in particular , we are assuming that the blocks are not jammed or anything . the proper way to define mechanical advantage $m$ is as the ratio between the distance $d$ traveled by the driving force and the distance $l$ traveled by the load . then $d$ is just the length of cable that passes through the driving winch at the base during a certain amount time , and $l$ is the height that the drill pipe moves during the same time . this ratio is equal to the inverse of the ratio of the tension $t_d$ on the cable at the winch and the tension $t_l$ on the cable attached to the drill pipe : $m = \frac{d}{l} = \frac{t_l}{t_d}$ this ensures that the amount of work done by the winch , $t_d d$ , is equal to the work done on the pipe , $t_l l$ . if the drill pipe gets stuck , then it will require a much larger tension to pull it from the ground , right ? this means $t_l$ will shoot up . but the mechanical advantage $m$ is the same , so the tension $t_d$ at the winch will likewise shoot up . this doesn’t mean the mechanical advantage is lost . to summarize : the ratio between $d$ and $l$ is not affected by the pipe jamming ; it’s just that it becomes more difficult to get them to move at all . therefore , the mechanical advantage $m=d/l$ doesn’t change i’m not sure that this is feasible , but you could confirm this by measuring the tensions at the winch and at the pipe individually .
there is a much better description here of fizeau 's nineteenth century experiment . some of the key features that enabled fizeau to succeed : a lens to collect the light from the source a collimating lens to prevent the light diverging during its journey a large diameter beam to minimise broadening of the beam by diffraction more lenses to focus the light on the detector the light went directly into a very sensitive detector : the human eye . almost certainly he did this experiment at night .
yes , and it would be very cold . the paper " finite temperature in a desitter universe " explains that the cosmological constant ( if it is really a constant ) creates a " horizon " that acts somewhat like an inside-out event horizon : objects that get too far away from you are unreachable . this horizon will radiate hawking radiation at an extremely low temperature of 10^ ( -30 ) k ; the wavelength range of light this corresponds too is the same order of magnitude as the horizon 's radius ( also the case for black holes ) . it is conceivable that some relatively compact system could have an excited quantum state so close to it is ground state that it is thermally accessible even at this extremely cold temperature . thus , there is still some form of heat swimming around . however , there is no reservoir colder than this temperature to dump heat into so this heat can not be converted into useful energy .
as explained by iwo bialynicki-birula in the paper quoted , the maxwell equations are relativistic equations for a single photon , fully analogous to the dirac equations for a single electron . by restricting to the positive energy solutions , one gets in both cases an irreducible unitary representation of the full poincare group , and hence the space of modes of a photon or electron in quantum electrodynamics . classical fields are expectation values of quantum fields ; but the classically relevant states are the coherent states . indeed , for a photon , one can associate to each mode a coherent state , and in this state , the expectation value of the e/m field results in the value of the field given by the mode . for more details , see my lectures http://www.mat.univie.ac.at/~neum/ms/lightslides.pdf http://www.mat.univie.ac.at/~neum/ms/optslides.pdf and chapter b2: photons and electrons of my theoretical physics faq .
without doing the analysis , i would think that a cooling system is more effective at extracting heat from a warm container than from a cold one . for a fridge , the effectiveness ( or coefficient of performance ) is $eff=q_c/w$ is the ratio of the heat removed from the cold source ( the fridge ) to the energy used for the purpose . it increases with the temperature of the cold source . this is actually the prime factor to be considered in the analysis . if we assume that , for a given current temperature of its contents , and thus for a given coefficient of performance , the cooling capacity of the fridge ( heat removed per second ) is limited only by the power of its cooling engine ( i do not know whether that is the case ) , then the heat pump will pump more heat per second when the fridge is warm . hence it is better to put all bottles at once and get the fridge warmer to have a maximum heat pumpimg capacity from the heat pump . heat sharing rate within the fridge may also be an important issue , but there are no data available to measure how important . if it is really low , thus leaving an important temperature gradient in the fridge , it may be useful to exchange the position of bottles , so as to have the warmer part of the load near the heat pump and have work with highest possible coefficient of performance . precise figures about the load do not matter very much . however , a load with large heat capacity will take longer to cool and will thus allow more time for heat sharing . in the second part below , we prove formally that all bottles should be cooled at once , and we use the understanding to discuss the heat sharing issue in some more depth . the variability of the coefficient of performance with temperature is central to this analysis . formal statement and proof a refrigerator is a carnot machine functionning as a heat exchanger , where we are interested in removing heat from the low-temperature reservoir , using work from an engine that provides compression . the effectiveness , or coefficient of performance , noted here $z$ , is defined as $z=q_c/w$ where $w$ is the work provided and $q_c$ is the amount of heat extracted from the cold source ( the refrigerator ) with that work . if we note $q_h$ the amount of heat delivered to the hot source ( outside the refrigerator ) , we have the equality $q_h=w+q_c$ . for an ideal carnot cycle , we have $z_{ideal}=q_c/w=q_c/ ( q_h-q_c ) =t_c/ ( t_h-t_c ) $ where $t_h$ and $t_c$ are the temperatures of the hot and cold source . ( see http://en.wikipedia.org/wiki/coefficient_of_performance ) . of course , the actual coefficient of performance $z$ is less that the carnot ideal . short of knowing its specific , we will only assume that , like the ideal coefficient , it depends monotonically on the temperature $t_c$ of the cold source , the hot source ( outside the refrigerator ) being considered at constant temperature . hence we only assume that the coefficient of performance $z$ is a strictly increasing function of ( cold source ) temperature , i.e. , such that $t_1&lt ; t_2\ \rightarrow\ z ( t_1 ) &lt ; z ( t_2 ) $ we also assume that the mechanical power available for compression is invariant , i.e. does not depend on the temperature of the sources , at least within the range of temperatures considered . finally , we also assume that the heat capacity of the refrigerator itself is negligible , and that heat sharing within the refrigerator takes negligible time compared to cooling time so that the content may be considered to have uniform temperature . these later two assumptions will be discussed afterwards . with the above assumptions , given two masses $m_1$ and $m_2$ to be cooled in the cold source , it is faster to cool booth simultaneously than to try to cool one first and later add the second one . it also consumes less energy . proof cooling a mass $m$ actually , the formulae above are about heat and work increments . this is necessary since $z$ is temperature dependant , and temperature may vary . also , since we intend to analyze the system from the point of view of the cold source , the heat increments are actually removed from that source and must be countd as negative . so we can write $z= -dq_c/dw$ , or $dq_c/dw=-z$ . the power provided by the compressor is a constant $p=dw/dt$ . hence $dq_c/dt= ( dq_c/dw ) \times ( dw/dt ) =-z\times p$ . on the other hand we know that removing heat reduces the temperature according to the formula $\delta q=-cm \delta t$ , where $m$ is the mass being colled and $c$ is the specific heat for the substance constituting that mass . hence we have $dq_c/dt= cm ( dt_c/dt ) $ . combining the two formulae , we get $dt_c/dt=-zp/cm$ . but we cannot resolve this equation since $z$ is an unknown function of $t_c$ . what we know is that $z ( t_c ) $ , $p$ , $c$ and $m$ are strictly positive values . so $dt_c/dt$ is strictly negative . hence $t_c$ will decrease with time . since the function $z ( t_c ) $ is a strictly increasing function , its value will also decrease with time , and hence the absolute value of the derivative $dt_c/dt$ will also decrease with time . hence the graphical representation of the evolution of the temperature will look like the red curve in the figure , where $t_0$ is the initial temperature at time $t_0$ . cooling the same mass $m$ in two steps if we consider cooling independently ( in an identical refrigerator ) another mass $m_1$ , smaller than $m$ , with initial temperature $t_0$ we get another curve , like the curve in blue in the left part of the figure up to point c , corresponding to the equation $dt_c/dt=-zp/cm_1$ . it is below the red curve because the smaller mass $m_1$ cools faster than $m$ . formally , if we draw a horizontal line like the line cutting both curves in a and b , this corresponds to the same temperature for both curves , hence to a common value of the coefficient of performance $z$ . then $m_1&lt ; m\ \rightarrow\ ( dt_c/dt ) _a&lt ; ( dt_c/dt ) _b$ . since this is true for any value of the temperature $t_c$ , it confirms that the blue curve for $m_1$ decreases faster than the red curve for $m$ . suppose now that at time $t_2$ the mass $m_1$ has been cooled to temperature $t_2$ corresponding to point c below the red curve . we add to $m_1$ another mass $m_2$ such that $m=m_1+m_2$ , the mass $m_2$ being at the initial temperature $t_0$ . the mass $m_2$ being warmer than $m_1$ will share its heat with $m_1$ ( in negligible time according to our hypothesis ) so that both reach the temperature $t_1$ corresponding to point d , and pursue cooling . at any time between $t_0$ and $t_2$ , the temperature of $m_1$ ( blue ) is less than the temperature of $m$ ( red ) . hence the refrigerator works with a lower coefficient of performance $z$ for $m_1$ than for $m$ , and less heat has been removed from the refrigerator containing $m_1$ than from the refrigerator containing $m$ at time $t_2$ . when we introduce the mass $m_2$ with $m_1$ , the total heat introduced in the refrigerator is that of $m_1+m_2$ at temperature $t_0$ . this is exactly the same as the heat introduced in the refrigerator containing $m$ . since less heat was removed from the $m_1+m_2$ refrigerator at time $t_2$ , it is at a higher temperature that the $m$ refrigerator . hence the point d is above the red curve . the $m_1+m_2$ refrigerator now contains the same mass as the $m$ refrigerator . hence it will follow an identical curve . but it is at temperature $t_1$ that was attained earlier , at time $t_1$ by the $m$ refrigerator . so the right part of the blue cooling curve for $m_1+m_2$ , starting at point d is the same as the right part of the red coling curve for $m$ starting at point b , translated by a duration $\delta t=t_2-t_1$ . conclusion the masses $m_1+m_2$ will always reach any temperature with a delay $\delta t$ after the mass $m$ has reached it . actual figures would be required to be more precise . given the problem , the cooling engine will be working at maximum power to get the fastest possible cooling in both cases . then it is obvious that the faster solution is also the most economical energetically . this assumes either that the cooling is started just at the right time , or that the cooling power is reduced once the right temperature has been attained . these results are based exclusively on our assumptions , independently of any actual figures . we will now discuss some of these assumptions . discussion heat capacity of the refrigerator we have assumed that the heat capacity of the refrigerator itself is negligible . we should however analyse its effect . we first note that the objective is to extract heat from a given number of bootles to bring them from $t_0=30^{\circ}c$ to $t_f=6^{\circ}c$ . that corresponds to a precise amount $q$ of heat to be removed , independently of the process used for that purpose . if the refrigerator itself is initially at temperature $t_f$ , it will at the start share heat with the mass to be cooled , thus warming up and cooling the bootles . but then it will have to be cooled down to $t_f$ again , so that it net contribution to the cooling process will be null , and the same amount $q$ of heat has to be removed by the heat pump . however , by sharing heat at the beginning , it induces an early cooling , thus making the whole heat removal process operate at a lower temperature , hence with a lower coefficient of performance $z$ . the net effect of the heat capacity of the refrigerator is thus to provide some early cooling , which can sometimes be considered an advantage , but at the expense of a lower effective coefficient of performance $z$ . these effects increase with the heat capacity of the refrigerator . note that if the initial temperature of the refrigerator is below the targeted final temperature $ t_f$ , the difference multiplied by the heat capacity is a net contribution to the refrigeration process , though the loss on the coefficient of performance remains . hence it is better not to have anything else in the refrigerator , even already cooled , unless it is cooled to a much lower temperature than the final temperature $ t_f$ intended for the bottles . heat sharing rate as we have seen from the previous discussion , the main objective is to remove a given amount $q$ of heat , and the effectiveness of the removal decreases as temperature is lowered . if the rate of heat sharing within the refrigerator is small , the volume near the cooling system will cool faster , thus reducing the coefficient of performance , i.e. the rate of heat removal . hence , ensuring the best possible heat sharing can help in all circumstances . it should be noted that direct sharing between cylindrical bottles will be reduced to a minimum : just one line of contact . so , if space allows , it is probably preferable to help air circulate between the bottles . keeping the bottles in vertical position will help if the refrigerator has grid shelves that let air through , rather than glass shelves . and , of course , the bottles must be unpacked . opening the door to quickly exchange bottles so that the warmer ones are placed near the cooling system will improve the coefficient of performance and reduce cooling time . it may have some cost in warming the refrigerator , but that is less important if the heat capacity of the load to be cooled is large ( actual measurements would be useful ) . exchanging bootles will also avoid having to cool those close to the cooling system below the required temperature $ t_f$ in order to have all the bottles temperature at least as low as $ t_f$ .
as a practical---rather than in principle matter---the lightest particle with internal structure would be a positronium atom with a mass of around $1 \ , \mathrm{mev}$ and a maximum excitation energy of about $6.8 \ , \mathrm{ev}$ . that thing simply can not absorb enough momentum from a photon to make it relativistic . the calculation is similar for any system bound by the electromagnetic interaction . so the next thing to do is to consider a light system bound by the strong interaction . the lightest choices are the charged pions with masses of $139 \ , \mathrm{mev}$ . the excited states of these things are rhos with masses of $770 \ , \mathrm{mev}$ . that excitation requires $ ( 770 - 139 = 631 ) \ , \mathrm{mev}$ and because it is provided by a photon there is a transfer of $631 \ , \mathrm{mev}$ ( 1 ) of momentum as well , which is an appreciable fraction of the mass of the resulting state and implies that the resulting meson will by mildly relativistic in the rest frame of the incident meson . a similar calculation can be performed for the neutral pion excited to the neutral rho or the omega-0 . note that $631 \ , \mathrm{mev}$ photons can be hard to come by . ( 1 ) strictly $\mathrm{mev/c}$ , but i am working in natural units .
first of all , the first equation for $ds^2$ is only valid if $f$ is nothing else than the azimuthal angle $\phi$ . second , if you are evaluating $x_i x^i$ , the squared distance from the origin without any infinitesimals , then it is exactly equal to $-t^2+r^2$ and nothing else . the polar coordinate $r$ is chosen as $\sqrt{x^2+y^2}$ so its square already includes two of the three terms one encounters in cartesian coordinates . there is nothing wrong if the formula for a squared distance from a particular point ( the origin ) just has two terms .
some key reading if you want to understand this stuff is chapters two to five of the iers technical note 36 , the iers conventions ( 2010 ) . it is not just the j2000/fk5 frame ( aka the eme2000 frame ) that is associated with some epoch date . every earth-centered inertial frame has some epoch date . there are two fundamental reasons why this must be the case : the earth 's rotation axis is not constant . solar system astronomers are regularly improving their best estimate of what constitutes an inertial frame . note that the j2000/fk5 frame is now twice passé . the current best estimate of what constitutes an inertial frame is the international celestial reference frame 2 ( icrf2 ) . it is predecessor , the icrf , represented a vast improvement over the j2000/fk5 frame . the icrf2 is even better than the icrf . the icrf was supposed to be co-aligned with the j2000/fk5 frame on j2000.0 ( 12 noon terrestrial time on 2000 january 1 ) . it turned out that this was not the case ; there is a slight bias between the frames at the epoch . the j2000/fk5 frame also turns out to be rotating a tiny bit , about 3 milliarcseconds/year . unless you are doing milliarcsecond astronomy , you can ignore that bias and rotation . for most applications , j2000/fk5=icrf=icrf2 . the first item , that the earth 's rotation axis is not constant , is important . the earth 's rotation axis precesses with a period of about 26,000 years . accounting for the change in the precession between the epoch time and the time of interest ( e . g . , today ) yields a transformation from the epoch frame ( e . g . j2000 ) to the mean of date frame . in addition to this long-term precession , the earth 's axis also displays some shorter term variations in where it points . these short-term variations ( from ~5.5 days to 18.6 years ) are collectively called nutation . accounting for the earth 's nutation on top of the precession yields the transformation to the true of date frame . finally , the earth rotates at about one revolution per sidereal day about this precessed and nutated axis . applying this rotation of the true of date frame yields the earth-centered , earth-fixed frame . a somewhat widely-used name for the result of this process is the earth rnp ( rotation , nutation , and precession ) matrix . well , almost . precession and nutation are semi-analytical models , as is the concept of one revolution per sidereal day . there are some things those models just can not capture . that one revolution per sidereal day is incorrect for two reasons . one is that the earth 's rotation rate is very gradually slowing down . another is that when you look at the rotation rate very closely , the earth sometimes rotates faster than nominal , other times slower . there are two key parameters that describe this , dut1=ut1−utc and δt=tt-ut1 . if you care about this detail i suggest you use the latter as it is continuous . dut1 has discontinuities at the leap seconds . this is a correction that you add when you compute the rotation part of the rnp matrix . there are some things that the semi-analytical precession and nutation model just do not cover ( yet ) . the chandler wobble , for example . these are collectively called " polar motion " , and can only be observed ( and predicted to some extent ) . polar motion needs to be applied after computing the rnp matrix . the full result is sometimes called the prnp ( polar motion , rotation , nutation , and precession ) matrix . these fine scale variations in the earth 's orientation , along with dut1 and δt , are called the " earth orientation parameters " . these are published on a regular basis as " iers bulletins a and b " . i will say more about this below . you do not want to code this on your own . you can get code to do these calculations from a number of places . the best sites are : international astronomical union ( iau ) standards of fundamental astronomy ( sofa ) the sofa code is the " official " version of all of the concepts described above . you can get both fortran and " ctran " ( fortran converted to ugly c ) versions of the sofa code from the sofa website . also be sure to check out the cookbooks , particular the cookbook on " sofa tools for earth attitude " . naval observatory vector astrometry software ( novas ) the us naval observatory is responsible for iers bulletins a and b . they have their own software , distinct from the sofa code . the novas software is available in fortran , c , and python . the difference in terms of results is negligible , in the microarcseconds . that is the kind of error one would expect from using double precision and performing the same computations a bit differently . there are a number of others ( e . g . , jpl spice , gsfc gmat , orekit ) , but i suggest you go to the source , and that would be either the iau or the us naval observatory . i mentioned iers bulletins a and b couple of times above . the international earth orientation and reference systems service ( iers ) is the worldwide organization responsible for defining things such as the icrf and for determining how the earth is oriented . ( yes , the acronym does not match . it used to before they changed the name but not the acronym . ) as far as those technical bulletins are concerned , they just contain numbers ( and a tiny bit of text ) . these numbers are time-tabulated values for the earth orientation parameters . these bulletins are updated monthly . a couple of final points : i put the link to iers technical note 36 at the top of this answer . read it . be very careful of time . there are a number of time scales involved in this modeling . a few of them that you will run into are : tai - international atomic time . time according to an earthbound physicist who uses an atomic clock at sea level . tt - terrestrial time . time according to an earthbound astronomer . physicists and astronomers disagree by 32.184 seconds . ut1 - universal time . conceptually , what a sundial says the time is , but smoothed to eliminate things such as the equation of time . utc - coordinated universal time . that is what the clock on your computer shows if you are using network time protocol . next time we have a leap second ( probably the end of next year ) , you will be able to see a minute with 61 seconds in it . utc ticks at the same rate as tai and tt but occasionally has leap seconds so as to stay within a second of ut1 . tcb - barycentric coordinate time . a general relativistic timescale that on average ticks faster than clocks on the surface of the earth . tdb - barycentric dynamical time . a general relativistic timescale that on average ticks at the same rate as clocks on the surface of the earth . gast - greenwich apparent sidereal time . if you run into this time scale you are looking at an out-of-date concept for calculating earth rotation . use the newer earth rotation angle concept , which relies on ut1 . update i did not answer the title of the question , how to determine satellite position in j2000 from latitude , longitude and distance from earth ? this is the easy part . the only tricky aspect is that latitude is almost always geodetic latitude rather than geocentric latitude . i will assume that latitude $\phi$ , longitude $\lambda$ , and altitude $h$ are in the wgs84 reference system . see [ department of defense ( 2000 ) , " world geodetic system 1984: its definition and relationships with local geodetic systems " nima tr8350.2 ] ( http://earth-info.nga.mil/gandg/publications/tr8350.2/wgs84fin.pdf ) equations 4-14 and 4-15 in the above reference describe the transformation from latitude $\phi$ , longitude $\lambda$ , and altitude $h$ are in the wgs84 reference system to cartesian earth-centered , earth-fixed ( ecef ) coordinates . the equations below use two key parameters that describe the shape of the earth ( see tables 3.1 and 3.3 of the above reference ) : $$ \begin{aligned} a and = 6378137\ \text{m} and and \text{earth equatorial radius} \\ e^2 and = 6.69437999014\times10^{-3} and and \text{square of earth eccentricity} \end{aligned} $$ first you need to compute the " radius of curvature in the prime vertical " ( equation 4-15 in the reference ) : $$n = \frac a {\sqrt{1-e^2\sin^2\phi}}$$ then simply compute the ecef coordinates via equations 4-14: $$ \begin{aligned} x and = ( n+h ) \cos\phi \cos\lambda \\ y and = ( n+h ) \cos\phi \sin\lambda \\ z and = ( ( 1-e^2 ) n+h ) \sin\phi \end{aligned} $$
i am going to offer a completely different way of doing this . sometimes it is nice to work in symbols before getting into the very specific numbers . i take my inspiration from the bohr atomic model here . the total energy of the electron in orbit around the nucleus at any given radius is calculated as follows ( wikipedia 's equation here , not mine ) . that is , the sum of the kinetic and potential energy is just half the potential energy ! neat , is not it ? so , how would we apply this to the earth ? well , $z k_e e^2$ is going to have to be replaced with $g m m$ . but let 's not forget , the entire point was to introduce a radius $r&#39 ; =1.05 r$ , and i am seeking a value of $\delta e = e&#39 ; -e$ . also , the kinetic energy is 1/2 the magnitude of this total energy metric , i will use $e_k$ for that . $$\delta e = gmm/2 \left ( -1/r&#39 ; + 1/r \right ) = e \left ( -1/1.05+1\right ) =e\frac{0.05}{1.05} = 2 e_k \frac{0.05}{1.05}$$ so the energy would change by about 9.5% times the original kinetic energy . given your original energy , i believe this would be $8.8 \times 10^9 j$ . this all said , your question says : if the initial magnitude of the satellite’s mechanical energy was $e_{m , i} = 9.26 \cdot 10^{10}$ j and it continues at the same speed , how much work was done by the rockets in moving the satellite to the higher orbit ? our work assumed that it would attain a new speed . maybe the question is written wrong . i do not know .
current physics formulation has two frameworks . one is the macroscopic one , in dimensions comensurate with our physical faculties of observation , larger than micrometer sizes . this is the classical framework which was studied and formulated mathematically until the beginning of the twentieth century . since then we have found out that the classical framework emerges from a more fundamental one , the quantum mechanical framework that describes the microscopic world , molecules , atoms , nuclei etc . there is wave–particle duality . according to this theory , light is a wave and a particle at once . the statement on wave particle duality is a statement on the quantum mechanical framework . in that framework yes light can behave like a billiard ball ( particle ) or like a wave ( with a sine/cosine wave like behavior in the possibility of detecting it ) . what about magnetic field ? can it be so , the magnetic field is a macroscopic concept described by the classical electromagnetic theory . that it is also a wave and particle , but this particle has not yet been discovered ? so no , it is not a wave and a particle in the quantum mechanical sense . the magnetic field emerges from the underlying quantum mechanical framework in a strict mathematical way , that you might learn if you continue your physics studies , but has no description as a quantum mechanical/wave-particle . as a handwaving explanation , the magnetic field emerges from the virtual coherent overlapping of innumerable photons , which photons are the particle/wave form of the electromagnetic force in the quantum mechanical framework . virtual because of the mathematics involved which do not allow them to light up as a light source : ) but remain tied up to the atoms and molecules creating the classical field . non virtual photons do obey the wave particle duality , depending on the way they are detected . is magnetic field discrete ? it is discrete only in the sense that it is generated by individual atoms that carry a magnetic field , not in the sense of the particle/wave duality .
lead is used to block radiation because : it is very dense . this means that the number of interactions that a radiation particle will undergo is higher over a fixed distance which causes the radiation to attenuate . it has a high proton number z . this means that the charged radiation particles will scatter through large angles , also causing attenuation .
this is a quantum partition function , not a statistical mechanical partition function . he is just talking about an idealized self-interacting field . if you have a scalar with cubic self interactions , you write the lagrangian as $$ \partial_\mu \phi \partial^\mu \phi - \lambda \phi^3 $$ if you fourier transform the field variables , this is $$ \int_k k^2 |\phi_k|^2 + \int_{k_1 , dk_2 , dk_3} \delta ( k_1+k_2+k_3 ) \phi_{k_1}\phi_{k_2}\phi_{k_3} $$ which , if you think of k as a lattice , can be abstrated to the form banks writes down . the remaining s_eff term is from renormalization , which changes the low energy theory according to the contributions to the low-energy effective action from high-energy degrees of freedom you are neglecting . this is heuristic , because a real renormalizable model requires a $\phi^4$ term too .
y-dwarfs are a subtype of brown dwarfs , which do not produce energy like ( or certainly do not produce as much as ) normal stars . brown dwarfs have an upper limit and a lower limit on their masses . both these limits are informal and approximate . these limits are not like planets , for which the iau has an accepted definition , but they are reasonably well-defined . broadly , bigger stars are hotter , so the smallest brown dwarfs will be the coldest , and it turns out that is really cold , by stellar standards . the upper limit is the smallest object that fuses hydrogen into helium . above this limit , the dwarf would shine like a regular star e.g. the sun . but below this limit , hydrogen will be fused , but not all the way to helium . so a little bit of energy is produced , but relatively little , because the real energy kick comes from the last step of the proton-proton chain , when two 3 he atoms fuse into 4 he and two protons . so if the core is not hot enough for this step to happen , some energy is produced , but really not much . the lower limit is basically when no fusion happens at all . that is , not even the first step of hydrogen fusion ( into deuterium ) takes place . this limit is less precise , but below it we had be talking about something more like a big version of jupiter . and that is cold . although the brown dwarf may produce a tiny amount of energy through deuterium fusion , most of the surface heat is probably left over heat from when it formed , and it may have had 10 billion years in which to cool . for reference , keep in mind that the surface temperature of jupiter , for example , is about 165&nbsp ; k , over 100&nbsp ; k below freezing water . so if you make a big version of it that is not producing much energy , it is not crazy that it would not be much hotter . the confusion probably just comes from referring to brown dwarfs as " stars " , when they are not stars of the sort that most people are familiar .
there is no integration of the radial part because , as you said yourself , we want the probability of finding the electron somewhere in the spherical shell between $r$ and $r+dr$ from the nucleus . ( in a differential shell between $r$ and $r+dr$ , and no need to integrate over $r$ . )
faraday 's cage is known to block static and non-static electric fields . the mechanism of blocking depends on whether the electric field is static or non-static ( em field ) . i suppose you question is about how the cage works in non-electrostatic case . in em case ( time changing field ) , two scenarios could happen . the first is electric discharge where the current flows from a distant electrode to the cage . the second is an em wave with high power propagating toward the cage generating its current locally within the conductor . i will explain how the cage works for both cases . with respect to the first case , it can be mathematically described by charge continuity equation ( equation 3 in this link ) . this equation basically relates the current flowing through a conductor to the charge accumulating in it . what happens in the first scenario is that the external current ( being moving charges ) coming from the electrode accumulates at the point where it ( the spark or the streamer ) hit the cage . because the cage is a conductor the charge continuity equation tells us that the local accumulation of charge where the spark hit the cage will cause current to flow within the conductor to remove that accumulation . the characteristic time required to remove the accumulation is called the relaxation time . it can be derived from charge continuity equation . for the derivation have a look at pages 57-59 of this document . i think that is taken from a book called elements of electromagnetics chapter 5 . if the conductor is made from a material with infinite conductivity , the relaxation time is zero . that means the current will keep flowing though the cage without any problem and that the electric field in the conductor is always zero . in other words , the electrostatic point of view holds even for non-electrostatic case if the conductivity is infinite . that is a direct consequence from charge continuity equation . for non infinite conductivity cases , the electric field within the conductor will survive within the conductor with a time scale related directly to relaxation time of that conductor . i hope it is clear now with respect to first case . the second case is related to em waves where they generate their currents locally within the conductor , that is where the skin effect comes into play . an em wave penetrates into a conductor the skin effect occurs . in general , em waves when they penetrate a conductor they are attenuated until their fields become almost zero . a characteristic depth of penetration is called skin depth . the skin depth is the distance it takes an em wave to be attenuated to certain value . this skin depth depends on many factors such as conductivity and frequency , the following figure taken from wikipedia shows the skin depth of different materials for different frequencies : for the cage to protect from em waves , it is thickness has to be larger than multiples of skin depth at the particular frequency of interest . so briefly with respect to the second scenario , the skin depth becomes relevant when we speak about shielding from electromagnetic waves rather than discharge current . the first and the second scenarios can be put together in frequency spectrum , the first scenario describes why the cage protects current in low frequencies while the second scenario describes why it protects from both current and radiation at high frequencies . i think the cage in the picture shows scenario 1 . you can clearly see the distant electrodes and the point at which the spark hits the cage i hope that answered your question
work done by tension on both the blocks can be regarded as 0 . this can be said by the virtual work method . the virtual work method : consider that block 1 ( mass $2kg$ ) displaces by a certain $d\vec{s_1}$ . infinitesimal work done on the block 1 by tension will be given by $$dw_1 = \vec t . d\vec s_1=tds_1\cos\theta_1$$ similarly , for block 2 we can say that $$dw_2=\vec t . d\vec s_2=tds_2\cos\theta_2$$ using string constraint , we can say that displacement of each block along the string is zero ( because the string is inextensible ) . so we get $$ds_1\cos\theta_1+ds_2\cos\theta_2=0$$ notice that i have used the same $\theta$ for each block as in tension because the direction along the string is the direction along tension vector .network done by tension thus becomes $$dw_t=t ( ds_1\cos\theta_1+ds_2\cos\theta_2 ) $$ $$\therefore dw_t=0$$ $$w_t=0$$ the solution to the actual problem : if we apply $w=\delta k$ on the system of the two blocks from initial position to the final position where block 1 is at the bottom of the cicrular arc , we get $$m_1g\delta h_1+m_2g\delta h_2=\frac 12 m_1 v_1^2+\frac 12 m_2 v_2^2$$ i do not include work done by tension on the system because i proved it to be 0 . we now need to find a relation between $v_1$ and $v_2$ . we can do this by applying string constraint . $$v_1\cos\theta=v_2$$ where $\cos\theta=\frac 35$ ( by geometry ) . $$\therefore \frac 35 v_1 = v_2$$ substituting $v_2$ in terms of $v_1$ in the above equation , we can find $v_1$ . then applying $w=\delta k$ on block 1 only we get $$w_t + w_{gravity} = \frac 12 m_1v_1^2$$ substitute $w_{gravity}$ and $v_1$ in this equation and find $w_t$ .
general relativity complicates things a bit , but in special relativity an inertial frame is a time independent co-ordinate system that is homogenous and isotropic . the time independence means that everything in that frame is at rest wrt everything else , and the homogenous and isotropic bit means the co-ordinate system has no curvature . take your question 1: you could define a rotating frame that rotates along with your rod , but this frame would not be isotropic ( except at the pivot point ) because there is an acceleration acting towards the pivot point and this picks out a particular direction . it is also not homogeneous because the acceleration varies with position . incidentally , your example of the rod is basically the same as the rotating disk i mentioned in my answer to your other question . you can treat the rod as a radial strip in the disk . re your other questions , can you ask these separately . as it is , the answer is heading towards essay size !
this is the solution for $n=2$ , $d=2$: you are trying to compute the average entropy of $$ \rho ( u_1 , u_2 ) = u_1|0\rangle\langle0|u_1^\dagger + u_2|0\rangle\langle0|u_2^\dagger $$ with $u_1$ and $u_2$ distributed according to the haar measure . this is equal to the entropy of $$ \rho' ( u ) = |0\rangle\langle0| + u|0\rangle\langle0|u^\dagger $$ where $u=u_1^\dagger u_2$ , which is of course still distributed according to the haar measure . in turn , this is $$ \tilde\rho ( |\psi\rangle ) = |0\rangle\langle0| + |\psi\rangle\langle\psi| $$ with $|\psi\rangle$ a haar-random qubit state . we know that a haar-random qubit state can be parametrized as $$ |\psi\rangle = \cos\tfrac\theta2 \ , |0\rangle + e^{i\phi}\sin\tfrac\theta2\ , |1\rangle\ , $$ where one needs to integrate in spherical coordinates ( i.e. . , with a measure $\tfrac1{4\pi}\sin\theta\ , \mathrm{d}\theta\ , \mathrm{d}\phi$ , $\theta\in [ 0 , \pi ] $ , $\phi\in [ 0,2\pi ] $ ) . we now have $$\tilde\rho ( |\psi\rangle ) = \frac12 \left ( \begin{matrix}1+\cos^2\tfrac\theta2 and e^{i\phi}\sin\tfrac\theta2\ , \cos\tfrac\theta2\\ e^{-i\phi}\sin\tfrac\theta2\ , \cos\tfrac\theta2 and \sin^2\tfrac\theta2\end{matrix}\right ) \ . $$ the entropy of this state is independent of $\phi$ ( so we set $\phi=0$ ) , and then we can easily check ( e . g . using mathematica ) that the von neumann entropy ( defined with the logarithm base $2$ ) is $$ h ( \tilde\rho ) = \tfrac12\left [ -\cos^2\tfrac\theta4\log_2\cos^2\tfrac\theta4 + \sin^2\tfrac\theta4\log_2\sin^2\tfrac\theta4\right ] \ . $$ the haar-average can then again be evaluated with mathematica and turns out to be $$ \frac{1}{4\pi}\int_0^{2\pi}\mathrm{d}\phi \int_0^\pi\sin\theta\ , \mathrm{d}\theta\ , h ( \tilde\rho ) = \frac13+\frac{1}{6\ln 2} \approx 0.5738 \ . $$
the given formula ( in that website ) is correct . assume we have a cylinder with permanent ( axial ) magnetization $\mathbf{m}$ . it will cause a surface current density $\mathbf{k}$: $$\cases{\bf{k}=\bf m \times \hat n \\ \mathbf m=m_0\hat z}\to \mathbf{k}=m_0\hat \phi$$ so it is like a finite length solenoid . to find the field on it is axis ( $z$ , point $p$ in the below picture ) , from biot–savart law we will arrive at : $$db=db_z=\frac{\mu_0k\mathrm{d}z}{2}\frac{r^2}{\xi^2}$$ $$\cases{dz=\frac{rd\theta}{\sin^2\theta}\\ \frac{r}{z}=\tan \theta \\ \xi=\frac{r}{\sin \theta}}\to db=-\frac{\mu_0 k}{2}\sin \theta d\theta$$ $$b=\int_{\theta_1}^{\theta_2}db=\frac{\mu_0 k}{2} ( \cos \theta_1-\cos \theta_2 ) $$ so $$\boxed {\mathbf{b}=\frac{\mu_0 m_0}{2} ( \cos \theta_1-\cos \theta_2 ) \hat z}$$
this is an extremely comprehensive review of electronic properties in two-dimensional electron systems ( 2dess ) : http://rmp.aps.org/abstract/rmp/v54/i2/p437_1 but , as you can imagine , it covers almost everything there is to cover in 2dess . for areas ( in transport ) you are focusing on you will find only sections iv c and d useful ; it involves computation of relaxation times in certain regimes . the research/review articles that i have come across so far do not talk comprehensively about transport ; most of them are limited to certain cases . general transport theory is covered in many excellent textbooks . one such example is this book by lundstrom : http://www.amazon.com/fundamentals-carrier-transport-mark-lundstrom/dp/0521631343 chapter 2 of the about book discusses computing relaxation times due to various scattering mechanisms : electron-impurity , electron-phonon ( optical and acoustic ) , electron-electron , as well as various types of scattering : inter- and intra-valley scattering etc . the rest of the characteristic scales can easily be determined from the relaxation time ( except the phase coherence length ) . the core of the book , however , makes use of the boltzmann transport equation ( bte ) , discussed in chapter 4 , which is inherently classical . however , since we often use band theory ( quantum ) alongside the bte , this is called semiclassical transport . this is why i said that you can compute all characteristic scales except for the phase coherence length . you need a fully quantum treatment to determine the phase coherence length . such a treatment is possible using non-equilibrium green 's function ( negf ) formalism . this book by datta : http://www.amazon.com/quantum-transport-transistor-supriyo-datta/dp/0521631459 gives you a very good intuition for negf since it approaches negf from the landauer formalism . if you are already familiar with the landauer formalism then all you need to do is take a look at the comparison between negf and landauer on page 27 , and then skip to chapters 8-10 . even towards the end of lundstrom 's book , i.e. chapter 8 and 9 , you will start seeing crossovers from semiclassical to quantum , diffusive to ballistic etc . the two books by datta and lundstrom are mutually complementary , and serve as an extremely valuable resource on carrier transport . if you are overwhelmed by the content in these books ( not surprising if true ) and then you can always visit the website http://nanohub.org/ to watch video lectures given by both these authors . this is an excellent class taught by datta : http://nanohub.org/resources/6172 and follows the same textbook i listed above .
the undulations are of the probability amplitude for the particle to be someplace . the notion of probability amplitude is fundamental , and cannot be reduced to anything more primitive . it is described on wikipedia under " superposition principle " . the undulations are in the space of all possible universes , so that two particles are described by undulations in the 6 dimensional space of all possible pairs of positions , three particles are described by undulations in 9 dimensions . there is no physical way of making it like waves on water or sound in air , because physical waves travel in three dimensions of space . if you are asking about classical fields , like e and b fields , there are not undulations in anything either . they are the primitive things out of which things like atoms and water are built . you can make classical fields out of particles if they have the right statistics , and these fields , when they are made of matter , are called bose-einstein condensates or superfluids , depending on the density .
the temperature of the gas that is sprayed goes down because it adiabatically expands . this is simply because there is no heat transferred to or from the gas as it is sprayed , for the process is too fast . ( see this wikipedia article for more details on adiabatic processes . ) the mathematical explanation goes as follows : let the volume of the gas in the container be $v_i$ , and its temperature $t_i$ . after the gas is sprayed it occupies volume $v_f$ and has temperature $t_f$ . in an adiabatic process $tv^{\ , \gamma-1}=\text{constant}$ ( $\gamma$ is a number bigger than one ) , and so $$ t_iv_i^{\ , \gamma-1}=t_fv_f^{\ , \gamma-1} , $$ or $$ t_f=t_i\left ( \frac{v_i}{v_f}\right ) ^{\gamma-1} . $$ since $\gamma&gt ; 1$ and , clearly , $v_f&gt ; v_i$ ( the volume available to the gas after it is sprayed is much bigger than the one in the container ) , we get that $t_f&lt ; t_i$ , i.e. the gas cools down when it is sprayed . by the way , adiabatic expansion is the reason why you are able to blow both hot and cold air from your mouth . when you want to blow hot air you open your mouth wide , but when you want to blow cold air you tighten your lips and force the air through a small hole . that way the air goes from a small volume to the big volume around you , and cools down according to the equations above .
the index of refraction is found to be a function of the frequency in an analysis of radiation scattering in a medium : for example in the book of panofsky and philips " classical electricity and magnetism " chapter 22 , radiation scattering and dispersion , on paragraph 7 . the book seems to be available for a free download here . here is a link with feynman lectures on light to to get the extended framework on light .
actually , that first statement is not correct . the universe is not expanding due to dark energy . it is accelerating due to dark energy . the normal expansion , called metric expansion , is an effect of general relativity . when you get a homogeneous distribution of matter or radiation ( a perfect fluid , a uniform gas , radiation , a homogeneous distribution of galaxies in the case of the universe today ) , you can solve the einstein field equations for general relativity for an expanding universe , called the frw metric . in this metric , the distance in between bound objects ( i.e. . galaxies today ) increases over time . this does not require dark energy , just a universe filled with matter or radiation . a simple article about some misconceptions about the expanding universe is here , i recommend reading it : http://www.mso.anu.edu.au/~charley/papers/lineweaverdavissciam.pdf so , prior to the discovery of dark energy , the expanding universe was understood perfectly well . however , we assumed that this expansion was slowing down . however , a discovery in the later nineties that was awarded the 2011 nobel prize in physics showed that not only is the universe expanding , it is accelerating . so , this is the role of dark energy . many different candidates for dark energy have been proposed , but one is heavily favored , the cosmological constant . in einstein 's equations for general relativity , you can throw in an extra term , $\lambda$ , the would play the role of a negative pressure vacuum energy . this has the effect of accelerating the expansion of the universe . why are we confident dark energy is just a cosmological constant ? one of the defining features of a cosmological constant is its equation of state . the equation of state , $w$ , is given by $p \over \rho$ , where $p$ is the pressure it contributes , and $\rho$ is the energy density . a cosmological constant has $w=-1$ . the wmap seven year report recorded the value as $w=-1.1 ± 0.14$ . within the error margins , the cosmological constant fits very well . quantum field theory also predicts the existence of a vacuum energy , so it was hoped that this would match the value of the cosmological constant . however , the value calculated by qft was enormously higher . using the upper limit of the cosmological constant , the vacuum energy in a cubic meter of free space has been estimated to be 10^-9 joules . however , the qft prediction is a whopping 10^113 joules per cubic meter . this is the ' vacuum catastrophe ' . for a simple page from the usenet faq about the cosmological constant , see here : http://www.astro.ucla.edu/~wright/cosmo_constant.html for a very thorough description , see here : http://philsci-archive.pitt.edu/398/1/cosconstant.pdf so , because the cosmological constant works so well as a description of dark energy , and is supported by the evidence , we prefer that over a description such as quintessence , or something similar to the explanation you proposed . addition - regarding the quantum fluctuations : the very early universe was filled with an obscenely hot and dense plasma and a bath of radiation . the metric expansion of space cooled and redshifted the radiation , and broke up the plasma into a much less dense gas of hydrogen . this is the essential nature of the big bang model , which you should note has nothing to do with a ' bang ' . the model was been confirmed by observations , which you can read about here . however , there are a few problems - first is the flatness problem . we observe that the universe is very , very close to being spatially flat . since expansion would cause the universe to deviate away from flatness , it must have been even flatter at the time of the big bang . ridiculously flat . how did it get this way ? second is the horizon problem . we observe that the universe is homogeneous on large scales , that is , it is pretty much the same everywhere . this means that primordial plasma must also have been perfectly homogeneous , which is confirmed by observations of the cosmic microwave background . however , if the expansion of the universe was extremely rapid from time zero onward , how did this plasma come to equilibrium ? it certainly would not have the time to do this . and third is the monopole problem . grand unified theories , or guts , are theories that unify the electroweak interaction with the strong nuclear force . they have the unfortunate feature of predicting that hot temperatures of the early universe should have produced an abundance of heavy magnetic monopoles , which we certainly do not observe . fourth is the homogeneity problem - why are there no inhomogeneities besides galaxies ? what made the early plasma so ' smooth ' ? a model called inflation fixes all of these problems . inflation proposes that the very early universe underwent an enormous expansion , growing the universe by at least 60 $e$-folds . this expansion would be driven by the inflaton field . this field would reach an undesirable energy value , called a false vacuum . when it is in this false vacuum , it has the property that it exerts an enormous negative pressure ( somewhat similar to dark energy ) . this drives inflation . after a very short period of time , the inflaton field reaches it is true vacuum ( through normal quantum effects such as tunneling ) . when this happens , it decays into a bath of radiation , heating the universe so that the big bang model can go from there . so , how does this solve the problems of the big bang model ? well , the enormous expansion would eliminate any curvature , making the universe extremely flat . this solves the flatness problem . second , it would allow the universe to expand very slowly before inflation , allowing it to come to equilibrium . this solves the horizon problem . any monopoles produced in the early universe would have been spread out so that we would only see about one in the entire observable universe , so the monopole problem is solved . and finally , inflation would ' iron out ' any large scale inhomogeneities with the rapid expansion . so , this is where those quantum fluctuations come in - prior to inflation some regions of the primordial plasma would have become very slightly denser due to quantum fluctuations - when the universe inflates , the random changes in density that come from quantum mechanics will get magnified , and you end up with what is called a " scale free power spectrum . " it is like drawing a small line on a flat balloon . blow the balloon up , and the line will become very large . similarly , small density perturbations become primordial ' seeds ' . since these are due to random fluctuations , we would expect this to produce a universe that has an even distribution of galaxies , such as ours . from there , dark matter clumps around these seeds , which then draws in the rest of the matter to form proto-galaxies . from there , full galaxies develop .
they are two forms of the same equation , but clausius-clapeyron uses vapor pressure ( $p^*$ ) where van ' t hoff uses the reaction equilibrium constant ( $k$ ) . why does this work out ? well , think of vaporization as a chemical reaction : $$ \text{x} ( l ) \longrightarrow \text{x} ( \text{g} ) $$ the equilibrium constant is defined in terms of activity ( a ) : $$ k=\left . \frac{a_\text{x ( g ) }}{a_{\text{x} ( l ) }}\right|_\text{equil} $$ for an ideal liquid solution at modest pressure , a is just the mole fraction x . and for an ideal gas , a is just its partial pressure in bar : $$ k=\left . \frac px\ , \right|_\text{equil}$$ one equilibrium condition is $x=1\ $ and $p=p^* , \ $ so . . . $$ k=p^* $$ or $p^*$ is the equilibrium coefficient for vaporization . now since k is characteristic of a reaction at a given temperature , this would imply that if we change x , then the new partial pressure at equilibrium would change according to $$ k=p^*=\frac{p}{x} \qquad\rightarrow\qquad p=xp^*$$ and that is exactly what happens . neat , huh ?
x-rays in order to make metal radioactive one have to turn it into another element or isotope . this can be performed only with high-energy particles ( including photons ) . x-rays can be produces if an electron enters metal with very high speed in two ways : deceleration radiation ( bremsstrahlung ) an atom absorbs part of the electron 's kinetic energy , moves to one of the excited states and moves back to ground state emitting a high-energy photon in any case the energy of the incident radiation ( or particles ) must be comparable to the energy of x-ray or gamma photons . this is far from cellular phone frequency range for sure . shields and cages solid metallic shield reflects electromagnetic waves back . the material of the shield is important since it should have good conductivity . most of metals works well . as far as i know , copper and gold are the best especially for high frequencies ( microwaves ) . if the frequency is quite low there is no need for solid shield . the effective area that reflects the wave is proportional to $\frac{\lambda^2}{4\pi}$ , where $\lambda$ is the wavelength . it can be much larger than the antenna size . so metallic lattice works well if the distance between the wires is lower than $\lambda$ . gsm phones uses frequencies about 1 ghz which corresponds to $\lambda\approx$ 30 cm . for low frequencies the diffraction effects are important . if the size of the shield is comparable to $\lambda$ the radiation can just bypass the obstacle as it happens with sound . in this case metallic box is the best solution . it can be a cage with appropriate cell size . there should be no big holes like doors and windows . grounding grounding removes charges from the outside surface of faraday cage , but if you are inside there is no way to determine whether it is grounded or not . this is more concerned with safety . shape of the antenna this is important if you need something more interesting than just screening . using the effect of bragg diffraction , it is possible to build a shield that reflects one frequency and does not affect others ( this will work only for some directions ) . the shape of the shield also allows to control polarization of the radiation . edit 1 . answering the questions in the comments is the ' energy ' of incident radiation proportional to the frequency of transmitted emf waves , or what people commonly also call ' radiated power ' measured in watts/meter-sq ( as well ) ? there are two energy characteristics for emws : intensity - the amount of energy incident on unit area per unit time ( measured in w/m$^2$ ) . it describes total power of the radiation . energy of quantum - the energy of single photon ( measured in joules ) . it is equal to product of planck 's constant and frequency : $h\nu$ or $\hbar\omega$ . this value is very important in quantum mechanics since quantum system can absorb only integer number of photons . if one photon is not enough to change system state then even 1000 photons will just pass through with no effect . if emf radiation that is several hundred/thousand times in excess of international norms , could cause the x-ray generation this is possible if you have a system that can collect energy of low-frequency radiation and turns it to something else . for example , if emw induces plasma discharge between some metallic details and electrons collect enough energy before collision there can be x-rays ( i am not sure such situation is possible ) . edit 2 . answering the questions in the comments would the lattice structure/size computation be good enough if i base it on the highest frequency ( thus get the smallest lattice size needed ) ? also , does it matter if the material ( s . a . common steel mesh ) has the cross-over joints fused or insulated from each other ? if lattice period is smaller than $\lambda$ then it should work as a good screen . since you need computations and optimization it is better to ask someone who specializes in electrodynamics and antenna theory . may be it is better to ask this as a separate question . edit 3 . answering the questions in the comments is it possible to make practical application of bragg diffraction to cause destructive interference of the emf wave , when the waves are for large no . of different carrier waves , and clustered around 4-5 group of central frequencies ? this can be done with multiple bragg mirrors one for each frequency . afaik it is done for infrared radiation . apart from bragg diffraction are there other ways in which the emf can be reduced / nullified in a small region ( say within a radius of 5-6 meters ) , where the emf energy is captured using an antenna , converted to electrical energy , and converted to heat/light single antenna affects emf only within $\lambda$ distance . 5 meters is too much for 1 ghz which corresponds to $\lambda\approx$ 30 cm .
one possible explanation is delayed phase transformation in a nickel sulphide inclusion in toughened glass ( http://www.glassonweb.com/articles/article/330/ ) , which glass is sometimes used for cooking ware . as noted in a comment to the referenced article , this process can be accelerated by temperature variations . judging by the description , the pattern of failure seems typical for toughened ( tempered ) glass . again , i cannot be sure that this mechanism was indeed present in the case described in the question .
it may be a good idea to add the appropriate units in your calculations . doing so will help you to localize the mistake . at the end , your results are only wrong because of a multiplicative factor of $\sqrt{1,000}\sim 31.7$ that must be added to your result to obtain the right one . it is actually not hard to see where this wrong factor comes from . http://en.wikipedia.org/wiki/atomic_mass_unit in your denominators , you used a value for the mass of the nuclei that is based on the ratio of the type $32$ divided by avogadro 's constant ( number of particles per mole ) . however , in this way , you obtain the value that assumes the natural conversion factor $1\ , {\rm g/mole}$: avogadro 's constant was originally defined as the number of molecules in one gram-molecule . however , you want to get the masses in kilograms – and the proton mass is about $1.66\times 10^{-27}\ , {\rm kg}$ , to proceed in the si units . so effectively , the right easiest fix of your formulae is either to substitute the explicit masses in kilograms or to replace your avogadro 's constant by $6.023\times 10^{26}/{\rm mole}$ whose numerical value is the number of molecules in one kilogram-molecule ( note the kilo ) or , equivalently , keep avogadro 's and replace $32$ by $0.032$ etc . then you get the right results within some tiny error margins ( the masses 32 and 28 amu are not quite accurate : proton and neutron masses differ and there are additional corrections from electrons and from nuclear binding energies ) .
main point : you should allow the possibility of sign factors appearing into the definition of the hilbert space representation of fermionic operators , cf . fermionic fock space . in more detail , consider the car algebra $$\tag{1} \{c_{\sigma} , c_{\tau}\}~=~0 , \qquad \{c_{\sigma} , c^{\dagger}_{\tau}\}~=~\hbar {\bf 1} , \qquad\{c^{\dagger}_{\sigma} , c^{\dagger}_{\tau}\}~=~0 , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ next define $$\tag{2} c_{\sigma}\left|0\right&gt ; ~:=~0 , \qquad \left|\sigma\right&gt ; ~:=~ c^{\dagger}_{\sigma}\left|0\right&gt ; , \qquad \left|\sigma\tau\right&gt ; ~:=~ c^{\dagger}_{\sigma}\left|\tau\right&gt ; , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ note that these definitions imply that $$\tag{3} \left|\sigma\tau\right&gt ; ~=~ -\left|\tau\sigma\right&gt ; , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ in particular $$\tag{4} \left|\sigma\sigma\right&gt ; ~=~ 0 , \qquad \sigma\in \{\uparrow , \downarrow\} . $$
batteries produce a charge difference across the terminals as a result of a chemical reaction . a chemical gets changed into another one . even in a rechargeable battery a chemical change takes place that is reversed . an electrolytic capacitor uses chemistry to create a thin layer with an electric field across it . the thinner layer than a " regular " capacitor allows more charge to be stored in a smaller space . but the fundamental difference is that there is no chemical change when a capacitor is charged
to close the loop , andrew , the answer to your newest question is : the best and most famous reference about the electrodynamics of moving bodies is einstein , albert ( 1905-06-30 ) . " zur elektrodynamik bewegter körper " . annalen der physik 17: 891–921 . see also a digitized version at wikilivres:zur elektrodynamik bewegter körper . the english translation , " on the electrodynamics of moving bodies " , is here : http://www.fourmilab.ch/etexts/einstein/specrel/www/ the content of this paper became known as the special theory of relativity . i am just partly joking because for uniformly moving media , the lorentz boost to the rest frame is still the most natural way to proceed .
there are several qualitative and quantitative differences between gravity and magnetism . when you attract ' neutral ' bits of metal with a magnet , or attach it to something like a plate of metal , what is happening is that individual atoms of the metal react to the magnetic force . in a ferromagnetic metal , one with a similar electronic structure to iron or nickel , the individual atoms work like nanoscopic magnets ; but they are very weak , and they are not lined up with one another , so that their fields cancel one another out over any macroscopic distance . but if you bring a " large " magnet ( such as a fridge magnet ) up to them , the field of the large magnet causes them to align with the field , so that they are pulled towards the magnet &mdash ; and the magnet is pulled towards them . this is why some metal objects are attracted to magnets . other metals , such as aluminum or silver , also react to magnets , but much more weakly ( and in some cases repulsively ) : the way that they react to magnetic fields is described as paramagnetism ( for materials which align very weakly with magnetic fields ) and diamagnetism ( for materials which align very weakly against magnetic fields ) . the very fact that different materials react differently to magnetic fields is something that sets magnetism apart from gravity . gravitation works equally with masses of any sort , and is always attractive ( as noted by nic ) ; magnetism can both attract and repel , and do so with different degrees of force , as between ferromagnetic , paramagnetic , and diamagnetic materials . but of course , quite famously , even a single object can be both attracted and repelled by magnetic forces : the north poles of two magnets repel each other , as do the south poles ; only opposite poles attract each other . ( this , of course , is the basis on which compasses work . ) the way that these forces operate over distance also varies . gravity very famously ( but only approximately ) obeys an inverse-square law ; the field far from a bar magnet , however , decreases like the inverse of the cube of the distance from the magnet . finally , moving electric charges produce magnetic forces ; whereas they do not cause any gravitational forces which could not be accounted for just by the fact that the charged particles have mass ( whether moving or at rest ) . so , on both the macroscopic level and on the level of individual atoms , the forces of gravity and magnetism act quite differently .
i do not see much order on this particular chaotic picture . what i see is some localized mass . the mass is localized because of the attractive force of gravity . also , what we could see if you included an animation is that the galaxy is spinning . that is also why the shape resembles a disk . this increase of the spinning after the gravitational collapse - or clumping - is due to the conservation of the angular momentum , see why do galaxies and water going down a plug hole spin ? more generally , one could ask why many systems in nature that contain many degrees of freedom often look simpler - for example why diamond looks so nice and clear even though it is a complicated bound state of many carbon atoms . there is simply not choice because there must always exist an expansion of the behavior of a system around an infinite number of building blocks . the number of mathematically possible , qualitatively different behaviors is limited , so nature has to recycle them .
nb : i feel like this is a pretty half-assed job , and i apologize for that but having opened my mouth in the comments i guess i have to write something to back it up . we start with fermi 's golden rule for all transitions . the probability of the transition is $$ p_{i\to f} = \frac{2\pi}{\hbar} \left|m_{i , f}\right|^2 \rho $$ where $\rho$ is the density of final states which is proportional to $p^2$ for massive particles . to find the rate 1 for all possible final states we sum over these probabilities incoherently . when the mass difference between the initial and final states is much less than the $w$ mass the matrix element depends only weakly ( hah ! ) on the particular state and the sum is well approximated by a sum only over the density of states : $$p_\text{decay} \approx \frac{2\pi}{\hbar} \left|m_{}\right|^2 \int_\text{all outcomes} \rho . $$ this sum is collectively called the phase space available to the decay . in these cases the matrix element is also quite small for the reason that dr bdo discusses . the phase space computation can be quite complicated as it must be taken over all unconstrained momenta of the products . for decays to two body states it turns out to be easy , there is no freedom in the final states except the $4\pi$ angular distribution in the decay frame ( their are eight degrees of freedom in two 4-vectors , but 2 masses and the conservation of four momentum account for all of them except the azimuthal and polar angles of one of the particles ) . the decays that you have asked about are to three body states . that gives us twelve degrees of freedom less three constraints from masses , four from conservation of 4-momentum which leaves five . three of these are the euler angles describing the orientation of the decay ( and a factor of $8\pi^2$ to $\rho$ ) , so our sum is over two non-trival momenta . the integral looks something like $$ \begin{array}\\ \rho \propto \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - e_1 - e_2-e_3 ) \\ and \delta ( e_1^2 - m_1^2 - p_1^2 ) \\ and \delta ( e_2^2 - m_2^2 - p_2^2 ) \\ and \delta ( e_2^2 - m_2^2 - p_2^2 ) \\ and \delta ( \vec{p}_1 + \vec{p}_2 + \vec{p}_3 ) \end{array} $$ which is easier to compute in monte carlo than by hand . ( btw--the reason for introducing the seemingly redundant integral over the angle $\theta$ between the momenta of particles 1 and 2 will become evident in a little while ) . for beta decays the remnant nucleus is very heavy compared to the released energy , which simplifies the above in one limit . in the case of muon decay , it is not unreasonable to treat all the products as ultra-relativistic , and the above reduces to $$ \begin{array}\\ \rho \propto \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - e_1 - e_2 - e_3 ) \\ and \delta ( e_1 - p_1 ) \\ and \delta ( e_2 - p_2 ) \\ and \delta ( e_3 - p_3 ) \\ and \delta ( \vec{p}_1 + \vec{p}_2 + \vec{p}_3 ) \\ = \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - p_1 - p_2 - p_3 ) \\ and \delta ( \vec{p}_1 + \vec{p}_2 + \vec{p}_3 ) \\ = \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - p_1 - p_2 - \left|\vec{p}_1 + \vec{p}_2\right| ) \\ = \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta\left ( m_0 - p_1 - p_2 - \sqrt{p_1^2 + p_2^2 - p_1p_2\cos\theta} \right ) \end{array} $$ the integral over the angle will evaluate to one in some regions and zero in others and as such is equivalent to correctly assigning the limits of the other two integrals , so writing $\delta m = m_0 - m_1 - m_2 - m_3$ we get $$ \begin{array} \rho and \propto \int_0^{\delta m/2} p_1^2 \mathrm{d}p_1 \int_0^{\delta m-p_1} p_2^2 \mathrm{d}p_2 \\ and \propto \int_0^{\delta m/2} p_1^2 \mathrm{d}p_1 \left [ \frac{p_2^3}{3}\right ] _{p_2=0}^{\delta m-p_1} \\ and \propto \int_0^{\delta m/2} p_1^2 \mathrm{d}p_1 \frac{ ( \delta m - p_1 ) ^3}{3} \end{array} $$ which i am not going to bother finishing but shows that that phase space can vary as a high power of the mass difference ( up to the sixth power in this case ) . 1 the lifetime of the state is inversely proportional to the probability
you are not doing this wrong . as you know energy of each photon is $e = hf = 2.27ev$ so they can not produce any photoelectrons on a metal with work function greater than that .
there is a difference between finding a solution and recognizing a solution . oracle can recognize the solution or solve a particular instance of the problem but cannot give you the solution for complete problem . or in other words , oracles gives you a part of solution and you may need to consult oracle a number of times to get the complete solution . oracle also may be thought as a library function ( as in programming languages ) which will give you solution for one instance of a problem , and the real cost of computation is measured by how many times you call the function and not the inherent complexity of the function itself which is taken as black box . for example , lets say we have a oracle for a function $f ( x ) = x^2$ , on presenting this oracle with a pair $ ( a , b ) $ it will tell whether $b^2 = a$ or not . in this case time complexity is taken as how many time you need to consult the oracle to get the desired result . more concrete example can be taken from oracle for verifying if the number if prime . lets say we want to find the first prime number $p : a &lt ; p &lt ; b , a &lt ; b \in z^+$ . the problem has different complexity when you are given access to the oracle and when you are not given . physical example of an oracle : lets say our problem is to determine the angle between the floor and the wall which may not be necessarily $90^\circ$ to it . all you can do is throw a ball which will go elastic collision on the wall and return back . you have control of the angle you throw and you can note the angle it came back . each throwing of a ball can be compared with the calling of oracle function and the constraint on the angle of the returning ball ( reflection , which gives you a hint on the orientation of the wall ) can be considered an oracle . the number of times you need to repeat the throwing of ball to get the orientation with desired accuracy may be considered as the complexity of the problem relative to the oracle .
blue . the proton is way smaller than a wavelength of visible light . but blue light has a shorter wavelength than any other visible color , red light is longer wavelength , blue is shorter , other colors in the middle somewhere . white light is a mixture of all the colors of light , all the wavelengths in the visible range . if you illuminate the proton with white light , almost all the white light will just go past the proton , not reflect back , because the proton is so small . but of the small amount of light that does reflect back , a higher fraction of it will be blue light and a lower fraction of it will be red light . so the reflection would appear blue . this is pretty much the same effect that makes the sky blue . tiny particles in the sky do not reflect much of the sun 's light going past them , but of the small amount they do reflect , more of it is blue than any other color .
dear rodrigo , it is an interesting stronger version of the uncertainty principle for general operators $a , b$ that i have never seen before but i just verified it holds . just to be sure , the anticommutator is simply $$\{a , b\}\equiv ab+ba . $$ i like when the braces are only used for pairs of grassmannian objects but people use it as a bookkeeping device to simplify $ab+ba$ in all situations . nothing difficult about the notation . note that the commutator and anticommutator appear totally symmetrically in the inequality , a fact we will derive . to see why the stronger inequality holds , open wikipedia here http://en.wikipedia.org/wiki/uncertainty_principle#mathematical_derivations where only the simpler version of the inequality ( without the squared anticommutator ) is proved by combining two inequalities . the first one , $$ ||a\psi||^2 ||b\psi||^2 \geq |\langle a\psi|b\psi\rangle|^2 $$ remains unchanged . however , the second inequality from the wikipedia article may be strengthened to a full-fledged equality $$ |\langle a\psi|b\psi\rangle|^2 = \left| \frac{1}{2i} \langle \psi | ab-ba | \psi \rangle \right|^2 + \left| \frac{1}{2} \langle \psi | ab+ba | \psi \rangle \right|^2 $$ this identity simply says that the squared absolute value of a complex number is the sum of the squared real part and the squared imaginary part ( which was omitted on wikipedia ) . combining the previous two inequalities , one gets your " stronger " uncertainty principle . ( of course , the equation derived above is uselessly weak unless the expectation values of $a , b$ vanish themselves . it can be strengthened into yours by repeating the same prodedure for $\delta a = a-\langle a\rangle$ and similarly $\delta b = b-\langle b\rangle$ instead of $a , b$ . ) i wrote what the anticommutator means mathematically and why the inequality is true . now , what does the anticommutator term mean physically ? i do not know what this question mean . it is a term in an equation that i can read and explain for you again . the precise answers in physics are given by mathematics . so i guess that the answer you want to hear is that it means nothing physically , it is just pure mathematics . this fact does not mean that it can not be useful . well , in normal cases , the stronger version is not " terribly " useful because the anticommutator term is only nonzero if there is a " correlation " in the distributions of $a , b$ - i.e. if the distribution is " tilted " in the $a , b$ plane rather than similar to a vertical-horizontal ellipse which is usually the case in simple wave packets etc . maybe this is what you wanted to hear as the physical explanation of the anticommutator term - because $ab+ba$ is just twice the hermitean part of $ab$ , it measures the correlation of $a , b$ in the distribution given by the wave function - although the precise meaning of these words has to be determined by the formula .
in general , both iqhe and fqhe are rigid quantum states , whose rigidness is protected by the finite energy gap ( $h\omega$ for iqhe ) between the ground state ( s ) and the exited states . finite temperature can support excitations to overcome the gap , which destroys the rigidness of the state . under finite temperature , the quantization of the hall conductivity is no longer exact . if the energy scale of temperature $k_bt$ is greater than the gap , the rigidness will be completely destroyed , and the quantization effect can not be observed anymore . in particular for the iqhe you considered , the landau level is still quantized under any temperature , but the fermion occupation is not . the fermion occupation follows the fermi-dirac distribution . under finite temperature , the fermion surface is no longer sharp , so the fermions can not integer fill the landau level , as a result , the quantized transport will be smeared out by the temperature .
think of those not as a capacity to absorb/emit but as simply absorption/emission . imagine you put an cold metal cube next to a hot identic cube . the hot one will emit a lot of heat but receive very little amout of heat from the cold one . therefore αλ&lt ; ϵλ , and vice-versa for the other cube . after a while , they will reach the same temperature , and at that moment , the radiation form cube a will be equal as cube b 's . at that moment αλ will equal ϵλ since the " output " radiation and " input " radiation are equal . you are right when saying it does not take into accout the other object , but this equation becomes true only when both objects reach thermal equilibrium so it is not necessary to explicitly refer to the other one . it is like if the surface was a mirror , and you were asking with this equation when does the reflect equals the original ? you only need the original to answer this question .
assuming that the proton is heavier than the neutron , by more than the mass of the electron ( plus the mass of a neutrino , plus the ionization energy of hydrogen ) , this is easy to answer , it would just make hydrogen unstable to decay to a neutron an an electron positron pair , so that a mostly hydrogen universe will decay into neutrons and electron-positron pairs , which wil annihilate into photons . so i will assume that the difference between neutron mass and proton mass is less than the mass of the electron , so that both the proton and the neutron are stable . the most drastic effect of this is on big-bang nucleosynthesis , where two new stable species can be created , neutrons and tritium , and he-3 would be unstable to inverse beta-decay into tritium . so you would produce hydrogen , deuterium , tritium , helium , lithium , and neutrons . the initial conditions are mostly neutrons , not mostly protons , because the mass is inverted , and so you would get a lot of he-4 , very little h-1 , and most of the universe 's mass would consist of stable neutrons and alpha-particles . these neutrons might collide to form neutron clusters , which would then beta-decay to protons once the binding energy was greater than the neutron proton mass difference . there would not be stars , but there might be gravitationally bound neutron clusters . neutrons are neutral , and find it hard to dissipate energy , but the time scales are long , so they might be able to eventually settle down into neutron-star-like objects .
i have turned my comment into an answer so the question can be closed . a very similar question was asked and answered here .
for particles in a beam , making a superposition of spin is easy ; you just split the beam and recombine it after making appropriate modifications to the split beams . in creating a superposition , this works even if the beam has only a single particle in it . a particle in a box is a tougher situation as it is difficult to split the states . the problem is that the usual methods of measuring a particle 's position or momentum ( in a box ) are destructive . for example , after you measure the position of an electron in a potential well you destroy all information about its momentum and its previous positions . i will take a whack at it . . . to get what you are asking for , you would need to have a measurement that ( a ) is non destructive , and ( b ) allows you to make some modification to the electron ( s ) . suppose that the electron begins in its lowest energy state . you want a measurement that will split that energy state into two different states and then you make a modification to one of the states . suppose that you begin with an electron with spin-up and lowest energy in your potential well . one thing you could do is to apply a magnetic field in a direction other than up or down . this will split the electron into appropriate superpositions of spin along that new direction . if you make the new magnetic field in some horizontal direction u ( so you are perpendicular to spin-up ) , you will have a superposition : $ ( |+u\rangle+|-u\rangle ) /\sqrt{2}$ . those two superpositions have two different energies . when an electron absorbs a single photon , its spin flips . so you can convert a $|-u\rangle$ electron to the $|+u\rangle$ state by arranging for it to absorb a photon oriented in the +u direction . the $|+u\rangle$ electron can not absorb such a photon because of conservation of angular momentum . therefore , you have just added energy to one half of your superposition . now turn off the magnetic field . you have created a state that is a superposition of two energy states . ( maybe more , depending how copacetic the excited energy state with the magnetic field on are with the excited energy states with no magnetic field . ) in fact , the magnetic field really was not necessary . i put it in there to make you think of the electron in terms of the +-u basis for its spin . all you really have to do is to arrange for the photon to have spin in the +u direction . by the way , a photon with spin in the +u direction is " circularly polarized " . also , the turning on and off of the magnetic field needs to be done slowly . to arrange for particular relative phases in the superposition between the + and - spins you can use the technique of " quantum phase " , or " berry-pancharatnam phase " . this is the phase acquired when a system is slowly sent through a sequence of states ( but which also applies to sudden state changes with the same topology ) . one can induce a phase by a slow change to the spin axis . the phase one obtains is equal to half the spherical area cut out by the spin axis in the bloch sphere ( i.e. . the set of possible directions for a spin axis ) . to get a relative phase you had want to " park " one of the spin states by , say , arranging for it to have an energy that prevents its spin state from being modified , and then send the other state through a sequence .
the 4 degrees is the logical minimum for a refrigerator because below that ice crystals start forming which many time destroy the texture and taste of many foods . of course the lower the temperature the more inactive the decay bacteria and molds , but who wants a wilted salad . the first refrigerators were filled by ice bought by the iceman every day so 4 degrees was the minimum that could be obtained . in modern refrigerators a balance is struck , having various drawers for fruits and vegetables where the temperature is higher so they retain their texture and appearance . freezers are used for long time storage of food that is not destroyed by freezing , either in appearance or taste or texture in order to inactivate decay processes . in countries where freezing temperatures prevail in the winter it was observed that meat and fish etc retain their food properties well when frozen and can be kept for long months . the -18 must be an engineering compromise , a good temperature for long time storage but not too expensive in power consumption to retain . one also must keep in mind that ice grows as temperatures plummet so a compromise also must be found with the destruction of nutrients by the enlargement of ice crystals if temperatures are arctic . in any case both regrigerator and freezer have a thermostat that could be set for higher temperatures .
for steel , the specific heat would be $c_p=0.5 kj/kg k$ , with a density of $ \rho=7000 kg/m^3$ . suppose you want to increase the temperature bij say $\delta t=1100k$ of a piece of size $v= ( 15cm ) ^3$ then you would need a total energy of . $$e=\rho c_p v \delta t$$ which gives you typically $e=10^7 j$ now , the power of the sun on a bright day , would be of the order of $p=10^3 w/m^2$ . assuming that all the energy input is converted into heat the mirror is perfectly aligned no heat is lost during heating , no melting , e.g. no latent heat and your mirror had diameter $d$ and you let the process run for a time $t$ , then $$e=p \frac{\pi}{4}d^2 t $$ then you will get , in approximation $$d = \sqrt{\frac{e}{pt}}$$ so , suppose you are willing to wait for ten minutes , then the mirror diameter would be $d\approx 4m$ . considering we assumed an ideal system , this is only an order of magnitude assumption .
the electric and magnetic parts of an electromagnetic wave have the same wavelength and the same spatial relationship , so if the electric parts line up the magnetic ones do , too . there is only one wave , not three . if they are traveling in the same direction ( as in a laser ) they will stay in phase . constructive interference works fine-that is what makes lasers work and interference patterns in light .
analyzing one moving clock from the perspective of one stationary person will be inadequate to derive special relativity from . with just that set-up , you are not actually using the key fact that the speed of light is the same for all observers – all you are actually using is just the fact that the speed of light is finite . with just taking into account that the speed of light is finite , all you will arrive at is the non-relativistic doppler effect , which is different from time dilation .
an energy eigenstate is just an eigenstate of the hamiltonian . so , given a particular hamiltonian operator $h$ , the energy eigenstates $\lvert n\rangle$ satisfy $$h\lvert n\rangle = e_n\lvert n\rangle$$ where $e_n$ is just a number . the reason energy eigenstates are useful is that according to the schroedinger equation , they remain unchanged ( except for a phase factor ) over time . suppose $\lvert\psi ( 0 ) \rangle$ is the initial state of some system with a hamiltonian $h$ . if $\lvert\psi ( 0 ) \rangle$ is the $n$th eigenstate of $h$ , namely if $\lvert\psi ( 0 ) \rangle = \lvert n\rangle$ , the system 's state at a later time $t$ will be $$\lvert\psi ( t ) \rangle = e^{ie_nt}\lvert n\rangle = e^{ie_nt}\lvert \psi ( 0 ) \rangle$$ and since the schroedinger equation is linear , if the initial state is a linear combination of energy eigenstates , $\lvert\psi ( 0 ) \rangle = \sum_n \alpha_n\lvert n\rangle$ , the same holds for each of the eigenstates in the sum . essentially you can distribute the time evolution over the sum . accordingly , this lets you easily write down an expression for the state of the system at time $t$: $$\lvert\psi ( t ) \rangle = \sum_n\alpha_n e^{ie_nt}\lvert n\rangle$$ so if you can express the initial state as a sum of coefficients times the energy eigenstates , it makes it pretty trivial to express the state at any later time . that is where the inner products come in . it is often the case that eigenstates of $h$ form a complete orthonormal basis , and when you have an orthonormal basis , the way you decompose an arbitrary state into that basis is by taking inner products , $\alpha_n = \langle n\vert\phi ( 0 ) \rangle$ . none of this has anything to do with the uncertainty principle .
the period of the pendulum is roughly $t\approx2\pi\sqrt{\frac{l}{f}}$ where $l$ is its length and $f$ is the downward-pulling force ( usually gravity ) . let 's call our reference period $t_{rest}$ . now let 's examine your cases . 1 . when the train is in circular motion in a curve of radius r with constant speed . if the train is in circular motion the pendulum experiences a fictitious centrifugal force that points radially outward . firstly , that means that the total force on the pendulum is not straight down any more , but somewhat diagonally downward/outward . since you have to add this force ( vectorially ) to the gravitational force , $f$ will also be larger than before . looking at our equation above , this means that indeed $t_1 &lt ; t_{rest}$ . 2 . the train is going up a constant slope with constant speed . if the train travels along a straight line ( and a constant slope is a straight line ) at constant speed , then the pendulum is in an inertial frame . therefore , there are no fictitious or other forces adding to gravity . hence , $t_2 = t_{rest}$ . 3 . the train moves over a hill of radius r with constant speed . this one is tougher . the centrifugal force is still pointing radially outwards - but now it is not perpendicular to the gravitational force any more . for instance , at the top of the hill , it is pointing upward , and thus cancelling gravity partly . at other positions it will point diagonally upward . this case needs its own somewhat more involved treatment , because the total $f$ is not constant . therefore , $t_3$ will also change gradually . in particular , it will be maximal at the top of the hill , and will decrease symmetrically down both sides of the hill . sorry that i do not have any pictures to clarify this . feel free to ask if anything is unclear .
the multipole coefficients associated with a $1/|r|$ distribution $\rho$ depends on the choice of origin . for example , if you have a point charge and you choose the origin to be at that point charge , then it will have a pure monopole character . however , if you choose the origin to be elsewhere , it will have nonzero expansion coefficients other than the monopole . this is an artifact of your choice of coordinate system . to make this rigorous , let $\mathbf{i}=\{i_0^0 , i_1^{-1} , i_1^{0} , i_1^1 , . . . \}$ and $\mathbf{r}=\{r_0^0 , r_1^{-1} , r_1^{0} , r_1^1 , . . . \}$ be the set of irregular and regular solid harmonics . then the potential $v ( \mathbf{r} ) $ due to $\rho$ admits the exterior and interior multipole expansions $$v=\sum_{j=0}^\infty \mathbf{i}_j\rangle\left\langle\mathbf{r}_j , \rho\right\rangle\qquad\text{when }|\mathbf{r}|&gt ; r_\text{max} \\ v=\sum_{j=0}^\infty \mathbf{r}_j\rangle\left\langle\mathbf{i}_j , \rho\right\rangle\qquad\text{when }|\mathbf{r}|&lt ; r_\text{min} $$ or in matrix notation , $$v=\mathbf{i}\mathbf{r}^\dagger\rho\qquad\text{when }|\mathbf{r}|&gt ; r_\text{max} \\ v=\mathbf{r}\mathbf{i}^\dagger\rho\qquad\text{when }|\mathbf{r}|&lt ; r_\text{min} . $$ in the case where $\rho$ is purely real , we can use the real solid harmonics $\mathbf{i}'$ and $\mathbf{r}'$ , which are related to the standard solid harmonics by a unitary block diagonal matrix $\mathbf{u}$ via $\mathbf{i}'=\mathbf{i}\mathbf{u}$ from which we obtain the analogous real expansions $$v=\mathbf{i}\mathbf{r}^\dagger\rho=\mathbf{i}\mathbf{u}\mathbf{u}^\dagger\mathbf{r}^\dagger\rho= [ \mathbf{i}' ] [ \mathbf{r}' ] ^\dagger\rho= [ \mathbf{i}' ] [ \mathbf{r}' ] ^\mathsf{t}\rho\qquad\text{when }|\mathbf{r}|&gt ; r_\text{max} \\ v=\mathbf{r}\mathbf{i}^\dagger\rho=\mathbf{r}\mathbf{u}\mathbf{u}^\dagger\mathbf{i}^\dagger\rho= [ \mathbf{r}' ] [ \mathbf{i}' ] ^\dagger\rho= [ \mathbf{r}' ] [ \mathbf{i}' ] ^\mathsf{t}\rho\qquad\text{when }|\mathbf{r}|&lt ; r_\text{min}$$ which has the advantage that the list of multipole moments $ [ \mathbf{i}' ] ^\mathsf{t}\rho$ or $ [ \mathbf{r}' ] ^\mathsf{t}\rho$ are purely real . so , why does a point charge not located at the origin have moments other than a monopole ? it is for the same reason why a washing machine with a raccoon inside of it will shake around when it is on a wash cycle : it is not balanced , as the charges ( or mass ) are not located at the center of the relevant coordinate system . as an explicit proof of why a point charge not located at the origin can not have a pure monopole moment , suppose otherwise . then a test charge will be uniformly accelerated towards the center of the coordinate system , instead of towards the point charge . this is a contradiction . therefore , there must be higher moments involved . alternatively , a detailed justification can also be obtained by applying the addition theorem for spherical harmonics , but hopefully the proof given in the previous paragraph is sufficiently illuminating to show why higher moments will appear when a point charge is not located at the chosen origin . here 's a numerical example to compute the moments of a single point charge located at spherical coordinate $ ( r , \pi/2,0 ) $ in mathematica ( it also computes the potential $v$ at an arbitrary point and compares it to the potential obtained from direct application of $v=1/|\mathbf{r}-\mathbf{r}_0|$ ) : 0.332219 0.332273 $$\left ( \begin{array}{c} \{q\} \\ \left\{\frac{q r}{\sqrt{2}} , 0 , -\frac{q r}{\sqrt{2}}\right\} \\ \left\{\frac{1}{2} \sqrt{\frac{3}{2}} q r^2,0 , -\frac{q r^2}{2} , 0 , \frac{1}{2} \sqrt{\frac{3}{2}} q r^2\right\} \\ \left\{\frac{1}{4} \sqrt{5} q r^3,0 , -\frac{1}{4} \sqrt{3} q r^3,0 , \frac{1}{4} \sqrt{3} q r^3,0 , -\frac{1}{4} \sqrt{5} q r^3\right\} \\ \left\{\frac{1}{8} \sqrt{\frac{35}{2}} q r^4,0 , -\frac{1}{4} \sqrt{\frac{5}{2}} q r^4,0 , \frac{3 q r^4}{8} , 0 , -\frac{1}{4} \sqrt{\frac{5}{2}} q r^4,0 , \frac{1}{8} \sqrt{\frac{35}{2}} q r^4\right\} \\ \end{array} \right ) $$ note that there are nonzero moments of all orders whenever $r\neq 0$ . however , the potential at the test location is correct up to parts per thousand accuracy when the sum runs up to $l=4$ . how do i prove that a single point charge only has monopole ? set $r=0$ in the above triangle of numbers . everything vanishes except the monopole term .
how should i notate my bounds of integration ? i guess you mean for this integral , passing to gauge pressure \begin{align} \int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_s ( x , t ) +b ) \ dp_s ( x , t ) . \end{align} since $p_s=p_g+p_a$ , all you need to do is that change of variables . besides , $dp_s=dp_g$ because $p_a$ is a constant . also , there is no need to say $dp_s ( x , t ) $ , since these are dummy variables . \begin{align} \int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_s+b ) \ dp_s and =\int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_g+p_a+b ) dp_s\\ and =\int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_g+p_a ) dp_s+ ( p_{s , l}-p_{s , o} ) b\\ and =\int_{p_s ( o , t ) }^{p_s ( l , t ) }p_g dp_s+ ( p_{s , l}-p_{s , o} ) ( b+p_a ) \\ and =\int_{p_g ( o , t ) }^{0}p_g dp_g-p_g ( b+p_a ) \\ and =-\frac{p_g^2}{2}-p_g ( b+p_a ) =-\frac{p_g}{2} ( p_g+2p_a+2b ) , \\ \end{align} where in the last line $p_g$ is $p_{g , o} ( t ) $ . note that $p_{s , l}-p_{s , o}=-p_g$ . how does dividing atmospheres by absolute pressure give you gauge pressure ? here i think you are confused . psi , psig , atm , and pa are all units of pressure . that is why you can add gauge pressure to atmospheric pressure in the first place . the problem is that pressures expressed in these units are not proportional since they have different zero , which is the same problem when working between the different temperature scales ( farenheit , celsius , and kelvin ) . my advice is to make every algebraic and mathematical manipulation without units , or using a single system like si , and only translate to more practical units at the end .
no , it is not correct because of relativity . because you use $e=mc^2$ , clearly , you are using relativistic effects so you need to take the full theory of relativity into account . while you might think that you have only used the special theory of relativity , you are also considering gravitational effects – the gravitational binding energy – and the general theory of relativity is the only ( and right ) viable theory of gravity among those that are compatible with the special theory of relativity . in general relativity , it is still true that the ( absolute value of the ) gravitational binding energy never exceeds the original total latent energy $e=mc^2$ of the system . up to the purely numerical factors that are corrected by general relativity , your relationship between $r$ and $\rho$ is exactly the relationship we find for black holes . ( in 3+1 dimensions , $r=2gm/c^2$ for the schwarzschild . when combined with $m=c\rho r^3$ , we get $r=2cgr^3\rho/c^2$ , exactly your relationship , for a dimensionless numerical value of $c$ . ) the only special value of $r$ for a given $\rho$ is the black hole radius . the total mass/energy of a black hole is a subtle thing . in general relativity , the total mass/energy cannot really be written as a volume integral of a nicely behaving quantity . but it may be defined asymptotically , using a probe of the gravitational field at infinity ( adm mass ) . and the adm mass of the black hole may be seen to be positive . negative-mass black holes are prohibited because they have naked singularities etc . they also violate various energy conditions and should not arise in a consistent theory because the theory would be in conflict with causality ( causes preceded their effects ) . the marginal case is the $m=0$ black hole , and one may check that this massless black hole inevitably has $r=0$ , too . classically , using your argument , you might indeed think that the gravitational binding energy converted to mass may ultimately " beat " the mass you started with . but general relativity slows down how much mass you may " subtract " in this way when the gravitational binding gets really strong , and at most , you may converge closer to a black hole which guarantees that the original mass you started with is never cancelled ( and surely never exceeded ) .
for the reasons that you already mentioned in the question , it would not be possible to modify stereoscopic depth without adding artifacts . so instead i would argue if the depth is actually increased . it probably is not . we have gotten used to watching monoscopic ( regular ) photographs with both our eyes . theoretically the perceived scale of the scenes on those photographs should be huge . but we have gotten used to it . so , the depth effect of an stereoscopic image that is captured in the range of monoscopic ( 0 camera distance ) up to a regular human eye-distance will be perceived natural . a small amount of depth is already obvious to the brain . if you would increase the camera distance beyond human eye distance , then your brain would get an unusual stimulus hence it might conclude that the scene is a miniature . also note that the viewing angle ( field of view ) of a photograph or mobile screen is also significantly smaller than the captured angle . not that it would compensate for reduced depth , but just another example of our tolerance in accepting the illusion of a photograph . if you are shooting images for a more immersive viewing experience ( eg . 3d cinema ) , then the tolerances are probably much smaller .
if the magnet can support its own weight , i would stick it on the underside of a piece of well leveled , ( ferromagnetic but not magnetized ! ) sheet metal . it should hang from one pole or the other , as the center of mass of the magnet should end up vertically in line with the strongest part of the pole .
just compare the resolution of the two : prism depending on n , there is no good material n> 1.7 ( besides diamond ) depending on base length if you use a equilateral triangle have to use more than one to overcome this prism absorb light , you have got scattering ( stray light ) too now a grating : optimize it for your wavelength choose lines per milimeter resolution depending on the number of lines that are illuminated compact device just transmission gratings have got absorption , you can do your measurement in reflection with a blazed grating design your blazed grating to get the most light in e.g. 2nd order quantitatively prism : $\frac{\lambda}{\delta \lambda} = t \frac{dn}{d\lambda}$ grating : $\frac{\lambda}{\delta \lambda} = \frac{zd}{g}=zn$ where t is your base length , z . . . order of spectrum , g . . . grating constant , d . . . entrance beam diameter , n . . . number of illuminated lines so just use a grating , nowadays they can be fabricated in excellent quality . on my university learning the pros of a diffraction grating is part of the 1st year laboratory exercises .
start with differential form of poisson 's ratio : $$\frac{\text{d} x}{x}=- \nu \frac{\text{d} l}{l}$$ $$\int_{x_0}^{x_0+\delta x} \frac{\text{d} x}{x}=- \nu \int_{l_0}^{l_0+\delta l} \frac{\text{d} l}{l}$$ $$\ln \frac{x_0+\delta x}{x_0}=- \nu \ln \frac{l_0+\delta l}{l_0}$$ $$1 + \frac{\delta x}{x_0}=\left ( 1+\dfrac{\delta l}{l_0}\right ) ^{-\nu}$$
it does not sound exactly like a chord , but this type of technique was widely used in the 8-bit and 16-bit computer game eras , when the number of sound channels available was limited . it has a very distinctive retro video game sound , but the ear is able to identify the chord that it is supposed to be . here is a youtube video explaining how to achieve the effect using synthesis software - your situation is different , but you can probably adapt some of the advice . you can experiment with the speed of modulation , but the guy in the video sets it to around 30 hz , which sounds good . actually , i think you should be able to achieve the effect of a chord through a different method . to play a note at a frequency of $f_1$ at the same time as a note with frequency $f_2$ , you just need to time the sparks so that there are sparks at times $0 , \frac{1}{f_1} , \frac{2}{f_1} , \frac{3}{f_1} , \dots$ and also at times $\frac{1}{f_2} , \frac{2}{f_2} , \frac{3}{f_2} , \dots$ . this will sound to the ear exactly like the two notes being played at once , because that is essentially what it is .
you ask how can you formally go from the summation over $s$ to the double sum over $s_1$ and $s_2$ ? as we will see in a moment , passing to the double sum relies on the mathematical fact ( about tensor products of hilbert spaces ) that if $s_1$ labels a basis of states for system $1$ , and if $s_2$ labels a basis of states for system $2$ , then the set of all pairs $ ( s_1 , s_2 ) $ labels a basis of states for the composite system . let 's assume that the system at hand is a quantum system described by a state space ( hilbert space ) $\mathcal h$ . let the ( state ) labels $s$ index an orthonormal basis of $\mathcal h$ consisting of eigenvectors $|s\rangle$ of the total system 's hamiltonian $h$ ; \begin{align} h|s\rangle = e ( s ) |s\rangle , \end{align} then the partition function is obtained by summing over these states ; \begin{align} z = \sum_s e^{-\beta e ( s ) } . \end{align} notice that the state labels $s$ do not have to be numbers . they could , for example , be pairs of numbers or triples of numbers , whatever is most convenient to label the possible states for the system at hand . now , suppose that the system consists of a pair of subsystems $1$ and $2$ , then the hilbert space of the combined system can be written as a tensor product of the hilbert spaces for the individual subsystems ; \begin{align} \mathcal h = \mathcal h_1\otimes \mathcal h_2 . \end{align} let $h_1$ denote the hamiltonian for subsystem $1$ , and let $h_2$ denote the hamiltonian for subsystem $2$ . let $\{|1 , s_1\rangle\}$ be an orthonormal basis for $\mathcal h_1$ consisting of eigenvectors of $h_1$ , and let $\{|2 , s_2\rangle\}$ be an orthonormal basis for $\mathcal h_2$ consisting of eigenvectors of $h_2$ , then we have the following basic fact ; the tensor product states \begin{align} |s_1 , s_2\rangle := |1 , s_1\rangle\otimes|2 , s_2\rangle \end{align} yield an orthonormal basis for the full state space $\mathcal h = \mathcal h_1\otimes \mathcal h_2$ . in particular , the set of states one needs to sum over in the partition function can be enumerated by pairs $s= ( s_1 , s_2 ) $ . moreover , if the systems are non-interacting , then the hamiltonian of the full system is essentially the sum of the hamiltonians of the individual subsystems ( with appropriate identity operator factors ) ; \begin{align} h = h_1\otimes i_2 + i_1\otimes h_2 , \end{align} so that if $e_1 ( s_1 ) $ and $e_2 ( s_2 ) $ denote the energy eigenvalues of $h_1$ and $h_2$ corresponding to the states $|1 , s_1\rangle$ and $|2 , s_2\rangle$ , then the energy $e ( s_1 , s_2 ) $ of a tensor product basis state is their sum ; \begin{align} h|s_1 , s_2\rangle = e ( s_1 , s_2 ) |s_1 , s_2\rangle = \big ( e_1 ( s_1 ) + e_2 ( s_2 ) \big ) |s_1 , s_2\rangle , \end{align} and the partition function can be written as a sum over the tensor product basis states ; \begin{align} z = \sum_{ ( s_1 , s_2 ) } e^{-\beta e ( s_1 , s_2 ) } = \sum_{s_1}\sum_{s_2} e^{-\beta\big ( e_1 ( s_2 ) + e_2 ( s_2 ) \big ) } = \sum_{s_1} e^{-\beta e_1 ( s_1 ) } \sum_{s_2} e^{-\beta e ( s_2 ) } = z_1z_2 \end{align} where in the second equality we have used the fact that a single sum over all possible pairs $ ( s_1 , s_2 ) $ is equivalent to iterated sums over the possible values of $s_1$ and $s_2$ .
colin mcfaul 's answer is accurate . if you are wondering why it is in that weird form , it has to do with a fundamental postulate of special relativity . in 2d , say you are given the endpoint of a line that you know starts at the origin , and you want to find its length . according to the diagram below , if you are given a vector in $xy$ coordinates , $ ( a , b ) $ ( represented by the red line ) , you can look at it in an arbitrary coordinate system by rotating your head , but physically , the length of the arrow must remain unchanged . if the coordinates in this other coordinate system are $ ( c , d ) $ , we can phrase this with the pythagorean theorem : $a^2+b^2=c^2+d^2=\mbox{distance}$ if we have a special coordinate system drawn , $x'y'$ , then $d=0$ and we just have $a^2+b^2=c^2$ . in this new frame , its length is just the position on one axis . in relativity , we have one axis $ct$ ( where $c$ is the speed of light ) , and another axis $x$ . $ct$ and $x$ are both measured in meters . instead of a euclidean distance relation , the factor that remains unchanged from " rotations " is $ ( ct ) ^2-x^2$ . this is the crux of relativity . it means you can " rotate " space into time and vice versa . in one frame , the particle may have a constant velocity , so $x=v t$ and the law is conservation of $ ( ct ) ^2- ( v t ) ^2= ( c^2-v^2 ) t^2$ . if we choose a primed reference frame so that the speed is zero , then we must have $ ( c^2-v^2 ) t^2= ( ct' ) ^2$ . with some algebra we find that this equation implies that $$t=t ' \frac{1}{\sqrt{1-\left ( \frac{v}{c}\right ) ^2 }}$$ this is written as $t=\gamma t'$ . it all stems from the invariance of $ ( ct ) ^2-x^2$ . you can do analogous things when talking about $x$ and $x'$ .
the thing to understand is how we tag neutrinos for flavor in the first place . neutrinos are created and destroyed in reactions that also involve a charged lepton ( electron , muon or tau ) . at vertex level these are $$ w^\pm \to l^\pm + \nu_l $$ and various rotations . the flavor of a neutrino is defined as coincident with that of the charged lepton produced . ( i have neglected $z \to \nu + \bar{\nu}$ reactions here , but these are far off-shell at the energies available in the core of the sun , so they do not contribute . ) the reactions in the sun are fusion reaction that do not have enough excess energy to create a heavy lepton , so the neutrinos must ( by definition ) be electron-type . this rule for neutrino flavors can be tested at accelerator where we can generate beams of known neutrino content ( because we can count the number and type of hadrons decaying to charged leptons ) , and when the beam is directed onto a detector very near-by the number of charged-current neutrino interactions of each flavor that we detect agrees with the flavor content of the beam . but there is more , we can predict the energy spectrum of the solar neutrinos assuming that they are electron-type , and that is the spectrum that we detect . by conservation of energy neutrinos created in concert with heavy leptons ( that is , other flavors ) would have a different energy spectrum . now , the evidence that the theoretical oscillation framework we use is correct is pretty diverse , but some of the very best " one plot " evidence comes from kamland , where we plot the flux of electron-type anti-neutrinos from japanese power reactors as a function of observed energy and compare to the expected $\sin \left ( \frac{l}{e} \right ) $ behavior ( appropriately convoluted to account for the many different distances to reactors ) . ( image from http://kamland.lbl.gov/ ) . full disclosure : i was a member of the kamland collaboration for about 4 years and am named as an author on the paper from which that figure is drawn .
i give the answer with the general method even though a much straightforward way would be to guess the result because it is simple . the unit of $h$ is the inverse of time denoted by $ [ \mathrm t^{-1} ] $ . the dimension of a temperature is denoted by $ [ \theta ] $ . to find the numerical value of $t$ in kelvin , one should find a combination of $c : [ \mathrm{lt^{-1}} ] $ , $\hbar= [ \mathrm{ml^2t^{-1}} ] $ , $\mathcal g : [ \mathrm{m^{-1}l^3t^{-2}} ] $ and $k_{\mathrm b}: [ \mathrm{ml^2t^{-2}\theta^{-1}} ] $ ( $ [ \mathrm l ] $ and $ [ \mathrm m ] $ represent length and mass respectively ) such that $$ c^x \ ; \hbar^y \ ; \mathcal g ^z\ ; k_{\mathrm b}^u\times h$$ has the dimension of a temperature ( $x$ , $y$ , $z$ and $u$ are unknown ) . this gives the system $$\left\{\begin{array}{rcc} y-z+u and =0 and \quad [ \mathrm m ] \\ x+2y+3z+2u and =0 and \quad [ \mathrm l ] \\ -x-y-2z-2u and =1 and \quad [ \mathrm t ] \\ -u and =1 and \quad [ \theta ] \end{array}\right . $$ the solution is $$\left\{\begin{array}{cl} x and =0\\ y and =1\\ z and =0\\ u and =-1 \end{array}\right . $$ we obtain thus $$t=\frac{\hbar}{2\pi k_{\mathrm b}}h . $$ the value of $h$ is $h=67.8\ , \mathrm{km . s^{-1} . mpc^{-1}}=2.194\times 10^{-18}\ , \mathrm{m . s^{-1}}$ . we find a temperature of $t=2.67\times10^{-30}\ , \mathrm k$ .
after stating the solution , i will try to give some physical insights to the best of my knowledge and some more references . the dimension of the required state space is given by the verlinde formula , having the following form for a general compact semisimple lie group $g$ on a riemann surface with genus $g$ corresponding to the level $k$: $$ \mathrm{dim} v_{g , k} = ( c ( k+h ) ^r ) ^{g-1} \sum_{\lambda \in \lambda_k}\prod_{\alpha \in \delta} ( 1-e^{i\frac{\alpha . ( \lambda+\rho ) }{k+h}} ) ^{ ( 1-g ) }$$ ( please see blau and thompson equation 1.2 . ) . here , $c$ is the order of the center , $h$ is dual coxeter invariant , $\rho$ is half the sum of the positive roots , and $r$ is the rank of $g$ . $g$ is the genus , $\delta$ is the set of roots and $\lambda_k$ is the set of integrable highest weights of the kac-moody algebra $g_k$ . for the torus ( $g=1$ ) , this formula simplifies to : $$ \mathrm{dim} v_{\mathrm{torus} , k} = \# \lambda_k $$ i.e. , the dimension is equal to the number of integrable highest weights of the kac-moody algebra $g_k$ . the integrable highest weights of a level-$k$ kac-moody algebra are given by the following constraints : $$ \lambda - \mathrm{dominant} , 0 \leq \sum_{i=1}^r \frac{2 \lambda . \alpha^{ ( i ) }}{ \alpha^{ ( i ) } . \alpha^{ ( i ) }}\leq k$$ where $ \alpha^{ ( i ) }$ are the simple roots , please see , for example , the following review by fuchs on kac-moody algebras . ( my favorite reference for the representation theory of kac-moody algebras is the goddard and olive review which seems not available on line ) for example for $su ( 3 ) _k$ whose dominant weights are $2$-tuples of nonnegative numbers $ ( n_1 , n_2 ) $ , the above condition reduces to : $$\mathrm{dim} v^{su ( 3 ) }_{\mathrm{torus} , k} = \# ( n_1\geq 0 , n_2\geq 0 , 0\leq n_1 + n_2 \leq k ) = \frac{ ( k+1 ) ( k+2 ) }{2}$$ to perform the computations for the more general cases , one can use the seminal review by slansky . the verlinde formula was discovered before the chern-simons theory came into the world . originally it is the dimension of the space of conformal blocks for the wzw model . this formula has been derived in a large variety of ways , please , see footnote 26 in the fuchs review . it is still an active research topic , please see for example a new derivation in this recent article by gukov . the chern-simons theory may be the most sophisticated example in which the dirac quantization postulates can be carried out in spirit . ( more precisely their generalization in geometric quantization ) . i mean starting from a phase space and utilizing a specified set of rules to associate a hilbert space to it . in the case of the chern-simons theory , the phase space is the set of solutions of the classical equations of motion . the classical equations of motion require the field strength to vanish , in other words the connection to be flat . this phase space ( the moduli space of flat connections ) is finite dimensional , it has a kähler structure and it can geometrically quantized as a kähler manifold , just like the case of the harmonic oscillator . thus the problem can be reduced in principle to a problem in quantum mechanics . the case of the torus is the easiest because everything can be carried out explicitly in the abelian and the non-abelian case , please see the following explicit construction by bos and nair , ( a more concise treatment appears in dunne 's review ) . in the case of the torus , the moduli space of flat connections in the abelian case is also a torus and in the non-abelian case it is : $$\mathcal{m} = \frac{t \times t}{w}$$ where $t$ is the maximal torus of $g$ . basically , a fock quantization can be carried away , but there is a further restriction on the admissible wave functions coming from the invariance requirement under the large gauge transformations ( please see for example , the dunne 's review ) . the invariant wave functions are called non-abelian theta functions and they are just in a one to one correspondence with the kac-moody algebra integrable highest weights . ( in the abelian case , the wave functions are the jacobi theta functions ) . in the higher genus case , although the quantization program leading to the verlinde formula can be carried out in principle , few explicit results are known , please see the following article by lisa jeffrey ( and also the following lecture notes ) . the dimension of these moduli spaces is known . in addition . witten in an ingenious work computed their symplectic volumes and their cohomology ring in some cases . witten 's idea is that as in the case of a simple spin , the dimension of the hilbert space in the semiclassical limit ( $k \rightarrow \infty$ ) becomes proportional to the volume and the leading exponent of $k$ is the complex dimension of the moduli space ( please observe for example , that in the case of $su ( 3 ) $ on the torus , the leading exponent is $2$ which is the rank of $su ( 3 ) $ which is the dimension of the maximal torus $t$ ) .
no . when you hit the wall , the bicycle rotates around the front axis . the angular momentum l that you create for an arbitrary number of mass particles is $$l=\sigma_i ( r_i \times m_iv_i ) . $$ if you split location r=r+r_i and v=v+v_i with r and v being center of mass location and velocity , respectively , and r_i and v_i deviation from it , then it can be shown that l does not change when the center of mass does not change . so , the wood block on wheels should work ( in theory ) .
$$ \newcommand{\ket} [ 1 ] {|{#1}\rangle} \newcommand{\bra} [ 1 ] {\langle{#1}|} \newcommand{\braket} [ 2 ] {\langle{#1}\ , |\ , {#2}\rangle} \newcommand{\bracket} [ 3 ] {\langle{#1}\ , |\ , {#2}\ , |\ , {#3}\rangle} $$ well , i think by specifying mass $m$ and charge $q$ you simply define your system , a single electron , and then its complete non-relativistic description is indeed given by a wave-function $\psi ( \vec{r} , m_s , t ) $ as you mentioned . wave-function of the form $\psi ( \vec{r} , m_s , t ) $ , also known as spin-orbital , arises as follows . there is a postulate of qm which says that the state space $h$ of a system , composed of $n$ subsystems each associated with its own space $h_{i}$ , is the tensor product of this spaces , \begin{equation} h = h_{1} \otimes h_{1} \otimes \dotsm \otimes h_{n} \ , . \end{equation} the notion of a system composed of subsystems in the postulate above is not to be take literally : the state space can be written as a tensor product of state spaces which are not even associated with real physical systems . for instance , we can subdivide electron with position in space and spin system into 2 subsystems : electron only with position in space and electron only with spin . taking spin into account the state space for a particle is the tensor product of \begin{equation} h = h_{\text{space}} \otimes h_{\text{spin}} \ , , \end{equation} where $h_{\text{space}}$ is a state space spanned by eigenvectors $\ket{r}$ of the position operator \begin{equation} \widehat{\vec{r}} \ket{\vec{r}} = \vec{r} \ket{\vec{r}} \ , , \end{equation} and $h_{\text{spin}}$ is a state space spanned by eigenvectors $\ket{m_{s}}$ of a spin component operator conventionally chosen to be $\widehat{s}_{z}$ \begin{equation} \widehat{s}_{z} \ket{m_{s}} = m_{s} \hbar \ket{m_{s}} \ , . \end{equation} the resulting space $h$ is spanned then by $\ket{\vec{r} , m_{s}} = \ket{\vec{r}} \otimes \ket{m_{s}}$ by the property of a tensor product space and the expansion of a state vector $\ket{\psi} \in h$ then takes the following form \begin{equation} \ket{\psi ( t ) } = \sum_{m_s=-s}^{s} \ , \int\limits_{\mathrm{r}^{3n}} \psi_{m_s} ( \vec{r} , t ) \ket{\vec{r} , m_s} \mathrm{d}^3 \vec{r} \ , , \end{equation} where \begin{equation} \psi_{m_s} ( \vec{r} , t ) = \braket{\vec{r} , m_s}{\psi} \ , . \end{equation} it should be clear now that coefficients in the expansion of a state vector $\ket{\psi}$ over the basis $\ket{\vec{r} , m_{s}}$ are given by $2s + 1$ functions $\psi_{m_s} ( \vec{r} , t ) $ and that all them are required for the complete description of a state . thus , for instance , the state of an electron can be represented by a two-row vector \begin{equation} \ket{\psi ( t ) } \longleftrightarrow \begin{pmatrix} \psi_{+1/2} ( \vec{r} , t ) \\ \psi_{-1/2} ( \vec{r} , t ) \end{pmatrix} \ , , \end{equation} known as the two-component wavefunction . alternatively , $\psi_{+1/2} ( \vec{r} , t ) $ and $\psi_{-1/2} ( \vec{r} , t ) $ can be combined into single piecewise function $\psi ( \vec{r} , m_s , t ) $ which can also be used as the description of the state of an electron \begin{equation} \ket{\psi ( t ) } \longleftrightarrow \psi ( \vec{r} , m_s , t ) = \begin{cases} \psi_{+1/2} ( \vec{r} , t ) , and m_s = +1/2 \\ \psi_{-1/2} ( \vec{r} , t ) , and m_s = -1/2 \end{cases} \ , . \end{equation} another equivalent representation of the same idea is one which is usually used in quantum chemistry , where the spin dependence is separated out by introducing the " spin up " and " spin down " spin functions \begin{equation} \alpha ( m_s ) = \begin{cases} 1 , and m_s = +1/2 \\ 0 , and m_s = -1/2 \end{cases} \ , , \quad \quad \quad \beta ( m_s ) = \begin{cases} 0 , and m_s = +1/2 \\ 1 , and m_s = -1/2 \end{cases} \ , , \end{equation} and writing down $\psi ( \vec{r} , m_s , t ) $ in the following way \begin{equation} \psi ( \vec{r} , m_s , t ) = \psi_{+1/2} ( \vec{r} , t ) \alpha ( m_s ) + \psi_{-1/2} ( \vec{r} , t ) \beta ( m_s ) \ , . \end{equation} and since spin is relativistic phenomenon , such description is , in a sense , already relativistic , though , partly . fully relativistic description is different . the wave function is four-component , rather than two-component , and although , i could not tell you how it arises , the role of two additional components is to describe the " spin-up " and " spin-down " state of associated positron .
i would like to expand a bit on the answer to the second question , but for completeness i will do both . as said in brightblades ' answer , the expansion of space is not limit by the speed of light . so objects can be moving at moderate speeds , but because the space between them and us expands faster than light , it is emissions will never reach us . for this reason , there are already regions of space that are beyond our horizon . whether or not this will always be depends on what " big thing " your cosmology ends with . if it is a " crunch " , then everything comes back together in a reverse " bang " at the end of time . if it is just a " freeze " , then the acceleration expands and possibly even accelerates forever . abraham loeb wrote a curious paper ( available on the arxiv ) about how to reach cosmological conclusions in the absence of nearby galaxies . about 100 billion years from now , all the galaxies in our local group will be beyond the horizon of the milky way ( or rather milkomeda , after the milky way collides with andromeda ) , and the cmb will be at a wavelength longer than the observable universe . but you will still be able to reach conclusions about cosmology by using hypervelocity stars being ejected from the galaxy . the point is that you can still get accurate results about the global structure of the universe using local results . as for our current model , we have a great deal of evidence that the cosmos is structured according to the concordance model . you can always say " it might turn out to be wrong " , but it can not turn out to be that wrong because of said evidence . it is like gr as a generalization of newtonian gravity : yes , newton was " incorrect " but his theory was also quite accurate , to the extent that we still use it for , say , n-body simulations of star clusters .
light++ it is not open source but you can try to contact the author , werner benger . a few years ago we have access to the source code of ' light++' . not anymore : ( light++ raytracer ! ( general relativistic raytracing ) simulation of a black hole by raytracing the black earth about the simulation of galactic close encounters , or a n-body general simulation , under the constraints of gr i found nothing . edit add " i found nothing " can be read like this " there is not a single software package " because , afaik , no one knows how to apply gr in the computation of planetary and galaxy dynamics ( small scale with matter ) . the zeldovich approximation is used in the linearization of gr ( with caveats ) : and has been successfully applied to describe the large scale clustering in the distribution of galaxy clusters . . . however , within the zeldovich prescription , after a pancake forms in correspondence of crossing of particle orbits , such particles continue travelling along straight lines , . . i think that your aim is hopeless because gr is around since 1917 and no one succeeded . interesting questions , imo : how close to the reality are the simulations that are performed with newtonian codes . what kind of problems we may expect if we are gonna try to do a simulation code . edit add end
the system has constant pressure by definition . even in a system with changing pressure , though , for some small time , dt , there would be constant pressure . in that moment , and with those conditions , the relationship holds . adjust the pressure slightly , and a new , similar relationship is set up . the second question is related to this being a thought experiment ( an ideal situation ) . if gravity is not a factor , the pressure remains constant throughout the container . the international space station may be the best environment for testing this out , and i suspect someone may get around to trying it , if they have not already .
to calibrate our expectations , consider the largest nuclear weapon ever detonated , the tsar bomba . it is yield was at most about $58$ megatons tnt equivalent , or about $2.43\times10^{24}$ erg . now , let 's consider a smallish star , something like gliese 581 , which is reasonably nearby , small and faint , and has a planetary system ( of some sort : the number of planets is debated ) . it has a luminosity of $0.013$ times solar , which is roughly $5\times10^{31}$ erg . s$^{-1}$ . in other words , the luminosity of gliese 581b is about 20 million tsar bombas per second . this says nothing , however , about in what waveband the emission occurs , but i think the energetic argument is quite strong . . . ( i.e. . maybe if the nuclear bomb peaks in gamma-rays you could separate it from the starlight , but i do not know about our detection capabilities or the gamma-ray emission from the star ) . but , what about bigger things ? like an asteroid similar to the one that killed the dinosaurs ? it is yield was 100 teratons of tnt equivalent , or about one-twelfth of gliese 581 's per second luminosity . which might sound hopeful , but i suspect it took many seconds for that energy to come out , in which case it would still be washed about by the starlight . it turns out stars are quite bright in absolute terms !
you can derive the relativistic doppler shift from the lorentz transformations . let 's start in the frame of the moving rocket , and let 's take two events corresponding to nodes in the emitted wave ( i.e. . 1/$f$ ) . then in the rocket 's frame the two events are ( 0 , 0 ) and ( $\tau$ , 0 ) , where $\tau$ is the period of the radiated wave . to see what the period of the radiation is in our frame we just have to use the lorentz transformations to transform these two spacetime points into our frame . for simplicity we will take our rest frame and the frame of the rocket to coincide at $t = 0$ . this is convenient because then the first event is just ( 0 , 0 ) in both frames . now the lorentz transformations tell us : $$ t ' = \gamma \left ( t - \frac{vx}{c^2} \right ) $$ $$ x ' = \gamma \left ( x - vt \right ) $$ if we are tranforming from the rocket 's frame to ours , and the rocket is moving at velocity $v$ wrt us , then we have to put the velocity in as $-v$ , and we are transforming the point ( $\tau$ , 0 ) . putting these in the lorentz transformations we find that the point ( $\tau$ , 0 ) in the rocket 's frame transforms to the point ( $\gamma \tau$ , $\gamma v \tau$ ) in our frame . the last step is to note that if we are sitting at the origin in our frame the light from the event at ( $\gamma \tau$ , $\gamma v \tau$ ) takes a time $\gamma v \tau/c$ to reach us . so the time we see the second event is $\gamma \tau + \gamma v \tau/c$ and this is equal to the period of the radiation , $\tau'$ in our frame : $$ \tau ' = \gamma t + \gamma v t/c $$ we just need to rearrange this to get the usual formula . noting that $f'$ = 1/$\tau'$ and $f$ = 1/$\tau$ we take the reciprocal of both sides to get : $$ f ' = f \frac{1}{\gamma ( 1 + v/c ) } $$ to simplify this note that : $$\begin{align} \frac{1}{\gamma} and = \sqrt{1 - \frac{v^2}{c^2}} \\ and = \sqrt{ ( 1 - \frac{v}{c} ) ( 1 + \frac{v}{c} ) } \end{align}$$ and substituting this back in our expression for $f'$ we get : $$\begin{align} f ' and = f \frac{\sqrt{ ( 1 - v/c ) ( 1 + v/c ) }}{1 + v/c} \\ and = f \frac{\sqrt{ ( 1 - v/c ) }}{\sqrt{1 + v/c}} \\ and = f \sqrt{\frac{c - v}{c + v}} \end{align}$$ and presto it is proved !
first , why is the the static killing vector $\frac{\partial}{\partial t}$ equal to $-f^{\frac{1}{2}} dt$ ? the vectors $\partial/\partial t$ and $-f^{\frac{1}{2}} dt$ are not equal to each other . they are parallel to each other , and the factor of $-f^{1/2}$ is just so that the four-velocity is properly normalized . if you plug $u$ into the metric , you have to get 1 . second , why is the velocity , along the killing time vector ? what would happen if there is a component perpendicular to it ? does this mean , the fluid does not move through space ? the killing vector supplies a preferred frame of reference , one in which the observables ( e . g . , curvature scalars ) stay constant . a perfect fluid is one for which there exists a frame such that the stress-energy tensor is diagonal ; in this frame , there is no spatial flux of energy-momentum . so basically this means that in this frame , the fluid does not move through space ( in the sense that there is no flux ) .
heat pump should transfer heat from outside into the house . it should not generate heat ( ideally ) it should only force the heat to move . one joule of work executed by the heat pump can transfer several joules of heat - for example 4 joules ( it is the reason why the heat pump is efficient " source " of thermal energy ) . this ratio is called coefficient of performance or cop . the bigger is the difference between input and output temperature -> the more work must the heat pump do to transfer the heat . therefore the cop is decreasing . if the cop did not decrease with increasing temperature difference , it would be possible to construct a perpetual motion machine of the second kind . if work required to compress the gas was included into the computation the cop would be visible . and in order to achieve good cop the much lower output temperature would be needed . edit : computation of $cop$ with example . we will start from ideal heat engine modeled by carnot cycle . carnot cycle has efficiency $$\eta = \frac{t_h - t_c}{t_h}$$ let 's assume $t_h=310k$ , $t_c=270k$ and assume $100j$ of heat $q$ will be delivered from hot reservoir to the heat engine . as a result $$\eta \times 100j = \frac{310 - 270}{310} \times 100j = 12.9j$$ of work $w$ will be done by heat engine and $87.1j$ of heat will end up in cold reservoir . what happens when we reverse the process ? ( carnot cycle is reversible ) in the reversed process $87.1j$ of heat will be taken from cold reservoir , $12.9j$ of work will be delivered to the engine ( now the heat pump ) and $100j$ of heat will end up in the hot reservoir . the cop is $$cop = \frac{q}{w} = \frac{100j}{12.9j} = 7.75$$ and in general $$cop_{ideal} = \frac{q}{w} = \frac{q}{\eta \times q} = \frac{1}{\eta} = \frac{t_h}{t_h - t_c}$$
any physics equations you write down could be wrong , so you need to verify them experimentally , or you are just doing math . i can imagine a universe in which the gauss 's law does not work for moving charges ; and i have to test to see if we live in such a universe . in that sense , there is no non-experimental way to verify it . on the other hand , if you mean to ask if there is a mathematical way to prove it from the experimentally verified equations of electromagnetism , then sure ! but you need to be able to calculate the electric field of a moving charge . the derivation is a bit complicated , and i will leave it to you to find your favorite version of it , but the answer for a single charge moving at constant velocity is \begin{equation} \vec{e} = \frac{k\ , q\ , \vec{r}}{r^3}\ , \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}} \end{equation} it is enough to show this for just one charge because the integral is linear , so you can just add up contributions from different charges . also , we can take the integral over a sphere centered on this charge , since the divergence theorem tells us that moving the surface of integration will not change anything ( as long as we keep the charge inside ) . so , we do \begin{align} \oint \vec{e} \cdot d\vec{a} and = \oint \frac{k\ , q\ , \vec{r} \cdot \hat{r}}{r^3}\ , \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , r^2\ , \sin \theta\ , d\theta\ , d\phi \\ and = k\ , q\ , \int_0^{2\pi} \int_0^\pi \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , \sin \theta\ , d\theta\ , d\phi\\ and = 2\pi\ , k\ , q\ , \int_0^\pi \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , \sin \theta\ , d\theta \\ and = 4\pi\ , k\ , q~ , \end{align} which is just the usual gauss 's law . this , of course , is only for constant velocities , as peter kravchuk pointed out below . to see it more generally , it would probably be easier to go over to the differential form of gauss 's law , which is one of maxwell 's equations . then , you can note that maxwell 's equations are relativistically covariant .
there are many natural examples of quantities with fractional non-rational dimensions . these are ubiquitous in macroscopic physics , because the scaling laws of nature are emergent properties that do not usually care about differentiability . it is only in cases where you demand smoothness that you restrict to integer dimensions . the phenomenon discussed below are discussed at much greater length in mandelbrot 's " the fractal geometry of nature " , and in his publications . levy diffusion suppose a particle is undergoing levy diffusion , a process by which it hops from place to place with a jump whose distribution has a tail , meaning that the probability density of a hop of length l is : $$\rho ( l ) \propto {1\over l^{1+\beta}}$$ where $0&lt ; \beta&lt ; 2$ . then , if you take the limit of many steps , the analog of the central limit theorem guarantees that the probability of finding the particle at position x obeys a levy diffusion equation , which is easiest to write in a spatial fourier transform : $${\partial \rho ( k ) \over \partial t} = - d k^\beta \rho ( k ) $$ the constant d is the levy analog of the diffusion constant , and it has dimensions {l^\beta\over t} . the interpretation of d is that , when multiplied by t and extracting the $\beta$ root , it gives the typical displacement scale after time t . it is a natural quantity with fractional dimension . processes which undergo levy diffusion are relatively common . for example , advection of a dust particle by a turbulent flow ( see this experimental article:http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.152.6792 ) hausdorff measures when you try to define the " length " of a fractal shape , you run into the problem that the length is infinite . the way around this is to define a notion of length per length-scale , and to ask exactly how the length blows up . in mathematics , the most general way to do this , which works for essentially any set , is called the hausdorff measure . in practice , you can define a simpler version in terms of box-counting scaling which is sufficient for ordinary fractals . consider the coastline of england , and ask : given a boxes of length \epsilon , how many do i need to cover it up ? the minimum number n diverges as a power of \epsilon : $$ n ( \epsilon ) \approx {a\over \epsilon^\alpha}$$ the coefficient of the divergence , a , is the hausdorff measure , and a has units of $l^{\alpha}$ , and $\alpha$ can be anything between 1 and 2 for a coastline , and between 0 and 3 for a physical shape , like a cloud boundary , or a diffusion-limited-aggregation cluster . three dimensional quantum fields consider a scalar field in 2+1 dimensions , with a quartic self-interaction . at short distances , the correlation function of the field blows up . the scale dimension of the field is defined by this power : $$\langle \phi ( x ) \phi ( y ) \rangle = {1\over |x-y|^{1+2\nu}}$$ this might seem surprising if you are used to fields having canonical dimensions . the scalar in 2d has dimension 1/2 right ? wrong . the canonical dimension 1/2 only describe a free scalar theory or a cut-off theory with no short-distance self-interactions . after a successful continuum-limit by renormalization , the fields get new dimensions , which do not involve the dimensional quantity $\epsilon$ , which has gone to exactly zero . then the fields must be defined so that the left side and the right side of the equation above are dimensionally consistent . the exponent $\beta$ is very close to 0 , the correction is quadratic in " d-4" ( i hate this traditional name for the expansion parameter of the wilson-fisher point ) , but it is nonzero , and if you had to take bets , it will certainly be irrational . in four dimensions , ordinary local quantum fields only get logarithmic corrections to their classical scaling values , so the scaling of the field at short distances is either canonical ( like in qcd ) , or nonsensical ( like in qed ) . but in 3 dimensions , you get good anomalous dimensions , and fields have a true scale-invariant continuum limit . other fields if you are willing to leave physics , the quantities of finance often have levy behavior . if you consider the dollar a unit of nature , you can find fractional exponents there too .
if by large density you mean large baryon density , then i believe one of the fundamental large $n_c$ results is that at densities of order nuclear densities , but below the density where the baryons have dissolved into quarks , baryonic matter forms a crystalline structure . this has been analyzed in the skyrme model . i think this paper by klebanov was one of the first to point this out : " nuclear matter in the skyrme model , " igor r . klebanov , nucl . phys . b262:133,1985 . ads/qcd models can also be used to study qcd at large baryon chemical potential and low temperatures ( and necessarily at large $n_c$ ) . an instability to the condensation of vector mesons resulting from chern-simons couplings dictated by anomaly matching was found in a paper by me and s . domokos , arxiv:0704.1604 . i am not aware of any evidence that this phenomenon happens in the real world , and even its existence at large $n_c$ should be regarded as conjectural . i do not know whether this instability is related to the dgr instability or not .
susskind and polychronakos - and , independently , hellerman and van raamsdonk - have constructed a matrix model - based on string-theoretical d0-branes , which is the right fundamental way to think about any adjoint matrix model - to describe the fractional quantum hall effect . a review is here : http://iopscience.iop.org/1751-8121/42/30/304006 see also a hellerman-susskind realization of the quantum hall effect in string theory http://arxiv.org/abs/hep-th/0107200 moreover , there has been a whole new industry started by the 1997 maldacena 's ads/cft correspondence . on the cft side , one most typically has matrix-valued field theories , even though they are usually not just quantum-mechanical models but field theories in additional dimensions . chern-simons matrix models also have played a role . ads/something dualities have been successful to one extent or another in describing perfect fluids , fermi liquids , non-fermi liquids , superconductors , hydrodynamics , heavy ion physics , and other things . phenomena are usually translated to the behavior of black holes in a higher-dimensional spacetime .
you should show your work , but my guess is that you have to notice the change of variables : $$\frac{d\chi}{dx}=\frac{d\chi}{d\xi}\frac{d\xi}{dx}$$ you need to do this a second time ( using the derivate of a product . see if you can continue from there .
feeling silly now . just equating the component of velocities along the wall : $$1/2 sin\theta=cos\theta$$ we get $$\tan\theta =2$$ so , $$e=1/4$$
i see four reasons to make this assumption : the first one is simply that we do not know really how to deal with the non linear problem in general cases and therefore we linearize but i agree that it is not a good justification although this is most of the time the hidden reason why people use it the poisson-boltzmann equation is mostly used in aqueaous solutions and therefore you have a factor 80 ( owing to the dielectric constant of bulk water ) that appears in all your potentials . . . this more or less ensures that a monovalent ion does not generate a so high potential energy with other monovalent ions . if you have free ions in solution , it means that the thermal energy was enough to begin with to unbind them from the groups there were bound to it can be rigorously shown that at large distances from an ion at any valency , the generated potential decays exponentially with the distance ( or rather like a yukawa potential ) because of the screening owing to the mobile charges in solution ( this result holds in the non linear poisson-boltzmann case but also beyond the poisson-boltzmann theory ) . incindentally , a yukawa potential is what you can expect from the field generated by an isolated ion in solution within the linearized poisson-boltzmann theory . also , if you are a bit familiar with liquid theory , a particular closure of the orstein-zernike equation leads to an infinite summation over tree-like diagrams that leads to a total correlation function between two point ions in solutions that decays as a yukawa as well which tells you more or less that although you linearize , you still encapsulate important many body effects because of the last point , there is a huge litterature on charge renormalization of charged particles in solution as a function of their valency , size and salt concentration . in the context of the poisson-boltzmann theory , charge renormalization refers to a mapping from the non linear theory to the linearized one via the use of effective charge paramaters
in terms of your ordinary matrix multiplication , you have , for the case of a 4x4 matrix $m = g_{ab}$: $m\cdot m^{-1} = i$ , which is the same thing as $g_{ab} g^{bc} = \delta_{a}{}^{c}$ and $tr\left ( m\cdot m^{-1}\right ) = 4$ , which is the same thing as $g_{ab}g^{ab} = \delta_{a}{}^{a} = 4$
it could be any vector from bc to oa . let 's assume that $r$ is the vector perpendicular to bc to the point oa . any vector from bc to oa ( whether or not it is perpendicular to bc ) has the form $r + s$ , where $s$ is some vector parallel to bc . of course , you have told us that $\omega$ is also parallel to bc , so we could also write that as $r+\alpha\ , \omega$ , for some number $\alpha$ . so let 's take the cross product : \begin{equation} \omega \times ( r+\alpha\ , \omega ) = \omega \times r + \omega \times ( \alpha\ , \omega ) = \omega \times r + \alpha\ , \omega \times \omega = \omega \times r~ , \end{equation} since $\omega \times \omega = 0$ . so $\omega$ cross any vector from bc to oa will equal $\omega$ cross the perpendicular vector . so you do not need to specifically find the perpendicular .
non-physical/philosophical answer ( see comment of @annav ) : we orbit sun because we called the star we are orbiting " sun " . ( i think that is not actually as " stupid " as it may sound initially . see also anthropic principle ) physical answer : the earth formed from matter near to the sun , so it ended up near the sun , and is orbiting it , because gravity depends heavily on distance to the influencing masses , such that oher stars have negligible effects to earth 's orbit . ( see comment of @dumpsterdoofus and your reference to position ) to be exact , earth and sun are actually orbiting their total center of mass . but because of the mass difference , that is just above the surface of the sun . the center of mass is called barycenter in this context and is useful as origin of a coordinate system to describe the orbiting movements in .
before going into the details , let me tell you that this type of actions describe the deepest connection between geometry and physics and generalizations of these types of theories are still under active research even today . the lagrangian describes $n=1$ supersymmetric quantum mechanics on a riemannian manifold the bosonic part of this lagrangian is the kinetic term of a particle moving on a riemannian manifold $\mathcal{m}$ having a metric $g$ . as very well known the trajectories of the particles are the geodesics of the manifold . the functions $\phi^i$ are just the coordinates on the manifold . the fermionic parts of the lagrangian make the lagrangian invariant under the ( n=1 supersymmetry ) transformations : $$\delta \phi^i =\epsilon \bar{\psi}^i$$ $$\delta \psi^i = -i \gamma^0 \dot{\phi}^i \epsilon - \gamma^l_{jk} \bar{\epsilon} \psi^j \psi^k$$ please see the following article by luis alvarez-gaume ' , the supersymmetry operator can be written in terms of the canonical momenta : $$ \pi_i = g_{ij} \phi^j$$ as : $$q= i \pi_i \bar{\psi}^i - \gamma^0 \gamma_{ijk} \bar{\psi}^i \psi^j \psi^k $$ it can be easily checked that this operator generates the correct supersymmetry transformation given the canonical poisson brackets : $$\{\phi^i , \pi_j\} = \delta^i_j$$ $$\{\psi^i , \psi^*_j\} = g_{ij} ( \phi ) $$ the inclusion of the fermionic coordinates in the lagrangian gives spin to the particle moving on the riemannian manifold . this fact was discovered by berezin and marinov in 1975 , please see their original article ( they consider the case of flat space time ) . most importantly , when the theory is quantized , then if we add to the canonical quantization rules $$\pi_i \rightarrow i \frac{\partial}{\partial \phi^i}$$ canonical quantization rules for the fermionic coordinates $$\psi^i \rightarrow \gamma^i$$ i.e. , quantize the grassmann algebra to dirac matrices or clifford algebra ( please do not get confused with the gamma matrices in the classical action which must be treated as numerical coefficients ) , then the supersymmetry operator becomes the dirac operator ( in curved space ) . this is the reason why this action describes a spinning particle . also , the square of the supersymmetry operator is the dirac hamiltonian : $$ qq^{\dagger}+q^{\dagger}q = h = \pi_i \dot{\phi}^i + i g_{ij} \bar{\psi}^i \dot{\psi}^j - l$$ the four fermion term expresses the fact that in curved space , the dirac hamiltonian differs from the scalar hamiltonian . in differential geometry , this dirac hamiltonian is the laplacian on forms . one of the important applications of these types of actions is that they are used to provide quantum mechanical proofs of the various index theorems . now , it is not difficult to think the following generalizations . if supersymmetric quantum mechanics in $0+1$ dimensions describes a spinning particle , then a supersymmetric sigma model in $1+1$ dimension will describe a spinning string . in fact , witten used this observation to compute the index of the dirac operator on a loop space . one can think of the particles as probes for studying the geometry and topology of the spaces they are confined to move on . a classical particle can be used to study the geodesics . upon quantization , more information can be obtained for example the energies which constitute the spectrum of the laplacian can give topological and geometrical information . when the particle is given a spin , then even more topological information can be deduced due to these index theorems , for example spinning particles can see holes and handles in the manifold . specifically , the model under consideration can be used to prove the atiyah-singer theorem for the dirac operator index ( the difference between the number of zero modes of $q$ and $q^\dagger$ ) on a riemannian manifold and evaluate the result by means of the partition function path integral . please see the following article by friedan and windey $$ ind ( q ) = \int \mathcal{d} \phi \mathcal{d} \psi e^{i \int_{pbc} l dt}$$ ( pbc denotes periodic boundary conditions ) the zero modes of the dirac hamiltonian are just the harmonic forms which generate the de-rham complex of the manifold . finally , if we replace the particle by a string still we can probe much more topological and geometric information about the manifold .
simple answer : nothing is guaranteed 100% . ( in life or physics ) now to the physics part of the question . soft-answer : physics uses positivism and observational proof through the scientific process . no observation is 100% accurate there is uncertainty in all measurement but repetition gives less chance for arbitrary results . every theory and for that matter laws in physics are observational representations that best allow prediction of future experiments . positivism can overcome theological and philosophical discrepancies such as what is the human perception of reality . is real actually real type questions . the scientific process is an ever evolving representation of acquired knowledge based on rigorous experimental data . no theory is set in stone so to speak as new results allow for modification and fine tuning of scientific theory .
there is no " vectorlike " gauge theory in the standard model , and this is a consequence of naturalness . this means that all particles in the standard model are naturally massless , and the mass only comes from higgs mechanism . this is one of the great features of the standard model that is easy to break in any modification or extension . the teminology " vectorlike " comes from the 1950s , when people did not like 2-component spinors and thought that the world is fundamentally parity invariant . a " vectorlike " gauge field couples to a 4-spinor according to $\gamma^\mu a_\mu$ , while a " pseudovectorlike gauge field " couples to a 4-spinor according to $\gamma^5\gamma^\mu a_\mu$ . both are parity invariant , but in the first case , a is a vector ( meaning it changes sign under reflection ) , and in the second case it is a pseudovector . but the gauge fields in nature are neither vectors or pseudovectors , they are parity-violating . they couple as " v-a " meaning $ ( 1-\gamma_5 ) \gamma^\mu a_\mu$ , which is a projection operator to one two component part of the 4 component dirac spinor . this means that 4-component language is a little obfuscatory for this ( although 4 component spinor notation is still useful , becuase feynman trace identities are easier than fierz identities , and the 4-component notation most easily generalizes to higher dimensions ) . the point is that there is no parity , and the gauge fields are neither " vectors " or " pseudovectors " , they are parity violating vector fields which do not have a definite transformation under parity , because nature is chiral . so i would drop the " vectorlike " terminology , and use the term " naturally mass allowing " . a vectorlike gauge theory is " naturally mass allowing " because you can make the fermion massive . this means that the left and right partners have the same charges , and this can be considered an accident . the correct question is " why are all gauge theories in nature mass forbidding ? " this is true of all the fields on the standard model--- none of the right handed and left handed fields in the standard model can pair up to form a mass , because they are different su ( 2 ) multiplets and have different u ( 1 ) charge . why are they all unpartnered and charged ? there is a simple reason for this--- any field which can get partnered will have an arbitrary mass term in the lagrangian , and this term , without fine tuning , will end up generically being of order the planck mass . so the only fermions we see at low energies are those which are forbidden to have a mass , and therefore are chiral fermions without a partner to make a mass term with . further , all the fermions we see at low energies need to have a gauge charge , because without a charge of some sort , the fermion can get a majorana mass even without a partner , just by mixing with it is antiparticle . this is only forbidden if the particle is gauge charged in some way , so that the antiparticle has the opposite charge and the majorana mixing is forbidden . so all the fermions are chiral fermions with no partner to make a mass , so none of the low energy theories are vector-like . the simplest right way to formulate gauge theories in a parity violating universe is in terms of 2-component spinors , each with an independent coupling to a collection of gauge fields . this procedure can lead to an inconsistency , if there is an anomaly in one of the gauged symmetries , so there are global constraints on the type of chiral fermions and the representations they can be in . if none of the fermions have a partner , then the theory is natural , meaning " naturally massless " and the fermions can only get a mass from a higgs mechanism . the naturalness arguments say that the higgs mechnism must be the source of mass of all fermions in nature . but if the higgs is a fundamental scalar , the higgs itself can have a mass , and the naturalness argument fails for the higgs itself . so there is the question of why the higgs has an unnaturally light mass . this is the hierarchy problem .
the continuity equation without sink and sources reads $$\frac{\partial \rho }{\partial t} + \nabla \cdot ( \rho\vec v ) ~=~ 0 . $$ hint on how to derive it : establish first the integral form of the continuity equation for an arbitrary ( sufficiently regular ) 3d spatial integration region . next use the definition of the 3d divergence to argue the differential form of the continuity equation . the continuity equation can be rewritten with the help of a material derivative $\frac{d \rho }{d t}$ as $$\frac{d \rho }{d t} + \rho \nabla \cdot \vec v ~=~ 0 . $$ thus for an incompressible fluid $\nabla \cdot \vec v = 0$ , the density $\rho$ of a certain fluid parcel does not change as a function of time .
a brief history of the misapplication of magnetohydrodynamics to the analysis of the solar wind : 1959: soviet satellite luna 1 directly observed the solar wind for the first time and measured its strength . http://en.wikipedia.org/wiki/luna_1 so as of 1959 , by direct experimental observation , it was known that the heliopause was at least the radius of the earth or r⊙ . pneuman and kopp 1971 model : according to a more complex but still simplified mhd [ magnetohydrodynamics ] model of the coronal structure ( isp p . 114-117 etc . , the model of pneuman and kopp 1971 ) , the dipolar magnetic field lines form closed loops if they originate at solar latitudes of less than about 45° ( above or below the solar equator ) . however , those arising greater than about 45° are open field lines that may curve around the closed region to some extent but eventually extend far into space in all directions , at least beyond a heliocentric distance of about 2 r⊙ . http://www.mcgoodwin.net/pages/spacephysics_ess471.pdf ( page 36 ) this is the only paper i have been able to find that approximately reads on the magnetopause question . this is a highly cited paper and it is early enough to influence the expectations at the voyager launches ( 1977 ) . so i believe that the pneuman and kopp paper gave the expectation that the heliopaue would be at around 2 r⊙ based on mhd calculations . since this was a huge error , i have not been able to find any better detail . the man who developed mhd was hannes alfvén . he got the 1970 nobel prize in physics for this . his nobel prize lecture was partially dedicated to the task of claiming that his theory was being abused . in particular , he noted that the space physics situation was out of control . from his lecture , i have italicized the parts having to do with space physics predictions : plasma physics , space research and the origin of the solar system [ nobel prize lecture , 1970 , by hannes alfvén ] . . . the cosmical plasma physics of today is far less advanced than the thermonuclear research physics . it is to some extent the playground of theoreticians who have never seen a plasma in a laboratory . many of them still believe in formulae which we know from laboratory experiments to be wrong . the astrophysical correspondence to the thermonuclear crisis has not yet come . the reason for this is that several of the basic concepts on which the theories are founded , are not applicable to the condition prevailing in cosmos . they are " generally accepted " by most theoreticians , they are developed with the most sophisticated mathematical methods and it is only the plasma itself which does not " understand " , how beautiful the theories are and absolutely refuses to obey them . it is now obvious that we have to start a second approach from widely different starting points . if you ask where the border goes between the first approach and the second approach today , an approximate answer is that it is given by the reach of spacecrafts . this means that in every region where it is possible to explore the state of the plasma by magnetometers , electric field probes and particle analyzers , we find that in spite of all their elegance , the first approach theories have very little to do with reality . it seems that the change from the first approach to the second approach is the astrophysical correspondence to the thermonuclear crisis . . . . http://nobelprize.org/nobel_prizes/physics/laureates/1970/alfven-lecture.pdf the above lecture includes a table with a detailed comparison between the " first approach " and " second approach " . conclusion : the problem in estimating the heliopause was mostly due to theoreticians overestimating their understanding of the limitations of mhd . in particular , the mhd equations fail when electric currents are strong enough to overcome the magnetic field . this breaks the mhd assumption that ions and electrons remain pinned to magnetic field lines .
fields are the fundamental objects , and observable particles are their irreducible excitations . the particle content of a field theory can be inferred only from closer analysis . the bare particles which go into the description of the feynman diagrams ( and must already be renormalized to even make sense ) only tell part of the story . in the sense of an effective field theory , every classical theory can be quantized . see hep-ph/0308266 for a recent survey on effective field theories . but for a ''fundamental'' theory one usually requires renormalizability , which drastically restricts the allowed theories . ( but see also : j . gomis and s . weinberg , are nonrenormalizable gauge theories renormalizable ? http://arxiv.org/pdf/hep-th/9510087 ) general relativity is one of the classical theories that can be successfully quantized as an effective field theory ; see , e.g. , p . burgess , quantum gravity in everyday life : general relativity as an effective field theory living reviews in relativity 7 ( 2004 ) , 5 http://www.livingreviews.org/lrr-2004-5 but it is not perturbatively renormalizable , which makes many people search for a more fundamental way of quantizing gravity .
most higher derivative theories —and in particular lee-wick 's model— do not have ghost excitations but they are unstable ( hamiltonian unbounded from bellow ) . yes , almost everyone says the opposite but all them are unfortunately wrong . they do not quantize the theory properly . whenever a degree of freedom has negative energy at the classical level , it must have negative energy at the quantum level as well . people try to fix the problem of negative energy exchanging the frequencies between creation and annihilation operators at the price of introducing ghosts . but this is not the right way of quantizing the theory because the classical limit is totally wrong . one may canonically quantize the harmonic oscillator with an additional higher derivative term to convince himself of what i am emphatically claiming . anyway , either one quantizes the theory correctly getting an unstable theory or one quantizes the theory incorrectly getting a theory with negative norm states , the quantum theory does not make any sense . it is not a quantum theory . is there any general mathematical theorem by which we can show that a nth order derivative theory can be quantized into n different kind of particles ? the classical and the quantum theory must have the same number of degrees of freedom ( dof ) . the classical theory has half dof of the number of initial conditions must be given to determine a solution . in a normal field theory —let 's say klein-gordon— one must specify initial value of the field and momentum ( or velocity ) for every field . thus one has one dof for a real field , two for a complex field , four for two complex fields . when one adds higher time derivatives , one requires more initial conditions to know a solution ( initial accelerations , initial fourth derivatives , etc ) . when there are constrains the counting of degrees of freedoms is a little bit more subtle . for instance , electrodynamics is a second-order theory and the electromagnetic four-potential $a_{\mu}$ has four components so one could naively think that the theory has four degrees of freedom , but this is not true because there are gauge redundancies ( there are first class constraints ) that make different ( gauge related ) $a_{\mu}$ correspond to the physical situation . so that the theory has only two dof corresponding to the two polarizations of electromagnetic waves . regarding the quantum theory , the number of degrees of freedom corresponds to the number of particles ( each physical polarization counts as a particle ) .
the electric field $\vec{e}$ builds up because of the misbalance of charge on the left and right side of your model . to a test charge $q$ , which we place in the middle of the charge clusters , the left side cluster appears more negatively charged than the cluster on the right side . thus you can already distinguish between the cathode ( left , negative ) and the anode ( right , positive ) of the system . what you constructed is just a very simple form of a capacitor . so your question is the same to : towards which direction do electrons flow in a capacitor ? the force on a test charge is $\vec{f}=q\cdot\vec{e}$ . it is parallel to the electric field . but your question was concerned about the direction . one could try to answer it simply with the coulomb force , which tells us that , equally charged particles are repelling . the anode will exert a less repelling force on the test charge than the cathode . thus the direction is towards the anode .
there is a vertex with one gluon , and also a vertex with two gluons .
no , they do not all have the same voltage drop . if they were in series , however , they would . by ohm 's law , the voltage drop is proportional to the current flowing through a resistor . ( so in several series resistors with the same resistance , the drop across each one is the same , since the current across each one is the same ) . however , because b and c are in parallel , the current is split between those two , meaning that the current in b and c is different from that passing through a . however , since b and c have the same resistance , you know that the current from a is split 50/50 between them , so since each one gets half of the current , the voltage drop across each one is half of the voltage drop across a . to find the current , first find the equivalent resistance using parallel and series resistor simplification techniques . condense the two parallel 10 ohm resistors into a single resistor , and then combine this resistor with the series 10 ohm resistor . this tells you the total ( or " equivalent" ) resistance of the circuit . since you know the voltage produced and the resistance of the circuit , you can use ohm 's law to find current .
in classical electrostatics , gauss ' law can be used to derive the relationship between the electric potential , $\varphi$ for a homogeneous medium ( constant permittivity , $\epsilon$ ) and volume charge density , $\rho_{v}$ in the form of the poisson equation , which , in cartestian co-ordinates , is given by : $$\nabla^{2}\varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}=-\frac{\rho_{v}}{{\epsilon}_{r}{\epsilon}_{0}} , $$ where ${\epsilon}_{r}$ is the relative permittivity ( dielectric constant ) for the homogenous medium . now , for an ionic solution at thermal equilibrium and at temperature $t$ , the charges are uniformly distributed . under the effect of an electrostatic field , the positive ions are attracted towards the negative electrode and negative ions attracted towards the positive electrode . furthermore , positive ions become repelled by other positive ions and similarly negative ions become repelled by other negative ions , until a new equilibrium is reached . at equilibrium , the ions are distributed with various energies , $e$ given by the maxwell-boltzmann distribution law , in which the probability of a particle having energy $e$ is proportional to $\exp ( -\frac{e}{kt} ) $ where $k$ is boltzmann 's constant and $t$ is the ( absolute ) temperature [ in kelvins ] . if $z_{i}$ is the number of charges of the $i^{th}$ ionic species , then its electric potential energy is $z_{i}e\phi$ where $e$ is the elementary electric charge ( $e = 1.602\times10^{-10}$ coulombs ) . the concentration ( number density ) of the $i^{th}$ ionic species at position $\textbf{r}$ is then given by : $$n_{i} ( \textbf{r} ) =n_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) , $$ where $n_{i}^{\infty}$ is the number concentration of the $i^{th}$ ionic species in the bulk solution . the volume charge density is therefore : $$\rho_{v} ( \textbf{r} ) =\sum\limits_{i=1}^nz_{i}en_{i} ( \textbf{r} ) =\sum\limits_{i=1}^nz_{i}en_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) . $$ which , when substituted into the poisson equation , gives the poisson-boltzmann equation for potential of an ionic solution : $$\nabla^{2}\varphi=-\frac{1}{{\epsilon}_{r}{\epsilon}_{0}}\sum\limits_{i=1}^nz_{i}en_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) . $$ this is a non-linear partial differential equation , the solution of which is dependent upon the specific geometry and properties of the electrolyte . for the case of an infinite sheet ( plate ) electrode located in the y-z plane at the origin , with the potential of the plate $\phi=\phi_{0}$ at $x=0$ and the potential in the solution $\phi\rightarrow0$ , $\frac{d \phi}{dx}\rightarrow0$ as $x\rightarrow\infty$ , the solution , for low potential , $\phi$ , that is , $\lvert\frac{z_{i}e\phi}{kt}\rvert\ll 1$ , the poisson-boltzmann equation becomes linearized to $\nabla^{2}\varphi=\kappa^{2}\phi$ ( the debye-hueckel equation ) which has the solution : $$\varphi ( x ) =\varphi_{0}\exp ( -\kappa x ) $$ where $\kappa=\sqrt{\frac{2z^{2}e^{2}n}{\epsilon_{r}\epsilon_{0}kt}}$ . for the non-electrostatic case ( ie : current flow and/or ion flow ) , the ions in the solution will undergo transport under the effect of the applied electric field . these ions will initially accelerate up to a speed , $s$ , limited by the hydrodynamic properties of the solute ( stokes law ) . we can calculate the hydrodynamic force , $f_{h}$ exerted on an ion of radius $r$ , travelling at speed , $s$ through the solute of density , $\rho$ and viscosity , $\eta$ to get $f_{h}=6\pi\eta rs$ from which the stokes-einstein equation for the diffusion coefficient is derived : $d=\frac{kt}{6\pi \eta r}$ from the nernst equation we can calculate the electrochemical potential of an ionic species from ionic concentration ( activity ) gradients present in solution . when coupled with the conservation of mass , we get the nernst-planck equation : $$\frac{\partial n_{i}}{\partial t}= \nabla \cdotp \lgroup d_{i} ( \nabla n_{i}+\frac{q_{i}n_{i}}{kt}\nabla \phi ) \rgroup . $$ of course , these models include assumptions which may not always be valid , such as the use of stokes law or the assumption of non-interaction of ions . more accurate numerical models may require the use of empirical data to account for these and other effects , including chemical reaction kinetics .
no . there are two different types of an angular momentum . first is connected with the coordinate representation , so it can be interpreted from classical mechanics point of view . second is not connected with coordinate representation , but it exist in every particle of the free field ( i.e. . , is an own angular momentum ) which you have tested . they are the principal different types of an angular momentum . from the qm position , they both are the eigenvalues of the representations of 3-rotation generator , but first refers to reducible , and second - to irreducible representations . maybe it is more convenient for you to compare the spin and electrical charge . mainly we do not ask about origin of charge and do not interpret it as the result of other quantity . it is independ quantity , and it is existence leads to electromagnetical interaction . also , the existence of spin ( it is value ) leads to some spin interaction ( simplistically can imagine as result of fermi-dirac or bose-einstein statistics . also , we can make an analogy between the quantum spin of particle and an own angular momentum ( classical spin ) of the system of particles . first and second are not connected with motion the particle ( or system ) as whole .
have a look at the reference cited in this answer : can the kramers–kronig relation be used to correct transfer function measurements ? in a nutshell here is the point : because of causality , in the time-domain , the permittivity , conductivity , etc must be real but non-local . in the fourier domain , they must be complex . the nonlocality expresses itself as a convolution integral in the time domain . this is easier to write in the fourier domain as a multiplication ; as a result this often leads to misleading abuses of notation involving time-domain fields in the same equation as fourier domain optical constants . the answer indicated above has a link to a tutorial on this subject . oh and the phase delay applies to all material , not just dipoles , this is causality .
he gets the kinetic energy very easily as the sum of the kinetic energies of the individual particles ( the sum going over to an integral in the limit ) . let $\eta_i , i=1,2,3$ be the components of the displacement vector ( each \eta_i = $\eta_i ( x , y , z ) $ being a function of position ) . so the kinetic energy density is ${\cal t}= ( \mu_0/2 ) ( \dot{\eta}_1^2+\dot{\eta}_2^2+\dot{\eta}_3^2 ) $ , where is the equilibrium mass density . naively , the statistical model is a bunch of non-interacting point particles racing around , bouncing off the walls of the container . ( for simplicity , ignore gravity . ) just to clarify , it seems you understand it , the " particles " in these two cases are different . in the second they are just molecules , in the first they are macroscopic pieces of gas , small volumes . as for your question getting from microscopic energy to the macroscopic . i will show how do they related . $e_\text{mic} = \sum_{i=1}^n \frac{1}{2} m \boldsymbol c_i^2$ in that approach we will have to deal with $n$ molecules , that is like finding individual trajectories of all the molecules , which we do not want . there should be also the term to reflect either the interaction of molecules ( collisions ) either with each other or with the walls , but it does not matter right now . to reduce the number of degrees of freedom we introduce one-particle distribution function : $$f ( \boldsymbol r , \boldsymbol c , t ) $$ the quantaty $f ( \boldsymbol r , \boldsymbol c , t ) \ ; d\boldsymbol r d\boldsymbol c$ tells us how many molecules are there in the phase space element $d\boldsymbol r d\boldsymbol c$ $$e_\text{mac} ( \boldsymbol r , t ) = \int \frac{1}{2} m \boldsymbol c^2 f ( \boldsymbol r , \boldsymbol c , t ) \ ; d\boldsymbol c$$ as you remember we want to express macroscopic energy in terms of macroscopic kinetic energy , i.e. we need to introduce macroscopic speed $\boldsymbol v$: $$\rho ( \boldsymbol r , t ) = \int m f ( \boldsymbol r , \boldsymbol c , t ) \ ; d\boldsymbol c$$ $$\rho \boldsymbol v ( \boldsymbol r , t ) = \int m \boldsymbol c f ( \boldsymbol r , \boldsymbol c , t ) \ ; d\boldsymbol c$$ then $$ \begin{multline} e_\text{mac} ( \boldsymbol r , t ) = \underbrace{\int \frac{1}{2} m ( \boldsymbol c - \boldsymbol v + \boldsymbol v ) ^2 f ( \boldsymbol r , \boldsymbol c , t ) \ ; d\boldsymbol c}_{\text{averaging microscopic kinetic energy}} = \\ \underbrace{\frac{1}{2} \rho \boldsymbol v^2}_{\text{macroscopic kinetic energy}} + \underbrace{\int \frac{1}{2} m ( \boldsymbol c - \boldsymbol v ) ^2 f ( \boldsymbol r , \boldsymbol c , t ) \ ; d\boldsymbol c}_{\text{macroscopic potential energy}} \end{multline} $$ the part with $\boldsymbol v - \boldsymbol c$ was omitted for good reasons , but going into details would be too much , i have already essentially telling the kinetic theory . for the details you can consult appropriate books . so , we have extracted the macroscopic kinetic energy , what is left is macroscopic potential energy , pressure if you want . to put it another way , this suggests that in a gas made up of non-interacting point particles , with no external forces except for the hard-wall forces , sound waves could not propagate ( since the lagrangian density would reduce to the kinetic part ) . that does not seem right . ideal gas model does not assume particles to be non-colliding , it says that the potential energy of the molecular interaction can be neglected . abrupt hard wall potential with delta functions have zero overall energy . you can view gas made of non-interacting particles only for the equilibrium case . and even then you have to consider collisions with the walls . for the non-equilibrium case you have to take into account collisions . if they were not interacting , indeed , there would be no sound .
this document ( nb it is a pdf ) contains details of the beam operation . here 's a key graph nabbed from the presentation : at the end of an experimental run the beam is dumped , and it takes about an hour and a half to get the beam back up to full energy and intensity . once the beam is at full strength the lhc generates data continuously for somewhere between 10 and 20 hours before the beam intensity is too low and the beam needs to be dumped again . note that the lhc is not an experiment that runs once and generates one result , then repeated to generate a second result and so on . once the beam is live it generates data continuously and this data builds up for days and months . because signals like the higgs are so weak you need months and months worth of data , i.e. months and months of beam time , to get enough data to see these small signals . cern have made an animation showing how the higgs signal built up over time , which you can see here .
arriving at the same answer as quantum mechanics for one particular scenario by making a bunch of ad hoc assumptions ( for example - the calculation did not work , so we will make the orbital planes perpendicular ) is not useful . qm allows you to calculate much more than the ground states of atoms . any competing theory - and that paper does not contain anything which could be described as a theory - would have to have the same breadth of applicability as qm
in a word , no . the drift velocity is always very small in common circuits ( ~$10^{-4}cm/s$ ) . in this scenario , when the plates are connected , the electrons still travel slow but all of the electrons along the wire start moving almost at the same time , so even though they move slowly , the electric charge on the plates will vanish quickly . the electrons on the negative plate move into the wire and the positive plate is filled with electrons that were previously in the wire right next to it . this is the difference between the speed of current and the speed of the electrons . current travels very fast ( it is like the speed of sound in the free electron sea ) . electrons drift very slow .
yes , the resistance to the magnetic field is called reluctance . as the magnet moves through a copper coil ( consider circular ) , the change in magnetic field induces current in the coil . due to the current in the coil , another magnetic field is produced in the opposite direction to the magnet moving through the copper coil . also , the magnetic strength of both the field are same . but , the directions are exact opposite .
here is an explanation by bo danforth : shown in the picture above is a segment of a wurlitzer jukebox bubble tube . in the tube , the bubbles rise as expected , but as they approach the top , odd things begin to occur . instead of remaining the same size as one might think , or increasing slightly from a minute reduction in pressure , they instead decrease in size , in some cases disappearing entirely . the reason for this bewildering sight is that the bubbles do not actually contain air , but are pockets of heated vapor . the bubble tube is a sealed system where the air has been completely removed by a pump , creating a partial vacuum , which causes the liquid to fill the remaining space with vapor . the system is at a pressure where it will change states near room temperature . when this sealed tube is placed next to a heat source at the bottom ( the light bulb ) the liquid near it reaches a boiling point , according to the equation of pv=nrt . as the bubbles of vapor float upwards along the exposed glass and away from the heat source , they slowly shrink as the gas cools down , condensing back into a liquid state .
consider a solid rod made of a glassy substance , and model it as a set of atoms in random locations , held together by randomly oriented harmonic springs ( enough so that the graph is rigid ) . the hamiltonian of this system is in principle diagonalizable , and since all the springs are harmonic , the potential energy is quadratic in all the atomic coordinates , so the whole system is equivalent to a set of uncoupled harmonic oscillators , which are of course the normal modes . the lowest frequency mode will be rod flexing back and forth with two nodes , the second will be the second overtone with three nodes , and so on , but importantly this set of normal modes goes all the way up to modes with a number of nodes on the order of the number of atoms in the rod . none of these normal modes has a precisely defined wavevector , because of the lack of periodicity in the glass , but they do have approximate wavevectors , and of course they have precise frequencies corresponding to the eigenvalues of the hamiltonian . one weird thing about this that seems completely different from the usual treatment of phonons in crystals is that all these modes are standing waves - they do not propagate in a particular direction , and their approximate wavevectors are only defined up to sign . this is actually the case for a finite-size crystal as well - no propagating waves can actually be eigenstates , and instead the boundary conditions at the ends of the crystal cause them to mix into standing waves that are the exact physical eigenstates . the only reason we introduce periodic boundary conditions and talk about the propagating waves in crystals is that it is so much more convenient . of course , if you create a wave packet at one end of the material , either crystal or glass , you can always express that in whatever basis of eigenstates you want , and as they evolve the packet will end up moving through the crystal and spreading out according to some dispersion relation . i do not know how you would actually calculate that dispersion relation for a glass ( other than brute-force computation ) , but it is possible in principle . the same considerations also apply to quasicrystals , but with the interesting addition that there are now diffusively propagating modes called phasons with long relaxation times . 2 phonons in a gas is a really weird thing to think about because in an ideal gas , the particles are assumed to be non-interacting , and they have to be in a thermal distribution of single-particle quantum states for it to be a gas ( rather than a bose-einstein condensate or something ) . if the gas particles are delocalized , and do not interact with each other , then what the heck is a phonon ? yet sound waves in a gas obviously exist , so the question remains whether they are quantized or not . i can not answer this part of the question .
first of all , an observed , which is a material body , cannot reach the speed of light in vacuum since it requires an infinite amount of energy . second , you can not observe electrons , protons or quarks . there is something called the uncertainty principle , it states that you can not be sure about both the location of a particle and it is momentum at the same time . if you were sure where an electron , for instance , is located then you can not be sure about it is momentum that is where it will go and so you can not observe electrons or other particles . further information about the uncertainty principle : [ 1 ] third , let 's suppose the observer goes at $c$ , and that he can observe electrons , then we know from special relativity that length is contracted relative to the observer , and we can calculate that contraction using this formula : $$l'=l_0 \times \sqrt{1-\frac{v^2}{c^2}}$$ where $v$ is the speed of the observer . in your example the observer is moving at $c$ , so : $$\begin{align} l ' and =l_0 \times \sqrt{1-\frac{c^2}{c^2}}\\ and =l_0 \times 0\\ and =0 \end{align}$$ so the observer will see everything being contracted to a length of $0$ , this means that he will see nothing . can you see something that has $0$ length , no volume ? no . so he will not be able to observe what is inside the atom .
i think a good , and classic , reference for your case is the following , introductory functional analysis with applications the very last chapter of kreyszig deals with quantum mechanics . and , once you have learned how to " translate " the language of functional analysis into that of quantum mechanics , you can go to more advanced texts in specific topics .
suppose you have some collection of matter that is so dense it has an event horizon where the escape velocity is greater than the speed of light . the escape velocity is obviously due to the strong gravitational field of the matter inside the event horizon , and equally obviously that matter is also pulled by it is own gravity towards it is centre of mass . also obvious is that because the surface of your collection of matter is nearer to the centre of mass than the horizon is , the gravitational pull on it must be even stronger than the gravity at the event horizon i.e. the ( hypothetical ) escape velocity would be even faster than the speed of light . the reason why this situation is not stable is that the matter making up your object cannot resist the force of it is own gravity and is irrestably pulled inwards until it forms a singularity . at that point we have a standard black hole with an event horizon and a singularity at the centre . to understand why the matter within the event horizon cannot avoid being pulled down into a singularity you have to do some maths . if you are interested my answer to why is a black hole black ? gives a hopefully not too scary explanation of the maths . i think there is a semi-plausible way to explain why the matter can not avoid collapsing into a singularity , but do not take this too literally . i have mentioned above that if the escape velocity at the event horizon is the speed of light , the escape velocity inside the event horizon must be faster than light . but all forces , e.g. the electrostatic forces that hold you in shape , propagate at the speed of light . that means inside the event horizon the electrostatic force can hold matter in shape because it can not propagate outwards fast enough . this also applies to the weak and strong forces , and the end result is that no force is able to resist the inwards fall of the matter into a singularity .
i am guessing this is related to the archaic paris inch , which is $27.069$mm , i.e. $10^8 \times$ the conversion factor . reference : scientific papers vol 2 1881-1887 , john william strutt .
for the complete quantum processors , there is not much progress . in 2001 , we can factorize the number $15$ using nmr implementation . now , eleven years later , we can factorize the number $21$ . surely , the first experiment is to demonstrate the practicality of the quantum computer so it uses only the simplest implementation that is not necessary scalable . the development after that are more focus on different components that can be scalable when combined together . there are many different implementations and small variations can change the performance pretty much so it is hard to classify them . the review article of quantum computers has listed the requirements of quantum computer and the pros and crons for each implementation . the following shows the current data for coherence time , number of qubits and storage time . it should be noticed that the 1 . coherence time the table 1 in the review articles " quantum computers " ( 2010 ) gives the following values : $t_2\approx 25 s$ for nmr ( longest one ) $t_2\approx 15 s$ for trapped ions $t_2\approx 0.1 ms$ for infrared photon where $t_2$ is a measure of decoherent . 2 . scale of quantum computing the table 1 in the review articles " introduction to quantum algorithms for physics and chemistry " ( 2012 ) shows that the scale of two implementations : 6 qubits for trapped ions 4 qubits for nmr 2 qubits for quantum optics , but 6 quantum walks steps note that quantum walks is alternative implementation of quantum computer . 3 . storage time in quantum memory the table 1 in the review articles " quantum memories " ( 2010 ) shows storage time and the progress of different implementations of quantum memory .
you may simply replace the arguments $ x \to x+vt $ in all of those commutators . the way to prove that is to go to momentum space , where $\rho ( k , t ) = \rho ( k , 0 ) e^{i k v t} $ . proving that about the commutator $ [ \rho ( x , t ) , \psi ( x&#39 ; , t&#39 ; ) ] $ is a little trickier , because we do not know the explicit form of $\psi ( t ) $ , but the way i did it was to use $$ [ \rho ( x , t ) , \psi ( x&#39 ; , t&#39 ; ) ] = u^\dagger ( t&#39 ; ) [ \rho ( x , t-t&#39 ; ) , \psi ( x&#39 ; , 0 ) ] u ( t&#39 ; ) $$ where u is the time evolution operator . this way only the time evolution of the density is needed .
do not forget that the polarization tensors depend on the gauge choice via reference vectors ( call them $q$ , $q'$ ) now you have to check what happen when you chance the reference vectors from $q , q'$ to some new vectors $r , r'$ . the change of the vectors will lead to the new polarization vectors aquire a term proportional to its momentum $p$ . $$\epsilon ( p , r ) ^\mu \sim \epsilon ( p , q ) ^\mu+p^\mu$$ the contraction of the last term with $m_{\mu\nu}$ vanishes , i.e. you have shown gauge invariance . do you also have to show that $m_{\mu\nu}$ contracted into one of its momenta vanishes ?
the answer is in physics , but not ohm 's law . the lights are dimmed when you turn on the headlights because the car assumes that if the headlights are on , then the cabin is dark . if the cabin is dark , then control lights need to be dimmed so they do not distract the driver . this usually works just fine , unless you turn on the lights when it is not all that dark . the physics of this is called " dynamic range " - the lights will look brighter when you do not have the sun shining around them . in most cars , you can adjust the brightness . check your manual .
all of the ice core methods of measuring temperature in past millenia from gas trapped in bubbles in ice , and measuring concentrations , depend on the fact that permeability is small even in imperfect containers , as the ice in the glaciers . so the answer to " how long " would depend on the exact materials and geometry and temperature , and it will be in any case the gas will disappear if you wait long enough , and long is more than a few million years . there is of course the sublimation of the molecules of the flask which will eventually make holes no matter how perfect the material , but also the quantum mechanical " tunneling effect " will come to play a role if time is long enough .
your question can be translated into " if right now we would send a powerful omnidirectional light pulse from earth into space , would there be galaxies that never see this light pulse ? " the answer is " yes " . due to the accelerated expansion of the universe , as described by the lambda-cdm model , only galaxies currently less than about 16 billion light years ( the difference between the cosmological event horizon and the current distance to the particle horizon ) away from us will at some time observe the light pulse . a nice visual representation of this can be found in figure 1 of this publication .
you need to be careful what you mean by a force in general relativity . the usual definition of a force is that you get a non-zero reading on an accelerometer you are holding , but this can lead to some surprising conclusions . to illustrate this suppose you are falling towards some massive body ( with no atmosphere to complicate the issue ) . does the gravity of the massive body create a force ? the answer has to be no , because you are in free fall so you are weightless and any accelerometer you were carrying would read zero . suppose know we give you a harness and tie you to some support fixed wrt the massive body . now you feel a force , and your accelerometer reads non-zero . but this force is due to the fact you have been restricted ( by the harness ) from following the geodesic you would otherwise follow . it is the harness that exerts the force on you ( and you on it ) because it is pulling you away from geodesic motion and therefore imparting a non-zero four acceleration . the force is not due to gravity , it is due to the harness and without the harness there will be no force . incidentally , although it is peripheral to this issue there is a nice calculation of the four acceleration and force in twistor59 's aswer to what is the weight equation through general relativity ? . let 's go back to expanding spacetime . hopefully you will now see why we say that the expansion of spacetime does not create a force . suppose we place you and me at some distance apart in an frw universe and constant comoving position and we wait to see what happens . we will each have an accelerometer so we can tell if we are accelerating . if we now wait the expansion of spacetime will increase the proper distance between us - that is , we will move apart . however because both of us are at constant comoving position we are moving along a geodesic and experience no acceleration . our accelerometers will read zero , which means we feel no force . in this sense the expansion of spacetime does not produce a force . this is exactly analogous to the claim i started out with , that the gravity of a massive body does not create a force either . now suppose we tie ourselves together with a rope . once we have done this we cannot remain at constant comoving position and this means we must be accelerating . our accelerometers would now register a non-zero acceleration towards each other and we had feel a force . any objects we drop will fall away from us . this is exactly analogous to using a harness to support yourself against the gravity of a massive body . it is the rope between us that generates a force not the expansion of spacetime . by now you are probably thinking that this is all a bit of a swindle and i have just redefined what is meant by force to make it zero . well , yes , but this is key to understanding general relativity . we do not often talk about force , but four acceleration is precisely defined in gr and can be calculated as described in the question i linked . when a general relativist talks about force they implicitly mean four acceleration . this does mean they are using the term in a different way to the general public - hence the confusion .
in the restricted three body problem , where you consider two objects orbiting each other , such as the sun and earth , and the motion of a third object that does not affect the movement of the first two , but is affected by their gravity , you can sort of figure out how far/fast from one object you have to be to not be orbiting it anymore . the picture above is taken from shane d . ross ' ph . d . thesis . depending on the total energy of the third mass , it will never be able to go into the shaded areas . so if you are orbiting earth , which would be $m_2$ in the sun-earth example , or $m_1$ in an earth-moon one , there is a minimum energy at which can break out of the first and start orbiting the other body . the transition point is the lagrangian point $l_1$ . at a higher energy , it is possible to break away to infinity from both objects , the transition point corresponding to the lagrangian point $l_2$ . so depending on a more precise definition of you question , a possible answer is that a satellite beyond the sun-earth l1 point is more orbiting the sun than the earth . the sun-earth l1 point is , according to this , about 1% of the way to the sun . so that is about 1,500,000 km . you could of course calculate the corresponding $e_1$ enery and translate that to kinetic energy and velocity .
you do not want $1/r$ ( although technically it means the same ) but rather the full curvature term : $\delta p=\sigma \kappa$ . in fact you will get a source term in the navier-stokes equations that looks like this : $$\sigma \kappa \delta ( n ) \mathbf{n} $$ where $\delta ( n ) $ is the dirac delta function that only has a value at the interface and $\mathbf{n}$ is the interface normal . the curvature $\kappa$ can be written as the divergence of the unit interface normal : $$\kappa=\nabla \cdot \mathbf{\frac{n}{|n|}} $$ apart from the source term you indeed also have boundary conditions on the interface which are basically the standard free slip condition and a jump for the normal stress coming again from the laplace pressure . there is a good explanation of these in the first part of the seminal work on fluid-fluid cfd by brackbill . if you are interested in the curvature itself , i think slides 22-28 of this course on wetting are probably also a good source to take a look at for more background .
field theories are nonlinear because the quantum fields satisfy nonlinear dynamical equations . but renormalization does not make quantum fields into a nonlinear functional of test functions . the wightman distributions are , by definition , linear functionals of the test functions , and wightman distributions always encode renormalized fields . ) instead it changes the space of test functions to one where the interacting quantum fields are perturbatively well-defined . this gives a family of representations of the field algebra depending on an energy scale . all these representations are equivalent , due to the renormalization group , and the corresponding wightman functions are independent of the renormalization energy . ( in simpler , exactly solvable toy examples that need infinite renormalization , this can actually be checked . ) the dependence on the energy scale would not be present if contributions to all ordered were summed up ( though nobody has the slightest idea how to do this nonperturbative step ) . the energy scale is simply a redundant parameter the influences the approximations calculated by perturbation theory . the renormalization group is an exact but unobservable symmetry ( just like gauge symmetry ) that removes this extra freedom , but as computations in a fixed gauge may spoil gauge-independence numerically , so computations at a fixed energy scale spoil renormalization group invariance numerically . note that wightman functions are in principle observable . indeed , the kadanoff-baym equations , the equations modeling high energy heavy ion collision experiments . are dynamical equations for the 2-particle wightman functions and their ordered analoga .
the weight of liquid impact will be minimal ( since with a screw cap the force will be redirected along the screw threading ) - it is the lubricating effect of the liquid that actually helps . you start unscrewing , the liquid gets into the threading and lubricates it .
imagine your rod is made up from lots of little bricks stacked on each other . then the total potential energy is the sum of the potential energy of each brick . the diagram shows the rod and one of the bricks of size $dx$ and at a height $x$ . if $\rho$ is the mass per unit length , then the mass of the brick is $\rho \space dx$ , and the potential energy is $\rho \space dx \space g \space x$ . to get the total potential energy we just have to sum up the potential energies of all the bricks . to do the sum we let $dx$ go to zero and replace the sum with an integral so : $$ \begin{align} u and = \rho \space g \int_0^h dx . x \\ and = \rho \space g \left [ \frac{x^2}{2}\right ] _0^h \\ and = \frac{\rho g h^2}{2} \end{align} $$ and since $\rho h$ is just the mass $m$ this gives us : $$ u = \frac{mgh}{2} $$ as you say , this is the same result you get by just considering the centre of mass , but note that we have got this result without involving the centre of mass at all . i think this is a better way to understand why the potential energy is $mgh/2$ without just invoking the centre of mass and waving your arms in the air .
no . consider any state with a momentum wavefunction symmetric about zero . it is position-space and momentum-space norm-squared probability distributions are not changed by time-reversal , even though the wavefunction clearly is . here is an explicit example . take the four gaussian wavepacket of mean positions $x_0$ or $-x_0$ , mean momenta $p_0$ or $-p_0$ , and spatial spread $\sigma$: $\psi_{ ( \pm_x , \pm_p ) } ( x ) \propto e^{- ( x \mp_x x_0 ) ) ^2/4\sigma^2 - i x ( \pm_p p_0 ) }$ . ( here , $\pm_x$ and $\pm_p$ are two binary variables taking the values of plus or minus . $\mp_x$ and $\mp_p$ are their opposites . ) now consider the two superpositions $\phi_{\mathrm{away}} = \psi_{ ( + , + ) } + \psi_{ ( - , - ) } $ , $\phi_{\mathrm{toward}} = \psi_{ ( + , - ) } + \psi_{ ( - , + ) } $ . $\phi_{\mathrm{away}}$ is the superposition of two wavepackets separated by $2 x_0$ traveling away from each other with relative speed $2 p_0/m$ . $\phi_{\mathrm{toward}}$ is the same with the packets traveling toward each other . one can check that $\vert \phi_{\mathrm{toward}} ( x ) \vert^2 = \vert \phi_{\mathrm{away}} ( x ) \vert^2$ and $\vert \tilde{\phi}_{\mathrm{toward}} ( p ) \vert^2 = \vert\tilde{\phi}_{\mathrm{away}} ( p ) \vert^2$ . i do not know if there are examples other than with time-reversal .
it seems , the main reason is politics . the movement towards prohibition of nuclear tests just started . facility of this kind is an ideal polygon for nuclear tests . few hundred explosions per year plus mass production would result in few orders of magnitude cheaper and more effective weapons automatically . that time it was not a good idea to boost development of nuclear weapons that much . it was still too complicated for average countries and everyone wanted to postpone the time when these average countries get an access to nuclear weapons .
starting in the fifties , there was a lot of work ( see rdd-8 , v.c. 1 . g ) trying to build a pure fusion weapon for mainly two reasons : they promised to be cleaner than conventional thermonuclear devices ( important for peaceful uses and some of the not-so-peaceful ones ) and they would not need relatively scarce fissionable materials . as you can use staging to scale to essentially unlimited yields , the problem was reduced to making the smallest possible fusion explosion ( early steps toward inertial fusion energy , p . 1-2 ) . eventually this program transformed into inertial confinement fusion research . the required energy to implode this " secondary " was originally estimated in approximately 1 mj , but this result assumed an ideal driver/primary . after the failure of over-optimistic attempts to get ignition with smaller drivers , a test program called halite/centurion was carried out to induce ignition of icf capsules using radiation from nuclear devices . this program was successful " putting to rest fundamental questions about the basic feasibility of achieving high gain " ( progress toward ignition and burn propagation in inertial confinement fusion ) . the exact results of this test program are still classified , but it seems that " some dozens mj of driver energies " ( edward teller lectures , p . 6 ) were required to reach ignition with x-rays from fission primaries . it sounds reasonable to assume that a more controllable driver can reach ignition with a smaller amount of energy and nif is trying to reach ignition using only 1.8 mj of driver energy .
in that case , by symmetry , if you take any loop of the coil , for every field line that goes through it , if it goes to the negative pole , there must be another symmetrical that comes from the positive , so the total flux will be $0$ , and constant , so there will be no current induced .
edit the method you want to use is ok , and gives a quick result . here it is : $$i=\int\prod d\theta^{*}d\theta\theta_{k}^{*}\theta_{l}exp ( \theta^{*}b\theta+\eta^{*}\theta+\theta^{*}\eta ) =\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \int\prod d\theta^{*}d\theta exp ( \theta^{*}b\theta+\eta^{*}\theta+\theta^{*}\eta ) $$ from where it follows that $i$ is equal to $$\left . i=\mathrm{det}b\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \mathrm{exp} ( \eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) \right\vert_{\eta^{*}=\eta=0}$$ $$i=\mathrm{det}b ( b^{-1} ) _{kl}$$ note : i use the shift $\theta_{i}\rightarrow\theta_{i} + ( b^{-1} ) _{ij}\eta_{j}$ and $\theta_{i}^{*}\rightarrow\theta_{i}^{*} +\eta_{j}^{*} ( b^{-1} ) _{ji}$ . here i used the fact that the second moments can be calculated as derivatives in the following way . $$\left . \langle\eta_{l}\eta_{k}^{*}\rangle=\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \mathrm{exp} ( \eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) \right\vert_{\eta^{*}=\eta=0}= ( b^{-1} ) _{ij}$$ more or less , you can take this as a definition . but if you still want to prove this , do the following : $$j=\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \mathrm{exp} ( \eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) =\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \prod_{ij} ( 1-\eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) $$ after straight forward differentiation you get $$j=\prod_{ij}\delta_{kj}\delta_{li} ( b^{-1} ) _{ij}= ( b^{-1} ) _{kl}$$ you can ignore whats below the line , that was my first answer . but i will leave it because it may be instructive for others . since i cannot comment yet , i will sketch a proof for a simpler case $$i=\int\prod_{i}d\theta_{i}^{*}d\theta_{i}exp ( \theta_{i}^{*}b_{ij}\theta_{j} ) =det ( b ) $$ and hope that this will help you compute your integrals . after expanding the all the exponential we arrive at $$i=\frac{1}{n ! }\int d\theta_{1}^{*}d\theta_{1}\dots d\theta_{n}^{*}d\theta_{n} ( \theta_{i_1}^{*}b_{i_{1}j_{1}}\theta_{j_{1}} ) ( \theta_{i_2}^{*}b_{i_{2}j_{2}}\theta_{j_{2}} ) \dots ( \theta_{i_n}^{*}b_{i_{n}j_{n}}\theta_{j_{n}} ) $$ at this point some explanations are in order . the factor $1/n ! $ appears from the expansion of the exponentials . to see how the integrand was obtained , let 's look at the case when $n=2$ . we will have something like this $$\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\left ( 1+\theta_{i}^{*}b_{ij}\theta_{j}+\frac{ ( \theta_{i}^{*}b_{ij}\theta_{j} ) ^2}{2 ! }+\cdots\right ) $$ it is obvious that only the quadratic term will contribute to the above integral , because only this term can saturate the number of grassmann variables in the integral measure . $$\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\left ( \frac{ ( \theta_{i}^{*}b_{ij}\theta_{j} ) ^2}{2 ! }\right ) =\frac{1}{2}\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\sum_{i_{1} , i_{2} , j_{1} , j_{2}}^{2} ( \theta_{i_1}^{*}b_{i_{1}j_{1}}\theta_{j_{1}} ) ( \theta_{i_2}^{*}b_{i_{2}j_{2}}\theta_{j_{2}} ) $$ expanding the sum and performing all four integrals , we get $$\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\left ( \frac{ ( \theta_{i}^{*}b_{ij}\theta_{j} ) ^2}{2 ! }\right ) =\frac{1}{2 ! } [ 2 ( b_{11}b_{22}-b_{12}b_{21} ) ] =detb$$ now , let us return to our original integral $i$ . the next step before performing the integrals is to reorder the integrals and the grassmann numbers . $$i=\frac{1}{n ! }\int d\theta_{1}^{*}\dots d\theta_{n}^{*}\theta_{i_1}^{*}\dots\theta_{i_1}^{*}\int\theta_{1}\dots d\theta_{n}\theta_{j_1}\dots\theta_{j_1}b_{i_{1}j_1}\dots b_{i_{n}j_n}$$ from where we finally arrive at $$i=\frac{1}{n ! }\epsilon_{i_{1}\dots i_{n}}\epsilon_{j_{1}\dots j_{n}}b_{i_{1}j_1}\dots b_{i_{n}j_n}=\mathrm{det}b$$ ( note : the ordering result $a_{1}b_{1}\dots a_{n}b_{n}=a_{1}\dots a_{n}b_{1}\dots b_{n} ( -1 ) ^{n ( n-1 ) /2}$ has to be used twice for the integrals ) . i found this method to be the most simple one of all when dealing with these sort of integrals . i hope this helps you prove those relations . the same method can be applied very easy in your case ( just a strait forward extension ) . and with the method you described , it seams you are over complicating yourself . however , i will work on it a bit and see what comes up .
the non-compact $u ( 1 ) \cong \mathbb{r}$ symmetry , which the book conformal field theory by philippe di francesco et . al . is referring to , is the translation symmetry $$\varphi\to\varphi+a , \qquad a\in \mathbb{r} , $$ of the main boson field $\varphi$ of the coulomb-gas formalism , reflecting the zero-mode of the $\varphi$-field .
you have a problem with the requirements described in your question . you say you a want to prepare the state of ( a1 , a2 , a3 ) to have 3-way entanglement . in that case , the state of ( a1 , a3 ) will be a mixed state , since the system of ( a1 , a3 ) is entangled with a2 . but you want to use the pair ( a1 , a3 ) for an entanglement swapping protocol . this protocol requires ( a1 , a3 ) to be in a state which is maximally entangled ( such as a bell state ) . but a mixed state is never maximally entangled , and therefore you cannot use the usual entanglement swapping protocol in this case . of course the same reasoning applies to ( b1 , b2 , b3 ) . the deeper reason for the problem here is that entanglement tends to be " monogamous " , in the sense that if a1 is entangled with a2 , it puts limits on the entanglement it can have with a3 . if you still want to try and perform a bell state measurement on a3 and b3 , even without satisfying the requirements of the entanglement swapping protocol , in general you will get some sort of 4-way entanglement in the state of ( a1 , a2 , b1 , b2 ) . the specifics of the state you will get depend on the states you started with for ( a1 , a2 , a3 ) and ( b1 , b2 , b3 ) . as to whether the entanglement in the systems ( a1 , a2 ) and ( b1 , b2 ) will remain intact , they could not have been very strongly entangled in the first place ( again because of entanglement monogamy ) , but they would tend to be even less entangled after the above measurement since now they are a part of a 4-way entangled state instead of a 3-way entangled state .
the clock on $a$ is ticking steadily so the interval between each tick , $t_a$ , is constant . since $b$ is moving steadily with respect to $a$ it seems reasonable to suppose that the time interval between the ticks received by $b$ will also be constant . note that we are not saying anything about the period of these ticks , just that they are equally spaced so $t_b$ is constant . so we need some equation to relate $t_b$ to $t_a$ , and the question is how to show that : $$ t_b = k \space t_a $$ for some constant $k$ that will depend on factors like velocity but not on $t_a$ . given that we know $t_a$ and $t_b$ then we can obviously calculate a value for $k$ because $k = t_b/t_a$ . if we use the subscript $0$ to show this is the equation for our reference clock we can write : $$ t_{b0} = k_0 \space t_{a0} $$ note that at this stage we are not claiming this applies to all clocks , only to the clock that ticks with an interval $t_{a0}$ . now suppose we ignore every second tick sent by $a$ . this means we have a clock on $a$ with an interval of $t_{a1}$ , where $t_{a1} = 2t_{a0}$ . we can work out the interval on $b$ simply by ignoring every second tick on $b$ , and we get $t_{b1} = 2t_{b0}$ . as above we can write an equation to relate the intervals : $$ t_{b1} = k_1 \space t_{a1} $$ but we know that $t_{a1} = 2t_{a0}$ and likewise for $t_{b1}$ , and if we substitite these values and both sides by $2$ we just get back our first equation . that means $k_1 = k_0$ . now we can choose only every third tick on $a$ to get a clock running one third as fast , $t_{a2} = 3t_{a0}$ , and the same argument will tell us that if we write : $$ t_{b2} = k_2 \space t_{a2} $$ then $k_2 = k_0$ . and so on taking every fourth tick , every fifth tick and so on . for all of these clocks we find that : $$ t_{b} = k_0 \space t_{a} $$ so , simply by assuming the ticks are received regularly we have shown that $t_b \propto t_a$ for all clocks with frequencies $f$ , $f/2$ , $f/3$ , and so on . so it seems reasonable to suppose this is a general rule and for any clock on $a$ of any frequency we have : $$ t_{b} = k \space t_{a} $$ for some $k$ that does not depend on $t_a$ .
this effect is called capillarity and is not that straightforward . the contact between water and a solid surface is determined by the chemical bonds . it is macroscopically observed in the contact angle that the water/air surface makes with the solid surface . this angle depends on the strength of the bonds between the solid and the water molecules . you can see this when you pour water in a glass : the water at the edge of the glass is a bit higher than in the center ; it makes an angle with the glass surface . now , if there is a lot of solid around the water , such as water in a tiny tube , there are a lot of contact points . therefore , the water/air interface will be strongly curved . the curvature of this interface modifies the surface tension , which represents the energy contained in that surface . a good way to interpret the effect of curvature is that you surround a given portion of the interface by more ( or less ) water molecules as you curve the interface . the pressure on the interface is thus reduced or increased depending on the curvature . in a small vertical tube , the curvature can be such that the pressure is higher than for a flat interface . thus , it can counteract the gravity more easily . in conclusion , the energy comes from the thermal ( pressure ) energy of the water molecules which push from the bottom .
let 's try the hard way without feynman 's argument . just snell 's law . we can choose a frame $ ( x , z ) $ where $z$ is along $oo'$ and $o$ is at $ ( 0,0 ) $ . we suppose the curve equation is written $z=f ( x ) $ where $ ( x , z ) $ is the location of point $p$ . the vector normal to the curve at $p$ can be written $\vec n = ( 1 , -1/f' ( x ) ) $ . now , knowing snell 's law , you can write $\sin\theta =n\sin\theta'$ , and rewrite it with vector products as $$\frac{\vec{op}\times\vec n}{op\ ; \ ; n}=-n\frac{\vec{o'p}\times\vec n}{o'p\ ; \ ; n}$$ where $n$ can be eliminated on the denominator . expressing all terms as a function of $x$ and $z=f ( x ) $ , you can obtain a differential equation on $f ( x ) $ . $$ [ x+f ( x ) f' ( x ) ] \sqrt{ ( x'-x ) ^2+ ( z'-f ( x ) ) ^2}+n [ ( x'-x ) + ( z'-f ( x ) ) f' ( x ) ] \sqrt{x^2+f ( x ) ^2}=0$$ that seems really difficult to solve unless there is some clever calculus to do . in fact , feynman argument saying that the light should take the same time to travel that distance , whatever the trajetory , is very clever . it should not be absolutely necessary to use that , but it simplifies the problem : $op+no'p=\mathrm{cste}=c$ . this translates into $$\sqrt{x^2+z^2}+n\sqrt{ ( x'-x ) ^2+ ( z'-z ) ^2}=c$$ which you can rewrite as a fourth order equation .
carroll and ostlie , and shu are both excellent introductory texts which have good discussions of star formation . the former is a little more quantitative , the latter qualitative . also the online notes of mark krumholz are fantastic if you have some background in physics . the wikipedia page is also not bad for concepts . star formation the most basic treatment of star formation is generally ' jeans collapse ' ( or ' instability' ) . you start with a large extent of gas , and estimate at what mass and radius ( the " jean 's mass " and " jean 's radius" ) the ' cloud ' will collapse and start forming stars . the initial material has to have a non-zero temperature , because 1 ) reaching 0 kelvin is impossible , 2 ) the thermal motion and pressure is what keeps the gas from having already collapsed . as you suggest , any initial density perturbations ( which there are always plenty of ) can be seeds for initial collapse . if you imagine a perfectly uniform distribution of cold matter , any increase in density is unstable - and will trigger gravitational collapse . collapse is generally thought to be ' hierarchical ' --- a large cloud or ' clump ' ( 1,000s - 10,000s of solar masses ) will start to collapse , then smaller ' cores ' ( 100 's of solar masses ) will collapse within it , then finally protostars within cores . if gas is hot , it will not collapse because the thermal pressure resists gravity . thus star formation requires cooling ( the details of which are a very active area of research ) . to form stars , gas needs to reach about 10 kelvin ( very cold ! ) . at such low temperatures , hydrogen is neutral ( ions exist at higher temperatures ) , and eventually combined into ' molecular hydrogen ' ( $h_2$ ) . dust ( molecules heavier than $h_2$ in astro-parlance ) helps cooling , and thus is associated with enhanced star formation . dust and molecular gas are the main things you see in active star formation regions , like the carina nebula
the activity of a radioactive source is measured ( si units ) in bq - becquerels . one bq = 1 disintegration per second . frequently you will see the curie ( ci ) which is $37 \cdot 10^9 bq$ . the energy of radiation depends on the decay scheme . for example , for cs-137 you find ( source : http://upload.wikimedia.org/wikipedia/commons/6/66/caesium-137_decay_scheme-de.svg ) here you see that there are two different ways for the cs-137 to decay : one gives rise to ba-137 with the emission of a $\beta -$ ( electron ) with energy up to 1.17 mev , while the other goes through an intermediate ( metastable ) 137-ba which subsequently decays to stable 137-ba with the emission of a gamma ray with 662 kev of energy . if you want to include the beta energy in your " intensity " calculation you will find that extremely hard since there is a lot of self-absorption ( betas do not travel very far in matter ) . if you are only interested in the gamma radiation , then you can find the total energy per second ( emitted into $4\pi$ steradians ) as $$energy = activity ( bq ) * 0.95 * 662 * 10^3 * 1.6 * 10^{-19} j$$ at a given distance $r$ , the total area is of course $a = 4\pi r^2$ so you can divide energy by area and get intensity . it would be unusual to express this in $w/m^2$ . when you look at radiation damage , you actually do use a measure of energy . the gray is the si unit , expressed as $j/kg$ - in other words , the amount of energy deposited per kg of material absorbing . that depends not only on the radiation emitted , but also on the material receiving . materials with higher z will typically stop more energy per unit mass and thus have higher values for radiation dose . note that the gray is used for non-living materials . for biological materials , the sievert ( sv ) is preferred since it represents " damage " and not just " absorbed energy " . for a more complete explanation see for example http://en.wikipedia.org/wiki/gray_(unit)
@dmckee guessed correctly . from an excerpt from an address delivered before section a of the american association for the advancement of science , on august 23 , 1882 , by prof . win . harkness , chairman of the section , and vice president of the association : ( ref ) he was destitute of what would now be regarded as the commonest instruments . the invention of telescopes was only twenty years old , and a reasonably good clock had never been constructed . his observatory was situated in paris , and its appliances were of the most primitive kind . by admitting the solar rays into a darkened room through a small round hole , an image of the sun nine or ten inches in diameter was obtained upon a white screen . for the measurement of position angles a carefully divided circle was traced upon this screen , and the whole was so arranged that the circle could be made to coincide accurately with the image of the sun . to determine the times of ingress and egress , an assistant was stationed outside with a large quadrant , and he was instructed to observe the altitude of the sun whenever gassendi stamped upon the floor .
how does an electrical field really work ? there are two formulations that describe the known data on electric and magnetic fields . a ) the classical electromagnetic theory ruled by maxwell 's equations . this works well in describing the macroscopic data , of which the electric field is a component . b ) the quantum mechanical formulation that leads to an explanation of how fields are built up , which is necessary to explain effects like the " photelectric effect " , the behavior of atoms and molecules , the internals of atoms and molecules . for a ) the electric field is a fundamental component of the behavior of matter . for b ) the electric field is built up coherently by innumerable virtual particle exchanges , mainly virtual photons , between the generators of the field and the detectors of its existence , so it is not fundamental . charge is fundamental in this framework , and charge is quantized ( +/-1/3 , +/-2/3 , +/-1 ) in absolute value electron charge units . that is why it is a quantized theory of the world . how come that an electron can exert a force on another electron without physical contact ? for a ) it is an action at a distance , the field of the electron exerts a force on other charged matter ; similar to classical newtonian gravity , where the masses exert a force on each other . what is it in an electron that creates the field , where does the energy for doing the work come from ? it is the charge of the electron . when we are talking of electrons we are really in the realm of b ) , quantum mechanics , because its size is of the size where quantum mechanics has to be used to understand the data . in qm language the electron , when looked at individually , is continually exchanging virtual photons with the boundaries of its containment . virtual means that energy and momentum are not conserved because nothing real is exchanged with the other electrons/ions except an information " i am here " . when many electrons are involved , the surface of a charged metal sphere for example , the collective electric field is built up out of those exchanges . the energy was supplied in this case by the experimenter who provided work to separate the electrons from the rest of the molecules , turning them into ions . either by the triboelectric effect or the classical generators of electricity , using magnetic fields and providing a current of electrons in metals . ultimately it is kinetic energy turned into electric energy . ( actually sun energy stored in fuels or water works , turned into kinetic energy . . . ) now magnetic fields , used to generate most of our electricity , are a bit of a different story , but similar and again needing quantum mechanics to be understood . in scenario a ) they also are fundamental .
dear robert , the answer to your question is trivial and your statement holds pretty much by definition . you know , the green 's functions contain terms such as $$g ( \omega ) = \frac{k}{\omega-\omega_0+i\epsilon}$$ where $\epsilon$ is an infinitesimal real positive number . the imaginary part of it is $$-2\im ( g ) = 2\pi \delta ( \omega-\omega_0 ) $$ so it is the dirac delta-function located at the same point $\omega$ which determines the frequency or energy of the particle species . at $\omega_0$ , that is where the spectrum is localized in my case . if there are many possible objects , the $g$ and its imaginary part will be sums of many terms . this delta-function was for a particle of a well-defined mass ( or frequency - i omitted the momenta ) . if the particle is unstable , or otherwise quasi- , the sharp delta-function peak will become a smoother bump , but there is still a bump . because you did not describe what you mean by " peak " more accurately , i can not do it , either . it is a qualitative question and i gave you a qualitative answer . cheers lm
this is quite far from a silly thought although this is not apparent at first sight . apart from a couple of details which are well understood and have firm physics behind them - such as the fact that deuterium and tritium exist in some proportion and the hyperfine-structure distinction between ortho- and parahydrogen , as far as we can tell all hydrogen atoms are exactly the same . this is in fact the case for all atoms and molecules : all iron atoms are exactly replaceable ( so long as you take the right isotope ) and nitrogen molecules are all the same ( so long as you take them in the correct electronic , nuclear and spin states ) , and so on . this is one of the most profound symmetries in nature and it holds irrespective of geographical / astronomical position , chemical history , temperature , and so on . how can we tell ? well , the very fact that we can do chemistry with atoms is why - the basic tenet is that the world is made of a finite set of " blocks " and that combinations of them make the interesting materials around us . the success of chemistry as a discipline means that there is something to that basic tenet . how can we tell that atoms in places we have not been are the same as here ? of course , our evidence for that is not as strong , but it is built on the fact that astrophysics works just using physics of different kinds we can see experimentally here on earth . we can do spectral analysis of the solar corona , for example , and if we see energy levels slightly displaced then we can explain that as doppler shifts or magnetic fields that let us explore a richer and ( as far as we can tell ) fully consistent physical picture . we can do chemistry on the atmospheres of other planets and , though it is rather hard , come up with consistent chemical explanations for all our observations . we can link the nuclear physics we observe in accelerators and reactors to explain our observations of our sun and other stars and see that they match what we do here . this represents another of nature 's deepest symmetries that is ( again , as far as we can tell ) completely exact : physics is all the same wherever and whenever you do it . emission and absorption of light works exactly the same as here , and so on and so forth . so what happens when something comes up that is not quite right ? well , so far we have always been able to explain that as a result of new physics . some of these observations are in play right now . for example , the physics of em emission and absorption is ( possibly ) slightly different in other galaxies ; to explain this a " drift " of the relevant constant ( the fine-structure constant $\alpha$ ) has been proposed , and there are currently earth-bound experiments to measure this drift going on . ( also this paper . ) so far , however , and despite the number of open problems in physics , no definite evidence for physics being different elsewhere has come up .
first , there are too much errors in the context of your question . the last term in the expression $1$ is certainly false , because $h_{\alpha\beta\alpha'\beta'}$ is antisymmetric in the transformation $\alpha \to \beta , \alpha ' \to \beta'$ , while the last term is symmetric for this same transformation . moreover , the terms $h_{ ( \alpha\beta ) }$ are certainly zero because this a multiplication of $\epsilon^{\alpha\beta}$ , antisymmetric ( in $\alpha , \beta$ ) , and $h_{ ( \alpha \beta ) \dot {\alpha }\dot {\beta }}$ , a symmetric quantity ( in $\alpha , \beta$ ) . secondly , it is certainly false that : $\varepsilon^{\dot {\alpha } \dot {\beta } } ( \sigma^{\mu} ) _{\alpha \dot {\alpha}} ( \sigma^{\nu} ) _{\beta \dot {\beta}} = \pm ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) _{\alpha \beta } , $ with your notations , $\sigma^\mu$ has indices $ ( \sigma^{\mu} ) _{\alpha \dot {\alpha}}$ , and $\tilde {\sigma }^{\nu }$ has indices $\tilde {\sigma }^{\nu }_{\beta \dot {\beta}}$ ( the standard notation $\tilde {\sigma }^{\nu }_{\dot \beta {\beta}}$ is preferable ) , anyway you cannot have a matrix $ ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) $ with indices $ ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) _{\alpha \beta }$ . even if you define a matrix $\tilde { ( \sigma }^{\nu } ) ^{\beta \dot {\beta}}$ or $\tilde { ( \sigma }^{\nu } ) ^{\dot \beta {\beta}}$ , this does not work , you are unable to find 2 lower indices $_{\alpha\beta}$
sure , feynman was concerned with a physical system doing the actual computation , and all physical evolution is unitary , and hence the hamiltonians must be unitary . a slight but trivial generalization would be to allow for evolution with completely positive maps , but such evolution can always be mimicked by unitary evolution with some auxiliary degrees of freedom followed by tracing out of those extra degrees of freedom . allowing for non-unitary evolutions would allow for very unphysical situations ( signalling , . . . ) and would also allow quantum computers to solve np-complete problems ( which is certainly not expected to be possible )
the point is that , with your second equation , you are dealing with in a local coordinate patch , say an open set $u\subset m$ equipped with coordinates $x^\mu \equiv x^1 , x^2 , x^3 , x^4$ . therefore , if $p\in u$ , you can handle two bases of the tangent space $t_pm$ . one is made of ( pseudo ) orthonormal vectors $e_a$ , $a=1,2,3,4$ and the other is the one associated with the coordinates $\frac{\partial }{\partial x^\mu}|_p$ , $\mu= 1,2,3,4$ . the metric at $p$ reads : $$g_p = \eta_{ab} \omega^a \otimes \omega^b$$ where , by definition , the co-vectors $\omega^a \in t^*_pm$ ( defining another pseudo-orthonormal tetrad but in the cotangent space at $p$ ) satisfy $$\omega^a ( e_b ) = \delta^a_b\:\: . \qquad ( 0 ) $$ correspondingly you have : $$\omega^a = \omega^a_\mu dx^\mu|_p \: , \qquad ( 1 ) $$ and $\omega := [ \omega^a_\mu ] $ is a $4\times 4$ invertible matrix . invertible because it is the transformation matrix between two bases of the same vector space ( $t_p^*m$ ) . similarly $$e_a = e_a^\mu \frac{\partial}{\partial x^\mu}|_p\: , \qquad ( 2 ) $$ where $e:= [ e^a_\mu ] $ is a $4\times 4$ invertible matrix . it is an elementary exercise to prove that , in view of ( 0 ) : $$e= \omega^{-1t}\: , \qquad ( 3 ) $$ so you can equivalently write $$e_a^\mu = ( \omega^{-1} ) _a^\mu$$ where the transposition operation in ( 3 ) is now apparent from the fact that we have swapped the positions of greek and latin indices ( compare with ( 1 ) ) . an identity as yours : $$r^{\mu}_{\nu \lambda \tau} = ( \omega^{-1} ) ^{\mu}_a \ , \omega^b_\nu \ , \omega^c_\lambda \ , \omega^d_\tau \ , r^{a}_{bcd}$$ is understood as a trivial change of basis relying upon ( 1 ) and ( 2 ) . it could equivalently be written down as : $$r^{\mu}_{\nu \lambda \tau} = e^{\mu}_a \ , \omega^b_\nu \ , \omega^c_\lambda \ , \omega^d_\tau \ , r^{a}_{bcd}\: . $$
the eigenvalue of the generator $t_a$ are integer multiples of $g_{min}$ because $t_a$ is a generator of a ( cyclic ) $u ( 1 ) $ group and $$ \exp ( 2\pi i t_a/ g_{min} ) = 1 $$ holds as an operator equation . this equation says that the exponentiation of the generator with some imaginary coefficient that i parameterized as $2\pi i / g_{min}$ is equal to the identity . the rotation of a sphere by $2\pi$ is an example . when the equation above acts on an eigenstate , $t_a$ is replaced by its eigenvalue , but because $\exp ( 2\pi i n ) =1$ only for $n\in{\mathbb z}$ , it follows that the eigenvalue of $t_a/g_{min}$ is integer . the representation of a lie algebra and the representation of the corresponding lie group is the same thing . one may be used for the other .
they basically measure the intensity of the infrared blackbody radiation in some wavelength region and calculate the temperature needed to give that intensity according to planck 's law .
major mechanisms for depositing the energy is through compton scattering and/or pair production -- the gamma photon " hits " an electron ( or produces a pair ) , and ejects it , and then these electrons interact with the other atoms in the material and so on . so the steps are : get the photon to eject one or more electrons , the electrons interact with the material via bremsstrahlung , which is more effective with higher density materials ( and higher atomic numbers ) , to give off x-rays , go back to ( 1 ) ( i.e. . the bremsstrahlung x/gamma-rays ionize other electrons ) repeat until you get down to non-ionizing radiation . due to the cross section of the photon scattering processes you need a lot of depth to ensure that almost all of the photons and electrons in the cascade interact with , and stop in , the material . high density materials provide more opportunities , per unit distance , of having one of these discrete interactions . also note that alot of the compton scattering is off of electrons in the inner shells of the atoms , and do not involve the conduction band electrons . all of this is well described in terms of particle " ballistics " and does not involve inducing a normal , bulk , electric current in the material ( which would be influenced by the conductivity ) . although classical evaluation of the skin-depth indicates that higher conductivity would be more effective , it ignores the quantum-mechanical effects that are relevant because a single gamma ray photon can have so much energy ( relative to any of the energy scales that characterize the material ) .
the curving x-axis is a trick in the context of mechanics--- you use the forces in the direction of the curving x-axis , in this case $$f= m_1 g \sin ( 53 ) - m_2 g \sin ( 37 ) $$ ( you can determine that it is sin and not cosine quickly in a test situation by thinking about what happens when the angle is small . if the angle-factor goes to zero , it is sine , if it goes to 1 , it is cosine--- but beware of the occasional tangent ! ) . this force is the remaining component of gravity along the inclines , and this gives you the acceleration : $$ a={f\over m_1+m_2}$$ this solves your problem , by plugging in $m_1=1 , m_2=2$ . this simple thing is the result of the complicated combination of the normal forces , the pully forces , and the gravitational forces , and it is not 100% clear why this works in newtonian mechanics ( although it is true when you solve it ) the reason you have such a simple result is because of hamiltonian/lagrangian mechanics , which is indispensable for revealing the mathematical structure of problems like this , and justify heuristics like a " curving x-axis " which are really unjustifiable in other ways . hamiltonian/lagrangian mechanics says that when you have a conservative mechanical system ( if energy is conserved ) , the energy on the constrained surface of allowed motions determines the dynamics just the same as newton 's laws , except without having to deal with the constraint forces . the constrained motion in this case has one parameter , x , which is the position of either block relative to some starting point ( their displacements are constrained to be the same ) . the total kinetic energy is $$ {m_1\over 2} \dot{x}^2 + {m_2\over 2}\dot{x}^2$$ where $\dot{x}$ is the rate of change of x , which is the speed of either block . the potential energy is , up to an additive constant , the amount of height gained/lost by the masses at displacement x : $$ m_1 g x sin ( 57 ) x - m_2 g x sin ( 37 ) $$ which is the sum of $mg$ times the height for each mass . the resulting energy function is $$ e = {m_1 + m_2\over 2} \dot{x}^2 + ( m_1 g \sin ( 57 ) - m_2 g \sin ( 37 ) ) x $$ which is exactly the same as a one dimensional mass of size $m_1+m_2$ respoding to a constant force of the magnitude given by the formula for f . in the lagrangian formulation , you subtract the pe from the ke , and write in terms of the velocity . in the hamiltonian formulation , you add the pe and ke , but you use the momentum as the fundamental variables . in this case , the momentum corresponding to x coming from the lagrangian is $ ( m_1+m_2 ) \dot{x}$ the lagrangian method ( method of " curved x-axis" ) becomes even more elegant when you want to include the rotational inertia of the pullies . if you always have stick-conditions on the pullies , the system is still conservative and lagrangian , and there is additional kinetic energies from the rotation of the pullies , which is $$ {i\over 2} \omega^2 $$ where $\omega = {\dot{x}}/r$ and the moment of inertia of the pully is either given or calculated from the mass distribution . if you add this to the lagrangian or hamiltonian energy function , you get an increase in the effective mass , and that is it . further , if you have a massive rope , you get an additional potential contribution which is a little complicated , but just gives a force eitehr steadily increasing or decreasing with x ( an upside-down spring ) . you can solve the problem exactly for this case too . the lagrangian method is not usually taught to elementary students , but it is so convenient , that professors introduce the " short-cut " of a curving x-axis instead . i would recommend that you write down the energy function and derive the effective mass and effective force from this , since this is the most fundamental description anyway .
energy is conserved so it can not be created or destroyed . all we can do is change energy from one form to another . in your example we are changing the potential energy of the mass $m$ into kinetic energy . the increase in kinetic energy must be equal to the decrease otherwise energy would not have been conserved . by an external force i assume you mean some third party outside the system . to give a slightly ridiculous example this could be me standing well away from the earth and the mass and poking the mass with a long pole to accelerate it . in this case the energy of the earth + mass would not be conserved , but also my energy would not be conserved . however the energy of the earth , the mass and me would be conserved . the distinction between internal and external forces is a bit artificial because all systems are closed and all forces are internal if you look on a big enough scale .
the lhc is a discovery machine . it was designed to maximize the probability of seeing new physics outside the standard model , but it is a poor experiment for discriminating between two or three states that the only signature they have is missing energy and small to zero missing mass . the reason is that the strong interactions initiating the scattering are a many body problem , 3 quarks on three quarks and a great number of gluons on gluons , thus the only " closed " system is a subset defined by a high transverse momentum transfer , and this is contaminated by debris that is not really relevant to the deep scattering . the use of monte carlo simulations is imperative , and mc means that a model of the interactions is assumed in the creation of the sample . not exactly circular , but not helping in fine details . if the lhc finds photino or neutralino candidates a leptonic collider would be needed to be able to discriminate theories like the one proposing a fourth neutrino . imo of course
no , that is not enough to say that $b=0$ . you must also consider that $$\nabla\times e=-\frac{\partial b}{\partial t}$$ which means that for a magnetic field that is constant spatially but not in time , your conditions would be true but your $b$ field would not be $0$ if , however , we had a case where $\nabla\times e=0$ as well , then ( aside from being a very boring situation ) we could say that $b=0$ . this is done as a simplification tactic . in this case , the $b$ field is constant over space and time . it also means that the $b$ field cannot be affecting any charges because if it were , those charges would accelerate , which would change the $e$ field . changing the $e$ field means that the curl of $b$ would not be $0$ and $b$ would not be a constant . thus , since $b=const$ , it must not affect charges . since it does not contribute to $e$ and since it does not affect anything else , we can simplify any calculations by assuming that $b=0$ . the physical meaning of this is that we are letting the background magnetic field that exists everywhere and is unchanging be zero . similar to the electric potential , we can set the background to be zero and just measure the difference in $b$ between the background and the field of interest . or , that is at least one way to interpret it .
this problem originated with passengers using electronics ( they call them ped 's - portable electronic devices ) during flight . while all consumer electronics have to be qualified by a regulatory body ( fcc , etc . ) to prove they do not emit harmful interference , this does not mean they emit no interference especially to high gain sensitive navigation equipment . " the first national committee that investigated interference by passenger-carried peds was created in the early 1960s . its activities were initiated by a report that a passenger-operated portable fm broadcast receiver caused an airplane navigation system to indicate that the airplane was off course by more than 10 deg . the airplane was actually on course and , when the portable receiver was turned off , the malfunction ceased . a final report from this committee , rtca do-119 , was issued in 1963 and resulted in the revision of the faa federal aviation regulations ( far ) by establishing a new rule ( far 91.19 , now 91.21 ) , which states that the responsibility for ensuring that peds will not cause interference with airplane navigation or communication systems remained with the operator of the airplane . " -- from boeing this reference also has some incidents which i hightlight here : 1995 , 737 a passenger laptop computer was reported to cause autopilot disconnects during cruise . 1996/1997 , 767 over a period of eight months , boeing received five reports on interference with various navigation equipment ( uncommanded rolls , displays blanking , flight management computer [ fmc ] / autopilot/standby altimeter inoperative , and autopilot disconnects ) caused by passenger operation of a popular handheld electronic game device . 1998 , 747 a passenger’s palmtop computer was reported to cause the airplane to initiate a shallow bank turn . one minute after turning the ped off , the airplane returned to " on course . " now this " all electronics off " rule has become legacy and broadly applied . there are a mix of opinions about how relevant it is today . but the fact remains that while manufacturers do a decent job of limiting rfi ( radio frequency interference ) there is still a good chance that the very sensitive radios on board aircrafts could have a problem with one of the rfi emissions . and due to the nature of the aircraft radios it is almost impossible to test every possible interference scenario . these radios do a lot of frequency mixing , amplification and filtering . if you have ever done a detailed spurious analysis on a mixer , you will know that each time you mix a single you create a tremendous number of possible interference problems . there has been recent studies showing rfi issues but with a very low probability ( 1:1,000,000 ) of harmful interference . ( sorry , you have to pay $63 to read the report ) . so for everyone 's safety the " all electronics off " rule during critical takeoff/landing has remained in effect . for a good overview , i suggest you read the first link in my answer from boeing .
i think there would not be a hard collision of the object with the ground . it would pass through a column of air of comparable mass , so i suspect the energy of the object would be consumed before it hits the ground . this would be a sort of massive cosmic ray event . the interaction energy of atoms would be comparable to tevatron energy ( 1/10 of that or so ) and what would reach the ground would be a huge pulse of secondary particles that generate an enormous thermal-mechanical shock wave . at 10^3kg the energy would be $e~\simeq~\gamma mc^2$ or about $10^{19}j$ of energy . one ton of explosives is 4.18 mega joules of energy , so the energy released would be about the equivalent of $2\times 10^7$ megatons of explosives . clearly you would not want to be anywhere near this . however , i suspect the damage done to the earth’s surface would not be due to the body actually impacting the surface , but from this huge shock front . so one would have to search out what is known about that sort of mechanics .
i do not have a figure for the total area subtended by the objects in the oort cloud , and i think it would be very difficult to get even within an order of magnitude estimate for this , since we do not know a lot about the oort cloud . however there is a simple answer to your question . all we have to do is ask how often we see stars occluded by objects in the oort cloud , and the answer is never . that tells us that the amount of light blocked by the oort cloud is insignificant .
there is no difference in physics between a propeller and fan . in english the distinction is probably that a fan moves air while the propeller moves the vehicle through the stationary air ( or water ) . edit : although aerospace engineers call the front stage of a turbine engine a " fan " . there is a difference between an aerodynamic regime at low speeds and higher pressures where the air behaves as a fluid , and low pressure regimes where it is purely mechanical ' billard balls ' hitting flat blades - as in a high vacuum turbo molecular pump
this is a pretty vague question , but i take it that you are groping for some " physical significance " . the clearest one is that the logarithm is the inverse of the exponential function $x\mapsto e^x$ which itself arises whenever the rate of quantity 's variation is equal to or proportional to that quantity , a fairly common statement describing physical processes . for example : rates of chemical reactions , radio active decays , attenuation of light or other em radiation through mediums all follow such laws . given this " physical definition " it follows then that the inverse function is simply that given by $x\mapsto \int_1^x \frac{\mathrm{d}z}{z}$ and then this definition is broadened into the punctured complex plane $\mathbb{c}\sim \{0\}$ by analytic continuation . moreover the functions $\exp$ and $\log$ defined in this way have particularly simple taylor series ( the former is universally convergent , the latter convergent in an open unit radius circle about $z=1$ ) that make their definitions relatively easy to broaden to objects other than numbers such as matrices , operators and so forth . the idea of a rate of a quantity 's variation being proportional to that quantity is further generalized in operator equations and , in particular , in the theory of lie groups , where $\exp$ and its inverse $\log$ play central roles in mapping neighbourhoods of the group 's identity to and from the " lie algebra " , i.e. the space of the linear transformations that play the role of generalized " rates of change " - these can now be complex numbers , quaternions or in general square matrices ( for the lie algebra they can always be thought of as square matrices - ado 's theorem - but this is not always so for the lie group ) . again , it is the natural base $e$ logarithm that falls from the definitions by dint of its taylor expansion around the identity . the theory of lie groups , with its fundamental reliance on $\exp$ and $\log$ , plays many important roles in physics and the sciences in general . in an even more generalized setting , the schrödinger equation is also a generalized " rate of change proportional to the quantity " equation , as are the descriptions of flows and the exponential map defining geodesics in differential geometry . lastly , since you ask about thermodynamics and the formula graven on boltzmann 's headstone , the logarithm is the grounding of the natural encoding of the idea that numbers of possibilities ( volumes of phase spaces ) multiply , whereas intuitively the corresponding " entropies " , as extensive protperties of thermodynamic systems should add . whilst it should be clear that the logarithm 's base does not matter for this definition ( indeed information theorists choose base 2 logarithms to write informational entropies in bi nary digi ts or bits ) , one could argue that the natural base $e$ logarithm that is the " prototypical " isomorphism ( which is what boltzmann 's intuitive idea is all about ) between the group of reals and addition and the group of strictly positive reals and multiplication that arises from the lie theoretical idea of mapping the lie group $ ( \mathbb{r}^+\sim\{0\} , \ , \times ) $ onto its lie algebra $ ( \mathbb{r} , \ , + ) $ what is the probability of all this happening ? it is precisely equal to unity : for the above ideas are how we define the natural logarithm ( i.e. . as the ones defined above as opposed to logarithms with another base or even indeed other functions altogether ) .
dear john , it does not matter whether a quantity is measured or calculated . in some sense , you may say that the proper time or proper length are measured in the other frames , too . calculation is sometimes a necessary part of the measurement . because the values are not affected by the transformations , proper lengths and proper times are formally scalars , much like the rest mass which is the proper length of the energy-momentum vector , if you wish . however , we usually do not use the word " scalar " for such quantities because they are just some " somewhat unnatural " functions of vectors . for example , the proper length is $\sqrt{dx^\mu dx_\mu}$ assuming that $dx^\mu$ ( and the convention for the metric ) is spacelike . so fundamentally , the proper length is some information about a vector . because the proper length is just some " partial information " about a vector , you can not build a " field theory " out of proper lengths themselves . in relativistic field theory , we talk about " scalar fields " and " vector fields " and other " tensor fields " , aside from " spinor fields " etc . however , a fundamental theory could have no elementary field associated with the " proper length " even if such constructions made sense : it would have the whole " vector field " while the length of this vector would be a " derived quantity " .
is it possible to focus the sun in such way ? yes , as others have pointed out , all of the ideas in your sketch are already used in existing designs - perhaps excepting the shutter ( which actually performs no useful purpose so far as i can see ) . as chris white commented - " this exact design ( with the shutter permanently open ) is a schmidt-cassegrain telescope , probably the most popular high-end consumer scope these days . " is it possible to increase the power of the beam by making it bounce between the mirrors no , focusing a beam of light , or reflecting it , does not increase the power . the amount of power is the amount of light energy entering the system per second . that is limited by the diameter of the entry pupil . energy is conserved .
i ) op essentially asked ( v1 ) : if two lagrangian densities ${\cal l}$ and $\tilde{\cal l}$ have the same eqs . of motions , must they necessarily differ by a total divergence ? answer : no , one e.g. can always multiply a lagrangian density ${\cal l}$ with a constant factor $\tilde{\cal l}=\lambda {\cal l}$ different from one $\lambda\neq 1$ without altering the el equations , but the difference $$\tilde{\cal l}-{\cal l}= ( \lambda-1 ) {\cal l}$$ is not a total divergence if ${\cal l}$ is not . ii ) op essentially asked ( v4 ) : if el equations are trivially satisfied for all field configurations , is the lagrangian density ${\cal l}$ necessarily a total divergence ? answer : yes , modulo topological obstructions in field configuration space . this follows from an algebraic poincare lemma of the so-called bi-variational complex , see e.g. ref . 1 . we should mention that an elementary follow-your-nose-type proof exists for lagrangians of the form $l ( q^i , \dot{q}^j , t ) $ without higher-order derivatives , see e.g. ref . 2 . we stress that the proof-technique of ref . 2 does not work in the presence of higher-order derivatives or in the case of field theory . references : g . barnich , f . brandt and m . henneaux , local brst cohomology in gauge theories , phys . rep . 338 ( 2000 ) 439 , arxiv:hep-th/0002245 . j.v. jose and e.j. saletan , classical dynamics : a contemporary approach , 1998 ; section 2.2.2 , p . 67 .
as djbunk mentions the delta function is symmetric $$\delta ( x ) =\delta ( -x ) $$ so you certainly have $$\delta ( x-x' ) =\delta ( x'-x ) . $$ but you should also know that in general we have $$\langle a | b \rangle = \langle b | a \rangle^* $$ and since in this case the inner product is real , you will also have $$\langle x | x ' \rangle = \langle x ' | x \rangle . $$ so it does not matter which way you write the delta function or the inner product .
your picture is ok , so long as you realise that this is a snapshot in time . if you looked " from above " the electric field of the waves ( by your definition of the polarisation direction ) would be pointing towards or away from you - so this becomes rather difficult to draw in the same way . it might be better to think of the e-field as a field of arrows , where the length of the arrow is proportional to the instantaneous e-field magnitude and the direction , well , shows the direction . in which case , from above , you would be looking down on a field of arrow-heads and arrow-tails . if the laser had a gaussian profile in both x and y directions ( assuming the beam is heading towards z ) , then the maximum height of the arrows ( in either direction ) would diminish towards the edge of the beam .
as you have in the commutation relations , $\sigma_i \sigma_j= \sigma_j\sigma_i$ e.g. spin operators on different sites commute , so there is no minus sign to pick up .
as you described , we substitute $y=0$ and $x=r$ into the trajectory equation : $$0=h+r\tan{\theta}-r^2\frac{g}{2u^2}\sec^2\theta . \tag{1}$$ then , differentiating with respect to $\theta$ and setting $\frac{dr}{d\theta}=0$: $$0=r_{max}\sec^2\theta-r_{max}^2\frac{g}{2u^2}2\sec^2\theta\tan\theta , $$ which simplifies to $$r_{max}=\frac{u^2}{g}\cot\theta . \tag{2}$$ solving $ ( 1 ) $ and $ ( 2 ) $ will yield the desired expressions for $\theta$ and $r_{max}$ .
per request , made to an answer from a comment : it was john stuart bell in 1964who proved by simple arithmetics that there are no hidden local variables behind the statistical nature of quantum processes , and behind the spooky non-locality displayed by entangled particles . consequently , the paradox presented in the 1935 einstein-podolsky-rosen paper upon which they claimed that quantum physics cannot be complete ( "since it relies on statistical laws , it cannot give the ultimate full description of nature" ) is inherently wrong . we understand causality as a relation that links post-events ( effect ) to prior-events ( cause ) ( note that this does not necessarily mean similar chronological sequence , see here ) . in this sense , observable phenomena are dependent on deeper , possibly hidden variables , that nevertheless can be usually uncovered , at least at the macroscopic level . however , as bell has proven , there are no hidden variables responsible for lowest-level quantum processes e.g. the random decay of radioactive elements . therefore i would say that there exist no lower-level , ultimate cause for these processes .
note : chocopouce 's answer is the same as mine but is more mathematical . you have a ( spherically symmetric ) probability density distribution $\rho$ in space ( which we get from the square of the amplitude ) . the " radial probability density " is roughly the chance that the electron is at a given radius , say $r = 0.1\mathrm{nm}$ ? in other words , how much of this distribution is in at the $0.1\mathrm{nm}$ shell ? we will not exactly $0.1\mathrm{nm}$ , so we take a thin shell : how much is between $0.1$ and $0.1+ε\mathrm{nm}$ ? amount $=$ ( average prob . density in shell ) * ( volume of shell ) . since the shell is so thin ( $\varepsilon$ is very small ) , the density will be almost constant and the shell 's volume is given by $\mathrm{area}\times\mathrm{thickness} = 4\pi* ( 0.1nm^2 ) *\varepsilon nm$ . thus the probability is $4\pi\rho ( r ) r^2\varepsilon$ , where $\rho$ only depends on $r$ since it is spherically symmetric . but $\varepsilon$ is an arbitrary " small " thickness we defined , and it is best to divide by $\varepsilon$ to get the probability per unit radius , which is called the ( radial ) probability density $4\pi\rho ( r ) r^2$ . the most likely radius ( which is different from the average radius ) maximizes this function .
well , it is not . to our best present knowledge the cosmological constant is positive , which means the universe undergoes accelerated expansion and never crunches . hawking 's book is a little out of date .
if two particles are close to each other , there is more space for the rest of the particles to move . this gives rise to an effective entropic attraction between the particles because when looking at two particles for different separations while " tracing out " over the degrees of freedom of the rest of the system , the entropy of the rest is higher when the two tagged particles you are looking at are close to each other . in fact at high density , you should also observe oscillations in the g ( r ) and not a single peack . the width of the bumps in these oscillations is related to the particle size .
imho , the current use of the word helicities happens only when one is looking at some representation of $su ( 2 ) $ . 1 ) now , a first point of view is to try to go back to representations of $ \otimes^n su ( 2 ) $ , when working with representations of $so ( d-2 ) $ . in the best case , you will have different kind of " helicities " . suppose we work with $d=6$ , so spin-$1$ massless particles are in the fundamental representation of $so ( 4 ) $ , which i write $4$ . in term of $su ( 2 ) \otimes su ( 2 ) $ representations , this gives : $4 \to ( 2,2 ) $ [ here i write the number of states in the representations ] so , multiplying photon representations gives $4 \times 4 \to ( 2,2 ) \times ( 2,2 ) = ( 3,3 ) + ( 1,3 ) + ( 3,1 ) + ( 1,1 ) $ $ ( 3,3 ) $ is the graviton traceless symmetric representation that we are looking for , with $9 = \dfrac{6 ( 6-3 ) }{2}$ so here photons have " helicities " $ ( \pm 1 , \pm 1 ) $ , while gravitons have " helicities " $ ( 0 \pm 1 , 0 \pm 1 ) $ gravitons states could be written from photons states , for instance : $ ( +1 , +1 ) = ( +1 , +1 ) ( +1 , +1 ) $ $ ( -1 , -1 ) = ( -1 , -1 ) ( -1 , -1 ) $ $ ( +1 , -1 ) = ( +1 , -1 ) ( +1 , -1 ) $ $ ( -1 , +1 ) = ( -1 , +1 ) ( -1 , +1 ) $ $ ( +1,0 ) = \frac{1}{\sqrt 2} [ ( +1 , +1 ) ( +1 , -1 ) + ( +1 , -1 ) ( +1 , +1 ) ] $ $ ( 0,0 ) = \frac{1}{ 2} [ ( +1 , -1 ) ( +1 , -1 ) + ( +1 , -1 ) ( -1 , +1 ) + ( -1 , +1 ) ( +1 , -1 ) + ( -1 , +1 ) ( -1 , +1 ) ] $ and so on . 2 ) a second point of view is to work directly with the representations of $so ( d-2 ) $ let us use this ( french ) lie group on-line tool ( université de poitiers ) . choose $d3 ( so ( 6 ) ) $ , " tensor product decomposition " ( then " proceed" ) . let 's type $ ( 1,0,0 ) \times ( 1,0,0 ) $ , ( then " start" ) , and you get $ ( 2,0,0 ) + ( 0,1,1 ) + ( 0,0,0 ) $ . here we are working with dynkin indices . so $ ( 2,0,0 ) $ is the graviton symmetric traceless representation , and it is also the highest weight state of the representation . you may get the other states of the representation by substracting with the simple roots you may directly from the cartan matrix of $d3= so ( 6 ) = su ( 4 ) $ ( they are the lines of the cartan matrix ) until you get no positive number . here the simple roots are $ ( 2 , -1,0 ) , ( -1,2 , -1 ) , ( 0 , -1,2 ) $ . so , for instance , substracting the first root , you get the state $ ( 2,0,0 ) - ( 2 , -1,0 ) = ( 0,1,0 ) $ , and so on . so each state for the gravitons ( or the photons ) could be represented by $3$ integers , so it is an alternative way to classify the states into a given representation .
because it is structure displays translational symmetry in 2d . atoms themeselves are 3d as in other materials , but their are placed on a 2d flat plane . compare to 1d fullerenes .
this is exactly the approach taken in bernard shutz 's note " gravitational waves on the back of an envelope " ( am . j . phys . 50 vol 5 pp 412 ) . the abstract reads : using only newtonian gravity and a little special relativity we calculate most of the important effects of gravitational radiation , with results very close to the predictions of full general relativity theory . used with care , this approach gives helpful back‐of‐the‐envelope derivations of important equations and estimates , and it can help to teach gravitational wave phenomena to undergraduates and others not expert in general relativity . we use it to derive the following : the quadrupole approximation for the amplitude h of gravitational waves ; a simple upper bound on h in terms of the newtonian gravitational field of the source ; the energy flux in the waves , the luminosity of the source ( called the ‘‘quadrupole formula’’ ) , and the radiation reaction in the source ; order‐of‐magnitude estimates for radiation from supernovae and binary star systems ; and the rate of change of the orbital period of the binary pulsar system . where our simple results differ from those of general relativity we quote the relativistic ones as well . we finish with a derivation of the principles of detecting gravitational waves , and we discuss the principal types of detectors under construction and the major limitations on their sensitivity . ( if you do not have access to am j phys , this talk seems to recapitulate the details . ) a major difference in this newtonian scalar theory from the real gr theory of gravitational waves is in the effect of the waves on an inertial test particle . this newtonian theory predicts that the waves would appear as an oscillating force along the direction between the source and the test particle . by contrast , gr predicts an oscillating differential tidal effect in the plane perpendicular to the line connecting the source and the test mass . as a result , while i think ligo would still detect the " newtonian " form of gravitational waves , the antenna pattern of the detector would be different . the l-shaped ligo detector has optimal sensitivity to a source located directly overhead in the gr case ( allowing the gravitational wave to stretch one arm while it is compressing the other ) . there would be no sensitivity to a " newtonian " source directly overhead . however , you could detect it if the " newtonian " source were aligned with either arm . by the way , " newtonian noise " ( the near-field action of newtonian gravity arising from density waves in the material near the detector ) is a real concern for terrestrial gravitational wave detectors ! p.s. to be pedantic , it is best to avoid the term " gravity wave " ( as opposed to " gravitational wave" ) , since a " gravity wave " ( "newtonian gravity wave " even ! ) is something completely different .
any photon ( pure ) state may be described by a q-bit formalism : $$|photon\rangle = \alpha |0\rangle + \beta|1\rangle$$ where $|0\rangle$ and $|1\rangle$ represent the two possible polarizations of the photon . so , any photon " is " a q-bit . you do not have to " create " q-bits . just prepare photons is some state . an entangled state of $2$ photons may be described by a $2$-photons ( $2$-qbit ) state , for instance : $$|entangled\rangle = \frac{1}{\sqrt{2}} ( |0\rangle|0\rangle + |1\rangle|1\rangle ) $$
however , i had difficulty understanding that answer and would like to understand how to do it this way . that is to say , i would really like to know what property or identity that i am missing before i can use use the bianchi identities to show that it is manifestly zero . the other proof uses the first bianchi identity . that is where the starting assumption $r^a{}_{bcd}\xi^d = \xi^a{}_{ ; bc}$ comes from . if you want to use the second bianchi identity , it is $$ ( \nabla_\xi r ) ( x , y ) + ( \nabla_x r ) ( y , \xi ) + ( \nabla_y r ) ( \xi , x ) = 0\text{ , }$$ and therefore applying it and the leibniz rule produces : $$\begin{align} \underbrace{\nabla_\xi [ r ( x , y ) ] +\nabla_x [ r ( y , \xi ) ] +\nabla_y [ r ( \xi , x ) ] }_\mathrm{foo} = \underbrace{r ( \mathcal{l}_\xi x , y ) + r ( \mathcal{l}_xy , \xi ) + r ( \mathcal{l}_y\xi , x ) }_\mathrm{bar}\text{ , } \end{align}$$ where it was assumed that the torsion vanishes , so that $\mathcal{l}_ab = \nabla_ab-\nabla_ba$ . additionally , $$\begin{eqnarray*} ( \mathcal{l}_\xi r ) ( x , y ) and = and \mathcal{l}_\xi [ r ( x , y ) ] - r ( \mathcal{l}_\xi x , y ) - r ( x , \mathcal{l}_\xi y ) \\ and = and \mathcal{l}_\xi [ r ( x , y ) ] - r ( \mathcal{l}_\xi x , y ) - r ( \mathcal{l}_y\xi , x ) \\ and = and \underbrace{\mathcal{l}_\xi [ r ( x , y ) ] - [ \mathrm{foo} ] + r ( \mathcal{l}_xy , \xi ) }_\mathrm{qux}\text{ . } \end{eqnarray*}$$ so the objective is to show that the right-hand side , $\mathrm{qux}$ , is identically zero whenever $\xi$ is a killing vector field . let 's write $s^a{}_b = [ r ( x , y ) ] ^a{}_b = r^a{}_{bcd}x^cy^d$ , and just crank it out : $$\begin{eqnarray*} \mathcal{l}_\xi s^a{}_b and = and \nabla_\xi s^a{}_b - s^e{}_b\xi^a{}_{ ; e} + s^a{}_e\xi^e{}_{ ; b}\\ and = and \nabla_\xi s^a{}_b + x^cy^d ( r^a{}_{ecd}\xi^e{}_{ ; b} - r^e{}_{bcd}\xi^a{}_{ ; e} ) \\ and = and \nabla_\xi s^a{}_b + x^cy^d ( \nabla_c\nabla_d-\nabla_d\nabla_c ) \xi^a{}_{ ; b}\text{ , } \end{eqnarray*}$$ where the last step is actually valid for arbitrary $z^a{}_b$ , not just $\xi^a{}_{ ; b}$ . the first term of this cancels with the first term of $\mathrm{foo}$ . so far we have not used the fact that $\xi$ is a killing vector field . let 's do so now by considering the other two terms of $\mathrm{foo}$: $$\nabla_x [ r ( y , \xi ) ] ^a{}_b - \nabla_y [ r ( x , \xi ) ] ^a{}_b = \nabla_x\nabla_y\xi^a{}_{ ; b} - \nabla_y\nabla_x\xi^a{}_{ ; b}\text{ , }$$ where the starting identity $r^a{}_{bcd}\xi^d = \xi^a{}_{ ; bc}$ was used . the same identity also gives : $$r ( \mathcal{l}_xy , \xi ) ^a{}_{b} = \nabla_{ [ x , y ] }\xi^a{}_{ ; b}\text{ . }$$ therefore , we have shown that for any vector fields $x , y$ , $$\begin{eqnarray*} x^cy^d ( \mathcal{l}_\xi r^a{}_{bcd} ) and = and \left [ x^cy^d ( \nabla_c\nabla_d-\nabla_d\nabla_c ) - ( \nabla_x\nabla_y-\nabla_y\nabla_x ) + \nabla_{ [ x , y ] }\right ] \xi^a{}_{ ; b}\\ and = and 0\text{ . }\end{eqnarray*}$$ ( if you have trouble with the last step , check christoph 's answer to the other question and modify appropriately . ) thus $\mathcal{l}_\xi r^a{}_{bcd} = 0$ , qed .
so this was the answer given in the key :
in general , decoherence and renormalization are two different things . decoherence is loss of quantum correlations due to lack of information ( either on the interaction , or - on states of some particles ) . the most typical example is when you loss a particle - then you need to trace out with respect to their degrees of freedom , effectively changing all entanglement in ( classical ) randomness . renormalization is a procedure , when you construct ( or deal with ) an effective theory , disregarding some degrees of freedom only to threat them as a single object ( e . g . instead of four particles only one " effective particle" ) . typically you do not " loss " particles , but treat collections of then as a new one . however , the procedure is lossy - i.e. you lose some properties ; in particular , from a pure state you can go to mixed ones . however , typically you rather still hold an approximate pure state , rather than the exact one which ( after neglecting some degrees of freedom ) is mixed ; see e.g. : pietro silvi , tensor networks : a quantum-information perspective on numerical renormalization groups ( 2011 ) , http://www.sissa.it/cm/thesis/2011/silvi.pdf
summarizing and expanding on the comments . . . first , the given solution only holds in the case $\rho \equiv \rho_0$ is constant ( though wald 's wording could stand to be clearer on this point ) . you will not find a nice general expression for an arbitrary equation of state , which is why numerical results are key for modeling real compact objects like neutron stars . proceeding with our assumption , the mass inside a sphere is given by $m = ( 4\pi/3 ) \rho_0 r^3$ , with the total mass related to the total radius in the same way : $m = ( 4\pi/3 ) \rho_0 r^3$ . then the differential equation can be rewritten $$ \frac{\mathrm{d}p}{\mathrm{d}r} = - ( p + \rho_0 ) \frac{ ( 4\pi/3 ) \rho_0r^3 + 4\pi r^3p}{r ( r - 2 ( 4\pi/3 ) \rho_0r^3 ) } = -\frac{4\pi}{3} ( p + \rho_0 ) \frac{ ( \rho_0 + 3p ) r}{1-8\pi\rho_0r^2/3} . $$ we now proceed with separation of variables . the only physics left is in the boundary condition : $p = 0$ at $r = r$ . 1 with this in mind , we can write $$ \int_p^0 \frac{\mathrm{d}p}{ ( p+\rho_0 ) ( 3p+\rho_0 ) } = \int_r^r \frac{r\ \mathrm{d}r}{2\rho_0r^2-3/4\pi} . $$ both of these integrals are quite solvable using cute tricks from high school calculus , nothing more advanced required . 2 the result of integrating is $$ \frac{1}{2\rho_0} \log\left ( \frac{p+\rho_0}{3p+\rho_0}\right ) = \frac{1}{4\rho_0} \log\left ( \frac{8\pi\rho_0r^2/3-1}{8\pi\rho_0r^2/3-1}\right ) . $$ using the earlier definition of $m$ this is clearly equivalent to $$ \frac{p+\rho_0}{3p+\rho_0} = \left ( \frac{1-2m/r}{1-2mr^2/r^3}\right ) ^{1/2} , $$ which can be solved for $p$ to yield the desired result . finally , it should be noted that ode solving is not the most interesting thing going on here . what is far more intriguing is how one gets the differential equation to begin with - it is not trivial . remarkably , karl schwarzschild did much of this computation in his 1916 paper " über das gravitationsfeld einer kugel aus inkompressibler flüssigkeit nach der einsteinschen theorie " ( " on the gravitational field of a sphere of incompressible fluid according to einstein’s theory " ) just a few months after einstein published his gr field equations . 1 alternatively one could say $p = p_\mathrm{c}$ at $r = 0$ and integrate the other way , finding a formula for pressure at any radius in terms of the central pressure and mass . the solution can then be inverted to find the radius in terms of the pressure , and the value of this at $p = 0$ would give the size of the object . this procedure make sense if one has a better handle on the microphysics responsible for pressure than on the actual sizes of objects , as is often the case . 2 for the pressure integral , you can complete the square to show that the integrand looks like the derivative of the inverse hyperbolic tangent ( or equivalently the inverse tangent with $\sqrt{-1}$ in appropriate places ) . then it is a simple matter to invert the exponential formula for $\tanh$ to find the logarithmic version of $\tanh^{-1}$ .
i second the suggestion of @chrisgerig in the comment above about reading the wiki article . this is the relevant paragraph : if the system consists of more than one particle , the particles may be moving relative to each other in the center of momentum frame , and they will generally interact through one or more of the fundamental forces . the kinetic energy of the particles and the potential energy of the force fields increase the total energy above the sum of the particle rest masses , and contribute to the invariant mass of the system . the sum of the particle kinetic energies as calculated by an observer is smallest in the center of momentum frame ( or rest frame if the system is bound ) . what they call a particle is not an elementary particle , i.e. one which is point like and whose invariant mass is constant on all frames . once there is a system of particles , even two photons , their invariant mass is variable .
in 3d there are 7 lattice systems which are classes of lattices having the same point group . one of them is the class of cubic lattices . this class contains three different bravais lattices which are distinguished by their translation group .
the term ' equation of motion ' is somewhat subjective as it depends on the context , but for any given context there is usually one single equation , or set of equations , which can be described as an equation of motion . these are typically differential equations in time , usually of second order , and for simple objects in newtonian mechanics they do not involve other partial derivatives . in that context , equations of motion are usually expressions of newton 's 2 nd law of motion . most importantly , the defining characteristic of an equation of motion is that its solutions , once appropriate initial conditions are fixed , must completely determine the evolution of the system after the initial time . beyond this , and without having more information about your problem , it is impossible to say anything else .
in 1st case : you do not have a battery , current flows and creates a backward force on the wire given by f = ilb , to overcome this force you need to apply a force and do work against it . this work done gets converted to joules heat . however in case 2 since no current flows , no force is applicable and f = 0 , since there is no force to work against there is no work done in 2nd case and hence no mechanical energy to conserve , if the rod is moving it continues to move without slowing .
a sun or a star is not possible to exist on this scale ; to be as massive as a core of a planet , it is just not massive enough . but you did not mention the size of it . so if we put that aside , first of all there is no such thing as no gravity . where there is mass there is gravity , and that gravity has to be strong enough to hold gas ( atmosphere ) . and the rocks will have to sink into the core since they are the denser objects . if however we compared this to an existing example , where the sun is in the center of the solar system and holding planets ( floating chunks of rocks ) , there is still vacuum in between . because at the distances these planets are from the sun , the sun 's gravity is not strong enough to hold gas . where the sun 's gravity is strong enough there is gas , and that ends as far as the outer atmosphere of the sun itself . which mainly does not extend to the planets . therefore it is not possible .
studying the problem some , i think what your showing is superposition of a voltage source and a current source . in the first circuit , you have zeroed the current source ( making it an open circuit ) , and in the second circuit , you have zeroed the voltage source ( making it a short circuit ) . in both cases , you appear to be trying to solve for the current through the capacitor and then summing the two results to find the total . if the sources are sinusoidal with angular frequency $\omega$ , then this is a very easy problem to solve . for the first circuit , the capacitor and resistor are in parallel . find the source current and then use current division to find the capacitor current : $i'_c = \dfrac{u_s}{ ( r||-jx_c ) + jx_l} \dfrac{r}{r -jx_c}$ for the second circuit , you have used current division correctly . however , it is not clear that ac analysis is appropriate since the sources are shown as functions of time . since ac analysis is done in the phasor domain , the sources are not shown as time dependent but , rather , as a complex constant .
every thermodynamic system satisfies the first law during a given process ; $$ \delta e = q-w $$ here $\delta e$ is the change in its internal energy , $q$ is the heat transferred to the system , and $w$ is the work done by the system . for a system undergoing a cyclic process , namely one for which it starts and ends in the same thermodynamic state , one has $\delta e = 0$ , and the first law then tells us that $$ q = w $$ now , suppose that the system is taking some heat $q_h&gt ; 0$ from a reservoir , and turning it into some work $w$ . let 's define $$ q_\mathrm{exhuast} = q-q_h $$ then we can write $$ q_h + q_\mathrm{exhaust} = w $$ the kelvin statement tells us that we must have $q_\mathrm{exhaust}\neq 0$ because otherwise , we would have $q_h = w$ ; the sole result of the process would have been to transform the heat it took in into work . the symbol $q_\mathrm{exhaust}$ is what one commonly calls the exhaust heat .
my question is , whether this is caused by an modification of the frequency/wavelength or simply by my eye combining the two incoming lights . short answer no so your second picture is accurate . frequency is not modified , the two different waves are added up by your eye to produce the light that you perceive . most computer screens operate on a rgb colour model , i.e. they only have red green and blue lights . so the " yellow " light you see coming from your screen contains zero photons of yellow frequency but the exact right proportion of red green and blue to trick your eye into firing neurons the same way it would if photons of the yellow spectrum were striking it . when " adding " light colours you start with a white wall , that is un-illuminated ( black ) . and then you add red green and blue ( the additive primary colours ) lights to get any colour you want . however when " subtracting " colours , if you start with a white sheet of paper illuminated with white light ( white ) and start adding paints ( subtracting colour ) you can make any colour by using the tree primary colours red yellow and blue ( the subtract-ative primary colours ) , or more accurately as referred to in the printing industry cmyk , cyan magenta yellow and key ( black ) , black is needed because adding enough coloured paint to make an image black is inefficient and expensive . if you are allowed to add and subtract colours you can mix ( almost ) any three different colours to generate the illusion of any desired colour . ie you get to choose any three colours as your primary colours . exactly three colours is required since there are three different types of cones in your eye ( three degrees of freedom ) . also note that the srgb does not encapsulate all different colours , so watching a movie in cinema with a reel film may display hues that cannot be rendered on a computer monitor . so far i have only dealt with the eye and completely ignored the interaction of the light with the interface . in general light is absorbed , reflected or transmitted through the medium . reflected light can be coherent light in a mirror or diffuse like a painted wall . however there are a large number of non-linear optical effects where the photons do interact with each other and may combine . there are also interactions with the medium such as those seen with a black light . links are to the respective wikipedia pages .
something moving such that it " measures the proper time " is simply something moving from point 1 at time 1 , to point 2 at time 2 , as seen in s ( you have the data ) . recall the basic definition of velocity .
it means that the charge operator $q$ is a lie algebra generator for some lie group $g$ . the field $\phi\in v$ takes values/transform in a representation $v$ of the lie group . ( note that any lie group representation $v$ is also lie algebra representation of the corresponding lie algebra . ) the charge operator $\rho ( q ) $ in the representation $\rho : g \to gl ( v ) $ is proportional to the identity . the proportionality factor/eigenvalue is the actual charge of the field . to see an example of this , say the ( strong ) $u ( 1 ) $ hypercharge $y$ , see this answer .
from the point of view of people standing on the earth , no effect whatsoever . that is because the so-called " relativistic mass " is an effect of different frames of reference . ( it is also pretty trivial from the pov of someone at rest with respect to the sun , surpressed by factors of order $\frac{30000\text{ m/s}}{300000000\text{ m/s}} = 10^{-4}$ . ) for many purposes working scientists have almost entirely stopped using the phrases " relativistic mass " and " rest mass " , finding the former concept to be of little practical use .
i do not have the book here right now , so i am not sure what he is referring to exactly by that comment . but by modifying the energy-momentum tensor , you will change the central charge and actually get a whole new cft . so i would not call that a symmetry of the free scalar theory . however the theory does have a lot more symmetry , let me focus on one chiral sector only . as you know already , the special feature of two-dimensional cft 's is that the spin-two conserved current ( em-tensor ) $\bar{\partial}t ( z ) =0$ , automatically implies that there is an infinite number of conserved currents $\bar{\partial} ( z^n\ , t ( z ) ) = 0$ in the theory . this is essentially the reason why the conformal algebra , extends to the infinite-dimensional virasoro algebra i two-dimensions . lets use the notation $w_2\equiv t$ , where the index refers to the spin . free-field theories , however , contain an infinite tower of higher-spin conserved currents $w_s ( z ) $ where $s= 2 , 3 , \dots$ is the spin . similar to the $w_2$ case , for any conserved current $w_s$ , there is infinite number of associated conserved currents $\bar{\partial} ( z^n\ , w_s ( z ) ) =0$ . thereby each current $w_s$ extends the virasoro algebra with infinite number of new generators , and there is by itself an infinite number of currents $w_s$ . so this is a vast extension of the virasoro algebra . for example in the case of free complex scalar theory the currents are given by [ a ] $w_s ( z ) = b ( s ) \sum_{k=1}^{s-1} ( -1 ) ^k\ , a^s_k\ , :\partial^k\phi\ , \partial^{s-k}\bar{\phi}: ( z ) $ , where $b ( s ) $ and $a^s_k$ are constants . see in particular equations ( 2.11 ) and ( 2.18a ) - ( 2.18e ) in [ a ] . here $w_2 ( z ) $ is the usual energy-momentum tensor leading to $c=2$ and the virasoro algebra . all the generators combined give rise to the so-called $w_\infty^{prs}$-algebra , which contain the virasoro algebra as a " small " subalgebra . similar things can be done for other free-field theories , i myself used a similar construction for the free ghost system in a recent paper . higher-spin extensions of the virasoro algebra are usually called $\mathcal w$-algebras and they do not lead to conventional lie algebras , but certain types of non-linear algebras . see [ b ] for a review . the free field theories realize the rare type of $\mathcal w$-algebras which are usual ( linear ) lie algebras . in higher dimensions free-field theories also have an infinite tower of higher-spin conserved currents and thereby an infinite dimensional symmetry algebra . but each conserved current only lead to a finite number of generators in the algebra , unlike the the two-dimensional case . maybe polchinski is referring to this vast number of higher-spin symmetries of the free-field theories ? [ a ] : bakas and kritsis - bosonic realization of a universal $\mathcal w$-algebra and $z_{\infty}$ parafermions [ nucl . phys . b 343 , 185 ( 1990 ) ] [ b ] bouwknegt and schoutens - $\mathcal w$ symmetry in conformal field theory [ phys . rept . 223 ( 1993 ) 183-276 ]
the length scale $l$ has to be present in the denominator for dimensional reasons – only logarithms of dimensionless quantities are really " well-defined " unless one wants to introduce bizarre units such as the " logarithm of a meter " . on the other hand , the dependence on $l$ is largely trivial and unphysical for most purposes . replace $l$ by $k$ and you will get $$ v_k ( x , y ) = -\ln \left ( \frac{|\vec x|}{k} \right ) = v_l ( x , y ) +\ln ( k/l ) $$ which only differs by the additive shift , $\ln ( k/l ) $ from the original potential you mentioned . shifts of potentials by a constant are largely inconsequential . in particular , the gradient of $v$ , $\nabla v$ , is not changed at all . to derive the simple claim about the shift above , i only used $\ln ( a/b ) = \ln ( a ) -\ln ( b ) $ a few times . the fourier transform of your potential may be derived by realizing what the laplacian of the potential is . the laplacian is the two-dimensional delta-function . in the momentum basis , it is equivalent to the identity $$ ( p_x^2+p_y^2 ) \tilde v ( p_x , p_x ) = 1 $$ which is easily solved by $\tilde v =1/ ( p_x^2+p_y^2 ) $ . however , the behavior of $\tilde v$ is not quite well-defined for the point $p_x=p_y=0$ where one can add a multiple of a delta-function . this is because $$ ( p_x^2+p_y^2 ) \delta ( p_x ) \delta ( p_y ) = 0$$ so $\tilde v \to \tilde v + k \delta ( p_x ) \delta ( p_y ) $ transforms a solution into another solution . of course , the two-dimensional delta-function in the momentum space is nothing else than the fourier transform of the constant term $\ln ( k/l ) $ we discussed in the position basis so the two ambiguities are the same . now , you could think that the momentum-basis form of the potential , $1/ ( p_x^2+p_y^2 ) $ , is unique because it has no length scale in it and no delta-function in it while we do not see a corresponding unique form of the position-basis potential – because the expressions with any length scale are equally good . but this is really an illusion . as a distribution , $1/ ( p_x^2+p_y^2 ) $ is ill-defined ( in the very same sense as $\ln ( x^2+y^2 ) $ would be in the position basis ) and we must specify what its behavior near the origin is . this ambiguity is the two-dimensional generalization of the subtleties connected with the one-dimensional " principal value " of $1/x$ as a distribution . $1/x$ multiplied by a test function is well-defined if we agree that the symmetric region $x\in ( -\epsilon , +\epsilon ) $ is removed . that is what we mean by the principal value . on the other hand , if you compute the two-dimensional integral of $1/ ( p_x^2+p_y^2 ) f ( p_x , p_y ) $ where $f$ is continuous near the origin , you may switch to the polar coordinates where $r$ in $r\ , dr\ , d\phi$ is beaten by $1/r^2=1/ ( p_x^2+p_y^2 ) $ so you still have a divergent integral that has to be regulated . a way to regulate it is to cut if off and remove the disk $r&lt ; p_{\rm min}$ for some small $p_{\rm min}$ . such a cutoff induces an additive shift dependence that is logarithmic in the cutoff . for the same dimensional reasons as before , one has to take the logarithmic dimensionless so what we need to subtract ( or add ? ) to erase most of the dependence on the cutoff is something like $$ f ( 0 ) \ln ( p_{\rm min} / l_p ) $$ where $l_p$ is the counterpart of $l$ , the length scale you started with . sorry if i omitted some dimensionless coefficients . clearly , the change of $l_p$ is equivalent to redefining the distribution by an additive shift by $\delta^{ ( 2 ) } ( \vec p ) \times l_p$ and $l_p \sim 1/l$ plays the same role of the scale we had before , in the position basis .
the roar is indeed due to turbulence . when a solid ( or liquid ) burns it is not the solid that burns . the heat causes the solid to vaporise or emit vapour and it is the vapour that burns . when you have a steady flame the vapour burns smoothly . however , when you blow on it you make the vapour flow , and therefore the flame , turbulent . under these circumstances the vapour burns as , in effect , a series of tiny explosions and this causes the roar . i could not find a basic article on this subject ( for once wikipedia let me down ) , but if you google for " flame turbulence sound " you will find lots of scientific papers on the subject .
as far as i see , $a_y = - k \cos ( \varphi ) &lt ; 0$ , but $a_x = k \sin ( \varphi ) &gt ; 0$ ! at least if you are throwing the projectile in the downward direction . . .
how do you know something is electrically charged ? well , it interacts with other charges . classically , this is described by the maxwell equations , i.e. by fields . one special case of such an electromagnetical field , the plane wave , is what we call light ( amongst other phenomena , radio transmission etc . ) . that is pretty much it ! perhaps it is actually easier understand from a more " modern " point of view : in qft , the electromagnetical fields are quantised to particles called photons . in " slow " processes like electrostatic repulsion of balloons , those photons have low frequency , and thus tiny energy $e=\hslash\cdot \omega$ , where $\hslash$ is the minute ( by classical measures ) planck quantum . therefore , we recognise the interactions readily as smooth fields . at higher frequencies , such as light ( $\approx 10^{18}\:\mathrm{hz}$ ) , the energy for each quantum is much higher , so when an atom emits light it is much more naturally single photons we are dealing with . but in principle , both are the same thing .
in your first reference , page $58$ , equation $ ( 3.55 ) $ , there is a personal definition by the author of what it calls " spinor adjoint of a matrix": $\overline m \stackrel {def}{\equiv} \gamma_0 m^\dagger \gamma_0$ with this definition , as you noticed , you have obviously $\overline {\gamma^\mu}= \gamma^\mu$ the above definition of " spinor adjoint of a matrix " is compatible with the definition of the adjoint $ ( 3.54 ) $: $\overline \psi = \psi ^\dagger \gamma_0$ in the following way : $\overline {m\psi} = ( m\psi ) ^\dagger \gamma_0 = \psi^\dagger m^\dagger \gamma_0 = ( \psi^\dagger \gamma_0 ) ( \gamma_0 m^\dagger \gamma_0 ) = \overline {\psi}~ \overline {m}$
you mistake is that you use the absolute value " of the spatial components " ( your words ) of the velocity only . picking spatial components only is clearly not a lorentz-covariant procedure , so it cannot calculate the invariant " feelings of the astronauts " . instead , the right condition is given by the same inequality but $|d u^\mu / d \tau|$ is the length of the four-vector one obtains by differentiating the four-velocity $u^\mu$ , where $u_\mu u^\mu = 1$ , over the proper time $\tau$ . the vector $d u^\mu / d \tau$ is spacelike and perpendicular ( according to the lorentzian metric ) to the velocity vector $u^\mu$ itself ; but this derivative is not a purely spatial vector in any inertial system . in a frame in which the spatial components of $u^\mu$ are already nonzero , $d u^\mu / d \tau$ contains a nonzero time component , too . when you calculate it correctly , the proper time needed to achieve the speed of light is infinite . the easiest way to calculate it is one that assumes some knowledge of the lorentzian geometry and how it is analogous to the euclidean geometry . a uniformly accelerating object in the euclidean spacetime would produce a circular world line . in the real , minkowski space , the world line is a hyperbola . the coordinates after proper time $\tau$ may be written in analogy with sines and cosines but they are hyperbolic ones : $$ t = \sinh ( \tau/\tau_0 ) , \quad x = \cosh ( \tau/\tau_0 ) $$ here , $\tau_0$ is a constant depending on the acceleration . consequently , the speed after proper time $\tau$ is simply the ratio , $$ v = \tanh ( \tau/\tau_0 ) $$ for a small $\tau$ , this gets reduced to $\tau/\tau_0$ in the limit and the $\tau$-derivative $1/\tau_0$ should be the ( maximum ) acceleration $g_m$ so $\tau_0=1/g_m$: $$ v = \tanh ( \tau g_m ) $$ in the $c=1$ units . you may invert it : $$ \tau = \frac{c}{g_m} {\rm arctanh} ( v/c ) $$ where i restored the powers of $c$ for your convenience . note that arctanh of one is infinity . for a small $v/c$ , one uses ${\rm arctanh}\ , x\approx x$ and the right formula reduces to your nonrelativistic formula from the original question .
an object is not necessarily heated to a plasma when it falls into a black hole . with quasars matter spirals down towards the event horizon so both it is speed and density increases , but this does not heat it directly . it is because matter interacts with the other matter around the event horizon that you get collisions and heating and the spectacular x-ray emission . by contrast the black hole at the centre of our galaxy is thought to be fairly quiet because it is already gobbled up all the matter in it is vicinity . if you jumped into it you had probably make it through the event horizon unharmed and it would only be near the singularity that tidal forces squished you . you need to bear in mind that once matter has passed the event horizon it is fall to the singularity is very quick , so there is not very much matter within the event horizon that has not already fallen into the singularity . what happens to the matter at the singularity no-one knows . response to comment : my point is that an acretion disk is not a feature of all black holes . accretion disks only form where a black hole is actively swallowing matter . also accretion disks will form around any heavy object . arguably saturn 's rings are a form of accretion disk with the matter in them eventually falling into saturn . my other point is that for matter falling into a black hole nothing special happens when it crosses the event horizon . if you were falling into a black hole then you would not be able to detect when you had crossed the event horizon . so if you had been heated up by friction in an accretion disk before you hit the event horizon you had be in pretty much the same state after you had crossed it . the state you are in , whether it is plasma or not , is dependant on how much you got heated up as you fell through the accretion disk ( if an accretion disk is present ) and is not anything specifically related to the presence of an event horizon .
the residual resistivity can be due to any kind of imperfection which destroys the complete periodicity of the lattice ; most famous kinds are structural defects ( provided that they happen in a disordered manner ) as you correctly mention and impurities ( again they should be distribited in a disordered manner ) . the way alloys are made , their lattice structure is certainly disordered since people do not make them by manipulating the lattice to make it perfect , and this disorder introduces a mean-free-path of order of the microscopic inhomogenities ( the spatial fluctuation of the density of the different metals composing the alloy ) of the alloy to the electric transport . this disorder has a small temperature dependence ( at least at low temperatures ) compared to the contribution from phonons and i guess it usually remains the main source of resistivity . now as to why the trend is more linear in the case of alloys , your graphs are not on the same scale to be used for the comparison and i was not able to find a good collection of graphs either , so let 's wait for a better collection of graphs before the final conclusion ; maybe that is not always the case .
motion of the particles indeed can be described as in your first point : for example , propagation of electron can be seen as a creation of a virtual electron-positron pair ahead of the propagating electron , and later annihilation of the first electron with the positron so the newly-created electron remains .
i have come to the conclusion that the language used in the paper is probably not completely accurate . the paper mentions determining displacement based on " reflection time . " i believe that the device actually uses trigonometry and the angle formed by the bounced laser beam to determine the displacement of the torsion pendulum . the angle is measured using a ccd . ( note : there were going to be significantly more links for sources . however , i am unable to make a post with more than 2 links until i have 10+ reputation . . . ) i have come to this conclusion based on assumptions and other research . first the paper suggests that this is a relatively common piece of equipment in the aerospace industry . in addition other papers suggest that sub-millimeter time of flight ranging is near state of the art . these papers used a component called a single-photon avalanche diode ( spad ) to measure the time of flight . additional research on this component indicates that they can commonly measure events in the $\approx100$ps range and state of the art devices can measure down to the $1$ps range . this is fast enough for millimeter resolution , but not the sub-micrometer resolution claimed by the paper . further the paper refers to the measuring device as a linear displacement sensor . after additional research , you can find off-the-shelf devices ( matching my earlier description ) that claim sub-micrometer resolutions . with all of that in mind , the answer to this question is two-fold . first the paper does not measure time of flight of the reflected photons , but instead measures the angle of the beam formed by the reflection . second , high resolution ( sub-millimeter ) time of flight for ranging can be achieved with a sensor that utilizes a spad .
the standard book is introduction to solid state physics ( 8th ed . 2005 , isbn 0-471-41526-x ) by charles kittel . your question should be answered in chapter 13 . all that is said there should in principle be applicable to semiconductors . but since their bandgap is lower than in dielectrics you might have a problem measuring this ( they might just be too conductive an the effect is lost ) . on the other hand in your prototypical gaas crystals there is a rather strong effect in the ( 111 ) direction due to the stacking of ga and as layers and their asymmetric response to stress . . . note though that this has nothing to do with electron/hole pairs . this is strictly a polarisation phenomenon .
since this question is still open and therefore not definitely answerable at present , i save the valuable discussion of the topic in the comments as an answer such that it does not get lost : this is just an accident of 10 dimensions--- there is too much supersymmetry to have a full susy superspace . it is a very good question , but research level , if you answer it fully , everyone will breathe easier . – ron maimon apr 12 at 2:11 3 up voted there are superspace formulations for 4d n=4 theories , the problem is that they only are on-shell formulations . the n=4 multiplet contains the n=2 hypermultiplet and there is a no-go theorem saying that there is no off-shell formulation with a finite number of auxiliary fields . thus you get projective and harmonic superspaces with infinite numbers of auxiliary fields . this works for n=2 , but in n=4 all constraints to reduce the unconstrained superfields down to the physical multiplets force the fields on-shell . – simon apr 12 at 3:47 3 up voted and as @ron says , the reason why it is so difficult to construct such formulations is an open research-level question . if the reason was known , then we had either have a workable n=4 superspace formulation or a no-go theorem by now . . . – simon apr 12 at 3:48 3 up voted dear dilaton , good question . ron and simon have already answered to some extent and i will only offer a different extent , extending ron 's comment in particular . if you want to make n=1 susy in 4d i.e. 4 real supercharges manifest , you need 4 superspace fermionic coordinates . with 16 supercharges , you would probably need at least 16 fermionic coordinates in the superspace but then the fields would have 216=256 components which is pretty high give that you only need 16 on-shell components only . most of the component fields would have to be auxiliary , linked to deritives of others , etc . hard – luboš motl apr 12 at 5:37 4 up voted there is also an interesting twistor-like transform for the 10d super yang-mills by a guy called witten – luboš motl apr 12 at 5:39 thanks guys for these valuable comments and the cool links therein . i would " like " and appreciate them as " partial " answers ( since as you say there is no full answer on this yet ) too . . . :- ) . – dilaton apr 12 at 8:45 @lubošmot l must admit that twistors are one of my black holes of ignorance ( i did not get it from roger penrose 's " road to reality" ) :-/ . . . so i am checking from time to time if i can find a nice " pedagogical " introduction to this on trf ; - ) – dilaton apr 12 at 8:50 1 up voted @dilaton : this is the problem with research level stuff , i can not answer because i do not feel confident enough in my biases about what the answer could or should be to put them in writing , and i would mention a bunch of things that i tried and did not work to answer this , and are not interesting , and i think everyone else is hesitant to answer too for similar reasons . maybe you could copy the comment thread into an answer box , and accept your own answer . – ron maimon 2 hours ago 1 up voted i mean , if you want a little more on this--- there is the question of whether superspace is fundamental in the first place--- it is just a trick for writing multiplets in a way that takes the susy off shell naturally , but the physical reasoning has always eluded me . i tried nicolai maps as an alternative , but it never worked , and it always is tantalizingly close to working , and i tried learning harmonic superspace for on-shell n=4 , but although it is correct , its so annoyingly complicated to work with ! and the s-matrix is simple , so there is a better language out there , i do not know what . – ron maimon 1 hour ago thanks @ron maimon , that is a good idea to save the discussion into an answer . i`m somehow intrigued too by the question if superspace itself could have some physical meaning . . . – dilaton 2 mins ago even though the question is still open , it could probably nevertheless be worthwhile to know what you tried and why it did not work . i mean similar to some kind of " null results " who can be interesting too . . . ? – dilaton
if you consider a standard differential operator $b$ working on functions defined in $\mathbb r^n$ , like $\partial/\partial x_i$ or a polynomial of partial derivatives , and pick out a sufficiently smooth function $f$ vanishing in a neighbourhood $\omega$ , you see that also $bf$ vanishes therein . this is the relevant notion of locality for operators . in the rhs of the equation you wrote down an operator shows up which does not fulfil locality in the sense i said . that equation is , in fact , the equation satisfied by the positive energy solutions of klein-gordon equation . the operator in the rhs cannot be defined by formal taylor expansion ( it works only formally ) , but one has to use spectral theory . in the considered case it is equivalent to translate that equation in fourier transform . non locality arises here due to a known property of the operator $a:= \sqrt{-\delta + ai}$ and , more generally , for $ ( \delta + ai ) ^\nu$ with $\nu \not \in \mathbb z$ . this property is called anti locality ( i.e. . segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ) and is related to the famous reeh and schlieder property in qft . anti locality means that if both $f$ and $af$ vanish in a bounded region $\omega \subset \mathbb r^3$ then $f$ is everywhere zero . if $f$ has support included in a bounded open set $\omega$ , then , remarkably and very differently from what happens for standard differential operators , $af$ does not identically vanish outside $\omega$ otherwise $f$ would be the everywhere zero function .
the first image shows an object traveling at mach 1 ( $v=c$ ) . the second one shows the object traveling at some supersonic velocity ( $v&gt ; c$ ) . for both the cases , the longitudinal pressure waves pile up . say the observer is standing in the ground and the object is traveling at $c$ . the observer can not hear the pitch of sound because , the waves reach him all at once and hence , he had hear a loud " bash " . the most necessary thing is that he had to wait until the source arrives . when the source is directly overhead , he hears the shock waves . when the object breaks the sound barrier ( supersonic ) , it is somewhat worse . the same loud " thump " is produced here . but , the observer would notice a delay in sound ( i.e. ) he has to wait for the shock waves to reach him . there is also this mach cone produced by these waves since the waves group so fast behind the object . and so , there is a region of high pressure at first followed by a low pressure zone . thus , if the object passes by in some comparable distance , it makes a lot of disturbance , " breaking things " , etc . . . the comic thing is , for someone inside the aircraft , he can still speak with his partner , can hear the bump of a ball on the plane , etc . the problem is only for the distant observer who suffers . . .
1 ) yes . you are right with the perception of white and black . regarding other colors , it depends on the energy of photon which excites your cone and rods . 2 ) the brightness in physical ( cosmological to be specific ) terms , is differentiated into luminosity and flux . both are quite comparable . luminosity is the amount of energy emitted per second ( $j/s$ or simply , watts ) from the source whereas flux is the power received per unit area $ ( wm^{-2} ) $ . both are related simply by using the surface area of a sphere of radius $r$ , which is probably the distance from the source and the relation is $l=f ( 4\pi r^2 ) $ . this flux tells you how much energy is emitted as a function of time which explicitly shows the amount of photons impinging on your retina . when colors combine to produce different colors , is there any photon combining that exists or is it because your eyes see mixtures of photons and not photons themselves ? for now , there is a difference . photons do not combine . instead ( as i have previously mentioned ) , different photons excite your photo-receptors at different or same time periods . based on the excitation , the color is observed by you . on the other hand ( if those were assumed to have wave character ) , they can constructively or destructively interfere to produce additive or subtractive color . now , to other question . to get light blue , is not it a mixture of mostly blue photons with white light ( photons of all frequencies ) to produce a blueish white or a light blue ? this does produce light blue . but , the flux factor produces greyish blue and not whitish blue . now as white light contains all the frequencies , you will perceive all in the same way but with several blue-ly energized photons along with it . imagine this to be the surface area of a white circular object with some blue spots in it . at a comparable distance , you can see it as whitish blue ( your favorite ) . . . if you are still confused of brightness , wiki has a nice quote on it . . . " brightness " was formerly used as a synonym for the photometric term " luminance " and ( incorrectly ) for the radiometric term " radiance " . in rgb color space , brightness can be thought of as the arithmetic mean of r , g and b color coordinates ( although some components make the light seem brighter than others , which again , may be compensated by some display systems automatically )
it will actually weigh $f_b+\rho_lv_lg$ which is $\rho_lv_bg+\rho_lv_lg$ , and not one of the two options you say . consider the liquid as the system and the block as an external body . now we know that the liquid applies a buoyant force on the block . according to newton 's third law , the block will apply a reaction force on the liquid , equal in magnitude and opposite in direction . thus total force on liquid is $f_b+f_g$ which gives $\rho_lv_bg+\rho_lv_lg$ . note that this is the weight when the block is attached to the spring balance , and suspended in the liquid . if the block is kept on the floor not attached to the spring balance , the reading weight will be different . edit : in the case when the block is resting on the floor , the weight will simply be $\rho_lv_bg+\rho_lv_lg$ , because when you consider the block and liquid as a system , the buoyant force will become an internal force and cancel out on the whole system . so the only force responsible for the weight will be gravity .
enthalpy ( h ) is the internal energy ( u ) of a material plus the product of pressure ( p ) and volume ( v ) . $h = u + pv$ by definition when something boils , the gas phase takes up more volume than the liquid phase . so unless the boiling is in a vacuum , work is being done by expanding against a pressure , such as atmospheric pressure . this represents a change in the pv term of enthalpy . at a given temperature and pressure ( say boiling water at atmospheric pressure ) , pv is greater for the gas phase than the liquid phase . additionally , intermolecular forces hold molecules of liquids together . for example , molecules of a particular compound ( say fluoromethane ) may have a permanent net dipole moment , and the positive end of one molecule is attracted to the negative end of the other . there are other types of intermolecular forces such as hydrogen bonding and london forces . it takes energy to pull the molecules apart against such forces . this represents a change in the internal energy ( u ) term of enthalpy . at a given temperature and pressure ( say boiling water at atmospheric pressure ) , u is greater for the gas phase than the liquid phase . in summary , a desired region is cooled because some of its energy is transferred to the refrigerant to increase its enthalpy . part of the energy goes to expanding against a pressure ( the pv term ) and part goes to the increase in internal energy ( the u term ) .
you are right and the author is wrong . the problem of p=np is a pure mathematical problem , which has nothing to do with physics . even though quantum mechanics ( or whatever physical system ) can solve all problem in blink of eye , it still does not prove whether p=np or not . the key point is that all computations are based on physics , but not the reverse . in computer complexity theory , they treat these ( existing or imaginary ) superpower machine as an oracle machine , which can give an answer in a single computational step . this formulation allows them to analyze quantum computer . the claim of non-observable macroscopic quantum effects because of p ! =np is based on the following argument : to prove macroscopic quantum effects , we need to compare the physical system with the simulation results of schrodinger equation . so , if we can not simulate schrodinger equation efficiently , then we can not prove any quantum effect . as shown in the paper : this implies that in the case , in which the problem $\phi_\psi$ would be intractable , the deterministic quantum model of a macroscopic system ( built around the exact solutions to the system schrodinger equation ) would be without predictive content inasmuch as there would be no practical means to extract the prediction about the system future state from the schrodinger equation . in this manner , a schrodinger cat state – as a linear combination of the exact ( and orthogonalized ) solutions to the system schrodinger equation – would be predictively contentless and for this reason unavailable for inspection . the author clearly does not familiarize with quantum mechanics , nor the schrodinger equation . schrodinger equation is only a part of qm . he also does not understand the particle concept in the schrodinger equation . a particle is not an atom . this is a basic concept that most physics student should have understand after half dozen courses in qm . the interference of one c$_{60}$ molecule can be described by one particle wavefunction $\psi ( x ) $ . there is no need to solve a 60-particles wavefunction $\psi ( x_1 , . . . , x_{60} ) $ , which is already extreme hard to solve by current computers . if a schrodinger cat state exists , you can always perform a bell-state type measurement , even at the macroscopic level . there is no need to solve schrodinger equation with large number of variables in wavefunction $\psi ( x_1 , . . . , x_{10^{23}} ) $ to know the result , since the system should be effectively described by a two state system .
who wrote that passage ? it contains some misunderstandings . all i know is that $10^{500}$ is a very large number . it is a finite number . how many theories do you know which have a finite number of solutions ? have you tried to count the number of solutions of plain einstein-yang-mills-dirac-higgs theory without its string-theoretic uv completion ? there are not only infinitely-many solutions , there is a hugely infinite- dimensional space of solutions . this is the usual state of affairs for most every theory of physics ever considered . string theory is special in that it puts many more constraints on the solutions , such as to even leave just a finite number ( under some assumptions ) . what exactly is a ' solution ' in string theory ? a background for perturbative string theory is a choice of 2-dimensional superconformal qft of central charge -15 . this can be interpreted as describing an effective target space geometry which is a solution to a higher dimensional supergravity theory with higher curvature corrections . a " solution " to string theory is a solution of the equations of motion of that . at least without non-perturbative effects taken into account . see on the nlab : landscape of string theory vacua for more . i thought string theory was supposed to be finite , why do perturbative series still diverge ? string theory is thought to be loop-wise finite , thus being a renormalized perturbative theory . no sensible remormalized perturbative qft can have converging perturbation series . the perturbation series must be an asymptotic series to be realistic , and it comes out exactly like this in string perturbation theory . see at the string theory faq on the nlab the item isn’t it fatal that the string perturbation series does not converge ? is there any experimental technique to limit the number of ' solutions ' ? will experimental techniques be able to pinpoint a solution within present day string theorists ' lifetimes too ? models that have been and are being built in string phenomenology approximate the standard model to more detail than probably most people are aware the standard model even has . check out some of the references there . given the slow but continuous flow of new articles on these matter , one sees that some people are slowly but surely working on improving ever further . check out the references at string phenomenology . [ edit : i have now added a corresponding item to the nlab string theory faq : what does it mean to say that string theory has a “landscape of solutions” ? ]
buying directly from the shop ensures you will get collimated binocular instead of buying it online . but there are some companies such as garret opticals from us . they ship with great care and the binocular comes with excellent collimation . i ordered their binocular gemini 15x70 it came with excellent collimation . so if you are in us then you can order from them . yea the difference between the optical clarity depends on the type of prism used .
this problem is called wigner 's friend . basically the answer according to quantum mechanics is 2 . if my friend makes a measurement causing collapse of the superposition , then when i return i will not see any quantum interference , so a superposition state would predict the wrong outcomes . if my friend does not tell me what happened in his measurement , then i must assign a mixed state , or classical probability distribution , to the system . whether you think this is weird or not depends on your idea about what quantum mechanics is telling you . if you think that quantum states just represent knowledge , then there is nothing weird : collapse just means updating your probability distribution when you learn new information . if you think that quantum states are physically real things , then there is no problem if the system is localised : the measurement must involve a physical interaction which changes the physical state . the spanner in the works occurs when you deal with entangled states . then the collapse in my lab also appears to have consequences for measurement outcomes in distant places . bell 's theorem tells you that you cannot have both a local and realistic description of this process . if you think quantum states are only information , then you must accept that physical properties of quantum systems do not exist independently of observation . if you think quantum states are physical objects , then you must accept that the interaction in my lab had a non-local influence on the system in the distant lab . most people like locality , so choose the former option . particularly because the concept of locality is deeply rooted in our understanding of elementary particle physics and gravity . but the two options are not distinguishable by any currently known experiment , so the inevitable and continuous arguments between people from the two camps are pretty pointless . nevertheless , i expect at least one person will start arguing with me over this post : )
in canonical quantization , a quantum field is a linear combination of so-called " creation and annihilation operators " . that means that the field $\phi$ creates and destroys particles of type $\phi$ . the state $|0\rangle$ is the vacuum : the state with no particles . if $\phi$ is a quantum field that creates and destroys particles , it must be that $\langle 0 | \phi | 0 \rangle=0$ , because the state with particles created/destroyed , $\phi|0\rangle$ must be orthogonal to the empty state $|0\rangle$ . we have no choice , then , but to write $\phi=v+\eta$ , such that $\langle \eta \rangle=0$ . th new field $\eta$ now has a good particle interpretation , whereas the original field $\phi$ did not , because $\langle\phi\rangle = v$ .
the gas will not ignite as long as the concentration is higher than uel/ufl ( upper explosive/flammability limit ) . so as long as there is no hole , you should be fine . that said , i would not play with hydrogen if i can use helium instead !
i was always told that to find whether or not a field is conservative , see if the curl is zero . this is almost always true , but not always true . i have now been told that just because the curl is zero does not necessarily mean it is conservative . correct ! to illustrate what is going on , let 's do an example . conside the following vector field : $$\vec{v} ( x , y ) =\frac{-y\hat{x}+x\hat{y}}{x^2+y^2} . $$ note that $\vec{v}$ is not defined at the origin . is $\vec{v}$ conservative ? let 's define " conservative " as follows a vector field $\vec{v}$ is conservative if for any closed path $c$ , the integral $\int_c \vec{v}\dot\ , d\vec{l}=0 . $ consider the path parametrized as $x ( t ) =r\cos ( 2\pi t ) $ and $y ( t ) =r\sin ( 2 \pi t ) $ for $t$ going from 0 to 1 . this path is just a circle of radius $r$ centered on the origin . the displacement on the path is $$\frac{d\vec{l}}{dt} = 2\pi r \left ( - \hat{x}\sin ( 2\pi t ) + \hat{y}\cos ( 2\pi t ) \right ) . $$ if we integrate our example $\vec{v}$ on this path we get $$\begin{align} \int_c \vec{v}\cdot d\vec{l} and =\int_{t=0}^1 \left ( \frac{-y\hat{x}+x\hat{y}}{x^2 + y^2} \right ) \cdot ( 2\pi r ) \left ( -\hat{x}\sin ( 2\pi t ) + \hat{y}\cos ( 2\pi t ) \right ) \ , dt\\ and = 2\pi \end{align}$$ which shows that $\vec{v}$ is definitely not conservative . note that the integral does not depend on the radius $r$ of the path . now , we compute the curl of $\vec{v}$ . for convenience , define $r\equiv x^2 + y^2$ , i.e. $r$ is the radial polar coordinate . $$\begin{align} \nabla \times \vec{v} and \equiv \frac{d\vec{v}_y}{dx} - \frac{d\vec{v}_x}{dy}\\ and =\frac{r^2 - 2 x^2}{r^4} - \frac{-r^2 + 2 y^2}{r^4} \\ and = \frac{2r^2 - 2r^2}{r^4}\\ and = 0 . \end{align}$$ we have now shown that $\vec{v}$ has zero curl . a consequence of this is that if we were to integrate $\vec{v}$ along any little path around a point where $\vec{v}$ is defined , we are guaranteed to get zero . thus , $\vec{v}$ has zero curl but is not conservative . what is going on ? if you picture $\vec{v}$ you will see that it is a swirl of vector lines circling the origin . the magnitude of the lines decreases as you move away from the origin . this decrease is just right so that if you were to integrate around a little loop which does not encircle the origin ( i.e. . if you check the curl ) you get zero . however , because of the global circling around the origin , if you integrate along a loop which does enclose the origin , you get something non-zero . thus , you can think of the integral as either " feeling " the presence of the origin and picking up the $2\pi$ we calculated , or not feeling the origin and giving zero . it is like the origin is a special point worth $2\pi$ . this is really interesting ! our field $\vec{v}$ is conservative everywhere locally , but if you make a path around the origin you can get a nonzero integral , so $\vec{v}$ is not conservative globally . remember we pointed out that $\vec{v}$ is not defined at the origin ? this is not an accident . vector fields which are conservative locally but not globally must have " holes " at which they are not defined . in fact , these vector fields must be approaching infinity near their holes , which $\vec{v}$ most certainly is , as you can check [ 1 ] . those infinite points have " residues " which show up in integrals which go around them . for the experts in the audience , this is exactly the same residue you get from integrating around a simple pole in the complex plane . let 's get back to your question to show this is conservative i would go ahead and take the curl . it will be zero - but that is not definitive proof it is conservative ? how would i show it is ? as you have said , and we have demonstrated , having zero curl does not guarantee that a field is conservative . what does guarantee that a field is conservative is that you can express it as the gradient of a scalar function . in more general mathematical terms , if there exists a function $f$ such that $\nabla f = \vec{v}$ , then $\vec{v}$ is said to be exact . an exact vector field is absolutely 100% guaranteed to conservative . so , one answer to your question is that to show a vector field is conservative , just show that it can be written as the gradient of a function . another answer is , calculate the general closed path integral of the vector field and show that it is identically zero in all cases . let 's go on though , because this is really super interesting stuff . vector fields with zero curl are guaranteed to be exact , meaning that zero curl guarantees conservativeness , unless the vector field has holes ( i.e. . points at which it is not defined ) . so , the mantra you learned that zero curl indicates conservativeness is almost always true , but it fails for vector fields which have holes , like our example $\vec{v}$ does at the origin . now here 's the really amazing part . if i tell you that a vector field has exactly 1 hole , and that it has zero curl but is not exact , there is only one vector field it can possibly be ( up to addition of other exact vector fields ) . in other words , if i tell you that a vector field $\vec{w}$ has zero curl , has one hole , and is not a gradient of any function , then you know for sure that $\vec{w}=\vec{v} + \vec{\lambda}$ where $\vec{\lambda} = \nabla f$ for some $f$ . if repeat the same situation but with two holes , then you know that $\vec{w}$ is expressible as a linear combination of specific curl-less but not exact vector fields associated to the two holes . this whole business generalizes to high dimensional spaces . if you like it , read up on differential forms . you can try the book " analysis on manifolds " by munkres , although that is a very " mathy " book . one last thing . instead of talking about having zero curl and being the gradient of a function , you can talk about having zero divergence and being the curl of another vector field . normally , if a vector field has zero divergence , you can write it as the curl of something else . the electric field of a point charge is conservative and has zero divergence . however , it is not the curl of any vector field . in fact , it is the only $^{ [ 2 ] }$ vector field in three dimensions which has zero divergence and is not a curl of something else . and of course , the electric field of a point charge goes to infinity at the charge point , so this is one of those fields where having a " hole " allows it to break the usual rules . how did nature know to do that ? [ 1 ] the numerator of $\vec{v}$ goes as $r$ while the denominator goes as $r^2$ . therefore the whole thing goes as $1/r$ which diverges near the origin . [ 2 ] this is a slightly incorrect statement , but it is ok for now .
i ) the proofs of both the first ( algebraic ) bianchi identity and the second ( differential ) bianchi identity crucially use that the connection $\nabla$ is torsionfree , so they are not entirely consequences of the jacobi identity . proofs of the bianchi identities are e.g. given in ref . 1 . ii ) the second bianchi identity may be formulated not only for a tangent bundle connection but also for vector bundle connections . iii ) the lie bracket in the pertinent jacobi identities is the commutator bracket $ [ a , b ] :=a\circ b -b\circ a$ . the jacobi identity follows because operator composition "$\circ$" is associative . iv ) in the context of yang-mills theory and em , the second bianchi identity follows because the gauge potential $a_{\mu}$ and the field strength $f_{\mu\nu}$ may be viewed as ( part of ) a covariant derivative and corresponding curvature tensor , respectively . references : m . nakahara , geometry , topology and physics , section 7.4 .
i will formulate the following in such a way , that the language does not change too much within the answer . this also emphasizes the analogies of related concepts . classically , you have a configuration/state $\psi$ , which is characterised by coordinates $x^i , v^i$ or $q^i , p_i$ and/or any other relevant parameters . then an energy is a function or functional of this configuration $$h:\psi\mapsto e_\psi , \ \ \mbox{where}\ \ e_\psi:=h [ \psi ] . $$ here $e_\psi$ is some real ( energy- ) value associated with the configuration $\psi$ . to name an example : let $q$ and $p$ be the coordinates of your two-dimensional phase space , then every point $\psi= ( q , p ) $ characterises a possible configuration . the configuration/state $\psi$ here is really just the pair of coordinates . the scalar function $h ( p , q ) =\frac{1}{2m}p^2+\frac{\omega}{2}x^2$ clearly is a map which assigns a scalar energy value $e_\psi$ to every possible configuration $\psi$ . the evolution of $\psi$ in time is determined by $h$ , see hamilton 's equations . this might be viewed as the point of coming up with the hamiltonian in the first place and it is typically done in such a way , that the energy value $e_\psi$ will not change with time . see also this thread for a related question . what you call " energy " is pretty much determined by this criterium . in the case of a time independent hamiltonian ( as in the example ) and if the time developement of observables $f$ is governed by $\frac{\mathrm{d}f}{\mathrm{d}t} = \{f , h\} + \frac{\partial f}{\partial t}$ , then you have $\frac{\mathrm{d}h}{\mathrm{d}t} = \{h , h\} = 0$ and the conservation of the quantiy $e_\psi:=h [ \psi ] $ is evident . of course , you might want to model friction processes and whatnot and it then might be difficult to define all the relevant quantities . in quantum mechanics , your configuration $\psi$ is given by a state vector $|\psi\rangle$ ( or an equivalence class of such vectors ) in some hilbert space . there are many vectors in this hilbert space , but there are some vectors $|\psi_n\rangle$ , which also span the whole vector space and which are also special in the following sense : they are eigenvectors of the hamiltonian operator : $h|\psi_n\rangle = e_n|\psi_n\rangle$ . here $e_n$ is just the real eigenvarlue and i assume that i can enumerate the eigenstates by an descrete index $n$ . now for every point in time , your state vector $\psi$ is just a linear combination of the special states $\{\psi_n\}$ . ( as a remark , notice that all the time dependencies of states are left implicit in this post . ) therefore , if you know how $h$ acts on all the $\psi_n$ 's , you know how $h$ acts on any $\psi$ . since a hilbert space naturally comes with an inner product , i.e. a map $$\omega:|\psi\rangle\times|\phi\rangle\mapsto\omega ( |\psi\rangle , |\phi\rangle ) \equiv\langle\psi|\phi\rangle\in\mathbb{c} , \ \ \mbox{satisfying}\ \ \langle\psi|\psi\rangle&gt ; 0\ \ \forall\ \ |\psi\rangle\ne 0 , $$ you can define a new map $$\omega_h:\psi\mapsto e_\psi , \ \ \mbox{where}\ \ e_\psi:=\omega_h [ \psi ] , $$ with $$\omega_h [ \psi ] :=\omega ( |\psi\rangle , h|\psi\rangle ) \equiv\langle\psi| h|\psi\rangle . $$ compare the lines above with the classical case . here $e_\psi=\ . . . =\langle\psi| h|\psi\rangle$ is then called the expectation value of the hamiltonian in the phyical state . it is the energy value associated with $\psi$ , which is real due to hermiticity of the hamiltonian . also , like in the classical case , the time evolution of any state $\psi$ ( resp . state vector $|\psi\rangle$ ) is determined by the observable $h$ , an operator in the qm-case . and as stated above , exactly this $h$ , together with the state/configuration $\psi$ , gives you the energy values $e_\psi$ associated with $\psi$ . this relation of time and energy is by construction : the schrödinger equation is an axiom ( but a natural one , see conservation of probability ) , which relates time evolution and hamiltonian . now , if the time dependency of the state is governed by the hamiltonian ( whatever it might look like in your scenario ) , then so is the time dependency of $\langle\psi| h|\psi\rangle$ . and if $\ i\hbar\frac{\partial}{\partial t}|\psi\rangle=h|\psi\rangle\ $ is true for all vectors in your hilbert space , i.e. if $i\hbar\frac{\partial}{\partial t}=h$ holds as an operator equation , then these two really are just the same operator . if you ask for an interpretation for this , then i would suggest you hold on to the quantum mechanical relation between frequency and energy . regarding the equation which determines time evolution , quantum mechanics is much easier than classical mechanics in a sense , especially if you come with some lie group theory intuition in your backpack .
the central charge term as an example of a quantum anomaly ; a symmetry that is modified in the quantized version of a classical theory . the central charge is , in fact , often referred to as the conformal anomaly . as di-francesco et . al . put it at the start of section 5.4.2: the appearance of the central charge $c$ , also known as the conformal anomaly , is related to the " soft " breaking of conformal symmetry by the introduction of a macroscopic scale into the system . they then go on to show that if , for example , you consider a generic conformal field theory on $\mathbb c$ , and if you map the theory onto a cylinder of circumference $l$ with coordinate $w$ , then \begin{align} \langle t_\mathrm{cylinder} ( w ) \rangle = -\frac{c\pi^2}{6l^2} \end{align} they also , in appendix $5a$ , go on to show that when a conformal field theory is defined on a curved two-manifold , then the central charge is related to the so-called trace anomaly ; \begin{align} \langle t^\mu_{\phantom\mu\mu} ( x ) \rangle = \frac{c}{24\pi} r ( x ) \end{align} where $r$ is the ricci scalar . the central charge can be seen to arise naturally in radial quantization in the operator formalism of cft : see di-francesco et . al chapter 6 . anomalies arizing from quantization are not restricted to conformal symmetry . see , for example , the chiral anomaly or the gauge anomaly .
yes , your understanding is correct . someone who is conscious and/or usefully using the result of the measurement is not necessary for the experiment to be modified ; it is the particles used to detect the slit or other information that modify the experiment . they interact with the photon in the slits etc . ( although it is hardly other photons because photon-photon interactions are virtually unmeasurably weak ) . quantum mechanics implies that the experiment is modified regardless of any consciousness that is really not a part of physics . however , the existence of a conscious observer who learns the value of an observable is a sufficient condition for the experiment to be altered . without an alteration of the experiment , one could never be " aware " of the slit through which the photon went , for example . so the conscious realization of an outcome is a possible proof – one of the possible proofs – that some particles or objects measuring the physical system ( double slit experiment , for example ) have modified it . that is an " indirect proof " in some sense – one may always say that the actual " constructive " proof involved the actual particles/apparatuses that were used to measure the " which slit " information or any other observable .
the lifetime of hydrogen bonds in room temperature water is picoseconds . the idea that they could persist long enough to drink is wrong by a factor of trillions .
the space station could shoot itself , but it is extremely unlikely to happen by accident . assuming your space station is in a circular orbit you can calculate it is position in polar co-ordinates as a function of time $ ( r ( t ) , \theta ( t ) ) $ very easily since $r$ is constant and $\theta = 2\pi t/\tau$ where $\tau$ is the orbital period . when you fire the cannon the shell is in a different orbit , and specifically it is in an elliptical orbit $ ( r' ( t ) , \theta' ( t ) ) $ . for the station to shoot itself the two orbits must intersect , i.e. at some time $t$ you have simultaneously : $$r ( t ) = r' ( t ) $$ $$\theta ( t ) = \theta' ( t ) $$ the problem is that for a generic elliptical orbit the expressions for $r' ( t ) $ and $\theta' ( t ) $ are not at all simple so there is no easy way to solve the above simultaneous equations and work out at what time , if ever , they intersect . there is certainly no obvious reason to suppose they should intersect . you can see that it is possible for the spaceship to shoot itself . if you fire the cannon radially outwards the shell will fall behind the space station . if you fire in the direction of motion the shell will move ahead of the space station . so there must be some angle in between where the shell hits the space station . however this will be the exception rather than the rule . i am aware this is not a great answer since i can not solve the equations of motion and give you a rigorous answer . if anyone else can do this i would be very interested to see the calculation .
the mmf due to a current is determined by the current through the surface bounded by the closed path along which the magnetic field is integrated . a closed path within a cross-section of a conductor with , say , a uniform current density , will have a non-zero mmf associated with it and thus , a non-zero magnetic field exists within the conductor .
the problem ignores the friction before it hits the spring because it is not relevant to the question . the question asks for the maximum compression of the spring . the force of friction prior to the spring is irrelevant because you are given the velocity at the point of impact . the friction before impact would be relevant if you were not given the velocity . i am not sure what you mean about the physical meaning . the use of one or both of the quadratic roots depends on the setup of the origin and its axis . a negative number will not be part of a quadratic solution with these types of problems , usually ! in this problem a negative value will not be considered because it does not make sense for the problem since we know the spring moved to the right ( in the positive direction ) . therefor - . 25 cannot be used .
two collegues of mine wrote a monograph faragó , karátson : numerical solution of nonlinear elliptic problems where conditioning and preconditioning are an issue . this may be some help though . . . if you want it more elementary , then larsson , thomee is the best . do you have a special equation in mind ? that would help .
in a sense yes , if you are very careful about what you are holding constant . stating that a variable is proportional to another variable implies that all other relevant quantities are being held constant . for example , there is a simple relation $d=vt$ that describes the distance $d$ something travels in a time $t$ when traveling at speed $v$ . one might say that $d$ is proportional to $t$ . however , this relation is only valid if the speed $v$ is constant throughout the interval $t . $ in your example , one could say that mass is proportional to displacement if the $k$ and $a$ are constant , but you will need to do some work to figure out what physical system ( s ) meet such requirements .
what about positivity ? the product of bounded positive operators is positive if they commute ( see proof below ) , otherwise there is no guarantee . if your initial povms are not compatible , in general , the operators of the final candidate povm is not made of positive operators and thus they do not define a povm . proposition . if $a , b \geq 0$ where $a , b :\cal h \to \cal h$ are bounded with $\cal h$ hilbert space and $ab=ba$ then $ab \geq 0$ . proof . it is known that if $a\geq 0$ is bounded , then there is a unique positive bounded operator , $\sqrt{a}$ , such that $\sqrt{a}^2 =a$ . moreover that operator commutes with all bounded operators commuting with $a$ . in the present case $ab= \sqrt{a}\sqrt{a}b$ and , since $a$ and $b$ commute , $ab= \sqrt{a}\sqrt{a}b= \sqrt{a}b \sqrt{a}$ . finally , using the fact that a bounded positive opertor is self-adjoint , $$\langle x , ab x \rangle= \langle x \sqrt{a}b \sqrt{a} x \rangle = \langle \sqrt{a} x , b \sqrt{a} x \rangle = \langle y , b y \rangle \geq 0$$ because $b\geq 0$ . since $x\in \cal h$ is arbitrary , it implies that $ab \geq 0$ . qed
the accelerations of k and l will be different from one another and also from the particle a . to solve this consider $a_p$ to be the acceleration of the pulley j and $a_r$ be the relative accelerations of the particles k and l relative to the frame of reference of the pulley j . then the net acceleration of the particle k will be $a_p-a_r$ and that of particle l will be $a_p+a_l$ . now apply the equations for the particles k and l and the acceleration term wont cancel out . ( you can interchange the accelerations of both the particles and the only change it will bring is the sign of the answer and that will tell you the actual direction of the particles k and l ) for k $$s-12g=12 ( a_p-a_r ) $$ and for l $$s-9g=9 ( a_p+a_r ) $$ . solve this and you will get your answer .
a quantum of em radiation has energy $e=hc/\lambda$ . for a number state , the energy is $e=nhc/\lambda$ . to find the power this corresponds to , imagine that all of those excitations are being generated in a laser in a time $\tau$ . the rate of energy production ( the power ) is then $p=nhc/\lambda\tau$ . the average occupation number of a coherent state is $\left&lt ; n\right&gt ; = |\alpha|^2$ . this gives us $$p=\frac{|\alpha|^2hc}{\lambda\tau}$$ now take as an example a laser pointer operating at 534 nm , having a power of 1 mw . take $\tau$ = 1 s in order to get a proper s.i. unit ( joules per second ) . if i set my calculator on that i get $\alpha\approx 5\times10^7$ . ( check my work on that . )
it is true quite generally ( for seperable hilbert spaces at least ) . the original proof is in " proof of the strong subadditivity of quantum mechanichal entropy " , by lieb and ruskai ( j . math . phys . 14 , 1938–1941 ( 1973 ) ) . the main idea is to prove the finite-dimensional case , and then extend it by taking a limit of the inequality on finite-dimensional subspaces ( which is why seperability is needed ) .
no . of course , to argue if a definition applies , we must first agree on a definition . wikipedia gives this one : a crystal or crystalline solid is a solid material whose constituent atoms , molecules , or ions are arranged in an orderly repeating pattern extending in all three spatial dimensions . humans are certainly solid-ish , and our constituent molecules are arranged in a somewhat orderly pattern in all three dimensions . however , i think we fail the ' repeating ' portion of this definition . if you want to use a more broad definition of a crystal , link to it . finally , supposing the answer was ' yes ' . what are the practical predictions which follow from this assertion ? we certainly do not diffract x-rays into a regular grid , for example .
a gravitational wave will distort space-time and the light that is on a path affected by such a wave will be similarly affected , but it will still take longer ( or shorter ) to travel that path . imagine a car travelling along the surface of a trampoline , a wave on the trampoline could cause its path to become longer , but it will not be impossible to detect whether that path was longer . the key is to monitor path lengths in two orthogonal directions and compare them continuously , a gravitational wave will make one shorter and the other longer for long enough to be detectable by a laser . the wavelengths will not necessarily be affected , but the time of flight and thus the returning phase of the waves will be , and that can be measured very precisely using interferometric techniques similar to a michelson-morley device . as far as the energy to distort matter , that is irrelevant , it is space-time that is being distorted by gravitational waves , the energy calculations for compressing solid matter do not apply . pick up a single coin , that coin is distorting all of space-time in , effectively , the entire universe ! now move the coin around , now it is sending out gravitational waves which affect space-time in the entire universe as well . even , eventually , distant neutron stars or entire galaxies . granted , for a single coin the perturbations are so tiny as to be utterly indetectable and inconsequential , but they still happen . phenomena on a much larger scale such as closely orbiting neutron stars will send out much , much stronger gravitational waves , which we might be able to detect with sufficiently precise instruments . however , they still compress and expand the space-time all around us , but this is not a process that requires a continuous input of energy anymore than it takes a continuous input of energy to keep an object in orbit of another .
unfortunately if you want to read haliday , resnick , walker , you will need to learn calculus . if you are motivated , it is entirely do-able on your own . have a look at khan academy 's tutorials . how long it takes to get to the level required for hrw will depend how much mathematics you know already . you can learn some physics without hardcore calculus though . start with popular physics and khan academy ( again ) . let the things you learn from these sources motivate you to learn more mathematics , which in turn will allow you to learn more physics . does your high-school have a physics cirriculum ? if not , you could look up what they learn in another school system - for example , a-level physics in the united kingdom , or the international baccelauriate . these courses will be designed to get you a good grounding in physics without needing to know lots of mathematics .
the mass of the black holes in galactic centres are typically of a few million solar masses , while the galaxies have masses that are hundreds , thousands or even tens of thousand times larger . so while the central supermassive black hole of a galaxy may have played a part in forming the galaxy and determining the central dynamics of the galaxy , the overall dynamics of the galaxy is dominated by the surrounding dark matter halo , not by the central black hole . the apparent motion of galaxies has two components : one which is due to the cosmological expansion of space , and one which is due to local gravitational interactions with other galaxies . neither , however , are correlated with the galaxies ' proper rotation or alignment .
why do textbooks never mention this ? because in order to travel at supersonic speeds , human beings must be enclosed in a rigid metal tube of some sort . also , these metal tubes they ride in at those speeds generally tend to be insulated against noise from the outside . as for trying to place some sort of microphone outside said metal tube , the propulsion system would risk drowning out any atmospherically transmitted noise ( i.e. . noise can be transmitted through the body of the structure ) . now , if you run the math , it still does not work quite like hearing it backwards ( see the comment by eudoxos ) . although you would encounter the soundwaves in " reverse " order , the shockwaves around you would disrupt anything around you as to make the notion of noise from them irrelevant .
i am going to assume that omer is specifically asking why the centre of mass is at the focus ( well , one of the foci ) of the orbits . omer , if this is not what you meant please ignore what follows because it is completely irrelevant ! if you have a body moving in a central field ( i.e. . the force is always pointing towards the centre ) , and the field is inversely proportional to the square of the distance from body to the centre , then the orbit is an ellipse with the centre at one of the foci . for now let 's just assume this and we can come back to prove it later . so if we can show that both of the bodies feel a central inverse square force , with the com at the centre , this guarantees the orbits will be ellipses with a focus at the com . given that the force is due to the two bodies attracting each other , and that both bodies are orbiting around , it may seem a bit odd that each body just feels a central inverse square force , but actually this is easy to show . the picture shows the two masses and the com . i have not shown the velocities because it does not matter what they are . for now let 's just consider $m_1$ and calculate the force on it . by newton 's law this is simply : $$ f_1 = \frac{gm_1m_2}{ ( r_1 + r_2 ) ^2}$$ first is this force central ? we know the centre of mass does not move . for two bodies this seems obvious to me , but in any case dmckee proved it in his answer . if the com does not move it must lie on the line joining the two mases , otherwise there would be a net force on it . so the force $f_1$ must always point towards the com i.e. the force is central . second is this an inverse square law force i.e. is $f_1 \propto 1/r_1^2$ ? well the definition of the centre of mass is that : $$m_1r_1 = m_2r_2 $$ or $$ r_2 = r_1 \frac{m_1}{m_2} $$ if we substitute for $r_2$ in the expression for $f_1$ we get : $$ f_1 = \frac{gm_1m_2}{ ( r_1 + r_1 ( m_1/m_2 ) ) ^2}$$ or with a quick rearrangement : $$ f_1 = \frac{1}{ ( 1 + m_1/m_2 ) ^2} \frac{gm_1m_2}{r_1^2}$$ and this shows that $f_1$ is inversely proportional to $r_1^2$ . i will not work through it , but it should be fairly obvious that exactly the same reasoning applies to $f_2$ so : $$ f_2 = \frac{1}{ ( 1 + m_2/m_1 ) ^2} \frac{gm_1m_2}{r_2^2}$$ this is the key result . even though the two bodies are whizzing around each other , each body just behaves as if it were in a static gravity field , but the strength of the field is reduced by a factor of $ ( 1 + m_1/m_2 ) ^2$ for $m_1$ or $ ( 1 + m_2/m_1 ) ^2$ for $m_2$ . this applies to all two body systems , even such unequal ones as the sun and the earth ( ignoring perturbations from jupiter etc ) . i did start by assuming that a body in a central gravity field orbits in an ellipse with the foci at the centre , but i am going to wimp out of proving this since it would double the length of this answer and you had all go to sleep . the proof is easily googled . nb this only applies to two body systems . for three or more body systems the orbits are generally not ellipses with the centre of mass at the focus .
photoelastic constant i have typically seen this as part of the optical property list of glass or other optical materials . it predicts the birefringence . it is also called the stress-optic constant ( defined in mueller , the theory of photoelasticity , 1938 ) : $$ b=\frac{n_p - n_n}{p} $$ where $p$ is the pressure , $n_n$ is the index of refraction normal to the direction of pressure ( with a ${\bf{k}}$-vector normal to the pressure direction ) , and $n_p$ is the index of refraction parallel to the pressure direction . this equation is for a non-crystalline material , you get two constants and two equations for a uniaxial crystal for example . photoelastic coefficient i found a few papers where this was used to mean photoelastic constant . i also found a definition for photoelastic coefficient in properties of group-iv , iii-v and ii-vi semiconductors by adachi : $$ \alpha_{pe} = \frac{\delta\epsilon_{ij}}{x} $$ for a uniaxial crystal where "$\delta\epsilon_{ij}$ is the change in the dielectric constant parallel and perpendicular to the direction of stress $x$ . " in this definition they are using the dielectric constant ( aka the permittivity ) instead of the index of refraction . this means that the units will be different between the two definitions , but they are basically measuring the same thing in the optical regime ( where $\mu \approx 1$ , $\mu$ is the permeability ) . a lot of materials ( other than glasses ) papers seem to use this definition . acousto-optic coefficient the only reference that i could find to the above term was in a conference paper for cleo/pacific rim 2001 : sound field measurement through the acousto-optic effect of air by using laser doppler velocimeter by nakamura . this paper is so terse that i cannot figure out what he is actually calling the acousto-optic coefficient . i have never heard this term before . i found another definition from a thesis from moscow state university here : morozov thesis $$ \gamma = \frac{\partial n}{\partial p} $$ where $n = n ( p ) $ is the pressure dependent index of refraction and $p$ is the pressure , which is the differential case of the stress-optic coefficient .
is it because the acceleration is too weak ? it is too weak with respect to the four forces we measure . the fact that the four known forces are so much stronger means that agglomerates of particles , up to the scale of galaxies are not internally affected , they keep their structure intact , like the famous raisins in the rising bread . it is only at the level of clusters of galaxies that the expansion and the acceleration can be observed . and if there is a small apparent force , what direction is it in ? a cluster of galaxies sees expansion in all three space dimensions . the balloon surface analogy , blowing up the balloon with a gnat on it , might give an insight . the gnat sees the surface expanding away from it in both surface directions .
if you have a projective representation of a lie group $g$ , you can directly construct a central extension in the following way : try to make the projective representation into an honest representation , and keep track of how it fails . more specifically , a projective representation $\rho$ attaches to each element $g$ a whole line 's worth of linear transformations , i.e. , a collection of matrices of the form $\lambda a$ for some matrix $a$ , and $\lambda$ complex . to make this into an honest representation , you choose for each $g$ a nonzero linear transformation $\hat{\rho} ( g ) $ from the line $\rho ( g ) $ . the reason this might fail is that $\hat{\rho} ( g ) \hat{\rho} ( h ) $ may not equal $\hat{\rho} ( gh ) $ . because $\rho$ is a projective representation , we know these two matrices lie on the same line , hence are constant multiples of each other , but our arbitrary choice of matrix from this line may make this constant something other than one . the collection of these nonzero constants yields a function $\phi$ from $g \times g$ to $\mathbb{c}^\times$ , known as a 2-cochain . the 2-cocycle condition is given by $\frac{\phi ( h , k ) }{\phi ( gh , k ) } \frac{\phi ( g , hk ) }{\phi ( g , h ) } = 1$ for all $g , h , k \in g$ , and you can check that it is satisfied here by examining lifts $\hat{\rho}$ of triple products . if you have a 2-cocycle $\phi$ on $g$ with coefficients in $\mathbb{c}^\times$ , you can make a central extension in the following way : take the set $g \times \mathbb{c}^\times$ of pairs $ ( g , x ) $ , and twist the multiplication rule with $\phi$: $$ ( g , x ) ( h , y ) = ( gh , xy \cdot \phi ( g , h ) ) $$ checking that this yields an actual group ( with associative multiplcation ) amounts to checking that the function $\phi$ satisfies the cocycle condition . the second part is checking that any choice of matrices on the respective lines yields the same central extension . this arises from a notion of equivalence of cocycles known as cohomology . the equivalence classes of these cocycles form a group called the degree 2 cohomology group of $g$ with coefficients in $\mathbb{c}^\times$ , and elements of this group classify central extensions .
when discussing an ideal parallel-plate capacitor , $\sigma$ usually denotes the area charge density of the plate as a whole - that is , the total charge on the plate divided by the area of the plate . there is not one $\sigma$ for the inside surface and a separate $\sigma$ for the outside surface . or rather , there is , but the $\sigma$ used in textbooks takes into account all the charge on both these surfaces , so it is the sum of the two charge densities . $$\sigma = \frac{q}{a} = \sigma_\text{inside} + \sigma_\text{outside}$$ with this definition , the equation we get from gauss 's law is $$e_\text{inside} + e_\text{outside} = \frac{\sigma}{\epsilon_0}$$ where " inside " and " outside " designate the regions on opposite sides of the plate . for an isolated plate , $e_\text{inside} = e_\text{outside}$ and thus the electric field is everywhere $\frac{\sigma}{2\epsilon_0}$ . now , if another , oppositely charge plate is brought nearby to form a parallel plate capacitor , the electric field in the outside region ( a in the images below ) will fall to essentially zero , and that means $$e_\text{inside} = \frac{\sigma}{\epsilon_0}$$ there are two ways to explain this : the simple explanation is that in the outside region , the electric fields from the two plates cancel out . this explanation , which is often presented in introductory textbooks , assumes that the internal structure of the plates can be ignored ( i.e. . infinitely thin plates ) and exploits the principle of superposition . the more realistic explanation is that essentially all of the charge on each plate migrates to the inside surface . this charge , of area density $\sigma$ , is producing an electric field in only one direction , which will accordingly have strength $\frac{\sigma}{\epsilon_0}$ . but when using this explanation , you do not also superpose the electric field produced by charge on the inside surface of the other plate . those other charges are the terminators for the same electric field lines produced by the charges on this plate ; they are not producing a separate contribution to the electric field of their own . either way , it is not true that $\lim_{d\to 0} e = \frac{2\sigma}{\epsilon_0}$ .
euler 's rotational theorem only states , that we can determine a unique axis of the rotation for any given moment . that does not necessarily mean that the axis of rotation 's direction is fixed forever . quite opposite , let 's suppose that we have body with no external force acting upon it . if axis of rotation at some ( starting ) moment does not coincide with one of the three principal axis of moment of inertia , the axis of rotation shall change and you shall have complex rotation of that body ( precession ) . you can see that directly from euler 's equations by putting torque to be zero .
the word comes from the ancient greek meaning " pertaining to man ; " ' man ' here means human . the etymologyonline dictionary is helpful . the anthropic principle is so-named because it is fundamentally based on the fact of a human observer .
the velocity of the orbiting space junk is a vector , with both a radial and a tangential component . $$\vec{v}_f = \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}$$ ( my $r_f$ is your $r$ ) the equation for conservation of angular momentum involves only the tangential component of velocity , because it comes from the cross product of the radius vector and the velocity . $$\vec{l}_f = m\vec{r}_f\times\vec{v}_f = mr_f\hat{r}\times\bigl ( \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}\bigr ) = mr_f ( r_f\dot{\theta}_f ) \hat{z} = mr_fv_f\hat{z}$$ but the equation for energy conservation involves both components . $$e_f = \frac{1}{2}mv_f^2 - \frac{gmm}{r_f} = \frac{1}{2}m\dot{r}_f^2 + \frac{1}{2}mr_f^2\dot{\theta}_f^2 - \frac{gmm}{r_f}$$ the combination $r_f\dot{\theta}_f$ corresponds to your $v_f$ , but the equation as you have written it in the question is missing the $\frac{1}{2}m\dot{r}_f^2$ term , which corresponds to the energy of motion toward or away from the moon . at apapsis and periapsis ( furthest and closest points of the orbit ) , $\dot{r}_f = 0$ momentarily , so you can ignore this term , as is done in the problem you are asking about . but for other points on the orbit , that term is nonzero , which means you have an extra variable . that prevents you from finding a unique solution without specifying which point in the orbit you are at .
having given it some more thought , there is an unambiguous philosophical difference , with practical implications . the two-slit experiment provides a good example of this . in a classical universe , any particular photon that hits the screen either went through slit a or slit b . even if we did not bother to measure this , one or the other still happened , and we can meaningfully define p ( a ) and p ( b ) . in a quantum universe , if we did not bother to measure which slit a photon went through , then it is not true that it went through one slit or the other . you might say it went through both , though even that is not entirely true ; all we can really say is that it " went though the slits " . ( asking which slit a photon went through in the two-slit experiment is like asking what the photon 's religion is . it simply is not a meaningful question . ) that means that p ( a ) and p ( b ) just do not exist . here 's where one of the practical implications comes in : if you do not understand qm properly [ i am lying a bit here ; i will come back to it ] then you can still calculate a probability that the particle went through slit a and a probability that it went through slit b . and then when you try to apply the usual mathematics to those probabilities , it does not work , and then you start saying that quantum probability does not follow the same rules as classical probability . ( actually what you are really doing is calculating what the probabilities for those events would have been if you had chosen to measure them . since you did not , they are meaningless , and the mathematics does not apply . ) so : the philosophical difference is that when studying quantum systems , unlike classical systems , the probability that something would have happened if you had measured it is not in general meaningful unless you actually did ; the practical implication is that you have to keep track of what you did or did not measure in order to avoid doing an invalid calculation . ( in classical systems most syntactically valid questions are meaningful ; it took me some time to come up with the counter-example given above . in quantum mechanics most questions are not meaningful and you have to know what you are doing to find the ones that are . ) note that keeping track of whether you have measured something or not is not an abstract exercise restricted to cases where you are trying to apply probability theory . it has a direct and concrete impact on the experiment : in the case of the two-slit experiment , if you measure which slit each photon went through , the interference pattern disappears . ( trickier still : if you measure which slit each photon went through , and then properly erase the results of that measurement before looking at the film , the interference pattern comes back again . ) ps : it may be unfair to say that calculating a " would-have " probability means that you do not understand qm properly . it may simply mean that you are consciously choosing to use a different interpretation of it , and prefer to modify or generalize your conception of probability as necessary . v . moretti 's answer goes into some detail about how you might go about doing this . however , while this sort of thing is interesting , it does not appear to me to be of any obvious use . ( it is not clear that it gives any insight into the disappearance and reappearance of the interference pattern as described above , for example . ) addendum : that has become clearer following the discussion in the comments . it seems that it is thought that the alternative formulation may have advantages when dealing with more complicated scenarios ( qft on curved spacetime was mentioned as one example ) . that is entirely plausible , and i certainly do not mean to imply that the work lacks value ; however , it is still not clear to me that it is pedagogically useful as an alternative to the conventional approach when learning basic qm . pps : depending on interpretation , there may be other philosophical differences related to the nature or origin of randomness . bayesian statistics is broad enough , i believe , that these differences are not of any great importance , and even from a frequentist viewpoint i do not think they have any practical implications .
kugo and ojima 's work was one of the major breakthroughs in understanding the role of brst in the quantization of gauge theories . historically brst was discovered in the path integral formalism . the understanding of this theory as a cohomology theory started from the kugo and ojima 's work . now , the action is brst invariant with and without the gaussian integration over the auxiliary field $b^a$ ( called the lautrup-nakanishi multipliers ) . they are introduced in order to not have explicit dependence on the gauge parameter in the ward identities ( please see a recent review by becchi ) . the brst invariance ward identities is a crucial step in the unitarity proof . kugo and ojima actually solved the brst cohomology problem of the yang-mills theory . they actually identified the physical and unphysical states of the theory ( in terms of the brst operator ) as follows : the physical states correspond to the states annihilated by the brst operator and in addition of positive norm . the unphysical states are arranged in degenerate quartets . this is called the kugo - ojima quartet mechanism . one quartet corresponds to a ghost , anti-ghost , longitudinal and temporal gluons . in their formalism these states can be generated from the vacuum by the action of the ghost antighost operators as well as by the field $b^a$ and its conjugate momentum . they also conjectured that since colored quark operators and transversal gluon operators belong to the quartet sector , then these states must be confined .
in the $s'$ frame , your variables are $x ' = x - t\cdot u \cos\theta $ and $y ' = y - t\cdot u \sin\theta$ . if you do the change of variable , you get that the motion now is described by $$x ' = 0$$ $$y ' = -\frac{g}{2}t^2$$ so in your new frame of reference you have vertical free fall from rest . this is not very helpful in finding out when or where does the projectile hits the ground , but is very relevant if you want to know where will the projectile be after releasing it from a plane moving at constant velocity : right below it all the time . disregarding air resistance , of course . edit the system with a prime is moving with velocity $ ( u \cos\theta , u\sin\theta ) $ , so if you have a velocity in the unprimed system , to convert it to the primed system , you have to substract the velocity of the origin : $$\vec{v'} = \vec{v} - ( u \cos\theta , u\sin\theta ) $$ integrating this , you can get the relation for the position vector : $$\vec{r'} = \vec{r} - ( u \cos\theta , u\sin\theta ) t + \vec{r}_0$$ where $\vec{r}_0$ is the position of the origin of the primed system for $t=0$ . both systems share origin for $t=0$ , so $\vec{r}_0=\vec{0}$ . now replace $\vec{r'}= ( x ' , y' ) $ and $\vec{r}= ( x , y ) $ and you will get the equations above .
there are many common misconceptions of the universe . here 's a brief list : many people think that the big bang was a physical explosion . in fact , the big bang is basically a theory of the beginning of the universe , starting with a creation of matter , energy , and spacetime and spacetime 's expansion . that is it . nothing more . it is commonly believed that stars burn to create energy . burning is combustion , a chemical process that has nothing to do with stars . stars ' crush ' together several lighter nuclei to form new heavier nuclei ( e . g . crushing a deuterium nucleus and a tritium nucleus to form a helium nucleus and a neutron , releasing energy ) , and in the process releases energy , fueling stars , their luminosity , and preventing them from collapsing due to gravitation black holes are popularly thought to be fundamentally different from other gravitational sources . many believe that a black hole , even if it had the same mass as another object , would somehow attract them more strongly and ' eating ' all matter around it . many believe astronomical observations reflect the current . in fact , according to relativity there cannot even exist an objective definition of simultaneity . light takes time to travel from astronomical objects to earth , so by watching the skies we are practically observing the past . most people think that all matter in the universe consists of atoms . in fact , even if you count in plasma ( which does not really consist of atoms ) , they only account for about 4% of the ' mass ' of the universe . the rest are dark matter and dark energy . i think that is about as much as i can think of for now .
you are right , it is wrong to think that in gauge theory " gauge transformations are just a redundancy " . this becomes true only if one abandons locality , ignores all boundary effects , all instanton effects , hence most of what is interesting about gauge theory . of course forming gauge equivalence classes ( say of observables ) is something one wants to do every now and then , but considering only gauge equivalence classes means to kill gauge theory . examples : 1 ) instantons : every gauge bundle on a n-disk is equivalent to the trivial one . yet there are non-trivial gauge bundles on the n-sphere -- the instanton sectors . if you think that only gauge equivalence classes count , then there is only the trivial gauge bundle on one hemisphere , the trivial gauge bundle on the other hemisphere , and you have to glue them trivially on the equator to get a global trivial gauge bundle . instead , what really happens is that gauge transformations are not a redundancy , but are all that makes up the non-triviality of the instanton sector , by the clutching construction . ignoring this means to have non-trivial global structures that are not obtained by gluing local structures , hence means to break the locality principle . 2 ) boundary fields . the way that the wzw model appears on the boundary of chern-simons theory : the gauge transformations of the chern-simons theory on the boundary become the very fields of the wzw model . 3 ) higher codimension defects : wilson loops . similarly , the fields on the wilson loop in chern-simons theory are entirely the gauge transformations of the ambient gauge field , restricted to the loop . see here for review and pointers to the literature . 4 ) generally : locality in gauge theory -- it breaks if one disregards gauge transformaitons , see the pointers here . [ edit : a commenter below points out that this is all fine , but does not seem to address specifically the construction inquied about by the op . in fact it does , here is how : 5 ) gauge fields from local gauging : the traditional physics textbook way of deriving gauge fields from local gauge symmetry is an example of the local relevance of gauge transformations as follows . every fermion bundle is locally gauge equivalent to the trivial such , and so carries the trivial connection , given just by the derivative . but remembering that gauge transformations are a local reality , one observes that these take this trivial connection to one with a non-vanishing vector potential $a$ . while this will still have vanishing field strength , already here something may happen globally : if a bunch of these $a$ are glued by gauge tansformations , we may still have globally a non-trivial instanton sector . given this then we are led to allow general local vector potentials $a$ and find then that by gluing them across patches by gauge transformations , we find the full moduli space of all possible gauge fields . if however , and that is the correct point the op observes , one declares that all local gauge transformations are just redundancies , then that means to replace each local vector potential $a$ by its gauge equivalence class . gluing these across patches never produces all global gauge field configurations ( for instance if the gauge equivalence class was the 0-class , one never finds the global torsion class instanton sectors ) . ] mathematically what is going on here is the statement that gauge fields do not form a moduli space but a " moduli stack " . the mathematical concept of stack is all about what it means to combine locality with the gauge principle . an exposition of this is in our arxiv:1301.2580 . for instance what govers examples 2 ) and 3 ) above , where gauge transformations in higher dimension become genuine fields in lower dimension is essentially an instance of the looping construction on stacks : the moduli stack $\mathbf{b}g$ of $g$-instanton sectors has a single component $$ \pi_0 ( \mathbf{b}g ) \simeq \ast $$ ( hence " there is only one gauge equivalence class" ) but it nevertheless remembers the full nature of gauge transformations $$ \pi_1 ( \mathbf{b}g ) \simeq \pi_o ( \omega \mathbf{b}g ) \simeq g \ , . $$ thinking that " gauge equivalence is a redundancy " means thinking that the moduli stack $\mathbf{b}g$ may just as well be replaced with its 0-truncation $\tau_0 \mathbf{b}g\simeq \ast$ , which means thinking that local $g$-gauge theory is trivial . the same kind of arguments apply to the full moduli stack $\mathbf{b}g_{conn}$ of $g$-gauge fields ( instead of just their instanton sectors ) . then $\pi_0 ( \mathbf{b}g_{conn} ) $ is the sheaf of gauge equivalence classes of $\mathfrak{g}$-valued differential 1-forms . that is more than just the point as before , but still just a faint shadow of what gauge theory is about .
if you want , you can go and use the ansatz : $$ds^{2} = -a ( r ) dt^{2} + b ( r ) dr^{2} + 2c ( r ) \ , dt\ , dr + f ( r ) \left ( d\theta^{2} + \sin^{2}\theta d\phi^{2}\right ) $$ where the functions only depend on $r$ due to the fact that $t$ generates a symmetry of the spacetime -- you are assuming a static spacetime . note , however , that you are free to arbitrarily rescale $r$ . well , if you choose $r = \sqrt{f ( r ) }$ , then this is rewritten in the form $$ds^{2} = -a' ( r ) dt^{2} + {\hat b} ( r ) dr^{2} + 2c' ( r ) \ , dt\ , dr + r^{2}\left ( d\theta^{2} + \sin^{2}\theta d\phi^{2}\right ) $$ where the prime denotes that you have to feed $f^{-1} ( r^{2} ) $ into the function , and ${\hat b} = b' ( r ) \left ( \frac{dr}{dr}\right ) ^{2}$ . to get rid of the $c$ term , you can make the definition : $t = t + \int\ , dr\ , c'$ which gives $$ds^{2} = - ( a' ( r ) -c' ( r ) ) dt^{2} + dr^{2}\left ( {\hat b ( r ) }+ ( 1-a ( r ) ) ( c' ( r ) ) ^{2}\right ) + r^{2}\left ( d\theta^{2} + \sin^{2}\theta d\phi^{2}\right ) $$ so , we can just redefine our factors , and we get the " standard " starting form of $$ds^{2} = - \alpha ( r ) dt^{2} + \beta ( r ) dr^{2} + r^{2}\left ( d\theta^{2} + \sin^{2}\theta d\phi^{2}\right ) $$ without losing any generality .
which units are fundamental and which are derived is pretty much a matter of arbitrary convention , not an objective fact about the world . you might think that the number of fundamental units would be well-defined , but even that is not true . take electric charge for example . in the si system of units ( i.e. . , the " standard " metric system ) , charge cannot be expressed in terms of mass , length , and time : you need another independent unit . ( in the si , that unit happens to be the ampere ; the unit of charge is defined to be an ampere-second . ) but sometimes people use different systems of units in which charge can be expressed in terms of mass , length , and time . by decreeing that the proportionality constant in coulomb 's law be equal to 1 , $$ f={q_1q_2\over r^2} , $$ you can define a unit of charge to be ( if i have done the algebra right ) $ ( ml^3/t^2 ) ^{1/2}$ , where $m , l , t$ are your units of mass , length , time . whether charge is defined in terms of mass , length , time , or whether it is an independent unit , is a matter of convenience , not a fact about the world . people can and do make different choices about it . similarly , some people choose to get by with fewer independent units than the three you mention . the most common choice is to decree that length and time have the same units , using the speed of light as a conversion factor . you can even go all the way down to zero independent units , by working in what are often called planck units . in summary , you can dial up or down the number of " independent " units in your system at will . one more example , which seems silly at first but is actually of some historical interest . you can imagine using different , independent units of measure for horizontal and vertical distances . that'd be terribly inconvenient for doing physics , but for many applications it is actually quite convenient . ( in aviation , altitudes are often measured in feet , while horizontal distances are measured in miles . in seafaring , leagues are horizontal and fathoms are vertical . yards are pretty much always used for horizontal distance . ) it sounds absurd to think of using different units for different directions , but in the context of special relativity , using different units for space and time ( different directions in spacetime ) is sort of similar . if we had evolved in a world in which we were constantly zipping around near light speed , so that special relativity was intuitive to us , we had probably think that it was obvious that distance and time " really " came in the same units .
656 beagle is a small ( ~53 km diameter ) asteroid with a 3.15 au semimajor axis . it is luminosity is very faint , with h = 9.92 . nothing seems to be known about its geologic basis , but it certainly does not possess a measurable atmosphere . see http://occsec.wellington.net.nz/planet/2008/updates/080912_656_19331_u.htm for more information .
the expectation value of the energy stays the same after the doubling of size but it does not mean that the spectrum is the same . for a normalized $\psi$ , the expectation value of the energy is simply $$ \int_{-l}^{+l}dx\ , \psi^* \left ( -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} + v ( x ) \right ) \psi $$ because the integral may be reduced to the interval as the wave function vanishes outside the interval . now , immediately when you double the size of the well , the value of $\psi ( x ) $ remains the same so it still vanishes outside the interval $ ( -l , l ) $ and makes the integrand vanish as well ( even though the second derivative could refuse to vanish ) . that is why the integral above may still be rewritten as $$ \int_{-2l}^{+2l}dx\ , \psi^* \left ( -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} + v ( x ) \right ) \psi $$ without any change . it is the expectation value of the new hamiltonian . note that $v ( x ) =0$ wherever $\psi ( x ) \neq 0$ so the potential term may be omitted . you are right that there is some probability that in the larger well , the particle sits at a lower-than-the-initial-energy-eigenvalue value of energy . however , there is some probability that the energy is raised as well – the wave packet is unnecessarily squeezed in a small part of the well which adds more kinetic energy than the minimum possible one . these positive and negative changes cancel in the expectation value of the energy : the calculation above showed that it stayed constant . the expectation value of the energy stays constant when the particle evolves according to the larger-well hamiltonian , too . the probabilities for each energy eigenvalue are constant for all $t&lt ; 0$ and then for $t&gt ; 0$ but there is a discontinuity at $t=0$ . however , as the simple calculation above shows , in the expectation value of the energy itself , the change of the spectrum etc . at $t=0$ cancels when it comes to the expectation value of the energy .
assuming non-relativistic velocities , the power radiated by a charge accelerating at constant acceleration $a$ is given by the larmor formula : $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c^3} $$ to do the calculation properly is surprisingly complicated , but it is easy show that the effect of the radiation on the electrons fall is negligible . if the electron falls a distance $h$ then the time it takes is given by : $$ h = \frac{1}{2}gt^2 $$ so : $$ t = \sqrt{\frac{2h}{g}} $$ if we assume the electron is accelerating at a constant rate of $g$ , the total energy radiated is just power times time or : $$ e_{rad} = \frac{e^2 g^2}{6\pi \epsilon_0 c^3} \sqrt{\frac{2h}{g}} $$ in your question $h$ is 1000m , so : $$ e_{rad} = 7.83 \times 10^{-51}j $$ the potential energy change is , as you say , just $mgh$: $$ e_{pot} = m_e g h = 8.94 \times 10^{-27} j $$ so the ratio of the radiated energy to the potential energy is about $10^{-24}$ , and therefore the effect of the radiation on the electron 's fall is entirely negligible . response to comment : the power radiated from the electron produces a force that opposes the acceleration due to gravity . assume we can ignore the deviations from accelerating at a constant rate $g$ , then in a small time $dt$ the energy radiated is $pdt$ . the energy is force times distance ( $dx$ ) so to get the force we divide by the distance : $$ f = p\frac{dt}{dx} = \frac{p}{v} = \frac{p}{\sqrt{2gh}} $$ using $v^2 = 2as$ . the acceleration produced by this force is just $f/m_e$ , so the net acceleration on the electron is : $$ a_{net} = g - \frac{p}{m_e \sqrt{2gh}} $$ so the electron does accelerate slightly more slowly than $g$ , but the difference between the acceleration and $g$ is inversely proportional to distance fallen so it gets increasingly negligible the further the electron falls . you have probably spotted that the above equation says the force should be infinite at the moment you release the particle . that is because as you approach the moment of release it is no longer safe to make the approximation that you can ignore the change in the acceleration due to radiation .
the broken gauge symmetry model for the w and z boson masses is correct with scientific standards of certainty , since it is exactly verified by relating the ratio of their masses to the ratio of the coupling constants , and there is no chance for such a precise agreement by accident . the pattern of breaking tells you that there is a charged condensate in the vacuum of the form of the usual higgs , a scalar su ( 2 ) doublet ( "spin 1/2" ) representation with u ( 1 ) charge 1/2 ( the same as the lepton doublet ) . but there is no real reason to think that this must be a fundamental scalar field with these charges . it is good to have as wide a range of models for the higgs sector as possible . it is not possible for this field to just what we have seen so far , just the transverse modes of the w and z , because these modes can not be unitary all by themselves . their dynamics is that of a particular nonlinear sigma model , the limit as the self-coupling $\lambda$ goes to infinity of the standard model higgs , and this limit is inconsistent ( see addendum ) . so you need something to unitarize in the absence of something like the higgs , a good dynamical field theory which reprouces the higgs condensate . so it is just inconceivable that the lhc will find nothing at all . once you get data on the missing component ( s ) of the higgs , it will reveal if the higgs condensate is a composite fermion condensate , as in technicolor , or a scalar condensate as in the standard model and susy variants , or something else entirely ( like a composite scalar made out of technicolor scalars or something even more exotic like an infraparticle of a higher energy banks-zaks theory or whatever ) . the goldstone theorem just does not work when there are gauge fields coupled to the symmetry , this is the whole point of the higgs mechanism . the long-range coulomb interaction coupled to a charged condensate produces no goldstone bosons . the argument is summarized on wikipedia in the page on the higgs mechanism , in the section on superconductivity . the mechanism is often called the " eating " of the goldstone bosons by the gauge bosons . it is no more mysterious than the statement that plasma waves have a finite frequency at infinite wavelength , because of the instantaneous coulomb repulsion ( which is still relativistically instantaneous in dirac gauge ) . addendum : why coupling blow ups to infinity are inconsistent like many field theory arguments , good guidance is provided by the ising model . the ising model is the $\lambda$ goes to infinity version of the scalar with field potential $\lambda ( \phi^2 - 1 ) ^2$ . in the large $\lambda$ limit , when you discretize the action on a lattice , you force the field to be $\pm 1$ . the coupling between neighboring field values reproduces an ising model action of some kind ( perhaps with next-to-nearest neighbor coupling , or whatever , it is the same universality class ) . when you look at this statistical theory at long distances , you reduce to a $\phi^4$ theory with a $\lambda$ which goes down with larger distances . if you back-trace the evolution of $\lambda$ , it gets stronger at shorter distances , and it blows up at the order of magnitude of the lattice scale ( this is completely obvious , because the lattice scale is exactly where $\lambda$ is infinite , so that the ising description is correct . averaging over blocks of spins allows the field to fluctuate away from plus or minus 1 , so it reduces the $\lambda$ . this is also formally well known from the $\beta$ function calculation in $\phi^4$ theory ) . so the bare ising model lattice scale is where the coupling blows up , and we know by construction that there are no shorter distances consistently defined in this model . the lesson learned is that any scalar theory must have some new physics at the scale of its landau pole ( the scale where the coupling blows up ) , otherwise , you will see the lattice , or whatever structure is hiding behind the long-wavelength quantum/statistical field theory . the nonlinear sigma model defined by the classical large $\lambda$ limit of the standard model can be defined at some microscopic scale , but then at long distances , it will flow to the standard model scalar higgs with a weak coupling . if the cutoff is much larger than the tev higgs scale , the coupling is bounded above by this triviality argument and the constraint that the location of the landau pole is higher than the cutoff scale . this gives weinberg 's higgs mass bound . the reason it is a mass bound and not a coupling bound is because the higgs particle mass is determined by the curvature of the higgs potential in the hard direction of the mexican hat ( the soft directions are the goldstone bosons that are eaten by the w an z ) , and the curvature in this hard direction is proportional to $\lambda$ . this is called the unitarity bound , or the triviality bound , depending on who is speaking , and it certainly excludes an infinite coupling at tev scales , as required for a nonlinear sigma model which would keep only the longitudinal models of the w and z , and discard the higgs boson by moving its mass to infinity . note that this argument does not work for abelian higgs mechanism , when the higgsing is of a ( noncompact ) u ( 1 ) gauge theory , because you can take the higgs charge to zero and the higgs self-coupling to infinity and the condensate value to infinity while keeping the mass of the u ( 1 ) photon finite , and the nonlinear sigma model limit ( just a circle ) is the stueckelberg affine higgs mechanism . this limit evades the triviality argument because the circle becomes big at the same time as the coupling becomes big . this does not work in the nonabelian case , because the charges are quantized with a lower bound . this addendum is provided because i did not feel comfortable just saying " weinberg says so . " but if it is too telegraphic , then , well , weinberg says so .
when a particle is deflected by gravity the gravitational field will also be modified by the particle . to form a conservation law for momentum you need to take into account the momentum in the gravitational field as well as the particle . this can be done e.g. using pseudo-tensor methods . this works but remember that momentum is a relative concept . even in newtonian dynamics it depends on the velocity of your reference frame . in general relativity it also depends on the frame but a much wider class of frames is valid . this means that momentum conservation depends on the choice of co-ordinates . locally you can pick an inertial reference frame but over extended regions there is no inertial frame . a momentum in one location cannot be simply added to a momentum vector in another location . nevertheless , momentum conservation laws over extended regions do work correctly in general relativity . for an extended description of the formalism and why it works see my article at http://vixra.org/abs/1305.0034 edit : in the comments below mwt p457 has been cited to support the idea that energy and momentum are only conserved in specific cases . i am adding this to directly refute what has been said there . mwt begin by saying that there is no such thing as energy or momentum for a closed universe because " to weigh something one needs a platform on which to stand to do the weighing " this is pure wheeler rhetoric of the type for which he is greatly admired , but in this case it is simply misleading . weight is a newtonian term with no useful counterpart in general relativity except in the specific case of an isolated system in an asymptotically flat spacetime . for other situations such as the closed cosmology energy and momentum conservation take a different but equally valid form . they go on to say that in a closed universe total energy or momentum or charge is " trivially zero " they justify that it is zero because you can use gauss 's divergence theorem to write the charge , energy or momenta as a boundary integral . for a closed universe the boundary disappears making the result zero . this is of course correct , but they give no justification for calling this answer trivial . energy and momenta are initially defined as a sum of volume integral contributions from each physical field including electromagnetic fields , fermionic fields , gravitational field etc . it is in this sense that we understand that conserved quantities can move and can transform from one form to another but the total remains constant . it is a property of gauge fields that when the dynamical field equations are used the conserved quantity is the divergence of a flux from just the gauge field so that it can be integrated over a volume and be calculated as a boundary surface integral . this gives charge/energy/momenta a holographic nature where they can be considered either as a volume integral over contributions from different fields of a surface integral over the gauge field flux . the important thing to understand is that to go from the volume to the surface form the field equations must be used . this means that the total charge/energy/momenta in a closed universe is zero but that this is not in any sense a trivial result . if you calculate total energy as a volume integral for a configuration of fields that do not satisfy the equations of motion the answer will not necessarily be zero . stating that it is zero is therefore making a non-trivial assertion about the dynamics . this is what conservation laws are all about . mwt go on to explain why it would make no sense to have an energy-momentum 4-vector globally . the invalid assumption they are making is that energy and momenta need to form a 4-vector . a 4-vector is a representation of the poincare group and is the natural form that energy-momentum takes in special relativity where poincare invariance is the global spacetime symmetry . in general relativity the global spacetime symmetry is diffeomorphism invariance so the correct expectation is that all quantities should take the form of a representation of the diffeomorphism group for the manifold . this is what happens . if you demand an energy-momentum 4-vector then of course you will only get an answer locally , and also for an asymptotically flat spacetime where poincare symmetry is valid at spatial infinity , but demanding such a 4-vector is simply the wrong thing to do in general relativity . in the fully general case of any spacetime we can apply noether 's theorem using invariance under diffeomorphism generated by any contravariant transport vector field ( observe that it is invariance of the equations that is required , not invariance of the solutions . some people like to confuse the two ) the result is a conserved current with a linear dependence on the transport vector field . this is the correct form for a representation of the diffeomorphism group . i refer to my cited paper for the mathematical details . this current gives conservation laws for energy , and momenta including generalizations of angular momenta as well as linear momenta depending on the transport vector field chosen . if it transports space-like hypersurfaces in a timelike direction it will give an energy conservation law and if it transports in spacelike directions it gives momenta conservation laws . these energy and momenta do not normally form 4-vectors but they can be integrated to give non-trivially conserved quantities . the global form a conservation law must take is that the total energy and momenta in a volume must change at a rate which is the negative of the flux of the quantity over the boundary , and this is what you get with the currents derived from noether 's theorem . it may be that other people will want to add comments here that dispute the validity of energy conservation in other ways . i refer once again to my new article at http://vixra.org/abs/1305.0034 where i refute all the objections that i have heard . triviality is dealt with in item ( 6 ) and 4-momentum is dealt with in item ( 8 ) . unless someone comes up with a novel objection i will just refer to the numbered objections in this paper in future . remember , there are no authorities in science and any expert may be shown to be wrong either by reasoning or by experiment .
i would view the situation slightly differently . the constant speed of light is a result of lorentz symmetry , and the key assumption is that the universe is lorentz symmetric . any mathematical model postulated to describe the universe must include lorentz symmetry , and therefore any such model will predict a constant speed of light . the point is that your argument seems to be : maxwell 's equations are laws of physics maxwell 's equations are lorentz invariant therefore physics must be lorentz invariant but i would say you have this the wrong way round . we start with the assumption that lorentz symmetry is fundamental , and if we assume this then maxwell 's equations are one possible mathematical model to describe the universe ( and of course they have been proven a correct description by experiment ) . so the physical arguments for the finite speed of light are every experiment that observes lorentz invariance e.g. measurements of time dilation .
it is a direct consequence of equilibrium . it can be proved mathematically that proving the moment is zero at one point of a sytem in equilibrium ensures it is zero everywhere . have a look at the first two pages of this , you can find its proof
so you know about how to get the effective moment of all the forces $$ \vec{m} = \sum_{i} \vec{r}_i \times \vec{f}_i $$ and the total forces $$ \vec{f} = \sum_i \vec{f}_i $$ to get the location where the moments balance out ( the line of action of the combined force ) you do the following $$ \vec{r} = -\frac{\vec{m} \times \vec{f}}{\vec{f} \cdot \vec{f}} $$ for example a force $\vec{f}= ( 1,0,0 ) $ located at $\vec{r}= ( 0 , y , z ) $ creates a torque of $\vec{m}= ( 0 , z , -y ) $ . to recover the location of the force do $$r = -\frac{ ( 0 , z , -y ) \times ( 1,0,0 ) }{ ( 1,0,0 ) \cdot ( 1,0,0 ) } =- \frac{ ( 0 , -y , -z ) }{1} = ( 0 , y , z ) $$
yes , it can be made precise and corresponds to the leading order of the semiclassical expansion ( wkb approximation ) in $\hbar$ . see faddeev-yakubovsky 's " lectures on quantum mechanics for mathematics students " ( §20 , formula ( 13 ) ) . an approach inspired by geometric quantization is explained in chapter 4 in bates-weinstein 's lectures on the geometry of quantization .
the expression $$ k_b \frac{\omega}{\bar{\omega}} $$ equals $$ k_b\frac{1}{\bar{\omega}}\frac{d\bar{\omega}}{de} $$ which equals $$ \frac{ds}{de} . $$ in thermodynamics , where $s$ is the clausius entropy , this is equal to $1/t$ where $t$ is the kelvin temperature . in statistical physics , this expression can be taken as a definition of $1/t$ of a system from the microcanonical ensemble $e , a$ .
there are several ways of knowing what states should be there . for simple cases such as this , the easiest way is just by counting all the possibilities or micro-states . since you have 2 " equivalent " electrons in $p^2$ ( equivalent meaning that they share quantum numbers $n$ and $l$ , related to the energy of the system ) there are $$\left ( \begin{array}{l} 6 \\ 2 \end{array} \right ) = 6 \cdot 5/2 = 15$$ micro-states ( possible ways of assigning $n$ , $l$ , $m_l$ and $m_s$ to the outer ( valence ) electrons . know you have to count all the possible ( allowed by pauli 's principle ) different arrangements of $m_{l , 1}$ , $m_{l , 2}$ , $m_{s , 1}$ , $m_{s , 2}$ . you can find them explicitly in any physical chemistry book ( e . g . mcquarrie and simon ) . writing $m_{l , 1} , m_{l , 2}$ and $m_s$ omitted ( $m_s = 1/2$ ) or with an over bar ( for spin $m_s = -1/2$ ) , the microstates are : $ ( 1 , \bar{1} ) , ( 1,0 ) , ( 1 , \bar{0} ) , ( 1 , -1 ) , ( 1 , -\bar{1} ) $ , $ ( 0,1 ) , ( 0 , \bar{1} ) , ( 0 , \bar{0} ) , ( 0 , -1 ) , ( 0 , -\bar{1} ) $ , $ ( -1,1 ) , ( -1 , \bar{1} ) , ( -1,0 ) , ( -1 , \bar{0} ) , ( -1 , -\bar{1} ) $ now you have to group these states by characterising $l$ and $s$ ( since , barring spin-orbit coupling , for a given $l$ and $s$ the $m_l$ and $m_s$ states form a manifold of degenerate states ) . there are several ways of doing so , but i will be sketchy to avoid lengthiness . for instance you can first think in those cases with $m_{s , 1} \neq m_{s , 2}$ where $m_{l , 1}$ can be equal to $m_{l , 2}$ . then $m_s = m_{s , 1} + m_{s , 2} = 0$ ( $s$ can still be 1 -triplet or 0 - singlet ) . $m_l = m_{l , 1} + m_{l , 2} = 2,1,0$ . so you can have states with $l = 2,1,0$ . the states with $l = 2$ must be $s = 0$ since , as you realised , $m_{s , 1}$ cannot be equal to $m_{s , 2}$ . thus you identify 5 micro-states ( $m_l = +l , \ldots , 0 , \ldots -l$ ) corresponding to a single level $^1$d . of the remaining 10 states you can clearly see from the listing of micro-states that your $l=1$ level is a triplet ( you can find microstates with $m_s = 1$ and $m_l = 1$ so the remaining $m_s = 0 , -1$ and $m_s = 0 , -1$ have to be there too ) . for a $^3$p level there are $3\times 3 = 9$ microstates . finally , the remaining micro-state must correspond to a $^1$s state . regarding your question of the wave function . again , think only on your last 2 electrons ( it is not difficult to " enlarge " your slater determinant with the other electrons ) . each of your previous micro-states would correspond to a single slater determinant . for example , for $ ( 1 , \bar{0} ) $ you would have the wave function $$ \frac{1}{\sqrt{2}} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right|$$ where $2p_{m}$ is the wave function in atomic orbital $2p_{m}$ and $\alpha$ and $\beta$ are the spin functions . some states belonging to the levels with well-defined $l$ and $s$ correspond directly to the micro-states . for instance $ ( 1 , \bar{1} ) $ is exactly $|l=2 , s=0 , m_l=2 , m_s=0\rangle$ . however , most states must be written as linear combinations of slater determinants . for instance , to the state $|l=2 , m_l=1 , s=0 , m_s=0\rangle$ corresponds the wave function $$\frac{1}{2} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{2} \left| \begin{array}{cc} 2p_0 ( 1 ) \alpha ( 1 ) and 2p_1 ( 1 ) \beta ( 1 ) \\ 2p_0 ( 2 ) \alpha ( 2 ) and 2p_1 ( 2 ) \beta ( 2 ) \end{array} \right|$$ whereas for the state $|l=2 , m_l=0 , s=0 , m_s=0\rangle$ the wave function would be $$\frac{1}{2\sqrt{3}} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_{-1} ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_{-1} ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{\sqrt{3}} \left| \begin{array}{cc} 2p_0 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_0 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{2\sqrt{3}} \left| \begin{array}{cc} 2p_{-1} ( 1 ) \alpha ( 1 ) and 2p_{1} ( 1 ) \beta ( 1 ) \\ 2p_{-1} ( 2 ) \alpha ( 2 ) and 2p_{1} ( 2 ) \beta ( 2 ) \end{array} \right|$$
using this from my book ( physics for scientists and engineers with modern physics 9th ed ) : if more than one force acts on a system and the system can be modeled as a particle , the total work done on the system is just the work done by the net force . if we express the net force in the x direction as o fx , the total work , or net work , done as the particle moves from xi to xf is σw = the integral of σf from xi to xf a proof that d ) the speed of the particle must be unchanged is true : if σw = the integral of σf from xi to xf = 0 , then δx = 0 or σf = 0 ( this alone is proof that e ) there must be no displacement is false ) . in the case that δx = 0 , the particle is not displaced and the speed is unchanged . if σf = ma0+ma1+ . . . + man = m ( σa ) = 0 and m ≠ 0 , then σa = 0 and the speed is again unchanged . therefore , if σw = 0 , then it is necessarily true that the speed remains unchanged .
to quote a comment scott aaronson made on his blog : can you perform an arbitrarily long computation with minimal effort , by leaving your computer on earth , boarding a spaceship that accelerates to close to the speed of light , then turning around and returning to earth , where you find civilization collapsed , your friends long dead , and the sun going cold , but your important computation finished ? here , as i like to point out in talks , the crucial problem is the energy needed to accelerate to relativistic speed . indeed , if you want to get a superpolynomial speedup by the above means , it’s not hard to show that you need to accelerate your spaceship to faster than c-1/p ( n ) for any polynomial p . but that , in turn , requires a superpolynomial expenditure of energy ( assuming , of course , that you have nonzero mass ! ) . so , as long as we make the reasonable ( and separately justifiable ) assumption that in t seconds you can only collect poly ( t ) joules of energy , the extended church-turing thesis once again seems safe . to summarize : yes , physically possible . no , probably not more useful than regular computation . black hole computers are often lumped in with the rest of relativistic computers , but i am not so clear about what is the problem with them .
there are two equivalent descriptions$^1$ of the reduced two-body problem with a central potential $v ( r ) $: in an inertial frame with no fictitious forces : here $\frac{1}{2}\mu r^{2}\dot{\theta}^{2}$ is the angular part of the kinetic energy . in a rotating frame following the reduced particle with fictitious forces and only 1d radial kinematics : here $-\frac{1}{2}\mu r^{2}\dot{\theta}^{2}$ is the centrifugal potential ( notice the minus sign ! ) . each description leads to the same lagrangian $$l~=~t-u~=~l=\frac{1}{2}\mu ( \dot{r}^{2}+r^{2}\dot{\theta}^{2} ) -v ( r ) . $$ $^1$ the reduced particle is confined to an orbit plane , so the problem is effectively just two-dimensional described by two coordinates $r$ and $\theta$ . in both descriptions , the center-of-mass serve as the origin of the reference frame .
the question is a little tricky as stated . . . because your force is conservative , it can be written as the gradient of a scalar potential field . but the potential field , i.e. the potential energy , is defined only up to a constant . that is , your potential energy field is $$u ( x ) = \frac{kx^2}{2}-\frac{kx^4}{4a^2}+u_0 , $$ for any value of $u_0$ . this u_0 comes up because we have done an indefinite integral of the force field to find it . so in all purity , the total energy of your particle is $t_0+u_0$ , so it can be whatever you want , because you can choose $u_0$ freely . . . for gravitational potential the convention is to place zero energy at infinity , for potentials such as yours it makes more sense to have zero energy at the origin . this is equivalent to choosing $u_0=0$ . which is a very reasonable thing to do , but in no way mandatory . the full definite integral thing to arrive at conservation of energy from $f = ma$ is as follows , $$ma = f ( x ) $$ $$m\frac{dv}{dx}v=f ( x ) $$ $$mvdv = f ( x ) dx$$ $$\int_{v_0}^{v_1}{mvdv}=\int_{x_0}^{x_1}{f ( x ) dx}$$ $$\left . \frac{1}{2}mv^2\right|_{v_0}^{v_1} = \left . ( -\frac{kx^2}{2}+\frac{kx^4}{4a^2} ) \right|_{x_0}^{x_1}$$ $$\frac{1}{2}mv_1^2 - \frac{1}{2}mv_0^2 = -\frac{kx_1^2}{2}+\frac{kx_1^4}{4a^2} + \frac{kx_0^2}{2}-\frac{kx_0^4}{4a^2}$$ $$\frac{1}{2}mv_1^2 +\frac{kx_1^2}{2}-\frac{kx_1^4}{4a^2} = \frac{1}{2}mv_0^2+ \frac{kx_0^2}{2}-\frac{kx_0^4}{4a^2}$$ $$t_1 +u_1 = t_0+u_0$$
i did not find the equation and the argument you quoted in that paper . but , yes , it is the brouwer degree , deg$ ( \hat{\phi} ) $ , which equals the monopole number $$ n\equiv\frac{1}{4\pi}\int_{\mathbb{r}^3} \mathrm{tr} ( f_a\wedge d_a ( \phi ) ) =\frac{1}{4\pi}\int_{\mathbb{r}^3} d ( \mathrm{tr} ( \phi ) f_a ) =\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr} ( \phi f_a ) $$ $$ =\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr} ( \hat{\phi} f_a ) $$ where the one has used bianchi identity , stokes ' theorem , to obtain the first two equalities , and jaffe and taubes show in their book that one can replace $\phi$ by $\hat{\phi}$ . now this coincides with the brower degree , for which there is an explicit formula : $$n=\mathrm{deg} ( \hat{\phi} ) =-\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr ( \hat{\phi} d\hat{\phi}\wedge d\hat{\phi}} ) \in \mathbb{z}=\pi_2 ( s^2 ) = [ s^2 , s^2 ] $$ ( what you wrote . ) this is physically understood as an infinite wall potential , separating the monopole sectors corresponding to different integers . now , to actually answer your question , you can compute this integral for the t'hooft-polyakov monopole solution , for which $$ \hat{\phi}= ( \sin ( \theta ) \cos\phi , \sin\theta \sin\phi , \cos\theta ) _i\cdot \sigma^i , $$ and you will find $$n=-\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr ( \hat{\phi} d\hat{\phi}\wedge d\hat{\phi}} ) =+\frac{1}{4\pi}\int_{ [ 0,2\pi ] } \int_{ [ 0 , \pi ] }\sin\theta d\theta\wedge d\phi=1 . $$
i am not sure if there is a definitive answer because i have seen it discussed recently at high level . i do think there is some broad agreement that entropy is important because it has an irreversible property : closed systems progress from low entropy states to higher entropy states . so we can define the passage of time more precisely by talking about increasing total entropy . sean carroll of cosmic variance has written many interesting posts on the subject . i think his ideas boil down to the universe was initially in a low entropy state and our conscious experience increases total entropy , so our conscious experience coincides with the time direction being away from what we call the beginning of the universe . ( i stand to be corrected . i am not even sure the arguments can be stripped down so far . )
there is much more matter in the interstellar medium than in the visible stars . our best estimate of the total matter/energy content of the universe is shown in this image from nasa : now the dark energy is not really matter - it acts like an anti-gravitational vacuum energy which is responsible for the accelerating expansion of the universe . the dark matter is probably some kind of matter that does not have any strong nuclear or electromagnetic interactions with our ordinary ( baryonic ) matter ( atoms ) . it may possibly only have gravitational interactions , but all the current searches for dark matter assume that it also has a weak nuclear interaction with ordinary matter . so that leaves $4.6\%$ of the universe that is made from atoms . it is believed that only about $\frac{1}{2}\%$ of the mass of the universe is in the form of the atoms that are inside stars ( and planets ) . so for every atom in a star , there are 9 atoms in the interstellar medium . some of the atoms in the interstellar medium would be gas clouds inside of galaxies and some of it would be the diffuse interstellar gas between galaxies and even clusters of galaxies . the amount of mass/energy in black holes is a more difficult question ( and about which i know much less ) . some of the atoms have already collapsed into black holes such as the super-massive black holes in the center of most galaxies and the black holes that may result when stars go supernova . the super-massive black holes in the centers of galaxies are still only a very small fraction of the total mass of the galaxy so all of these " known " black holes are probably only a small fraction of the $\frac{1}{2}\%$ of the atoms that are in the form of stars . however , there could be some primordial black holes left over from the big bang . these primordial black holes could constitute part of the dark matter of the universe . most standard cosmological theories do not predict any ( or many ) primordial black holes and there have been some searches for them with gravitational lensing . in this area , i know much less , but i believe these primordial black holes cannot be a big fraction of the dark matter , but i do not know what the upper limit is precisely .
this should be a comment as i am not knowledgeable about the lhc refrigeration , but it is too long for a comment and i can hazard a pretty good guess . sheer heat capacity likely accounts for a great deal of this time . assuming the total amount of kit that needs to be cooled is , say $2\times10^4$ tonnes , and if it has roughly a $1{\rm kj k^{-1} kg^{-1}}$ heat capacity , that means we have to extract $20{\rm gj}$ for every degree k that we cool . once we get down below $100{\rm k}$ were going to see some serious multipliers happenning . an ideal heat pump needs work input $w = q_{lhc} \left ( \frac{t_{out}}{t_{lhc}} - 1\right ) $ to pump out heat $q_{lhc}$ from the kit at temperature $t_{lhc}$ and dump it to the environment at $t_{out}$: this is the reversible heat pump . so if we are drawing this heat out and dumping it at $300k$ , say , the energy needed to get from $300k$ to $5k$ we get as a rough estimate ( assuming heat capacities stay constant , which they will not , but there will not be any phase changes of most of the kit ) : $$w_{total} = \sigma \int\limits_{t_{lhc}}^{t_{out}} \left ( \frac{t_{out}}{t} - 1\right ) \ , {\rm d}t$$ where $\sigma$ is the $20{\rm gj k^{-1}}$ total heat capacity i estimated above . plugging in the numbers $t_{lhc} = 5k$ ( not everything will need to be cooled all the way down to $1.9k$ ) and $t_{out} = 300k$ we get : $$w_{total} = \sigma \left ( t_{out}\left ( \log\left ( \frac{t_{out}}{t_{lhc}}\right ) -1\right ) +t_{lhc}\right ) = 933 \sigma \approx 20{\rm tj}$$ this is the total output of a $5{\rm gw}$ power station for over an hour , roughly the energy released by the first of the only two nuclear weapons brought to bear in combat . $5gw$ electricity generation is the electricity consumption of two million australians , and we are extremely greedy electricity users by world standards , so i do not know how many normal people this would represent . the lhc site quotes a peak power consumption of $180mw$ and about $30mw$ is used for cryogenics . $30{\rm tj}$ at $30mw$ is about ten days . another factor is stresses in the kit induced by too swift cooling or warming . i am slightly familiar with the design of some of the magnetic beamsteering hardware , and much of this kit is toleranced to within tens of microns . one can not brook even the tiniest of any irreversible , plastic deformations of the kit and still have it work properly . so it is likely that heat transfer could not go much faster than this even if the refrigeration capacity were there . there will also be economic considerations too . even if one can cool faster than with $30mw$ refrigeration and still meet technical / engineering constraints , refrigeration capacity is expensive , particularly if you are only using this full capacity for cooldown during maintenance . the rest of the time the refrigeration needs are much less , so you have an economic tradeoff between capital spent on capacity that is unused most of the time and the cost of project delays arising from downtime . i am absolutely sure exactly this calculation has been done , as it ones like it are done for all soundly managed engineering projects .
diamond dust ( or dust of any other material ) will not conduct heat anywhere close to as well as the solid material . at a molecular level the dust is not in very good contact with other grains of dust . there is plenty of separation and air in between the particles that will retard heat conductivity . if you were to compress the dust so significantly that it did conduct as well , i am certain the pressure would be great enough to cause the dust to bond with other dust into a larger solid . for more a more technical treatment of the thermal conductivity of powder beds , see this paper .
if you ask whether there is a phase difference of 90° between the electric field and the magnetic field , the answer is yes . the electric field and the magnetic field oscillate in quadrature . you can see that from the conservation of the total electromagnetic energy during one oscillation cycle , or as you mentioned it , to conserve the energy of a " single photon " during a cycle . plug quickly $$e = e_0 cos ( \omega t ) $$ and $$b = b_0 sin ( \omega t ) $$ in the instantaneous electromagnetic energy density ) , remembering that $$cos ( \omega t ) ^2 + sin ( \omega t ) ^2 = 1 $$ and $$ b_0 = e_0/c $$ in the present case of transverse electromagnetic waves propagating in free-space .
you may have encountered it in a different context , but i recognize it from the topic of singularities of correlation functions in quantum field theory . in massless field theories such correlation fucntions becomes singular for points which are lightlike separated , and the structure of such singularities is determined by good physical principles such as locality and unitarity . then again , i may be completely off the mark . in any event , except for the linguistic similarity , i do not think it has to do with null singularities in spacetime .
energy ( n . ) 1590s , " force of expression , " from middle french énergie ( 16c . ) , from late latin energia , from greek energeia " activity , operation , " from energos " active , working , " from en " at " ( see en- ( 2 ) ) + ergon " work , that which is wrought ; business ; action " ( see urge ( v . ) ) . used by aristotle with a sense of " force of expression ; " broader meaning of " power " is first recorded in english 1660s . scientific use is from 1807 . energy crisis first attested 1970 . http://www.etymonline.com/index.php?term=energy huygens ( 1650 's ) was the first to develop the terminology , stating that : energy is not like matter energy does not have size , shape or occupy space energy does not have inertia instead , it was defined that energy is a measure of the ability of a physical system to perform work http://abyss.uoregon.edu/~js/ast122/lectures/lec03.html
the standard way to propagate uncertainties is , in this case , $$ \delta d = \left|\frac{\partial d}{\partial r}\right|\delta r=\frac{c\ , \delta r}{r^2} , $$ where $\delta r$ is a positive quantity . then $\delta d&gt ; 0$ gives you half the width of your uncertainty interval in $d$ .
so i am not an expert in limit cycles by any means but i am intrigued by this problem so here is what i came up with . let 's treat the nonlinear term perturbatively . this will not be enough to prove the existence of the limit cycle for large values of $\mu$ , but given that apparently there is a proof that this works perturbatively , it will be enough for us . let 's take an ansatz \begin{equation} x ( t ) =\bar{x} ( t ) +\mu \delta ( t ) \end{equation} where \begin{equation} \bar{x} ( t ) =a\cos ( \omega_0 t ) \end{equation} here $\mu\delta ( t ) $ is a small perturbation . just to be clear , i am thinking of $\mu$ as the small parameter , $\delta$ is not a small function . i am assuming the perturbation will scale like $\mu$ ( as opposed to $\mu^2$ or $\mu^3$ ) , this will be justified later . ( ok i do not actually explicitly justify it later . the point is that the $o ( \mu ) $ equation below would not have given any useful information had this scaling been wrong ) . by the way , if we could calculate the form of the limit cycle for large $\mu$ we could generalize the analysis by making $\bar{x}$ equal to the limit cycle and then running through all the steps below . the point of the small $\mu$ approximation is that the limit cycle must be approximately the harmonic oscillator path in this limit . i do not know much about this stuff but i would not be surprised if there was a way to calculate the limit cycle curve . what we expect to happen is there to be a special value of $a$ such that this ansatz is stable ( meaning that $\delta$ will not blow up ) . numerically you have discovered that this value is $a=2x_0$ , we would like to see if we can see this peturbatively as well . so we expand out the equation . at $o ( \mu^0 ) $ , we find the harmonic oscillator equation , of course . at $o ( \mu ) $ we get an equation for $\delta$: \begin{equation} \ddot{\delta}+\omega_0^2 \delta = ( x_0^2 -\bar{x}^2 ) \dot{\bar{x}} \end{equation} after subbing in the form for $\bar{x}$ and using some trig identities we find \begin{equation} \ddot{\delta}+\omega_0^2 \delta = a \omega_0 x_0^2 \sin ( 3\omega_0 t ) + a\omega_0 ( a^2-4x_0^2 ) \cos^2 \omega_0 t \sin \omega_0 t \end{equation} this is a forced harmonic oscillator : the right hand side has two forcing terms . let 's look at the second one : \begin{equation} \cos^2 ( \omega_0 t ) \sin ( \omega_0 t ) = \sin ( \omega_0 t ) - \sin^3 ( \omega_0 t ) = \frac{1}{4}\sin ( \omega_0 t ) + \frac{1}{4} \sin ( 3 \omega_0 t ) \end{equation} there are multiple terms here , but the problem is that there is a term with frequency $\omega_0$ . this drives the oscillator at its resonant frequency , creating an instability . so the fluctuations are unstable so long as that second term is present . but precisely when $a=2x_0$ , the dangerous resonant driving term vanishes , and the fluctuations are stable . voila .
consider derivative at $t=0$ ; denote $\psi ( 0 ) $ as $\psi_{0}$ $ ( \partial /\partial t ) t\psi ( t ) \big|_{t=0}=\displaystyle lim_{h\rightarrow0} ( ( t\psi_{0} ) ( h ) -t\psi_{0} ) /h$ since $t\psi$ evolves according to $i ( \partial /\partial ( -t ) ) t\psi ( t ) =ht\psi ( t ) $ so $ ( t\psi_{0} ) ( h ) = exp ( ihh ) t\psi_{0}$ . hence we have : $\displaystyle lim_{h\rightarrow0} ( ( t\psi_{0} ) ( h ) -t\psi_{0} ) /h$ $=\displaystyle lim_{h\rightarrow0} ( exp ( ihh ) ( t\psi_{0} ) -t\psi_{0} ) /h$ $=\displaystyle lim_{h\rightarrow0} ( texp ( -ihh ) \psi_{0}-t\psi_{0} ) /h$ $\:\:\ ; \ ; \ ; $ ( since $t$ and $h$ commute , and $t$ is antilinear ) $=\displaystyle lim_{h\rightarrow0}t ( exp ( -ihh ) \psi_{0}-\psi_{0} ) /h$ $=t ( \partial /\partial t ) \psi ( t ) \big|_{t=0}$ notation : $ ( t\psi_{0} ) ( h ) $ means we first act $\psi_{0}$ by $t$ and then time evolve the resulting state by an amount of time $h$ . another argument : following argument seems more relevant here than above one :- we have a one parameter family of states $\psi ( t ) $ which satisfy $i ( \partial /\partial ( t ) ) \psi ( t ) =h\psi ( t ) $ for definiteness suppose $t\in [ 0,1 ] $ , and suppose we partition this interval into $n$ equal parts ( where $n$ is some large number ) as {$0=t_0&lt ; t_1&lt ; . . . . &lt ; t_{n-1}&lt ; t_n=1$} . denote $\psi ( t_j ) $ as $\psi_j$ for $j=0 , . . . , n$ , and let $1/n=\delta$ ( length of one small interval ) . then above differential equation can be written as a set of $n$ linear equations in terms of states $\psi_j$ 's as : $i ( \psi_1-\psi_0 ) /\delta=h\psi_0$ $i ( \psi_2-\psi_1 ) /\delta=h\psi_1$ . . . . $i ( \psi_j-\psi_{j-1} ) /\delta=h\psi_{ ( j-1 ) }$ . . . . $i ( \psi_n-\psi_{ ( n-1 ) } ) /\delta=h\psi_{ ( n-1 ) }$ now in zee 's book the one parameter family of vectors $t\psi ( t ) $ is required to satisfy the differential equation $-i ( \partial /\partial t ) t\psi ( t ) =ht\psi ( t ) $ . or in discretised form it is required that the set of vectors $t\psi_0 , t\psi_1 , . . . . . , t\psi_n$ satisfy following linear equations : $-i ( t\psi_1-t\psi_0 ) /\delta=ht\psi_0$ $-i ( t\psi_2-t\psi_1 ) /\delta=ht\psi_1$ . . . . $-i ( t\psi_j-t\psi_{j-1} ) /\delta=ht\psi_{ ( j-1 ) }$ . . . . $-i ( t\psi_n-t\psi_{ ( n-1 ) } ) /\delta=ht\psi_{ ( n-1 ) }$ now since $t$ is linear wrt addition of states so it can be taken out : $-it ( \psi_1-\psi_0 ) /\delta=ht\psi_0$ $-it ( \psi_2-\psi_1 ) /\delta=ht\psi_1$ . . . . $-it ( \psi_j-\psi_{j-1} ) /\delta=ht\psi_{ ( j-1 ) }$ . . . . $-it ( \psi_n-\psi_{ ( n-1 ) } ) /\delta=ht\psi_{ ( n-1 ) }$ in continuum limit these equations are equivalent to : $-it ( \partial /\partial t ) \psi ( t ) =ht\psi ( t ) $ edit : question : consider a one parameter family of states $\psi ( t ) $ which satisfy schrodinger equation $i ( \partial /\partial t ) \psi ( t ) =h\psi ( t ) $ . is it possible to find an invertible linear operator $t$ that commutes with $h$ and such that for any $\psi ( t ) $ as above , $t\psi ( t ) $ satisfies $-i ( \partial /\partial t ) t\psi ( t ) =ht\psi ( t ) $ ? our previous argument ( 2nd one ) extends to one proof that it is not possible ; here is another one : if $t$ is such an operator then $t\psi ( t ) =exp ( ith ) t\psi ( 0 ) $ . ( because $t\psi ( t ) $ solves time reversed schr . equation ) ----- ( 1 ) also $\psi ( t ) =exp ( -ith ) \psi ( 0 ) $ ( because $\psi ( t ) $ solves usual schr . equation ) . ------- ( 2 ) substituting ( 2 ) into ( 1 ) we get $texp ( -ith ) \psi ( 0 ) =exp ( ith ) t\psi ( 0 ) $ now using the fact that t is invertible we get : $exp ( -ith ) \psi ( 0 ) =t^{-1}exp ( ith ) t\psi ( 0 ) $ again using the fact that $t$ is linear and commutes with $h$ we get $exp ( -ith ) \psi ( 0 ) =exp ( ith ) \psi ( 0 ) $ ( note that if $t$ were antilinear then in place of $exp ( ith ) $ on rhs we would have $exp ( -ith ) $ , and hence there would be no problem ) now multiplying on both sides with $exp ( -ith ) $ we get $exp ( -2ith ) \psi ( 0 ) =\psi ( 0 ) $ differentiating with respect to $t$ and putting $t=0$ we get $h\psi ( 0 ) =0$ but $\psi ( 0 ) $ was any arbitrary state in our space of states . so we have $h=0$ identically . hence the required linear operator is not possible unless $h$ vanishes identically .
for a fixed muzzle velocity the time the cannonball stays in the air depends on the vertical component of the velocity , so trajectory a would stay in the air longest . the trajectory with the longest duration is firing directly upwards . if the angle to the ground is $\theta$ , then the vertical velocity is $v_0sin\theta$ , and the time in the air ( neglecting air resistance ) is $2v_0sin\theta/g$ , where $g$ is the acceleration due to gravity .
note that friction opposes ( or tends to oppose ) relative motion and ceases to act when there is no tendency of relative motion . friction acts on lower point of disc till it comes at rest with respect to ground . with respect to ground the lower point can considered as superposition of 2 velocities : $v_{cm}$ ( linear ) and $w_{cm}r$ ( due to rotation of disk about centre ) in opposite directions where $cm$ stands for centre of mass of disc which is its centre ( here ) . friction provides force and torque to adjust them together till $v'=w'r$ where v ' and w ' are new speeds and angular speeds respectively . now no friction acts . so , would it matter if friction can act or not ? $no . $
acuriousmind has it right . the properties of a scalar field would not in principle be different from any other scalar field theory . it depends on the lagrangian for the theory of course , but the form of the lagrangian would still be constrained by the same restrictions on other theories ( general covariance , renormalizability , phase invariance of some sort , etc . ) . the interactions would arise from a term like $\mathcal{l}_{int}\sim \lambda\phi ( x ) f ( g_{\mu\nu} ) $ where $\phi ( x ) $ is your scalar field and $f$ is some function of the tensor field in question . in the case of brans-dicke theory , this term is $\phi r$ with $r$ the ricci scalar for the metric tensor . the coupling constant would naturally determine the strength of the interaction .
in $y$ direction you have accelerated movement with constant acceleration , thus $$v_y = v_{y0} - g t$$ and after putting initial conditions $$|v_y| = g t$$ i have no idea whatsoever what did you want to do with your calculation .
yes , indeed , first of all it tells you that a particle of momentum $p$ in one frame looks like a particle of momentum $\lambda p$ in another frame which is related to the first by lorentz transformation $\lambda$ . that means $p$ is a lorentz vector and hence gives you more certainty that it indeed captures momentum of the particle . if you read a few more pages in the book , you will see that $\sigma$ would correspond to spin for a massive particle and helicity for a massless particle . so , the above equation is telling us that not only do you find a different momentum for the particle but also that you might find the particle carrying a different spin ( or more concretely the probabilities to find the particle in different spin states would be changed ) when you do a lorentz transformation . compare this with a usual case one encounters in qm . we have a massive motionless particle as our physical system . and we now want to study how the physical state transforms under a rotation $$u ( r ) \psi_{p=0 , \sigma} = c_{\sigma ' , \sigma} ( r , 0 ) \psi_{p=0 , \sigma'}$$ here $\sigma$ labels spin of the particle and $u ( r ) $ forms a representation of rotation group which is generated by the angular momenta $j_1 , j_2 , j_3$ . so , this equation is telling you that transformation of single particle states in relativistic quantum mechanics is a straightforward union of the transformations that we expect from our earlier study of relativity and quantum mechanics .
after some amount of on and off thinking here 's what i have come up with . please pardon the coarse picture . the interpretation of the dispersion as energy is applicable to non-interacting particle . in general , for interacting particles , $e ( \vec{k} ) $ cannot be interpreted as energy ( of ? ) . however , frequency $\omega$ is always proportional to energy of the system . one could see it in the following way : the schrödinger’s equation , $$i \frac{\partial}{\partial t} \psi = \hat{h} \psi , $$ on fourier transforming is given by , $$ \omega \psi = \hat{h} \psi . $$ therefore , the set of ( discrete ) frequency $\omega$ is the set of eigenvalues of the hamiltonian operator $\hat{h}$ . so the conjugate variable to time $t$ should always correspond to energy of the system . in non-interacting case $\omega \propto e ( \vec{k} ) $ . but , generically in the presence of interactions it should not be the case . comments and corrections are very welcome .
water does not form a liquid at very low pressure . here is the this phase diagram : ( image from http://www.lsbu.ac.uk/water/phase.html and appears to be under the creative commons non-commercial no-derivatives ) the lowest pressure at which water is liquid is that of the triple point : 273.15 k and 611.73 pa . that pressure is about 0.006 atmospheres . wikipedia puts the surface pressure on mars at around 636 pa , from which we conclude that there is just barely room for water to be a liquid in a very narrow temperature band above the earthly freezing temperature ( and even this relies on getting the full pressure quoted above which will not be true except at the lowest elevations ) . no asteroid has sufficient atmosphere to maintain liquid water . if you have ice and heat it , it will sublime ( convert directly to vapor ) .
it depends on the hamiltonian , i.e. , the interactions the beam is subject to after the first measurement . if the beam is allowed to propagate freely , then a pure state , say $|0\rangle$ , will remain in that state always because the hamiltonian only contains a momentum operator that will not affect the spin component of the state vector . if on the other hand , after the first measurement , there are other interactions like a magnetic field , then it can lead to a non-trivial schrodinger evolution of the pure state ( spin part ) which will change the outcome at the second measurement .
it seems your three equations are not linearly independent and so check your math . you should have 3 independent equations to solve for 3 variables like $t$ , $f$ and $a$ . in general you solve for one variable in one equation and substitute it into the others . like equation #2 $t= m ( g-a ) $ to get $$ f = - m g \sin\theta - ( m+m ) a + m g $$ . then repeat for the next variable . in your case pluggin the above $f$ into equation #1 returns $m a=m a$ which is not helpful and hence i stated that your equations should be independent to solve correctly . can you post a diagram in order to check your equations also ?
ok , my initial guess is the votable format , but after some digging i found that the format is one of . cat , table and scat . you can save any catalogue as a local file and open it in the editor and see . generally speaking jskycat should in principle take on many formats as the gaiaskycat or skycat does since it is developed from that . the software also " listens " to the simple application messaging protocol ( samp ) servers , so that is the reason that i think votable is its choice . edit : i found some documentation at the features page for jskycat where it actually confirms my guess .
for a star of a given mass , we can calculate , based on theoretical models , how long it has to live . for example , the sun is currently 5 gyr old and will live another 5 gyr . so if you observe the sun from , say , andromeda , the light would be about 2 million years old and you could therefore conclude that the sun is still alive even though you are observing " old " light . if you flip it around , if we see a sun-like star in andromeda , we could safely say it is still alive . broadly , while fusing hydrogen in their cores , more massive ( and therefore brighter , hotter ) stars live shorter lives . on the one hand , it means that we observe some small stars whose lives will be longer than the current age of the universe . on the other hand , we could theoretically observe stars whose lifetimes are shorter than the distance to them , in light-years . so , a star 80 times as massive as the sun might have a lifetime of a few million years , so if we currently observe it in andromeda near the end of its life , that star is probably gone now . off the top of my head , i would say most of the stars we presently observe still exist as we see them now . we just can not resolve stars that far away . for what it is worth , there are some distant phenomena that we see that happen much faster than the light takes to reach us . for example , the 2011 nobel prize went to the leaders of two teams that observe type ia supernova at redshifts up to about $z=1$ . these are events that lasted less than a year that happened nearly 8 billion years ago . who knows what they look like now !
i will mention just one example of the complexity that curved space introduces into the quantization process . consider minkowski space quantization of the free klein gordon field , which satisfies $$ ( \box+m^2 ) \phi=0$$ a fundamental step in the procedure is performing the mode expansion $$\phi ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}} ( a_{\bf{k}}e^{-ikx}+a^*_{\bf{k}}e^{ikx} ) d^{3}\bf{k}$$ here we have a splitting into a negative frequency part $$\phi^- ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}a_{\bf{k}}e^{-ikx}}d^{3}\bf{k}$$ and a positive frequency part $$\phi^+ ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}a^*_{\bf{k}}e^{ikx}}d^{3}\bf{k}$$ upon quantization , the $a^*_{\bf{k}}$ in the positive frequency parts become creation operators and the $a_{\bf{k}}$ in the negative frequency parts annihilation operators . the splitting is covariant - the exponentials contain lorentz scalars . now if we try to do the same thing in curved space to the ( covariant form of ) the klein gordon equation , we can find spacetimes for which there is no clear way to perform this splitting because in general , there is no " natural " time coordinate . in the minkowski case , we had the action of the poincare group to allow us to deal with the different possible time coordinates - we even had a poincare invariant vacuum state , but here there is no equivalent of the poincare group action . the particle content of the theory depends upon this splitting , and the ambiguity we have in the curved case ( or even in the flat case if we allow non inertial frames ) is the origin of the hawking and unruh effects . that was just a single example of a problem that crops up right from the word " go " in curved space quantization . there has been a lot of effort expended over the last few decades studying quantization on curved spacetimes . for a review , see here .
your intuition about the charge repulsion and strong force acting on protons more is less important that you think . the strong nuclear force is a few orders of magnitude greater than electromagnetism so coulomb repulsion just does not contribute much . what matters most is the nuclear binding energy to separate a proton from the nucleus . if the resulting system is below the proton separation energy it is possible for the proton to tunnel out . see proton emission and proton drip line for more information about this . it does happen but remember neutron emission is also rare . $\beta^+$ and $\beta^-$ and alpha emission are much more common .
i am in part trying to understand this myself . the berry phase is computed from differential forms , such as the one-forms $\omega$ constructed from states $$ \omega~=~\langle\psi|d\psi\rangle $$ and with the covariant differential $d~=~d~+~\omega$ the two-forms $$ \omega~=~d\omega~=~d\omega~+~\omega\wedge\omega $$ the tensor components of the 2-form $f$ are elements of a self-adjoint principal bundle $p$ . the determinant of these elements $$ det\big|1~+~\frac{ixf}{2\pi}\big|~=~\sum_nc_jx^n $$ which is a characteristic polynomial which represents the chern class . each $c_n ( p ) $ is an element of $h^{2n} ( m ) $ . so the curvature form for the berry phase , or the fubini-study metric $\omega=~dz\wedge d{\bar z}/ ( 1~+~|z|^2 ) ^2$ is evaluated $\int\omega~=~2\pi i$ and gives $c_1~=~1$ so there is a nontrivial cocycle on the “2-level . for this projective geometry there are alternating betti numbers $1 , ~0$ for even and odd . if you had some product of states $\prod_n |\psi_n\rangle$ , say in an entangled state etc , you could apply the differential $d$ up to $n$ times and form and $n$-form . for instance the product $|\psi_1 , ~\psi_2\rangle$ $=~|\psi_1\rangle|\psi_2\rangle$ defines the one-form $$ \omega~=~d|\psi_1 , ~\psi_2\rangle~=~d|\psi_1\rangle|\psi_2\rangle~+~|\psi_1\rangle d|\psi_2\rangle $$ and one could then build up a system of differential forms on various chains . the analogue of the projective geometry for this is a $g_2 ( v ) $ grassmannian and this continues up for n-product spaces .
i see two current main axis of research around entanglement . i have not really followed the last years of research , so i do not know exactly which questions are open and which have been solved . i sadly have no time to elaborate this week , but since its a community wiki , others can complete my answer . entanglement characterization the question is on how to characterize entanglement as a resource and how to interconvert various entanglement form into others . while the problem has essentially been solved for two-party-entanglement , many open question subsists in n-party-entanglement . for a recent technical review , you can read the horodecki family paper in review of modern physics ( restricted access ) ( arxiv version ) . finding quantum physics from information theoretic consideration it is well known that quantum entanglement does not allow any faster than light communication . this is called the " no-signalling " principle . however , the no-signalling principle does not forbids theories with stronger correlations than quantum mechanical entanglement ( see here . ) it has been shown that if one could build such a " super-entangled " machine , one could perform various unintuitive communication-complexity related tasks ( see here and here for technical papers about this , and here and here for blog posts about the second paper ) . the interesting open question is the following : can we find all the quantum-entanglement correlations ( and the quantum mechanics formalism ) from reasonable information-theoretic constraints similar to the no-signaling principle .
due to internal stress in the material . this stress might be there due to fabrication technology or due to heat cycling while usage ( less likely ) .
ball lightning could definitely be some atmospheric pressure plasma phenomenon . you can make a pretty impressive ball plasma by discharging a kilojoule-scale capacitor bank into a bucket of salt water . check out free-floating atmospheric pressure ball plasma . in most of those pictures they are using a copper sulfate solution , but that is not essential ( sodium chloride also works ) . these ones only last a ( significant ) fraction of a second , but i am sure if you made a larger one ( e . g . by a lightning strike ) , they could last longer . btw , this was the subject of a killer science fair project : http://www.youtube.com/watch?v=se6sbanskoc
i think you have a misunderstanding of the technical terms vapor pressure , boiling and partial pressure . vapor pressure or better equilibrium vapor pressure is the pressure at which an equilibrium is reached between evaporation and condensation at the liquid surface . usually it is a function of liquid temperature . e.g. water has a vapor pressure of about 0.03 bar at 25°c . thus , if the steam pressure in the surrounding is below 0.03 bar , more water will evaporate than steam condensates at the water surface . if the steam pressure is above 0.03 bar , more steam will condense at the water surface than water evaporates . so far this is not related to boiling . boiling occures if the liquid is heated so far that the vapor pressure of the liquid is as high as the ambient pressure . e.g. water at 100°c has a vapor pressure of 1 bar , which is the usual ambient pressure . so at a surface of hot water at 100°c the steam pressure totally replaces the ambient air and the steam is transported away from the surface . this strong evaporation process is called boiling . partial pressure is not related to evaporation processes at all . if you describe air as a mixture of oxygen and nitrogen the total pressure $p_t$ is just the sum of the partial pressures of oxygen $p_{oxygen}$ and nitrogen $p_{nitrogen}$: $p_t = p_{oxygen} + p_{nitrogen}$ . both oxygen and nitrogen are non-condensable gases under standard conditions . this concept applies for any gas mixture also without condensating gases . coming back to your statements : this one is wrong . vapor pressure is a physical property of a liquid at a given temperature . as described above it is the pressure at which condensation and evaporation are in equilibrium . so far it is not related to boiling at all . this one is right . if the steam pressure in the ambient atmosphere is equal to the vapor pressure , condensation and evaporation are in equilibrium and one could say evaporation has stopped . but , be aware that the term partial pressure is not related to evaporation processes only , because also non-condensable gases have a partial pressure in a gas mixture .
for the path element $d\vec{l}$ around a circle with radius $a$ you can write $d\vec{l}=rd\phi\vec{e_\phi}$ with $\vec{r}=-r\vec{e_r}$ ( note the minus sign , since the vector points from the wire to the center ) you get $$\vec{h}=\frac{i}{4\pi}\oint \frac{r^2 ( -\vec{e_\phi}\times \vec{e_r} ) }{r^3}d\phi$$ substituting $a$ for $r$ and integrating $\phi$ from 0 to $2\pi$ and realizing $-\vec{e_\phi}\times\vec{e_r}=\vec{e_z}$ $$\vec{h}=\frac{i}{4\pi}\frac{1}{a}\vec{e_z}\int_0^{2\pi}d\phi = \frac{i}{2a}\vec{e_z}$$
the answer to your first question is yes . building on demetrios christodoulou 's seminal work showing that black holes can form " generically " from focusing of gravitational waves starting from an initial space-time that is arbitrarily close to flat , pin yu has recently shown that one can also dynamically ( and generically , in the sense that the formation is stable under small perturbations ) form a black hole starting with only electromagnetic waves . of course , the interaction between electromagnetism and gravity means that as soon as you set the thing in motion , you will pick up gravitational radiation . and also that since a precise covariant notion of local gravitational energy is not available , the idea that the space-time starts out with only electromagnetic waves is a specific , frame dependent mathematical definition ; one should keep that in mind before trying to draw too much physical significance out of the casual statement of the theorem . for your specific second question , the answer is also yes . einstein 's equation specifies that $$ g_{\mu\nu} = t_{\mu\nu}$$ the left hand side , the einstein tensor , is purely geometrical , and reflects the curvature of space-time . the right hand side comes from the energy-momentum contributions from the matter fields . the standard way of coupling electromagnetic waves to general relativity ( einstein-maxwell theory ) gives that the right hand side is zero only when the electromagnetic field vanishes . so the content of einstein-maxwell theory is based on that electromagnetic radiation can curve space-time .
do not get too confused by the " bladeless fan " marketing babble . something , probably a traditional blower , is pushing air around inside the device . this is ducted so that the flow blows in one direction from little nozzles on the inside of a ring . that causes a lot more air to be moved by bernoulli 's principle . basically , the ring and nozzles converts high pressure low flow air into low pressure high flow air . overall , this system is likely to be ( i do not have any numbers , just a guess on my part ) less efficient than a traditional fan . the claimed advantage is that you do not feel pulses as individual fan blades spin around . i find that argument rather hard to swallow since i never noticed pulses from a traditional fan . after a relatively short distance the flow will break up and become turbulent anyway , even if it started out perfectly smooth , so this whole issue smells strongly of marketing bs to me .
when the notions of electric and magnetic fields were conceptualized , they imagined that there was an invisible fluid being pushed around by charges , and they leveraged some of the equations and terminology of fluid mechanics . modern understanding of field have largely gotten rid of this picture , but some colorful langauge like " electric flux " remains . if you want to picture positive charge as " amount of fluid added to region per unit time " and negative charge as " amount of fluid removed from region per unit time " , you can , but this thinking only gets you so far . safer to just think of it as an abstract mathematical definition .
work is described by the following formula : $$w=\vec{f} \cdot \vec{s} = f\cdot s \cdot \cos \alpha$$ with $s$ is the distance , 40 m in this case , and $\alpha$ the angle between $\vec{f}$ and $\vec{s}$ , which in this case happens to be zero , giving $w = f \cdot s$ $f$ is force , in this case work is done against gravitational force , so $$f=mg$$ $m$ is the liquid mass and $m=\rho v$ $\rho$ is the liquid 's density , $v$ is the liquid 's volume . $$w=\rho vgs$$ and therefore $$\rho=\frac {w}{vgs}$$
okay ! i apply a transformation , which converts my comment into a slightly more self-contained answer . the transformations mentioned here are most naturally described by means of tensor calculus or , more generally , differential geometry . it is needed , roughly speaking , when one wants/needs to introduce a coordinate system at each point in space and study the relations between different points and their coordinate systems . in this case there is a coordinate transformation defined from cartesian system $x^\mu= ( x , y , z ) $ to ellipsoidal coordinates $x^{\mu'}= ( \eta , \mu , \nu ) $ , and an inverse transformation . $\mu$ and $\mu'$ are indices , which can take values from $1$ to $3$ . trasnformation matrix $t^{\mu}_{~\mu'}$ is defined as $t^{\mu}_{~\mu'}=\dfrac{\partial x^{\mu}}{\partial x^{\mu'}}$ . for the given transformation $t^{\mu}_{~\mu'}$ is diagonal , because $x$ depends only on $\eta$ , and so on . metric tensor with matrix $g_{\mu\nu}$ is a quantity , which defines the scalar product of basis vectors $g_{\mu\nu}\equiv \vec{e}_\mu\cdot\vec{e}_{\nu}$ . in cartesian coordinates $x^{\mu}$ , therefore , $g_{\mu\nu}=\textrm{diag}\{1,1,1\}$ . in primed coordinates , for a given point in space ( with its own set of basis vectors ) the same definition holds : $g_{\mu'\nu'}=\vec{e}_{\mu'}\cdot\vec{e}_{\nu'}$ . from tensor calculus , $g_{\mu'\nu'}$ and $g_{\mu\nu}$ are connected by $g_{\mu'\nu'}=g_{\mu\nu} t^{\mu}_{~\mu'} t^{\nu}_{~\nu'}$ , where summation over the same indices is implied . because $t^{\mu}_{~\mu'}$ is diagonal , $g_{\mu'\nu'}$ is also diagonal , as it should be . hence $\vec{e}_{\mu'}$ are orthogonal . now , in orthogonal bases $\vec{e}_{\mu}$ scale factors are defined as $h_{\mu}=\sqrt{\vec{e}_{\mu}\cdot \vec{e}_{\mu}}$ ( which has some physical meaning if one thinks about decomposing a vector in such a basis ) , here summation is not implied . in cartesians , therefore , $h_1=1$ , whereas in ellispoidal coordinates $h_{1'}=\sqrt{\vec{e}_{1'}\cdot \vec{e}_{1'}}=\sqrt{g_{1'1'}}=\sqrt{g_{\mu\nu} t^{\mu}_{~1'} t^{\nu}_{~1'}}=\sqrt{g_{11} t^{1}_{~1'} t^{1}_{~1'}}=t^{1}_{~1'}=t^{1}_{~1'} h_1$ . or , alternatively , one can derive ( without using $h_1 = 1$ ) that $h_1=h_{1'}t^{1'}_{~1} = h_{1'} ( t^{1}_{~1'} ) ^{-1}$ . using the above described , $t^{1}_{~1'}=\dfrac{\eta}{x}=\dfrac{\eta}{\sqrt{\eta^2-a^2}}$ . substituting $t^{1}_{~1'}$ and $h_{1'}$ into $h_1= h_{1'} ( t^{1}_{~1'} ) ^{-1}$ and using $\eta\sim a \rightarrow \infty$ , one gets $h_1 = 1$ . the answer might be hard to read if you have never studied differential geometry , but the key point is that scale factors are not scalars and a simple variable change does not work for them . however , as they are defined for orthogonal systems , the transformation rules that they follow are relatively simple .
given a motion function $\mathbf{x} = \mathbf{\chi} ( \mathbf{x} , t ) $ , the deformation gradient is given by $$ \mathbf{f} = \frac{\partial \mathbf{\chi} ( \mathbf x , t ) }{\partial \mathbf x} $$ if you carry this out , you should find that $det ( \mathbf f ) = ( 1+t ) ( 1+t^2 ) &gt ; 0 \quad\forall t&gt ; 0$ as for finding the inverse motion function , you have to do a bit of algebra . you are given $\mathbf{x} = \mathbf{\chi} ( \mathbf{x} , t ) $ , the components of which form 3 linearly independent equations which you must use to solve for $\mathbf{x}=\mathbf{\chi}^{-1} ( \mathbf{x} , t ) $ .
you essentially answered your own question in your comment . the force per unit length between two current-carrying wires is $$\frac{f}{\delta l} = \frac{\mu_0 i_1 i_2}{2 \pi r}$$ for a derivation , see here . the currents are all equal , $i_1=i_2=i_3=i$ . the distances are all equal , $r_{12}=r_{13}=r_{23}=l$ . $\mu_0 =4\pi \times10^7 n/a^2$ is a constant , so you can leave it in your answer as $\mu_0$ . this will only give you the magnitude of the force . you will need to figure out the direction from the directions of the magnetic fields . to get the force $f_1$ on wire 1 , you can consider the force $f_{12}$ on wire 1 from wire 2 , then add the force $f_{13}$ on wire 1 from wire 3 . ( remember to add the forces as vectors ! their directions will matter . ) once you find the force on wire 1 ( including direction ) , the forces on wire 2 and wire 3 will have the same magnitude , and you should be able to find their directions easily .
i claim that the partial and total time derivatives of the hamiltonian are equal whenever the hamiltonian is evaluated on a solution to hamilton 's equations of motion . for conceptual simplicity , let 's restrict the discussion to systems with a two-dimensional phase space $\mathcal p$ with generalized coordinates $ ( q , p ) $ . it is important to note what the total time derivative and partial time derivative mean in this context . in particular , recall that the hamiltonian is a function that maps a pair consisting of a point $ ( q , p ) $ in phase space and a point $t$ in time , to a real number $h ( q , p , t ) $ . when we say that we are taking the partial time derivative of $h$ , we mean that we are taking a derivative with respect to its last argument ( in my notation ) . when we say that we are taking a total time derivative , we have in mind evaluating the phase space arguments of the hamiltonian on a parameterized path $ ( q ( t ) , p ( t ) ) $ in phase space , then then taking the derivative with respect to $t$ of the resulting expression , like this ; \begin{align} \frac{d}{dt}\big ( h ( q ( t ) , p ( t ) , t ) \big ) \end{align} if we use the chain rule , we find that this total time derivative can be related to the partial time derivative of $h$ as follows : \begin{align} \frac{d}{dt}\big ( h ( q ( t ) , p ( t ) , t ) \big ) = \frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \dot q ( t ) + \frac{\partial h}{\partial p} ( q ( t ) , p ( t ) , t ) \dot p ( t ) + \frac{\partial h}{\partial t} ( q ( t ) , p ( t ) , t ) \end{align} i have deliberately not abbreviated notation here to make explicit what exactly is going on so that there is no confusion . for example , the expression \begin{align} \frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \end{align} means that we take the partial derivative of $h$ with respect to its first argument ( which i labeled $q$ ) , then then i evaluate the resulting function on $ ( q ( t ) , p ( t ) , t ) $ . now the question is , when are the total and partial time derivatives the same ? well , the relationship between them that we derived above shows that this happens if and only if the other stuff in the equation vanishes ; \begin{align} \frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \dot q ( t ) + \frac{\partial h}{\partial p} ( q ( t ) , p ( t ) , t ) \dot p ( t ) =0 \end{align} notice , now , that this equation definitely does not hold for a general path $ ( q ( t ) , p ( t ) ) $ in phase space . i will leave it to you to find a simple counterexample . so , for what paths does this relationship hold ? well , notice that this relationship is satisfied provided the path satisfies hamilton 's equations ; \begin{align} \dot q ( t ) and = \frac{\partial h}{\partial p} ( q ( t ) , p ( t ) , t ) \\ \dot p ( t ) and = -\frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \end{align} in other words , we have demonstrated the claim i started with .
what do you mean by " prove " ? if you mean in a strict mathematical sense , then looking for such assurances is a lost cause . there are quite a wide variety of papers on this matter . curie attacked this particular problem in 1913 with radium . they immersed a radium source in liquid hydrogen for more than an hour and did not find a change of more than 0.1% in its activity . you can read more from the paper by curie and kamerlingh onnes entitled , " the radiation of radium at the temperature of liquid hydrogen " in knaw , proceedings , 15 ii , 1912-1913 , pp . 1430-1441 . people even claimed , from russia , that polonium 's activity varied depending on geography . hardly the case . more recently , work has been done on the half-life decay rate of $^{97}ru$ without seeing a noticeable temperature dependence near 20k compared with rt . see the paper by goodwin , golovko , iacob and hardy entitled , " half-life of the electron-capture decay of $^{97}$ru : precision measurement shows no temperature dependence " in physical review c ( 2009 ) , 80 , 045501 . it could be that there is a small dependence , but not even the russian paper mentioned above by martin agrees there is a measurable temperature dependence .
the phase constant is needed only if you have a specific initial condition , e.g. if i told you where $x$ was at time $t = 0$ , you could solve for $\varphi$ . otherwise you can just choose whatever you want for it : note that it is the same in all functions . choosing some value for $\varphi$ is analogous to you manually setting the time origin to something you like . imagine observing the harmonic motion for a while , and then deciding that $t = 0$ should be the time where the piston is all the way up . or you could decide that $t = 0$ should be the time where the piston is all the way down . or right in the middle . thus , for your problem , you can just set $\varphi = 0$ .
the reason is because the heat loss occurs mostly in the windows and the fenestration . the idea is that you would like the incoming air to be heated up . also , it creates an air curtain that prevents more heat from being lost through this exposed areas . the final reason is to make the temperature of the room more or less uniform . if the heaters were placed at the center of the room , you would create a large temperature gradient , resulting in drafts and discomfort for the occupant .
yes , the comination $j_1 + j_2$ determines the spin of the particle . note however , that this is an addition of angular mementum which may be complicated . furthermore , you can count the degrees of freedom : in $ ( j_1 , j_2 ) $ , each contribute $2j_1 + 1$ states and we construct a tensor product , so $ ( j_1 , j_2 ) $ gives $ ( 2j_1 + 1 ) * ( 2j_2 + 2 ) $ degrees of freedom . for the vector we have $ ( 1/2 , 1/2 ) \mapsto 2 * 2 = 4$ degrees of freedom . if the representation is reducible , i.e. of the form $ ( j_1 , j_2 ) + ( k_1 , k_2 ) $ , then you simply add the d.o.f. you get from each pair . the dirac spinor has $ ( 1/2 , 0 ) + ( 0 , 1/2 ) \mapsto ( 2 ) + ( 2 ) = 4$ degrees of freedom , the field-strength tensor has $ ( 1 , 0 ) + ( 0 , 1 ) \mapsto 3 + 3 = 6$ d.o.f. as a sidenote : the representations $ ( 1 , 0 ) $ do not corresond to vectorlike degrees of freedom , but rather to antisymmetric self-dual tensors . $ ( 0 , 1 ) $ is the antisymmetric anti-self-dual tensor . the vector ( and the only way to get a vector out of this ) is $ ( 1/2 , 1/2 ) $ !
clouds move with the wind , so the cloud velocity is just the wind velocity . the recent storm in the philipines reached wind velocities of 200 mph , though the higest speed reported is apparently 253 mph . the fastest moving clouds known are on neptune , where the winds reach 1340 mph .
you might not like this answer , but you want to solve for $x\left ( t\right ) $ in $$ m \ddot{x} + \omega^2 x = f\left ( t\right ) , $$ with $\dot{x}\left ( 0\right ) = x\left ( 0\right ) = 0$ . a laplace transform gives you $$ x\left ( t\right ) = \mathcal{l}^{-1}\left\{\frac{\mathcal{l} \left [ f\left ( t\right ) \right ] \left ( s\right ) }{m s^2 + \omega^2}\right\} , $$ where we assume $\mathcal{l} \left [ f\left ( t\right ) \right ] \left ( s\right ) $ exists and $f\left ( t\right ) = 0$ for $t &lt ; 0$ . here , $$ \mathcal{l} = \int_{0}^{\infty} dt \ e^{-s t} , $$ $$ \mathcal{l}^{-1} = \frac{1}{2 \pi i}\int_{\mathcal{l}} ds \ e^{s t} , $$ and $$ \int_{\mathcal{l}} ds $$ denotes integration in the complex $s$ plane along the laplace contour $\mathcal{l}$ . the integral is then evaluated using closure and residue techniques .
it is conservation rules ( mostly that of energy ) that controls what decays are possible , and what channels are allowed ( the strong interaction respected quark flavor , but the weak interaction does not ) . if the final state has higher energy than the initial state there can be no spontaneous process leading from one to the other . the energy environment of a nucleus is a complicated place because it is affected by the strong nuclear force , electromagnetic forces and by the limits on state occupation imposed by pauli exclusion . there are a lot of question already on the site about when various processes can proceed and related subjects $\alpha$ decay to more than one nuclear state what stabilizes neutrons against beta decay in a neutron star ? how can a proton be converted to a neutron via positron emission and yet gain mass ? why is the ( free ) neutron lifetime so long ? nuclear fission and half life adding many more neutrons to a nucleus decreases stability ? . . . your question about the case where a nucleus has multiple modes have different half-lives is tricky . the measured result is the same half-life for each mode , but we can ask " if we could magically block modes and study the isolated half-life of each mode , what would they be ? " if you perform the ( often approximate ) calculation for that question you generally find different half-lives . the interesting part is the the branching fractions for each decay are related to ratios of the calculated " isolated " half-lives , so you are seeing the effect of the different probabilities when you look at the frequency of different decays .
interesting , i was just studying the fourier decomposition of vowels for half an hour yesterday . first , you must distinguish vowels and consonants . words like " bee " ( which is how we spell the letter " b" ) of course are not uniform sounds . they start with a consonant , in this case one created by lips . depending on the way how they are created , consonants are divided to many groups . see a table of consonants here . in general , consonants are types of noise because they do not have a well-defined basic frequency . it means that the fourier series for a consonant is composed pretty much of all sufficiently high frequencies . the color of the noise – whether higher frequencies tend to be more strongly represented than the lower one etc . – determines the type of the consonant . there are consonants with a throat sound added , like l , m , n , which may be fourier decomposed similarly to the vowels , and noise-based consonants such as b , d , g , v , z which are the sound-equipped cousins of p , t , k , f , s , and so on , and so on . the most monochromatic sounds are the vowels . they can be sung so they have a well-defined base frequency . whether one gets u , o , a , e , i – or , in english , oo , aw , ah , eh , ee etc . – depends on how the mouth is opened . this modifies the shape of the resonance cavity and therefore the preferred additional frequencies that are excited by the action of the base frequency coming from the throat . the presence of higher harmonics is essential for the difference between vowels . i recommend you to look at a page about it , for example this one : http://hyperphysics.phy-astr.gsu.edu/hbase/sound/vowel.html the vowel u ( oo in english ) has the lowest representation of the higher harmonics . it is the closest one to the harmonic sound and it is achieved by changing one 's mouth into a passive tube through which the sound penetrates . on the other hand , a ( ah ) and i ( ee ) have a huge , important contribution of higher harmonics and when i say higher , i do not mean the 2nd or 3rd . the 20th harmonic etc . etc . are very important . in fact , it is more accurate to talk about the absolute frequencies . the vowel a ( ah ) has lots of those higher harmonics that are close to 1,000 hz ( cycles per second ) which are already suppressed in u ( oo ) and partly suppressed in o ( aw ) . as you continue to go towards e ( eh ) and i ( ee ) , the contribution from frequencies close to 3,000 hz starts to increase . these frequencies are calculable from the size of the mouth opened in the right way , from the length of the resonant cavity that becomes comparable to the wavelength of the sound waves that become important . in principle , all vowels may be emulated by the fourier expansion using just the base frequency ( pressure goes like $\sin \omega t$ ) plus higher harmonics but the very harmonics including $\sin 20\omega t$ are still very important for the character of the vowel . phonetics is the portion of linguistics that studies how language sounds ; the experts partially learn some physics although fourier series are not their primary tool . still , to understand phonetics , one has to accept various basic things . i found out that native english speakers misunderstand phonetics because they do not really decompose the language into " pure sounds " . your representation of " b " as a sound analogous to " a " may be an extreme example because you may have meant " b " in the sense of " bee " which is clearly composed of two sounds , the consonant " b " and the vowel " ee " . but even when it comes to vowels only , english ( and french and some other languages ) is deliberately obscuring the reality as it pronounces many vowels in a variable way . for example , " my " is pronounced as " mai " where the vowel gradually changes from " ah " at the beginning to " ee " at the end . some native english speakers do not even realize that this " y " in " my " is not a single uniform vowel . there are many other examples , of course .
there are two related but distinct questions : how do you keep a wormhole stable ? how do you make the wormhole in the first place ? courtesy of matt visser we can give one answer to the first question . matt 's example is to make the wormhole cube shaped , and in that case all you need to do is construct a cube from string i.e. the twelve edges of the cube are made from string . however the string would have to have a negative tension , and indeed it would have to have the ridiculously high negative tension of $−1.52 \times 10^{43}$ joules/metre . this is where your exotic matter comes in since the tension in any string made from normal matter would always be positive . the second question is harder . matt 's analysis applies to a time independant wormhole , i.e. one that has existed for an infinite time . constructing your cube of exotic string would warp spacetime in the manner required for a wormhole , but calculating what happens as you tie the strings into a cube is probably impossible at present . response to comment : this is going to be a bit hard to explain , but the space inside the cube does not exist . it is not part of the manifold on which the universe exists . if you travelled towards the cube you would not hit anything - you had just keep going without feeling anything as you past where it is wall is , but now you had be travelling in the other region of spacetime on the other side of the wormhole . re your comment it does not sound like there is a lot of control , the wormhole matt describes is not the same as the sort of wormhole sci-fi writers use to allow interstellar travel . as far as i know there is no theoretical support for the interstellar travel type wormhole . the wormhole matt describes connects two regions of spacetime but makes no statement about the global topology , so the region of spacetime the other side of the wormhole need not be , and almost certainly is not , some distant region of the universe around us . the wormhole does not allow ftl travel to e.g. alpha centauri . it just allows travel ( at up to the speed of light ) to the new region of spacetime on the other side of the wormhole .
he means that since it is a second order differential equation , it is completely determined by two sets of information , namely the value of $\phi$ and ${\dot \phi}$ at some time $t=t_0$ . in other words , $\phi ( t_0 ) $ and ${\dot \phi} ( t_0 ) $ paramaterizes the solution . we can therefore choose them to take any values , some which will imply a negative value of $\rho$ .
the general carelessness with the so-called " principle of least action " it that even in very good and reliable sources it is incorrectly stated that the action must be minimal . while the principle only requires that the action must be stationary , e.g. $\delta s = 0$ . so , more correctly , it should be called a " principle of stationary action " . concerning your example -- both of your trajectories are stationary , therefore both of them might be the true trajectory of your body .
it depends on the fan , but i would guess the majority of domestic fans will use less power at lower speeds . i can state with authority that the fan in my car ( a ford focus ) uses roughly the same power regardless of speed because i have just had to replace the ballast resistor that is uses to control fan speed . when you select a lower speed the fan dissipates power as heat in the ballast resistor so the speed setting makes little difference to the power drawn . i can not be sure about domestic fans , but in the car fan the heat dissipated in the ballast resistor is very noticable and indeed theresistor gets too hot to touch . the fan on my desk does not get hot when used at a lower speed , so i think it is very likely it does not simply dissipate power to lower the speed and therefore it will use less power at lower speeds . it is probably significant that the car fan is dc while domestic fans are ac . it is much easier to control power in ac circuits because you can use a thyristor or something similar to control the power delivery in a lossless way . the only way to be sure is , as energynumbers suggests , to measure the power drawn . a simple power meter like this one is all you need . unfortunately i am working away from home this week otherwise i could measure the power drawn by my own fan and give you a definitive answer . however i am sure there must be some fan owning , power meter armed , experimental physicists reading this :- )
without knowing more about your data set i can only offer a few random suggestions : if you have some kind of pid reason to believe that the tracks might be electron you just assume that they are and compute the energy from the momentum and $m_e$ . in you have a calorimeter in the detector stack you measure the energy , and project the maximum likelihood energy loss back to the vertex . note , however that the measured signal may have error much larger than $m_e$ , so it may be better to use the device for pid and fall back on the previous suggestion .
the data set you show is a jumble of multiple contributions ( and the cern fits agree , see how many times those lines intersect and how they all lie close together in the horizontal band on the right side of the plot ? ) , and that makes for a problem . you need some way to disentangle these bits . the first thing i would tell a grad student to try is to separate the data-set on the basis of some other variables and fit each line to a sub-set of the data that she was confident contained mostly the particle they were interested in . this would generally be done by taking advantage of other data recorded about each hit in the plot{*} . if no additional data was available i would suggest using geometric cuts in the $de/dx$--$p$ plane to select the data to fit . that is fit to the parts of the plot which identifiable belong to a single particle species . if you are ambitious you could use a track density-and-width-model to sequentially subtract the results of each fit you get in order to make the next one easier ( at the cost of making the uncertainty analysis high correlated---yuck ! ) . additional options : start with a model for a particular particle and impose strict limits on the fitting parameters . consider the more fitting and log-likelyhood options to the fitter . also consider giving an artificially high weight to the data in the regions where the bands are well separated ( the is an alternative to the geometric cuts ) . if your monte carlo is good fit the lines to mc and simply plot them on the data . . ( this is not an uncommon thing to do ; does the caption on the cern data say where the lines come from ? ) {*} possibly something as simple as that plot representing the overlay of multiple particle-specific data-sets , or by taking advantage of pid detector systems , or particle species identifications arising from event topology . frankly you do not care what you get and you do not care if you lose a lot of the data in the selection ( statistics are not your problem when you have 7 million measurements ) .
suggestions : study how to derive the lie algebra $so ( p , q ) $ from the lie group $so ( p , q ) $ , cf . e.g. this phys . se post . work from now on at the level of lie algebra ( as opposed to lie group ) . show that $$\hat{j}^{\mu\nu}=\hat{x}^{\mu}\hat{p}^{\nu}-\hat{x}^{\nu}\hat{p}^{\mu}$$ are generators ( of a representation ) of the lie algebra $so ( p , q ) $ . in this way the issue of taylor series of the exponential map would be encountered only under pt . 1 , and only in the form of finite-dimensional matrices ( as opposed to differential operators ) .
i ) if we expect $\omega ( e ) $ to depend analytically on the variable $\hbar\omega&gt ; 0$ extended to ( parts of ) the complex plane , then we may regularize by introducing an $i\epsilon$ prescription , and substitute $$\tag{1} \hbar\omega ~\longrightarrow ~ \hbar\omega ( 1-i\epsilon ) . $$ the variable $$\tag{2} q~:=~ e^{-i\hbar\omega k}~\longrightarrow ~ e^{- ( i+\epsilon ) \hbar\omega k} $$ in the geometric series $$\tag{3} \sqrt{q}\sum_{n=0}^{\infty} q^n $$ will then have $$\tag{4} |q|~&lt ; ~1 , $$ so that the geometric series ( 3 ) is convergent . then all steps in schwabl 's derivation of $\omega ( e ) $ are mathematically well-defined . at the end of the calculation of $\omega ( e ) $ , we may put $\epsilon=0$ . ii ) concerning a complex ( as opposed to real ) stationary solution in the method of steepest descent / stationary phase method/saddle-point method , this is just part of the method . for a rigorous argument , one would have to consult the proof of the method . heuristically , it is because when one evaluates the gaussian integral over ' quantum fluctuations ' $$\tag{5} \int_{\mathbb{r}} \ ! dx~ e^{-\frac{a}{2}x^2+bx} ~=~\sqrt{\frac{2\pi}{a}}\exp\left ( \frac{b^2}{2a}\right ) , $$ for two complex constants $a , b\in\mathbb{c}$ , one only needs the condition $$\tag{6} {\rm re} ( a ) ~&gt ; ~0$$ to ensure convergence of the integral ( 5 ) . there is no need to also assume that the stationary solution $\frac{b}{a}$ is real . equation ( 5 ) follows from the fact that $$\tag{7} \alpha\int_{\mathbb{r}} \ ! dx~e^{-\frac{1}{2} ( \alpha x+\beta ) ^2} ~=~\int_{\gamma} \ ! d ( \alpha x+\beta ) ~ e^{-\frac{1}{2} ( \alpha x+\beta ) ^2} ~=~ \int_{\mathbb{r}} \ ! dx~ e^{-\frac{1}{2}x^2}~=~\sqrt{2\pi}$$ for any straight line in the complex plane $$\tag{8} \gamma ( x ) ~=~\alpha x+\beta , \qquad \alpha , \beta~\in~\mathbb{c} , \qquad x~\in~\mathbb{r} , $$ with slope $$\tag{9} |\arg ( \alpha ) |&lt ; \frac{\pi}{4} , $$ because in that case , it is possible to close the contour along exponentially suppressed arcs .
well the answer is that the body will indeed loose the momentum . but since the mass of the body will decrease as well due to radiation , the velocity should not change .
the problem is a statics one , where the sum of the forces on the beam equal zero and the sum of the moments about any point equals zero also . let say the hinge is point a at the coordinate origin . the rope is at a point b with coordinates $\vec{r}_b = ( 6\cos ( 30^\circ ) , 6\sin ( 30^\circ ) ) $ . the center of gravity is at a point c with coordinates $\vec{r}_c = ( 4\cos ( 30^\circ ) , 4\sin ( 30^\circ ) ) $ . if the weight is $w$ , the tension is $t$ and the pivot reaction $a_x$ and $a_y$ then the equations of statics are $$ a_x - t \cos ( 10^\circ ) = 0 \\ a_y + t \sin ( 10^\circ ) - w = 0 \\ 6\cos ( 30^\circ ) t \sin ( 10^\circ ) + 6\sin ( 30^\circ ) t \cos ( 10^\circ ) - 4\cos ( 30^\circ ) w = 0 $$ which you can solve for $t$ , $a_x$ and $a_y$ . i can confirm that the tension you found is indeed $t=13,203\ , {\rm n}$ conceptually you are stating that a steady state solution means balance of forces with zero accelerations .
the curie temperature or curie point is the temperature at which a ferromagnetic or a ferri-magnetic material becomes paramagnetic when heated . the effect is reversible . on the other hand , the curie-weiss temperature is the temperature at which a plot of the reciprocal molar magnetic susceptibility against the absolute temperature t intersects the t-axis . the curie-weiss temperature can adopt positive as well as negative values . i hope , now you will get the difference .
in this case the index can vary on the number of spatial dimensions ( three if you are in 3d ) . the $i_n$ notation refers to the fact that you are " repeating " the scalar product "$\mathbf{a}\cdot\nabla$" n times : $$ ( \mathbf{a}\cdot\nabla ) ^n = ( a_i\nabla_i ) ^n = ( a_{i_1}\nabla_{i_1} ) ( a_{i_2}\nabla_{i_2} ) \dots ( a_{i_n}\nabla_{i_n} ) = a_{i_1}a_{i_2}\dots a_{i_n}\nabla_{i_1}\nabla_{i_2}\dots \nabla_{i_n} $$
the mean anomaly relates position and time of an orbiting body . it is zero at the perihelion and increases with time . its formula is : $m=m_0+nt$ . so , in this case $m_0=-3.289°$ is the mean anomaly when the measurement was made . $n=0.9856$ is the mean motion , which is $2\pi$ divided by the duration of the full orbit . more information here .
no , it cannot be enough . stokes ' theorem says that the volume ( $\omega$ ) integral of $d\omega$ , a form that is the exterior derivative of another one ( of $\omega$ ) , may be written as a surface integral . but it does not allow us to rewrite the volume integral of a general integrand ( which is not the exterior derivative of anything ) such as the lagrangian density ${\mathcal l}$ as a surface integral . so the stokes ' theorem is useless for dealing e.g. with the action $s$ that defines the dynamics of a general theory in the volume . one should mention that when the action is topologically invariant , ${\mathcal l}$ may indeed be locally written as a " total derivative " , and in that case , the theory has indeed a provable relationship with lower-dimensional theories ( a major example is chern-simons theory in 3 dimensions and the related wznw theories in 2d ) . but the general theories we know – the standard model coupled to gravity – are not of this special type , at least not manifestly so . what is happening in the volume is general – we surely do care about values of some fields such as the electric field in particular places of the volume – and there apparently is not any " counterpart degree of freedom " on the surface that we could associate it with . some people including leonard susskind and steve shenker etc . do suspect that there exists some " conceptually simple " proof of the holography in which almost all the degrees of freedom in the volume would be unphysical or topological – some huge gauge symmetry that allows one to eliminate all the bulk degrees of freedom except for some leftovers on the surface . but such a proof of holography remains a wishful thinking . meanwhile , we have several frameworks – especially the ads/cft – that seem to unmask the actual logic behind holography . the surface theory is inevitably " strongly coupled " ( i.e. . strongly dependent on quantum corrections ) if the volume description appears at all so things can not be as simple as you suggest , it seems .
the ordering ambiguity is the statement – or the " problem " – that for a classical function $f ( x , p ) $ , or a function of analogous phase space variables , there may exist multiple operators $\hat f ( \hat x , \hat p ) $ that represent it . in particular , the quantum hamiltonian is not uniquely determined by the classical limit . this ambiguity appears even if we require the quantum operator corresponding to a real function to be hermitian and $x^2 p^2$ is the simplest demonstration of this " more serious " problem . on one hand , the hermitian part of $\hat x^2 \hat p^2$ is $$ \hat x^2 \hat p^2 - [ \hat x^2 , \hat p^2 ] /2 = \hat x^2\hat p^2 -i\hbar ( \hat x\hat p+\hat p\hat x ) $$ where i used your commutator . on the other hand , we may also classically write the product and add the hats as $\hat x \hat p^2\hat x$ which is already hermitian . but $$ \hat x \hat p^2\hat x = \hat x^2 \hat p^2+\hat x [ \hat p^2 , \hat x ] = \hat x^2\hat p^2-2i\hbar\hat x\hat p $$ where you see that the correction is different because $\hat x\hat p+\hat p\hat x$ is not quite equal to $2\hat x\hat p$ ( there is another , $c$-valued commutator by which they differ ) . so even when you consider the hermitian parts of the operators " corresponding " to classical functions , there will be several possible operators that may be the answer . the $x^2p^2$ is the simplest example and the two answers we got differed by a $c$-number . for higher powers or more general functions , the possible quantum operators may differ by $q$-numbers , nontrivial operators , too . this is viewed as a deep problem ( perhaps too excessive a description ) by the physicists who study various effective quantum mechanical models such as those with a position-dependent mass – where we need $p^2/2m ( x ) $ in the kinetic energy and by an expansion of $m ( x ) $ around a minimum or a maximum , we may get the $x^2p^2$ problem suggested above . but the ambiguity should not really be surprising because it is the quantum mechanics , and not the classical physics , that is fundamental . the quantum hamiltonian contains all the information , including all the behavior in the classical limit . on the other hand , one can not " reconstruct " the full quantum answer out of its classical limit . if you know the limit $\lim_{\hbar\to 0} g ( \hbar ) $ of one variable $g ( \hbar ) $ , it clearly does not mean that you know the whole function $g ( \hbar ) $ for any $\hbar$ . many people do not get this fundamental point because they think of classical physics as the fundamental theory and they consider quantum mechanics just a confusing cherry on a pie that may nevertheless obtained by quantization , a procedure they consider canonical and unique ( just hat addition ) . it is the other way around , quantum mechanics is fundamental , classical physics is just a derivable approximation valid in a limit , and the process of quantization is not producing unique results for a sufficiently general classical limit . the ordering ambiguity also arises in field theory . in that case , all the ambiguous corrections are actually divergent , due to short-distance singularities , and the proper definition of the quantum theory requires one to understand renormalization . at the end , what we should really be interested in is the space of relevant/consistent quantum theories , not " the right quantum counterpart " of a classical theory ( the latter is not fundamental so it should not stand at the beginning or base of our derivations ) . in the path-integral approach , one effectively deals with classical fields and their classical functions so the ordering ambiguities seem to be absent ; in reality , all the consequences of these ambiguities reappear anyway due to the uv divergences that must be regularized and renormalized . the process of regularization and renormalization depends on the subtraction of various divergent counterterms , to get the finite answer , which is not quite unique , either ( the finite leftover coupling may be anything ) . that is why the renormalization ambiguities are just the ordering ambiguities in a different language . whether we study those things as ordering ambiguities or renormalization ambiguities , the lesson is clear : the space of possible classical theories is not the same thing as the space of possible quantum theories and we should not think about the classical answers when we actually want to do something else – to solve the problems in quantum mechanics .
a string is a " particle with a complicated internal structure " . to see the rough emergence of particle species , you may start with a hydrogen-string analogy . the hydrogen atom is a bound state of a proton and an electron . it may be in various energy eigenstates described by the quantum numbers $ ( n , l , m ) $ . they have a different angular momentum and its third polarization and different energies that mostly depend on $n$ . it is similar for a string . a string may be found in various states . the exact " spectrum " i.e. composition of these states depends on the background where the string propagates and the type of string theory ( more precisely , the type of the string theory vacuum ) . but for the rough picture , consider string theory in the flat space , e.g. in the 26-dimensional spacetime . take an open string . its positions $x^\mu ( \sigma ) $ may be fourier decomposed and each of the fourier modes , labeled by a positive integer $n$ , produces coordinates of a 24-dimensional harmonic oscillator . so an open string is equivalent to a $24\infty$-dimensional harmonic oscillator ( yes , it is twenty-four times infinity ) . each of the directions in this oscillator contributes $nn/ \alpha'$ to the squared mass $m^2$ of the resulting particle where $n$ is the total excitation level of the harmonic oscillators that arise from the $n$-th fourier mode . at any rate , the possible values of the squared mass $m^2$ of the particle are some integer multiples of $1/\alpha'$ . this dimensionful parameter $1/\alpha'$ is also called $1/l_{string}^2=m_{string}^2$ . the ground state of the string , $|0\rangle$ of the harmonic oscillator , is a tachyonic particle with $m^2=-1/\alpha'$ in the case of bosonic strings . these tachyons are filtered away in the superstring . the first excited state of an open string is $\alpha^\mu_{-1}|0\rangle$ which carries one spacetime lorentz vector index so all these states behave as a vector with $m^2=0$ . they give you a gauge boson . and then there are massive modes with $m^2\gt 0$ . closed strings of similar masses have twice larger number of indices , so for example , the massless closed string states inevitably produce a graviton . so different masses of the resulting particles arise from different values of $nn$ – and the very fact that the values may be different for different excitations is analogous to the same feature of the hydrogen atom or any other composite particle in the world . in string theory , however , one may also produce states with different values of the angular momentum – also somewhat analogous to the hydrogen atom which is a sufficient model – or different values of the electric charge and other charges . for example , in some kaluza-klein-like vacua , the number of excitations of $x^5_{n}$ , the fourier modes of the ( circular ) fifth dimension $x^5$ , will be interpreted as the electric charge and it will behave as the electric charge in all physical situations , too . there are other ways how $u ( 1 ) $ electric-like charges and other charges arise in string theory . see e.g. this popular review http://motls.blogspot.com/2012/08/why-stringy-enhanced-symmetries-are.html?m=1 of ways how yang-mills gauge groups and charges may emerge from different formulations and vacua of string theory . if even this review is too technical , you will have to be satisfied with the popular brian-greene-like description stating that particles of different mass , spin , or charges emerge from strings vibrating in different ways . i am sort of puzzled about your question – and afraid that my answer will be either too simple or too off-topic given your real question – because you must have heard and read these basic insights about string theory about hundreds of times already .
diffractive optics are not magic , they are simply another tool that can be used in designing an optical system . they can do things that refractive optics cannot , and they are often lighter and smaller than an equivalent refractive optic . it is important to keep in mind , however , that the benefits of a diffractive optical element ( doe ) are not free . does have limitations of their own . they are harder to produce , and typically produce the desired results only under very specific conditions . for example , lets say you want to produce a circular laser beam with a very uniform intensity profile . what are your options for achieving this ? most laser sources produce a roughly gaussian beam , so you could expand this beam heavily with a refractive expander , and then mask out everything but the center of the beam . this will give you a relatively uniform beam , but you will waste a lot of light . you could use a more complicated refractive design , like a micro-lens array . this is difficult to engineer , and will not give perfect results , but it can do a very good job under a variety of conditions . the beam intensity can be made uniform over a large distance , and the input beam to the micro-lens array will not need to be perfectly collimated . it will also work across a relatively broad wavelength range . finally , you could design a doe beam shaper . these can be designed to give any intensity profile you like , but it will be expensive to produce . it may ( depending on what it is doing ) have certain flaws characteristic of does , like a strong zero-order beam ( where a large fraction of input to the doe passes through without being shaped ) . it may be very sensitive to errors in the input beam wavelength or collimation , and it may only produce the desired intensity profile over a short range of distances . like any tool available to a lens designer , doe 's have their uses . they can have very strong negative dispersion , which is often useful to correct chromatic aberration , and as i said they can be designed to produce arbitrary illumination patterns which would be outrageously difficult to make with purely refractive optics . lastly , while you can say they are " just fresnel lens with smaller features , " it is important to understand that a fresnel lens is a diffractive optic , just a very simple one . in fact , when your understanding of diffraction is deep enough , you will realize that , in some sense , all lenses are diffractive optics . while you can engineer the phase profile of a doe to produce a highly complex optical field , you could also design one to produce a simple focal spot ; the resulting design would be a simple lens !
in qed there are 4 kinds of divergences : ultraviolet divergences . naive calculations depend on the cut-off in such a way that they go to infinity as the cut-off do . however , qed is a perturbatively renormalizable theory so that non-naive , well-done computations ( see regularization and renormalization ) give sensible results . landau pole . the coupling constant $\alpha={e^2\over \hbar \ , c}$ , which is the expansion parameter in the perturbative series , grows with energy and goes to infinity for a finite value of the energy . it turns out that this finite value of energy is larger than the electroweak scale , where qed merges with the weak interaction and qed is not a good theory of nature anymore . therefore , it is not a real ( phenomenological ) problem . infrared divergences . these are due to the fact that photons are massless . they however cancel out once one takes into account all the effects that contribute to a measurable observable . non-convergent series . the $n$-th term of the perturbative expansion is of the form $\left ( {\alpha\over 2\pi}\right ) ^{n}\ , ( 2n-1 ) ! ! $ , so that the series is not convergent but asymptotic because the factor $ ( 2n-1 ) ! ! $ grows very fast for large values of $n$ . this means that we cannot give a non-perturbative definition of qft by summing up all the terms of the series . however , the first terms are meaningful and actually give predictions that accurately agree with observations . the ' first terms ' are approximately $n\sim {\pi\over \alpha}\sim 430$ . and for this value of $n$ , $\left ( {\alpha\over 2\pi}\right ) ^{n}\ , ( 2n-1 ) ! ! \sim 10^{-187}$ . therefore , as long as we are not interested in a precision of one part in $10^{187}$ , this is not a real problem either . note that qed is the theory of nature that has been confirmed with greatest precision — one part in $10^{9}$ in electron 's anomalous magnetic dipole , for which $n=4$ . for qcd points 1 , 3 , and 4 are more o less the same . however , point 2 does not apply since in qcd the coupling constant $\alpha_s$ gets lower with the increasing of energy , and in fact it goes to zero as energy goes to infinity . see asymptotic freedom . to summarize , infrared divergences are due to not taking into account effects that contribute to the observable magnitude . the asymptotic nature of qft perturbative expansions prevents a non-perturvative ( exact ) definition of the theory ( through its series ) , but does not entail a practical problem when comparing predictions with measurements . the lack of perturbative divergences and landau-like poles are a necessary condition for a theory to be well-defined at arbitrarily high energies . however , theories that contain these divergences ( ultraviolet or landau-like poles ) can still be very useful at energies above some scale . on the other hand , theories without these divergencies ( ultraviolet or landau-like poles ) , such as qcd , do not have to be valid to all energies as theories of nature . as m . brown points out in the comments , there is a relation between instantons and renormalons and the asymptotic nature of series . please , see these notes snd the questions instantons and non perturbative amplitudes in gravity and asymptoticity of pertubative expansion of qft reply to graviton 's comment : in my opinion , a fundamental theory of nature ( whatever it means ) should have a non-perturbative definition . if the perturbative expansion is not convergent , it cannot provide this non-perturbative definition . however , in principle , this does not necessarily mean that theory cannot have a non-perturbative definition or an exact solution , but this must be given by other means .
that seems like a fun question ! according to wikipedia the day is currently 2ms too long , so that is a factor of 2.31e-8 . so we need to reduce the angular momentum of the earth by this factor . to make life easy consider a mountain on the equator , with a mass $m$ , treat it as a point mass and assume we manage to move it $d$ meters nearer the centre of the earth . the change in angular momentum is : $$\delta l = m ( r_e - d ) ^2 - m ( r_e + d ) ^2 = -4mr_e d$$ where $r_e$ is the radius of the earth . assuming the earth is a uniform sphere it is angular momentum is : $$l_e = \frac {2}{5} m r_e^2$$ so i get the fractional change of the angular momentum to be : $$\frac {\delta l}{l} = 10 \frac{d}{r_e} \frac {m}{m}$$ bearing mind that we are modelling the mountain as a point mass , i would say about 10km was a reasonable distance to move it , i.e. from 5km above sea level to 5km below sea level , so taking $d$ as 10km and $r_e$ as 6380km and setting the change equal to 2.31e-8 gives : $$\frac {m}{m} = 1.5 \times 10^{-6}$$ so if the mass of the earh is about $6 \times 10^{24}$kg , you had need to move about $10^{19}$ kg of mountain . for comparison , a quick google suggests the mass of mount everest is of the order of $10^{15}$ to $10^{16}$ kg so that is somewhere between 1,000 and 10,000 mount everests .
for someone with perfect vision , the lens in their eye focuses a point source in their field of vision to a point source on their retina . for someone with less than perfect vision , the focus lies either before or behind the retina , resulting in a large spot on the retina , instead of a point . see the top illustration . when you squint , or roll your fingers into a tube in front of your eye , you are placing an aperture in the way of the light . see the bottom illustration . as the diagram shows , the focus stays the same distance away from the retina , but since the angles are smaller , the spot on the retina is also smaller . a smaller spot is closer to a point , and therefore seems " sharper " - even though the vision is just as bad . the camera obscura works on a similar principle .
the answer is in front of you $$ a = \frac{{\rm d} ( u ) }{{\rm d} t} = \frac{{\rm d}}{{\rm d} t} \left ( \frac{{\rm d} ( r ) }{{\rm d} t} \right ) = \frac{{\rm d}^2 ( r ) }{{\rm d} t^2}$$ as a matter of convention . it is just the way we write 2nd derivatives .
entropy never increases during this " mixing process " as it normally would , because it is not really a mixing process . the only thing that happens is a huge distortion of the distribution of the colors , but no information about it is lost . compare it to performing a fourier transform on some function : this also results in something that looks completely different , sometimes chaotic and high-entropy-like , but it is really just an invertible transformation .
i am afraid the actual situation is much more complicated than you have been told . for one thing , the superconductivity does not occur between neutrons , but between quarks themselves . the topic of high density qcd is a very cool interplay of condensed matter and high energy physics , and a very nice review is available by frank wilczek . however , that article does need some background in qcd and superconductivity simultaneously to appreciate . a shortened version might go something like this : inspiration : free fermions are incredibly unstable to superconductivity , in that any attractive interaction will cause it ( in fact , there is an old theorem by pierls ( ? ) that almost all interactions ( even repulsive ones ) will cause superconductivity if you cool far enough ) . in qcd , quarks naturally attract already ! so at sufficiently high density and low temperature , we can imagine that qcd can cause a strong attractive instability to a fermi gas of quarks . complications : 6 flavours , chiralities , masses of quarks are different , etc . simplification by complication : realise that the normal state of qcd ( i.e. . 3- and 2-quark combinations ) is just that : one possible state . other phases of qcd exist , and we can study the phase boundaries and so forth even if we can not compute things exactly ( universality saves the day ! ) . we find that at really high densities , quarks pair up to give a background diquark condensate , through which single quarks move , and through the anderson-higgs mechanism gains a large mass by eating some goldstone modes . all gluons become gapped ( again , anderson-higgs ) , apart from one which mixes with the photon . the symmetries of this solution is actually the same as that of normal matter --- replace baryons with quarks ( + their diquark condensed background ) and mesons with diquarks ; this suggests that they are really the same phase in theory . in practise , getting from one to another requires some other phases in the middle , which are more complicated and arise due to the quark masses and the number of flavours , etc .
tldr : cheap optics - the image gets worse with each new element added , pretty quickly . high end optics - the image may get better , may stay pretty much the same , or may get worse ; if it does get worse , it is by such a small amount that you can usually ignore it . this assumes a scope that is in good shape , otherwise all bets are off . now the long version : a t-adapter normally has no optical components , so there is zero loss of quality there . anyway , some actual answers : it is all a matter of price . use cheap mass-produced optics , and the quality will degrade pretty quickly . use more expensive elements and the image will stay amazingly clean even after passing through a large number of optical elements . a cheap plossl eyepiece has only a few lenses and the image looks like junk . a tele vue ethos eyepiece has 9 different lenses in 5 groups and the image looks incredibly sharp . keep in mind the quality of the primary mirror is also important . many mass-produced scopes come with primary mirrors that barely pass the rayleigh criterion . some are randomly better . the secondary mirror also matters . the corrector plate again introduces its own problems in catadioptric designs . " amateur " does not mean much . a mass-produced gso-made dob is " amateur " . but so is a clone made with optics figured by a reputable optician . yet one might be a terrible less than λ/4 instrument , end to end , while the other may well be above λ/15 and pretty high strehl factor . looking at tight double stars , the loose instrument may not even resolve the pair , while the good optics may put a clean sharp sliver of black between the two airy disks . one will show a few stripes on jupiter and not much else . the other will show plenty of detail . their performances will be very different . but so will be their prices . each element in the optical chain matters , and their individual performance levels are very much dependent on price . a single cheap element may ruin the image , while you could add tons of high quality optics and the image stays sharp and contrasty . this is all with amateur components that you will actually see used by various folks when your local astro club throws a star party over the hill south of the city . and then there is collimation . there are folks out there who ask " what is collimation ? " : ) for cryin ' out loud , people , that is not acceptable . the best optics in the world will mean nothing if the scope is not well collimated . the best car in the world will not work well if you forget to change the oil . then there is seeing . you take your expensive hand-figured scope outside , and seeing is terrible . you may as well look through a pirate 's spyglass , that is how bad the turbulence is . and then the next day the jet stream moves over some other place , and you get clean , pristine , sharp views . then there is thermal stabilization . a cheap mass-produced dob with a fan blowing over the primary mirror may actually beat the custom-made clone with no fan . well , after a few hours the fanless clone may catch up thermally and its view may improve a lot ( or not ) , but you get my point . goes on and on . there are no simple answers . edit : sometimes the image is improved when you add more glass . this is obvious in fast newtonians ( f/5 or faster ) , where coma is visible in the eyepiece . but then add a coma corrector , like a paracorr , to the optical chain , and the image is dramatically better . in fast newts , only with a coma corrector will the high end eyepieces achieve their whole potential . low end eyepieces need not apply , their own problems are worse than coma . same idea for astrographs : the field of virtually every scope is curved . the sensor of virtually any camera out there is flat . that is a mismatch . add a field corrector and the mismatch disappears . the image gets better when you add glass . heck , if you are ambitious and design a short achromat refractor , you will get chroma trouble in the eyepiece . but then filter out the offending wavelengths , and voila , no more chromatic aberration . a lowly filter has improved the image . sure , the image is tinted , but the overall resolution is better . ( this is a reason why you do not see short achromats too often ) edit2: let me confuse you a little . all of the above was a " whole field " discussion . if we are talking about strictly on-axis performance ( for folks who do planetary observations in tracking scopes ) , then less glass tends to be better . again , if the glass is low quality then it is no good , but as a rule , if you watch a very small low contrast target like a planet , and keep it on axis , then there are eyepieces dedicated for this kind of thing that manage to eek out a tiny bit more performance from the scope by using clever minimalist designs . some people have even tried ball lenses , extracted from laser couplers , which are just simple balls of glass or sapphire , and reported a very small improvement for on-axis planetary observations , compared to any eyepiece . these are not mere glass marbles , but high quality optics ; a tiny 9mm ball like that might go for \$20 . . . \$40 online . but this is kind of a fringe issue ( unless you have a very special thing for observing planets , in which case you will see this discussion in a very different light , so to speak ) . the question you asked is very complex .
i am going to assume that you have some experience with information theory as it relates to computation theory . this book : http://books.google.com/books/about/optical_computing.html?id=zffraaaamaaj is a good foundational look at how optical computing works , combining the physical processes with the computational understanding . this paper will provide a more up-to-date overview of where the state of optical computing is today : http://www.hindawi.com/journals/aot/2010/372652/ and more importantly provide further direction on where to look for more information .
you are operating under a misconception . when a real image is formed , we can see it , provided that our eye is positioned in a location such that rays from the image can enter your eye . compared to a hologram , the situation is different for a couple of reasons . ( 1 ) the possible locations of your eye are more restricted . ( 2 ) you may pick up psychological cues such as the framing of the field of view , which cause your brain not to interpret the real image as being where it actually is . by the way , the word is spelled " lens , " not " lense , " and it is not always true that a convex lens produces a real image .
no . you are correct that the kinetic energy is equal to the change in the potential energy , $mgh$ , where $h$ is the distance fallen , but because the object is accelerating $h$ is not simply velocity times time . if the object starts at rest then ( ignoring air resistance ) : $$h = \frac{1}{2} g t^2 $$ so substituting this into $mgh$ gives : $$e_{kinetic} = \frac{1}{2} m g^2 t^2 $$ note that $gt$ is just the velocity at time $t$ , so this expression is the same as : $$e_{kinetic} = \frac{1}{2} m v^2 $$ which may look familiar :- ) note however that the velocity $v$ is a function of time .
according to http://en.wikipedia.org/wiki/isotopes_of_carbon#table it emits two protons
based on your comment , i think you are indeed asking a more profund question than your teabag suggests : why is it that gravity is so weak compared to the other forces ? the answer is : we do not know . seriously , that is one of the holy grails : to first find the grand unified theory of nature in which all forces except gravity are explained as coming from one single symmetry group which is broken at our normal energies , and to then find the theory of everything that understands all four fundamental forces with a single , unified concept . in such a theory , it is hoped , the differences between the forces emerge naturally , instead of just being put in by measuring the relative strengths experimentally . but we do not know yet . we do not know which of the manifold concepts that are at the forefront of theoretical thinking right now might turn out to be indeed the ones realized in nature . but the question why gravity is so weak is really still unsolved . string theorists let gravity act in other dimensions , thereby weakening it in our four comparative to the others . others employ a form of the anthropic principle to argue that only in a universe where gravity is as weak as here life could have evolved and asked that question , so there is no deeper reason except that , for us to be here , it has to be like that . i apologize to all those whose theory i cannot name and briefly , unjustifiedly compress into a few words off the top of my head . that is it . we really do not know .
i think this is largely just terminology . strictly speaking the coordinate is $ct$ not $t$ , and of course $d ( ct ) = cdt$ . in any case we usually choose units where $c = 1$ and just ignore it .
first , your wave equation is wrong . you can see this from dimensional analysis . it should be \begin{align} \frac{\partial^2 \phi}{\partial t^2} = c^2 \frac{\partial^2 \phi}{\partial x^2} \end{align} second , you made a mistake in the cross terms for the $\partial^2 /\partial x^2$ term . the cross term should have the coefficient $-2\gamma^2 v/c^2$ . third , use the fact that $\gamma = \frac{1}{\sqrt{1 - v^2/c^2}}$ . you will get the desired result .
strictly speaking , you have 4 equations and 5 unknowns . however , given that the coefficient a is applied to the incoming wave-function , you could arbitrarily set it equal to 1 ( because it represents 100% of the wave ) and solve the system of equations for e . then $t=e$ . this is how the problem is handled in most cases . alternatively , if you absolutely cannot set $a=1$ , then try assuming a is a given and solve the 4 equations for b , c , d , and e in terms of a . then , again , perform $t=e/a$ . in theory , the ratio for any a will be the same as for a=1 . ( i checked , it is , the a divides out in the end ) . edit you can easily solve for b , c , d , and e using matrices , where your four system equations are : $$\begin{pmatrix}-1 and 1 and 1 and 0 \\ i \mathcal l and \mathcal k and -\mathcal k and 0 \\ 0 and e^{\mathcal kd} and e^{-\mathcal kd} and -e^{i\mathcal ld} \\ 0 and \mathcal ke^{\mathcal kd} and -\mathcal ke^{-\mathcal kd} and -i\mathcal le^{i\mathcal ld}\end{pmatrix} \begin{pmatrix}b \\ c \\ d \\ e\end{pmatrix}=\begin{pmatrix}a \\ i\mathcal la \\ 0 \\ 0\end{pmatrix} $$ optionally , $a=1$ . but if you invert the matrix and solve for e , you should get : $$e={4ia\mathcal k\mathcal l\over\mathcal k^2 e^{i\mathcal ld-\mathcal kd}-\mathcal k^2 e^{d\mathcal k+id\mathcal l}+2i\mathcal l\mathcal ke^{id\mathcal l-d\mathcal k}+2i\mathcal l\mathcal ke^{id\mathcal l+d\mathcal k}-\mathcal l^2 e^{id\mathcal l-d\mathcal k}+\mathcal l^2 e^{id\mathcal l+d\mathcal k}}$$ and , of course , a=1
let $x$ denote the length of the rope that is on the table , then $$ m ( x ) = \frac{m}{l}x $$ is the mass of the rope on the table . it follows that the force of friction on the rope on the table is $$ f ( x ) = \mu_k m ( x ) g = \mu_k\frac{m}{l}xg $$ if the rope moves an amount $dx$ then the work done by friction is $$ dw = f ( x ) dx = \mu_k\frac{m}{l}gx dx $$
33 mj is the electrical energy . i think the projectile is about 1kg , so the efficiency is about 10% , not so bad . plasma from electrical breakdown , which then gets accelerated the same way the projectile does , with exb force
if a body moves only because of the influence of a torque , then it will rotate about the center of gravity . there is no location for torques , only directions . you you take the equations of motion as seen here ( stackexchange-url you will see that the location of the torque does not enter into the equations . only the location of the forces . as a result the acceleration of the center of mass is zero , and only angular velocity will exist . the body will rotate about its center of mass . note that these two statements are equivalent : a pure force thorugh the center of gravity ( with no net torque ) will purely translate a rigid body ( any point on the body ) . a pure torque any point on the body ( with no net force ) will purely rotate a rigid body about its center of gravity consider a motionless rigid body with a pure instantenous torque $\vec{\tau}$ applied on it . the motion of any point a not on the center of gravity is $$ 0 = m \vec{a}_a - m \vec{c}\times \vec{\alpha} \\ \vec{\tau} = i_c \vec{\alpha} - m \vec{c} \times \vec{c} \times \vec{\alpha} + m \vec{c} \times \vec{a}_a $$ where $\vec{c}$ the position vector of the center of gravity relative to point a . the soltution to the above is $$ \vec{a}_a = \vec{c} \times \vec{\alpha} \\ \vec{\tau} = i_c \vec{\alpha}- m \vec{c} \times \vec{c} \times \vec{\alpha}+ m \vec{c} \times \vec{c} \times \vec{\alpha} = i_c \vec{\alpha} $$ $$ \vec{\alpha} = i_c^{-1} \vec{\tau} \\ \vec{a}_a = \vec{c} \times i_c^{-1} \vec{\tau} $$ from the above is it obvious that the only point a not moving is at $\vec{c}=0$ and all points parallel to $ \vec{\alpha}$ through the center of mass .
ok i think i see where your confusion lies . you are talking about the four velocity and acceleration in the instantaneous rest frame $f'$ , and as you say in this frame the four velocity is $ ( 1 , 0 , 0 , 0 ) $ . your mistake is to assume the four velocity is constant in $f'$ , because it is not . remember that after an infinitesimal time $dt$ the rocket is not longer in $f'$ - it is in a new instantaneous rest frame $f''$ . the rocket 's velocity in the new rest frame $f''$ is still $ ( 1 , 0 , 0 , 0 ) $ , but in old $f'$ frame it has now changed due to the acceleration . hence $d{\bf u}/dt$ in $f'$ is not zero . if you are interested chapter 6 of gravitation by misner , thorne and wheeler derives the equations of motion that you started with .
[ this is now a long answer . in summary , generally you need a physical assumption , the clock postulate , which people tend to omit , but is necessary , and can not be argued for a priori . however sometimes special relativity plus a restricted version of the postulate suffices . mundane experience is sufficient to verify this restricted version . ] let $\lambda = t$ the time according to inertial $o$ , and let $\vec{x}'$ be the spatial position of $o'$ according to $o$ , while $t'$ is the time measured by $o'$ . if $o'$ is piecewise inertial , then along each piece , $$c^2 ( \delta t'/\delta t ) ^2 = c^2 - ( \delta\vec{x}'/\delta t ) ^2\qquad [ 1 ] $$ and what you are trying to justify is that , even if $o'$ is not piecewise inertial , $$c^2 ( dt'/dt ) ^2 = c^2 - ( d\vec{x}'/dt ) ^2\qquad [ 2 ] $$ so , the problem is , special relativity strictly speaking only makes claims about inertial observers . and if you do not make any assumptions whatsoever about the experience of accelerated observers , then i think you are just stuck , mathematically i do not think you can go from $ [ 1 ] $ to $ [ 2 ] $ . ( for example , we can not rule out that proper acceleration itself further contributes to time dilation . ) i suggest : the motion of $o'$ is smooth . [ a1 ] for every $\epsilon&gt ; 0$ , there is a $\delta&gt ; 0$ such that , if , from the point of view of a certain unaccelerated observer $a$ , the magnitude of the velocity of another observer $b$ never exceeds $c\delta$ betwen time $t_0$ and $t_1$ , then time lapse $\delta t_b$ on $b$ 's clock satisfies $ ( t_1-t_0 ) ( 1-\epsilon ) &lt ; \delta t_b &lt ; ( t_1-t_0 ) ( 1+\epsilon ) $ . [ a2 ] pick $\epsilon&gt ; 0$ , use [ a2 ] to get $\delta$ ; use [ a1 ] to break the motion of $o'$ into intervals small enough such that , from the frame of reference of an interial observer travelling between the endpoints of a piece , the velocity of $o'$ never exceeds $c\delta$ ; use [ a2 ] to make $ [ 2 ] $ true within $\epsilon$ . since this works for all $\epsilon&gt ; 0$ , [ 2 ] is simply true . now [ a1 ] might look suspect , since we have used a piecewise inertial observer , whose motion is obviously not smooth ! so we can not even assume anything about what this piecewise inertial observer experiences at the corners ! but that is okay , [ a2 ] only refers to the individual pieces and not the whole . use a family of ( truly ) inertial observers that meet at the appropriate points . as for [ a2 ] , it is a bit opaque , but what it says that if you are not moving too fast relative to an inertial observer , your experience of time is almost the same . this does not follow logically from anything in particular , it is just a physical assumption . but note that special relativity is so hard for many people to accept precisely because [ a2 ] is a fact of life , for reasonably small $\epsilon$ . to make it true for even smaller $\epsilon$ requires more than everyday experience , but it is still " common sense " , and presumably testable to quite small values . now , to believe it literally for arbitrarily small $\epsilon$ requires quite a leap , but do not take differential equations literally . ( added : ) aha ! i found the clock postulate for accelerated observers , and i do believe [ a2 ] is interchangeable with it . and yes , it is often omitted but cannot be derived from other assumptions . it has been tested . ( second addendum ) : even though they are interderivable , mine is better :- ) i have given the accuracy of [ 2 ] directly in terms of the accuracy of [ a2 ] . for example , we do not need the full clock postulate for the twin paradox ( which you mention as a motivating example in a comment ) : the proper acceleration of $o'$ is continuous and its magnitude is bounded by $a_{max}$ . [ a1' ] ( over any finite interval , [ a1 ] does imply [ a1' ] for some value of $a_{max}$ . and [ a1' ] is sufficient for the above argument . ) now , even with mundane accelerations , the twin paradox can produce a sizeable mismatch in ages within a human lifetime . ( besides , if they are not survivable accelerations , the travelling twin 's lifetime ends ! ) so , there is a usable $a_{max}$ for [ a1' ] . and , mundane experience alone proves [ a2 ] up to that $a_{max}$ and down to fairly small $\epsilon$ . so [ 2 ] holds sufficiently accurately to give the twin paradox . we only need special relativity plus a mundane restricted clock postulate . ( i realise you can bypass the whole acceleration question by altering the paradox so that there are three inertial observers who compare clocks as they pass . but then it is not the twin paradox anymore , duh ! )
if you take your lagrangian , including the $a^\alpha a_\alpha$ and vary it with respect to $a^\alpha$ , you will get the classical equation of motion : $\partial_\beta \partial^\beta a^\alpha + \mu^2 a^\alpha = 0 $ . if you use a plane wave as a trial solution for this : $a^\alpha = e^{i p\cdot x} \epsilon^\alpha $ where the $\epsilon^\alpha$ 's are polarization vectors that obey your gauge ( calibration ) condition , you will get : $ ( p^2 -\mu^2 ) e^{i p\cdot x} \epsilon^\alpha= 0 $ . which enforces : $ ( p^2 -\mu^2 ) = 0 $ . expanding out the four momentum we get : $e^2-|\vec{p}|^2-\mu^2 = 0 $ . after rearranging we get : $e^2 =|\vec{p}|^2+\mu^2 $ which is the dispersion relation for a relativistic particle of mass $\mu$ .
spectral geometry is one of the many ways mathematicians think about geometry . the general idea is that if you have some manifold equipped with a metric , you can cook up some canonical differential operators . these operators can be thought of as linear operators , acting on ( infinite-dimensional ) vector spaces of functions , tensors , spinors , and the like . each such linear operator will have a set of eigenvalues . spectral geometry is concerned with relationships between these eigenvalues and the geometry of the manifold you started with the most obvious linear operator to associate to a metric is the laplacian , which is a linear operator on the space of functions on the manifold . in the early/middle part of the 20th century , mathematicians started wondering " if you know the set of eigenvalues of the laplacian , can you reconstruct the manifold ? " , or as mark kac famously put it : " can one hear the shape of a drum ? " the answer is no ; the set of eigenvalues alone does not let you reconstruct the manifold and its metric . the map which sends a manifold with metric to the set of eigenvalues of the laplacian is not invertible . but it is such a pretty idea that people have not given up on it . alain connes , for example , figured out that the answer to a slightly different question is " yes " . if you have a commutative spectral triple ( basically , the dirac operator on a compact spin manifold ) , you can reconstruct the metric from this data . the physicists interviewed in the linked article are trying a slightly different variation on the spectral geometry problem . they are considering systematic finite-dimensional approximations to the " derivative " of the map $f$ which sends a manifold to its set of eigenvalues of its laplacian acting on tensors of low-degree , and trying to show that these approximations are invertible . this should let them show that this map $f$ is invertible in small regions of the space of manifolds . it is a nice idea , and looks like some fun experimental mathematics . trying to write down a theory of gravity in explicitly gauge invariant terms is also a good idea . but i would be quite surprised if this line of thinking bears any fruit . it seems more likely to me that we need some essentially new physical ideas than a clever way of rewriting what we already have .
fundamental particles are identical . if you have two electrons , one from the big bang and the other freshly minted from the lhc , there is no experiment you can do to determine which one is which . and if there was an experiment ( even in principle ) that could distinguish the electrons then they would actually behave differently . electrons tend to the lowest energy state , which is the innermost shell of an atom . if i could get a pen and write names on all of my electrons then they would all fall down into this state . however since we can not tell one electron from another only a single ( well actually two since there are two spins states of an electron ) electron will fit in the lowest energy state , every other electron has to fit in a unique higher energy level . edit : people are making a lot of comments on the above paragraph and what i meant by making electrons distinguishable , so i will give a concrete example : if we have a neutral carbon atom it will have six electrons in orbitals 2s1 2s2 2p2 . muons and tauons are fundamental particles with very similar properties to the electron but different masses . muons are ~200 times more massive than electrons and tauons are ~3477 times more massive than an electron . if we replace two of the electrons with muons and two of the electrons with tauons all of the particles would fall into the lowest energy shell ( which can fit two of each kind because of spin ) . if in theory these particles only differed in mass by 1% or even 0.0000001% they would still be distinguishable and so all fit on the lowest energy level . now atoms are not fundamental particles they are composite , i.e. composed of " smaller " particles , electrons , protons and neutrons . protons and neutrons are themselves composed of quarks . but because of the way that quarks combine , they tend to always be in the lowest energy level so all protons can be considered identical , and similarly with neutrons . to take the example of carbon , there are several different isotopes , different number of neutrons , of carbon ( mostly $^{12}c$ but also ~1% $^{13}c$ and ~0.0000000001% $^{14}c$ {the latter which decays with a half life of ~$5,730$ years [ carbon dating ] but is replaced by reactions with the sun 's rays in the upper atmosphere} ) . if we take two $^{12}c$ atoms , and force all of the spins to be the same . this is not too difficult for the electrons of the atom since the inner electrons do not have a choice of spin because every spin in every level is already full . so only outer electrons matter . the nucleons also have spin . with our two $^{12}c$ atoms with all of the same spins , we now have two indistinguishable particles which if you set up an appropriate experiment ( similar in principle to the electrons not being able to occupy the same state ) we will be able to experimentally prove that these two atoms is indistinguishable . answer time : are atoms unique ? no . do atoms have any uniquely identifying characteristic besides their history ? their history of a particle does not affect it* . no particles are unique . atoms may have isotopes or spin to identify one from another , but these are not unique from another particle with the same properties . would it contain information with which we could positively identify that they two are the same ? yes only because we could positively identify that this carbon atom is the same as almost every other carbon atom in existence . *unless it does , in which case it may be considered a different particle with different properties .
i do not know if this is what you mean to ask , but what i know as a quantum quench is a sudden change in the potential , sufficiently fast that it can be considered instantaneous . in that case the state does not change instantaneously , but obviously its time evolution does : from that point it evolves according to the new hamiltonian . if originally you were in an eigenstate , generally you will be in a superposition of states for the new hamiltonian after the quench .
an engine operating in a cycle can operate continuously i.e. for any number of cycles . if you just pull out one step in the cycle you do not have a useful engine because it can only operate once and then only for a short time . an isothermal process is reversible by definition because temperature is not defined in an irreversible process . so an isothermal process does not increase ( total ) entropy . incidentally this does not violate the second law since the second law says only that entropy cannot decrease . it does not forbid energy staying the same .
one revolution per minute is one sixtieth of a revolution per second . you have it the opposite way .
velocities in general relativity can only be compared at a point , where local tangent planes coincide . talking about the velocities of far-away stars in any sort of absolute sense is an empty question . saying ' the coordinate velocity of andromeda is 10^huge m/s ' is , in a sense , not a statement about physics , but rather about your coordinate system . in order to get a meaningful prediction , you would have to devise an experiment whereby you compare the two velocities--say , andromeda sends the earth a light signal at a preassigned 100 hz . an earth-based observer then measures the redshift for the light signal , and then uses that to decide their relative velocities .
so , what is antimatter ? even from the name it is obviously the " opposite " of ordinary matter , but what does that really mean ? as it happens there are several equally valid ways to describe the difference . however , the one that i think is easiest to explain is that in antimatter , all of the electrical charges on all of the particles , at every level , have been switched around . thus ordinary electrons have negative charges , so their antimatter equivalents have positive charges . protons are positive , so in antimatter they get the negative charges . even neutrons , which have no overall charge , still have internal parts ( quarks ) that very definitely have charges , and those also get flipped around . now to me the most remarkable characteristic of antimatter is not how it is differs from ordinary matter , but how amazingly similar it is to ordinary matter . it is like an almost perfect mirror image of matter -- and i do not use that expression lightly , since it turns out that forcing ordinary matter into becoming its own mirror image is one of those other routes i mentioned for explaining what antimatter is ! the similarity is so close that large quantities antimatter would , for example , possess the same chemistry as ordinary matter . for that matter there is no reason why an entire living person could not be composed of antimatter . but if you do happen to meet such a person , such as while floating outside a space ship above earth , i strongly recommend that you be highly antisocial . do not shake hands or invite them over , whatever you do ! the reason has to do with those charges , along with some related factors . everyone knows that opposite charges attract . thus in ordinary matter , electrons seek out the close company of protons . they like to hang out there , forming hydrogen . however , in ordinary matter it also turns out that there are also all sorts of barriers -- i like to think of them as unpaid debts to a very strict bank -- that keep the negative charges of electrons from getting too close to the positive charges of the protons . thus while the oppositely charged electrons and protons could in principle merge together and form some new entity without any charge , what really happens is a lot more complicated . except for their opposite charges , electrons do not have the right " debts " to pay off everything the protons " owe , " and vice-versa . it is like mixing positive apples with negative oranges . the debts , which are really called conservation laws , make it possible for the powerfully attracted protons and electrons to get very close , but never close enough to fully cancel out each other 's charges . that is a really good thing , too . without that close-but-not-quite-there mixing of apples and oranges , all the fantastic complexity and specificity of atoms and chemistry and biochemistry and dna and proteins and us would not be here ! now let 's look at antimatter again . the electrons in antimatter are positively charged -- in fact , they were renamed " positrons " a long time ago -- so like protons , they too are strongly attracted to the electrons found in ordinary matter . however , when you add electrons to positrons , you are now mixing positive apples with negative apples . that very similarity turns out to result in a very dangerous mix , one not at all like mixing electrons and protons . that is because for electrons and positrons the various debts they contain match up exactly , and are also exactly opposite . this means they can cancel each other 's debts all the way down to their simplest and most absolute shared quantity , which is pure energy . that energy is given off in the form of a very dangerous and high-intensity version of light called gamma rays . so why do electrons and positrons behave so very badly when they get together ? here 's a simple analogy : hold a rubber band tightly at its two ends . next , place an aaa between the strands in the middle . ( this is easier for people with three arms . ) next , use the battery to wind up the rubber band until it is quite tight . now look at the result carefully . notice in particular that the left and right sides are twisted in opposite directions , and in fact are roughly mirror images of each other . these two oppositely twisted sides of the rubber band provides a simple analog to an electron and a positron , in the sense that both store energy and both have a sort of defining " twistiness " that is associated with that energy . you could easily take the analogy a bit farther by bracing each half somehow and snipping the rubber band in the middle . with that more elaborate analogy the two " particles " could potentially wander off on their own . for now , however , just release the battery and watch what happens . ( important : wear eye goggles if you really do try this ! ) since your two mirror-image " particles " on either side of battery have exactly opposite twists , they unravel each other very quickly , with a release of energy that may send the battery flying off somewhere . the twistiness that defined both of the " particles " is at the same time completely destroyed , leaving only a bland and twist-free rubber band . it is of course a huge simplification , but if you think of electrons and positrons as similar to the two sides of a twisted rubber band , you end up with a surprisingly good feel for why matter and antimatter are dangerous when placed close together . like the sides of the rubber band , both electrons and positrons store energy , are mirror images of each other , and " unravel " each other if allowed to touch , releasing their stored energy . if you could mix large quantities of both , the result would be an unraveling whose accompanying release of energy would be truly amazing ( and very likely fatal ! ) to behold . now , given all of that , how " real " is antimatter ? very , very real . its signatures are everywhere ! this is especially true for the positron ( antimatter electron ) , which is the easiest form of antimatter to create . for example , have you ever heard of a medical procedures called a pet scan ? pet stands for positron emission tomography . . . and yes , that really does mean that doctors use extremely tiny amounts of antimatter to annihilate bits of someone 's body . the antimatter in that case is generated by certain radioactive processes , and the bursts of radiation ( those gamma rays ) released by axing a few electrons help see the doctors see what is going on inside someone 's body . signatures of positrons are also remarkably common in astrophysics , where for example some black holes are unusually good at producing them . no one really understands why certain regions produce so many positrons , unless someone has has some good insights recently . positrons were the first form of antimatter predicted , by a very sharp fellow named paul dirac . not too long after that prediction , they were also the first form of antimatter detected . heavier antimatter particles such as antiprotons are much harder to make than positrons , but they too have been created and studied in huge numbers using particle colliders . despite all of that , there is also a great mystery regarding antimatter . the mystery is this : where did the rest of the antimatter go ? recall those debts i mentioned ? well , when creating universes physicists , like other notable entities , like to start the whole shebang off with pure energy -- that is to say , with light . but since matter has all those unbalanced debts , the only way you can move smoothly back and forth between light and matter is by having an equal quantity of antimatter somewhere in the universe . an amount of antimatter that large flat-out does not seem to exist , anywhere . astrophysicists have by now mapped out the universe well enough to leave no easy hiding places for large quantities of antimatter . recall how i said antimatter is very much like a mirror image of matter ? that is an example of a symmetry . a symmetry in physics is just a way of " turning " or " reflecting " or " moving " something in a way that leaves you with something that looks just like the original . flipping a cube between its various sides is a good example of a " cubic symmetry , " for example ( there are fancier words for it , but they mean the same thing ) . symmetries are a very big deal in modern physics , and are absolutely critical to many of our deepest understandings of how our universe works . so matter and antimatter form an almost exact symmetry . however , that symmetry is broken rather spectacularly in astrophysics , and also much more subtly in certain physics experiments . exactly how this symmetry can be broken so badly at the universe level while being only very subtly broken at the particle level really is quite a bit of a mystery . so , there you have it , a mini-tutorial on both what antimatter is and where it occurs . while it is a bit of overkill , your question is a good one on a fascinating topic . and if you have read through all of this , and have found any of what i just said interesting , do not just stop here ! physics is one of those topics that gets more fascinating as you dig deeper you get into it . for example , some of those cryptic-looking equations you will see in many of the answers here are also arguably some of the most beautiful objects ever uncovered in human history . learning to read them well enough to appreciate their beauty is like learning to read great poetry in another language , or how to " hear " the deep structure of a really good piece of classical music . for physics , the reward is a deep revelation of structure , beauty , and insight that few other disciplines can offer . do not stop here !
unfortunately for you the authority says no . compound prefix symbols , that is , prefix symbols formed by the juxtaposition of two or more prefix symbols , are not permitted . this rule also applies to compound prefix names . source however , you might use $145\times10^3\text{ mpa}$ .
if you have watched any of the popular science programmes on the higgs boson you have almost certainly got the wrong idea about it . in particular , the title to your question suggests you have heard the description " god particle " once too often since the higgs is not the root of all elementary particles . physicists believe that particles acquire mass through a process called electroweak symmetry breaking . i am not sure that this has been proven , but the theory fits observations so well that everybody believes it . anyhow , the higgs mechanism is one of many mechanisms by which the symmetry breaking could occur . actually it is the simplest and most elegant mechanism , and all the other suggestions have various problems associated with them , so most people 's money is on the higgs , especially now there is a hint of it at the lhc . but there are various other possible mechanisms including a composite higgs theory known as the little higgs , and it is possible that experiment could prove the little higgs theory to be correct . alternatively another symmetry breaking mechanism like technicolor , that does not have a higgs boson at all , could turn out to be correct . so while it currently seems most likely that the simple higgs boson model is correct , the higgs might not be a fundamental particle or might not exist at all . at the end of the day electroweak symmetry gets broken and particles acquire masses , so all the alternatives to the higgs have to do basically the same thing . failing to find a simple higgs boson would not be the end of physics .
the criterion you mention is roughly the threshold for the formation of the coulomb gap in the hubbard model or the local moment in the anderson model . it is a common break-down region for many approaches starting from one of the limits ( insulator/local moments versus conductor/mixed valence ) . for perturbation theory in $u$ , see the prb 36 , 675 ( 1986 ) by horvatić et al . and references to and form that paper . a more comprehensive discussion can be found in the monograph by hewson . as far as i remember , perturbation in $u$ on the level of self-energy does not give the expected exponential dependence on $u$ for the kondo temperature . unfortunately , i do not know specifics of flex method to help you in more detail .
check your algebra in the part where you need to compare $ ( \partial b ) c$ between the two normal orderings , i got $\lambda ( \lambda-1 ) / ( 2z^2 ) $ for the difference . the result follows immediately . ( by the way , thanks for the symbol ${}^{{}_\circ}_{{}^\circ}$ , i have been looking for that forever . )
from a purely algebraic standpoint , you could never have a state which is perfectly correlated with respect to every basis , simply because it is incompatible with the probabilistic formulation of quantum mechanics . specifically , if you accept the current linear formulation of quantum mechanics , any two-qubit state which had perfect positively correlated measurement outcomes for each basis would have a density operator whose eigendecomposition includes negative probabilities . so , you would either have to explain what negative probabilities correspond to , or you must abandon the linear structure of quantum mechanics . either alternative is likely to raise people 's hackles , and give rise to exotic phenomena such as superluminal signalling . the proof is not difficult , if you are comfortable with pauli ( spin ) operators . on two-qubit states , the fact that the singlet is anticorrelated in all bases is related to the fact that $$ \rho \ ; :=\ ; |\psi^-\rangle\langle\psi^-| \ ; \ ; =\ ; \ ; \tfrac{1}{4}\bigl ( \mathbf 1 \otimes \mathbf 1 \ ; -\ ; x \otimes x \ ; -\ ; y \otimes y \ ; -\ ; z \otimes z\bigr ) $$ which indicates at least that the outcomes of x , y , and z observable measurements will be anticorrelated : for each observable $\mathcal o \in \{x \otimes x , \ ; y \otimes y , \ ; z \otimes z\}$ , we have $\mathop{\mathrm{tr}} ( \mathcal o \rho ) = -1$ . this is the maximum negative value such an expectation value may have by virtue of the fact that $\rho$ has eigenvalues bounded between zero and one , and that each observable $x \otimes x$ , &nbsp ; $y \otimes y$ , and $z \otimes z$ have eigenvalues $\pm 1$ . this also means that the anticorrelation is the greatest possible ; that is , the outcomes of the measurements will be opposite with certainty . suppose instead that you wanted a hermitian operator $\varrho$ , whose eigenvalues have magnitude at most one and which sum to exactly one , representing a state which was perfectly positively correlated in each basis . in particular , such an operator would have to satisfy $\mathop{\mathrm{tr}} ( \mathcal o \varrho ) = +1$ for each $\mathcal o \in \{x \otimes x , \ ; y \otimes y , \ ; z \otimes z\}$ , so we could decompose it as $$ \varrho \ ; \ ; =\ ; \ ; \tfrac{1}{4}\bigl ( \mathbf 1 \otimes \mathbf 1 \ ; +\ ; x \otimes x \ ; +\ ; y \otimes y \ ; +\ ; z \otimes z \ ; +\ ; r\bigr ) , $$ where $r$ is some operator consisting of a linear combination of distinct pauli operators $p \otimes q$ for $p \ne q$ ( taking advantage of the fact that we may use the pauli operators as a basis with real coefficients , for hermitian operators on two qubit state vectors ) . in particular , the operators $ ( \mathbf 1 \otimes \mathbf 1 ) r$ , $ ( x \otimes x ) r$ , $ ( y \otimes y ) r$ , and $ ( z \otimes z ) r$ have zero trace . because $\varrho^2$ has trace bounded above by one , and as $\mathbf 1 \otimes \mathbf 1$ is the only two-qubit pauli operator with non-zero trace , we may expand $\varrho^2$ to obtain $$\begin{align*} 1 \ ; and \geqslant\ ; \mathop{\mathrm{tr}} ( \varrho^2 ) \ ; =\ ; \cdots \\ and = \tfrac{1}{16}\bigl ( \mathop{\mathrm{tr}}\bigl ( ( \mathbf 1 \otimes \mathbf 1 ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( ( x \otimes x ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( ( y \otimes y ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( ( z \otimes z ) ^2\bigr ) + \mathop{\mathrm{tr}}\bigl ( r^2\bigr ) \bigr ) \\ and = \tfrac{1}{16}\bigl ( 4 + 4 + 4 + 4 + \mathop{\mathrm{tr}} ( r^2 ) \bigr ) . \end{align*}$$ then $r^2$ must be traceless ; but as the square of a hermitian operator , it can only be traceless if all of the eigenvalues of $r$ were zero , that is if $r = 0$ . thus it follows that $$ \varrho \ ; \ ; =\ ; \ ; \tfrac{1}{4}\bigl ( \mathbf 1 \otimes \mathbf 1 \ ; +\ ; x \otimes x \ ; +\ ; y \otimes y \ ; +\ ; z \otimes z\bigr ) \ ; =\ ; \tfrac{1}{2} ( \mathbf 1 \otimes \mathbf 1 ) - \rho , $$ where $\rho$ again is the density operator for the singlet state which we wrote above . because the bell states form an eigenbasis for both $\rho$ and $\mathbf 1 \otimes \mathbf 1$ , they form an eigenbasis for $\varrho$ as well , yielding the following eigen-decomposition of $\varrho$: $$ \varrho \ ; =\ ; \tfrac{1}{2}|\phi^+\rangle\langle\phi^+| \ ; +\ ; \tfrac{1}{2}|\phi^-\rangle\langle\phi^-| \ ; +\ ; \tfrac{1}{2}|\psi^+\rangle\langle\psi^+| \ ; -\ ; \tfrac{1}{2}|\psi^-\rangle\langle\psi^-| . $$ this is not a positive operator , and thus not a density operator . in particular , if you performed a bell measurement , there is not any well-defined meaning to the probability distribution you would obtain , if you supposed that you could have a system in a state described by $\varrho$ .
on-shell dof : consider the constraints of " equation of motion " plus " gauge condition" ( lorenz gauge , coulomb gauge , . . . ) , #dof=4-2 . off-shell dof : consider the constraints of " gauge condition " ( lorentz gauge , coulomb gauge , . . . ) only , #dof=4-1 .
the sound by the cable is produced because of the kármán vortex shedding . this empirical formula from the wikipedia page relates the frequency of the vortex shedding with the reynolds number : $$ \frac{fd}{v}=0.198\left ( 1-\frac{19.7}{\mathrm{re}}\right ) , $$ where $f$ is the frequency , $d$ cable diameter and $v$ is the flow velocity . the reynolds number $\mathrm{re}$ in turn is defined for this system as $$\mathrm{re}=\frac{vd}{\nu} , $$ where $\nu$ is the kinematic viscosity of the medium . for the air at $15\ , {}^\circ \text{c}$ it is $1.48\times 10^{−5}\ , \text{m}^2/\text{s}$ . so solving the equations for the velocity $v$ we obtain $$ v = 5.05 \left ( f d + 3.90 \frac{\nu}{d}\right ) . $$ of course , this formula implies idealized conditions , so in a more realistic situations ( including for instance turbulence in the wind flow ) extracting the vortex shedding frequency from the sound spectrum could be tricky .
in your post when you say you ' know ' the position and momentum of a single photon you really do not know anything , you are just making a prediction , not making a measurement . in your head you are basically assuming classical physics and using the initial parameters of the system to calculate the final parameters . in order to actually know any properties about a system you will have to perform a measurement , and to really say anything conclusive you will have to do this many times . take your single photon source and measure the momentum and position of the outgoing photons numerous times - the product of the standard deviation in momentum and position will be greater than $\frac{\hbar}{2}$ .
the diagram looks like the fat man bomb that was dropped on nagasaki . the wikipedia article gives lots of info on the design if you are intereted in pursuing it further . the casing is just to make it aerodynamically stable so it fell in a controlled way . the bomb itself is spherical so the case could be spherical as well if it were not for aerodynamic requirements .
if the magnetic dipoles in a material are ordered , the material has a lower entropy because there are many fewer ways how the spins may be oriented if most of them ( or all of them ! ) are required to be aligned . such an alignment also reduces the heat capacity because before the dipoles got aligned , the orientation ( direction ) of each dipole was a degree of freedom that was storing something like $o ( kt ) $ of energy where $k$ is boltzmann 's constant and $t$ is the temperature in kelvins . however , when the dipoles are kept aligned , the direction is no longer variable so the heat capacity from each dipole decreases by $o ( k ) $ or so , or $o ( r ) $ if expressed per mole . it is a similar difference as the heat capacity of monoatomic vs diatomic gas , which carry $3kt/2$ vs $5kt/2$ per molecule , respectively . ( $3/2$ is from 3 linear momenta and the extra $2/2$ is from the longitude and latitude remembering the rotational motion and/or direction of the diatomic molecule . ) note that the diatomic gas has a greater energy per molecule which scales with $t$ , and therefore also steeper dependence of the energy on the temperature ( the capacity is $5r/2$ ) because the energy may also be stored in the random rotations of the diatomic molecule that do not exist for the monoatomic molecule . the random chaotic thermal rotation of the dipoles is analogous to the random rotation of the diatomic gas molecules and it becomes banned or indistinguishable for monoatomic gas as well as the magnetic material with aligned spins which is why the heat capacity decreases analogously to the decrease from $5r/2$ to $3r/2$ in the transition from a diatomic molecule to a monoatomic one .
if the moon were exactly the same as the earth , then sure , there is no major reason to suspect it would be any different . it is in the same orbit around the sun as us , so it gets heated by the same amount . this would place it in the habitable zone . however , habitability is not the same as being in the habitable zone , and the detailed answer depends on how you make the surface gravity match that of earth . the surface gravity of a sphere of radius $r$ and average density $\rho$ is $$ g = \frac{4\pi}{3} g \rho r . $$ most rocky bodies in the solar system have about the same density - that of a rock - so making the moon 's gravity match the earth 's is just a matter of making it bigger . essentially it would become earth 's twin in every way . on the other hand , maybe you intended to keep the size the same . in that case you would have to increase the density . it is not clear what you would make the interior out of , but it is pretty certain you will not get the same geology as on earth . for one , smaller bodies cool off too fast to be geologically active at this age ( roughly 5 billion years ) . you see , when the planets condensed out of the gas and dust swirling around the sun billions of years ago , they were hot - gravitational potential energy went down , and so thermal energy went up . their heat capacity is proportional to their volume , but their heat losses are proportional to their surface areas . thus objects with high surface area-to-volume ratios ( i.e. . small things ) cool quickly . the thing is , earth 's geologic activity probably had a large role in building up and maintaining the atmosphere and oceans we know and love . in either case , there is also the problem of tidal locking . it is suspected by some that having tides was crucial for the development of life . the moon is already tidally locked with the earth - we only ever see one face of it - so it has no tides . if you scaled it up , you might tidally lock the earth as well . the moon would essentially be in a geostationary orbit , and we would not have tides . this is the case for the pluto-charon system , for instance .
it is a stupid mistake . when r is less than the radius of the cylinder , the electric field will be 0 . i solved the problem not taking this into account and just plugged in the numbers . my method is perfectly valid for any r greater than 4cm .
no . it does not measure time of flight . kinect is , deep down , a structured light scanner , meaning that it projects an infrared pattern ( so invisible for us ) . according to the underlying technology firm primesense , the structured light code is drawn with an infrared laser . this pattern is then read by an infrared camera and the 3d information is reconstructed from the distortions of the pattern . this results in a depth channel which is made available through usb . if you want to see the pattern , you may have some luck by turning off all the lights in the room , turn on the kinect , and try to use your cellphone camera . generally , these camera sensors are sensitive to ir , which appears as green . you may verify if this is the case by trying the same with a tv remote and pressing the buttons . the led should turn green .
the process used in this kind of source is the spontaneous parametric down conversion ( spdc , see , e.g. wikipedia for details ) . it is a nonlinear optical process in which from a photon with angular frequency $\omega_0$ you get two photons with frequencies $\omega_1$ , $\omega_2 = \omega_0-\omega_1$ . these photons are then phase matched and have correlated polarization ( either the same or opposite , we speak of type i spdc and type ii spdc , respectively ) .
the formula is just a force balance . if the contact line is stationary the forces at it must balance so taking the horizontal component of the forces gives you : $$\sigma_{gas-liquid} cos \theta + \sigma_{liquid-solid} = \sigma_{gas-solid}$$ and hence the formula you quote . if you replace the gas with a liquid the force balance calculation is just the same , so the formula remains valid .
if you mean special conformal transformation x-> 1/x conformal invariance of maxwell equations is known since 1909 . see here : http://cts.iisc.ernet.in/personnel/pages/asinha/draft1shouvik.pdf or here : http://arxiv.org/pdf/hep-th/9701064
good question . yes , every photon at the lasing wavelength will potentially eat some of the excited state population through stimulated emission - regardless of the photon 's origin . this appears in the rate equations governing the excited state populations and photon number as loss factors . i believe the key here is that the positive feedback due to the optical resonator will consume the excited state population faster than the random photons will , leaving essentially no gain for them to use . note that the nd:yag rod will always fluoresce some part of its energy in every direction as 1064nm photons . having a filter to weed out any stray 1064 nm photons would theoretically lower the losses , but an absorptive filter will most likely only contribute to the amount of heat that needs to be taken away from the system , which in the case of a flash-lamp pumped nd:yag is already quite a lot . there is a good reason why diode pumping is more favorable than flash-lamp pumping ! principles of lasers by orazio svelto is in my opinion one of the best text books on lasers for additional information .
it is the " latent heat of fusion " produced when water freezes that protects the crop , see p . 3 here : http://fruit.wisc.edu/wp-content/uploads/2011/02/understanding-frost-in-fruit-crops1.pdf
oozing honey through pipes the solution below is for a very viscous fluid which has negligible inertia and large viscosity . it is wrong for water in real pipes , because it neglects the pressure drop which comes with the changing velocity of water . this term is higher order in v , but it is obviously relevant for real water pipers . i leave it , because it is an interesting exercise with a direct analogy to resistive current flow , the correct solution is at the end . the way to do this is to note that the pressure at the divergence point is equal for all 4 pipes , and that there is a given law for pressure drop along a pipe per unit length at any a given flow rate . the answer is different depending on whether you have a fixed pressure forcing the water through the pipes ( as you do in a water main system ) or whether you are forcing a given volume of water through per unit time , as you suggest , and which is appropriate when you have a large pressure drop along a very long pipe before you get to your splitter . i will assume that the 4 pipes have a given length , and that they empty at atmospheric pressure , which i will label as 0 , and that the water flow is sufficient to keep the pipes filled until near the exit point , otherwise the problem requires more information . consider the fixed flow rate problem first . if the imposed flow rate is f units of water per second , the first equation is the mass conservation equation $$\sum_i f_i = f $$ where $f_i$ are the flow rates along the pipes . phill . zitt gave this formula , but it is not enough--- it is analogous to the current kirchhoff law . you also need the analog of the voltage kirchhoff law . the voltage law tells you that the flow rate $f_i$ is proportional to the pressure drop along pipe i . i will call the proportionality constant the " flow conductance " $c_i$ ( it is the analog of the reciprocal of resistance in an electrical circuit ) : $$ f_i = c_i \delta p $$ for the four pipes , $\delta p$ is equal , so that $$ f_i \propto c_i $$ and along with the sum rule , you find : $$ f_i = {c_i f \over \sum_i c_i } $$ so the only thing you need to know are the $c_i$ , just as in a resistor network . two pipes with flow conductances $c_1 , c_2$ connected in series have a flow conductance c given by the formula : $$ {1\over c} = {1\over c_1} + {1\over c_2}$$ for the same two pipes in parallel , $$ c = c_1 + c_2 $$ so that conductances add in series and parallel just like the reciprocal of the resistance ( the electrical conductance ) in circuits . you have a problem of 4 parallel resistors connected in series to an input resistor , just like a resistor connected to 4 resistors in parallel . for a cylindrical pipe of length l and radius r , the laminar flow profile is exactly parabolic in the radial cylindrical coordinate r : $$ v ( r ) = v ( 1 - {r^2\over r^2} ) $$ so that the total flow as a function of r is $$ f ( r ) = \int_0^r v ( r ) 2\pi r dr = {\pi v r^2\over 2}$$ the navier stokes equations reduce to something very simple in the laminar pipe flow case--- all the terms drop out except the viscosity term , which tells you the diffusion of momentum out of the pipe , and so the pressure drop per unit length . ( see here : is there an analytical solution for fluid flow in a square duct ? ) the equation is $$ \nu \nabla^2 v = \delta p $$ so that $$ 2\nu {v\over r^2} = {\delta p \over l} $$ this gives you the flow rate as a function of r and l , $$ f = {\pi v r^2 \over 4} = {\pi r^4\over 8\nu l} \delta p$$ so that the conductance is $$ c ( r , l ) = {\pi r^4 \over 8 \nu l} $$ and this determines the flow through the i'th pipe in terms of the total flow and the geometry : $$ f_i = {f {r_i^4\over l_i} \over \sum_k {r_k^4\over l_k}} $$ this solves the constant flow-rate problem purely geometrically . the limit of constant flow rate is achieved when there is a long pipe feeding into the whole thing with a much larger pressure drop than the pressure drop after the split . the total flow is determined by the total conductance , which is essentially equal to the conductance of the long pipe , so no matter what you attach at the end , so long as the part at the end has much more conductance than the initial pipe . the same problem can be solved at a fixed pressure at the divergence point , the outgoing flow is just the conductance times the shared pressure . for question 2 , the issue of constant pressure or constant flow rate is essential . at constant pressure , if you attach the contraption to the side of a wide water main at high pressure , closing one pipe does nothing to the flow in the other pipes . at constant flow rate , closing pipe number 4 increases the flow through the other 3 by the factor $$ c_1 + c_2 + c_3 + c_4 \over c_1 + c_2 + c_3 $$ for non-rigid pipes , you just need to know the r as a function of the pressure . this will be a fine approximation if the pressure drops are slow in the pipe as usual , so that the radius change slowly with length . in normal pipes , the radius does not change hardly at all with the pressure , so i did not bother to calculate anything , but you can split up the pipe into slices with a radius r ( p ) , giving a conductance , which you add according to the series rule . water in pipes i will assume the flow is laminar in the pipes , but that the pipes are short , so that the pressure drop due to viscosity is negligible between the two ends . this is the correct limit for water pipes . the pressure does work on the water which is not dissipated significantly in the pipes , and comes out as kinetic energy in the water , not as heat in the pipe . given a pressure drop from p to atmospheric pressure 0 , the water in each of the four pipes will adjust it is velocity so that the bernoulli principle is obeyed--- the work done by the pressure is the energy gained by the water . the energy flow in a cross section of the pipe is : $$ \int {\rho v ( r ) ^2\over 2}v ( r ) 2\pi r dr $$ with the laminar profile ( the flow f is as before ) , and this gives $$ f {\rho v^2\over 4} $$ where v is the velocity at the center , as before . the work done by the pressure difference at the two ends is $pf$ , so you get a version of bernoulli 's equation for laminar pipes : $$ p + {\rho v^2\over 4} = {\rho v_0^2\over 2}$$ the velocity in the pipes are then $$ v= \sqrt{{4p\over \rho} + {v_0^2\over 2}} $$ and they are equal . so that the flow rate in this limit ( the right limit for water ) is proprotional to the cross section area of the pipe , to r^2 . if you have a fixed flow rate , the pressure rises to the point where the total outflow is equal to the inflow , and the water flow is partitioned according to the cross section area : $$ f_i = {f r_i^2\over \sum_k r_k^2 }$$ this neglects the incoming velocity $v_0$ , assuming the water coming out is significantly faster than the water coming in . the answer for 2 and 3 is not changed in the water case compared to the honey .
i will once again state that string theory , any theory , cannot be proven right by any experiment . the experiment might validate the theory , i.e. come as a result of a prediction from the theory . at the moment there does not exist one string theory in the manner that there exists one general relativity theory . there are many models based on string theory , though . why such an interest ? because at the moment string theories are the only theories that can accommodate the standard model of particle physics and at the same time allow for the quantization of gravity , which has been the holy grail of theoriticians the past fifty years . that is they promise a " theory of everything , toe ) . what might disprove the usefulness of string theories for a toe would be if supesymmetry were falsified at the lhc , for example . if nothing is seen other than the higgs at the lhc , ss would seem as a nice try but bad luck . then the usefulness of strings becomes doubtful . if ss is seen in the lhc and studied as well as the sm in the international linear collider to be built in the future , then strings will be good as candidates of a toe .
0 . caveat lector : this was done before i drank my morning coffee , so there may be some errors in the reasoning ( well , the physical reasoning , the mathematics should be kosher ) . 1 . perfect fluid . so we have two stress-energy tensors here . one is the stress energy tensor for a perfect fluid $$\tag{1}t^{\alpha\beta}_{\text{fluid}} = \rho \ , u^\alpha \ , u^\beta + p \ , h^{\alpha\beta}$$ where we have the worldlines of the fluid 's particles have velocity $u^\alpha$ the projection tensor $h_{\alpha\beta} = g_{\alpha\beta} + u_\alpha \ , u_\beta$ projects other tensors onto hyperplane elements orthogonal to $u^\alpha$ the matter density is given by the scalar function $\rho$ , the pressure is given by the scalar function $p$ . we had need extra terms if there were heat flow or shear involved . 2 . scalar field . now , we have another distinct stress-energy tensor for a massless scalar field : $$\tag{2}t^{\mu \nu}_{\text{scalar}} =\partial^{\mu}\phi\ , \partial^{\nu}\phi-\frac{1}{2}g^{\mu \nu}\partial_{\rho}\phi\ , \partial^{\rho}\phi$$ we would use this equation when modeling , e.g. , massless pions ( or some other massless spin-0 field ) . 3 . problem : are these two related ? now if we take our matter density to be , in the appropriate units , $$\tag{3a} \rho = 1 + \frac{1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ and the pressure $$\tag{3b} p = \frac{-1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ then ( 2 ) resembles ( 1 ) . this is after pretending $\partial^{\mu}\phi=u^{\mu}$ , which terrifies the original poster ( but that is what condensed matter physicists do , so i suppose i could end here content ) . is this kosher ? we should first note if we wanted to take the derivative of some function along the worldline $x^{\mu} ( s ) $ with respect to the " proper time " ( length ) $s$ we have $$\tag{4} \frac{\mathrm{d}f}{\mathrm{d}s}=\frac{\mathrm{d}x^{\mu}}{\mathrm{d}s}\frac{\partial f}{\partial x^{\mu}}$$ by the chain rule . for general relativity , we use the " comma-goes-to-semicolon " rule , but for a scalar quantity $f$ we have $$ \nabla_{\mu}f = \partial_{\mu}f . $$ ( if this is not obvious , the reader should consider it an exercise to prove it to him or herself . ) the punchline : identifying $\partial^{\mu}\phi=u^{\mu}$ is kosher . how ? observe in equation ( 4 ) the guy in front , the $\mathrm{d}x^{\mu}/\mathrm{d}s$ is just some vector . so in the very , very special case that equations ( 3a ) and ( 3b ) hold , and $\mathrm{d}x^{\mu}/\mathrm{d}s= ( 1,0,0,0 ) $ , we see that we can indeed recover the first stress-energy tensor as a special case of the scalar field 's stress-energy tensor .
your equation is almost correct . updated using x-axis along ab taking the x-axis along ab yields $$ 50 t \sin ( \theta-15^\circ ) = 26 t \sin ( 40^\circ ) $$ $$ \sin ( \theta-15^\circ ) = 0.52 \sin ( 40^\circ ) $$ $$ \theta = 34.527^\circ $$ $$ \cos ( \theta-15^\circ ) = \sqrt{1-\sin^2 ( \theta-15^\circ ) } $$ and the y-axis perpendicular to ab $$ 50 t \cos ( \theta-15^\circ ) = 20 + 26 t \cos ( 40^\circ ) $$ $$ t = 0.735 $$ using x-axis along ac ( interception pt ) taking y-axis perpendicular to ac $$ 50 t = 20 \cos ( \theta-15^\circ ) +26 t \cos ( 55^\circ-\theta ) $$ $$ t = \frac{20 \cos ( \theta-15^\circ ) }{50 - 26 \cos ( 55^\circ-\theta ) } $$ taking x-axis along ac $$ 20 \sin ( \theta-15^\circ ) =26 t \sin ( 55^\circ-\theta ) $$ which when expanded you need to solve an equation of the form $$a\cos \theta + b \sin \theta = c$$ for $\theta$ with the same results as above .
you are using standard si units for all the other terms ( no unit multipliers ) . if you look at the specific heat of water ( at standard atmospheric pressure ) you will find the specific heat is $\approx 4.2 \mathrm{kj\ , kg^{-1}\ , k^{-1}}$ or $\approx 4200 \mathrm{j\ , kg^{-1}\ , k^{-1}}$ . it is the latter you want to use . i hope this helps .
the localization or delocalization of the excess charge in conductors and insulators can be understood in a way similar to the uncharged case using band theory . please refer to the first diagram in : http://en.wikipedia.org/wiki/work_function for simplicity let us consider the zero temperature case . the fermi energy $e_f$ can be thought of as a level which separates the occupied states from the empty states . now , as you may know , the fermi energy in a metal would lie in a band . in other words , in metals you have half filled bands ; this is why electrons , near the fermi energy , are delocalized in metals . in semiconductors , however , the fermi energy lies within the band ( as shown in the figure ) leaving all the bands either completely filled or empty . as a result , the electrons are localized . since the fermi energy separates the filled and empty states , it can , intuitively , be thought of as the surface of a fluid in a container ; the fluid is analogous to the electrons in the system . now , as you add or remove the fluid , its surface will either rise or drop respectively . changes in the fermi energy can be visualized in the same way . there is , however , one caveat : the vacuum energy $e_{vac}$ will also change , in addition to $e_f$ , as excess charge is introduced . $e_{vac}$ is considered as the energy at which the electron is no longer bound to the solid . if the solid is charged positively or negatively , it will be harder or easier for the electron to escape respectively . as a result , one only considers changes in work function $\phi$ or electron affinity $e_{ea}$ . the former is often used in the case of metals and the latter in the case of semiconductors or insulators . to sum it all up , excess charges will result in changes in $\phi$ and $e_{ea}$ . after these changes have occurred , by the introduction of access charges , it is a question of where the fermi energy sits . depending on that , the electrons will either be localized or delocalized . for a reasonable value of excess charge the fermi energy will only move by a small amount , and will still typically lie in one of the bands or in the band gap in a metal or insulator respectively . this is why the excess charge will be delocalized , and cover the entire surface of the metal ; whereas the excess charge will be localized in an insulator . if you want to get a better feel for how $e_f$ , $e_{vac}$ , $\phi$ , and $e_{ea}$ change as excess charge develops on metals , insulators , and semiconductors , you can take a look at chapter 2 of : http://www.amazon.com/field-effect-devices-volume-edition/dp/0201122987/ref=sr_1_1?ie=utf8qid=1348762717sr=8-1keywords=field+effect+devices
i am not sure how far back you want to go , but one of the earliest " careful " treatments ( in the hamiltonian formalism ) is in the paper by regge and teitelboim : t . regge and c . teitelboim , “role of surface integrals in the hamiltonian formulation of general relativity , ” annals phys . 88 , 286 ( 1974 ) analogous work in the lagrangian formalism did not happen until much more recently . see the paper by mann and marolf , or the treatment in this relevant but shameless self-promotion : mann , marolf , mcnees , and virmani , " on the stress tensor for asymptotically flat gravity , " ( http://arxiv.org/abs/0804.2079 ) as far as calculations specific to aads spacetimes there is the paper by skenderis and papadimitriou papadimitriou and skenderis , " thermodynamics of asymptotically locally ads spacetimes , " ( http://arxiv.org/abs/hep-th/0505190 ) which attempts to formulate things in a language similar to the wald paper referenced by moshe . all of these papers are concerned with the consistent treatment of the variational problem on spacetimes with a spatial infinity . the references of the last two will turn up additional relevant works . you should immediately discount any paper which claims that the inclusion of the gibbons-hawking-york surface term gives a good variational problem for a gravitational theory . this topic -- the proper variational formulation of gravitational theories -- is one of my main interests . i am trying to take the red line from loyola down to u of c for seminars this term ( as teaching allows ) , so perhaps we can talk about it some time .
the morally correct answer is that the measurement of one spin in the epr-entangled pair eliminates the entanglement as it picks a particular factorized basis vector for the measured spin , and the total wave function therefore has to factorize to $\psi_\text{just measured}\otimes \psi_{\rm something}$ . if you parameterize the multi-fermion states in terms of " fermion 1" and " fermion 2" , you will have to antisymmetrize , so no multi-particle wave function will ever tensor factorize . ( this is true even for bosons with the exception of the states where all the bosons are placed to the same one-particle state . ) however , as you say , this obstacle to factorization ( in the form of the required antisymmetrization , and similarly symmetrization for bosons ) is sort of " trivial " . there is a technical way to support the claim that this state is really not entangled . if you write the " subsystems " not as " fermion 1" and " fermion 2" but as " region around $r_a$" and " region around $r_b$" , using your notation , then you will see that the wave function ( al ) for the whole space is almost accurately tensor factorized to the wave functional that only depends on the fields near $r_a$ ( where a spin-down electron excitation is added ) and the fields around the point $r_b$ ( where the spin-up electron excitation lives ) . $$ \psi_{qft} = c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) \otimes c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) $$ of course , the formula above is just a suggestive way to show that the ordinary , correct state of a quantum field theory $$ c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) |0\rangle $$ is really a simple tensor product of a sort . for this reason , the notion of " entanglement " is usually modified for identical particles so that the unentangled states are not just the states having the form of the simple tensor product but all states obtained as ( anti ) symmetrization of a tensor product .
thermodynamics is a phenomenological description of macroscopic systems , and it is laws are based on empirical observations . the first law , first states that a state function called internal energy $u$ exists for macroscopic systems ( an experimental fact ) , that can be thought of as the analog of potential energy in mechanics , for macroscopic systems ; then defines heat intake of the system : 1 ) for an adiabatically isolated macroscopic system ( i.e. . , when the only sources of energy are mechanical ) , the amount of work required to change the state of the system only depends on the initial and final states . ( an observational fact ) 2 ) when the adiabatic constraint is removed the amount of work is no longer equal to the change in the internal energy , and their difference is defined as the heat intake of the system : ( definition of heat ) $$\delta q=du-\delta w$$ here , $\delta q$ and $\delta w$ are not separately functions of state , but their sum ( internal energy ) is . note that $\delta w$ is the work done on the system .
einstein 's theory of gravity is already relativistic so i think that what you are asking is this : beginning with newtonian gravity and making an analogy with coulomb 's law ( where mass is analogous to electric charge etc . ) , and taking into account special relativity effects of a ( mass ) current etc . , does the analog of magnetic force pop out ? the answer is : yes . however and unfortunately , the gravitational waves that also pop out , analogous to electromagnetic waves , transport negative energy .
kamland borexino has set moderately strict limits of the total power of a central geo-reactor . see for instance geo-neutrino : experiments ( pdf link ) a talk by one of my colleagues . ( jalena notes that borexino 's limit is the strongest one going , but kamland was the leader for a while . ) the upper extreme of these limits is less than half the total geological power , but quite non-trivial . the bottom goes all the way to zero . there is also a recent paper ( that i have yet to read ) on a variation of this idea : the kamland-experiment and soliton-like nuclear georeactor . part 1 . comparison of theory with experiment . i have no idea , how the rest of these ideas stand .
i am almost certain there used to be an answer to this question , but it seems to be gone , i will write another one . the earth and sun both orbit their mutual barycenter ( disregarding any other objects of course ) . that one point is a focal point of both ellipses , and all three focal points are collinear . it may appear asymmetric because the sun 's motion is so small , which comes from the asymmetry in the masses . imagine smoothly scaling the sun down to an earth mass . as you did so , the ellipses would approach each other in size , and your missing symmetry would be restored . the key is to notice that there are three , not just two , foci in the system .
as richard points out , you can derive the second equation by setting $\psi$ to be a position eigenstate in the first one . doing that , you turn the general case $$\langle \mathbf{r}\lvert \mathbf{l}\rvert \psi\rangle =\mathbf{r} \times ( -i\hbar\nabla\langle \mathbf{r}|\psi\rangle ) $$ into the relation $$\langle \mathbf{r}\lvert \mathbf{l}\rvert \mathbf{r}'\rangle =\mathbf{r} \times ( -i\hbar\nabla_\mathbf{r}\langle \mathbf{r}|\mathbf{r}'\rangle ) =\mathbf{r} \times \left ( -i\hbar\nabla_\mathbf{r}\delta ( \mathbf{r}-\mathbf{r}' ) \right ) . $$ in here , you can change the $\mathbf{r}$ 's into $\mathbf{r}'$s using the fact that both vectors are equal at the support of the delta function . thus you can change $\mathbf{r}\times$ for $\mathbf{r}'\times$ , but the derivative is a bit trickier : since the argument of the delta fuction is $\mathbf{r}-\mathbf{r}'$ , its derivatives w.r.t. $\mathbf{r}$ differ from its derivatives w.r.t. $\mathbf{r}'$ by a sign , and you must change $\nabla_\mathbf{r}$ for $-\nabla_{\mathbf{r}'}$ . with this , then , $$\langle \mathbf{r}\lvert \mathbf{l}\rvert \mathbf{r}'\rangle =\mathbf{r} \times \left ( -i\hbar\nabla_\mathbf{r}\delta ( \mathbf{r}-\mathbf{r}' ) \right ) =\mathbf{r}' \times \left ( +i\hbar\nabla_{\mathbf{r}'}\delta ( \mathbf{r}-\mathbf{r}' ) \right ) =\mathbf{r}' \times \left ( +i\hbar\nabla_{\mathbf{r}'}\langle \mathbf{r}|\mathbf{r}'\rangle\right ) . $$ once it is in this form , you simply have a global factor of $\langle\mathbf{r}|$ , which you can simply " cancel out " . ( more formally , since the $|\mathbf{r}\rangle$ are a complete set , the projections on the $\langle \mathbf{r}|$ completely determine any vector . or , if you prefer , simply multiply the equation by $|\mathbf{r}\rangle$ and integrate over all $\mathbf{r}$ . ) doing that , then , and dropping the primes , you get , finally $$\mathbf{l}\rvert \mathbf{r}\rangle =\mathbf{r} \times \left ( +i\hbar\nabla_{\mathbf{r}}|\mathbf{r}\rangle\right ) \tag1$$ as you wanted to get . i must say , though that this relation is not particularly useful . what is useful , though , is its adjoint relation , which you can get from the original $$ \langle \mathbf{r}\lvert \mathbf{l}\rvert \psi\rangle =\left ( \mathbf{r} \times ( -i\hbar\nabla ) \langle \mathbf{r}|\right ) |\psi\rangle $$ by simply " cancelling out " $|\psi\rangle$ . ( or , more formally , by noting that the linear functionals on both sides coincide for all $|\psi\rangle$ , and must therefore be equal as linear functionals . ) this gives simply $$ \langle \mathbf{r}\lvert \mathbf{l} =\mathbf{r} \times ( -i\hbar\nabla_\mathbf{r} ) \langle \mathbf{r}| , \tag 2$$ which is evidently the adjoint of ( 1 ) . ( what is remarkable is that the vector calculus remains valid . ) the reason i say that this is the form that is actually useful is that you very , very rarely deal with position ket $|\mathbf{r}\rangle$ , as they are very much not physical states , but you do deal regularly with position bras $\langle \mathbf{r}|$ , as they are an essential ingredient in well-written position representations . the form ( 2 ) then lets you find the position-representation wavefunction of the transformed vector $\mathbf{l}|\psi\rangle$ from the original wavefunction $\langle \mathbf{r}|\psi\rangle$ . this is analogous to the way to make precise the intuition that $\mathbf{p}$ equals the derivative $-i\hbar \nabla$ , by considering its actions on bras instead of kets , to get $$\langle \mathbf{r}|\mathbf{p}=-i\hbar\nabla_\mathbf{r}\langle \mathbf{r}| , $$ as i have said before in this answer . while this looks slightly unintuitive at first , it is actually more useful if you use it right .
first , try and see if you can get the 6 year old to think about " what if there are colors we can not see " ? explain to her that the color we see is the color of " light " . now , show her a remote control , and press some button . there is an ir bulb up front , ask her if it flashes when you press the button ( it should not ) . now , use a phone camera to look at the ir bulb , most phone cameras will show white light when the button is pressed . explain to her that the light coming from the remote is " invisible " , in the sense that it is of a color we can not see . however , the camera can see it because the camera sees slightly more " colors " than we can , and when it tries to display it it shows it as white . explain to her that this is " infrared " light , a light that is " more red than red itself " . whenever someone turns on the tv , a light signal is sent to the tv . ( you may want to explain that this light has some " bending " capabilities , but that is not entirely necessary ) . this ought to get her past the mental block when it comes to " light that is not light " . mentioning that some animals see more/less colors than we do helps . now , talk about the spectrum : explain that the light that we can see is a very small portion of the kinds of light that actually exist . the spectrum is what she sees when she looks at a rainbow , but it really does not " stop " at red or purple ; she just can not see it . if you wish , you can then talk about radio waves , and how they are light that can easily " bend " ( i.e. . diffract ) . talk about x-rays , which is light that can pass through skin but not bones . this can actually lead to an interesting side track where you explain how an x-ray is nothing but a photograph with a different kind of light . once you reach here , it is easy to explain uv . mention that while the sun emits a lot of visible light , it is not limited to the visible spectrum and emits a significant amount ( much less , but not negligible ) of uv and ir as well . you can actually extend this to sound as well , talk about how there are sounds we can not hear . for that matter , sounds just outside your hearing range will be clearly audible to most six year olds . if you can generate increasing frequencies from your computer ( it is actually possible for our vocal cords to work in the inaudible ranges , but it takes some practice to get that to work so it is just easier to use a computer ) , you can both show here that different people/ages have different frequency ranges 1 , and that there are sounds that even she can not hear . ( to do the latter you may want to set up a microphone and have it show the amplitude on the screen or something ) . similarly , you can go to lower frequencies ( and show the transition from invisibly fast vibrations but audible sounds to visible vibrations and inaudible sounds in a string instrument or possibly a rubber band ) . it is a good opportunity to explain how a dog whistle works , too . the concept of there being light that we can not see and sound that we can not hear is a really amazing one when one hears of it first . i certainly was intrigued by it when i learned about this as a child . 1 . this may not be so easy and may not be desirable , see cleonis ' comment below
1 . not all states produced by $\text{cnot} \ ; ( h \otimes i ) $ are entangled . for the first part of your question : no , not all states which arise as the output of $\text{cnot} \ ; ( h \otimes i ) $ are entangled . specifically , you can consider $$\begin{align*} |\psi\rangle \ ; and =\ ; ( h \otimes i ) \ ; \text{cnot} \ ; \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \bigr ] \\ \ ; and =\ ; ( h \otimes i ) \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle \bigr ) \bigr ] \\ \ ; and =\ ; \tfrac{1}{2} \bigl ( |00\rangle + |01\rangle + |10\rangle - |11\rangle\bigr ) , \end{align*}$$ which is a maximally entangled state ( notice from the second line above that it differs from a bell state by a local unitary ) . however , by construction , if you apply $\text{cnot} \ ; ( h \otimes i ) $ to $|\psi\rangle$ , you will get back out the state $$ \text{cnot} \ ; ( h \otimes i ) \ ; |\psi\rangle \ ; =\ ; \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \ ; =\ ; |+\rangle\otimes|0\rangle\ ; , $$ which has no entanglement at all . even if you are interested only in inputs which are product states , we can see that the circuit maps $|+\rangle \otimes |0\rangle$ to another product state &mdash ; specifically , $|0\rangle|0\rangle$ . 2 . all maximally entangled two-qubit states can be easily described by a similar circuit to $\text{cnot} \ ; ( h \otimes i ) $ acting on the standard basis . for the second part of your question : if you allow yourself arbitrary single-qubit unitaries , then for any maximally entangled two-qubit state $|\psi\rangle$ , you can certainly construct a circuit which constructs $|\psi\rangle$ from standard basis states , and which allows you easily to see that the state is maximally entangled . every two-qubit state has a schmidt decomposition , $$ |\psi\rangle \ ; =\ ; u_0|\alpha_0\rangle|\beta_0\rangle \ ; +\ ; u_1|\alpha_1\rangle|\beta_1\rangle \ ; , $$ where $u_0 \geqslant u_1 \geqslant 0$ &mdash ; in particular , $u_1 = 0$ for $|\psi\rangle$ a product state &mdash ; and where $\{ |\alpha_0\rangle , |\alpha_1\rangle \}$ and $\{ |\beta_0\rangle , |\beta_1\rangle \}$ are each orthonormal bases for $\mathbb c^2$ . in order for $|\psi\rangle$ to be maximally entangled , we need $u_0 = u_1 = \tfrac{1}{\sqrt 2}$ . consider single-qubit unitary matrices $a$ and $b$ , such that $$\begin{alignat*}{4} a |x\rangle \ ; and =\ ; |\alpha_x\rangle and \quad and \text{for $x \in \{0,1\}$} , \\ [ 1ex ] b |y\rangle \ ; and =\ ; |\beta_y\rangle and and \text{for $y \in \{0,1\}$} ; \end{alignat*}$$ then we can describe $|\psi\rangle = ( a \otimes b ) \ ; \text{cnot}\ ; ( h \otimes i ) \ ; |0\rangle|0\rangle$ , that is , the effect of applying $ ( a \otimes b ) $ to the bell state $|\phi^+\rangle = \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle ) $ . furthermore , it is not hard to show that any standard basis state is mapped by that circuit to some maximally entangled state similar to $|\psi\rangle$ , and that any circuit of this form ( whatever $a$ and $b$ might be ) maps any standard basis state to a maximally entangled two-qubit state .
first of all , nantennas in general do not violate the second law of thermodynamics , so they are not perpetual motion machines of second kind . as long as the total entropy goes up , the second law is obeyed . in other variables , it really means that a part of the incoming heat has to heat the nantenna up but there may still be a lot of energy left for energy production , much like in any other heat engine . the wikipedia suggestion that natennas could violate the second law only referred to a particular application hypothesized by mr novack . if he could be cooling the room while getting energy out of it , and if the gadget to cool the room were not connected to any cooler heat bath , then it would indeed be a perpetual motion machine of second kind and it would be impossible . the reason why nature makes it impossible is kind of trivial . if the room has temperature $t$ , then the nantenna or " power plant " may only be kept at the same temperature $t$ if there is equilibrium . but if that is the case , the nantenna emits thermal radiation , too . so even if it absorbs some incoming radiation , it still radiates its own . they are balanced and the energy gain is zero . solar cells and " legitimate applications " of nantennas can only create energy because they work with incoming light whose " own " temperature is higher than the temperature of the solar cell or nantenna itself . for example , solar radiation has the temperature comparable to 5,500 celsius degrees . the solar cells are effectively heat engines operating between this high temperature and a much lower temperature of the ground . the same is really true about life on earth , too . the energy from the sun may be converted and is often converted to useful energy or work because the high-energy photons from the sun – which correspond to a high temperature and therefore a low entropy per unit energy ( $e\sim ts$ ) – are processed on earth and the energy is finally emitted in much lower-temperature " infrared " thermal photons – which carry a higher entropy . so the entropy can go up even if a part of the incoming energy is converted to useful work . the temperature inequality between the solar surface ( and the solar radiation ) on one hand and the cool temperature of the outer space is necessary for the sun to play this often praised beneficial role .
if you take the simplest form of capacitor , two parallel plates , the the capacitance is proportional to the area of the plates and inversely proportional to the distance between the plates : $$c \propto \frac{a}{d}$$ when you are making a capacitor out of a snapple bottle you are actually making something similar to the simple " two plate " capacitor but the " plates " are curved round the surface of the bottle , with the foil as the external plate and the ( conducting ) salt water in the bottle acting as the internal plate . the $d$ in the equation above is the thickness of the glass . so you can make a capacitor out of any bottle , jar or anything similar . all that matters is the area of the foil and the thickness of the glass . if you want to increase the total capacitance you can just link any number of bottles in parallel i.e. link the external foil covers as one electrode and the internal brine solution as the other electrode . when you join up capacitors in this way , " in parallel " , you get the total capacitance just by adding up the individual capacitances of all the bottles you have joined . you ask about the effect of the increased voltage : the charge stored in a capacitor is given by : $$q = cv$$ where $c$ is the capacitance and $v$ the voltage , so using 12kv instead of 9kv just means you get 33% more charge for a given capacitance , or alternatively you can get away with a smaller capacitance to hold the same charge . do i sound a bit like a grandma if i point out you need to be careful with this experiment . a typical tesla coil can not generate enough current to kill you , but if you gang together enough capacitors the stored charge in them will kill you ! finally , i normally point people to wikipedia if they want to learn more , so see http://en.wikipedia.org/wiki/capacitor and http://en.wikipedia.org/wiki/leyden_jar for the sort of capacitor you are making . however be warned that the wikipedia article on the capacitor is a bit technical .
the answer is yes , and the up and down quarks in the proton are already close enough to massless that the proton is hardly affected by making them completely massless . the proton mass would shift by a few mev in this case , and the proton and neutron would be very close in mass , with the difference in their mass due entirely to the proton 's electromagnetic field , so the proton with it is electric field would end up slightly more massive than the neutral neutron . the confinement mechanism with massless fermionic particles is very interesting and complicated compared to qed , this is why it took so long to understand historically . the fermions are massless , so they do not want to bind into anything , while the confinement means that they are not allowed to propagate separately , because they carry a qcd string along . this tension is hard to resolve . what qcd does to resolve it is to make a quark condensate in the vacuum . this condensate is actual stuff--- it is like a relativistically invariant superfluid filling all space , and it makes it so that quarks are not chirally symmetric , so while they are still massless , the vacuum does not allow them to propagate while keeping their chirality , when you take their mixing with other quarks in the vacuum into account . this is analogous to a nematic fluid breaking rotational invariance . the nematic molecules , if they were , in free space can rotate with a fixed angular momentum , since they are rotationally invariant . but in the nematic , if you try to spin up one of the long molecules , you get oscillations in the nematic order parameter . the only sign that it was originally rotationally invariant is that the associated nematic orientation waves are long-range goldstone-like bosons whose correlations fall off as a power . because of the condensate 's presence , it is completely wrong to treat quarks as isolated particles--- the low energy particles are collective excitations of the quark fluid ( and the glue fluid ) , just like sound-waves in a solid or orientation waves in a nematic . all the low-energy particles ( with the exception of the broad resonances , the f0 ( 660 ) , and the f ( 980 ) ) can be understood as shaking of this low energy fluid . the pions are the goldstone bosons of the broken chiral symmetry , the rho is the effective gauge bosons of the remaining isospin , and the a ( 1260 ) isotriplet is the gauge boson for chiral isospin . these effective gauge bosons emerge because the qcd string is like a fundamental string , and global symmetries end up effectively gauged . they are massive because they come with a scalar partner , because they are in 5d by ads/cft , not in 4d . the rest of the particles are from extending this picture to strangeness , and the nucleon is the skyrmion . this picture relates the low energy particles only to the symmetry properties of quark field products . this means that the quarks are not composing these particles the way that a proton and a neutron make a hydrogen atom . they make up these particles the way that iron atoms make up a sound-wave in iron . only the symmetry properties matter , and the exact atom that makes up the wave changes as the wave propagates . similarly , as the pion propagates , the quarks that make it up constantly change , as they flow in and out of the dynamical vacuum . the same is true for rho mesons and for protons and neutrons . in more qualitative detail the qcd glue field is random on a scale slightly larger than the proton radius , and this is the origin of confinement . what this means is that if you look at two boxes separated by more than a proton radius , the gauge field is completely uncorrelated in the vacuum in the two boxes ( this is true for 4 dimensional boxes in the euclidean probabilistic version , but the square-root of the euclidean vacuum probability distribution is the ground state wavefunction--- so you should think of the glue correlation in spacelike separated 3d boxes to make this precise ) . a random gauge field makes lots of quarks and antiquarks from the vacuum , because it has a flat power-spectrum at energies less than the randomness scale . this means that at any energy less than 1gev , quarks and antiquarks are freely created and annihilated by the random gauge field . these quarks and antiquarks fill up space with a sea , whose qualitative properties are difficult to predict from the fundamental theory . this sea is like a conduction electron sea in a metal , in that there is fermionic stuff there , but it is not like a conduction band because the total number of vacuum quarks and antiquarks is equal , so you do not have a fermi surface , which would be something which would pick out a rest-frame . instead you have a pair-order parameter , which is very much like the pair-order parameter in a bcs superconductor ( except this pair-order parameter is neutral , so that you get a paired fermion superfluid instead of a supercounductor ) . the chiral condensate is a soup of quarks and antiquarks which make an expectation value for q-qbar . this vacuum condensate breaks the chiral symmetry of the quarks , meaning that quarks are not chirally symmetric below the condensation scale , on the order of 1gev . the shaking of the condensate consists of light pions , while the vector mesons emerge from the shaking condensate linked by a qcd string , which gives emergent massive gauge bosons because string-theory strings turn flavor symmetries into gauge symmetries in one higher dimension , and qcd strings are not qualitatively too different from string theory strings . in addition , you can tie a knot in the pion condensate , the knot is the skyrmion , and this knot is the nucleon . the pionic field of a proton is not simply described by high-energy calculations in qcd , it is better described by the classical configuration of the long-range effective condensate theory . the skyrme model is not very precise , it is only good to something like 30% accuracy ( quantitatively , this is very bad ) in predicting the field distribution in protons and the structure of small nuclei . but it is at least qualitatively consistent with the known vacuum structure of qcd , unlike the picture of the proton as 3 quarks making a bound state . the picture that the low-lying excitations of qcd are simple bound states made of quarks is completely dynamically wrong . it has no justification , and it is sold to the public , because until recently , the concept of a dynamical vacuum was too far-out . it was nambu who introduced the dynamical vacuum in 1960 , and this pioneering work has too often been buried in the succeeding decade . with nambu 's recognition with a nobel prize , some of this outrageous recent history can be put right .
sound waves propagate very similarly to how ' the wave ' propagates at baseball stadiums : http://www.youtube.com/watch?v=h0k2dvb-7wy at some point something ( your vocal cords , a piano string , a speaker ) hit a bunch of air particles ( atoms , molecules , it really does not matter ) . these particles in turn hit the particles next to them , these hit the ones next to them and so on . no pressure here is simply the absence of any particles , so nothing communicates the orders to move . this is like ' the wave ' in that everyone communicates the motion of the wave of the person standing next to them , and if there is no one standing next to you , the wave ends with you . hearing a sound is the last bunch of air particles next to your ear drum getting the instructions to vibrate which in turn vibrates your ear drum and your brain turns this response into the perception of sound .
the scatter direction depends on the size of the particle and the wavelength . very small particles ( e . g . nitrogen atoms of the atmosphere ) scatter isotropically . there is still an effect on the polarisation of the scattered light ( bees use that to locate the sun if they can not see it directly ) . often gaussian beams are used to describe how the intensity propagates in an optical train ( a system of lenses and mirrors ) http://en.wikipedia.org/wiki/gaussian_beam . note that this describes the intensity of a laser with a gaussian intensity profile ( this is a good approximation for many lasers , especially if you focus them through a pinhole ) . if you have an extended light source you will have to add the intensities of several such beams . once you have a numeric intensity profile ( i guess 2d is enough for your usage ) , you can try an exponential decay law to estimate the effects of scatter . your focus spot will look more intense the higher the numerical aperture of your lens is . if you use a lens that has 2.5 cm diameter and f=10cm the spot will not look as intense as with a lens that has f=3cm . have you thought of using fluorescence ? you could use dissolve some colour in water and use laser protection goggles to see the fluorescent light . then you do not have to cope with scattering . you can get polygonal mirrors out of old laser printers . that way you can scan the beam in one direction . if you use a laser diode , you can modulate the intensity very fast . i recently purchased a 405nm laser with 120mw for $120 from lasever.com. 120mw is very dangerous . if you do not have protection goggles or you share your space with other people do not use lasers> 0.5mw !
the basis of the hilbert space in schrödinger 's picture is assumed to be time-independent regardless of any properties of the hamiltonian . the hamiltonian is just another operator . if the hamiltonian is time-dependent , its eigenstates and eigenvalues are obviously time-dependent , too . both equations you write down only express the fact that the basis of eigenstates of $h ( t ) $ is still a basis , so a general ket vector , including the actual state vector of the system , may be expanded as a linear superposition of these basic vectors with some general complex coefficients $c_n ( t ) $ . the two expansions only differ by the phase one includes into the coefficients $c_n ( t ) $ or into the basis vectors $|n ; t\rangle$ . one convention includes the phase $\exp ( i\theta_n ( t ) ) $ , another one does not , and so on . obviously , there is no " universally mandatory " rule that would dictate the right phase of these vectors so there is some freedom about the notation . note that a phase factor times an eigenstate is still an eigenstate . whatever your convention for the phases is , if you carefully follow the maths and remember what the symbols mean – the defining equations – you will be able to derive the invariant claims about the adiabatic theorem . the wikipedia-sakurai conventions treat the phases wisely and naturally , to speed up the derivations .
the most probable position would be such as where the global maximum of the distribution is located . this is different to the expectation value of a distribution , but it happens that for a gaussian function the mean and the most probable value are the same .
the riemann tensor encapsulates all information about the 4-dimensional space-time . this information can generally divided into two sectors : information about the curvature of space-time due to the existence of matter . this is given by the ricci tensor according to the einstein equation $$ r_{\mu\nu} - \frac{1}{2} g_{\mu\nu} r = 8 \pi g t_{\mu\nu} $$ information about the structure of gravitational waves in the space-time . this is given by the trace-free part of the riemann tensor , namely the weyl tensor . often , we are not quite interested in the exact structure of the space-time , but only if gravitational waves can exist or their structure . in these cases , one studies the weyl tensor rather than the ricci tensor . for example , in the setup of quantum gravity , one requires to study the asymptotic structure of spacetime . in these theories , a good understanding of the weyl tensor is more important .
i will translate your post into the language of translations . then i will answer this question about translations . then i will answer your original question . translation question i am confused about a trivial concept . let the displacement of a rigid body be described by the equation $\vec{x} ( t ) =\delta \vec{x} ( t ) +\vec{x} ( 0 ) $ , with $\delta \vec{x} ( 0 ) =0$ . then , at each instant there is only one unit vector in the direction of displacement that we may call $\hat{w} ( t ) $ and which we may take to be normalized . that vector $\hat{w} ( t ) $ is what geometrically we would call the ( instantaneous ) direction of velocity [ note : this sentence is wrong ] . kinematically , however , the instantaneous direction of velocity $\vec{v}$ is the derivative $\dot{\vec{x}} ( t ) =\dot{\delta \vec{x}} ( t ) $ . that is the direction of $\vec{v} ( t ) $ . as is obvious ( for example an object not moving in a straight line ) , in general $\hat{w} ( t ) $ and $\vec{v} ( t ) $ are not parallel . so , why are there two directions for velocity , and does $\hat{w} ( t ) $ play any role in the kinematics/dynamics of the motion ? answer to translation question you are wrong that $\hat{w}$ is the direction of velocity . $\hat{w}$ was defined as the direction of $\delta \vec{x}$ , that is , the direction of the displacement . as kevin said the total displacement is not needed because you only need to know a objects current position and velocity to get its motion in the future . answer to the rotation question now we are ready to answer the rotation question . we just translate the answer of the translation question to rotation language . you are wrong that $\vec{v}$ is the direction of angular velocity . $\vec{v}$ was defined as the axis of rotation for $r$ , that is , the direction of the rotation between the initial and final orientation . as kevin said this rotation is not needed because you only need to know a objects current position and velocity ( here we are talking about the velocity everywhere in the object , which for a rigid object can be summarized by a linear velocity and an angular velocity ) to get its motion in the future .
so first of all , the first equation you gave is only correct , if the $|ø\rangle$ form a basis . it has nothing to do with " in which basis they are " . the easiest way to understand this is probably with a 3d vector-analogy . so if $b_i$ , $i=1\dots3$ form a basis , for any vector $v$ it is legitimate to write $$v=\sum_{i=1}^3 b_i ( b_i\cdot v ) $$ there , the $b_i\cdot v$ are the components of $v$ in the representation of the $b_i$ . it is the very same for bras and kets . it is " just " not 3d but has infinite dimension , so if we have a basis of infinite $|ø\rangle$ , $ø\in \mathbb{r}$ or $|r\rangle$ , $r\in \mathbb{r}^3$ , denote the scalar product ( $a\cdot b$ ) using dirac notatiton ( $\langle a|b\rangle$ ) , and write integrals instead of sums we get te formulas given by you ( mathematically this is non trivial ) . therefor the $\langle r|\psi\rangle$ are the components in the position basis .
at the risk of oversimplifying : bonds form because the bonding electrons can lower their energy by being attracted to both nuclei . take the simplest possible example of h$_2$ . the electron density looks like : ( image from hyperphysics ) . the formation of the h-h bond increases the electron density between the two protons , and the electrons in this region have a lower energy because they feel the attractive force of both protons . if you now consider a bigger atom , then the inner electron shells that do not participate in the bond will increase the size of the atom and therefore increase the distance between the nuclei . because the electrons in the bonding region are farther from the two nuclei the energy gain they obtain from being in the bond is less , so the bond is weaker . there are not any higher atomic weight analogues of hydrogen , but consider the halogens instead as they form dimeric molecules . if you look at the formation enthalpy of the x-x molecule it varies down the group , i.e. with increasing atomic size , as : ( image from here ) apart from fluorine ( see below ) you see the bond enthalpy decreasing with increasing atomic size . the reason fluorine is anomalous is because the f atom is so small that the lone pair electrons get crowded in f$_2$ and decrease the bond strength . if you look at the hx molecule instead you do not get this effect ( because h does not have any lone pairs ) and you see the trend you expect : and this applies to carbon and silicon as well . the reason the si-si bond is weaker than c-c is simply that because the si atom is bigger than the c atom the electrons forming the bond are farther from the nuclei and do not lower their energy as much by forming the bond .
conformal transformations are used in order to analyze the structure of a given spacetime . one can map a formally infinite spacetime to a compact interval , and study its properties there . this process is referred to as " conformal compactification " , and enables one to draw penrose diagrams . they serve to identify and classify horizons , infinities and singularities and are popular for various spacetimes such as minkowski space and schwarzschild black holes . for example , minkowski space , given by $$ds^2=-dt^2+dr^2+r^2d\omega^2 , $$ can be conformally compactified by a change of coordinates from $ ( r , t ) $ to $ ( u , v ) $ by the transformations $$u=\arctan ( t-r ) , $$ $$v=\arctan ( t+r ) , $$ which leads to $$ds^2=\frac{1}{4\cos^2u\ , \cos^2v}\left ( -4dudv+\sin^2 ( v-u ) d\omega^2\right ) . $$ due to the nature of the $\arctan$ function , the coordinates will now take on values on the interval $ ( -\pi/2 , \pi/2 ) $ and are hence compact . another part of gr where conformal invariance plays a role regards curvature : the traceless part of the riemann tensor , i.e. the weyl tensor , is conformally invariant . further applications of conformal invariance related to general relativity include string theory , which is conformally invariant on the string worldsheet , and the ads/cft correspondence , where a string theory on a 5-dimensional ads space is equivalent to a 4-dimensional ( supersymmetric ) conformal field theory . in certain coupling limits of this duality , the string theory part reduces to supergravity , which again reproduces standard general relativity once one breaks supersymmetry . such models are used for example to describe duals of qcd-like theories within holography .
even without a graph , the answer is straightforward . the potential energy stored in a spring is proportional to the square of the difference b/w stretched and unstretched length , $$v = \frac{1}{2}kx^2 $$ thus , the work required to stretch it ( mind you , you have to do this slowly so that the kinetic energy is not changed ) would be : $$w = \frac{1}{2}k ( 2d ) ^2 - \frac{1}{2}k ( d ) ^2 = \frac{3}{2}kd^2 = 3w_0$$ this is just straightforward conservation of energy .
gravity may be fundamentally different from electromagnetism , as wouter says , but it seems to me that , as far as your question is concerned , gravity is not fundamentally different from electromagnetism : there is gravitational field ( http://en.wikipedia.org/wiki/gravitational_field#general_relativity ) , which is indeed " some " measurable material " of gravity between say the earth and the moon " . afaik , you are right , and " gravity in theory , has an infinite range and moving a flower on earth could move an atom on jupiter ever so slightly . " ( i removed your " maybe " and question marks ) . however , in general , an atom on jupiter does not " feel " a movement of a flower on earth instantaneously - gravity is widely believed to have a finite propagation speed ( http://en.wikipedia.org/wiki/speed_of_gravity ) equal to the speed of light . so again , it seems that , as far as your question is concerned , the situation with gravity is not fundamentally different from that with electromagnetic field : the coulomb field has an infinite range , and there is " measurable material " - electromagnetic field ( in the form of electrostatic field ) between two charges , but , if one charge moves , the other charge does not " feel " that movement instantaneously . thus , both gravitational and electrostatic forces are mediated by fields . one could say that gravitational field is just space curvature , but i do not feel that would change much . as for gravitons . . . again , afaik , while it is not clear yet how gravitational field should be quantized ( http://en.wikipedia.org/wiki/quantum_gravity ) , that does not seem to matter much as far as your question is concerned .
provided that $\mathcal{l}$ is a lorentz scalar , the quantity $\partial\mathcal{l}/\partial ( \partial_{\mu}\phi ) $ has to carry an upper index . since $\mathcal{l}$ is a function of $\phi$ and $\partial_{\mu}\phi$ , the only object that can give such an index is $\partial^{\mu}\phi$ . hence \begin{equation} \frac{\partial\mathcal{l}}{\partial ( \partial_{\mu}\phi ) } \propto \partial^{\mu}\phi . \end{equation} then , \begin{equation} \begin{split} \frac{\partial\mathcal{l}}{\partial ( \partial_{\mu}\phi ) } \omega^{\sigma}{}_{\mu}\partial_{\sigma}\phi \ , and \propto \ , \omega^{\sigma}{}_{\mu}\ , \partial_{\sigma}\phi \ , \partial^{\mu}\phi\\ and =\omega^{\sigma\mu} \partial_{\sigma}\phi\ , \partial_{\mu}\phi\\ and =0 . \end{split} \end{equation} the last expression vanishes because $\partial_{\sigma}\phi\ , \partial_{\mu}\phi$ is symmetric under the interchange of indices while $\omega^{\sigma\mu}$ is antisymmetric . i actually do not understand why tong did not simply write \begin{equation} \delta \mathcal{l} = -\omega^{\mu}{}_{\nu} x^{\nu}\partial_{\mu}\mathcal{l} . \end{equation} after all , $\mathcal{l}$ should have the same transformation rule as $\phi$ because they are both lorentz scalars . one can verify the above equation by noting that \begin{equation} \delta \mathcal{l} = - \partial_{\mu} ( \omega^{\mu}{}_{\nu}x^{\nu}\mathcal{l} ) = -\omega^{\mu}{}_{\nu} x^{\nu}\partial_{\mu}\mathcal{l} - \omega^{\mu}{}_{\mu}\mathcal{l} , \end{equation} and that \begin{equation} \omega^{\mu}{}_{\mu} = \eta_{\mu\rho}\omega^{\mu\rho} = 0 \end{equation} because $\eta_{\mu\rho}$ is symmetric and $\omega^{\mu\rho}$ is antisymmetric .
ok , so there are quite a few parts to your question - i hope that i address most of them to your satisfaction . the author of the notes is essentially using the background field method ( bfm ) to calculate the effective action . ⁰ many of his choices follow from his focus on calculating only the one-loop effective potential . the background field method works in all theories with scalar , gauge , fermions , gravity , superfields¹ etc . . . it also works at all loops , however at higher loops , diagrams are still helpful for organising the calculations . two of the seminal papers did not use diagramatics : schwinger 's on gauge invariance and vacuum polarization and the follow-up two-loop calculation of ritus . however , since the bfm perturbation theory still uses a propagator-interaction separation , diagrammatics are only natural . one advantage of using bfm calculations is that you only need to calculate " vacuum " diagrams , i.e. , those with no external legs . this makes the diagrammatics and combinatorics easier . the other advantage is that the calculations become gauge covariant . the trade-off is the use of more complicated propagators . it is most often used to calculate low energy corrections ( like the effective potential for scalar fields ) by keeping only the lower terms in the derivative/momentum expansion . in particular , to find the effective potential , you only need constant background scalar fields . to find the kinetic terms , you need fields with at most two derivatives . however , it can also be used to construct perturbation theories that are very similar to the standard feynman diagram calculations , but are explicitly covariant under gauge transformations . good examples of this are improved methods for supergraphs and new , improved supergraphs . a really nice three-loop calculation of the yang-mills beta-function using the covariant background field method is hep-th/0211246 . they step through the set-up of the calculation quite slowly , so it is a good paper to learn from . the bfm relies on the following idea , see , e.g. , abbott 's the background field method beyond one loop , but the result can be extended to other theories with more complicated background-quantum splittings : let $\gamma [ v ] $ be the effective action ( legendre transform of the connected generating function $w [ j ] $ ) where $v=v ( j ) =\frac{\delta w [ j ] }{\delta j}$ is the " classical " field generated by the sources $j$ . if we modify the classical action by splitting the quantum fields into quantum + background , then the resultant modified effective action $\gamma [ v ( j ) , v ] $ now depends on both $v ( j ) $ and the background fields $v$ . you can show ( under reasonable assumptions ) that $\gamma [ 0 , v ] = \gamma [ v ] $ . the effective action is gauge dependent , ² and the previous result is true in the background field gauge . in the notes you linked to , the beta function was found from the quartic , derivative free correction . it can also be found from the gauge kinetic term . this works , because the method is background gauge covariant , which forces the gauge coupling and field renormalisations to be related . in fact , the background gauge potential field never needs to leave the covariant derivative and the beta function could be found from the invariant $\mathrm{tr} ( f_{\mu\nu}f^{\mu\nu} ) $ term . this was done in the original paper by schwinger . see abbott for more of a discussion on this point . also read about the schwinger-dewitt expansion for how to control some covariant expansions in effective action calculations . the classic paper is the physics report by barvinsky and vilkovisky . see avramindi and kuzenko and mcarthur for more info on covariant methods . for a good 2-loop bfm calculation involving fermionic backgrounds , see jack and osborn . although they do not calculate the finite parts of the propagators . low energy calculations with fermion backgrounds can be quite tricky , which also makes some supersymmetric bfm low-energy calculations tricky . ⁰ that said , the notes do collect some nice arguments about the structure of the effective action and effective potential . ¹ although , in some theories ( e . g . , n=1 supersymmetry ) , the quantum-background splitting has to be nonlinear . ² a gauge independent version of the effective potential does exist - but it seems to be not very practical to calculate or use - at least , it is not used much . see , e.g. , vilkovisky or becchi . . .
the energy you seem to refer to is the electric part of the poynting energy expression for some volume $v$: $$ e_{\text{poynting}} ( t ) = \int_v \frac{1}{2}\epsilon_0 \left|\mathbf e ( \mathbf x , t ) \right|^2 + \frac{1}{2\mu_0}\left|\mathbf b ( \mathbf x , t ) \right|^2 \ , d^3\mathbf x . $$ the vector $\mathbf e ( \mathbf x , t ) $ in this expression is the electric vector at position $x$ at time $t$ . there is no integration over time in this expression . if you want to express this electric part of energy with help of the fourier amplitude $\tilde{\mathbf e} ( \mathbf x , \omega ) $ defined by $$ \mathbf e ( \mathbf x , t ) = \int_{-\infty}^{\infty}\tilde{\mathbf e} ( \mathbf x , \omega ) e^{i\omega t} \frac{d\omega}{2\pi} , $$ you can simply substitute in the above expression : $$ e_{electric} ( t ) = \int_v \frac{1}{2}\epsilon_0 \left|\int_{-\infty}^{\infty}\tilde{\mathbf e} ( \mathbf x , \omega ) e^{i\omega t} \frac{d\omega}{2\pi}\right|^2 d^3\mathbf x . $$ energy is a function of $t$ only , and you can try to find a formula for its frequency dependent fourier components $e_{electric} ( \omega ) $ by calculating ft of the last expression with respect to time $t$ .
here is a link which discuses this problem in some depth . i do not think that this would be a good project for an ode class because temperature and density are not differentially related to each other i.e. one is not a differential form of the other .
i do not know where you measured 39° . the blue angle is 36.87° ( the sides of the red triangle are 3-4-5 ) , as it should be .
they would be linearly dependent if and only if there exist complex numbers $\alpha$ and $\beta$ such that $\alpha x_{1} ( t ) + \beta x_{2} ( t ) = 0 \forall t$ clearly , if $\omega_{0}=0$ then this is the case for $\alpha = 1$ and $\beta = -c_{1}/c_{2}$ . so then they are linearly dependent . however , if $\omega_{0}\neq0$ , you can not find a combination of $\alpha$ and $\beta$ that fulfills this requirement for all $t$ . the importance lies in the fact that ( 1 ) any linear combination $\gamma x_{1} ( t ) + \delta x_{2} ( t ) $ of the two functions is also a solution . ( just plug the linear combination into the equation to see this . ) ( 2 ) these are the only solutions . namely , if you would find a solution $y ( t ) $ you could always write it as a combination of $x_{1}$ and $x_{2}$ . so these are the only solution to care about , all the dynamics of the system is contained in them .
if an object is moving in a circular motion , its velocity $\vec{v}$ changes . the centripetal acceleration is just a formula that gives you the length of the derivative $\frac{d\vec{v}}{dt}$ which is the acceleration . it must be caused by some force , according to newton 's second law . if you are holding the object with a rope , then it is the tension of the rope , if it is a satelite on a circular orbit , then the force is of gravitational nature . when the asteroid hits the satellite , $\vec{v}$ changes , while the gravitional force remains the same . so , the force now creates the same acceleration , but now it does not coincide with ' centripetal acceleration ' for this speed ( which is just a number characterizing the orbit , not the object ) . this simply means that the object will leave the circular orbit , because its acceleration and speed now correspond to a different trajectory . this trajectory happens to be elliptic/parabolic/hyperbolic depending on the speed . these cases can be distinguished by total energy -- $e&lt ; 0$ , $e=0$ , $e&gt ; 0$ respectively .
so you want to know how much water a certain surface adsorbs . this is really dependent on the surface material/conditions . check adsorption and relative humidity on wikipedia . to where i have analyzed , it seems that there is about enough information in the two articles . i am not a specialist on the subject so i might be missing some important factor .
approximately $10^{15}$ . see this : photon flux of 540 nm light from the mechanical equivalent of light and the integrated spectral sensitivity of the human eye : $3.8&#215 ; 10^{15}\ photons/s$ ( photons per second ) $6.3&#215 ; 10^{-9}\ mol/s$ ( moles of photons per second ) also see this reference . note : this summarises robert 's answer in the question comments and is set to cw .
i do not think anyone here has really answered your question . in this case , the sound is " focused " using phased arrays . the face of the audio spotlight has multiple transducers : flickr the same signal is output from each of them , but delayed slightly by different amounts , so that the wavefronts all reach the same point in front of the device at the same time . this " virtual focus " is called beamforming . ref ref this is how modern radars focus their beams , too . instead of spinning a satellite dish around , they have lots of little elements that do not move , but the signals are delayed to produce different beam shapes .
when it is in water the buoyant pressures are distributed more evenly over the whale 's natural surface contour , resulting in less internal strain in the whale 's body . on land , the pressures are all concentrated in a planar surface at the bottom . the whale 's body is not naturally planar , so significant strain develops as the body attempts to conform to the planar surface in order to distribute the forces resisting gravity .
with the given $m$ and $k$ you indeed cannot calculate the damping coefficient $c$ . remember that you just have a model where you put some constants in and you can only derive other constants which somehow depend on them . the question concerning an actual measurement was answered in investigating the dampening of a spring . greets
your intuition is right , an object ( at least of the ideal sort these formulas are used to describe ) returns to the same size when it is brought back to the same temperature . the reason your math is not giving that result is that $l ' = l + l\alpha\delta t$ is a first-order approximation , valid for small values of $\alpha\delta t$ . this means , among other things , that when doing calculations with it , your results only have to agree to first order - in other words , if you have two quantities that differ only by a multiple of $ ( \alpha\delta t ) ^2$ ( or any higher power ) , they can be considered the same . by this reasoning , $l'' = l$ to first order ; or , as it would be written in symbols , $l'' = l + \mathcal{o}\bigl ( ( \alpha\delta t ) ^2\bigr ) $ . if you want to be a little more rigorous about this whole business , you can derive the full , non-approximate formula for thermal expansion like this . for a small value of $\delta t$ , the corresponding change in $l$ is $$\delta l = \frac{\mathrm{d}l}{\mathrm{d}t}\delta t$$ from the approximate thermal expansion equation , you know that $\delta l = l\alpha\delta t$ , which gives $$l\alpha = \frac{\mathrm{d}l}{\mathrm{d}t}$$ integrating this gives $$l ' = le^{\alpha ( t ' - t ) }$$ which is the real expression ( or at least a mathematically consistent better approximation ) . you can verify that $l ' = l\alpha\delta t$ is the first-order approximation to this . another way to do this derivation is to think about heating a material from temperature $t$ to $t ' = t + \delta t$ , which takes it to length $$l ' = l ( 1 + \alpha\delta t ) $$ then to a slightly higher temperature $t'' = t ' + \delta t$ , which takes it to length $$l'' = l' ( 1 + \alpha\delta t ) = l ( 1 + \alpha\delta t ) ^2$$ then to a slightly higher temperature $t'''$ , which takes it to $$l''' = l'' ( 1 + \alpha\delta t ) = l ( 1 + \alpha\delta t ) ^3$$ and so on . if your desired final temperature is $t_f = t + n\delta t$ , then the final length will be $$l_f = l ( 1 + \alpha\delta t ) ^n = l ( 1 + \alpha\delta t ) ^{ ( t_f - t ) /\delta t}$$ in the limit as $\delta t\to 0$ , this becomes $$l_f = l e^{\alpha\delta t}$$
you are right that friction points up the hill . what happens when you solve this is that you get a friction force that is negative . a negative force pointing down the hill is the same as a positive force pointing up the hill , so everything works out okay . it would have been more clear if the diagram author showed the friction vector pointing uphill to begin with , though .
another very fresh paper presented at dark attack yesterday , one by hektor et al . , http://arxiv.org/abs/1207.4466 also claims that the signal is there – not only in the center of the milky way but also in other galactic clusters , at the same 130 gev energy . this 3+ sigma evidence from clusters is arguably very independent . all these hints and several additional papers of the sort look very intriguing . there are negative news , too . fermi has not confirmed the " discovery status " of the line yet . puzzles appear in detailed theoretical investigations , too . cohen at al . http://arxiv.org/abs/1207.0800 claim that they have excluded neutralino – the most widely believed identity of a wimp – as the source because the neutralino would lead to additional traces in the data because of processes involving other standard model particles and these traces seem to be absent . the wimp could be a different particle than the supersymmetric neutralino , of course . another paper also disfavors neutralino because it is claimed to require much higher cross sections than predicted by susy models : http://arxiv.org/abs/1207.4434 but one must be careful and realize that the status of the "5 sigma discovery " here is not analogous to the higgs because in the case of the higgs , the " canonical " null hypothesis without the higgs is well-defined and well-tested . in this case , the 130-gev-line-free hypothesis is much more murky . there may still exist astrophysical processes that tend to produce rather sharp peaks around 130 gev even though there are no particle species of this mass . i think and hope it is unlikely but it has not really been excluded . everyone who studies these things in detail may want to look at the list ( or contents ) of all papers referring to weniger 's original observation – it is currently 33 papers : http://inspirehep.net/search?ln=enp=refersto%3arecid%3a1110710
i am assuming that this section of the book is talking about the ultraviolet catastrophe , where an ideal black body in thermal equilibrium will emit an infinite amount of power through radiative means . the source goes on to say : the ultraviolet catastrophe results from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes ( degrees of freedom ) of a system at equilibrium have an average energy of kt . following that : according to classical electromagnetism , the number of electromagnetic modes in a 3-dimensional cavity , per unit frequency , is proportional to the square of the frequency . this therefore implies that the radiated power per unit frequency should follow the rayleigh–jeans law , and be proportional to frequency squared . thus , both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered : this is clearly unphysical as the total radiated power of a cavity is not observed to be infinite , a point that was made independently by einstein and by lord rayleigh and sir james jeans in the year 1905 . essentially , if in a cavity there are an infinite number of electromagnetic modes possible ( think standing waves ) , the equipartition theorem says that a system in equilibrium has an average of $k_{b}t$ worth of energy per mode . this is not what is actually observed , since we do not see an infinite amount of power radiated . how was this solved : max planck solved the problem by postulating that electromagnetic energy did not follow the classical description , but could only be emitted in discrete packets of energy proportional to the frequency , as given by planck 's law . the radiated power eventually goes to zero at infinite frequencies , and the total predicted power is finite . ( 1 ) the formula for the radiated power for the idealized system ( black body ) was in line with known experiments , and came to be called planck 's law of black body radiation . based on past experiments , planck was also able to determine the value of its parameter , now called planck 's constant . the packets of energy later came to be called photons , and played a key role in the quantum description of electromagnetism . ( 1 )
the hamiltonian has the very legal definition that it is the legendre transform of the lagrangian function . so , in any physical case to find the hamiltonian of a system , you have to take $l = t - v$ and then perform the ritualistic legendre transform process shifting coordinates from $q , q'$ to $q , p$ . the time symmetry of the system leads to the conservation of the so called jacobian function and not the hamiltonian . the hamiltonian will however be the total mechanical energy if thesystem is conservative and in this case the jacobian function also equals the total mechanical energy . the jacobian is the mechanical one and not the mathematical one . ref goldstein .
i have not been to space : ( and do not know of any accounts to point you to but i suspect that the view would be marginally better than that on earth . first , the " black of space " would be really black i.e. no light . even in dark skies there is a bit of scattered light in the atmosphere ( even if just from scattered starlight ) so you had have higher contrast . additionally , you would not have any " seeing " effects , the bluring of the starlight by the shifting atmosphere so the stellar images would be more concentrated . not that you had probably be able to consciously notice . the net effect would be to sharpen the stellar image on you eyes and increase the contrast a bit more . i suspsect you had see a few more stars for the above reasons , especially the latter as you eyes would have a better chance to detect light from the sharper stellar images . however , it would not be a huge amount more as the stars are intrinsically faint and your eyes are only so senstive .
in the book by p.g. de gennes superconductivity of metals and alloys , it is written $n ( 0 ) v &lt ; 0.3$ . also written : lead and mercury are two notable exceptions with low $\theta_{d} \left ( =\hslash \omega_{d}/k_{b} \right ) $ , giving , respectively , $n ( 0 ) v=0.39$ and $n ( 0 ) v=0.35$ . more details on p . 112 , p.g. de gennes superconductivity of metals and alloys , westview ( 1999 ) . the first edition dated back to 1966 . to my knowledge there is no change between the editions . see also the table 4-1 on p . 125 of the same book for several specific values for the pure metals . this table is reproduced below for commodity . $$\begin{array}{cccc} \mbox{metal} and \theta_{d}\left ( \mbox{k}\right ) and t_{c}\left ( \mbox{k}\right ) and n\left ( 0\right ) v\\ \mbox{zn} and 235 and 0.9 and 0.18\\ \mbox{cd} and 164 and 0.56 and 0.18\\ \mbox{hg} and 70 and 4.16 and 0.35\\ \mbox{al} and 365 and 1.2 and 0.18\\ \mbox{in} and 109 and 3.4 and 0.29\\ \mbox{tl} and 100 and 2.4 and 0.27\\ \mbox{sn} and 195 and 3.75 and 0.25\\ \mbox{pb} and 96 and 7.22 and 0.39 \end{array}$$ post-scriptum : there are ( surprisingly ! ) nothing about this question on the book by j.r. schrieffer , superconductivity , benjamin ( 1964 ) . there is not even a discussion on the gap equation as far as i can check . . . there is a repetition of the gennes data on the book by a.i. fetter and j.d. walecka , quantum theory of many-particle systems , dover publications ( 2003 , first edition 1971 ) , p . 448 . but this table is less complete . there is also no discussion about the numerical value in the original paper by bcs [ bardeen , j . , cooper , l . n . , and schrieffer , j . r . ; theory of superconductivity . physical review , 108 , 1175–1204 ( 1957 ) . http://dx.doi.org/10.1103/physrev.108.1175 -> free to read on the aps website ] , but there is a possibly interesting expression ( eq . ( 2.40 ) , written below in your notation / in the original bcs paper , the gap is written $\varepsilon_{0}$ ) : $$\delta\left ( t=0\right ) =\frac{\hslash\omega_{d}}{\sinh \frac{1}{n ( 0 ) v}}$$ which might be of help for calculating the critical line $\delta ( t ) $ .
i met a paper published in prl using the orietational order parameter and global bond_orietational order parameter . although the structures in this paper are about the short chain alkanes , i think the definitions of the two kind of order parameters may provide you some insights into your question .
dear sina , if you replace "0" and "1" by " up " and " down " , you get a similar state for two spins - which is referred to as the singlet . all these states are mathematically analogous except that the states "0" or "1" , or " up " and " down " , or " plus " and " minus " ( as indices of your $\chi$ ) may mean physically different things - i.e. these states may influence the interactions of the system with other degrees of freedom differently . for example , the spin " up " and " down " likes to add some $-\mu . b$ energy in a magnetic field that depends on the direction of the spin . other degrees of freedom interact differently - and must be prepared by different apparata , depending on the context . at the level of " information " , you always have two subsystems whose 1 qubit of information is correlated with the other in the same way ; from the viewpoint of all physics , they can be very different things ( just think about all the ways how qubits may be realized in quantum computers ) . however , the state of the form $|01-10\rangle$ is always entangled : the quantum numbers of the two fermions ( or subsystems ) $1,2$ in the state are nontrivially correlated . this does not prove any interaction - it just proves that they were prepared to have correlated properties . to see that the state is entangled , regardless of the symbols , note that it cannot be written as a tensor product of a state for the fermion or subsystem 1 , multiplied by another state of the subsystem or fermion 2 . equivalently , you may trace over the 2 degrees of freedom , to get a density matrix for the subsystem 1 . and you will get $\mbox{diag} ( 0.5,0.5 ) $ which has a nonzero entropy $\rho \ln ( \rho ) $ , proving that the state is not poor . because the induced 1-particle state is not poor , it proves that the original state of the two particles was entangled . almost all states in the multi-particle hilbert space are entangled , of course . however , there are often reasons to assume that two systems are not entangled - because they did not influence each other in the past ( or at least not much ) .
the answer seems to near 60 degrees . additional description here i have found this mathematica description of the ecliptic plane relative to the galactic plane . wolfram . com
as michael brown mentioned in the comments , no one will explain this as well as feynman ( at least , no one we know of that is alive ) . but that does not mean your question does not deserve at least our attempt . so here is mine and i will try to keep this in the simplest terms i can . ( aside : to all of the physicists reading this , i apologize in advance but in my simplification , i may intentionally omit or contradict the true physics . for instance , i doubt i will be saying how the photon arises as a gauge boson in local u ( 1 ) symmetry ) . see ( 2 ) see ( 1 ) . just kidding . i have lumped 1 and 2 together because to explain what a photon is is to essentially explain where it comes from . hopefully , everyone reading this will be aware of the wave-particle duality that most ( all ) things enjoy . in that way , as mentioned , a photon is a particle in its own right . but does that mean that people can think of the photon as a tiny billiard ball ? no , that would be silly . the photon is a wave packet that , for all intents and purposes , is indivisible . consider a vibrating electron , it is motion one way or the other constitutes a current , which radiates a magnetic field . since this magnetic field is changing as the electron speeds up and slows down , it induces an electric field that radiates outward . since this electric field is also changing continuously , this in turn induces a magnetic field that radiates outward . rinse and repeat . the result is a self-propagating combination of electric and magnetic fields travelling outward from the electron . this is em radiation . the photon is the unit of an em wave . what is one photon ? say we shine a laser , then we block half the beam with a metal plate . the other half still comes through . if there were only one photon in the beam , when you block half of it with the metal plate , none of it would come through . one photon is the largest amount of energy of an em wave where this would still be true . in physics terms , we write it as $e=h\nu$ . the energy of one photon of a wave is equal to h ( a very small constant ) times the wave 's frequency . because the photon is indivisible , we can say it represents the smallest unit of energy of that particular em wave . a different wave would have a different smallest energy . to address what was mentioned in the comments , a photon may have full particle status , but it is not similar to an electron . photons have no mass , they are not matter and , when you examine the properties of a photon , there is no denying that they are packets of energy in every respect ; fluctuations in the background em plane . i will admit , at first glance it does seem very silly for us to say electric and magnetic fields can produce action at a distance and then say , " no , you need photons to cover that distance and actually do the interacting " . but it is true . at least , we can say it is true . without teaching everyone about field theory and symmetries , my short answer to this would be that in advanced physics , we have a certain equation that originally did not work out . as we have done a million times in the past , we had to modify this equation to work and we found that we could only do that by introducing a massless particle that mediates the em force . afterward , we noticed that this particle happened to have the same properties of a photon . in fact , if we tried the hypothetical situation where we assumed this particle was a photon , this one equation produced all of the laws and equations from electromagnetics that we already knew and loved . thus , we said , " we are pretty sure this equation is the right one to use . we assumed this particle we invented was a photon , and it resulted in the equations and laws we have in the real universe . so this must be the way it actually is ! " . having said that , we can never actually observe the photon as it mediates the force . this is simply because if we were to observe the photon , it would no longer be able to mediate the force because we have no method of observing a photon without destroying it . i have already explained how they are created , how they are destroyed is much simpler . when a photon hits something , it can either reflect , transmit , or be absorbed . the latter is destroying it . when it is absorbed , this means whatever it struck ( usually an electron ) absorbs all of the energy of that photon . that is it . how we know they exist . . . we know because we can do experiments with just one photon . we can see the effects of one photon . but most importantly , theoreticians say , " if a photon did not actually exist , what would happen in some experiment ? well jim , we would see outcome . and if they do exists , we should see different outcome . " then experimentalists perform the experiment and 10 times out of 10 we always see the outcome predicted by the existence of photons . this particle/wave packet is called a " photon " because it was first theorized specifically about light . i believe " photo " is from the greek word meaning light and the extra n was added because all of the particles known at that date ended with an n ( proton , electron , neutron . why not photon ? ) i am willing to bet i have missed something important , so let me know . if i can merge what i have missed with the general form of the answer , i will be happy to put it in .
in principle yes . but to make a good $\text{zn}_3\text{n}_2$ homojunction led you need the capability to incorporating both p-type and n-type dopants ( normally oxide materials are naturally n-type ) which might not be possible . from what i have read , this material has been proposed as a way of making p-type zno ( which is naturally n-type ) by a post growth annealing step . this means that via $\text{zn}_3\text{n}_2$ you maybe able to make a p-zno/n-zno homojunction led . more info here , http://pubs.rsc.org/en/content/articlehtml/2013/ra/c3ra46558f .
liquid nitrogen boils when it comes in contact with skin , so small amounts of spatter are no danger at all-- the droplets just bounce off . i regularly pour a liter or so ( a bit at a time ) out on a lab table when i do liquid nitrogen demos , with no problems or safety gear . the biggest risk from the low temperature is getting it into fabric of some sort , which will hold it in closer proximity to skin for longer than the drops by themselves will . i have , on occasion spilled some on my pants , which is annoyingly cold , but not too bad unless you are wearing really tight clothing . really , the biggest hazard from any of the nitrogen demos i do is not the temperature but the expansion . when it boils , it expands to something like 700 times the volume of the liquid , so if you put some in a sealed container , it can make a big bang . i know of a case where a grad student at mit destroyed a bathroom with a 2-liter bottle of liquid nitrogen . you can use this expansion for a kind of cool demonstration if you take one of those little dropper bottles with the angled spouts that are common in chem labs , and seal a little nitrogen inside . put it down on the floor , and it spins like a firework . the problem with that is , you almost always end up getting a little water vapor condensing in the spout , which plugs it up , and then the bottle will go bang . i had a student a few years ago who had one go off in his hand , and he said it stung pretty badly . i have seen videos of people ( jearl walker in particular ) " drinking " liquid nitrogen by taking a small amount into their mouth , and holding it there for a second or so-- the instant boiling will keep it from giving you oral frostbite for a little while , and you can spit the liquid out after breathing out over it , which makes an enormous plume of steam . it is really cool to see , but kind of risky to do . i have never had the guts to try it myself .
i think a very good orignally german qm book is : straumann : " quantenmechanik : ein grundkurs über nichtrelativistische quantentheorie " there is also a second volume : straumann : " relativistische quantenfeldtheorie " another good book is from g . grawert : quantenmechanik you may also have a look at thirrings books about qm ( it is mathematically more advanced ) . i think straumanns book is not " canonical " . often used by students are the books by nolting and greiner . from the experimental point of view you may have a look at demtröders book .
think about this : a function that maps points on a 2d space to numbers can describe the shape of terrain , but i would not say that it is the terrain . in the same way , a mapping of points to objects ( scalars , vectors , tensors , etc . ) is the mathematical description of a field , but if you think of the field as just the mapping , you are missing out . fields can have various physical properties . for example , just as a particle can have a certain amount of energy , so can an electromagnetic field . the difference is , since the field is spread throughout space , so is the energy ; therefore , it makes more sense to talk about the density of energy rather than the amount . same applies for momentum , or any other physical quantity carried by the field . just as the field could be described by a mapping of points to vectors , so the energy density can be described by a mapping of points to numbers . given the vector value of the field at any point , you can calculate the numeric value of the energy density at that point . but remember that these numbers ( i.e. . the mapping ) are just a mathematical description of the energy density . now , you may notice that the mapping that describes the energy density ( $u ( x ) $ ) satisfies the naive definition of a mathematical description of a field : it associates a number with each point in space . but physicists would not normally call that mapping a " field , " because in a sense , it is not really independent . mathematically , you can calculate $u ( x ) $ from $a ( x ) $ ; physically , the energy " field " is completely determined by the em field . in physics , we tend to reserve the term " field " to talk about something that can not be obtained by a simple calculation from some other field . does that mean , that the " numbers " that make up the field in each point in space stay constant with temperature or time ( or both , i am not sure . ) i am not sure how you got that from the quote you listed . . . no , the numbers that make up the mathematical description of the em field do not stay constant with either time or temperature . in fact , one of the things that characterizes a physical field is that it has dynamics - mathematically , this means that the numbers ( or whatever ) making up the field change with respect to time and space , but in a predictable manner which can be described with differential equations . but there are things you can calculate from a field that do stay constant . for instance , you can calculate the total energy stored in the field by calculating the energy density and then integrating it over the volume of the field . you could also calculate the temperature of the field , by some mathematical procedure . in many cases , these quantities are more closely based on the manner in which the field changes than the actual values that describe it . ( in fact , in some sense , it turns out that you can describe a field by the way that the numbers change , just as well as you can with the numbers themselves . read up on the fourier transform and momentum space if you are interested . )
consider a ball of water floating in zero g , as demonstrated on the iss . ignoring for the moment the surface tension of the water ( i will come back to that ) the pressure inside the water is the same as the pressure of the air around it . this is simply because without any forces , like gravity , acting on the water there is nothing to cause a pressure gradient . well the pressure is not quite the same . in my first paragraph i mentioned that the air-water interface has a surface tension , and this acts a bit like the elastic of a balloon . it compresses the water inside the drop and increases the pressure slightly . the hyperphysics site has an article deriving the pressure and the result is that the extra pressure is given by : $$ \delta p = \frac{2\gamma}{r} $$ where $\gamma$ is the surface tension , which is about 0.07 n/m for pure water , and $r$ is the radius of the ball of water . if we take a radius of 1 cm we get $\delta p = 14$ pa . this is negligable compared with the atmospheric pressure of $101,325$ pa , so it is a very good approximation to say the pressure inside the ball of water is the same as the pressure of the air around it .
answering my own doubts in order:- no . it got to the correct answer , but was wrong . i think she was getting confused between cause ( force ) and effect ( acceleration ) . when the train brakes , the ball and train acquire a relative acceleration . no other force comes into play when one 's inertial frame of reference is the train 's frame . she was basically saying $ a_{iron} = a_{rubber} $ and $ m_{iron} &gt ; m_{rubber} $ , so therefore $ f_{iron} &gt ; f_{rubber} $ which is correct . but then she also says that the iron ball will move a longer distance which implies that both the balls will eventually come to a stop while simultaneously claiming to ignore air resistance and friction . so then which force eventually stops the balls ( as otherwise they will keep moving at the constant speed otherwise as dictated by one of newton 's laws of motion ) ? none that i could think of ( which are probably all of them considering this is such a simple question ) . not mentioned , but probably not . in most idealized physics questions ( especially textbook ones ) , air resistance is not something to be considered . this is to some extent implied by the phrase " on a smooth floor " . if we do not consider it , then no external forces will be acting on the ball ( barring friction ) and both the balls will move at the same speed till they like slide off the train 's surface or hit a wall or something meaning this was a trick question ( which seems atypical as it is not the modus operandi of the crappy , unenlightening educational system ) . if we do consider it , then b'coz of the reasons in the ( my ) original answer , the iron ball will have a slight greater speed than the rubber ball and thus will move a bit further . probably not as the question mentions the phrase " a smooth floor " . although it could be said that while this would affect the absolute values of friction , it would not change the relativity , i.e. , the fact that the rubber ball will experience more friction that the iron one . also , considering it would mean we should probably also consider air resistance . and then the question becomes unsolvable as there will be unknown variables required then . for that and the reason above , it should probably not be considered . 10x , @nathaniel and @tromik .
but how do you calculate the g factor of a point particle or an extend particle ? this is done for a point particle , and any experimental deviation from the calculated value for a point particle would suggest structure beyond a point particle . dirac theory predicts g=2 . anomaly from g=2 has qed , hadronic and weak contributions , which are each calculated . the hadronic and weak contributions are small and considered to be well understood . the qed contribution to the anomaly is the main contribution and extremely difficult to calculate . hundreds of feymann diagrams are involved . see new determination of the fine structure constant from the electron g value and qed for more information .
when it is explained as opposite , it is usually the heat released when a crystal is formed , and since the word " released " already makes up for the sign of heat ( i.e. . if heat is -10 that means the heat released or evolved is +10 ) , it is taken as positive . nonetheless , some books also present a negative value to it but it does not matter unless you are careful about it in calculations .
in this context , a " current " is an object obeying an affine lie algebra , also called current algebra and a special case of a kac-moody algebra . it is an algebra formed by unit weight operators : take for example a current $j^a ( z ) $ , where $a$ is a label and $z$ is a complex coordinate . the algebra is given by $$ [ j^a_n , j^b_m ] =i{f^{ab}}_cj^c_{n+m}+mkd^{ab}\delta_{n+m} , $$ where $$j^a_n=\frac{1}{2\pi i}\oint dz \ , z^{- ( n+1 ) }j^a ( z ) . $$ the integer $n$ denotes the mode number , the integer $k$ is the level and $d^{ab}= ( t^a , t^b ) $ defines the inner product between generators . the word " boundary " refers to the fact that the symmetry group underlying the algebra preserves a certain structure at the boundary of the geometry at infinity . in the case of the paper you are reading , the symmetry group is $u ( 1 ) $ and the boundary is given by $\mathcal{i}^+$ . additional information : affine lie algebras play a role in string theory/conformal field theory , where they can be used to generate states in certain representations of a group . for example , the state $$j^a_{-1}\tilde{\alpha}^{\mu}_{-1}|0\rangle$$ corresponds to a massless vector $a^{\mu a}$ in the adjoint representation of the underlying group ( $\tilde{\alpha}^{\mu}_{-1}$ is a creation operator ) .
hints to the question ( v5 ) : op correctly imposes two conditions because of the delta function potential at $x=-a$ , but op should also impose the boundary condition $\psi ( x\ ! =\ ! 0 ) =0$ because of the infinite potential barrier at $x\geq 0$ . there is zero probability of transmission because of the infinite potential barrier at $x\geq 0$ . ( recall that transmission would imply that the particle could be found at $x\to \infty$ , which is impossible . ) hence there is a 100 percent probability of reflection , cf . the unitarity of the $s$-matrix . see also this phys . se answer . as op writes , away from the two obstacles , one has simply a free solution to the time-independent schrödinger equation , namely a linear combination of the two oscillatory exponentials $e^{\pm ikx}$ . this solution is non-normalizable over a non-compact interval $x\in ] -\infty , 0 ] $ . to make the wave function normalizable , let us truncate space for $x&lt ; -k$ , where $k&gt ; 0$ is a very large constant . so now $x\in [ -k , 0 ] $ . one may then define and calculate the probability $p ( -a \leq x\leq 0 ) $ of finding the particle between the two barriers via the usual probabilistic interpretation of the square of the wave function . if we now let the truncation parameter $k\to \infty$ , then we can deduce without calculation that this probability $p ( -a \leq x\leq 0 ) \to 0$ goes to zero .
the ideal kelvin boat wake ignores surface tension , and it assumes deep water waves with an ( in general ) broad spectrum of frequencies $\omega$ with dispersion relation $\omega^2=gk$ , where $g\approx 9.8 \frac{m}{s^2}$ . the ideal kelvin wake furthermore assumes that the ship sails with a constant velocity , and that the wave amplitudes of the partial waves are so small that they obey a linear superposition principle . the kelvin wake does not describe the narrow turbulent band behind a ship , nor shock waves . the kelvin wake consists of two types of waves : transverse and divergent waves . there are two characteristic angles $$\alpha\approx 19^{\circ} \qquad \mathrm{and} \qquad \beta\approx 35^{\circ} , $$ corresponding to $$\tan ( \alpha ) = \frac{1}{2\sqrt{2}} \qquad \mathrm{and} \qquad \tan ( \beta ) = \frac{1}{\sqrt{2}} , $$ or equivalently , $$\sin ( \alpha ) = \frac{1}{3} \qquad \mathrm{and} \qquad \sin ( \beta ) = \frac{1}{\sqrt{3}} . $$ in polar coordinates $ ( r , \theta ) $ of a co-moving coordinate system , where the position of the boat is at the origin , the transverse waves are in the region $|\theta|\leq \beta$ , and divergent waves are in the region $\alpha\leq |\theta|\leq \beta$ . the angles $\alpha$ and $\beta$ are constant in at least two ways : firstly , they do not depend on the distance $r$ to the ship . this is because the speed of each partial wave ( with frequency $\omega$ ) is independent of the position $ ( x , y ) $ . secondly , $\alpha$ and $\beta$ are , evidently , universal angles , independent of , for instance , $g$ . this is explained in the references below . references : 1 ) howard georgi , " the physics of waves " , chapter 14 . ( hat tip:user1631 . ) 2 ) mit on-line open course ware , mechanical engineering , wave propagation , lecture notes , fall 2006 , chapter 4.7 . 3 ) wikiwaves .
yes , single photon radio waves have been constructed . radio communication is now possible on the most elementary level : scientists at the eth zurich and the max planck institute for the science of light in erlangen have used two molecules as antennas and transmitted signals in the form of single photons , i.e. light particles , from one to the other . since a single photon usually has very little interaction with a molecule , the physicists had to use a few experimental tricks for the receiver molecule to register the light signal . a radio connection established via individual photons would be ideal for various applications in quantum communication – in quantum cryptography or in a quantum computer , for example . the discussion in this question should enlighten you , and the links given there . there is a one to one correspondence between classical and quantum electrodynamics , both rely on the maxwell equations as you will see if you read the detailed answer . in the link in my answer the way one goes mathematically from one to the other is shown . no need for extra experiments since the mathematics is rigorous . for low energies of the electromagnetic waves it is not very smart to use all the mathematical panoply if one can do the job with the classical representation , except as in this case of single photon transmissions .
i do not think it is fair to say " the higgs looks like it is going to be at higher energies then anticipated " . in fact , my money was on 160 gev , based on models coming from noncommutative geometry . but the basic constraints on the higgs mass from the standard model were really not very good ( the following comes from the review article of djouadi , 0503172v2 ) . short story : unitarity starts failing around 900 gev , perturbation theory fails around 700 gev . a lower bound can be gotten by requiring that the higgs quartic coupling remain positive , which gives $m_h&gt ; 70$ gev . this depends on the cutoff-scale ; the 70 gev comes from assuming a 1 tev cutoff scale . if the sm is valid up to gut scales , this rises to $m_h&gt ; 130$ gev . so , although the current values for higgs are actually just below that 130 gev , i think it is not fair to make any statement except that " it seems fine for the standard model " - it is too early to say " the higgs mass implies new physics " . all of these estimates are based on measured parameters such as the top mass , which has it is own uncertainty associated with it . there is also the fine-tuning problem , but the above bounds generally give the same or slightly better estimates then that . if someone wants to mention susy implications of a $\sim 125$ gev higgs , be my guest - there certainly are some . but susy can not possibly be real anyway , so i am ok not knowing them ; - )
conservation of energy refers to systems looked from the same reference frame , it does not make sense to require that energy of the same system to be the same in different reference frames . as a consequence of time translational symmetry , energy conservation is usually true unless we drive the system externally which may break this symmetry . similarly , momentum conservation is a consequence of space translational symmetry . the ( invariant ) mass $m$ is the same in all inertial reference frames , on the other hand , energy $e$ and momentum $p$ are connected through the famous equation \begin{equation} e^2= ( pc ) ^2+ ( mc^2 ) ^2 \end{equation} where $c$ is the speed of light . this equation is valid in any inertial reference frame , to go from one frame to another , one has to do lorentz transformation of both energy and momentum , and it turns out the final result is that the changes in energy and momentum compensate each other and validate this equation in every frame . for the example you gave , if there is only that ball in the universe , in reference frame a , it cannot stop by momentum conservation . if it stops , you have to exert an external force , which may explicitly change the energy of this ball even in reference frame a . then from frame b , roughly speaking , you exert a force ( you may want to work out the transformation of the force between these two frames ) to the left direction and the ball gains energy , so nothing is wrong .
yes , you can simply set $m=0$ in the fierz-pauli equation ( if it is written correctly : ) ) . the only thing to remember is that at $m=0$ it becomes gauge invariant under $\delta h_{\mu\nu}=\partial_\mu\xi_\nu+\partial_\nu\xi_\mu$ . it is this gauge invariance that reduces properly the number of degrees of freedom . from the gravity point of view the gauge invariance is nothing but linearized diffeomorphisms .
while there is a general consensus aligned with the big bang theory 's historical and current stages of the universe . to note , there are three theories with focus on this topic regarding the future , namely : the open universe , flat universe and closed universe theories . ultimately , the fate of the universe depends on the outcome of the competition between the expansion and pull of gravity . basically : the open universe model carries the idea that the universe will continue to expand indefinitely . the flat universe model says that the universe will continue to expand at an ever decreasing rate that approaches zero as time reaches infinity . the consequence of both of these is that the universe will eventually become a very , very dark and lonely place , with all galaxies , stars , et cetera , being so far away from any other , or burned out , with all original hydrogen used up ; we can forget about our ancestors observing andromeda , and the like - humans watching the sky would see nothing but blackness , voids in the distance . the closed universe model says that our universe will not proceed to expand forever , but that gravity will slow down the outward expansion to an eventual halt before beginning to collapse back inward on itself . if this is the case , the fate is a big crunch , where , as the universe contracts , galaxies fall inwards toward each other , wreaking catastrophe , until all matter is as it once was : crushed into an extremely hot , super dense state . there is also the oscillating universe model which is a variant that says another big bang would then occur , resulting in a brand new universe born out of the same matter . what we have to understand is that the best model is the one that agrees best with observations ( for instance , the steady state theory contradicts observations and so is largely discounted . ) it would be nice if we could observe the universe as it was billions of years ago , but alas , we can not do that directly . so , we use other methods such as looking at far away places in space at different distances - this way we can measure redshifts , but our instruments still are not sufficiently evolved to provide precise measurements at such distances - now , the data used to check against cosmological models are wrought with uncertainties ; this means we need still yet more information to fully determine which of the theories are correct .
is there any physical system which can be kept in mind as a simple example of the same ? yes . consider a single spin $1/2$ particle , like an electron . in this case , the matrix will be $2$-by-$2$ since its a representation of $\mathrm{su} ( 2 ) $ acting on the two-dimensional spin-$1/2$ hilbert space . the idea here is that when you rotate the physical system by a rotation $r$ say , then the spin state " rotates " as well ( the states in the hilbert space " rotate " into each other if you will ) as follows : \begin{align} |\tfrac{1}{2} , m'\rangle\longrightarrow d^{1/2}_{m , m'}|\tfrac{1}{2} , m'\rangle \end{align} in fact , for a rotation by an angle $\theta$ about the unit vector $\mathbf n = ( n_x , n_y , n_z ) $ , we have \begin{align} ( d^{1/2}_{m , m'} ) =\begin{pmatrix} \cos\frac{\theta}{2}-in_z\sin\frac{\theta}{2} and ( -n_y-in_x ) \sin\frac{\theta}{2} \\ ( n_y-in_x ) \sin\frac{\theta}{2} and \cos\frac{\theta}{2}+in_z\sin\frac{\theta}{2} \\ \end{pmatrix} \end{align} so that when $\theta = 0$ , this is the identity matrix ; nothing happens to the state , while when $\theta = 2\pi$ ( a full rotation ) , this matrix is $-1$ times the identity , and the state therefore rotates into itself multiplied by $-1$ ; pretty strange is not it ? a general explanation of the idea of irreps , beyond just the wigner-d matrix , would be appreciated . this is an extremely broad question . the general idea behind a representation of a group $g$ is that it is a mapping that assigns an invertible matrix $d ( g ) $ to each group element $g$ such that the group structure is preserved ( the technical term for this is that it is a group homomorphism ) . the representation is said to be irreducible if it has no nontrivial invariant subspaces . concretely , this means that there is no similarity transformation that puts all of the representation matrices into block diagonal form .
the geometry of special relativity is called lorentzian geometry , or in full : the " pseudo-riemannian geometry of minkowsk spacetime " . this is also the cartan geometry of the lorentz group inside the poincar&eacute ; group . see on the nlab at lorentzian geometry for further pointers . see the references there for introductions and surveys .
it goes a little something like this : \begin{align*} \delta g_{ab} ( \sigma ) and =g_{ab}^{\zeta} ( \sigma ) -g_{ab} ( \sigma ) \\ and =\exp ( 2\omega ( \sigma-\delta\sigma ) ) \frac{\partial ( \sigma^c-\delta \sigma^c ) }{\partial \sigma^a}\frac{\partial ( \sigma^d-\delta \sigma^d ) }{\partial \sigma^b}g_{cd} ( \sigma-\delta \sigma ) -g_{ab} ( \sigma ) \\ and \approx ( 1+2\omega ) ( {\delta^c}_a-\partial_a\delta \sigma^c ) ( {\delta^d}_b-\partial_b\delta \sigma^d ) ( g_{cd} ( \sigma ) -\delta\sigma^e\partial_eg_{cd} ( \sigma ) ) -g_{ab} ( \sigma ) \\ and \approx 2\omega g_{ab} ( \sigma ) -\partial_a\delta\sigma_b-\partial_b\delta\sigma_a-\delta\sigma^e\partial_eg_{ab} ( \sigma ) . \end{align*} the last expression we recognize as the lie derivative of the metric along the vector field $\delta\sigma^a$ . what you wrote down is an equivalent form using the covariant derivative . p.s. you made it to the hardest chapter : )
the heat equation for these kind of problems ( assume 1d ) , reads $$\frac{\partial t}{\partial t} = a \frac{\partial^2 t}{\partial x^2} $$ where of course $t$ is temperature , $t$ is time , $x$ is position and $a$ the thermal diffusivity : $a=\frac{\lambda}{\rho c_p}$ ( respectively thermal conductivity , density and heat capacity . this equation can be derived from fourier 's law . suppose you have a block at temperature $t_0$ which you put in contact with a block of temperature $t_1$ at $x=0$ . then you have a set of boundary conditions $$ t ( x , 0 ) = t_0 \\ t ( 0 , t ) = t_1 \\ t ( x\to\infty , t ) =t_0$$ it is not easy , but it has been derived that the solution to this equation is $$\frac{t-t_0}{t_1-t_0}=1-\frac{2}{\sqrt{\pi}}\displaystyle\int_0^{\frac{x}{2\sqrt{at}}} e^{-s^2}ds$$ where the solution to this integral is referred to as error function this solution describes the transient and spatial profile of the heated piece of material . for short times , only a certain amount of the material is heated . using the error function , one can define the penetration depth , which is $x_p=\sqrt{\pi a t}$ , which is obviously a measure for how far the increased temperature ranges into the material . suppose you domain has a finite length and is isolated at the other end . the heat transfer coefficient from the wall at $x=0$ is nearly constant , and the average temperature of the block will converge with an exponential decay to the boundary temperature ( see vladimir 's answer ) . this can be derived from the heat equation by assuming that $\frac{t-t_0}{t_1-t_0}=1 - f ( t ) g ( x/l ) $ note : this book was used as a reference for some of the equations .
your second equation , $p ( \nu , t ) = \frac{2 h {\nu}^3}{c^2}$ $\frac{1}{\exp\bigl ( \frac{h \nu}{kt}\bigr ) - 1}$ is what is commonly referred to as planck 's law for radiation , although a more standard symbol used is $b_\nu ( t ) $ . this is the energy radiated per time , per area , per frequency interval , per steradian . it is a formula for the ' specific intensity ' of a source , which intuitively is the energy flux along a ray of radiation in a given direction , and so you must normalize by the solid angle subtended by that ray . to get the total energy per time per area radiated by a patch of a black body , integrate over solid angle and over frequency . be careful performing the solid angle integral , however , because you must include the geometric factor $\cos \theta$ that accounts for the projected area of the patch ( $\theta = 0$ corresponds to a ray emitted in the normal direction ) . rays leaving one side of a patch can only be directed into the upper hemisphere of the solid angle sphere . so the solid angle integral looks like this : $$ f_\nu = 2 \pi \int_0^{\pi/2} b_\nu ( \theta ) \ , \cos\theta \ , \sin \theta \ , d \theta$$ the $2 \pi$ out in front is for the azimuthal angle . here , $f_\nu$ is what is commonly referred to as the specific flux ( 'specific ' because it is still per unit frequency interval ) . then , either by reading up on the riemann $\zeta$ function , or just using a computer to tell you the answer , you can perform the frequency integral and get $$ f = \sigma \ , t^4$$ here $f$ is what we commonly think of as the flux ( energy per area per time ) , and $\sigma$ is the stefan-boltzmann constant , $$\sigma \equiv \frac{2 \pi^5 \ , k_\mathrm{b}^4}{15 \ , h^3 \ , c^2}$$
there are some rules about what happens to light rays passing through lenses , which are derived from snell 's laws . in short : a ) a ray passing through the focal point into the lens will exit the lens parallel to the optical axis . b ) a ray passing straight into the center of the lens ( at any angle with respect to the optical axis ) will exit at the same angle . c ) a ray entering the lens parallel to the optical axis will exit the lens and pass through the focal point . these are standard in any intro physics text , but what the heck i will draw a simple diagram : the object is on the left , i have labeled the three rays appropriately , and the image is on the right ( focal points are dots ) . as you can see , the rays a ) and b ) are incident with the lens at an angle with respect to the optical axis , which should answer your question . the same rules apply for concave lenses as well , and also curved mirrors if you make the appropriate adjustments .
the gravitation lens generates smaller angular deflections at large impact factors . a converging lens generates larger angular deflections at larger impact parameters . so , a first guess would be " no way " .
you are right . the thermal equilibrium will eventually be reached . in this process , heat is transferred from the water to the thermometer . this increases the temperature of the thermometer and decreases the temperature of the water until they are equal . however , generally , the amount of water is large so that the heat it loses is too small to significantly change its temperature .
my memory of peskin and schroder is a little hazy , but they are probably discussing the shifman-vainshtein-zakharov sum rules . the idea is that you can use the opes for composite operators representing mesons/hadrons to derive formulae that express meson/hadron n-point functions in terms of the vevs of various qcd condensates . ( edit : just discovered that shifman has some very nice lecture notes on the subject . ) more generally : opes are always relevant ( if not always easy to use in a given situation ) because they carry almost all the information about a field theory . you can actually define a qft by writing down the set of local observables , the opes between them , and the vevs of the local observables . it is as good a formalism as the hamiltonian or path integral formalisms -- better in some ways , because it applies when the path integral does not .
it is a common misconception to think that because the higgs mechanism is the origin of mass it is also the origin of gravity . this is a misconception because the origin of gravity is not simply mass . instead it is a quantity called the stress-energy tensor . the stress-energy tensor is usually represented as a 4 $\times$ 4 matrix containing 10 independant entries ( 10 not 16 because the matrix is symmetric ) and in most cases the only significant entry is the top left one , $t_{00}$ , which gives the energy density . the key point is that as far as the stress-energy tensor is concerned mass and energy are the same thing related by einstein 's famous equation $e = mc^2$ . immediately before the electroweak transition all particles were massless , and immediately after they had a finite mass , but this change did not make any difference to the stress-energy tensor and therefore to gravity . before and after the transition the energy density was the same ( well similar anyway ) so the contribution of the particles to the stress-energy tensor and therefore to gravity was the same . so there is no contradiction between the higgs mechanism and the idea of entropic gravity .
yes . you have probably heard that string theory predicts the universe is 10 dimensional ( and m-theory predicts it is 11 dimensional ) while we only see 4 dimensions . however this is not because the number of dimensions has changed , but because 6 ( or 7 ) of the dimensions are rolled up into a very small circle . having said this , there have been suggestions that the universe started out as 2 dimensional ( 1 space and 1 time ) then the number of dimensions increased to 10/11/whatever as the universe evolved . however the idea comes from causal dynamical triangulation , and this is pretty speculative even by the standards of quantum garvity theories . there are even wilder suggestions that the spacetime dimension might be fractal .
in special relativity , you think of a 4-dimensional space-time . the key point here is that two events , 1 , and 2 happening at $t_{1} , x_{1} , y_{1} , z_{1}$ and $t_{2} , x_{2} , y_{2} , z_{2}$ have a distance given by ${}^{1}$ $$ ( \delta s ) ^{2} = -c^{2} ( \delta t ) ^{2} + ( \delta x ) ^{2} + ( \delta y ) ^{2} + ( \delta z ) ^{2}$$ now , we can therefore give any two events a unique relation to each other : 1 ) $ ( \delta s ) ^{2} &lt ; 0$: these events are considered timelike seperated . 2 ) $ ( \delta s ) ^{2} = 0$: the events are lightlike seperated 3 ) $ ( \delta s ) ^{2} &gt ; 0$: the events are spacelike seperated the key point is that , in all reference frames , timelike and null events happen in the same order ( this is derivable from the fact that all observers follow timelike paths ) . but , you can also show that there is no unique ordering of spacelike events--events that happen in order 1 > 2 > 3 in frame a may happen in order 2 > 1 > 3 in frame b . in fact , for any two spacelike seperated events , it is possible to find a reference frame where you can reverse the order in which they happen . so , the answer to your question is " maybe " , but the trick would have to be that you throw the second dart much more quickly than the first , and it hits the dartboard a distance x a way at a time t later than the first , in such a way that $t &lt ; x/c$ . you do this , then there will be a reference frame where it appears that the second dart hits first . ${}^{1}$here $\delta t = t_{2} - t_{1}$ , etc
there are lots of possibilities , depending on the energy of the antiproton beam . the hadron spectrum is quite complicated . probably the most likely channel is pion production : $$ \bar p \to \bar n + \pi^- . $$ this reaction requires a " spectator " nucleus to exchange energy and momentum with the $\bar p$ , and so might be more properly written as $$ a + \bar p \to a^* + \bar n + \pi^- $$ where by $a^*$ i mean that the spectator nucleus might also end up in an excited state . the negative pions will eventually either decay ( mostly $\pi^-\to\mu^-+\bar\nu_\mu$ ) or be captured on another nucleus in a reaction like $$ \pi^- + p \to n . $$ that is not the only available channel : with a spectator nucleus , you can make other antibaryons and mesons , for instance \begin{align} \bar p and \to \bar\delta + \pi and and \text{ ( which could make $\pi^\pm$ or $\pi^0$ ) } \\ \bar p and \to \bar\lambda + k^- \\ \bar p and \to \bar\sigma + k and and \text{ ( could be a $k^0$ or a $k^-$ ) } \\ and \vdots \end{align} here 's a review of low-energy nucleon-antinucleon interactions , which i have not yet read .
imagine what happens when $\delta \phi$ keeps increasing to make a full rotation of $360^ \circ$ . then the angle of $p_2$ increases by $360^ \circ$ so that $p_2$ comes back to $p_1$ . also we know that after the full rotation $\vec{v}_1$ must be equal to $\vec{v}_2$ again . since $\vec{v}_2$ is going around in a circle at the same time $p_2$ does , its angle with $\vec{v}_1$ seems like it should be the same as the angle $p_2$ makes with $p_1$ . more rigourously , the direction of $\vec{v}_1$ is just the direction of $p_1$ rotated by $90^\circ$ . similarly the direction of $\vec{v}_2$ is just the direction of $p_2$ rotated by $90^\circ$ . then since the difference in angle between $p_1$ and $p_2$ is $\delta \phi$ , and $\vec{v}_1$ and $\vec{v}_2$ are essentially rigidly rotated copies of $p_1$ and $p_2$ , the angle between $\vec{v}_1$ and $\vec{v}_2$ must also be $\delta \phi$ .
i know little about baseball , but if you are willing to extend your question to include god 's own sport of cricket , the fastest speed recorded by a bowler was 100.3 mph . this speed was recorded at the moment when the ball leaves the bowlers hand ( the ball slows down once it is released ) so the bowler 's hand was indeed travelling at 100.3 mph . as alexander points out , you can attain these high speeds using leverage . a typical fast bowler 's arm is around a meter long from the shoulder joint to the cricket ball , and the rules demand the arm be kept straight during bowling . dividing the speed by the circumference of the circle traced by the arm give the rotation speed as one rotation in 140ms , which is not superhumanly fast . it is certainly faster than i can manage , but then i am not paid millions of pounds per year to play cricket !
provided the intervals between all events are spacelike they can appear in any order . see this article for a popular science level description , or this paper for the full details .
your procedure gives : $$ a_{xz} = \sqrt{a_x^2 + a_z^2} $$ then : $$ a_{total} = \sqrt{a_y^2 + a_{xz}^2} $$ but if you substitute for $a_{xz}$ in the second equation you get : $$ a_{total} = \sqrt{a_y^2 + ( \sqrt{a_x^2 + a_z^2} ) ^2} = \sqrt{a_y^2 + a_x^2 + a_z^2}$$ so you do not need to split the calculation into two steps . your accelerometer may already exclude the acceleration due to gravity . if it does not then yes you need to use the inclination to work out the three components of gravity then subtract them from $a_x$ , $a_y$ and $a_z$ . it is hard to say exactly how to do this without knowing how your phone reports it is inclination . response to comment : suppose you have your device held flat so $a_z$ = -1 . now move the device downwards at and angle of $\theta$ as shown below : assuming it is moving in the $xz$ plane the value of $a_z$ will be decreased a bit and the value of $a_x$ will increase from zero . suppose you are applying an acceleration to the phone of $2g\space cos ( \theta ) $ - you will see why i have chosen this value in a moment . now the values of $a_x$ and $a_z$ are : $$ a_x = 2g\space cos\theta \space sin\theta $$ $$ a_z = g - 2g \space cos^2 \theta $$ you now calculate $a_{total}$ by just squaring and adding as we discussed above to get : $$a_{total}^2 = 4g^2 \space sin^2\theta \space cos^2\theta + g^2 + 4g^2 \space cos^4\theta - 4g^2 \space cos^2\theta $$ and a bit of rearrangement gives : $$a_{total}^2 = g^2 + 4g^2 \space cos^2\theta \left ( sin^2\theta + cos^2\theta - 1\right ) $$ and because $sin^2\theta + cos^2\theta = 1$ the quantity in the brackets is zero so you end up with : $$a_{total}^2 = g^2 $$ that is : $$a_{total} = g $$ which is the same as when the phone is stationary . so it is possible to be accelerating the phone and still have the total acceleration come out as $g$ ( $g$ = -1 in the phone 's units ) . that is why just subtracting one is not a reliable way to tell if the phone is accelerating .
the thing " in between " is an action principle . the most accessible explanation appears in feynman 's the character of physical law . see what&#39 ; s the interpretation of feynman&#39 ; s picture proof of noether&#39 ; s theorem ?
i think you are being a bit hard on scientific american . it is a popular ( if slightly geeky ) magazine so you would not expect its articles to have all the gory details . the best way to find info about areas like this is to search arxiv . org . for example googling for " super wimp site:arxiv . org/abs " finds http://arxiv.org/abs/0812.0432 and this looks like a good place to start . there have been various suggestions for particles that only interact by the gravitational force . one example is the sterile neutrino .
you can define and do the geometry several ways but i would say the reasons are linearity , isometry and handedness ( preservation of left/right handedness : this one is not needed to prove orthogonality so it is a bit more than what you asked for , but it is what sets rotations aside from other isometries ) . handedness is sometimes rather loftily called chirality . intuitively , you need to think of a grid of $x$ , $y$ and $z$ co-ordinates being ruled throughout the space taken up by the rotated object and also think of what happens to drawings and 3d sculptures in that space . after the rotation , all the $x$ , $y$ and $z$ gridlines are still orthogonal and not distorted . distances between all mapped points are the same as what they were before the rotation , and so angles between vectors are left unchanged . we know a rotation leaves at least one point in space fixed . so let 's arbitrarily put our origin at such a point . then the lack of global distortion in our grid shows that the transformation is linear . i say lack of " global distortion " because some nonlinear transformations ( conformal ones ) can also have zero local distortion - little drawings are undistorted - but beget distortion in big enough drawings and sculptures . so , with the origin fixed , our transformation is linear and homogeneous . so our transformation $\mathcal{u}$ can be represented by a matrix $\mathbf{u}$ so that : $$\mathcal{u}:\mathbb{r}^n\to \mathbb{r}^n:\ ; x\mapsto \mathbf{u}\ , x$$ now as discussed above , lengths of positions vectors stay the same , as do angles between position vectors . this means the inner product $\left&lt ; x , \ , y\right&gt ; = x^t\ , y = y^t x$ between any pair of position vectors $x$ and $y$ is unchanged . therefore : $$\left&lt ; \mathbf{u}\ , x , \ , \mathbf{u}\ , y\right&gt ; = ( \mathbf{u}\ , x ) ^t\ , ( \mathbf{u}\ , y ) = x^t\ , \mathbf{u}^t\ , \mathbf{u}\ , y = \left&lt ; x , \ , y\right&gt ; = x^t\ , y$$ or rather : $$x^t\ , \left ( \mathbf{u}^t\ , \mathbf{u} - \mathbf{i}\right ) \ , y = 0 ; \ ; \forall x , \ , y\in\mathbb{r}^n$$ it is now not hard to show , since we can put any pair of basis vectors $x$ , $y$ into the above equation , that we must have $\mathbf{u}^t\ , \mathbf{u} = \mathbf{i}$ as an identity . therefore the matrix must be orthogonal . here naturally $\mathbf{i}$ is the identity matrix . this essentially answers your question , but rotations are not the only orthogonal transformations . reflexions are too ; in $\mathbb{r}^3$ the orthogonal matrix $\operatorname{diag} ( 1 , -1,1 ) $ reflects in the $x-y$ plane and it fulfills $\mathbf{u}^t\ , \mathbf{u} = \mathbf{i}$ . so let 's go a little further . any rotation of angle $\theta_0$ can be thought of as being joined to the identity transformation ( rotation through angle of nought ) through a continuous path of rotations , all about the same axis and with angles between $0$ and $\theta_0$ . it belongs to the identity connected component of the group of all orthogonal transformations . therefore : $$\mathbf{u} = \exp ( \theta\ , \mathbf{h} ) $$ for some constant matrix $\mathbf{h}$ . by imposing the orthogonality condition on the expression we get $\mathbf{u}$ orthogonal iff $\mathbf{h} = -\mathbf{h}^t$ , i.e. $\mathbf{h}$ is skew-symmetric . this then is the general form of an $n$ dimensional rotation : it is a matrix of the form $\exp ( \mathbf{h}_\theta ) $ for some skew-symmetric $\mathbf{h}_\theta$ . in three dimensions , the most general such matrix is : $$\theta\ , \mathbf{h} = \theta\ , \left ( \begin{array}{ccc}0 and \gamma_z and -\gamma_y\\\gamma_z and 0 and \gamma_x\\\gamma_y and -\gamma_x and 0\end{array}\right ) $$ where $\gamma_x^2 + \gamma_y^2 +\gamma_z^2 = 1$ , $ ( \gamma_x , \gamma_y , \gamma_z ) $ is a unit vector defining the axis of rotation , as you can prove by finding the eigenvectors and values of $\mathbf{h}$ and showing that this vector is the eigenvector corresponding to an eigenvalue of 0 ( therefore the exponential of $\exp ( \theta\ , \mathbf{h} ) $ has this vector as an eigenvector and its eigenvalue is $e^0 = 1$ , i.e. it is an axis left invariant by the transformation ) . also note that $\det \mathbf{u} = \exp ( \operatorname{trace} ( \theta\ , \mathbf{h} ) ) = 1$ , as in the comments . this is the last ingredient , namely handedness i spoke of at the beginnning . a reflexion has a determinant of $-1$ and maps a right handed co-ordinate system into a left handed one and contrawise . you can find the wonted expressions for rotation operators using the rodrigues formula grounded on the $\mathbf{h}$ matrix 's characteristic equation : working through this reasoning in 3d : the three eigenvalues of $\mathbf{h}$ are $0 , \ , \pm i$ , so by the cayley-hamilton theorem : $$\mathbf{h}^3= -\mathbf{h}$$ which relationship is then used to simplify the exponential 's taylor series : $$\exp ( \theta\mathbf{h} ) = \mathbf{i} + \theta \mathbf{h} + \frac{\theta^2}{2 ! }\mathbf{h}^2 + \cdots$$ leading to : $$\mathbf{u} = \mathbf{i}+\sin\theta\ , \mathbf{h} + ( 1-\cos\theta ) \ , \mathbf{h}^2$$ whence can be worked out the wonted formulas for a 3d rotation of angle $\theta$ about an axis defined by the unit vector $ ( \gamma_x , \gamma_y , \gamma_z ) $ . in higher dimensions , a real valued skew-symmetric matrix $\mathbf{h}$ has the eigenvalue $0$ ( possibly repeated ) as well as imaginary eigenvalues in conjugate pairs $\pm i\ , \theta_j$ . it should be mentioned here that $\mathbf{u}$ , being orthogonal is also normal ( commutes with its adjoint - here equal to its transpose ) and so it can always be diagonalised ( has a strictly diagonal jordan normal form ) and its eigenvectors are all orthogonal . the rotation then has an invariant hyperspace given by the kernel ( nullspace ) of $\mathbf{h}$ - this is the generalization of the rotation axis in 3d and then one or more linearly independent 2d hyperplanes ( indeed orthogonal hyperplanes , given normalness of $\mathbf{u}$ ) , each spanned by the pair of eigenvectors corresponding to the eigenvalues $\pm i\ , \theta_j$ . so the idea of an " axis " is no longer really useful : in 3d it is useful because the nullspace of $\mathbf{h}$ must be precisely one dimensional . sometimes authors require " rotations " to be transformations that leave all of $\mathbb{r}^n$ invariant aside from precisely one 2d hyperplane , but i do not think this is particularly useful because the composition of two such transformation is not then a rotation ( unless the hyperplane is the same one for the two composed " rotations" ) : there is no group of rotations defined in this way . it is easier and more useful simply to talk of simply orthogonal transformations with unit determinant , i.e. members of the group $so ( n ) $ . if , as michael brown 's comment suggests , you are thinking of representations of rotations , then further discussions of the lifting of $so ( 3 ) $ to its universal ( in this case double ) cover $su ( 2 ) $ can be found in the second section " what the lie bracket does not " remember " about the group : global topology and the fundamental group " in my answer here and especially in the stillwell references my answer gives .
the classical field has a straightforward interpretation in the bosonic case--- it is determined by the density and phase of the superfluid condensate of the particles and both are simultaneously measurable in the thermodynamic limit . if there is no superfluid , the field is zero . where there is a superfluid , it is the square-root of the density , with a phase whose gradient is the local superflow . you can measure the value of psi-squared at x , that is determining the superfluid condensate density , which can be done by shining light through the fluid to get the index . you can also measure the value of the velocity by the exact way light refrects ( or if it is dense enough , by putting little dust specks in the fluid ) . a gaseous bose-einstein condensate , or even liquid helium , is described precisely by the classical limit of this formalism in the thermodynamic limit . if you take the classical limit wrong , by insisting that the particle number n is fixed , you do still get hbars lurking around . in order to take a good classical field limit , you need enormous occupation numbers for the field , which requires that you take n to infinity and the mass to zero keeping the density fixed . in this limit , the conjugate variables density/phase become commuting . as for measuring the " x-projection of operators " , i could not figure out what that meant . you can measure the field momentum too , of course , by just taking the complex conjugate of the measurement of the field ( the imaginary part of the field and the real part are not independent , and you can describe the whole system using only the real part and its time derivative , as described in the wikipedia article ) .
the distance where light has a circular orbit is actually $1.5r_s$ not the event horizon . this distance is known as the photon sphere . in principle a shell observer hovering at this distance could indeed see their own back . the proper distance is indeed just $2\pi r$ , however the object would look bigger than expected because the curvature of spacetime has a focussing effect . light slightly outside the photon sphere will spiral outwards away from the black hole while light slightly inside the photon sphere will spiral inwards towards the black hole . therefore the light will not travel in a straight line . if we unroll the circular object to make it easier to draw the light rays we get something like :
since you are dealing with an inelastic collision , energy is not conserved when the bullet hits the block . you should try to find a relation between the initial velocity of the bullet and the velocity of the combined system ( bullet+block ) after the collision from conservation of momentum .
materials , and certainly materials transparent to light , have few magnetic properties . they are not composed out of atoms that have strong ferromagnetism . but all atoms have strong electric fields . this means that light , as it goes through a transparent medium has small probability to interact with its magnetic field component with the medium , which is mainly transparent to it . take the wire grid polariser as a more simple example it consists of a regular array of fine parallel metallic wires , placed in a plane perpendicular to the incident beam . electromagnetic waves which have a component of their electric fields aligned parallel to the wires induce the movement of electrons along the length of the wires . since the electrons are free to move in this direction , the polarizer behaves in a similar manner to the surface of a metal when reflecting light ; and the wave is reflected backwards along the incident beam ( minus a small amount of energy lost to joule heating of the wire ) . a wire-grid polarizer converts an unpolarized beam into one with a single linear polarization . coloured arrows depict the electric field vector . the diagonally-polarized waves also contribute to the transmitted polarization . their vertical components are transmitted , while the horizontal components are absorbed and reflected . the magnetic component in this setup cannot interact to affect the absorption of the light the way the electric can with the free electrons in the metal of the wire .
it should not be too hard with a van de graaff generator . assuming a generator of radius to the order of a decimeter , we need to generate a potential of $\frac{q}{r} \tilde~ \frac{10^9}{0.1}\tilde~10^{10} v$ to get a charge of one coulomb . that would be rather hard , though if we want a microcoulomb , that can be arranged with a ( van de graaff ) generator capable of producing voltages in megavolts . once we have this charg on the generator , we can transfer it to the electrode via conduction ( which will only transfer a fraction of it ) , or induction ( which will induce an opposite and equal charge on the electrode ) so yes , charges in nanocoulomb/microcoulomb/millicoulomb are not that hard to generate and collect . 1 coulomb of charge -- not so much . note that it is not too hard to have 1c of net charge in some given volume--the earth has some net charge which is probably in coulombs . the issue comes when you have to concentrate it enough to be able to transfer it . i somehow forgot about capacitors . capacitors can store a large amount of charge ( till one kilocoulomb ) though the net charge stored is zero . however , it is generally hard to transfer this high charge elsewhere without neutralizing it or pushing it into another capacitor .
forget the webcam . attach the secondary mirror to the wall , at a height that is near the height of the center of the primary mirror . then adjust the horizontal and vertical tilt of the primary mirror to center the multiple images of the secondary mirror within each other on the primary mirror . if you have a cheap laser pointer and a carpenter 's square , you can set up the laser pointer so that it is exactly square to the wall ( vertically and horizontally ) and then adjust the primary mirror such that the laser beam goes back to the laser .
sorry , the answer to this technical question needs some mathematical technology . the space you are looking for is $h^1 ( \mathbb r ) $ , the first sobolev hilbert space . it is made of the functions in $l^2 ( \mathbb r ) $ admitting weak first derivative represented by a $l^2$ function in turn . $h^1 ( \mathbb r ) $ , is a complex hilbert space if equipped with the scalar product : $$\langle \psi| \phi \rangle := \int_{\mathbb r} \overline{\psi} ( x ) \phi ( x ) dx + \int_{\mathbb r} \overline{\frac{d\psi}{dx}} \frac{d\phi}{dx} dx\qquad ( 1 ) $$ where $d/dx$ denotes the weak derivative ( see below ) . equivalently , $h^1 ( \mathbb r ) $ can be defined as the space of $l^2$ functions $\psi ( x ) $ whose fourier ( -plancherel ) transform $\hat{\psi} ( k ) $ admit finite $l^2$ norm with respect to the measure $ ( 1+ k^2 ) dk$ instead of the simpler $dk$ . indeed it holds , where the scalar product is the same as in ( 1 ) : $$\langle \psi| \phi \rangle := \int_{\mathbb r} \overline{\hat{\psi}} ( k ) \hat{\phi} ( k ) ( 1+ k^2 ) dk \qquad ( 2 ) \: . $$ obviously it also holds : $h^1 ( \mathbb r ) \subset l^2 ( \mathbb r , dx ) $ sticking to work in the physically sensible hilbert space $l^2 ( \mathbb r , dx ) $ for qm , it turns out that $h^1 ( \mathbb r ) $ is the natural domain where the momentum operator is self-adjoint ( not only hermitian or symmtric ) . however , in that hilbert space , the momentum operator , whose correct defintion is : $$p = -i \frac{d}{dx}\quad \mbox{in weak sense , and with domain $d ( p ) = h^1 ( \mathbb r ) $}$$ is always unbounded , i.e. discontinuous . so , to see $p$ as a bounded ( i.e. . continuous ) operator it is not enough to restrict it to an appropriate domain , but you also have to change the topology ( norm ) of the space , passing from that of the simple $l^2$ to that of $h^1 ( \mathbb r ) $ . note 1 . a ( measurable ) function $f : \mathbb r \to \mathbb c$ is said to admit weak derivative $\frac{df}{dx}=g$ , where $g$ is another ( measurable ) function , if , for every $h : \mathbb r \to \mathbb c$ of class $c^\infty$ and compactly supported , one has : $$\int_{\mathbb r} f ( x ) \frac{dh}{dx} dx = - \int_{\mathbb r} g ( x ) h ( x ) dx \: . $$ for instance $f ( x ) = |x|$ does not admit derivative at $x=0$ however admits weak derivative given by $sgn ( x ) $ . the dirichlet function $d ( x ) = 1$ if $x$ is rational $d ( x ) = 0$ if $x$ is not rational does not admit derivative anywhere , but it admits weak derivative given by the zero function . note 2 . if dealing with the particle in $ [ 0,2\pi ] $ the situation is analogous . the self-adjointness domain of $p$ is $h^1 ( ( 0,2\pi ) ) $ and $p$ is defined as before . the only change is that passing in fourier rep . one has to use fourier series instead of fourier transform . in this case the $h^1 ( ( 0,2\pi ) ) $ scalar product becomes : $$\langle \psi | \phi \rangle = \sum_{n \in \mathbb z} \overline{\psi_n} \phi_n ( 1+ n^2 ) \: . $$ in the hilbert space $h^1 ( ( 0,2\pi ) ) $ , $p$ is continuous , otherwise , it is unbounded as usual . what it is true in the statement of your professor , is that the set of $c^1$ functions on $ [ 0,2\pi ] $ with $f ( 0 ) = f ( 2\pi ) $ is included in $h^1 ( ( 0,2\pi ) ) $ .
at the risk of confusing you even more : the value " that would be if the measurement was not made " simply does not exist . take a your favourite simple quantum system , e.g. , the spins of two electrons . the values of the various components and combinations of their spin do not exist before you decide which observables you will measure . it is simply impossible to assign coherent ( =non-contextual ) values to all possible measurements that could choose to make , that do not depend on your choice what you will measure . you may want to read about the kochen-specker theorem , it shows that choosing what you measure does not just " change existing values " of your observables , but " brings them into existence " . see , e.g. , http://en.wikipedia.org/wiki/kochen%e2%80%93specker_theorem http://ncatlab.org/nlab/show/kochen-specker+theorem
the sign is purely based on convention . usually it is just written as $ f_{n} = -mg $ which does point up , but only because we take $g$ to be negative already . the fact that it is based on convention should make things easier for you , since you can chose which representation that you are comfortable with . typically though , if you are not dealing with vectors in vector notation , you do just want to go with the lengths . questions that do not present info to that depth do not merit answers to the same depth . that said , your book leaves out the negative sign because it is indeed only dealing with magnitudes . enjoy !
actually , the answer is a bit more subtle than just density . the principle that is behind floating objects is archimedes ' principle : a fluid ( liquid or gas ) exerts a buoyant force , opposite apparent gravity ( i.e. . gravity + acceleration of fluid ) on an immersed object that is equal to the weight of the displaced fluid . thus , if you have an object fully immersed in a fluid , the total force it feels is given by ( positive sign means down ) : $f = gravity + buoyancy = \rho_{object} v g - \rho_{fluid} v g = ( \rho_{object} - \rho_{fluid} ) v g$ thus , if the average density of the object is lower than that of the water , it floats . if the object is partially immersed , to calculate the buoyant force you have to consider just the immersed volume and its average density : $f = \rho_{object} v g - \rho_{fluid} v_{immersed} g$ note that when i was talking about density , i was talking about the average density of the object . that is its total mass divided by its volume . thus , a ship , even if it is made out of high-density iron it is full of air . that air will lower the average density , as it will increase the volume considerably while keeping the weight almost constant . if you want to understand this better you can give the following problem a try : ) what is the height an ice cube of side l floats in water ?
i will approach this question theoretically , although i feel the intuition follows nicely . if we talk about kerr black holes - rotating black holes described by their mass and angular momentum , with no additional parameters such as charge etc . - then you can show that the radius of the event horizon is given by $\boxed{r=m + \sqrt{m^2-a^2}}$ where $a=\frac{j}{m}$ . ( this value of $r$ is found by finding where the kerr metric blows up ; hence event horizon . in fact , finding where the metric blows up involves solving a quadratic equation , so we get two values of $r$ and in kerr black holes we therefore have two event horizons ; unlike in schwarzschild black holes . ) regarding your first point about maximum angular momentum , if we set $g=1$ and $c=1$ , the maximum angular momentum you stated is given by $a=m$ and if we plug this into our equation for $r$ above we see that we have $r=m$ . we know that the radius of the event horizon in a schwarzschild black hole ( no rotation ) is $r=2m$ . so therefore we can see that at maximum angular momentum , the radius of the event horizon is half of what it would be if the black hole were not spinning . to this end , we can also see that at zero angular momentum , $a=0$ , we have $r=2m$ which is what we want as at zero angular momentum we of course should have the schwarzschild radius . using the boxed equation for $r$ at the top , it is easy to test out different values of $a$ to see what happens to the event horizon . for example , this equation alone is sufficient to show that for $a&gt ; m$ we do not have an event horizon , in which case we have what is a called " fast kerr " which is just a singularity with no event horizon .
i think the short answer is , you do not . the reason we call the unit of force a newton and not a kg m/s$^2$ is because it is convenient and it expresses the relation you want to convey when used elsewhere ( e . g . , $f=-kx$ for a spring ) . similarly , it is convenient to " hide " the mks base units into a single term , the potential $v$ in this case , so that the formula is easier to remember and that the relation is conveyed , in this case the relation between potential difference , current , and resistance .
please be aware that plutonium cores are supposed to be plated with another metal ( nickel or silver , if my memory serves me right ) . machining plutonium is very hazardous and is done with remote manipulators , since it increases risk of inhalation . source : http://toxnet.nlm.nih.gov/cgi-bin/sis/search/r?dbs+hsdb:@term+@na+@rel+plutonium,+radioactive : absorption through the skin can occur through occupational exposure . experiments show that the skin is an effective barrier and the percentage absorbed /seldom/ exceeds 0.05% for intact skin . [ seiler , h.g. , h . sigel and a . sigel ( eds . ) . handbook on the toxicity of inorganic compounds . new york , ny : marcel dekker , inc . 1988 . , p . 724 ] peer reviewed source : plutonium anl factsheet oct 2001 plutonium metal . plutonium isotopes are primarily alpha-emitters so they pose little risk outside the body . here the plastic bag , gloves , and outer ( dead ) layer of skin would each alone stop the emitted alpha particles from getting into the body . what happens to it in the body ? when plutonium is inhaled , a significant fraction can move from the lungs through the blood to other organs , depending on the solubility of the compound . little plutonium ( about 0.05% ) is absorbed from the gastrointestinal tract after ingestion , and little is absorbed through the skin following dermal contact . after leaving the intestine or lung , about 10% clears the body . the rest of what enters the bloodstream deposits about equally in the liver and skeleton where it remains for long periods of time , with biological retention half-lives of about 20 and 50 years , respectively , per simplified models that do not reflect intermediate redistribution . the amount deposited in the liver and skeleton depends on the age of the individual , with fractional uptake in the liver increasing with age . plutonium in the skeleton deposits on the cortical and trabecular surfaces of bones and slowly redistributes throughout the volume of mineral bone with time . what is the primary health effect ? plutonium poses a health hazard only if it is taken into the body because all isotopes but plutonium-241 decay by emitting an alpha particle , and the beta particle emitted by plutonium-241 is of low energy . minimal gamma radiation is associated with any of these radioactive decays . inhaling airborne plutonium is the primary concern for all isotopes , and cancer resulting from the ionizing radiation is the health effect of concern . the ingestion hazard associated with common forms of plutonium is much lower than the inhalation hazard because absorption into the body after ingestion is quite low . laboratory studies with experimental animals have shown that exposure to high levels of plutonium can cause decreased life spans , diseases of the respiratory tract , and cancer . the target tissues in those animals were the lungs and associated lymph nodes , liver , and bones . however , these observations in experimental animals have not been corroborated by epidemiological investigations in humans exposed to lower levels of plutonium . as a note , the common myth that plutonium is the “deadliest substance known to man” is not supported by the scientific literature . it poses a hazard but is not as immediately harmful to health as many chemicals . for example , for inhalation – the exposure of highest risk – breathing in 5,000 respirable plutonium particles , about 3 microns each , is estimated to increase an individual’s risk of incurring a fatal cancer about 1% above the u.s. average “background” rate for all causes combined . ) edit : as an aside : i recommend reading eileen welsome 's the plutonium files : america 's secret medical experiments in the cold war to get some idea of what early plutonium health safety experiments really meant ( e . g . injecting a solution of plutonium salt into a patient 's leg ) .
for what it is worth : http://www.coolmagnetman.com/magmeter.htm - a home-made device based on a hall effect device - for about $40 .
it is because once the higgs couples two weyl fermions together , they become the two chiralities of a massive charged fermion . the standard 4-component spinor formalism disguises how natural this coupling is because it makes every fermion into a dirac fermion , and projects out the unphysical states at every vertex . if you do not do that , if you only include the 2 spinor corresponding to each physical field , the higgs coupling is extremely natural : it is the most general renormalizable gauge-invariant coupling of an su ( 2 ) doublet higgs with hypercharge 1/2 ( in one usual normalization ) to chiral weyl 2-spinors . you can think of a yukawa coupling as a mass term with a scalar field taking the role of the mass . the mass term for a charged fermion has to be of dirac type , because it must be invariant under phase rotations , and only the term $\bar\psi\psi$ is invariant . this means that higgs fields will always couple opposite chiralities of charged massive fermions , i.e. everything in the standard model that couples to the higgs . to see a situation where the coupling is for one chirality only , consider the nonrenormalizable two-higgs two lepton interaction : $$ h h l l $$ where l is the su ( 2 ) doublet left-handed lepton field , h is the higgs field , also an su ( 2 ) doublet , and each l 's su ( 2 ) index is contracted with one of the h 's su ( 2 ) index using the su ( 2 ) epsilon tensor , and the l 's lorentz indices are contracted with each other using the space-time epsilon tensor ( in 2 index formalism ) . this nonrenormalizable term becomes a neutrino majorana mass , and it is suppressed by a large scale , but it is the right order of magnitude to explain the majorana masses . in this case , the two-higgs field is coupled to a single chirality . the allowed couplings are always determined by matching all the su ( 2 ) , su ( 3 ) and lorentz indices together in a 2-spinor formalism , and it becomes the chirality restriction only by coincidence in the standard model . if you had a fundamental su ( 2 ) symmetric tensor higgs field t ( it would be the su ( 2 ) " spin 1" representation , not spin 1/2 ) with twice the hypercharge of the standard model higgs , it could give neutrinos a majorana mass with renormalizable yukawa couplings , just by replacing $hh$ with $t$ above , and contracting the indices the same way .
the argument is not a mathematical taylor expansion , there is an implicit physical argument here which is very well known but not given . the rate of a reaction ( like recombination ) at low density should be the product of the densities , with a coefficient that is independent of the density . this happens to be a mathematical taylor expansion in the densities . but zero is a special point for an expansion , because there are natural power-law change of variables , and you can not determine which power is right without knowing the physics . for example , the period of a pendulum goes to zero as the length of the pendulum goess to zero , does this mean that the period must be proportional to l ? of course not . it is the square-root of l . the correct argument when you have two objects which are moving randomly and have to find each other , the probability each a object finding a b object per unit time is proportional to the density of b objects . if you have twice as many b objects per unit volume , it takes half as long to find one , just from independent search statistics . similarly , the rate is doubled if you double the concentration of a objects . so the leading term is the so-called mass-action term , the product of the concentrations . at higher concentrations , the a 's and b 's start to have excluded volume effects--- it is slightly easier for a to find b because the mass-action assumes indepedent motion of the different b 's while the real b 's only can travel in the volume not excluded by other b 's . there are also corrections to the diffusion rate fron interactions . the corrections are negligible when the gas is dilute compared to the physical cross-section of interaction , and this limit defines the domain of applicability of mass action dynamics . the argument that the molecules are in the mass-action limit is what is used here , and the taylor expansion phrasing is suboptimal . but the result is that the physics of recombination is by the first term in a taylor expansion in the two densities , so the calculation that follows is correct , even though the physical argument is wrong . this is typical of textbooks , and you should probably throw this one away .
linearly - then there is exactly the same number of both spins . this is incorrect . if you have a collection of photons in which half are left hand circularly polarized ( $l$ ) and half are right ( $r$ ) , then you have unpolarized light ( not linearly polarized ) . if you have linear polarized light , then each photon is in a ( quantum ) superposition of r and l at the same time . it is equally true to say that circularly polarized light is in a superposition of horizontally ( $h$ ) and vertically ( $v$ ) polarized light . what this means is if you take circular light and shine it on a linear polarizing beamsplitter , half will go in each path ( i.e. . each photon 's wavefunction will collapse to either $h$ or $v$ polarized with 50% probability ) . however if you tried to measure or seperate the light based on $l/r$ , all the light would be measured in one path . how can be linearly polarized light different , than not polarized at all ? if the light is unpolarized then if you try to measure the polarization in the $l/r$ basis , half the light will be found in each path ( just like linear polarization ) , however if you try to measure $h/v$ , half the light will also be measured in each path in this experiment as well ( unlike linearly polarized light ) .
you surely need to consider einstein 's field equations at some point because the stress-energy tensor in general relativity is defined as the object that is set equal to the einstein curvature tensor ( with the appropriate coefficient ) . so if you vary the whole action $$ \int \left ( \frac{r}{16\pi g} +{\mathcal l}_{\rm matter} \right ) \sqrt{-g}\ , d^d x $$ with respect to the metric , you will simply obtained einstein 's equations that has a multiple of the einstein tensor and a multiple of the stress-energy tensor defined in the way you mentioned . what you could have asked is why this definition of the stress-energy tensor is equivalent to other definitions you may know – i.e. non-gravitational ones . to answer this question , you would have to mention which other definition you mean . well , you would probably mean the stress-energy tensor that is locally covariantly conserved , $\nabla_\mu t^{\mu\nu}=0$ in the context of general relativity . you could derive this stress-energy tensor by the noether procedure by considering spacetime translations in non-gravitational theory , and then by covariantizing the result that you get in some right way . why is the conserved stress-energy tensor the same thing as the stress-energy tensor obtained by varying the metric ? they have to be the same because einstein 's equations set the stress-energy tensor equal to a multiple of the einstein tensor and the latter is covariantly conserved as a matter of identity . the equation $$\nabla_\mu g^{\mu \nu}=0$$ holds identically , for any metric tensor field configuration , and because the einstein tensor is set proportional to the stress-energy tensor , the same condition ( vanishing of the covariant divergence ) has to hold for the stress-energy tensor , too . this is no accident and this fact may be formulated in other ways . for example , we say that gravity has to use the diffeomorphism symmetry ( it is doubly important as the gauge symmetry in the quantum mechanical framework to get rid of the unphysical , negative-norm polarizations of the gravitons ) . such a local symmetry has to be coupled to a " conserved current " ( conserved stress-energy tensor ) for consistency . you will therefore typically end up with the same formula for both stress-energy tensors . however , they may end up differ by certain terms whose covariant derivative is zero identically as well . for example , the non-gravitational conserved stress-energy tensor is not always quite canonical and it does not even have to be a symmetric tensor . the gr definition of the tensor is automatically symmetric .
i think you will find this article ( with pictures ) helpful for dot convention and this one for mutual inductance . let me know if this does not clear it up .
imagine that you are on a train , traveling at a steady speed of 50 miles per hour ( mph ) . your physics textbook on the table in front of you . now , you and the textbook ( and the train ) are all moving at the same speed . to an outside observer standing next to the train tracks , you and the book are each rushing by at 50mph . but , from your point of view , the book is not moving at all . that is , it is not getting closer to or farther from you . you are moving at 50mph relative to the observer next to the tracks . you are not moving , relative to the textbook . the book , the train , and yourself are not moving at all , relative to each other .
at certain positions in the waves , the em field is zero and thus zero energy is stored at those positions . but at other positions , the em field is at a maximum , and those points are local maxima of energy . that pattern of oscillation between zero energy and maximum energy moves in the direction of propagation of the wave but never changes - in particular , the maximum value of the em field ( the amplitude ) stays constant , and there is no time at which the em field is zero everywhere . as for your conclusion from the definition of the poynting vector that the energy disappears at certain times : it is not correct , but i could not tell you why without seeing how you did it . what i can do is show the calculation for an electromagnetic plane wave , defined by $$\vec{e} ( z , t ) = e_0\hat{x}\sin ( kz - \omega t ) $$ the corresponding magnetic field is $$\vec{b} ( z , t ) = \frac{1}{c}\hat{k}\times\vec{e} ( z , t ) = \frac{e_0}{c}\hat{y}\sin ( kz - \omega t ) $$ since i am setting the direction of propagation as $\hat{k} = \hat{z}$ . check that this satisfies maxwell 's equations if you want . the energy density is $$\begin{align} u ( z , t ) and = \frac{\epsilon_0}{2}e ( z , t ) ^2 + \frac{1}{2\mu_0}b ( z , t ) ^2 \\ and = \frac{\epsilon_0}{2}\biggl ( e_0\hat{x}\sin ( kz - \omega t ) \biggr ) ^2 + \frac{1}{2\mu_0}\biggl ( \frac{e_0}{c}\hat{y}\sin ( kz - \omega t ) \biggr ) ^2 \\ and = \epsilon_0 e_0^2\sin^2 ( kz - \omega t ) \end{align}$$ using $\frac{1}{c^2} = \epsilon_0\mu_0$ . this energy density does vary from point to point , but at any fixed time , if you take the average over one cycle , a length $\frac{2\pi}{k}$ , you get $$k\int_0^{2\pi/k} u ( z , t ) \mathrm{d}z = k\int_0^{2\pi/k} \epsilon_0 e_0^2\sin^2 ( kz - \omega t ) \mathrm{d}z = k\epsilon_0 e_0^2 \frac{\pi}{k} = \pi\epsilon_0 e_0^2$$ which does not depend on time . so the average energy density is constant , it does not ever go to zero .
actually that is not too far off the mark , although i am not sure " knot " or " kink " is the best word . quantum field theory , the best theory we currently have to describe particles , says that particles correspond to excitations of a field , which are kind of like waves in water ; you could consider the surface of a pond " excited " whenever it is not flat . just as with water waves , there is an infinite variety of " shapes " you can have for these excitations . for example , you could have a repeating wave , in which the surface of the water cycles up and down over a large area , or you could have just one wave front that just propagates across the water without spreading out very much . the former case is pretty typical for things like light waves , and the latter case is pretty typical for particles of matter , although technically any kind of field ( whether it is the electromagnetic field for light , or a quark field in matter , or whatever ) can have any of the different types of excitations . by the way , according to special ( and general ) relativity , even an object that is standing still is moving through time . so all of these excitations move through spacetime in one way or another . but only certain ones ( the excitations in fields corresponding to massless particles ) can move through space in such a way that they appear to us to be traveling at the speed of light .
i think you are misinterpreting the statement that " it does not have any effect " . this statement does not mean that the faddeev-popov methodology " does not work " , as you wrote later . instead , it means that it is completely unnecessary . if you look at the faddeev-popov ghosts ' lagrangian , you will see that for abelian groups , the structure constants $f_{abc}$ vanish and we are left with $$ {\mathcal l}_{\rm ghost} = \partial_\mu \bar c^a \partial^\mu c^a $$ which means that the ghosts are completely decoupled . they do not interact with the gauge fields ( photons ) . you may still use the faddeev-popov machinery and the brst formalism based upon it to identify the physical states as the cohomologies of $q$ , the brst operator . but what this brst machinery tells you is something you may easily describe without any faddeev-popov ghosts , too . it just tells you that the excitations of $\bar c , c$ are unphysical much like the excitations of time-like and longitudinal photons . that is why the brst problem in the case of abelinan gauge groups is " solvable " in such a way that you may simply eliminate the ghosts completely , together with 2 unphysical polarizations of the photon . and that is why qed may be taught without any faddeev-popov ghosts and one may still construct nice feynman rules for any multiloop diagrams . for non-abelian theories , the counting still works – ghosts , antighosts , and two polarizations of gluons etc . are unphysical . however , because there are interactions of ghosts with the gluons in that case , there is no easy way to describe the physical states without the faddeev-popov ghosts .
i ) the $x$-boost formula of the stand-up physicist ( also known as doug sweetser ) can be simplified to $$\tag{1} q^{\prime}-bq\bar{b}~=~ \bar{q} \frac{\bar{b}^2-b^2}{2} $$ $$~=~ - \bar{q} i {\rm im} ( b^2 ) ~=~ - \bar{q} i\sinh ( 2\alpha ) ~=~ ( q_{\perp}-\bar{q}_{\parallel} ) i\sinh ( 2\alpha ) , $$ where $$b~:=~\cosh ( \alpha ) +i \sinh ( \alpha ) ~\in~ \mathbb{c} , \qquad b^2~=~ 1+ i \sinh ( 2\alpha ) , \qquad |b|^2~=~\cosh ( 2\alpha ) , $$ $$q~:=~q_{\parallel}+q_{\perp}~\in~ \mathbb{h}~\cong~m ( 1,3 ; \mathbb{r} ) , \qquad q_{\parallel}~:=~t+ix , \qquad q_{\perp}~:=~jy+kz . $$ formula $ ( 1 ) $ indeed reproduces the well-known lorentz transformation formulas for $x$-boosts with rapidity $2\alpha$ , $$ t^{\prime}~=~t\cosh ( 2\alpha ) - x \sinh ( 2\alpha ) , \qquad x^{\prime}~=~x\cosh ( 2\alpha ) - t \sinh ( 2\alpha ) , $$ $$ y^{\prime}~=~y , \qquad z^{\prime}~=~z , $$ or equivalently , $$ q^{\prime}_{\parallel} ~=~q_{\parallel}\cosh ( 2\alpha ) -\bar{q}_{\parallel} i \sinh ( 2\alpha ) , \qquad q^{\prime}_{\perp}~=~q_{\perp} . $$ this is because $$bq\bar{b}~=~b ( q_{\parallel}+q_{\perp} ) \bar{b}~=~q_{\parallel}|b|^2+ q_{\perp}\bar{b}^2~=~q_{\parallel}\cosh ( 2\alpha ) + q_{\perp} ( 1- i \sinh ( 2\alpha ) ) . $$ ii ) as is well-known , the $x$-boosts $b$ form an abelian non-compact $u ( 1 ) $ subgroup of the lorentz group $so ( 1,3 ; \mathbb{r} ) $ . since the pertinent group acts freely on minkowski space , the $x$-boost formula $ ( 1 ) $ must also respect this group structure . in terms of rapidity $2\alpha$ , this abelian subgroup is just the additively written group $ ( \mathbb{r} , + ) $ , cf . this and this phys . se post . iii ) there are many important descriptions of the minkowski space $m ( 1,3 ; \mathbb{r} ) $ and the lorentz group ( meaning both rotations and boosts ) , but as far as i can tell , the quaternions $\mathbb{h}$ are not too useful in this respect . for instance , the set of hermitian matrices $u ( 2 , \mathbb{c} ) $ seems to be a much more powerful description of minkowski space $m ( 1,3 ; \mathbb{r} ) $ , cf . this phys . se post . in contrast , the quaternions $\mathbb{h}$ play an important role for the euclidean compact counterpart $so ( 4 , \mathbb{r} ) $ of the lorentz group $so ( 1,3 ; \mathbb{r} ) $ , because the lie group $u ( 1 , \mathbb{h} ) \times u ( 1 , \mathbb{h} ) $ is ( a double cover of ) $so ( 4 , \mathbb{r} ) $ . iv ) the lie group $$u ( 1 , \mathbb{h} ) ~:=~\{r\in \mathbb{h} \mid \bar{r}r=1\}~\cong~ su ( 2 , \mathbb{c} ) $$ is ( a double cover of ) the rotation group $$so ( 3 , \mathbb{r} ) ~\cong~ so ( {\rm im} ( \mathbb{h} ) , \mathbb{r} ) , $$ which in turn is both a subgroup of $so ( 4 , \mathbb{r} ) $ and $so ( 1,3 ; \mathbb{r} ) $ . a rotation $r$ is implemented as $$\tag{2} \tilde{q}~=~rq\bar{r} , \qquad q\in \mathbb{h} , \qquad r\in u ( 1 , \mathbb{h} ) . $$ similarly , $$ \qquad \tilde{q}^{\prime}~=~rq^{\prime}\bar{r} , \qquad \tilde{b}~=~rb\bar{r} , \qquad \tilde{i}~=~ri\bar{r} . $$ hence , the formula $ ( 1 ) $ behaves covariantly under rotations , i.e. formula $ ( 1 ) $ holds not just for boosts $b$ in the $x$-direction , but for boosts $b$ in any direction ! v ) potential problems lie elsewhere . is this formula $ ( 1 ) $ interesting ? i fail to see how formula $ ( 1 ) $ could be useful ( as compared to more potent standard approaches ) . it is apparently not lorentz covariant . why have a formula that works for boosts , but not for rotations , cf . formula $ ( 2 ) $ ? note that two successive boosts in different directions do in general not constitute a pure boost , i.e. boosts in generic directions do not form a proper subgroup .
for complete treatement , see [ this reference ] ( http://preposterousuniverse.com/grnotes/grnotes-seven.pdf ) page 172 ( formula 7.32 ) and following pages . the idea is to use an affine parameter $\lambda$ , such as : $$g_{\mu\nu} \frac{dx^{\mu}}{d\lambda} \frac{dx^{\mu}}{d\lambda} = - \epsilon$$ ( in a metrics $g = ( -1,1,1,1 ) ) $ for massive particles , you can choose $\lambda = \tau$ , which is the proper time of the particle , so $\epsilon = - 1$ for massless particles , $\lambda$ is different of $\tau$ , because $d\tau=0$ , in this case , you have $\epsilon = 0 $ . so you can make all the calculus with this $\epsilon$ , for instance , you will have an effective potential as : $$v ( r ) = \frac{1}{2} \epsilon - \epsilon \frac{gm}{r} + \frac{l^2}{2r^2} - \frac{gml^2}{r^3} $$ ( page 174 formula 7- 48 of the reference ) page 176 of the reference , you will see the different orbits for massive and massless particles .
i would expect metals to cool faster through convection because of a related heat property of theirs : conduction . metals are generally good heat conductors so as heat energy is removed from the surface of the metal by air , internal heat energy in the metal can quickly flow to the surface . the rate heat flows between two systems is dependent on their temperature difference . when air removes heat via convection , the surface of the object is cooler than the central portion of the object . if heat in the object flows slowly from the center to the surface like in many plastics then the surface stays closer to air temperature and heat transfers to the surrounding air more slowly . if the object is a metal , the heat removed at the surface is easily replaced by heat flowing from the center to the surface and the temperature delta between the surface and the air is greater so the object cools faster .
as brandon mentioned , two small objects could not orbit each other near a significant gravitational field . the hill sphere " approximates the gravitational sphere of influence of a smaller body in the face of perturbations from a more massive body . " therefore , your pebble 's hill sphere would be too small to permit orbits near earth . the wiki article has a calculation showing that an astronaut could not orbit the 104 tonne space shuttle 300 km above the earth since the shuttle 's hill sphere was only 120 cm .
in the situation you have described in the comment there is a drag force acting on a ball even if water is assumed to be inviscid . however , one should carefully calculate it in order to decide whether it matters at all . the effect responsible for the drag is called added mass . actually i refer you to that wikipedia article to find out the explanation . what matters is whether this drag is strong enough for your case . the force should be $$\boldsymbol f = -\frac{1}{2} \rho_w v \boldsymbol a$$ where $\boldsymbol a$ is the ball 's acceleration , $\rho_w$ is water density and $v$ is the ball 's volume . so if the acceleration is comparable to $\boldsymbol g$ then the drag is comparable to the buoyant force and could not be neglected . since you assume ball 's density to be zero then you have to take the drag into account because otherwise it acceleration would be equal to g .
it seems there is a simply way to do this for polynomials with finite degree $d$ . since $a^\dagger a$ is the number operator , we can take $a^\dagger a = n$ , where $n$ is the number of excitations corresponding to a particular level . then if the hamiltonian has the general form $h = \sum_{k=0}^d c_k ( a^\dagger a ) ^k$ , the energy corresponding to a particular state is $e_n = \sum_{k=0}^d c_k n^k$ . since $n$ is constant for a given eigenstate of the hamiltonian , the equation for $e_n$ is just a linear equation in the variables $\{c_k\}_k$ . since you have the spectrum , you have $e_n$ , and hence you can solve using guassian elimination ( or whatever your preferred technique is for solving linear systems of equations ) . even if the spectrum is infinite , you will only need $d+1$ equations to fix the values of $c_k$ , so this is a simple calculation .
suppose you had three electrons , with individual wavefunctions $\lvert \psi_1 \rangle$ , $\lvert \psi_2 \rangle$ , and $\lvert \psi_3 \rangle$ . let them all have the same $\vec{x}$ , $l$ , and $m$ , so they can only differ in intrinsic spin . since spin is a two-dimensional hilbert space , as you noted , then three vectors must be linearly dependent . that is , there exist complex numbers $\alpha$ and $\beta$ such that $$ \lvert \psi_3 \rangle = \alpha \lvert \psi_1 \rangle + \beta \lvert \psi_2 \rangle . $$ now the state of all three electrons is given by the tensor product $$ \lvert 1,2,3 \rangle \equiv \lvert \psi_1 \rangle \otimes \lvert \psi_2 \rangle \otimes \lvert \psi_3 \rangle . $$ the tensor product respects the structure of the underlying vector spaces , which is a fancy way of saying we can write $$ \lvert 1,2,3 \rangle = \alpha \lvert 1,2,1 \rangle + \beta \lvert 1,2,2 \rangle . $$ but $\lvert 1,2,1 \rangle$ , as a product state of identical fermions , is antisymmetric under interchange , in particular under interchance of the $\lvert \psi_1 \rangle$ components : $$ \lvert 1,2,1 \rangle = - \lvert 1,2,1 \rangle . $$ thus $\lvert 1,2,1 \rangle \equiv 0$ . similarly , $\lvert 1,2,2 \rangle$ vanishes . we have just shown that our 3-electron wavefunction is a linear combination of $0$-vectors , and so it too vanishes . this argument easily generalizes to more than three electrons by appending the combined wavefunction of all the others to the end of our state and just carrying it through the computations ; that is , just do the same thing splitting up $\lvert \psi_3 \rangle$ , but apply it to $$ \lvert 1,2,3 \rangle \otimes \lvert 4 , \ldots , n \rangle . $$
i would guess that the professor is explaining his/the ( ? ) theory that dark matter is neutrinos , produced via a scattering process he calls " witten 's dog " . it is funny because the neutrinos are coming out of the dog 's butt . in the standard humor classification , this is known as a " poop joke " .
i did a google search and found that : when a glass rod is rubbed with a paper towel , the glass becomes positively charged . therefore : 1 . ) ground the electroscope through touching it with you fingers ( removing any excess charge ) ; one can assume it is neutral/has no net charged at this point . 2 . ) touch one of the charged rods to the electroscope . the leaves should rise with an unknown charge . 3 . ) vigorously rub the uncharged glass rod with a paper towel to give it a positive charge . 4 . ) bring the positively charged rod close to the electroscope . if the leaves are repulsed , then you know that the glass rod of unknown charge was/is of the same charge , and therefore is positively charged . opposite charges attract . therfore if the leaves attract one knows that the glass rod of unknown charge was negatively charged . 5 . ) [ check ] repeat steps 1-4 to verify that that last rod is of opposite charge .
you really need to look at an introductory book on vectors because any answer we give on this site can only cover a tiny bit of the properties of vectors . having said that : you can add any vectors by thinking of them as a movement . for example vector d means " go 4cm north " and vector j means " go 4.5cm west " . adding the vectors then just means making the two movements ie d + j = " go 4cm north and 4.5cm west " . the sum d+j is the vector from the staring point to the end point shown by the dashed line . using this method you can add any two vectors in any two directions . this addition is exactly what asdfsdjlka is doing in his answer . he is representing the vector by two numbers $ ( x , y ) $ where $x$ means the direction east and $y$ means the direction north . then d is ( 0 , 4 ) i.e. zero cm east and 4 cm north and j is ( -4.5 , 0 ) i.e. -4.5 cm east and zero cm north . representing vectors in this way is convenient for addition because for any two vectors $ ( x_1 , y_1 ) $ and $ ( x_2 , y_2 ) $ the sum of the two vectors is just $ ( x_1 + x_2 , y_1 + y_2 ) $ . this works whether the vectors are parallel , perpendicular or indeed at any angle . it also works for vectors in 3d where the vector has the form $ ( x , y , z ) $ .
first normalize the state to find $a$ . then you need to express the state as a superposition of the stationary states of the infinite square well : $$ \psi\left ( x\right ) = a x \left ( a-x\right ) = \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) , $$ where $\psi_n\left ( x\right ) = \sqrt{2/a} \sin\left ( n \pi x / a\right ) $ is the $n$-th stationary state . you can do this using the orthogonality of the stationary states , $$ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) = \frac{2}{a} \int_0^a dx \ \sin\left ( \frac{m \pi x}{ a}\right ) \sin\left ( \frac{n \pi x}{ a}\right ) = \delta_{mn} , $$ by integrating the equation above : $$ \begin{align} \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ a x \left ( a-x\right ) \right ] and = \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \left [ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \delta_{m n} \\ and = c_m \end{align} $$ i will leave the $c_n = a \sqrt{2/a} \int_0^a dx \ \sin\left ( n \pi x / a\right ) x \left ( a-x\right ) $ integral for you to work out . once you have the $c_n$ 's , the most likely value of a measurement of the energy is the energy corresponding to the stationary state with maximum $c_n$ . to find the probability of measuring $9 \hbar^2 \pi^2 / 2 m a^2$ for the energy , determine the stationary state that this energy corresponds to , and compute $\left|c_n\right|^2$ . for the time evolution , since the potential is $0$ everywhere after $t=0$ , it is a free particle , and the general solution is : $$ \psi\left ( x , t\right ) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty dk \ \phi\left ( k\right ) \exp\left [ i\left ( k x + \frac{\hbar k^2}{2 m} t\right ) \right ] , $$ where $$ \phi\left ( k\right ) = \frac{1}{\sqrt{2 \pi}} \int_0^a dx \ \psi\left ( x , 0\right ) \exp\left ( -i k x\right ) = \frac{a}{\sqrt{2 \pi}} \int_0^a dx \ x\left ( a-x\right ) \exp\left ( -i k x\right ) . $$ so , now you just have to do this integral .
firstly , as the wedge is movable , it will move when the object slides onto it ( i guess that is what they meant ) . the steps you may follow : momentum is conserved . so , the initial momentum of the system will be $mu$ . the final momentum will be the combined momentum of the wedge and the block $ ( m+m*eta ) v$ . from here , you can find an expression for the final velocity in terms of u 2 . using conservation law of energy , $$k . e_i = k . e_f+mgh$$ 3 . by solving this , you can solve for minimum value of u
$\newcommand{\er}{\hat e_r} \newcommand{\et}{\hat e_\tau} \newcommand{\d}{\dot} \newcommand{\m}{\frac{1}{2}m} $ in radial coordinates , $\d\er=\d\theta \et$ , and ( useless here ) $\d\et= -\d r \er$ . $\er , \et$ are unit vectors in radial and tangential directions respectively . due to this mixing of unit vectors ( they move along with the particle ) , things get a little more complicated than plain ' ol cartesian system , where the unit vectors are constant . for your particle , writing $x+l\to r$ , the position vector is : $$\vec p= r\er$$ $$\therefore \vec v=\d{\vec p}= \d r\er + r\d\er=\d r \er + r\d\theta\et$$ $$\therefore v^2= \vec v\cdot\vec v= \d r^2+r^2\d\theta^2$$ substituting back the value of $r=x+l , \d r=\d x$ ( and mutiplying by $\m$ , we get the above expression ? as you can see in my expression for $\vec v$ , i had two components of velocity--radial and tangential . since they are perpendicular , i can just square and add , akin to $t=\m\left ( \d x^2 +\d y^2\right ) $ . the point is , it may be a scalar , but it contains a vector in its expression:$$t=\m v^2=\m|v|^2=\m \vec v\cdot \vec v=\m ( \dot x^2+\dot y^2 ) $$
lense-thirring frame dragging has been measured to about 10 or 20% accuracy by two satellite experiments . look up lageos and gp-b ( gravity probe b ) .
by ' density ' in this case i think you just mean " something on a manifold that can be integrated to give you a scalar " . by this definition , on an $n$-manifold , a density would be an $n$-form ( since if you integrate over a form of lower dimension you get zero ) . so in your 3d case , take 3 smooth functions $f , g , h:m^3\to \mathbb{r}$ , the form $df\wedge dg\wedge dh$ is a density . now , in your example you are integrating over a scalar field multiplied by a 3-form , which is again a 3-form , which can be integrated over a 3-manifold to get you the change in the region . but the scalar field $\rho:m^3 \to\mathbb{r}$ is not a density ( not a 3-form ) , so it cannot be integrated over to find the total charge . the charge density is $\omega=\rho dx\wedge dy\wedge dz , $ and $\rho$ just tells us how ' big ' this should be . in other words , the mathematical term ' density ' can be stated as '$n$-form on $m^4$' , whereas the colloquial ' density ' for ' something per unit length/area/volume ' is shorthand for what we really mean ( $n$-form ) .
a superposition sector is a subspace of the hilbert space ${\mathcal h}_i$ such that the total hilbert space of the physical system may be described as the direct sum $$ {\mathcal h} = {\mathcal h}_1 \oplus {\mathcal h}_2 \oplus\cdots \oplus {\mathcal h}_n$$ where $n$ may be finite or infinite such that if the state vector belongs to one of these superselection sectors $$|\psi ( t ) \rangle\in{\mathcal h}_i , $$ then this property will hold for all times $t$: it is impossible to change the superselection sectors by any local operations or excitations . an example in the initial comments involved the decomposition of the hilbert space to superselection sectors ${\mathcal h}_q$ corresponding to states with different electric charges $q$ . they do not talk to each other . a state with $q=-7e$ may evolve to states with $q=-7e$ only . in general , these conservation laws must be generalized to a broader concept , " superselection rules " . each superselection rule may decompose the hilbert space into finer sectors . it does not mean that one can not write down complex superpositions of states from different sectors . indeed , the superposition postulate of quantum mechanics guarantees that they are allowed states . in practice , we do not encounter them because the measurement of total $q$ – the identification of the precise superselection sectors – is something we can always do as parts of our analysis of a system . it means that in practice , we know this information and we may consider $|\psi\rangle$ to be an element of one particular superselection sector . it will stay in the same sector forever . in quantum field theory and string theory , the term " superselection sector " has still the same general meaning but it is usually used for different parts of the hilbert space of the theory – that describes the whole spacetime – which can not be reached from each other because one would need an infinite energy to do so , an infinite work to " rebuild " the spacetime . typically , different superselection sectors are defined by different conditions of spacetime fields at infinity , in the asymptotic region . for example , the vacuum that looks like $ads_5\times s^5$ ground state of type iib string theory is a state in the string theory 's hilbert space . one may add local excitations to it , gravitons , dilatons ; - ) , and so on , but that will keep us in the same superselection sector . the flat vacuum $m^{11}$ of m-theory is a state in string theory 's hilbert space , too . there are processes and dualities that relate the vacua , and so on . however , it is not possible to rebuild the spacetime of the $ads$ type to the spacetime of the $m^{11}$ time by any local excitations . so if you live in one of the worlds , you may assume that you will never live in the other . different asymptotic values of the dilaton ; - ) or any other scalar field ( moduli . . . ) or any other field that is meaningful to be given a vev define different superselection sectors . this notion applies to quantum field theories and string theory , too . in particular , when we discuss string theory and its landscape , each element of the landscape ( a minimum of the potential in the complicated landscape ) defines a background , a vacuum , and the whole ( small ) hilbert space including this vacuum state and all the local , finite-energy excitations is a superselection sector of string theory . so using the notorious example , the f-theory flux vacua contain $10^{500}$ superselection sectors of string theory . in the case of quantum field theory , we usually have a definition of the theory that applies to all superselection sectors . a special feature of string theory is that some of its definitions are only good for one superselection sector or a subset of superselection sectors . this is the statement that is sometimes misleadingly formulated by saying that " string theory is not background-independent " . physics of string theory is demonstrably background-independent , there is only one string theory and the different backgrounds ( and therefore the associated superselection sectors – the empty background with all allowed local , finite-energy excitations upon it ) are clearly solutions to the same equations of the whole string theory . we just do not have a definition that would make this feature of string theory manifest and it is not known whether it exists .
it really depends on how more gas is being " pumped in " . for example , a typical pump will take air in at ( roughly ) one atmosphere , and pressurize it by doing work on it , which will increase the temperature . on the other hand , if the gas is coming from a cylinder of compressed air , it will cool on entering your volume , so the temperature would actually drop . you can even imagine a " maxwell 's demon compressor " , which just allows a molecule to enter whenever no other molecule would leave . this should not change the temperature at all .
you do not need gauss ' law . coulomb 's law will do the trick here along with some calculus .
yup . inside the ( uniform spherical ) mass , iirc $\phi=-\frac{gm}{2r^3}\left ( 3r^2-r^2\right ) $ . or something like that . so , $$\phi=\begin{cases} -\frac{gm}{r} , and r&gt ; r \\ -\frac{gm}{2r^3}\left ( 3r^2-r^2\right ) , and r&lt ; r \end{cases}$$ the laplacian $\nabla^2\phi$ should be $$4\pi g\rho=\nabla^2\phi=\begin{cases} 0 , and r&gt ; r \\ 4\pi g\rho_0 and r&lt ; r \end{cases}$$ where $\rho$ is the scalar density field , and $\rho_0=\frac{m}{\frac{4}{3}\pi r^3}$ is the density of the ball . so this makes sense . remember that mass density is a field as well as gravitational potential , so calculating it at one point in space does not mean that you have done it at all points in space . the laplacian of a discontinuous $\phi$ will give $\rho$ only within the limits of continuity . you have to break the function up . just a note : even for a point particle , $\rho$ is not identically zero everywhere . it is infinity at the origin ( check your formula again ) , so you basically get a dirac $\delta$ function . for a nonspherical/nonuniform mass there is no such formula . you have to integrate it yourself .
can vectors in physics be represented by complex numbers ? absolutely . there exists a direct isomorphism between the 2d euclidean vector space and the argand plane , for a start . in fact , it is possible to talk of mathematical objects called quaternions and use quaternion algebra analogously to vector algebra . historically quaternions were used to represent geometrical operations and transformations in 3d space - in the days before vector algebra . ( they still are used , especially in areas such as computer graphics where they offer one or two advantages over the simpler world of vectors . ) in any case , the relationship to vector algebra is a very close one . can vectors in physics be divided ? in general , no , vector-vector division is not a well-defined operation . at least , not within the bounds of linear algebra . i.e. there exist none or multiple solutions to the equation $\vec{y} = \mathbb{a} \vec{x}$ . ( see the wolfram page . ) saying this , the concept of vector division has an interesting relationship with your first question . if we map vectors to complex numbers ( or quaternions in > two dimensions ) , we can use complex division or quaternion algebra respectively to define an analogous " vector division " operation . note that quaternions can in fact be extended to higher dimensions , which allows for interesting possibilities . this study of this falls under the area of clifford algebras . ( note : vector-scalar division is of course well-defined , as is point-wise division of equal-length vectors , but i presume you are not referring to that . )
no , the temperature of the water is not that important for the performance of an evaporative cooler . this is basically because the energy needed to increase the temperature of liquid water ( its specific heat capacity ) is very small compared to the energy needed to evaporate the same amount of water ( its enthalpy of evaporation ) . at room temperature the specific heat of liquid water is 4.18 j/ ( g$\cdot$k ) while the ethalpy of evaporation is 44.0 kj/mol . since the molar mass of water is roughly 18 g/mol , this means that approximately 585 times as much energy is needed to evaporate an amount of water as to increase the temperature of the same amount of water by 1 k . this means that even if the water starts at freezing temperature , is heated to 40 $^\circ$c ( 104 $^\circ$f ) and then evaporates , less than 7% of the energy absorbed by it is used for increasing the temperature . the temperature of the water might effect the rate at which evaporation occurs , but i guess evaporative coolers are designed to achieve 100% relative humidity in the wet bulb regardless of the temperature of the water , so this probably does not affect the performance .
when alpha particles ( helium nuclei ) or beta particles ( electrons ) are released in a radioactive decay they carry significant kinetic energy . as they go through the surrounding material they bump here and there occasionally kicking off some electrons from the surrounding atoms which are then ionised , until they finally stop .
a simple method to judge the chirality ( or in your words " orientation" ) of the hamiltonian is to evaluate the following quantity $$f=\frac{i}{2}\mathrm{tr}\frac{\partial h}{\partial{q_x}}\frac{\partial h}{\partial{q_y}}\frac{\partial h}{\partial{m}} . $$ the sign of this quantity $f$ gives the chirality of the hamiltonian . example : given the two hamiltonians $h_1=q_y\sigma_x-q_x\sigma_y-m\sigma_z$ and $h_2=-q_y\sigma_x-q_x\sigma_y+m\sigma_z$ , we can evaluate $$f_1=\frac{i}{2}\mathrm{tr} ( -\sigma_y ) \sigma_x ( -\sigma_z ) =1 , $$ $$f_2=\frac{i}{2}\mathrm{tr} ( -\sigma_y ) ( -\sigma_x ) \sigma_z=1 . $$ because $f_1$ and $f_2$ are of the same sign , so $h_1$ and $h_2$ are of the same chirality . the reason that this trick works is that it basically estimates the berry curvature , which is defined as $$f=\frac{i}{2}\mathrm{tr}\ , g^{-1}\mathrm{d}g\wedge g^{-1}\mathrm{d}g\wedge g^{-1}\mathrm{d}g , $$ where $g= ( i\omega - h ) ^{-1}$ is the single particle green 's function . the chern number is then simply an integral of the berry curvature , i.e. $c=\frac{1}{2\pi}\int f$ . since the berry curvature mostly concentrated around the origin of the momentum-frequency space , one just need to estimate the berry curvature at that point to determine the sign of the chern number . while in the formula for $f$ , $$g^{-1}\mathrm{d}g = ( i\omega-h ) d ( i\omega-h ) ^{-1}\sim g dh , $$ which gives the $\mathrm{d} h$ terms . and because the hamiltonian is gaped , so in the zero momentum and frequency limit , the green 's function is a constant $g\propto \partial_m h$ that is proportional to the mass term . putting all these pieces together , and to the leading order of momentum and frequency , we find the berry curvature can be roughly estimated from $f\sim f$ . so the quantity $f$ is of the same sign as the chern number , and can be used to determine the chirality of the hamiltonian . this estimate is exact around the dirac point , which is just the case of the examples you provided .
 the magnetic field lines are in the same direction as the upwards velocity of the electrons , so the v × b term in the lorentz force is 0 . magnetic field lines are curved , so they cannot be everywhere in the same direction . magnetic field in the metallic wire is not everywhere parallel to the velocity of the wire and produces electromotive intensity $\mathbf v\times \mathbf b$ circulating in the horizontal plane that is non-zero both below and above the center of the magnet . near the center of the magnet , the induced intensity is low due to fact you mention , i.e. velocity is parallel to magnetic field .
the key to this problem is the fact that the planet 's mass $m$ as it appears in newton 's law of gravitation , $$f=\frac{gmm}{r^2} , $$ is not actually constant . this is because the layers of the planet that are above you cause zero net force : if you are inside of a hollow spherical shell of mass then diametrically opposite elements of solid angle exert equal forces in opposite directions . thus , the effective mass of the planet in this problem is only that of a sphere of radius $r$ and density $3m_0/4\pi r^3$ , i.e. $m ( r ) =\frac{r^3}{r^3}m_0$ . the force is then $$f=\frac{gm_0m}{r^3}r$$ and it of course causes harmonic motion , with " spring constant " $k=gm_0m/r^3$ .
the number of physical degrees of freedom ( dof ) or dynamical variables is simply the number of generalized positions whose evolution is given by a second order in time differential equation . using the op 's notation , the number of dof is $${1\over 2} ( n-2m-s ) $$ for instance , in electrodynamics the phase-space is six-dimensional $\{a_i , f_{0i}\}_{i=1}^3$ and the gauss law is a first class constraint . thus $n=6 , \ , m=1 , s=0$ . so that there is two dof corresponding to the two polarizations of electromagnetic waves or the two photon 's helicities . one can take an alternative and equivalent point of view in which the phase-space consists of $\{a_{\mu} , f_{0\mu}\}_{\mu=0}^3$ and besides the gauss law one has the first class constraint $f_{00}\approx0$ ( the symbol $\approx$ is read " weakly zero " and means zero when the constraints are verified , you may perfectly write $=$ ) which poisson commutes with the gauss law and both are therefore first class constraints . then $n=8 , \ , m=2 , s=0$ and the number of dof is still two , of course . in the case of the gravitational field , the counting of dof is analogous . the phase-space consists of $\{h_{ab} , p_{ab}\}_{a=1 , b=1}^{a=3 , b=3}$ , with $h_{ab}$ the components of the spatial metric and $p_{ab}$ their conjugated momenta . the four $ ( 0 , \mu ) $ einstein equations are not dynamical equations —since they do not contain second order temporal derivatives— but first class constraints . hence $n=12 , \ , m=4 , \ , s=0$ so that the number of dof is two corresponding to the two polarizations of gravitational waves . however , consider the case of the procca field ( a vectorial field of mass $m$ ) . now the phase-space consists of $\{a_{\mu} , f_{0\mu}\}_{\mu=0}^3$ and there are two constraints $\partial_i\ , f_{0i}=m^2a_0$ —i am considering a theory with no matter fields besides the vectorial field , if one added other fields , then there would be a density of charge $\rho$ in the right hand side— which reduces to the gauss law when $m=0$ and $f_{00}=0$ like in the electromagnetic case . however , now due to the mass term , the two constraints do not poisson commute , thus the constraints are second class . hence $n=8 , \ , m=0 , s=2$ and the number of degrees of freedom is three corresponding to the three helicities of a massive vectorial particle .
you are right that the result you see is due to the chain rule . the author uses either spherical or cylindrical coordinates , so \begin{equation} r = \sqrt{x^2 + y^2 + z^2} \end{equation} or \begin{equation} r = \sqrt{x^2 + y^2} \end{equation} which you can differentiate to obtain \begin{equation} \frac{\partial{r}}{\partial{x}} = \frac{x}{r} \end{equation} hence \begin{equation} f_x ( r ) = -\frac{\partial{u ( r ) }}{\partial{x}} = -\frac{\partial{u ( r ) }}{\partial{r}}\frac{\partial{r}}{\partial{x}} = -\frac{x}{r} \frac{\partial{u ( r ) }}{\partial{r}} \end{equation}
if you consider a homogeneous piece of silicon the total flow of electrons through it is : $$ i = \frac{u}{r} = n \mu \frac{s}{d} u $$ where $r$ - the resistance of the piece , $u$ - external voltage applied to it . the resistance depends on : $n$ - the concentration of electrons ( number of electrons per m$^3$ ) , $\mu$ - the mobility of electrons ( ratio of velocity of the electron and electric field that makes it move ) , $s$ and $d$ - cross-section and length of the sample . the changes of geometric size with temperature are negligible . so the values that affect the resistance are concentration $n$ and mobility $\mu$ . the mobility depends on the temperature and also on the concentration of various defects in the sample . at 100 f the temperature dependence is dominating . the concentration is the most complicated point . there are the following cases : pure silicon . all the electrons ( and the same amount of holes ) are thermally generated . their concentration depends on the temperature exponentially . if you need total current do not forget about the holes . silicon doped with donors . the amount of thermally generated electrons is negligible . the concentration does not depend on the temperature . semiconductor device with p-n junction or/and heterojunction ( connection of different materials ) . the laser/led of optical computer mouse is this case . the sample is not homogeneous and the concentration is determined mainly not by temperature but by more interesting things like voltage polarity . this case requires more formulas and exact data concerning the sample structure . ! the laser is made of gaas and similar materials not silicon . the attempts to make silicon laser never stop though . edited ( 2011/12/15 ) : for the temperature dependence of electron mobility wikipedia gives $$ \mu ( t ) \approx \mu_0 t^{-2.4} $$ where $\mu_0 = 9.46 \cdot 10^{6} \text{m}/\text{ ( v s ) }$ , hope i have calculated it correctly from first point set ( black circles ) here this formula takes into account only electron scattering on the oscillations of the ions of the crystal . this effect is dominating at room temperature and higher . the temperature must be in kelvin degrees here . for the electron $n$ and hole $n_h$ concentration in pure silicon ( case 1 . ) at room temperature and higher one can use the following formula : $$ n = n_h = n_\text{eff} \ ; t^{\ ; 3/2} \exp \left ( -\frac{e_g}{2k_b t} \right ) $$ where $n_\text{eff}$ - some constant describing the shape of conduction and valence bands of silicon ( i have not found explicit value yet ) , $t$ - temperature in kelvin degrees , $e_g = 1.12\ ; \text{ev} = 1.79 \cdot 10^{-19} j$ - energy gap of silicon , $k_b$ - boltzmann constant .
a planet that was four times the mass of the earth would tend to drag and/or slingshot the earth out of its orbit ; multiple interactions are possible depending on the details of the first pass , but any further interactions would take place around the center of mass of the earth-melancholia system , which you would find by adding their initial vectors weighted by their mass ( assuming that we can neglect the sun , which we can not , but it gives you a very approximate idea of what would happen ) .
my gut feeling is that the pressure loss will be noticable , but managable with a small heating pump . 16mm diameter , however , seems awfully thin to me . you need to consider several things : how much thermal power will your heating system deliver , and at what temperatures ? from this you get the neccesary flow : $q=\frac{p_{therm}}{c_{heat}*\delta t}$ with this flow you can calculate the pressure loss with the formula found here : http://en.wikipedia.org/wiki/darcy%e2%80%93weisbach_equation - preferably in the pressure loss form . for such a small pipe diameter it is pretty safe to assume laminar flow , then you do not even need to check for the friction factor in a moody diagram . if the hose goes mainly straight up , you can calculate just the pressure loss from the distance and ignore fittings , bends , etc for a first shot . this is not a complete walkthrough , but i am quite confident you find everything you need one link from the wikipedia page i linked .
the orbital period of apophis is about 324 days , but the orbits of asteroids tend to be chaotic . because their orbits are eccentric they may make close approaches to other bodies like the earth or venus , and this will change the orbit in ways that are difficult to calculate . well , difficult to calculate far into the future anyway . technically the orbits of all solar system bodies , including the earth , are chaotic but since the earth does not make any close approaches to other heavy objects it is orbit changes only slowly and within narrow bands . anyhow , the variable orbit of apophis is why it is hard to be precise about whether it will hit the earth . the close approach in 2029 will change it is orbit in such a way that 7 years ( i.e. . 8 orbits ) later it could collide wth earth . but then we do not know exactly how close it will come in 2029 so we can not predict exactly how close it will come in 2036 . i am not sure that anyone knows where apophis came from . a quick google shed no light on the subject . i would guess it originated in the asteroid belt because if it came from much farther out it is orbital period would be longer . as to the impact , there are so many variables it is difficult to say what damage there would be . apophis is a lot smaller than the asteroid that killed off the dinosaurs , so we are not looking at the end of all life on earth . however if it hit a big city there would be little if anything left of the inhabitants !
no , because the wavefunctions are not waves in space . they are waves in enormous high-dimensional spaces of possibilities . if you have two particles , the wavefunction is waving in 6 dimensions ( the two positions of the two particles make a six dimensional space of possibilities ) , if you have three particles , the wavefunction is in 9 dimensions . so it is always wrong to think of it as a wave in space , like a field . there is a field which obeys the schrodinger equation , but this classical field is a classical wave , like e and b , which describes many coherent bosons in the same quantum state all moving together , like a superfluid or a bose-einstein condensate .
i have never seen de broglie 's relation written with vector quantities . a quick search online reveals a lack of vectors as well . in the relation $$\lambda = \frac{h}{p}$$ it is implied that the quantity $p$ is the magnitude of the momentum $\left | \vec{p} \right |=p . $ yes , the word momentum in a strict sense refers to a vector quantity , but often physicists will use this term when referring to the well-defined scalar $p$ . on a related note , the term wavelength typically ( always , probably ) refers to a scalar , not a vector . so trying to ascribe a direction for a wavelength is something one might expect is not done . though as david h described in comments , there is a commonly-used vector quantity that are related to wavelength : wave vector : $$\vec{k}=\frac{2\pi}{\lambda}\hat{v} , $$ where the unit vector $\hat{v}$ typically points in the direction of propagation . if you fiddle around with these definitions you can write something akin to the de broglie relation with vectors : $\vec{p}=\hbar \vec{k} , $ with $\hbar=h/ ( 2\pi ) $ .
by the eigenenergies you quote , i imagine you are dealing with an infinite potential well with walls at $x=0$ and $x=l$ . in this case , the wavefunction is zero outside of the well , so $$\phi_n ( x ) = \left\{\begin{array} and \sqrt{\frac{2}{l}}\sin\left ( \frac{n\pi}{l}x\right ) and \text{if }0&lt ; x&lt ; l , \text{ and}\\ \qquad \quad0 and \text{otherwise . }\end{array} \right . $$ the integral is therefore over $x\in [ 0 , l ] $ , and converges without a problem .
the photoelectric effect is usually explained in the following way : photons come in from some source and knock electrons out of the metal . these electrons then make up a current through the circuit . when the photons do not have enough energy to knock out an electron , there will be no electrons to go around the circuit . this causes the current to go to zero .
according to the american meteor society , the sonic boom of an asteroid or meteor ( sometimes referred to as a ' fireball' ) is due to if a very bright fireball , usually greater than magnitude -8 , penetrates to the stratosphere , below an altitude of about 50 km ( 30 miles ) , and explodes as a bolide , there is a chance that sonic booms may be heard on the ground below . this is more likely if the bolide occurs at an altitude angle of about 45 degrees or so for the observer , and is less likely if the bolide occurs overhead ( although still possible ) or near the horizon . and from caltech 's coolcosmos page when an object travels faster than the speed of sound in earth 's atmosphere , a shock wave can be created that can be heard as a sonic boom . the reason for asteroids causing sonic booms in the lower atmosphere , is according to the article how the falling meteor packed a sonic punch ( klotz , 2013 ) is due to because the meteor is supersonic , the waves , which travel at the speed of sound , can’t get out of the way fast enough . the waves build up , compress and eventually become a single shock wave moving at the speed of sound . looking a bit further in to what a sonic boom ( using a jet as an example ) is and how it occurs is illustrated in the following diagram image source so , if a meteor , asteroid is going faster than the speed of sound for particular part of the atmosphere , then a sonic boom will occur . going back to the american meteor society 's description of the likely cause of a sonic boom , they stated that if a meteor comes in below an altitude of about 50 km ( 30 miles ) then a sonic boom is likely to occur , one of the reasons is that the speed of sound is slower , due to the temperature of the atmosphere at that height and lower . below is a graph showing the speed of sound plotted against temperature as a function of atmospheric elevation : image source .
a proton is a bound state of three quarks . the quarks themselves are ( as far as we know ) pointlike , but because you have the three of them bound together the proton has a finite size . it does not have a sharp edge any more than an atom has a sharp edge , but an edge is conventionally defined at a radius of 0.8768 femtometres . protons are spherical in the same way that atoms are spherical even though they are made up of discrete electrons . the three quarks have a mass , but actually the proton is a lot heavier than the combined mass of the three quarks . that is because the binding energy of the quarks is very high , and that energy increases the mass in line with einstein 's famous equation $e = mc^2$ . so , yes , it does make sense to calculate a density for the proton just as it makes sense to calculate a density for an atom .
you need to formulate the mathematical model for this system . once you have differential equations describing the behavior of these objects ( cells ) then you can time-integrate these equations on a computer ; the only question is how many objects you want to include in the calculation - the more the bigger computer you will need . my guess is that a few tens of cells can be handled today by a laptop , if you need hundreds or more then probably a large parallel machine is needed to make it a practical calculation . this problem is not a bad as the n-body problem because here it sounds like only forces between nearest neighbors are important , this makes it easier . anyway , one thing that we know is the newton law - if you have forces acting on an object then you know its acceleration hence you can find its velocity and position . however , what you need to specify is what is the force between two objects , if you know their positions and radii . and , for this problem you need to describe the time-evolution of the cell radius - how it depends on the forces acting on the cell etc . all this needs to be formulated as differential equations ; but once this is done then solving it numerically is relatively straightforward . to get some feel what this calculation may look like i suggest keeping initially the radii of the cells constant , and use some simple potential to approximate the force between cells that are touching each other , and use some simple time-integration algorithm like the leapfrog method http://en.wikipedia.org/wiki/leapfrog_integration
i will provide just one example of how to make a game obey known physics here . note that there are countless others . much of classical physics is based on newton 's second law $\vec f_\text{net}=m\vec a$ . another way to write this expression is $d \vec p/dt = \vec f_\text{net}$ . or taking the liberties that physicists usually do : $$d\vec p = \vec f_\text{net}dt$$ this means that if you know all of the forces on some object ( which you can again choose to obey known laws ) , you can figure out how the momentum $\vec p$ of the object will change in a very short time interval . this is useful because $\vec p=m\vec v$ , so ultimately it allows you to figure out the trajectory of an object . i suppose a generalization can be made from this . if you know your initial conditions , you can use a governing differential equation to figure out how the conditions will change a very short time later . this is how many simulations are done i believe . by the way , too many spaceship games ( and tv/movies for that matter ! ) do not obey this law . they do something more akin to $\vec f_\text{net} \propto \vec v$ , which always bugs the heck out of me . if i turn my engines off i want to keep coasting , not stop .
first discretize the spacetime , assign a fermion pair $\bar{\psi} ( i ) $ and $\psi ( i ) $ at each point i . then assuming operator $\hat{a}$ is symmetric , hence which can be diagonalized by a unitary operator whose determinant is one , the path integral can be written in the following way : $$\int \pi_{i} d\psi ( i ) d\bar{\psi} ( i ) e^{i \delta \sum \bar{\psi} ( i ) \lambda_i \psi ( i ) }$$ where $\delta$ is the discretized spacetime volume element and $\lambda_is$ are the eigenvalues of $\hat{a}$ . now the path integral can be written as a product of many grassmanian integrals . in particular each one is : $$\int d\psi ( i ) d\bar{\psi} ( i ) e^{i\delta \lambda_i \bar{\psi} ( i ) \psi ( i ) }\\ =\int d\psi ( i ) d\bar{\psi} ( i ) ( 1+i\delta \lambda_i \bar{\psi} ( i ) \psi ( i ) ) = i\delta \lambda_i $$ multiplying all these together you get the determinant of $\hat{a}$ up to some irrelevant constant factor .
in purely theoretical models , susy may be completely unbroken in which case photinos would be massless – and all particles would have the same mass as their superpartners . in the real world , susy has to be broken and a photino must consequently be massive ( it is infinitely unlikely that the mass agreement will survive for any pair if susy is broken ) , otherwise a photino would be easily produced and seen . for example , a pair of massless photinos would be rather easily created in electron-positron annihilations at the lep collider a decade ago and manifested as lots of " missing energy " ( because the massless photinos would be neutral and rather weakly interacting ) . in general , all superpartners must be massive – and it is assumed that all of them are more massive than their known counterparts , quite possibly much more massive ( or even so massive that we will never produce them ) . a photino is a neutral fermion . all the neutral fermions – photino , zino ( these two may also be mixed as the bino and the neutral wino ) , neutral higgsinos ( which includes the superpartners of the neutral goldstone mode ) – are mixing with each other and we call them " neutralinos " and write their mass as a matrix . there are four ( majorana spinors worth of ) neutralinos in the minimal supersymmetric standard model so we need to consider a $4\times 4$ mass matrix for these neutralinos . the eigenstates of this matrix are the " real well-defined four species " of the neutralinos with well-defined masses ( eigenvalues ) . a similar mixing applies to two dirac charginos – a unified name for charged winos and charged higgsinos . in 2012 , physicists would mostly talk about " neutralinos " and " charginos " rather than " photinos " and " winos " although those concepts are different basis vectors depicting the same basic particles , as discussed above .
it is pretty easy to calculate . for geostationary orbits , the orbital period $t$ should be equal to the rotational period of the earth $\omega_e$: $ \matrix { t and = and 2 \pi \sqrt{a^3/\mu} \\ \omega_e and = and 1\ \mathrm{stellar\ day} } $ $ \ \ t=\omega_e \rightarrow a = \sqrt [ 3 ] {\mu \cdot \frac{\mathrm{day}^2}{4\pi^2} } = \sqrt [ 3 ] {398600.44 \cdot \frac{86164.099^2}{4 \pi^2}} \approx 42164\ \mathrm{km} . $ note that this equals the semi-major axis of the orbit , which means that if you want the altitude , you will have to subtract earth 's equatorial radius : $ h = a - r_e = 42241 - 6378 = 35786 \ \mathrm{km} $
in theory , you could get almost $100\%$ efficiency from a solar cell exposed to light with the photon energy just above the band gap . each absorbed photon generates an electron of almost the same energy . the problem is that there are only so many band gaps available , so you have to find a light source at the correct wavelength to match one . the intensity of the light does not change the efficiency , just how much power you can get out of the cell . over a wide range , if the intensity doubles , you can generate twice as much power . as an aside , the energy of a photon is measured in energy units : ev , ergs , joules . watts are power , or energy/time .
as far as i know , gallavotti proved the ergodicity of the lorentz gas , while sinai proved that of a system of $n \leq 5$ rigid spheres . anyway , this is a minor detail . for certain aspects , a more suitable model for the drude model is the boltzmann gas . lanford has shown ( in 1970s , i think ) that the entropy for this model is always increasing , but anyone has proved that boltzmann gas is ergodic . so the answer to your question is : if , at our level of accuracy , lorentz gas was an appropriate mathematical scheme for the drude model , than it would be ergodic . else we can not conclude , since sinai 's result is very important but too limited . ( at the present day . ) however , it is an interesting question from a mathematical point of view ( for me , for example , it really is ) , but for a physicist it is not very important , since drude model does not provide an appropriate level of precision for most of calculations carried out nowadays in solid state physics ( at least , concerning what i have seen in my course of condensed matter , i am not a specialist in solid state physics ) . moreover , any of that models takes in account coulombian interactions between charged particles . ( this remark restricts - in principle - a lot the domain of applicability of such schematizations . ) i think you could find very interesting the treatments of this subject ( ergodic theory ) by halmos and arnold in their classical monographs . references . p . r . halmos , lectures on ergodic theory v.i. arnold , ergodic problems of classical mechanics and mathematical methods of classical mechanics g . gallavotti , statistical mechanics and the elements of mechanics
from the wikipedia article , the vacuum flask consists of two flasks , placed one within the other and joined at the neck . the gap between the two flasks is partially evacuated of air , creating a near-vacuum which prevents heat transfer by conduction or convection . heat transfer by thermal radiation may be minimized by silvering flask surfaces facing the gap , but can become problematic if the flask 's contents or surroundings are very hot ; hence vacuum flasks usually hold contents below water 's boiling point . most heat transfer occurs through the flask 's neck and opening , where a vacuum is not present . vacuum flasks are usually made of metal , glass , foam , or plastic , and have their opening stoppered with cork or plastic . vacuum flasks are often used as insulated shipping containers . from the design one can see that the metal inside container can transfer from conductivity heat to the outside surface through the small neck , from where also the radiative and convective heat losses can happen . if you have ever cooked you will know that even if you stir the pot with a metal spoon the heat of the pot does not transfer to the handle since other mechanisms than conductivity keep the temperature low ( air convective cooling for one ) . thus the neck ring of metal through which any heat transfer must pass is too small to heat the outside larger metal mass and it is lost to air conduction , air convection and metal radiation at the neck , which are the reason such dewars will lose their internal heat after some time .
newtonian mechanics is not quite as predictable as it is made out to be , both in theory and in practice . in theory , it is an easy task to set up a newtonian system of particles that will eventually violate the lipschitz continuity condition . you can toss reversibility and predictability out the window when that happens . in practice , we can not know state perfectly , and that means the complicated dynamic systems you mentioned eventually become unknowable . they will eventually entire a region of strong chaos , and there is no telling what happens after that . with regard to entropy , consider an interstellar gas cloud . the jeans instability can make a portion of the cloud collapse to form a protostar and a protoplanetary disk . you will not see a protostar and protoplanetary disk undo themselves and reform that gas cloud because of entropy . that is yet another gravitational example , but there are plenty of non-gravitational examples . consider a semi-rigid body with three distinct principle moments of inertia rotating in space . eventually that body will end up spinning about the axis with the largest moment of inertia . this is an irreversible process , and it lies almost entirely within the realm of newtonian mechanics . the only non-newtonian aspect is that the body radiates heat generated by internal friction away into the universe .
$ [ f , p_k ] =\frac{\partial f}{\partial q_i}\frac{\partial p_k}{\partial p_i}-\frac{\partial f}{\partial p_i}\frac{\partial p_k}{\partial q_i}$ . second term is zero and first term is $\frac{\partial f}{\partial q_i }\delta_{ik}=\frac{\partial f}{\partial q_k}$
no need to make it that complicated--calculate the potential at the surface of a sphere with a uniform mass distribution with mass $m$ . then calculate the energy required to add a bit of mass with mass $dm$ and radius $dr$ to the top of this sphere . express $m$ and $dm$ in terms of $r$ and $dr$ and integrate .
1 ) why do you believe that instantaneous probability densities are not meaningful ? 2 ) essentially any non-stationary state for which you need to compute time-dependent wavefunctions : e.g. chemical reaction dynamics , particle scattering , etc . 3 ) yes , the time dependant schrödinger equation applies to isolated systems . 4 ) by definition energy is conserved in an isolated system . moreover , the schrödinger equation conserves energy because the generator of time translations is the hamiltonian and this commutes with itself $ [ h , h ] =0$ , i.e. energy is conserved . for isolated systems , the hamiltonian is time-independent ( explicitly ) and the time-dependent wavefunction $\psi$ has the well-known form $\psi = \phi e^{-iet/\hbar}$ , with $e$ the energy of the isolated system . 5 ) i do not understand the question .
there is no structural similarity between an atom and the universe . atoms are usually described as bound states of a quantum system , while the universe is usually described by general relativity , two very different and currently even incompatible approaches .
yes , a hole with energy $e$ is the same as an electron with a negative energy $e$ missing - that is why it is called a hole and that is how paul dirac first encountered it in the relativistic context ( in the form of positrons ) . a positively-charged positron may look more " particle-like " but one may describe it as the very same holes in the otherwise filled sea of negative-energy electron states . in both cases - semiconductors and positrons - you may assume that the negatively-charged electrons are the only " real " particles . however , you will always derive the existence of positively-charged holes that behave " just like electrons " . if you find states such that the energy $e$ is an increasing function of the momentum , the system will first try to fill the low-momentum , low-energy states , and you may add additional higher-energy , higher-momentum particles ( electrons ) . but some part of the spectrum may have the property that the energy $e$ is a decreasing function of the momentum $k$ . in that case , the electron states with a higher value of momentum are filled first , and you are adding them " inwards " . this is counterintuitive , so it is more logical to exchange the convention for what we mean by a " filled state " and what we mean by an " empty state " . once we do so , we also change the charge and energy of the particle in each state . consequently , we will deal with positive-charge holes whose energy $e$ behaves just like $-e$ of the original electrons , and increases with $k$ just like for ordinary electron states . the only difference will be in the charge .
if you want something that looks working and do not care much about the details , the standard solution is fluent . the nearest open source option is openfoam .
you are almost there . i am assuming your square well $v_0 ( r ) $ is nonzero and negative only on some interval $-r_0 &lt ; r &lt ; +r_0$ about the origin . in that case , for positive $a , b$ you already have that the isosinglet well is deeper than the isotriplet well . remember that the finite square well has a finite number of bound states , each with energy $-|v_0| &lt ; e &lt ; 0$ . find the width and depth of a well with a single bound state ( for fun , with the correct binding energy , 2.2 mev ) . next find the minimum $b$ so that a same-radius well , shallower by $\frac{3}{4}v_0b$ , has zero bound states . tada : a bound isosinglet with no excited states , and an unbound isotriplet .
the main question is " relative to what ? " for space probes and the like , the speeds that matter are be either with respect to the earth , the target object ( s ) ( mars , some asteroid , space station , etc . ) , and/or the sun ( or solar system barycenter ) . these speeds are measured mostly by doppler shifts in radio waves emitted by a radar the probe carries , reflected by the surface of some target the communication signal between probe and earth ( see for instance , the deep space network ) . other methods have been used ( image analysis between consecutive images taken by the space probe , the temperature of the heat shield on atmospheric entry , etc . ) but these are all much less precise than doppler measurements . space telescopes will measure redshift to some object ( star , galaxy , etc . ) ( which is very similar to doppler ) , which is more an indication of how fast that object is moving with respect to the entire solar system , rather than just the space telescope . parallax methods are also used ( see @ignaciovazquez-abrams 's answer ) , but such methods can only be used for objects relatively close by ( the parallax for most galaxies is too small to measure ) . other methods include cepheid variables , and of course the famous type 1a supernovae , which were used to conclude that the expansion of the universe is accelerating . but these are primarily measures of distance , and only crude measures of speed -- for objects at large distances , redshift is the only accurate way to measure the speed with respect to those objects .
if you frequency analyse the sound from your equipment then the fundamental frequency will the same as the rotational frequency . you could record the sound and use some software like audacity to do the frequency analysis . this is exactly what alemi did in his reply to can i compute the mass of a coin based on the sound of its fall ? . alternatively , and inevitably , there is an app for that .
the assumptions of the famous earnshow 's theorem on the impossibility of levitation have some " loopholes " ( http://en.wikipedia.org/wiki/earnshaw%27s_theorem ) , one of them is alternating current electromagnets ( http://en.wikipedia.org/wiki/magnetic_levitation#oscillating_electromagnetic_fields ) . probably , that is the principle used for the installation . another " loophole " is diamagnetism ( not necessarily the meissner 's effect in superconductors mentioned by l . motl ) .
the euler number ( often called the euler characteristic ) is given a in terms of nice integral formulas in the gauss-bonnet theorem , but it can be defined in other ways . the difference in the factors simply comes from the fact that the two-dimensional scalar curvature $r$ is twice the gaussian curvature $k$ ( see towards the end of the first paragraph here ) . since you are reading polchinski , you will probably have no problem with most proofs of the gauss-bonnet theorem ( which are all over the internet ) , but i think do carmo 's differential geoemetry has a rather nice , elementary proof .
an actual cup is slightly complicated because there are 2–3 distinct types of surfaces . let 's deal with free-floating cubes of water instead , both at an initial temperature t i in an environment with temperature t e . a cube with half the volume will have 50% the thermal mass c , but 63% of the surface area a . newton 's law of cooling implies $\frac{dt}{dt}=-h\frac{a}{c} ( t-t_0\ ! ) $ , where h is a property of the environment . so the smaller cube will cool 26% faster initially , when both cubes are at t i . if you want to know the temperature of a cube at any given time , the solution to the differential equation above is $\frac{t-t_e}{t_i-t_e}=\exp\left ( -ht\frac{a}c\right ) $ . if you solve for t , it follows that when the small cube is at a given temperature , it will take the large cube 26% more time to reach it .
entanglement is just a correlation in measured properties of the subsystems ( particles ) expressed in a quantum way . you may only determine whether properties are correlated ( or entangled ) in a given , " initial " state if you repeat some measurements of the system with the same initial state many times . if you only measure two spins , for example , once , you get some result , like up-up or up-down or down-up or down-down but none of the four possibilities is more or less entangled than others . all of them may occur in entangled initial states and all of them may occur in non-entangled initial states . entanglement only means " predicted properties of the two subsystems are correlated , moreover correlated in a way that is not captured by a simple classical model of correlation " . whether the predicted properties are correlated may be determined from the probability distribution ( s ) but to measure the probability distribution ( s ) , you have to repeat the experiment with the same initial state many times . more precisely , entangled states are those that can not be written as a tensor product of wave functions describing the separate subsystems . once at least one of the entangled variables is measured , the entanglement becomes meaningless because the value of the variable is suddenly known and we are only left with some general wave function for the other , previously entangled variable which remains unknown up to the second measurement ( this reduction of the dependence of the wave function is misleadingly referred to as the infamous " collapse" ) . and if there is only one variable , it can not be entangled . but nothing physical is changing about the variable that has not been measured yet . the overall probability distribution for various outcomes $y$ remains the same after the first measurement of the ( faraway ) variable $x$ is performed ( imagine it is a probabilistic distribution $\rho ( y ) =c\rho ( x_\text{just measured} , y ) $ that is left afterwords , $c$ is such that $\int dy\ , \rho ( y ) =1$ ) . so no information can be transmitted by the fact that the first measurement took place . quite generally , quantum mechanics does not need any genuine ( one that could transfer useful information ) faster-than-light communication to guarantee things such as correlations of measurements done with entangled states . and in relativistic , local theories – especially quantum field theories and string theory – one may prove completely generally that a superluminal transfer of information is not only unnecessary for quantum mechanics to work ; it is actually prohibited and impossible , by the basic laws of special relativity .
you can callculate the standart deviation as show in this link http://en.wikipedia.org/wiki/standard_deviation#generalizing_from_two_numbers the standart deviation $\sigma=\sqrt{\frac{\sum_{i=1}^n a_i^2}{n}-\left ( \frac{\sum_{i=1}^n a_i}{n}\right ) ^2}$ , where $a_i$ is the $i$-th number in your set and $n$ is the number of numbers you have in your set ( in your example $a_1=32.5$ , $a_2=32.0$ , $a_3=32.3$ so $n=3$ ) using the numbers from your question i got $\sigma \approx 0.20548$
first of all write down an explicit expression for the summation over all microstates . edit since you are treating the system classically this includes an integral over phase-space and a summation over all possible spin-configurations . $$ \sum_{\mu_s} = \sum_{\{s_i\}}\int\frac{d^np d^nq}{h^{3n}n ! }$$ the second thing is to realize that your hamiltonian is non-interacting and the canonical density $e^{-\beta h} $ is just a product of one-particle hamiltonians $$ e^{-\beta h} =\prod_{i=1}^ne^{-\beta h_i} $$ where of course $$ h_i = {\frac{ ( p_i + \gamma s_i ) ^2}{2m} -b s_i} $$ ( i renamed $\beta$ to $\gamma$ , because you do not mean the inverse temp . here ) so you have to evaluate $$ \frac{1}{n ! }\sum_{\{s_i\}}\prod_{i=1}^n\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = $$ because $h$ ist non-interacting , the n-particle phase-space-integral factorizes into n integrations over a 1-particle phase-space . similarily one may interchange the spin-summation and the product ( convince yourself that this is true ! e . g that one ends up with the same terms , whether you sum over spin first or not . ) that means , instead of summing over all the many-body microstates , one first sums over the possible configurations of a single particle and accounts for the fact , that there are many afterwards . additionally all $h_i$ are equivalent . they each just carry a different but redundant index : $$ z = \frac{1}{n ! }\prod_{i=1}^n\sum_{s_i=\pm1}\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = \frac{1}{n ! }\left ( \sum_{s=\pm1}\int\frac{dp dq}{h^{3}} e^{-\beta h}\right ) ^n$$ where $h$ is $h_i$ but without an index , because the reference is not to a specific particle anymore . /edit you will have to think about , what to do with the momentum integration . i have not calculated the result , but it might be you will not end up with a solution in closed form . there might be an approximation needed to do the momentum summation . edit i think a gaussian integration will do the trick . /edit let us know what you end up with !
this might help understand why the current also obeys a wave equation . the voltage is essentially the gradient of the electric field . so if you have an oscillating voltage , you are trying to set up an oscillating electric field in the wire . the current is related through ohm 's law $\mathbf{j} = \sigma \mathbf{e}$ . now , you cannot have an instantaneous change in \mathbf{e} all through the wire -- there is a time delay corresponding to the time taken for the em disturbance to propagate ( in fact , we should not be talking about voltage in the traditional sense any more , since this is not an electrostatics problem ) . so the " voltage " downstream on the conductor is different from that at the source . and after all , the electrons are driven by the changing electromagnetic field , so they will oscillate too . i am not sure if i am making sense , but in summary , the thought that voltage is constant throughout the conductor is a hang-over from electrostatics , and this need not be the case when we have changing electromagnetic fields .
pair production of electron/positron happens in the electric field of the atoms to satisfy conservation laws and the same will be true for off mass shell z0 going into a neutrino antineutrino pair , interacting with the weak field of the atoms . the equivalent to the photon weak interaction mediator is the z0 and neutrino antineutrino pairs can be formed that way . the weak interaction is orders of magnitude smaller than the electromagnetic one , and thus the probability of getting pairs of neutrinos-antineutrinos is high only in special situations as in the big bang or in a super nova explosion where the density of matter is high and there is energy available . the weak couplings lower the probability of interaction drastically so the advantage of a smaller neutrino mass with respect to the electrons is lost for experiments possible in the laboratory .
just to mention the basics for " other ways"' , you can hit your photon really hard with a fast moving electron or proton , i.e. inverse compton scattering . ics is very important in many astrophysical contexts . if you even reflect the photon off a moving mirror , you can slightly increase or decrease its wavelength
i will attempt an answer , though someone knowing the precise ground realities will most likely improve on my answer . you make a very good point about the speed staying constant if the space ship can be treated as a closed system . that is the sole point that we need to worry about . naturally , the atmosphere within a space ship has to be maintained ( at the values that can support human beings ) . if it was just a case of filling up the shuttle once with $21 \%$ oxygen and being done with it , astronauts would keep consuming it so that its levels would fall , and percentage of ${\rm co}_2$ would keep increasing . that is undesirable and in a simplified description , one can get around this by removing ${\rm co}_2$ via a chemical reaction with lithium hydroxide ${\rm lioh}$ . ( by the way , this is a fairly common use of ${\rm lioh}$ , as a carbon dioxide scrubber in breathing purification systems , as can be seen here . ) upon the reaction , these ''canisters'' can be stored and disposed off later . all the excess water ( i.e. . discounting the potable variety ) is directed to tanks , which can again be disposed later . excess heat is handled by converting to ammonia vapor and subsequent storage . ( though somewhat simplified , a description of this process can be found in the first link of this article . ) so , while space shuttles would ''maintain'' a requisite atmosphere , ( apparently ) nothing gets dumped on there an then basis . now , your question pertained to what would happen if this release happens ( or does not happen ) while the shuttle continues moving ahead at a uniform velocity $v$ - if it got dumped , then $v$ would change . does not seem to be the case . see , everywhere in physics , we make all sorts of approximations , the question is how valid they are in real situations . irrespective of which materials you may choose to build the spacecraft with , it will not make a perfect thermal insulator . while they may try to reduce this radiation loss to as low a value as possible , there will be some amount of heat radiated by the craft . so , while not absolutely ideal , it may be be a good approximation to an insulated body , or in the context , let 's say a closed system . in textbook situations , one always considers simplified descriptions . thus , armed with these links , i think you can safely go and pester your instructor , telling him that his original logic had a flaw ! !
there are two reasons why it may not be necessary . . . firstly the intensity of light from the candle is low unlike that of the laser . secondly , the laser encounters two mirrors while light from the candle only encounters one before reaching the detector .
wind load formula : $f_d = \frac{1}{2} \rho v^2 a c_d$ where $f_d$ is the force of drag ( or in this case force against the flat plate ) $\rho$ is the density of the air $v$ is the speed of the air against the object $a$ is the area of the object which the air is blowing against $c_d$ is the drag coefficient
let me make my comments into an answer : a dalitz plot is a tool for further study of resonances , not for determining their mass . resonances are seen on the plot for the invariant mass distribution , the the square root of the measure of the four vector of the sum of the constituent particles . as with the recent discovery of the higgs . in this plot , which is the invariant mass distribution of the sum of many particles , a number of cuts have been applied to clean up the resonance , and a fit ( red line ) gives the mass . the dalitz plot is for the simpler situation of a decay into three particles . in this case , if there are two resonances for example , the subset of invariant mass plots of pairs will have interference from the kinematic constraints . the plot allows to study this and also gives extra information on the three body parent state , if it is also a resonance , studying the interference patterns .
i have played the game , see my report : http://motls.blogspot.ca/2012/11/a-slower-speed-of-light-mit.html?m=1 and i join m . buettner . i am confident that all relativistic effects are incorporated . it includes the length contraction in the direction of motion , time dilation , but those basic things are rapidly changed by the fact that it really shows what you " see " and not what " is there " at a fixed value of your instantaneous coordinate $t'$ . so the effects that are " purely optical " and depend on the propagation of light and relativistic effects changing it include the relativistic doppler shift – things change the color immediately when you change the speed although the change of your location is negligible at the beginning – and the shrinking of transverse directions if you are moving forward ( or their expansion if you move backwards ) which makes object look " further " ( optically smaller ) if you are moving forward . because of this shrinking , you may effectively see " behind your head " . you also see things how they looked like some time ago . because of the transverse shrinking , you also see straight lines as curved ones if your speed is high enough . one should also verify that the streetcars moving in front of you from the left to the right are " rotated along a vertical axis " . i could not verify this effect but i see no reason to think that their simulation should do it incorrectly . good game . see also real time relativity and velocity raptor . you may get to those sources from my blog mentioned at the top . however , i am confident that the " general relativistic " comments are straw men . if the spacetime is flat , and in the absence of strong gravitational fields , it is , there is no reason why the proper simulation should consider general relativity . special relativity is enough , despite the fact that the child ( and the other stars of the game ) are accelerating . of course , acceleration " tears " solid objects because the proper lengths change asymmetrically etc . but if the material is flexible enough , the objects survive .
electrically charged particles interact via their fields and so there is , in general , wide range interaction throughout the gas . the electromagnetic interactions between particles of the gas/plasma can give raise to effects which are significantly different from neutral gas , such as e . g waves . so to what extend the ideas gas law can be considered to " hold " for a plasma will depend on the parameters of the system , temperature , pressure , etc . , but foremostly of the ionization degree of the gas/plasma . it is an involved issue , as this quantity will depend on all the other parameters . one commonly cited relation for certain parameter ranges is the saha equation , which relates temperature and particle density - which are both part of $pv=k_b t\cdot n$ too . microscopic considerations in such a " chemical system " , where the constituents can be ionized and thereby change their properties , lead you to the observation that the value charge density depends on the surroundings . so e.g. the poission equation takes a nonlinear form $\delta\phi=\rho ( \phi ) $ . it is then also related to new'ish system parameters like the debye length , which caracterize the overall bahaviour you ask for . i am sure there are debye length-temperature ranges where it is perfectly reasonable to apply a gas law , just watch out which part of the system makes up charged particles or neutral ones . e.g. i think in space , there are a whole lot of charged particles , but people work with ideal gas laws . a general rigourous classical look at it will lead you to $pv=k_b t\cdot \text{ln} ( \mathcal z ) $ , where the partition function contains the hamiltonian of the system , which include the potentials $\phi$ = energy expressions involving multiple variable-integrals over statistically weighted interaction potentials , see this link .
ref [ 19 ] in the arxiv paper , c . m . caves and b . l . schumaker , phys rev a 31 , 3068 ( 1985 ) , gives a clean description of a parametric amplifier as the prototype of a two-photon device , at the bottom of its second page : in a [ parametric amplifier ] , an intense laser beam at frequency $2\omega$ —the pump beam— illuminates a suitable nonlinear medium . the nonlinearity couples the pump beam to other modes of the electromagnetic field in such a way that a pump photon at frequency $2\omega$ can be annihilated to create " signal " and " idler " photons at frequencies $\omega\pm\epsilon$ and , conversely , signal and idler photons can be annihilated to create a pump photon . one way to think of the present situation would be as a dual of this description . that is , the medium , the josephson junction , oscillates at a pump frequency $2\omega$ , and interacts nonlinearly with the vacuum state . from the arxiv paper itself , there is a clear parallel , quantum theory allows us to make more detailed predictions than just that photons will simply be produced . if the boundary is driven sinusoidally at an angular frequency $\omega_d = 2\pi f_d$ , then it is predicted that photons will be produced in pairs such that their frequencies , $\omega_1$ and $\omega_2$ , sum to the drive frequency , i.e. , we expect $\omega_d = \omega_1 + \omega_2$ . one of the comments on the nature news page , edward schaefer at 2011-06-06 12:39:04 pm , makes a point that i think is worth highlighting , that " the mirror transfers some of its own energy to the virtual photons to make them real . "
you are not getting your facts right at all . how do we know from this $\langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx$ or this $\hat{h} = -\frac{\hbar^2}{2m}\frac{d^2}{dx^2} + w_p$ that we have an eigenfunctiuion and eigenvalue . answer : we do not . all i know about operator $\bar{h}$ so far is this equation where $\langle w \rangle$ is an energy expected value : \begin{align} \langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx \end{align} no , you do not . here 's the mathematical side of what an eigenfunction and eigenvalue is : given a linear transformation $t : v \to v$ , where $v$ is an infinite dimensional hilbert or banach space , then a scalar $\lambda$ is an eigenvalue if and only if there is some non-zero vector $v$ such that $t ( v ) = \lambda v$ . here 's the physics side ( i.e. . qm ) : we postulate that the state of a system is described by some abstract vector ( called a ket ) $|\psi\rangle$ that belongs to some abstract hilbert space $\mathcal{h}$ . next we postulate that this state evolves in time by some hermitian operator $h$ , which we call the hamiltonian , via the schrodinger equation . what is $h$ ? you guess and compare to experimental results ( that is what physics is anyway ) . next we postulate for any measurable quantity , there exists some hermitian operator $o$ , and we further postulate that the average of many measurements of $o$ is given by $ \langle o \rangle = \langle \psi | o | \psi \rangle$ . connection to wavefunctions : we pick the hilbert space $l^2 ( \mathbb{r}^3 ) $ to work in , so $\psi ( x ) = \langle x | \psi \rangle$ , and $\langle o \rangle = \int_{-\infty}^{\infty} \psi^* ( x ) o ( x ) \psi ( x ) dx$ . ok , that is the end . the form of $h$ does not follow from the energy expected value . wait ! i have not even talked about eigenvalues and eigenfunctions . this is a useless post ! answer : well you do not have to . but it is useful to find the eigenvalues and eigenfunctions of $h$ , because the eigenfunctions of $h$ form a basis of the hilbert space , and certain expressions become diagonal/more easily manipulated when we do whatever calculations we want to do . so to find the eigenvalues of $h$ , we simply solve the eigenvalue equation as stated above : solve \begin{align} h | \psi_n \rangle = e_n | \psi_n \rangle . \end{align} this is in the form $t ( v ) = \lambda v$ . so as alfred centauri says , we simply want to find the eigenfunctions of $h$ . a more subtle question would be , how do we know they exist ? the answer lies in spectral theory and sturm-liouville theory but nevermind for now , as physicists we assume they always exist . so your additional question : $\hat{a} \psi$ is an eigenfunction of operator$\hat{h}$ with eigenvalue $ ( w-\hbar \omega ) $ . well . . . . that just follows straightaway . you said you already proved that $h a^\dagger \psi = ( w - \hbar \omega ) a^\dagger \psi$ . so here $t$ = $h$ , $a^\dagger \psi = v$ , and $\lambda = ( w - \hbar \omega ) $ . which is an eigenvalue equation $t ( v ) = \lambda v$ . thus , $a^\dagger \psi$ is an eigenvalue of $h$ with eigenvalue $ ( w-\hbar \omega ) $ .
you are assuming that the kruskal–szekeres ( u , v ) coordinates have to be defined in terms of the schwarzschild ( r , t ) coordinates , but there is nothing special or fundamental about the schwarzschild coordinates . general covariance says that we can use any coordinates we like . if the k-s coordinates had been the ones originally chosen by schwarzschild , then someone could have come along later and defined ( r , t ) in terms of ( u , v ) . we would then say that a disadvantage of the ( r , t ) coordinates is that they do not cover all four quadrants . in general , if you have a solution to the einstein field equations , it makes sense to extend it as much as possible . physically , if a test particle reaches a nonsingular point in a finite proper time , then its world-line can and should be extended beyond that point , not just terminated there . it is only at singular points that the laws of physics break down and geodesics can not be extended . in the case of the schwarzschild spacetime , extending it in this way results in all four quadrants of the kruskal diagram . the maximally extended schwarzschild spacetime is not a realistic model of a black hole , however . when a black hole forms by gravitational collapse , you do not get quadrants iii and iv .
it is important to remember that quantum field theory is a theory about fields , not particles . i know you said shy away from equations , so i am just going to reference one part of one , and you can see this equation on any o'l web site , like wikipedia . take the dirac equation , here there is a quantity $\psi$ that shows up . and part of the history of this $\psi$ was what it meant . ultimately , it was determined to be a field : the fermion field . this is our fundamental understanding as of now about the world , that there are fields , and that interactions take place between fields , mediated by quantum excitations of these fields . in light of this , the wave function you talked about corresponding to the electron is not the fermionic field i mentioned above . the fermion field can be excited either to produce or destroy certain fermions like electrons and positrons . as far as how deep the oscillator analogy runs , i will just say this : how deep or how far it runs is debatable , but i do not think anyone will argue its fundamental role in developing qft . quantizing fields and placing field variables in terms of canonical field variables is pivotal for an understanding of qft , and before even getting to qft , a good understanding of the sho in quantum mechanics is indispensable . this is because the creation and annihilation of excitations in qft is analogous to the creation and annihilation of energy states in the non-relativistic quantum sho . i hope this helps .
the best calculation of hawking radiation is the one given on wikipedia , in the article for hawking radiation . this is the method implicitly described by unruh in his famous paper on accelerated observers in qft , and it is mathematically equivalent to the method hawking uses in papers of the 1977-78 period , which use periodicity of the analytically continued solution in imaginary time . hawking 's original calculation assumes that the emitted field quanta are non-interacting . the result is clearly true for interacting theories as well , but this is not completely clear in hawking 's original calculation . further , hawking 's 1976 method uses a pure-black hole solution ( not an eternal solution , no white hole ) , meaning that the horizon was created at a finite affine parameter in the past . the outgoing radiation , when traced back in time , is all coming from a sub-microscopic infinitesimal region right at the moment the horizon first formed . during the sojourn next to the even horizon , the peeling-off property of the horizon ( the fact that outgoing light rays peel away from stationary light rays right on the horizon ) lead to a big magnification of the tiny region where the black hole is first formed . any outgoing photon mode , extended back , comes from a place where its wavelength is trans-trans-trans planckian , this is right the moment of formation of the black hole horizon . the trans-planckian property led many people to be skeptical of hawking 's result . analytic continuation is impossible for general metrics , and it seems strange to use an analytic continuation to find a physical temperature . the calculation below is equivalent to the analytic continuation to imaginary time , but explains why this method works . unruh radiation when an observer is accelerating in minkowski space , the trajectory in space time is a hyperbola , which is the relativistic analog of a circle . the natural coordinates for an accelerating observer are the relativistic analog of polar coordinates . choosing the x coordinate to be the direction of the acceleration , these coordinates are given by $$ x = r\cosh ( \tau ) $$ $$ t= r\sinh ( \tau ) $$ the y , z coordinates transverse to the acceleration are unchanged , and the metric in these coordinates is the relativistic polar metric : $$ ds^2 = dr^2 - r^2 d\tau^2 + dy^2 + dz^2 $$ and it is important to compare this the standard euclidean polar metric $dr^2 + r^2 d\theta^2$ . it is obvious then that the time coordinate is like the angle , while the r coordinate is radial . a green 's function for a quantum field in the $x , t$ coordinates analytically continues to imaginary time , this is wick rotation , and it is the whole point of the feynman formalism . the natural hamiltonian for the $\tau$ coordinate analytically continues to a generator of rotations in the x-t plane , so that the $\tau$ coordinate is periodic with period $2\pi$ . this means that the accelerated observer measures a thermal green 's function for the field with a temperature that is diverging as $1\over r$ ( the $\tau$ period is $2\pi$ , but $\tau$ is dimensionless-- the physical local time coordinate is $r\tau$ , so that the physical energies of high frequency quanta ( those that can be localized ) are $1/r$ bigger ) . the divergence of the unruh temperature at the rindler acceleration horizon is obvious--- it is just the statement that a family of observers at different r in the $r , \tau$ coordinates have an acceleration that diverges as $1\r$ . the unruh radiation , when back-traced , also has a trans-planckian wavelength right at the acceleration horizon . the reason is just because any outgoing mode is redshifted an infinite amount from its value near the horizon , and the constant magnification of the near-horizon region is the analytic continuation of the constant rotation in imaginary time ( rotation is by cosines and sines , which continue to cosh 's and sinh 's , giving the magnifying at the acceleration horizon . the bogoliubov coefficients are given by these near-horizon blow-up factors . the bogoliubov formalism is not particularly enlightening . ) . since the unruh radiation is in flat space , there is no debate about it is correctness . you can model an accelerating detector in ordinary minkowski coordinates to see that the detector goes off as if it is immersed in thermal radiation . another important point about unruh radiation is that the only reason you see it all around you is because the radiation going out of the acceleration horizon free-falls back in , to hit you going the other way on its way back to the horizon . none of this thermal background can escape , because the rindler ( pseudo ) gravitational potential well is infinitely steep--- you need to do an infinite amount of energy to get away from an accelerating observer in the direction of acceleration . equivalence principle by the equivalence principle , a near-horizon observer by a black hole can not tell the difference between the black hole and nothing at all . the black hole horizon is identified with the rindler horizon for a nearby stationary observer , which must be accelerating quickly to keep from falling in . this is a powerful statement , because knowing that the black-hole metric is locally rindler ( using the coordinate $s=\sqrt{r-2m}$ ) means that one can continue into the black hole just by assuming that the horizon is not a special location . this continuation past the horizon is standard , it is how the black hole interior is derived from the exterior solution . it is sometimes called " analytic continuation " , but this is a misnomer , as it does not require analyticity to work . it just needs that the metric is asymptotically rindler , so that the spacetime is really still locally minkowksi space in different coordinates , there are no curvatures or singularities , and so continues into the interior . there are not many different stationary parametrizations of minkowski space , if you want the space to look stationary you need to use a rindler coordinate of some type . this means that any locally flat horizon is locally rindler . extremal black holes are not locally flat , and their near horizon limit is ads , not minkowksi , reflecting the diverging curvature at extremality , due to the pinching off of the interior . the local temperature seen by a near horizon observer diverges as $1/s$ , which is proportional to the radial distance to the horizon . this local temperature is equal to the local periodicity of the solution in imaginary time , because this is true in rindler space--- the $\tau$ periodicity translates into a $t$ periodicity near the horizon . but the black hole solution is stationary , which means that once you know the period in imaginary time near the horizon , if you extend the thermal radiation using the gravitational redshift factor , it stays in equilibrium . any outgoing radiation is redshifted to a lower temperature , while any incoming radiation is blueshifted to a higher temperature . the operator which moves $t$ to $t+a$ is not quite a hamiltonian , since it generates rotations near the horizon ( just like the rindler $\tau$ ) , so you find that for equilibrium , the thermal radiation is the unruh radiation plus a bath at a temperature that goes as the inverse redshift . for unruh radiation , the redshift factor , the square root of $g_{00}$ , is $r$ , it goes to infinity at $r=\infty$ . in the schwartzschild solution , the square root of $g_{00}$ is $\sqrt{1-{2m\over r}}$ , and it asymptotes to a finite limit at infinite r . the analytic continuation matching the unruh temperature gives the hawking temperature at infinity to be ( see wikipedia for the simple calculation , it is an exercise ) : $$t_h = { 1\over 8\pi m}$$ the argument from matching to the unruh limit shows why analytic continuation to imaginary time is correct--- the unruh periodicity continues to the long distance periodicity . the generator of time translations is a symmetry , and if it has a period at some position , imposing the condition that the period is constant gives a consistent background for a path integral , and this path integral is necessarily thermal , with asymptotic temperature given by hawking 's formula . physical emission the argument above only shows that the black hole is in equilibrium with a thermal gas whose temperature at infinity is $1\over 8\pi m$ . it also shows that the correct thermal ensemble for the exterior observer living with an interacting quantum field theory is the one from the path integral in an imaginary-time background which is periodic in t with period $8\pi m$ everwhere , with a background metric equal to the analytic continuation of the schwartschild metric . but what if there is no radiation at infinity ? then there is no infalling radiation , and the thermal emissions from the black hole are not compensated , so the black hole glows thermally . the actual radiation from a black hole is given by detailed balance--- the black hole is in equilibrium with a thermal gas at infinity at the hawking temperature , so the emission rate for a free field theory is related to the absorption by the condition that they balance out in thermal equilibrium , and , since the quanta are free , they are independent processes . this gives the graybody factors for black hole emission from a calculation of the absorption at the same wavenumber .
comments to the question ( v1 ) : beware that different authors have different conventions for the horizontal order of indices for the christoffel symbols$^1$ $\gamma^{\lambda}{}_{\mu\nu}$ and the riemann curvature tensor $r^{\sigma}{}_{\mu\nu\lambda}$ . some may e.g. write $\gamma_{\mu\nu}{}^{\lambda}$ and $r_{\mu\nu\lambda}{}^{\sigma}$ instead . it may be useful to write objects such as $\gamma^{\lambda}{}_{\mu\nu}$ and $r^{\sigma}{}_{\mu\nu\lambda}$ with covariant and contravariant indices not merely on top of each other a la $\gamma^{\lambda}_{\mu\nu}$ and $r^{\sigma}_{\mu\nu\lambda}$ but horizontally displaced to keep track of the horizontal position of the indices . then the indices can be raised or lowered by a metric $g_{\mu\nu}$ with no ambiguity in notation of which index was raised or lowered . in case of supermanifolds , the indices may correspond to grassmann-odd coordinates , and it may become a tedious exercise in book-keeping to assign consistent transformation laws with correct sign factors for tensor components under coordinate transformations of supercharts . some choices of horizontal orders of indices may be more natural in the sense of minimizing the appearance of sign factors . we mention this 3rd point because both authors barnich and brandt are experts on brst formalism , where grassmann-odd variables play an essential role . -- $^1$ it is covenient to call $\gamma^{\lambda}{}_{\mu\nu}$ christoffel symbols even if the tangent-space connection $\nabla$ is not torsionfree nor compatible with a metric .
when one proves that a small part of a greater system is described canonical ensemble , even though the greater system is described by a microcanonical ensemble , the key point is that the density of states of the greater system has the exponential form you mention , over a certain interval of energy . specifically what is important is that $\log \rho ( \eta ) = {\rm const} + \beta \eta$ over the range $\eta = \langle\eta \rangle \pm \delta e$ , where $\delta e$ is the energy fluctuation of the small system , and $\langle \eta \rangle = e - \langle e \rangle$ is the expected energy . the value of $\rho ( \eta ) $ is not really important for energies that are very far outside this range , since those energies never occur . in practice the form of $\rho ( \eta ) $ is usually something like $\log\rho ( \eta ) \approx {\rm const} + n \log \eta$ for some very large $n$ ( such a form is found for gases , in particular ) . this log can be taylor expanded to yield the needed form for the canonical ensemble . for a large environment ( one that is essentially in the thermodynamic limit and much larger than the attached system ) , the first order expansion is very accurate over the relevant energy range .
your calculations are correct . the reason we do not feel it is because the 1 atmosphere of pressure will be applied to all surfaces of our body , including the soles of our feet . in fact the interior of our body is also pushing out against our skin with 1 atmosphere of pressure so there is no net force being applied in either direction at the skin 's surface . the only way you could feel this pressure would be if part of your body was in contact with a vacuum while the rest of your body remained in air at 1 atmosphere of pressure . imagine that you were standing on an opening to a vacuum chamber that was almost , but slightly less than the size of the soles of your feet . if there was a perfectly air tight fit between the vacuum opening and your feet , you would indeed feel that pressure as that much force on your feet just as you calculated . as a simpler experiment , just put your hand over the hose of a vacuum cleaner - that is only a very partial vacuum but the force you need to exert to remove your hand is due to the 1 atmosphere of pressure around us all pushing your hand onto the partial vacuum . in fact if you measured that force needed to pull your hand off of the vacuum cleaner hose , you could compute what the air pressure in the vacuum hose was .
i believe that andy and cumrun did not want to say that this manifold would have a complex structure modulus in isolation . however , as is clear from the " conifold " setup , the manifold given by $xy-zt=\mu$ is being incorporated into a larger manifold , so this equation only describes the vicinity of some region . when you exploit the fixed asymptotic shape of the $xy-zt=\mu$ manifold which is $\mu$-independent and when you extend this deformed conifold geometry into a larger manifold , such as the quintic , the parameter $\mu$ connected with the neighborhood of the ( deformed ) singularity becomes a complex structure modulus labeling inequivalent complex structures of the whole ( complicated ) calabi-yau manifold . alternatively , you could count the single complex modulus even for this simple manifold itself but you would have to impose the constancy of the asymptotics i.e. ban your " scaling " transformations that were used to show the equivalence regardless of $\mu$ .
a simplified approach would put a lens with low distortion within its fov at a specific distance to the screen . luckily these screen magnifiers are commercially available for a 12" display , about 40us$\$$ . the principle of a magnifying glass with just one lens is applied . remember the the thick and heavy lenses of a magnifying glass . a frensel lens may be used to get a thinner lens .
a positive cosmological constant leads to positive scalar curvature by definition . just trace over the einstein equation and you end up with $$ r = 4\lambda - 8\pi t $$ which is just $$ r = 4\lambda &gt ; 0 $$ in vacuum . the implicit , but more interesting questions are probably the following ones : why can we interpret the cosmological constant as dark energy ? modelling matter as a fluid in equilibrium , ie $$ t_{\mu\nu} = ( \rho + p ) u_\mu u_\nu + p g_{\mu\nu} $$ the einstein equation reads $$ r_{\mu\nu} - \frac 12 r g_{\mu\nu} = 8\pi ( \rho + p ) u_\mu u_\nu + ( 8\pi p - \lambda ) g_{\mu\nu} $$ now , if we want to fold the $\lambda$ term into the matter terms , we require $$ \rho_\lambda + p_\lambda = 0 \\ 8\pi p_\lambda = -\lambda $$ which is $$ \rho_\lambda = -p_\lambda = \frac\lambda{8\pi} $$ a positive energy density with negative pressure . take note that this pressure is not directly responsible for any acceleration or deceleration of the cosmological expansion : it is uniform across space and stays constant in time , and lacking a gradient , does not induce any forces . its effect is purely gravitational in nature - after all , this is just the cosmological constant in disguise . does positive spacetime curvature actually lead to parallel geodesics converging ? not necessarily due to the lorentzian signature of the metric . take 1+1 de sitter space , which can be realized as a hyperboloid in minkowski space and would look like this ( picture taken from wikimedia commons ) : we get geodesics from the intersections of planes through the origin of the ambient minkowski space with the hyperboloid , and time-like ones from those which are angled less than 45° towards the time axis . the vertical lines thus correspond to time-like geodesics and clearly do not converge . this is where slicing into space-like hypersurfaces comes in : in flrw cosmology , there is a preferred slicing where the galactic fluid is homogeneous . in de sitter space , there is no matter and thus no preferred slicing , but we can nevertheless use it to illustrate various features of the cosmological standard model . the horizontal circles , which we obtain by intersecting a parallel family of planes in the ambient space with the hyperboloid , correspond to a spatially closed universe . choosing appropriate coordinates yields the metric $$ ds^2 = -dt^2 + \alpha^2\cosh^2\left ( \frac t\alpha\right ) d\omega^2 $$ where $d\omega$ is the metric of the euclidean sphere and $\alpha=\sqrt{3/\lambda}$ . by tilting our planes , we can also create flat slicings with corresponding metric $$ ds^2 = -dt^2 + e^{2t/\alpha}dy^2 $$ and open slicings with metric $$ ds^2 = -dt^2 + \alpha^2\sinh^2\left ( \frac t\alpha\right ) dh^2 $$ where $dh$ is the metric of the euclidean hyperbolic space . while the light-like geodesics shown above - corresponding to particles at rest in case of the closed slicing - diverge , the spatial curvature will determine what happens to particles in parallel motion through space . however , this is not something that can be shown in our picture of a 1+1 spacetime . how does this result in an accelerated expansion of the universe ? looking at the spatial part of the metrics , all three slicings ultimately lead to an exponential expansion of space , which , in case of a de sitter universe , is just a matter of geometry . in the closed case however , accelerated expansion happens only after a decelerating collapse to some minimal size determined by the value of the cosmological constant . in friedmann models , as long as the cosmological constant dominates over the matter content , we will eventually approach de sitter geometry and thus also exponential expansion .
it does not quite work like that . each of the three flavors is a quantum superposition of the three mass states . so if you make an electron neutrino , for example , you get a combination of the lightest neutrino state , the middle mass neutrino state , and the heaviest neutrino state . ( you can look up the coefficients but they are not important here . ) as the neutrino propagates through space , each of these three mass states goes on its way individually , just as a regular particle would . so there is nothing particularly strange about the way neutrinos interact with gravity .
none , really . such junctions form in semiconductor crystals . those are remarkable materials . let 's look at electrons in solids first . many atoms have weakly bound valence electrons in outer orbits . these orbits would have specific energies if the atom existed in isolation . but when man atoms are packed together and their outer orbits overlap , the energy of these orbits shift slightly . electrons in those orbits now can have a band of possible energies . now those weakly bound electrons are good for carrying currents , but to hop through the material the overlapping orbits better not be completely filled . there would not be room for the moving electron . in metals , there is in fact a band which is about half filled . ideal - there is plenty of room for electrons to move , but also still enough electrons to move . semiconductors have a full and an empty band close together , and with some external help ( doping , electric fields , etc ) this can be used to switch from conducting to non-conducting . regardless , as the outer orbits overlap and join to form a band , the electrons in that band no longer belong to a single orbit and therefore a single atom . at the quantum level , the probability function of the electron is smeared out over the crystal . and thus it is not sensible to talk about the exact atoms ionized .
this is really an open problem . even for the class of problems which quantum computers are known to be fast at - and shor 's algorithm in particular - there is no ' hard ' proof that classical computers must be slow there . ( to be clear : i do not think any serious computer scientist expects factoring to be in p , but there is no formal proof that it is not . ) it is not clear to me what you mean by ' finite state machine ' . shor 's algorithm does require a universal quantum computer , but any implementation must have finite registers and their size will determine the size of integer they can factor out . from what i can make out , you are asking whether there are special-purpose quantum devices that provide quantum speed-ups , compared to the best classical algorithms , for a particular problem . if that is the case , you may want to look at linear optical quantum computing implementations of the boson sampling problem , which is exactly in that class . some places to look are the computational complexity of linear optics . s aaronson and a arkhipov . proceedings of the 43rd annual acm symposium on theory of computing ( stoc '11 ) , pp . 333-342 . full paper ( 96 pages ) at arxiv:1011.3245 [ quant-ph ] . for a more understandable reference , try this blog entry by aaronson .
nic and approximist 's answers hit the main points , but it is worth adding an additional word on the reason the orbits lie roughly in the same plane : conservation of angular momentum . the solar system began as a large cloud of stuff , many times larger than its current size . it had some very slight initial angular momentum -- that is , it was , on average , rotating about a certain axis . ( why ? maybe just randomly ! all of the constituents were flying around , and if you add up those random motions , there will generically be some nonzero angular momentum . ) because angular momentum is conserved , as the cloud collapsed the rotation rate sped up ( the usual example being the figure skater who pulls in her arms as she spins , and speeds up accordingly ) . further collapse in the direction perpendicular to the plane of rotation does not change the angular momentum , but collapse in the other directions would change it . so the collapse turns the initial cloud , whatever its shape , into a pancake . the planets formed out of that pancake . by the way , you can see the signs of that initial angular momentum in other things too : not only are all of the planets orbiting in roughly the same plane , but so are most of their moons , and most of the planets ' rotations about their axes as well .
physically what is happening is this : when you touch the positively charged source to the conductor ( the metal sphere ) , electrons leave the conductor through the point of contact . this leaves the point of contact on the conductor with a large deficit of electrons , and thus the point has a positive charge density . the positive charge density produces an electric field in the conductor , which immediately pulls on remaining electrons in the conductor . the electrons remaining spread out until they have eliminated all of the electric fields in the conductor ( if there were remaining fields , the electrons would continue to rearrange ) . the electrons will now be ' more spread out ' than the protons ; the difference between the new electron surface density and the original tells you the distribution of ' excess positive charge ' on the surface . i hope this helps , let me know if you have an application in mind for this ; i often times find it helpful in thinking about problems to temporarily ignore the fact that in practice there is only one charge carrier ( the electron ) and just think about excess positive charge as positively charged particles spreading out .
most materials contract on cooling . the notable exception to the rule are some phase transitions and water . but even ice contracts on cooling . water expands on cooling only between $0^\circ\text{c}$ and $4^\circ\text{c}$ ( including phase transition ) . this corresponds to the part of the graph below , in which density rises with temperature ( note suppressed zero ) . as regarding to what material contracts most , what are you looking for is the coefficient of linear thermal expansion $\alpha$ $$\frac{\text{d}l}{l} = \alpha \delta t . $$ there is plentiful of various tables available on web , e.g. http://www.engineeringtoolbox.com/linear-expansion-coefficients-d_95.html it seems plastic materials contract most on cooling . ethylene ethyl acrylate ( eea ) for example has the largest one coefficient among the solids in this table .
assume sinusoidal grate -- sin(x) is the simplest possible periodic function . direct axis x horizontally , and y vertically , and lets calculate diffraction at the point (x,y) . according to fraunhofer diffraction we must first calculate distance from the observation point (x,y) to the source of secondary wave (x_1, h*sin(d*x_1))) which is r=sqrt((y-h*sin(d*x_1))^2+(x-x_1)^2) . here the h is the height of the ripple , and d its frequency . this r is plugged in into the amplitude integral Integrate(i*exp(-2*pi*i*r/lambda)/(r*lambda),x1=-R..R) . mathematica expression : Integrate[i*exp(-2*pi*i*sqrt((y-h*sin(x1))^2+(x-x1)^2)/lambda)/(sqrt((y-h*sin(x1))^2+(x-x1)^2)*lambda),{x1,-d,d}]  maple : Int(i*exp(-2*pi*i*sqrt((y-h*sin(d*x1))^2+(x-x1)^2)/lambda)/(sqrt((y-h*sin(d*x1))^2+(x-x1)^2)*lambda),x1=-R..R);  neither succeeded solving it analytically , so one have to regress to numerics . therefore , i assigned the following numeric values : the radius of the flat mirror : R=20 mm  wavelength : lambda=0.0004 mm (=400 nm)  defect density/frequency : d=10 (10 ripples/mm -- with higher values Wolfram alpha times out)  defect height h=0.0001 mm (1/1 wave)  location y=100 mm (10 cm above the mirror).  with all the assignments the only free variable remaining is x , so one should be able to plot intensity graph . unfortunately , wolfram alpha refuse to understand what Plot[Integral[]] is and suggests some stock market graphs instead . i had to calculate pointwise . here is an expression which calculates intensity at the axis ( x=0 ) : Integral[i*exp(-2*pi*i*sqrt((100-0.0001*sin(10*x1))^2+(0.0-x1)^2)/0.0004)/(sqrt((100-0.0001*sin(10*x1))^2+(0.0-x1)^2)*0.0004),x1=-20..20]  which walpfa evaluates to 0.62+3.81i the characteristic angle of periodic grating is lambda*d so the diffraction pattern linear dimension is lambda*d*y which in our case is conveniently 1 . therefore , we could see diffraction pattern by probing only three points : x=0.0 , x=0.5 , and x=1.0 . here are the intensities : x=0.0 -&gt; 0.62+3.81*i x=0.5 -&gt; 7.76+3.0*i x=1.0 -&gt; 3.51+1.8*i  in other words , the diffraction pattern is quite noticeable when defect size is comparable to lambda . to doublecheck , what if we shrink grating height 10 fold ? there : x=0.0 -&gt; 3.4+3.5*i x=0.5 -&gt; 4.1+3.1*i x=1.0 -&gt; 3.88+3.33*i 
since $\mathcal{h}$ is anti-self-adjoint , we have $$\langle f , \mathcal{h}f\rangle=\frac{\langle f , \mathcal{h}f\rangle-\langle \mathcal{h}f , f\rangle}{2}=0$$ since $f=|a|_x^2$ is real . thus $\frac{dp}{dt}=\frac{\alpha}{2}\langle f , \mathcal{h}f\rangle=0$ ( unless i am misreading your notation ) .
you asked a total of three questions , so i will answer each one in turn . but first , how much do you already know about what you see when light diffracts from an object ? i assume you are aware of the fact that the reciprocal lattice is the fourier transform of the crystal ? in general , there is a direct analogy between x-ray diffraction and directional phased-array radio broadcasting , and which is very helpful in understanding x-ray diffraction . if you had like , i can explain this analogy in another post , but the end result is that if you blast an object with x-rays with wavevector ${\bf k}$ , the object will re-emit that light in all directions , and the intensity of that emitted light as a function of angle is proportional to $$f ( {\bf \omega}+{\bf k} ) $$ where $f$ is the fourier transform of the object 's density function $f$ , and ${\bf \omega}={\hat n}|{\bf k}|$ where ${\hat n}$ is the unit vector pointing out of the crystal and in the direction of emission . as a result , a crystal 's x-ray diffraction pattern is determined by the fourier transform of it is underlying lattice , ie , the reciprocal lattice . now assume you have an imaging sphere ( ie , a sphere whose interior surface is covered by photographic film ) , you place the crystal in the center , you fire x-rays in through a pinhole , and they strike the crystal and diffract , striking the film on the walls and forming an image . what will the image look like ? looking at the expression ${\bf \omega}+{\bf k}$ , you will note that as ${\bf \omega}$ is varied in all directions , it traces out the surface of a bubble in reciprocal space whose surface touches the origin , whose radius is $|{\bf k}|$ , and whose center is located at ${\bf k}$ . this bubble is something called the " ewald bubble " . the most important intuition is this : the image recorded on the photographic film after bombardment is a photograph of the ewald bubble in reciprocal space . take a moment to think about that and let it sink in . what it is saying is that x-ray diffraction hands us a picture of the crystal in fourier-transform space . ( however , note that the formula $i ( {\bf \omega} ) \propto f ( {\bf \omega}+{\bf k} ) $ is perfectly general , and does not assume that the object $f$ is a crystal , is periodic , or anything else . so it works for non-periodic structures like a single unit cell as well . ) we can get a complete reconstruction of the reciprocal lattice by rotating the crystal while imaging . applying various rotations matrices $r ( \theta , \phi ) $ to the crystal ( which is equivalent to sending in the x-rays at different directions ) , we find that the set of images $s$ obtained is $$s=\{f ( {\hat n}|{\bf k}|+r ( \theta , \phi ) {\bf k} ) , {\hat n}\in\mbox{all directions} , \theta\in [ 0 , \pi ] , \phi\in [ 0,2\pi ] \}$$ $$=\{f ( {\bf r} ) , |{\bf r}|\leq2|{\bf k}| \} . $$ the last equality may take a bit of visualization , but once you see it it is pretty obvious ; essentially you just swing around the ewald bubble in all directions while keeping it is surface attached to the origin , which traces out a solid sphere of radius $2|{\bf k}|$ . the important intuition is this : using x-rays with inverse wavelength $|{\bf k}|$ , you can reconstruct the object in fourier transform space out to a radius of $2|{\bf k}|$ . now to finally try to answer your questions : if i take an xrd image of a single cubic unit cell , would the diffraction pattern simply be its reciprocal lattice ? no . it would be the ewald bubble image of the fourier transform of the density function of that single unit cell . the reciprocal lattice is the fourier transform of the density function of the periodically-repeated unit cells , not just one cell . the ft of a single cell would be something much blurrier , and obviously in real life it would be super hard to mechanically obtain a single unit cell of a crystal , let alone image it . i was wondering if these dots are reciprocal lattice points of the structure . yes , sort of . applying the formula $i ( {\bf \omega} ) \propto f ( {\bf \omega}+{\bf k} ) $ to a powder can be done by imagining that the powder is actually millions of little crystals , all oriented randomly . since rotation commutes with fourier-transformation , this yields a reciprocal-space function for the powder of $$f_r ( {\bf r} ) =\int_{so ( 3 ) } f ( r ( \chi ) \cdot {\bf r} ) d\chi$$ where the integral is over all the euler angles . it should be geometrically obvious that this function is just the reciprocal space after being smeared out by rotations , in a similar manner as to that observed when amateur astronomers photograph the night sky using timed-exposure film . as a result , the powder reciprocal space is radially symmetric about the origin , and the ewald bubble intersects the smeared-out reciprocal lattice points to generate rings on the surface of the bubble . these surface rings manifests in real life space as conical emission patterns , as shown in this diagram : http://upload.wikimedia.org/wikipedia/commons/c/c2/hex-2d-diffraction.png and if we were not looking at a powder but just a 2d hexagonal lattice , then the xrd pattern would simply be its reciprocal lattice ? yes . or at least , it would be the image of the ewald bubble of radius $|{\bf k}|$ in reciprocal lattice space .
consider a conformal theory of fields $\phi$ defined on minkowski spacetime $m=\mathbb r^{3,1}$ and with target space $v$ . let $\rho_m$ denote a representation of the conformal group $g$ on $m$ and let $\rho_v$ denote a representation of $g$ on $v$ . then we can define an action $\rho_f$ of $g$ on fields $\phi$ as follows : $$ ( \rho_f ( g ) \phi ) ( \rho_m ( g ) x ) = \rho_v ( g ) \phi ( x ) $$ this is a more notationally descriptive version of di-francesco eq . 2.114 ; $$ \phi' ( x' ) = \mathcal f ( \phi ( x ) ) $$ in other words , the action of $g$ on fields has two parts , a spacetime part , and a target space ( aka internal ) part . now , suppose that we find that there is another representation $\rho_d$ of $g$ acting on fields ( by way of differential operators for example ) for which $$ \phi ( \rho_m ( g ) x ) = \rho_d ( g ) \phi ( x ) $$ then notice that the first equation above can be written as follows : $$ ( \rho_f ( g ) \phi ) ( x ) = \rho_d ( g^{-1} ) \rho_v ( g ) \phi ( x ) $$ how does this manifest itself in terms of signs and generators ? well , suppose that for any group representation $\rho$ of $g$ , we have a corresponding , linear representation $r$ of its lie algebra $\mathfrak g$ such that if we write an element $g\in g$ as the exponential of a lie algebra element $x$ ; $g = e^x$ , then $$ \rho ( g ) = e^{r ( x ) } $$ then we can write the transformation of the fields as $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-r_d ( x ) }e^{r_v ( x ) }\phi ( x ) $$ for any $x\in\mathfrak g$ . if these lie algebra representations commute ( as they would if , for example , $r_d$ is a representation via differential operators and $r_v$ is some spacetime-independent representation on the target space ) , then we can write $$ ( \rho_f ( g ) \phi ) ( x ) = e^{r_v ( x ) -r_d ( x ) }\phi ( x ) $$ now , suppose that we choose a basis $x_a$ for the lie algebra $\mathfrak g$ such that every element $x\in \mathfrak g$ can be written as $$ x = i\omega_ax_a $$ for some real numbers $\omega_a$ . then using linearity of the representations $r_d , r_v$ we have $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-i\omega_a ( r_d ( x_a ) - r_v ( x_a ) ) }\phi ( x ) $$ so if we use di-francesco 's notation $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-i\omega_ag_a}\phi ( x ) $$ then we have $$ g_a = r_d ( x_a ) - r_v ( x_a ) $$ to see that this leads to consistent signs etc . , let 's look at an example : example . translations consider a field whose target space transforms trivially under translations ; $$ ( \rho_f ( g ) \phi ) ( x+a ) = \phi ( x ) $$ then if $ia^\mu p_\mu$ is the lie algebra element that corresponds to translations $x\to x+a$ , and if we define $$ r_v ( p_\mu ) = 0 , \qquad r_d ( p_\mu ) = -i\partial_\mu $$ then we have $$ g_\mu = -i\partial_\mu $$ so that $$ e^{-ia^\mu g_\mu}\phi ( x ) = e^{-a^\mu\partial_\mu}\phi ( x ) = \phi ( x-a ) = ( \rho_f ( g ) \phi ) ( x ) $$ as desired .
remember that your expression gives the energy difference per unit volume . so you need an additional factor of $\left [ l^{-3}\right ] $ on the left hand side .
measuring dead-time ( or other hardware efficiencies ) is a non-trivial proposition , and there is no completely general solution . the answer that john gives in the comments ( $\tau \times \text{number of events}$ ) is the best case : a system with few interconnections and no " extensible " contributions to the dead time . " extensible " describes a system where events coming in during the dead time re-set the recovery clock . because very high rates can leave such system dead essentially all the time such systems are also called " paralyzable " . so let 's try to categorize a few cases intrinsic dead-time of individual cascading particle detector elements . there are generally of the non-extensible , fix-time per event variety . often with quite a short $\tau$ . you measure dead time by counting events and you design your experiment to keep the total small . dead-time due to a background veto system ( cosmic ray veto for a activity measurement or some such ) . depending on the electronics design these can be extensible or non-extensible , but they probably should be extensible . yuck . if you are getting much extensions you need to reduce the background . the kamland muon veto was in principle extensible but the rates were so low that it did not happen often eoungh to matter . because muon vetos were recorded in the data stream the total dead time could be simple added up later . dead times that arise from veto signal can be estimated by counting a clock signal as vetoed and raw . of course , you need to have designed this in . computer dead-time due to read-out delays or a rate-limiting by imposed sampling window . this is generally a non-extensible dead time characterized by a simple counting again . the only issue here is that the $\tau$ may need to be evaluated by monte carlo if the system consists of multiple parts with different response times . ( because a trigger that occurs near the transition between " live " and " dead " periods might be missed if one part of the signal is registered on the other side of the cut off ; i needed to do that for a kamland study once . )
consider the system in equilibrium , with the rod hanging straight down . imagine taking a marker and drawing a vertical diameter across the disk . if the rod were fixed to the disk 's center so that the disk could not rotate , then would this marker line still be vertical mid swing ? what does this tell you about the rotation of the disk around its central axis relative to its starting position ? if the disk were not rigidly attached as in the former case , would there now be any torque on the disk around its central axis ? what would the marker line look like mid swing in this case ?
the result is straight forward . as landau and lifshitz explain in p . 41 , when a body disintegrates into two pieces of masses $m_1$ and $m_2$ respectively , their momenta must be equal in magnitude and oppositely directed by the law of conservation of momentum . so , let each body have momentum $p_0$ . then , $ ( 16.1 ) $ and $ ( 16.2 ) $ say that the difference in the internal energies equals the kinetic energy of the reduced mass . $e_i - e_{1i} - e_{2i} = \frac{p_0 ^2}{2m}$ where $\frac{1}{m} = \frac{1}{m_1} + \frac{1}{m_2}$ let us now see what happens when the body splits into more than two parts . of course , we cannot say anything about the magnitudes and directions of the momentum of each body . but , we shall try to obtain some information about maximum possible kinetic energy and so on . so , now we have one part with mass $m_1$ that we are interested in . if we sum the momenta of the remaining parts , we can conclude that the total momentum of the remaining parts is equal in magnitude and directed oppositely to the momentum of $m_1$ just like the previous case . let it be $p_0$ . hence , we can now use the same equations for masses $m_1$ and $m-m_1$ $\frac{p_0 ^2}{2} ( \frac{1}{m_1} + \frac{1}{m-m_1} ) = e_i - e_{1i} - e_{i}^{'}$ which on simplification gives $\frac{p_0 ^2}{2m_1} \frac{m}{m - m_1} = e_i - e_{1i} - e_{i}^{'}$ and the result follows .
as jan noted , the hamiltonian should have a minus sign : $h=\frac{ ( p-qa ) ^2}{2m}$ where $p$ is the canonical momentum , and the expression $p-qa$ is the kinetic momentum $p$ . a homogenous magnetic field is an interesting case , because the vector potential in a given gauge does not exhibit translation invariance , but the physical system clearly does . the solution to this dilemma is that you can preserve translational invariance by changing the gauge as you move the coordinates . there is a conserved quantity associated with this symmetry , but it turns out not to be the canonical momentum ( or the kinetic momentum , either ) . i do not know if it has a particular name , and since it is gauge-dependent there is no universal expression you can write for it . but , for example , in the gauge where $a=\frac{b}{2} ( -y , x ) $ , it is just $ ( p+qa ) $ . if anyone has an insight into any physical interpretation of this quantity i would be interested to hear it . there is a very nice example of all this , worked out concretely , in these notes .
turns out the book ( not paper ) is on google books ( hopefully the link works , otherwise go to books . google . com and search " viscosities of solutions and mixtures" ) . it says for salt ( nacl ) , the values for the three-parameter equation , $$ \frac{\eta_s}{\eta_0} = 1+a\sqrt{c}+bc + cc^2 , $$ are given by $$ a=0.0062 \quad b=0.0793 \quad c=0.0080 $$ unfortunately , the one-parameter values are not given in that book ( at least from my perusal ) , but i imagine you might be able to approximate it by comparing estimations with the above three-parameter values . ps : sorry about not responding to that comment , seems i totally missed that in my feed . hopefully this answer makes up for that !
the angular size of the object can be calculated by basic trigonometry : $\theta=2\cdot \arctan ( r/d ) $ , where $r$ is the radius of the object you are viewing , and $d$ is the distance between you and the object ( $\theta$ is the angle ) . the average ( volumetric ) radius of saturn is 58,232 km . the distance between titan and saturn is 1,221,830 km . plugging the numbers in gives an angular size of 5.46° . doing the same for our moon gives you 0.52° . dividing one by the other gives you a factor of $\sim 10.5$ difference . &nbsp ; note 1: when you do this math with a calculator , verify you get the correct results for the moon from earth before you go on to something else . you may encounter issues where the results of your arctan ( ) function will be given in radians , not degrees . if the math gives you a weird result , multiply by $180/\pi \approx 57.3$ . note 2: saturn would not actually be visible from the surface of titan due to the thick atmosphere of the moon . also , tidal locking has nothing to do with this problem other than if saturn may be visible from an arbitrary location on titan ( if you could see through its atmosphere ) .
best advice is : do not try to clean them ! a little dust has absolutely no effect on the images , but a bad cleaning can damage the lenses or their coatings irreparably . if you must clean them , use the techniques here : http://www.televue.com/engine/tv3_page.asp?return=adviceid=103 cleaning the mirrors of reflector telescopes is even less desirable and requires even more care : http://www.backyardastronomy.com/backyard_astronomy/chapter_15__polar_alignment,_collimation,_cleaning_and_testing_of_telescopes_files/appendix%20b-collimation.pdf
the answer is well-known from the analysis of open strings in perturbative string theory : at the end points , the coordinates $x ( \sigma ) $ must also satisfy either neumann or dirichlet boundary conditions for the very same reason that you mentioned . in fact , i think that you have almost answered your own question . the variation of the field is zero . but the very point of the variational calculus - or the principle of the least action - is that the variations may change an allowed configuration to any other nearby allowed configuration . so if you want a constraint of the kind $\delta x ( 0 ) =0$ - or , in your ads case , $\delta \phi|_{y*} = 0$ , the only reason to guarantee it is to impose the condition $x ( 0 ) =const$ or $\phi|_{y*}=const$ for the allowed configurations . you may imagine that the constant is zero - it is just a convention . but because it is a constant , the variation - the difference of the value of $x ( 0 ) $ between two nearby allowed configuration - vanishes . setting a scalar field at the boundary equal to a constant is called the dirichlet boundary condition . alternatively , the boundary term in the variation - the second term in equation 7 - may vanish because the arbitrary nonzero $\delta \phi$ is multiplied by a vanishing coefficient . the coefficient is called $b_{curly}\phi$ in the equation but $b_{curly}$ is an operator proportional to $\partial_5$ . so this method to make the boundary term vanish says that $\partial_5 \phi=0$ at the boundary , and this condition is called the neumann boundary condition . just to be sure , the $d_{curly}$ in the first term of equation 7 is also an operator - and it contains second derivatives . note that the first term has second derivatives integrated over the 5d space , while the second term has first derivatives integrated over the 4d space , so the dimensions agree .
light is a wave - an electromagnetic wave . radio waves and microwaves are also electromagnetic waves , they just have different wave lengths . wikipedia has a nice picture showing the electromagnetic spectrum why does it have a wavelength . . . it has a wavelength because there is physical space between the peaks of the waves - it is a real , physical , wave . just like water waves and sound waves , you can do " wave things " to light waves , such as send them through diffraction gratings and see the interference . and what creates its wavelength whatever creates the light gives it energy , and the wavelength is proportional to that amount of energy : $$\lambda = \frac{hc}{e}$$ does the light vibrate too ? if so , then how ? sound waves are energy waves that compress matter - you can not have a sound wave in a vacuum . light waves are energy waves too , but they do not need matter to go forward . that is why we can see sunlight , but we can not hear the sun . ( and that is why in space , no one can hear you scream . ) this subject can get really complicated really fast . because although light is a wave , it is also a particle . a lot of really smart people have been scratching their really smart heads over that , and will be for a long time .
the white cloud you see in the water is steam bubbles . the grains of salt provide nucleation sites that allow the water to vaporize as they fall through the superheated liquid ( so bowlofred had it right--although it is steam that is forming , not dissolved gasses coming out of solution ) . if you raise a pot of water to near boiling and toss in a handful of salt , the water can explode out of the pot due to this effect . counter to what you might think , the addition of salt to water actually raises the boiling point by about one half degree celsius for every 58 grams of salt dissolved per kilogram of water . so the steam is not caused by salty water around the dissolving salt crystals boiling at a lower temperature . the nucleation effect diminishes as the salt diffuses throughout the water . note that the effect is not limited to salt . if you toss in something that does not dissolve--like sand--you will see the same nucleation effect .
i will assume that both lasers are of the same type , i.e. that both are generated from the same physical mechanism . all lasers have some tuning range over which their wavelengths can vary . so , even though both are very monochromatic ( single frequency ) they will , in general , not have the exact same frequency . a hene laser , for example , has a tuning range of $1\ \text{pm}$ or $\sim1\ \text{ghz}$ . which frequency actually lases depends on many things , but it can be tuned by changing the temperature of the laser or by adjusting the resonator length inside of the cavity . so , what happens when you overlap two different lasers is that you will get an interference pattern between the two beams . the interference pattern will oscillate between bright and dark at the difference frequency of the two lasers ( the beat note ) . since your eye can only see frequencies up to $\sim30\ \text{hz}$ you will not generally be able to see the interference pattern . if you use a lens to focus the two beams onto a fast photodiode , you can see this beat note on an oscilloscope or spectrum analyzer though . if your lasers are adjustable , then you can use this beat note between the two lasers to phase lock them to each other . doing so requires some knowledge of electronics and control theory , but it will lock the frequency of the two lasers together so that as one laser drifts the other follows identically . if you phase lock the two lasers , then you will be able to see the interference pattern with your eye .
as stated , $\mathbf{n}$ is a unit vector and $n_x$ , $n_y$ and $n_z$ are its cartesian components . $\mathbf{n}$ is just a vector pointing in an arbitrarily direction with magnitude 1 . taking $\mathbf{n} \cdot \mathbf{\sigma}$ , we have \begin{equation} \mathbf{n} \cdot \mathbf{\sigma} = n_x\sigma_x + n_y \sigma_y + n_z \sigma_z \\ = n_x \left ( \begin{array}{cc} 0 and 1\\1 and 0\end{array}\right ) + n_y \left ( \begin{array}{cc} 0 and -i\\i and 0\end{array}\right ) + n_z \left ( \begin{array}{cc} 1 and 0\\0 and -1\end{array}\right ) \end{equation}
this vector potential can be written in every point on the plane except the origin as : $$ a_x = -\frac{\partial \psi}{\partial y}$$ $$ a_y = \frac{\partial \psi}{\partial x} $$ with $$\psi = \frac{1}{2}\mathrm{log} ( x^2+y^2 ) $$ $a$ is not exact , because $\psi$ is singular at the origin . but this means that the magnetic field is zero at every point except the origin . at the origin itself , the magnetic field must be infinite , because the flux through an arbitrary small loop is nonvanishing : $$\phi = \int a = \int_0^{2\pi} d\phi = 2 \pi$$ such a magnetic field can be generated by a infinite solenoid whose radius is shrunk to zero while keeping the flux constant . in the hydrodynamical terminology , the function $\psi$ is called the stream function , it satisfies the laplace 's equation ( harmonic function ) except at the origin . this specific stream function describes a vortex ( the vector potential describes the velocity field of the vortex ) . the streamlines of this velocity field are circles around the origin and its magnitude is inversely proportional to the radius . in order to see that the stream function is harmonic except at the singularity , and generalize the construction to the case of the sphere , we can use complex coordinates on the plane : $$ z = x+iy$$ in this representation we have : $$\psi = \frac{1}{2}\mathrm{log} ( \bar{z}z ) $$ applying the laplace operator , we get $$\nabla^2 \psi=\partial_{\bar{z}} \partial_z \psi = \delta^2_l ( z ) $$ where we have used $$ \frac{\partial}{\partial \bar{z}}\frac{1}{z} = \delta^{ ( 2 ) }_l ( z ) $$ is the complex coordinate on the plane . $\delta^{ ( 2 ) }_l$ is the two dimensional dirac delta function with respect to the lebesgue measure . i.e. , $$\int_{\mathbb{c}} f ( z ) \delta^{ ( 2 ) }_l ( z-z_0 ) \mathrm{dre} ( z ) \mathrm{dim} ( z ) = f ( z_0 ) $$ the vector potential in the complex representation has the form : $$ a_z = \frac{1}{i}\frac{\partial \psi}{\partial \bar{z}}$$ $$ a_{\bar{z}}= -\frac{1}{i}\frac{\partial \psi}{\partial z} $$ explicitly : $$a = \frac{1}{2i}\frac{zd\bar{z}-\bar{z}dz}{\bar{z}z}$$ this fact describes another physical interpretation of this vector potential as follows : in two dimensions a function satisfying the laplace 's equation ( harmonic function ) ( except at the point singularities ) qualifies to be a stream function whose anti-symmetrized gradient ( which is the vector potential in our problem ) describes the velocity field of a vortex . please observe that that this velocity field is invariant under rotations about the origi n and its magnitude is inversely proportional to the distance from the origin . in this interpretation , the line integral of the vector potential is the vorticity . we can change the location of the singularity ( flux line ) to any other point in the plane , say $ ( x_0 , y_0 ) $ $$a = \frac{ ( x-x_0 ) dy - ( y-y_0 ) dx}{ ( x-x_0 ) ^2+ ( y-y_0 ) ^2} = \frac{1}{2i}\frac{ ( z-z_0 ) d\bar{z}- ( \bar{z}- \bar{z_0} ) dz}{ ( \bar{z}- \bar{z_0} ) ( z-z_0 ) } $$ in this case it is not hard to verify that this vector potential can be derived from the stream function : $$\psi = \frac{1}{2}\mathrm{log} ( ( \bar{z}-\bar{z}_0 ) ( z-z_0 ) ) \equiv \mathrm{log} ( |z-z_0| ) $$ we can add several stream functions centered at various points on the plane with different vorticities to obtain a general solution representing fluxes at these points : $$\psi = \sum_k \gamma_k \mathrm{log} ( |z-z_k| ) $$ the constant $\gamma_k $ expresses the fluxes around the $k$-th center ( or the vorticity in the hydrodynamical terminology ) . one can easily verify that the single centered vector potential ( and also the corresponding stream function ) are invariants under the metric preserving automorphisms of the plane consisting of translations and rotations : ( which can be compactly written in the complex notations as : ) $$z \rightarrow e^{i\alpha} z + v$$ $$z_0 \rightarrow e^{i\alpha} z_0 + v$$ from the expression of the single centered stream function , one observes that the denominator is the geodesic distance on the plane , thus a candidate generalization to the sphere ( $s^2$ ) would be the replacement of this by geodesic distance on the sphere : $$|z-z_0|^2 \rightarrow \frac{|z-z_0|^2}{ ( 1+\bar{z}z ) ( 1+\bar{z}_0z_0 ) }$$ where $z$ is the stereographic projection coordinate on the sphere : $$ z = \mathrm{tan}{\frac{\theta}{2}}e^{i \phi}$$ ( $\theta$ and $\phi$ are the spherical surface coordinates ) . thus the candidate solution on the sphere is : $$\psi= \mathrm{log} ( |z-z_0| - \frac{1}{2} \mathrm{log} ( 1+\bar{z}z ) - \frac{1}{2} \mathrm{log} ( 1+\bar{z}_0z_0 ) ) $$ this solution is invariant under the metric preserving automorphisms of the sphere : $$ z\rightarrow \frac{\alpha z + \beta}{-\bar{\beta} z +\bar{\alpha} }$$ , with $|\alpha|^2+|\beta|^2 = 1$ the matrix : $$\begin{pmatrix} \alpha and \beta\\ -\bar{\beta} and \bar{\alpha} and \end{pmatrix} \in su ( 2 ) $$ which is the automorphism group of the round metric thus the candidate vector potential corresponding to this solution is obtained by applying the gradient operator in the sphere 's curvlinear coordinates : $$ a_z = \frac{1}{i} ( 1+\bar{z}z ) ^2 \frac{\partial \psi}{\partial \bar{z}}$$ $$ a_{\bar{z}}= -\frac{1}{i} ( 1+\bar{z}z ) ^2 \frac{\partial \psi}{\partial z} $$ explicitly $$a = \frac{1}{2i} ( 1+\bar{z}z ) ( 1+\bar{z}_0z_0 ) \frac{ ( z-z_0 ) ( 1+\bar{z}_0 z ) d\bar{z}- ( \bar{z}- \bar{z_0} ) ( 1+\bar{z}_0 z ) dz}{ ( \bar{z}- \bar{z_0} ) ( z-z_0 ) }$$ the laplacian on the sphere is given by : $$\nabla^2 = ( 1+\bar{z}z ) ^2 \frac{\partial}{\partial z}\frac{\partial}{\partial\bar{z}}$$ applying the laplacian operator to the candidate stream function , we obtain : $$\nabla^2 \psi= ( 1+\bar{z}z ) ^2 \delta^{ ( 2 ) }_l ( z-z_0 ) +1 = \delta^{ ( 2 ) }_s ( z-z_0 ) +1$$ where $ \delta^{ ( 2 ) }_s$ is the dirac delta function corresponding to the spherical measure : $$\int_{\mathbb{s^2}} f ( z ) \delta^{ ( 2 ) }_s ( z-z_0 ) \frac{ \mathrm{dre} ( z ) \mathrm{dim} ( z ) }{ ( 1+\bar{z}z ) ^2}= f ( z_0 ) $$ the additional constant term in the laplacian constitutes a problem because it means that this stream function is not harmonic outside the singularities . the solution to this problem on the sphere is to add up several solutions with a vanishing total flux ( vorticity ) . $$\sum_k \gamma_k = 0$$ in this case the constant contributions from all centers will cancel .
it is spinning forever . as you see , change of angular momentum $$\frac{\text{d}\vec{l}}{\text{d}t} = \vec{\tau}$$ is always perpendicular to angular momentum itself , which means that angular momentum 's direction is changed , while its magnitude is constant . note the mathematical analogy with velocity and acceleration in case of circular rotation with constant velocity : $$\frac{\text{d}\vec{v}}{\text{d}t} = \vec{a}_\text{cp}$$
the actual reason why one can not interpret the equation $$ \nabla_\mu t^{\mu\nu}=0 $$ as a global conservation law is that it uses covariant derivatives . if a law like that were valid with partial derivatives , you could derive such a law . but there is a covariant derivative which is one of the technical ways to explain that general relativity in generic backgrounds does not preserve any energy : http://motls.blogspot.com/2010/08/why-and-how-energy-is-not-conserved-in.html the text above also explains other reasons why the conservation law disappears in cosmology . however , despite the non-existence of a global ( nonzero ) conserved energy in general backgrounds , the tensor $t_{\mu\nu}$ is still well-defined . as twistor correctly writes , it quantifies the contribution to the energy and momentum from all matter fields ( non-gravitational ones ) and matter particles . and if you can approximate the background spacetime by a flat one , $g_{\mu\nu}=\eta_{\mu\nu}$ , which is usually the case with a huge precision ( in weak enough gravitational fields , locally , or if you replace local objects that heavily curve the spacetime , including black holes , by some effective $t$ , using a very-long-distance effective description ) , then $\nabla$ may be replaced by $\partial$ in the flat minkowski coordinates and the situation is reduced to that of special relativity and the " integral conservation law " may be restored .
water molecules don’t carry an electric charge ( and if they do , you don’t want them on your hands… ) . the dipole moment of water molecules can only be used to rotate them in space , not to move them . additionally , the forces that apply to water molecules on your hands also apply to water molecules in your hands . so even if you somehow managed to apply a sensible force on these water molecules , this would get rather uncomfortable . the same problem arises if you attempt to heat them up by means of electric resonance ( similar to a microwave ) . i therefore doubt that it would be possible to build a device based on electric fields rather than moving air , that removes water molecules from the surface of your skin . however , it might be possible to vaporise the water on your hands using strong infrared lamps . this might lead to other problems , though , such as the focusing of infrared radiation on small areas of the skin by water drops .
you expect that the paper will bend downwards due to the decreased pressure applied to the air gap causing an increase in its volume , but your observation is that instead the paper is bent upwards . i think that this is probably caused by water leakage . try the experiment again with a plastic seal over the glass , i expect you to see the film bend down . as an estimate of how much down , air pressure is around 30 feet of water . your glass is holding a few inches . so i would think this fraction ( 2/12 ) /30 = 1/180 would be about the amount of movement , on average . for 2 inches of water this would be about 1/90 of an inch , which is detectable ( look to see how reflections in the film are distorted ) .
2012-07-11 addendum based on excellent inputs , in particular from @annav , my answer is now " no , even a direct worst-case hit by the oh-my-god particle would not kill you , even by radiation , because there is insufficient distance and angle to generate a fatal radiation cone . thanks all , and be sure to look at the earlier answer that @dmckee pointed out . ** original answer** ( my answer seems to differ from the earlier ones that @dmckee aptly pointed out , so i will go ahead and risk posting it . my main difference is that i suspect that a head on collision with a large nucleus could produce a wide enough horizontal-splatter radiation cone to produce a fatal event . ) since the 1991 oh-my-god particle was most likely a proton and had the kinetic energy of a fast baseball , i am going out on a limb and saying yes , you could be killed by a single particle . this harvard physics site suggests an approximate energy transfer of about 0.2% in transfers with heavy nuclei , which as i discuss below may be enough to do you in with that kind of particle . but it would be the ensuing radiation event and cone that would do you in , not the kinetic energy of the particle . the main issue is that your head does not have anything in it remotely solid enough to stop or even slow down a particle with that much momentum . so , like a locomotive passing through a cloud of fog , it is going to zip through pretty much as if your head is not there . the question , then , is to ask not what the fog will do to the locomotive , but what the locomotive will do to the fog -- that fog being your head . even a single solid , exactly head-on collision with a nice fat iron nucleus just as an ultra cosmic ray proton enters your head would probably not be a pretty event in terms of the resulting secondary radiation shower . i am guessing ( nothing more , i have not tried to calculate anything ) that outward splattering of a nice little quark plasma , one created as the iron nucleus vaporizes during the transition event , could produce a sufficiently wide cone of particle-zoo ejecta to irradiate a fatal percentage of your brain . rapid heating of your brain would not be a problem , however , since 1/500 of the approximately 50 joules of kinetic energy would work out to be only about 0.1 j of heat energy tops . by comparison a standard firecracker releases about 500 j of energy . and what are the real odds on such a dead-center strike on a large nucleus near the surface of your brain , assuming you were an astronaut unprotected by out atmosphere ? low almost beyond belief . look at the date on the oh-my-god particle : 1991 . we have not seen one quite that feisty since . as @dmckee aptly notes , ordinary cosmic rays or their secondary outputs hit us all the time , and astronauts watch direct collision buzz through their retinas without much harm .
the analogy is wrong . a voltage source can only shock us if it is able to pass a considerable amount of current through our body ( ~ 250 ma or so , i dont know the exact value but you can google it ) . the circuit that you are trying to discuss , does indeed have 36 amps of current flowing through it , but once you connect yourself to the circuit , you are in fact adding an additional resistance to the circuit ( resistance of our body is usually within 100 - 150 k-ohms ) . this additional resistance will dramatically reduce the value of current in the circuit . all the potential difference will now drop across the high resistance , and very small amount would be leftover to the parallel resistors . with some rough values , there maybe approximately about 0.008 ma which is definitely not enough to be felt by a human being .
the number 10^123 emerges as ( roughly ) the number of planck areas contained within the boundary of the observable universe . if each planck area can be ( roughly ) in two states , a total of 10^123 yes/no questions suffice to describe the boundary of the universe and - via the ( still speculative ) holographic principle - the whole universe . in other words , if the universe is a hologram , about 10^123 bits of information are needed to describe it .
instead of using catapults , magnetic propulsion systems ( especially superconducting ) would be more useful for a non-rocket space-launch . particularly , the energy required for an average guy ( 70 kg ) to get outta this world would be $43.904 × 10^8 j$ your catapult is somewhat comparable to a space gun . but , it cannot place the payload in a stable orbit , since gravity certainly would not allow that . . . mass drivers which use superconducting coils would be a more efficient ( more than 90% ) of attaining such a large amount of kinetic energy ( escape velocity ) with a single lift . but , they are all in the future proposals list . . . they are so much powerful and probably do not have a weight limit . yes , we could launch satellites directly into their geostationary orbits without the use of rockets . even though it would be a lot of success , some million new bills should be invested to obtain it . . . also , i will add that space elevators would be more useful when comparing budget . all those wikipedia links are quite good in this subject . . . edit : okay . . . a trebuchet consists of two arms - projectile arm and main weight arm . the ratio of the length of the main weight arm to the length of the projectile arm is typically between 1/2 and 1/5 . the main weight arm has the counter-weight required to shoot the projectile and it should always be in multiples of the projectile weight ( the height of main weight arms also matters here ) . the angle we use commonly for a projectile is 45° . here to attain the escape velocity ( at least close to ) , you must use at least a million ton counter weight , a 10 lbs projectile , and a trebuchet more than 2 mile long , and use an angle of 90° would be helpful . . ! theoretically possible i would say . . . refer these trebuchet range and projectile range papers . . . edit : your first two questions were reasonable . but now , it is quite impossible to attain . as i have already told , gravity would not allow your projectile to be placed in a geo-stationary orbit . even though there is no air resistance ( friction in atmosphere ) and you have broken the escape velocity , you had escape into outer space and never return . . ! hence in the absence of a rocket propulsion system , you require at least some kinda magnetic systems ( not just wood and clockworks . . ! ) @ehryk : if those papers were not useful , here is an information from a simulation which gives an idea . there are some software such as atreb , wintreb or trebuchet simulator . . . you could refer the list of simulators provided . . .
initially , you have 6 equations given by ( 16 ) and ( 17 ) . now you insert both the expressions for the components of the metric and the power series expansion and determine its coefficients in such a way that the equations are satisfied order by order .
let 's distill your question down to its conceptual essence : given an lc circuit , the capacitance $c$ , the maximum voltage $v_m$ and the maximum current $i_m$ , can you find the inductance $l$ . that is really all there is to it since , once you find $l$ , you know the frequency and the period . ( by the way , in an lc circuit there is only one independent voltage and one independent current ) . the answer is yes , you can find $l$ . since you know the maximum voltage , you know the maximum energy stored by the capacitor and thus , you know the maximum energy stored by the inductor ( there is no energy lost in an lc circuit ) . thus , you have the relation : $\dfrac{cv^2_m}{2} = \dfrac{l i^2_m}{2}$ you can solve for l with this since you are given the other values and the rest follows .
the electron field transforms under the $\mathbf 1$ of $u ( 1 ) $ , i.e. , the generator is $i$ or $1$ depending on your convention/notation . the gauge fields transform in the adjoint representation , but they transform as a connection , as @adam mentioned . in other words , if $\psi \to g \psi$ , then $d_\mu \psi \to g d_\mu \psi$ implies that $a_\mu \to g d_\mu g^{-1}$ . it is a bit misleading for $u ( 1 ) $ because you do not see the non-abelian structure , but you get the idea .
i ) the closest cosmetic resemblance between the nambu-goto action and the polyakov action is achieved if we write them as $$\tag{1} s_{ng}~=~ -\frac{t_0}{c} \int d^2{\rm vol} ~\det ( m ) ^{\frac{1}{2}} , $$ and $$\tag{2} s_{p}~=~ -\frac{t_0}{c}\int d^2{\rm vol}~ \frac{{\rm tr} ( m ) }{2} , $$ respectively . here $h_{ab}$ is an auxiliary world-sheet ( ws ) metric of lorentzian signature $ ( - , + ) $ , i.e. minus in the temporal ws direction ; $$\tag{3} d^2{\rm vol}~:=~\sqrt{-h}~d\tau \wedge d\sigma$$ is a diffeomorphism-invariant ws volume-form ( an area actually ) ; $$\tag{4} m^{a}{}_{c}~:=~ ( h^{-1} ) ^{ab}\gamma_{bc} $$ is a mixed tensor ; and $$\tag{5} \gamma_{ab}~:=~ ( x^{\ast}g ) _{ab}~:=~\partial_a x^{\mu} ~\partial_b x^{\nu}~ g_{\mu\nu} ( x ) $$ is the induced ws metric via pull-back of the target space ( ts ) metric $g_{\mu\nu}$ with lorentzian signature $ ( - , + , \ldots , + ) $ . note that the nambu-goto action ( 1 ) does actually not depend on the auxiliary ws metric $h_{ab}$ at all , while the polyakov action ( 2 ) does . ii ) as is well-known , varying the polyakov action ( 2 ) wrt . the ws metric $h_{ab}$ leads to that the $2\times 2$ matrix $$\tag{6} m^{a}{}_{b}~\approx~\frac{{\rm tr} ( m ) }{2} \delta^a_b~\propto~\delta^a_b $$ must be proportional to the $2\times 2$ unit matrix on-shell . this implies that $$\tag{7} \det ( m ) ^{\frac{1}{2}} ~\approx~ \frac{{\rm tr} ( m ) }{2} , $$ so that the two actions ( 1 ) and ( 2 ) coincide on-shell , see e.g. the wikipedia page . ( here the $\approx$ symbol means equality modulo eom . ) iii ) now , let us imagine that we only know the nambu-goto action ( 1 ) and not the polyakov action ( 2 ) . the the only diffeomorphism-invariant combinations of the matrix $m^{a}{}_{b}$ are the determinant $\det ( m ) $ , the trace ${\rm tr} ( m ) $ , and functions thereof . if furthermore the ts metric $g_{\mu\nu}$ is dimensionful , and we demand that the action is linear in that dimension , this leads us to consider action terms of the form $$\tag{8} s~=~ -\frac{t_0}{c}\int d^2{\rm vol}~ \det ( m ) ^{\frac{p}{2}} \left ( \frac{{\rm tr} ( m ) }{2}\right ) ^{1-p} , $$ where $p\in \mathbb{r}$ is a real power . alternatively , weyl invariance leads us to consider the action ( 8 ) . obviously , the polyakov action ( 2 ) ( corresponding to $p=0$ ) is not far away if we would like simple integer powers in our action .
lehner and pretorius have recently given some persuasive numerical evidence that there are generic violations of cosmic censorship which arise in the time evolution of black strings in 5d gravity , aka the gregory-laflamme instability http://arxiv.org/pdf/1006.5960 . the failure of cosmic censorship certainly does not imply the downfall of causality . classically it means we can see a singularity without it being hidden behind a horizon . this seems to me like a very good thing . we all believe that there is some uv completion of gravity , although different camps have different views on what this is . this theory will cut off the singularity through some combination of classical modifications to einstein gravity and quantum effects . being able to see such a direct effect of the uv completion would be a wonderful thing , so i think we should all hope that there are also generic violations of cosmic censorship in four dimensions , although as far as i know the jury is still out on this .
first order of business is to find where the heck on earth you are . first , $\omega = 360 \sin ( \phi ) /day$ , where $\omega$ is 216.528 degrees ; $\phi$ is the latitude of your position . north of the equator is positive , south negative . this gives you a band to follow around the earth horizontally , positions where could possibly be . you can further narrow your position down because a foucault pendulum can be used to find the acceleration of gravity at its position . once you figure this out , you can go to nasa websites and check out when this location has its next or last total solar eclipse .
you have not really asked a precise question , or given an example , but i think i know what you are getting at . you have misunderstood what it means for the two states to ' have different symmetry ' . suppose , as you say , that $g$ is some operator representing a symmetry of the system . this means that $g$ is unitary , and $ [ g , h_0 ] = [ g , v ] = 0$ ( we could also consider $ [ g , v ] \neq 0$ , but i do not think this is what you need ) . since $g$ is unitary and commutes with $h_0$ , the ground state of $h_0$ will also be an eigenstate of $g$ ( here we assume non-degeneracy of the ground state ) : $g|\phi_0\rangle = \lambda|\phi_0\rangle$ for some complex number $\lambda$ . the same argument applies for $h$ , so $g|\psi_0\rangle = \lambda'|\psi_0\rangle$ for some ( possibly different ) complex number $\lambda'$ . now consider $\langle\psi_0|g|\phi_0\rangle$ . we can let $g$ act either ' forwards ' on $|\phi_0\rangle$ , or ' backwards ' on $\langle\psi_0|$ ( exercise : show that $\langle\psi_0|g = \lambda'\langle\psi_0|$ as a consequence of unitarity of $g$ ) , to get $$ \lambda\langle\psi_0|\phi_0\rangle = \lambda'\langle\psi_0|\phi_0\rangle ~ , $$ and therefore $$ ( \lambda-\lambda' ) \langle\psi_0|\phi_0\rangle = 0 ~ . $$ so if $\lambda ' \neq \lambda$ , we find $\langle\psi_0|\phi_0\rangle = 0$ . example : a good example would be a particle moving in one dimension , with $h_0 = \frac{p^2}{2m} + \lambda x^4$ , and $v = -\mu^2 x^2$ , where $\lambda$ and $\mu$ are real constants . there is a symmetry $g : x\to -x$ ; the ground state of $h_0$ is even under this symmetry , whereas the ground state of $h = h_0 + v$ is odd . in symbols , $g|\phi_0\rangle = |\phi_0\rangle~ , ~~ g|\psi_0\rangle = -|\psi_0\rangle$ .
you know the basic spring equation , right ? $f = xk$ , where $k$ is the spring constant , in units of force per distance . you also know work ( energy ) is force times distance , right ? so all you have got to do is integrate $xk dx$ from d1 to d2 . ( hint , you can pull $k$ out of the integral . ) you could do it on graph paper if you happened to know d1 and d2 . added : ok , here 's the graph paper approach : a graph of force $f$ versus displacement $x$ looks like this , right ?  | / | / | / F | / | / | / | / |/_______ x  the slope of the graph is $k$ . the area under the graph is work $w$ , because it is just the sum of a bunch of vertical slivers with area $f$ times the width $dx$ of each sliver . so here 's how you get the answer to your question : that help ?
they are variants , different kinds of quantum field theory , but they are not mutually exclusive . the different adjectives you mention separate quantum field theory to " pieces " in different ways . the different sorts of variants you mention are being used and studied by different people , the classification has different purposes , the degree of usefulness and validity is different for the different adjectives , and so on . conformal quantum field theory is a special subset of quantum field theories that differ by dynamics ( the equations that govern the evolution in time ) , namely by the laws ' respect for the conformal symmetry ( essentially scaling : only the angles and/or length ratios , and not the absolute length of things , can be directly measured ) . conformal field theories have local degrees of freedom and the forces are always long-range forces , which never decrease at infinity faster than a power law . they are omnipresent in both classification of quantum field theories - almost every quantum field theory becomes scale-invariant at long distances - and in the structure of string theory - conformal field theories control the behavior of the world sheets of strings ( here , the cft is meant to contain two-dimensional gravity but the latter carries no local degrees of freedom so it does not locally affect the dynamics ) as well as boundary physics in the holographic ads/cft correspondence ( here , cfts on a boundary of an anti de sitter spacetime are physically equivalent to a gravitational qft/string theory defined in the bulk of the anti de sitter space ) . conformal field theories are the most important class among those you mentioned for the practicing physicists who ultimately want to talk about the empirical data but these theories are still very special ; generic field theories they study ( e . g . the standard model ) are not conformal . topological quantum field theory is one that contains no excitations that may propagate " in the bulk " of the spacetime so it is not appropriate to describe any waves we know in the real world . the characteristic quantity describing a spacetime configuration - the action - remains unchanged under any continuous changes of the fields and shapes . so only the qualitative , topological differences between the configurations matter . topological quantum field theory ( like chern-simons theory ) is studied by the very mathematically oriented people and it is useful to classify knots in knot theory and other " combinatorial " things . they are the main reason behind edward witten 's fields medal etc . axiomatic or algebraic ( and mostly also " constructive " ) quantum field theory is not a subset of different " dynamical equations " . instead , it is another approach to define any quantum field theory via axioms etc . that is why it is a passion of mathematicians or extremely mathematically formally oriented physicists and one must add that according to almost all practicing particle physicists , they are obsolete and failed ( related ) approaches which really can not describe those quantum field theories that have become important . in particular , aqfts of both types start with naive assumptions about the short-distance behavior of theories and are not really compatible with renormalization and all the lessons physics has taught us about these things . constructive qfts are mainly tools to understand the relativistic invariance of a quantum field theory by a specific method . then there are many special quantum field theories , like the extremely important class of gauge theories etc . they have some dynamics including gauge fields : that is a classification according to the content . qfts are often classified according to various symmetries ( or their absence ) which also constrain their dynamical laws : supersymmetric qfts , gravitational qfts based on general relativity , theories of supergravity which are qfts that combine general relativity and supersymmetry , chiral qfts which are left-right-asymmetric , relativistic qfts ( almost all qfts that are being talked about in particle physics ) , lattice gauge theory ( gauge theory where the spacetime is replaced by a discrete grid ) , and many others . gauge theories may also be divided according to the fate of the gauge field to confining gauge theories , spontaneously broken qfts , unbroken phases , and others . string field theory is a qft with infinitely many fields which is designed to be physically equivalent to perturbative string theory in the same spacetime but it only works smoothly for open strings and only in the research of tachyon condensation , it has led to results that were not quite obtained by other general methods of string theory . we also talk about effective quantum field theories which is an approach to interpret many ( almost all ) quantum field theories as an approximate theory to describe all phenomena at some distance scale ( and all longer ones ) ; one remains agnostic about the laws governing the short-distance physics . that is a different classification , one according to the interpretation . effective field theories do not have to be predictive or consistent up to arbitrarily high energies ; they may have a " cutoff energy " above which they break down . it does not make much sense to spend too much time by learning dictionary definitions ; one must actually learn some quantum field theory and then the relevance or irrelevance and meaning and mutual relationships between the " variants " become more clear . at any rate , it is not true that the classification into adjectives is as trivial as the list of colors , red , green , blue . the different adjectives look at the framework of quantum field theory from very different directions - symmetries that particular quantum field theories ( defined with particular equations ) respect ; number of local excitations ; ability to extend the theory to arbitrary length scales ; ways to define ( all of ) them using a rigorous mathematical framework , and others .
this paper considers and relates uncertainty relations , and entropic relations in an information-theoretical sense ( amongst other things ) . maybe it is possible to extend that to entropic relations in a physical sense .
at david zaslavsky 's suggestion i will transfer this from the comments to the answers ( i was a bit hesitant because i do not know how reliable youtube videos are to still be around in , say , 6 months time ! ) : this little youtube video might help . you can only resolve the objects by looking at the reflected waves . the amount of detail you can get in the reflected waves can not be smaller than the wavelength ( roughly speaking ) . edit : the video shows incident waves being reflected off small irregularities in the surface at the bottom of the picture . the first case ( wavelength smaller than the irregularites ) shows information about the irregularities being " fed back " in the reflected waves : the last case ( wavelength larger than the irregularities ) shows much coarser information being fed back , making it not possible to get any information about , for example , the size of these irregularities : of course snapshots are a bit hard to read , you had really have to look at the statistics of the received reflected waves as a function of position to really see what was going on , but the video gives a general impression of the problem .
you are always subjected to a low background of ionizing radiation from a number of natural and artificial sources , which include cosmic rays , trace amounts of radioactive nuclei in the air and in food , and indeed from the ground . a good place to read up on this is the corresponding wikipedia article . the radiation from the core , however , has no chance of making it to the surface . gamma radiation is typically stopped by a few to maybe 20 or 30 cm of rock or soil . ( there is also the danger that material used to shield against gamma radiation becomes radioactive itself , but of course this is hardly an issue with the earth 's core . ) alpha and beta rays are even easier to stop . trace amounts of radioactive nuclei , though , can be present in the soil and buildings around you and will then expose you to a small amount of radiation . it is important to note that this is natural and nothing to worry about . the most significant contribution to background radiation , away from zones like hiroshima , chernobyl or fukushima , is the trace amount of radon in the air you breathe .
the direction of the thrown object is not important since there is not air resistance . whatever upward vertical component the object had will be downward when it passes you . also , the horizontal component does not change . so it does not matter if you throw the object straight up , or at 37 degrees , or at 0 degrees , the object will always have the same energy when it is at the same level . so the potential energy plus the kinetic energy at the start equals the kinetic energy at the end . $\frac {1}{2}m{v_i}^2 + mgy = \frac {1}{2}m{v_f}^2$
the problem is that heat flow ( in or out of an object ) is related to the temperature difference between the object and it is environment . for the sort of cooling usually found in the kitchen ( convective cooling ) the heat flow , and therefore the rate of temperature change is proportional to the temperature difference . so let 's take some example like a bottle of milk . if you want to heat it quickly that is pretty easy because it is easy to generate a large temperature difference on the hot side . just burn some gas . however to cool the milk quickly we need to generate a large temperature difference on the cool side , and that is hard . you mention liquid nitrogen , and indeed that is a good way to cool things quickly . however you are forgetting all the hours the liquid nitrogen supplier had to put in to cool nitrogen enough to make it liquify . in general it is hard to cool things quickly unless you cheat and start with something ( like liquid nitrogen ) that is already been cooled . response to comment : this started as a comment , but it got a bit involved so i thought i would put it in here . the temperature of anything ( e . g . milk ) depends on how much heat it contains . let 's not get into exactly what heat is , but basically if you add heat it increases the temperature and if you remove heat it reduces the temperature . the problem is that the milk is surrounded by the rest of the world , and this is around room temperature . heat will not flow from a cold place to a hot place , so heat will not flow out of the milk into it is surroundings unless we do some work . typically what we do is use energy to pump heat around . the area we have pumped the heat from becomes cooler and the area we have pumped it to becomes hotter . this is what you do to liquify nitrogen . you have to pump the heat out of it so te nitrogen gets cold and liquifies while the rest of the world gets hotter . once we have the liquid nitrogen we can use it to cool the milk , but it took a lot of work to make the liquid nitrogen . if you are interested in pursuing this , the mechanism for pumping heat around is called ( unsurprisingly :- ) a heat pump . it is basically a heat engine that runs backwards . heating things is easy because there are lots of systems that have stored energy that can be easily converted to heat . for example a gas/air mixturer has chemical energy that is converted to heat by burning it . you mentioned a microwave : this uses electricity that came from chemical energy i.e. from a power station burning gas or coal , so the heat froma microwave originally came from chemical energy . you might wonder why we can not easily convert heat to chemical energy e.g. mix carbon dioxide and water and have it convert to gas and oxygen and cool down in the process . if we could do this it would be an easy way to cool things . the reason why we can not do this is the second law of thermodynamics . explaining this would be a long essay in it is own right , but in brief it is highly probable that a gas/air mixture will convert to carbon dioxide and water ( i.e. . burn ) but it is very improbable that a carbon dioxide/water mixture would convert back to gas and air .
if you mix d$_2$o and h$_2$o you quickly get dho due to the grotthuss mechanism . i assume this is what you mean by heavy water being contaminated on exposure to humid air . obviously it would not be contaminated by exposure to dry air because there is no hydrogen present in dry air . the hydrogen atoms in polyethylene are not mobile and will not react with d$_2$o . however polyethylene is more porous than you might think and will eventually let water through .
transmission gratings are less sensitive to polarization and alignment , but cannot transmit at higher wavelengths ( ie typically ~2000nm ) .
protons are positively charged , and neutrons are neutral , so large nuclei are highly positively charged . a postively charged sphere will energetically prefer to break up into two separate charged droplets which move far apart , this reduces the electrostatic energy , since the electrostatic field does work during this process . this thing , spontaneous fission , is usually phase-space unlikely , since you need to have a large chunk of the nucleus tunnel away from another large chunk , and it is unlikely for all those particles to tunnel out together . but at large atomic numbers , you are unstable even just to shooting out an alpha-particle , and this does not require a conspiracy , so large z nuclei are alpha unstable , usually with long half-lives . the positive charge on nuclei puts a limit to the stable ones . the reason is simply that the electrostatic force is long range , while the cohesion force is short range . the same phenomenon causes the instability of water droplets , so that if you charge one up , it will break into a fine mist . the cohesion of the droplets is local , while the electrostatic repulsion is long range . the scale at which you get a fission instability directly can be estimated from surface-tension considerations . if you break a sphere into two adjacent spheres of same total volume , the radius is reduced by the cube-root of two , so that the surface area is decreases by the square of this , and you multiply by 2 ( since there are two spheres ) so the net factor is the cube-root of 2 , which is around 1.3 . so the extra surface tension energy is increased by a factor of 1.3 , or 30% . but in separating the two spheres , you have taken one ball of charge , with an energy of $q^2\over r$ and separated it into two adjacent balls of reduced radius and half the charge . adding up the electrostatic energy , it is about 80% of the original electrostatic energy in the single sphere . so spontaneous droplet fission will happen when you have a charged ball for which 30% of the surface tension energy is less than 20% of the charge energy . since charge goes up almost as the volume ( not quite , but close ) while the surface tension goes up as the area , there is a crossover , and charged droplets will spontaneously separate when they are too big . the surface tension can be found from the binding energy curve of nuclei , and these simple considerations limit stable nuclear size to about that of uranium . the u nucleus can spontaneously fission at an extremely low rate , but the transuranics become progressively more unstable because their electrostatic energy is increasing as the volume to a power greater than 2/3 , while their surface tension energy is increasing as the surface area , which grows as the 2/3 power of the volume . these considerations , in much more sophisticated form , are due to niels bohr in the seminal liquid drop model of the 1940s . this model explained the nuclear binding energy curve quantitatively , and accounted well for fission phenomena . the only major thing left out of this was the shell model and magic numbers , which was supplied by mayer .
as we are discussing above , it seems that the detector you are using seems to be functioning properly when used with verifiable radioactive material . the extra clicks are being caused by current induced in the counter by magnetic flux when you may be moving the detector . in order to get around this , it seems that you should use , in addition to the crinkled foil suggested by dmckee , a setup that involves keeping the business end of the detector and the sample under test stationary , thereby removing the current being induced by the changing magnetic field . many thanks to dmckee for the insights above .
gravitational waves have never been directly detected . gravitational waves are predicted by general relativity and have been inferred from other observations . strong evidence of gravitational waves is the change in period of the hulse-talyor binary star system . energy is being lost from the system at a rate consistent with radiation of gravitational waves . the march 17th 2014 report of evidence of primordial gravitational waves is not a direct observation of the waves . instead photons were being observed and gravitation waves inferred from the photons . now that the research group ( bicep2 ) has published their work in a peer reviewed journal , they state that dust may in fact be responsible for the effect attributed to gravitational waves . quoting their published paper " models are not sufficiently constrained . . . to exclude the possibility of dust emission bright enough to explain the entire excess signal " . so it is not yet confirmed that primordial gravitation waves have been detected . •why gw requires such energy levels ? there is no threshold energy for producing gravitation waves . for example a rotating rod , rotating perpendicualr to the axis of the rod , would produce gravitation waves . however gravitation is relatively weak force and it is very difficult , so far impossible , to detect gravitational waves . •why attractive gravity does not create gw ? at the risk of oversimplifying , time varying gravitational fields produce gravitational waves . or more technically , the nth time derivative of the n-multipole moment ( such as the 3rd derivative of the quadrople moment ) of the stress-energy tensor must be non-zero for gravitation waves to be produced . attractive gravity ( such a two objects attracting each other and orbiting their common center of mass ) can cause graviational waves if there is time dependence such as orbitting .
the question defines $v$ as the rate at which the string is pulled downward through the ring and consequently the rate at the which the radius changes , $-dr/dt$ ( negative as the radius is decreasing with time - the fixed length of string is being pulled down ) . this $v$ is different from $\underline{\mathbf{v}}$ , the velocity vector of the mass , given by $\underline{\mathbf{v}}=\frac{d\underline{\mathbf{r}}}{dt}$ where $\underline{\mathbf{r}}$ is the position vector ( independent of the coordinate system you choose ) . this is the quantity you rightly define in your second statement , $r ( t ) =\frac{|\underline{\mathbf{v}}|}{\omega ( t ) }$ . overall , you need to be careful between what are scalar quantities ( $v$ , $r$ and $dr/dt$ ) and what are vector quantities ( $\underline{\mathbf{v}}$ and $\underline{\mathbf{r}}$ ) . sometimes the vectors will only be bold which can make it difficult to distinguish but the context is key , it is rare to find an error in a textbook as much as we may sometimes wish it so . the devil is in the detail !
copying from the electronics . se since in properly constructed power network the neutral wire is maintained at a potential level close to ground potential , there is nearly no voltage between the neutral and the ground . hence , touching neutral will not cause current to flow through human body into ground . so by construction there is very little potential between the ground and the neutral . when a human touches the live wire he closes the circuit with the ground instead of the neutral because there is by construction so little difference between neutral and live .
quantum mechanics ( qm – also known as quantum physics , or quantum theory ) is a branch of physics which deals with physical phenomena at microscopic scales , where the action is on the order of the planck constant . quantum mechanics departs from classical mechanics primarily at the quantum realm of atomic and subatomic length scales . quantum mechanics provides a mathematical description of much of the dual particle-like and wave-like behavior and interactions of energy and matter . quantum mechanics is the non-relativistic limit of quantum field theory ( qft ) , a theory that was developed later that combined quantum mechanics with relativity . quantum field theory ( qft ) is a theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics , by treating a particle as an excited state of an underlying physical field . some of the relativistic quantum field theories would be qed , qcd , and the standard model . references : http://en.wikipedia.org/wiki/quantum_mechanics http://en.wikipedia.org/wiki/quantum_field_theory
i think this is a proper question to be answered with wiki . resources for job seeker general : jobs for phd academic jobs physics : spires american physical society ( aps ) astrophysics : american astronomical society ( aas ) european astronomical society ( eas ) european space agency ( esa ) european southern observatory ( eso ) gravitational waves : gravitational wave detection jobs wiki
typically you would attempt to measure rather than calculate the effect . perhaps by having a second , calibrated device with a long probe that provides an independent measurement of the temperature . you do this in situ if possible or in some reasonable test stand ( which might be as simple as disposable cooler filled with you working fluid ) . the only real alternative is to read the data sheet ; either for the whole device ( if is a off-the-shelf instrument ) , or for the particular chip ( if it is something that you manufactured to spec ) . as a desperate fall-back position you might be able to find a rule-of-thumb for devices in the same class , but those are unlikely to be centrally tabulated . ask around is my best suggestion .
the thin lens formula carries over with a few approximations directly to wave optics . this formula , together with an appropriate analysis of diffraction , will let you get a first approximation to the behaviour of many systems . leftaroundabout 's answer gives the overall picture . a good way to translate this into a wave picture is with the following assumptions : the paraxial approximation : i.e. all fields are a superposition of plane waves that are propagating at small angles to the optical axis ; gaussian scalar fields . these are fields that , on a transverse plane , vary like $\exp\left ( - ( ( x-x_0 ) ^2+ ( y-y_0 ) ^2 ) \left ( \frac{1}{2\ , \sigma^2} + \frac{i\ , k , \kappa}{2}\right ) \right ) $ where $\sigma$ is the spotsize and $\kappa$ the wavefront curvature . the " algorithms " are as follows . you begin with the helmholtz equation in a homogeneous medium $ ( \nabla^2 + k^2 ) \psi = 0$ . if the field comprises only plane waves in the positive $z$ direction then we can represent the diffraction of any scalar field on any transverse ( of the form $z=c$ ) plane by : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}\qquad ( 1 ) $$ in words : take the fourier transform of the scalar field over a transverse plane to express it as a superposition of scalar plane waves $\psi_{k_x , k_y} ( x , y , 0 ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) $ with superposition weights $\psi ( k_x , k_y ) $ ; note that plane waves propagating in the $+z$ direction fulfilling the helmholtz equation vary as $\psi_{k_x , k_y} ( x , y , z ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) $ ; propagate each such plane wave from the $z=0$ plane to the general $z$ plane using the plane wave solution noted in step 2 ; inverse fourier transform the propagated waves to reassemble the field at the general $z$ plane . now we make the paraxial approximation to the propagation relationship in step 2 above , i.e. we assume that the plane waves are not skewed at too steep angles relative to the $z$ axis so that $k_x^2+k_y^2 \ll k^2$ . then our two propagation equations above become the fresnel propagation integral : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \frac{k_z^2+k_y^2}{2\ , k} z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}\qquad ( 2 ) $$ now witness that a beam $\psi ( x , y , 0 ) $ with gaussian dependence on $x$ and $y$ becomes , under the fresnel diffraction integral , $\psi ( x , y , z ) $ with gaussian dependence on $x$ and $y$ . the fourier transform of a gaussian is a gaussian , a fact which does not change when we multiply by the $\exp\left ( i \frac{k_z^2+k_y^2}{2\ , k} z\right ) $ kernel in the fresnel diffraction integral , and of course a gaussian is recovered by the inverse fourier transform . furthermore , thin lenses can be thought of simply as phase masks , i.e. a field whose transverse variation is $\psi ( x , y ) $ passing through them is transformed by : $$\psi ( x , y ) \to \exp\left ( -i\ , k\ , \frac{ ( x-x_0 ) ^2+ ( y-y_0 ) ^2}{2\ , f}\right ) \psi ( x , y ) \qquad ( 3 ) $$ so the gaussian form is preserved by all the diffraction and lensing operations . best of all , for gaussian beams the diffraction integrals split up into a product of separate $x$ and $y$ dependences and these separate dependences are also operated on independently by the phase mask ( 3 ) , so propagation analysis can be done as a product of two decoupled one-transverse dimensional diffraction problems . therefore the propagation through a system comprising homogeneous , diffractive mediums and thin lenses can be done wholly in closed form expressions ( use mathematica though ! ) and you end up with something slightly more general than the thin lens formula that becomes the thin lens formula when the axial distances involved become longer than the rayleigh diffraction length ( of the order of wavelengths ) . this method assumes perfectly unaberrated waves . however , witness that any lens whose surface sag ( height ) near its vertex can be described by an analytic function of distance $r$ from the optical axis ( axis of symmetry ) will impart a phase mask well described by the above in the paraxial limit , i.e. by limiting the numerical aperture ( maximum skew angle ) in any field such that the support of any field in the transverse plane $z=0$ is small enough . as long as the width of the support needed to validate the analysis above stays big compared with a wavelength , the analysis above will be valid in the paraxial limit . this means it works in the paraxial limit for all lenses of practical curvatures in the neighbourhood of the optical axis .
i believe you only need one rule : " the angle of reflection equals the angle of incidence . " draw a triangle on a piece of paper . ( for this situation , the third dimension of the prism is irrelevant ) . draw two parallel lines originating from roughly the direction the apex of the prism is pointing . it is not necessary to be exactly in that direction , as you will see . those lines hit the two sides of the prism and reflect , relative to the normal to the surface , according to the rule i quoted . since you know ( i hope ! ) about complementary angles and the various theorems from trigonometry about the interior and exterior angles of a triangle , you will soon see why the formula works . the second formulation is similar , although you have added the possibility of an error term due to inexact reading of the goniometer .
from the equilibrium between drag and weight : $$ \frac{1}{2} c_x \rho v^2 s = m g $$ we can write the terminal velocity as $$ v = \sqrt{\frac{2 m g}{c_x \rho s}} $$ where $m$ is the mass of the phone , $c_x$ its drag coefficient , $s$ its section , $g$ the acceleration of gravity , and $\rho$ the density of air . now , this is not really a good answer , as the big question is how to estimate $s$ ( depends on the phone orientation ) and $c_x$ . but at least you can compute an order of magnitude , as in most cases $c_x$ of of order 1 .
the crucial fact about these idealized circuits and electric potential differences that leads to the assertion you want to justify is wires are modeled as perfect conductors ( ohmic resistors with negligible resistance ) for which there is zero potential difference between any two points . ( this was edited from " perfect conductors are equipotentials . " ) if we assume that the wires are perfect conductors ( zero resistance ) , then ohm 's law immediately gives the result above . having this fact in hand , we first notice that we have the following mathematical identity ( which is basically the " loop rule" ) : $$ v_a + ( v_d-v_a ) + ( v_c - v_d ) + ( v_b-v_c ) + ( v_a - v_b ) = v_a $$ which we can rewrite as $$ \delta v_{da} + \delta v_{cd} + \delta v_{bc} + \delta v_{ab} = 0 $$ now using the fact above , we note that the potentials at any two points connected solely by a wire are the same , so that $$ \delta v_{da} = 0 , \qquad \delta v_{bc} = 0 $$ which gives $$ \delta v_{cd} + \delta v_{ab} = 0 $$ and therefore since $$ \delta v_{ab} =- \delta v_{ba} $$ we get the desired result $$ \delta v_{cd} = \delta v_{ba} $$
for a pole of order $n+1$ , $$\mathrm{res} [ f , w ] =\frac{1}{n ! }\lim_{z\to w}\frac{d^n}{dz^n} ( z-w ) ^{n+1}f ( z ) $$ so for a function of the type $$f ( z ) =\frac{g ( z ) }{ ( z-w ) ^{n+1}}$$ $$\mathrm{res} [ f , w ] =\frac{1}{n ! }g^{ ( n ) } ( w ) $$ where $g$ is regular . now the important part is to notice that $o$ depends on $w$ and it is not the result of evaluating $z$ at $w$ . also because $\epsilon z$ is regular , we expect that $\epsilon z t ( z ) o ( w ) $ is of the form $\epsilon zh ( z , w ) $ , where $h ( z , w ) $ has the singularities . with this it becomes clear that the term $\epsilon h o ( w ) $ cannot arise from a simple pole as otherwise we will not get the factor $\epsilon z$ . but for a pole of order 2 , $$g^{ ( 1 ) } ( w ) =\epsilon h o ( w ) \rightarrow g ( z ) =\epsilon z h o ( w ) $$ on the other hand , the term $\epsilon w\partial o ( w ) $ can arise from a single pole : $$g ( w ) =\epsilon w \partial o ( w ) \rightarrow g ( z ) =\epsilon z \partial o ( w ) $$ therefore $$\epsilon z t ( z ) o ( w ) = . . . +\frac{\epsilon z h o ( w ) }{ ( z-w ) ^2}+\frac{\epsilon z \partial o ( w ) }{ ( z-w ) }+ . . . $$ thus we get the result $$ t ( z ) o ( w ) = . . . +\frac{ h o ( w ) }{ ( z-w ) ^2}+\frac{ \partial o ( w ) }{ ( z-w ) }+ . . . $$
it does not look possible , because the stopping power starts growing for very high energies even for non-strongly interacting particles ( see fig . 27.1 in passage of particles through matter ) . to do a best case estimate , let 's consider two cubic spaceships colliding , each of them with $1\ , {\rm mm^3}$ of volume , a density of $1\ , {\rm g\cdot cm^{-3}}$ and a number density of $10^{21}\ , {\rm cm^{-3}}$ . in general the atoms of the colliding spaceship will appear as a shower of nuclei and electrons but , for our purposes of lower bounding the deposited energy , let 's consider the colliding spaceship as a shower of muons , one for each atom . then we will have the equivalent of $10^{18}$ muons ( $10^{21}\ {\rm cm^{-3}}\cdot 10^{-3}\ {\rm cm^3}$ ) colliding with a target with an areal density of $0.1\ {\rm g\cdot cm^{-2}}$ . using the minimum ionization energy we get an stopping power greater than $1\ {\rm mev\cdot cm^2\cdot g^{-1}}$ , making each muon deposit approximately $100\ {\rm kev}$ of energy . as the number of particles in both spaceships is equal , the expected temperature of both spaceships after the collision will be around the energy deposited by each particle , $100\ {\rm kev} \approx 10^9\ k$ .
the term sector in quantum field theory typically refers to a portion of the lagrangian . for example , in the standard model lagrangian , we encounter a term , $$\mathcal{l}_{sm}= ( d_\mu \phi ) ^{\dagger} ( d^\mu\phi ) -\frac{m^2_h}{2v^2}\left ( |\phi|^2-v^2/2\right ) +\dots$$ plus others involving couplings with the higgs doublet $\phi$ , and we can refer to the subset as the higgs sector . the hidden sector features all the interactions and terms in the lagrangian which we expect exist , but have not explicitly written in the full standard model lagrangian .
no , euclidian space is not necessary . you can " model " intrinsic curvature using the beautiful language of riemannian geometry , whose great triumph was formulating a vocabulary that lets you talk about the curvature of a space without making reference to an extrinsic space in which the curved space is embedded : hence the term intrinsic . this is crucial to general relativity , since embedding a curved 4-space in flat euclidean space requires that the euclidean space be ten-dimensional , and dealing with that embedding would suck .
the quasiparticle description of photons is well known in condensed matter physics as the frequency dependent complex dielectric constant/magnetic-permeability of a material . you are asking if this quantity can be calculated from qed by a first principles method . this seems less difficult compared to other quasiparticles , because the photon usually stays noninteracting in the medium at ordinary temperatures , except for the absorption and dispersion . there is a mismatch of scale between the photon frequency/wavelength and other quasiparticles , which travel much slower than light . even so , i did not find many papers adressing this question , because we have good measurements of the dielectric constant as a function of frequency for any material . the material is doing the calculation for you . for dilute gasses , feynman adresses the question of calculating the dielectric constant from a first principles calculation in his 1963 acta physica polonica article which introduced ghost fields . the frequency dependent dielectric constant is determined from the scattering phase shift in the forward direction , which reproduces the qualitative behavior of even crazy cases like when you pass an atomic resonance . the reference is below . rp feynman , acta physica polonica , 24 ( 1963 ) 697 some classic reference for qed in media , which include calculations of the effects of dielectric properties at finite frequencies , are these original papers : e . m . lifshitz , “"the theory of molecular attractive forces” , sov . phys . jetp 2 , 73 ( 1956 ) . i.e. dzyaloshinskii , e . m . lifshitz , and l.p. pitaevskii , ”general theory of van der waals ' forces” , sov . phys . jetp 10 , 161 ( 1960 ) these papers use the given photon quasiparticle description to calculate the van-der-waals forces for an arbitrary configuration of macroscopic matter .
this was previously a comment to space_cadet 's answer but became long ( down-vote was not me though ) . i do not understand space_cadet 's talk about unstable orbits . recall that two-body system with coulomb interaction has an additional $so ( 3 ) $ symmetry and has a conserved laplace-runge-lenz vector which preserves the eccentricity . because interactions between planets themselves are pretty negligible one needs to look for explanation elsewhere . namely , in the initial conditions of the solar system . one can imagine slowly rotating big ball of dust . this would collapse to the sun in the center a disk ( because of preservation of angular momentum ) with circular orbits and proto-planets would form , collecting the dust on their orbits . initially those planets were quite close and there were interesting scattering processes happening . the last part of the puzzle is mystery though . if there were still large amount of dust present in the solar system it would damp the orbits to the point of becoming more circular than they are today . the most popular explanation seems to be that the damping of the eccentricity was mediated by smaller bodies ( like asteroids ) . read more in " final stages of planet formation " - peter goldreich , yoram lithwick , re'em sari .
if you build the square of the wave function , the result is a gaussian curve . if you compare your result with the general form of a normal distribution you can see that x0 is the expectation value . . . http://en.wikipedia.org/wiki/normal_distribution
as mentioned in the comments , you need one more piece of information to determine the magnitude of the velocity . you said that you might use the eccentricity , so in that case you can use the formula given here and deduce a quadratic equation on the velocity which yields : $$ v= \sqrt{\frac{g m}{r \sin ( \alpha ) } ( 1 \pm \epsilon ) } , $$ where $g$ is the gravitational constant , $r$ is the distance between the two masses , $m$ is the bigger mass ( i assumed here that one mass is much bigger than the other ) , $\alpha$ is the angle between the velocity vector and the radius , and $\epsilon$ is the eccentricity . note that since we had a quadratic equation , you still have two options for the velocity , both consistent with the given eccentricity .
if you do not want to use statistichal mechanics , you can view it as a completely mathematical thing . when you write the differential form $\delta q$ , you are not speaking of an exact differential , i.e. it is not really the differential of any function of the thermodynamical state . temperature is , in this case , called the integrating factor , which means that $\delta q/t$ is an exact form , in particular , it is $ds$ , the differential of entropy . this is a way to let entropy come out . on the other hand , much more physical explanations can be given . the first uses obviously stat mech , but without making calculations , i can just tell you that $s$ turns out to be very closely connected with the number of possible microscopical states a thermodynamical ( thus macroscopical ) state can admit . finally , a reason is that the quantity $\int \delta q/t$ is never negative in normal thermodynamical transformations , that is , it allows a simple formulation of the second principle . i hope this is what you were looking for .
i would likely to further dmckee 's answer by answering the op 's follow-up question : can you please explain why considering secondary sources as in huygen 's principle is justified , i.e. , why do we get correct results by assuming secondary sources when there are actually no sources other than the original source ? as feynman explains in case of electromagnetic waves , it is because the diffracted wave is equivalent to the superposition of electric fields of a hypothetical plug containing several independent sources . huygen 's principle is actually a fairly fundamental property of solutions of the helmholtz equation $ ( \nabla^2 + k^2 ) \psi = 0$ or the d'alembert wave equation $ ( c^2\nabla^2 - \partial_t^2 ) \psi = 0$ . for these equations the green 's function is a spherical wave diverging from the source . all " physically reasonable " solutions ( given reasonable physical assumptions such as the sommerfeld radiation condition ) in freespace regions away from the sources can be built up by linear superposition from a system of these sources outside the region under consideration . already this is sounding like huygen 's principle , but one can go further and , with this prototypical solution and the linear superposition principle together with gauss 's divergence theorem , show that waves can be approximately thought of as arising from a distributed set of these " building block " spherical sources spread over the wavefront : this result leads to the kirchoff diffraction integral , thence to various statements of huygens 's principle . this treatment is worked through in detail §8.3 and §8.4 of born and wolf , " principles of optics " or in hecht , " optics " , which i do not have before me at the moment .
the " wave " part of the wave-particle duality for particles such as electrons and protons ( as opposed to em radiation ) is called their wavefunction . it does not have any classical analogue and any attempt at understanding it using classical intuition can only be a crude analogy . however , if you are happy with the concept of a probability wave then it is exactly that . why is not this a problem with the uncertainty principle , then ? well , there is a corresponding uncertainty principle between the wavelength of any wave ( or more precisely its wavenumber $k=2\pi/\lambda$ ) and its position in space . the wavelength of a wave is only precisely definable , to arbitrary precision , if you have an infinite wave ; otherwise , you can only measure a finite number of periods and divide , and that will yield an imprecise measurement of $\lambda$ . ( even worse , the amplitude will taper out near the edges , so it will be hard to tell where each peak or trough is . ) to have a better-defined wavelength , then you need a bigger wavepacket , but this means that the position of the wavepacket in space , which only makes sense to a precision of the wavepacket size , has bigger uncertainty . this trade-off game can be expressed as $$\delta k\delta x \gtrsim1 , $$ and can be made precise using fourier analysis of waves .
it is because you would not hide in the corners like your kitty does ! a electric radiator is designed to be directional and therefore it does not heat the unnecessary part of your room . it makes you feel warming in front of it , but some part of the room do not get heated like those corner and the ceiling . in comparison , a vacuum cleaner heating the gas instead , so it is much more uniform , but less efficient from your point of view because you do not feel it ( your kitty might be happy about it though ) . as what @johnrennie , they dissipate the same amount of heat ( and some become noise ) .
this is simply the sum of the gravitational potential energy over all the points that make up the body . each point has a mass $\rho dv$ , meaning the mass density times the infinitesimal volume element , and this is multiplied by g and h , because the potential energy of a point at height h is $mgh$ . if you are asking why the potential energy is $mgh$ for a point , this can be argued using reversible elevators attached by pullies . if you want to raise a mass m by a certain amount h , you can do this by putting an equal mass on the other side of a pulley-elevator and lowering the mass by an equal amount . this process is easy--- you do not have to do work--- because the masses balance on the two sides . so if there is a conserved energy , it must be a quantity which is unchanged when you lower a mass by a given amount , so long as you raise the same amount of mass somewhere else by the same amount . since you can cut up big masses into small pieces , you can raise a big mass of mass 2m by h units of height by lowering two masses of size m each by h units . in all these processes , the sum of the mass times the height is conserved . when you have different gravitational forces , like deep in the interior of the earth , you can compensate by using a bigger mass . so it is mg which is the right unit to balance , the force , not the mass itself . this argument establishes that the potential energy is proportional to mgh , and that there is no numerical constant , that the potential energy is just equal to mgh , is just a best convention for defining the unit of energy from the unit of force . this argument is presented in detail in feynman 's lectures on physics vol 1 , in an early chapter . it is essentially due to archimedes , and it is also discussed in this answer to a related question : why does kinetic energy increase quadratically , not linearly , with velocity ?
assuming that by escape velocity you mean exhaust velocity , then the velocity comes from the maxwell-boltzmann distribution . this gives the velocity distribution of the particles in a gas as a function of temperature . for our purposes we can use the most probably speed , i.e. the peak in the distribution , as a rough estimate and this is given by : $$ v_e = \sqrt{\frac{2kt}{m}} $$ where $t$ is the temperature to which the reactor heats the propellant . so as you increase the molecular mass of the propellant the exhaust velocity falls as $1/\sqrt{m}$ . the problem is that if you carry a mass $m$ of propellant the total possible momentum change is $mv_e$ , so for propellant with a greater molecular weight you need to carry a greater mass of the propellant , $m$ , but then the extra weight of propellant adds to the weight of your rocket .
very briefly . the line of reasoning is the following : the acceleration $a^{\mu}$ is gr is rather formal construction , it is the covariant derivative of the speed $u^{\mu}$ with respect to some natural parameter $\lambda$ which parameterize a trajectory . for massive particles you can choose this parameter to be proper time $d\tau$ thus $a^{\mu}=du^{\mu}/d\tau$ , although it is not possible for massless particles , for which $d\tau=0$ . therefore your question is related to the following one : what is $du^{\mu}$ ? it turns out that the simplest ( and only ) way to construct the covariant differential of a vector field is to compare two infinitesimally separated vectors in the same point ( it is essential ) , e.g. , the vector $u^{\mu}\left ( x^{\alpha}+dx^{\alpha}\right ) $ and the vector $u^{\mu}\left ( x^{\alpha }\right ) $ which should be subject to a parallel translation to the point $x^{\alpha}+dx^{\alpha}$ . after the parallel translation we obtain a new infinitisimally close vector $u^{\mu\prime}=u^{\mu}\left ( x^{\alpha}\right ) +\delta u^{\mu}$ , thus $$ du^{\mu}=u^{\mu}\left ( x^{\alpha}+dx^{\alpha}\right ) -u^{\mu\prime}=du^{\mu }-\delta u^{\mu} , $$ where $du^{\mu}=u^{\mu}\left ( x^{\alpha}+dx^{\alpha}\right ) -u^{\mu}\left ( x^{\alpha}\right ) $ is the ordinary differential . therefore , the small addition $\delta u^{\mu}$ is the result of parallel translation . there are two obvious properties of $\delta u^{\mu}$: it should be linear in $u^{\mu}$ and should vanish with $dx^{\mu}\rightarrow0$ . therefore one can represent $\delta u^{\mu}$ as follows : $$ \delta u^{\mu}=-\gamma_{\alpha\beta}^{\mu}u^{\alpha}dx^{\beta} , $$ where $\gamma_{\alpha\beta}^{\mu}$ is the set of some matrices usually referred as “connection coefficients” or “christoffel symbols” . using $\gamma$ , one can generalize the covariant differential $d$ to any tensor quantities . although , there are no additional mathematical requirements on $\gamma$ , there are physical ones in gr — the equivalence principle requires that $\gamma$ should be symmetric $\gamma_{\alpha\beta}^{\mu}=\gamma_{\beta\alpha}^{\mu}$ and $dg_{\alpha\beta}=0$ . the last condition results in $$ \partial_{\mu}g_{\alpha\beta}=-\left ( \gamma_{\mu , \beta\alpha}+\gamma _{\beta , \mu\alpha}\right ) , $$ where $\gamma_{\mu , \beta\alpha}=g_{\mu\rho}\gamma_{\beta\alpha}^{\rho}$ . using the condition that $\gamma$ is symmetric one can find : $$ \gamma_{\beta\alpha}^{\rho}=\frac{1}{2}g^{\rho\sigma}\left ( \partial_{\beta }g_{\sigma\alpha}+\partial_{\alpha}g_{\sigma\beta}-\partial_{\sigma} g_{\alpha\beta}\right ) . $$ let 's now consider a parameterized trajectory $x^{\mu}\left ( \lambda\right ) $ , the contravariant vector called speed is $u^{\mu}=dx^{\mu}\left ( \lambda\right ) /d\lambda$ , therefore the contravariant acceleration takes the form : $$ a^{\mu}=\frac{du^{\mu}}{d\lambda}=\frac{du^{\mu}}{d\lambda}-\frac{\delta u^{\mu}}{d\lambda}=\frac{du^{\mu}}{d\lambda}+\gamma_{\alpha\beta}^{\mu }u^{\alpha}u^{\beta} . $$ if we choose $d\lambda=d\tau$ then in the locally-inertal frame ( $\gamma=0$ ) for the trajectory $x^{\mu}\left ( \lambda\right ) $ the acceleration $a^{\mu }$ coincides with the ordinary one $\left ( 0 , \mathbf{a}\right ) $ . and vice versa , $du^{\mu}=0$ means that $u^{\mu}$ is constant in the locally-internal frame although it does imply that it is constant in any other frame , in fact $du^{\mu}=\delta u^{\mu}$ implies that a free-fall trajectory is actually a parallel translation in gr , which ( for an external observer ) looks like the action of gravitational forces .
given some distribution or density $\rho ( x ) , $ a moment is the ' expectation value ' of some power of $x \in \mathbb{r}$ . to be precise , the $n$-th moment $m_n$ is given by $$m_n = \int_{\mathbb{r}} x^n \rho ( x ) \mathrm{d}x . $$ in the mechanics case , $\rho ( x ) $ is simply the mass density . you can extend this to vectors in $\mathbb{r}^d$ in a straightforward way ; for example , for the moment of inertia you replace $x^2$ by $\mathbf{x}^2 = x_1^2 + \ldots x_d^2$ to obtain $$i = m_2 = \int_{\mathbb{r}^d} \mathbf{x}^2 \rho ( \mathbf{x} ) \mathrm{d}^dx$$ which should match the definition given in your mechanics textbook . for the first moment of mass , you need to distinguish different directions . as you indicate , you can choose your coordinates such that $$\int_{\mathbb{r}^d} x_i \ , \rho ( \mathbf{x} ) \mathrm{d}^d x = 0$$ where $i$ runs over the coordinates . in three dimensions , you have $x_1 = x , x_2=y$ and $x_3=z . $
one of the biggest failure of theoretical condensed matter and/or material sciences is that up to now , nobody has ever been able to predict what compounds will be a good superconductors . of course , since we do not really understand high-tc superconductivity , we cannot predict which ceramic will or will not be a nice superconductor . but even in the case of more standard superconductors described by bcs or more refined theories ( like eliashberg 's theory ) , the predictive power of theoretical approaches is close to zero . to summarize , all superconductors are found experimentally , and then theorists try to explain why this particular alloy/compound has these properties .
i do not think there is a good general explanation of this ; the best i can do is give a few hand waving arguments . if you look at hydrogen sulphide , which the analogue of water moving one row down in the periodic table , you will find it does shrink when it freezes , just like most other liquids . so the difference between the h$_2$s and h$_2$o molecules must be responsible for the anomalous behaviour of water . the most obvious difference between the two molecules is the the h-o bond is highly polar and has a strong electric dipole associated with it . this means it interacts strongly with other h-o bonds ; the interaction is known as hydrogen bonding . hydrogen bonds are highly directional , as you had expect for an electric dipole , so the water molecules can be fitted together in ice any old how . they adopt well defined positions relative to each other , and the directionality of the bonds forces the water molecules into a relatively low density arrangement . it is interesting to note that the high pressure forms of ice are generally denser than water . presumably at high pressure the reduction in energy by the denser arrangement outweighs the reduction in the strength of the hydrogen bonds .
any object , whether it be magnetized or not ( and in particular whether it be ferromagnetic or para/diamagnetic ) , will only experience magnetic forces when it is placed in an external magnetic field . if you are thinking of a long , straight conductor , then the magnetic field it produces will increase as you get near it ( though it does decrease inside the conductor ) . that said , there are some configurations of currents that produce a magnetic field " outside " and but none " outside " . two examples are a toroidal solenoid , in which the field is confined to the solenoid interior and is zero in the donut hole , and two long cylindrical , coaxial solenoids carrying the same current in opposite directions , for which the field is confined to the middle region and cancels out in the inner region . a coaxial cable is an example of the latter . if you are thinking about an extended distribution of currents such as a wire of nonzero thickness , then yes , the field is zero at the centre and it is small nearby . however , you can hardly place a metallic object inside a wire .
there is the ground based observatory ( nice picture ) veritas . veritas ( very energetic radiation imaging telescope array system ) is a major ground-based gamma-ray observatory located at the basecamp of the fred lawrence whipple observatory in southern arizona , designed to observe and study very-high-energy ( vhe ) gamma-rays ( energies above ~100 gev ) . because it is very difficult to produce gamma-rays , the objects that emit them are very interesting to astrophysicists . high-energy gamma rays are associated with exploding stars ( supernovae ) , pulsars , quasars , and black holes rather than with ordinary stars or galaxies . there is the space based fermi large area telescope the lat is an imaging high-energy gamma-ray telescope covering the energy range from about 20 mev to more than 300 gev . such gamma rays are emitted only in the most extreme conditions , by particles moving very nearly at the speed of light . the lat 's field of view covers about 20% of the sky at any time , and it scans continuously , covering the whole sky every three hours . currently the lat scientific collaboration includes more than 400 scientists and students at more than 90 universities and laboratories in 12 countries . the collaboration has published papers on pulsars , active galactic nuclei , globular clusters , cosmic-ray electrons , gamma-ray bursts , binary stars , supernova remnants , diffuse gamma-ray sources and other subjects . a recent analysis of 130gev gammas mentioned in a blog drew my attention to it . so the energies detected go fairly high and there are means of measuring the energetic photons , developed for the earth bound accelerators .
it is simple . the dilaton-axion ( complexified ) field in supergravity ( and similar classical theories with a noncompact symmetry ) is invariant under $sl ( 2 , r ) $ transformations $$\tau \to \frac{a\tau+b}{c\tau+d} , \quad ad-bc=1$$ however , the same transformation must also transform the charges of objects . for example , one-dimensional branes always carry the charge like $m$ fundamental strings superposed on top of $n$ d1-branes . so the general charge ( density ) of a d1-brane is given by two numbers $ ( m , n ) $ . under the transformation above , they transform to $$ ( m , n ) \to ( am+bn , cm+dn ) $$ because the $sl ( 2 , r ) $ transformation mixes the two types of one-brane charges ( and similarly for other dimensions of branes , including the instantons ) . in the classical theory , the charge of a black $p$-brane , including the one-branes above , is ( a generalization of the charge of a charged black hole ) given by any real numbers ( charges ) $m$ and $n$ . however , quantum mechanically , $m$ and $n$ have to be integers in certain units . consequently , we only get allowed one-branes after the transformation if the final charges , $ ( am+bn , cm+dn ) $ , are integers for all integers $ ( m , n ) $ . this requirement of the integrality of charges implies that $a , b , c , d$ have to be integers by themselves and only the $sl ( 2 , z ) $ subgroup of $sl ( 2 , r ) $ maps allowed states in the hilbert space ( a superselection sector ) to other allowed states in the same hilbert space . note that the quantization ( integrality ) of the charges such as $m , n$ above is required by quantum mechanics . the one-branes are electromagnetic duals of five-branes that carry their charges , too – a combination of d5-brane and ns5-brane charges ( completely analogous to the two one-branes ) . because all these four types of charges ( d1 , f1 , d5 , ns5 ) are allowed to be nonzero but quantum mechanics enforces the dirac quantization rule that the spacing of the d1-brane charge is inverse up to a $2\pi$ factor to the spacing of the d5-brane charge , and similarly for f1 and ns5 , it follows that all these four charges must belong to a lattice . in other words , there has to exist a linear redefinition or convention in which all these four charges are integers .
probably not , but it depends on the geometry of your coil . for a couple of dollars at the hardware store you can get a big stack of those coin magnets . if the answer to your question were yes in general , it would be harder to break apart the big stack of magnets that to separate two of them . that is not consistent with my experience . in general for a dipole $\vec m$ in a field $\vec b$ , the force is $\vec f=-\nabla ( \vec m \cdot \vec b ) $ . if the dipole moment is a constant , and the dipoles are free to rotate , they will orient themselves so that $\vec m$ is antiparallel to $\vec b$ . in that special case , the force simplifies to $$\vec f \approx m \vec\nabla b . $$ in other words , the dipoles " want " to align antiparallel to the field , then to move up the gradient into the strongest part of the field . a dipole in a uniform field feels a torque , but not a net force . if your coil is set up to generate a uniform field , your two stacked magnets will feel the same force — but that force will be zero . if your coil is generating a magnetic field with some dipole component , your stacked magnets will rotate to align with the field , then ( since we have already assumed there is a field strength gradient ) one of them will see a weaker gradient than the other . the gradient $\vec\nabla b$ typically vary like $1/r^4$ , where $r$ is the distance from the center of the coil , so the force on the two coin magnets can vary considerably over a short distance . this is why strong magnets like to suddenly " leap " together and pinch your fingers when you are playing with them . now you could put your two coin magnets next to each other , symmetrically about the axis of your coil , and they would see the same $|\vec\nabla b|$ in slightly different directions . but with the two coin magnets next to each other , they would exert torques on each other , since they prefer to be stacked pole-to-pole ; you could engineer a solution like this , but there would be extra parts involved . i actually happened to have some magnets with this shape on my desk . in response to carl 's comment , i built a little string lifting harness to measure the force on a single magnetized paper clip : adding a second magnet increased , but did not double , the weight that the harness could hold : stacked number of clips magnets that stayed up 1 11 2 17 3 23 4 24 5 24  the dominant effect in calculating the force is the field is right at the surface of the stack of magnets ; it looks like that gets saturated , an effect i did not consider in the first part of my answer .
as can be seen in this pie chart taken from the wikipeia article on " universe " , the significant parts of the universe are , in descending order , dark energy , dark matter , gas , stars , and the ghostly subatomic particles called neutrinos . that is the most laymanish terms that can be used , because nobody knows what the first two actually are .
looks like i have to answer this question :- ) let me first answer the math question : every zero-energy eigenstate is of the form of a symmetric polynomial times the laughlin wave function . to be concrete , let us consider an $n$ boson system , with delta-potential interaction $v=g\sum \delta ( z_i-z_j ) $ where $z_i$ is a complex number describing the position of the $i^{th}$ boson . the zero energy state $\psi ( z_1 , . . . , z_n ) $ satisfies $\psi ( z_1 , . . . , z_n ) =p ( z_1 , . . . , z_n ) exp ( -\sum_i |z_i|^2/4 ) $ where $p$ is a symmetric polynomial that satisfy $\int \prod_i d^2 z_i \ \psi ( z_1 , . . . , z_n ) ^\dagger v \psi ( z_1 , . . . , z_n ) =0$ . now it is clear that all the zero energy state are given by symmetric polynomial that satisfy $p ( z_1 , . . . , z_n ) =0$ if any pair of bosons coincide $z_i=z_j$ . for symmetric polynomial this implies that $p ( z_1 , . . . , z_n ) \sim ( z_i-z_j ) ^2$ when $z_i$ is near $z_j$ . the laughline wave function $p_0=\prod_{i&lt ; j} ( z_i-z_j ) ^2$ is one of the symmetric polynomials that satisfies the above condition and is a zero energy state . since any other zero-energy symmetric polynomial must satisfy $p ( z_1 , . . . , z_n ) \sim ( z_i-z_j ) ^2$ , $p/p_0=p_{sym}$ has no poles and is a well defined symmetric polynomial . so every zero-energy eigenstate $p$ is of the form of a symmetric polynomial $p_{sym}$ times the laughlin wave function $p_0$ . more discussions can be found in the first part of arxiv:1203.3268 . however , a physically more relevant math question is : every energy eigenstate below a certain finite energy gap $\delta$ is of the form of a symmetric polynomial times the laughlin wave function for any number $n$ of particles . ( here $\delta$ does not depend on $n$ . ) we only have numerical evidences that the above statement is true , but no proof .
db is basically a ratio measurement in logarithmic form . the sound intensity level $l_1$ is found by applying the following formula to two intensities such as $i_1$ and $i_0 $ . $$ l_\mathrm{i}=10\ , \log_{10}\left ( \frac{i_1}{i_0}\right ) \ \mathrm{db} \ , $$ i.e. , $i_1$ is $l_1~db$ higher than $i_0$ . in your question , $l_1 = 2.00 . $ we also know that intensity is proportional to the inverse of distance squared : $$\frac{i_1}{i_0}= ( \frac{r_1}{r_0} ) ^{-2}$$ where $r_1$ is the nearer distance ( thus higher intensity ) . we are also given that $r_1 = r_0+1$ with this you should be able to solve for everything .
assume that the generating functional is given by a sum of all possible diagrams , i.e. $$z ( j ) =\sigma_{n_i} d_{n_i} . $$ furthermore , assume that each diagram d is given by a product of connected diagrams $c_i$ , i.e. a diagram d can be disconnected . we will write this as $$d_{n_i}=\pi_i\frac{1}{n_i ! }c_i^{n_i} , $$ where dividing by $n_i ! $ amounts for a symmetry factor coming from exchanges of propagators and vertices between different diagrams . combining this with our first expression , we get $$z ( j ) =\sigma_{n_i}\pi_i\frac{1}{n_i ! }c_i^{n_i} . $$ with some manipulation , this can be shown to be equivalent to $$z ( j ) =\text{exp} ( \sigma_i c_i ) . $$ taking the logarithm on both sides gives you the desired expression .
note that your formula $$e=\int p ( \omega , t ) dt=\int u ( \omega , t ) i ( \omega , t ) dt$$ can be rewritten as $$e=\int u ( \omega , t ) i ( \omega , t ) dt=\int\frac{u^2 ( w , t ) }{z}dt$$ now , let $u=u_0\sin ( wt ) $ and $z=const$ which is reasonable during a short period of time $t$ . thus : $$e=\frac{1}{z}\int_0^{t} u^2 ( w , t ) dt=\frac{u_0^2}{z}\int_0^{t} \sin^2 ( w , t ) dt$$ next : $$\int_0^{t} \sin^2 ( w , t ) dt=\frac{t}{2}-\frac{\sin ( 2wt ) }{4w}$$ so , if $w\rightarrow\infty$ , then $e$ does not depend of $w$ . edit : additions : to be more specific , the circuit is impedance $z$ depends on the frequency as $$z=\sqrt{r ( w ) ^2+\left ( wl-\frac{1}{wc}\right ) ^2}$$ where $l$ is the circuit is impedance and $c$ is the circuit is capacitance and $r$ is the resistance which depends also on $w$ due to the skin effect . that means $z \rightarrow wl$ as $w\rightarrow\infty$ if $l\neq 0$ so an answer , closer to reality is that $$e=\frac{u_0^2}{wl}\frac{t}{2} ; w\rightarrow\infty$$
believe it or not , but it is ( a ) . for some reason moon is rotating about its own axis with the same rotation frequency as it is rotating around earth , so we always see the same side of the moon .
sadly your tensor would have to obey , $$t_{abc} = -t_{cba} = -t_{bca} = t_{acb} = -t_{abc} , $$ and therefore would have to be equal to zero .
use wheatstone bridge : http://en.wikipedia.org/wiki/wheatstone_bridge this is classical way of measuring resistivities with no error due to imperfectness of measurement instruments ( e . g . galvanometer ) .
even considering the same fabric with different colours it will depend a lot on optical properties of the dye which we cannot tell only by the colour that we can see . i would need to know " how much the silver paint would reflect and in what spectral range " and also " how is the absorption coefficient as a function of the wavelength ( from near-uv , visible to near-infrared ) " . i believe that this is the most important point and not the colour that we perceive with our eyes . nevertheless , keeping things simple , i think that the reflective coating being outside or inside would make the same amount of radiation that passes through . the only difference is that putting the reflective layer inside you would be increasing the path length of the radiation , thus increasing the absorption , and therefore the temperature edit : in order to clarify my idea i added this picture . dashed layer is reflective and the the grey layer is the normal dark layer . i would like to divide this discussion in two : " radiation protection " and " heat protection " regarding the radiation protection , i believe that it depends on how the the dark layer absorbs light in all spectral range of the sun light , i.e. absorption coefficient and thickness of the material+dye . it also depends on how the reflective layer reflects the light , i.e. reflectance . i believe that the radiation protection does not depend on the arrangement , so both schemes protect from radiation as good . ( i am not considering far infrared radiation ) regarding heat protection , the thought is the same , except we should consider that in the first case the length of the radiation in the dark layer is longer , thus absorbing more radiation thus heating more . also , the protection from the heat will depend on the heat conductivity of the layers and surface morphology which will affect how well the layer will cool down . i understand that having a reflective coating outside an umbrella would be unpleasant for other people . but it would be the best choice for heat protection . however , regarding the emergency heat blankets , i do not know why they also have a reflective layer on the inside .
accelerated charged particles emit electromagnetic radiation . in this case , where the acceleration is caused by a magnetic field and is perpendicular to the velocity , the radiation is called cyclotron radiation . since the magnetic field does not work on ( electrically ) charged particles , the radius of the charged particle should reduce , as it is energy ( and so it is speed ) reduces due to the radiation .
wrong . you are neglecting the viscosity of the water . friction from the inner wall of the sphere will move the water , which will in turn move the marbles and they will likely rotate around the sphere , though at a speed slower than the rotation of the sphere . if your sphere was on earth , then the only reason the marbles stay at the bottom is gravity pulling them down . if you shake the sphere ( even assuming that the walls do not touch the marbles ) , then they will move in this situation too due to the force of the water flowing around the sphere .
you can reconcile both trains of thought by reconsidering your thoughts about pushing:- for dc circuits without changing magnetic fields , the voltage is the energy gained or lost per unit charge in moving from one position to another , say from the positive to the negative terminal of the battery . what a battery does , is that it creates certain junctions which intrinsically have a potential difference between them even in equilibrium . the battery , through chemical reactions , maintains this potential difference . the potential difference between the two ends of the battery is the culmination of all these junction potentials within the battery which is maintained by ongoing chemical reactions . so the battery 's job is to only maintain a potential difference across its terminals , and the rest of the drama plays out on its own . now , if we connect the two terminals of a battery by a conducting wire , the wire now has its ends at different potentials , that is , there is a potential difference across the ends of the wire . this potential difference sets up an electric field within the wire from the positive to the negative ( $\vec e=-\nabla v$ ) terminal . this field is what does all the pushing . this field propels the electrons towards the positive terminal . since the electrons keep colliding with the surrounding metallic kernels , they never acquire a constant acceleration , but they acquire a constant drift velocity with which all the electrons slowly edge towards the positive electrode . when the electrons collide , they lose energy as heat . therefore , when electrons having unit electric charge come from the negative end to the positive end , colliding along the way , the amount of energy dissipated as heat ( or any other way ) will be equal to the potential difference between the two ends of the wire , which is the terminal voltage of the battery . all what the battery does is that it carries on chemical reactions to maintain a constant potential difference between its ends . the condition when the circuit is setting up is distinct from the condition in steady state . when the circuit is starting the electric field is still not established along the wire , i.e. all the electrons along the wire do not feel the push of the electric field . here , the end closest to the terminal " feels " the potential difference first and starts moving . as only a small part of the conductor has a current , there will be unbalanced charges which make the next part feel the potential difference , and so on until the other end of the wire . this is similar to the kind of pushing you describe . but all of this happens to fast to matter in most circuits and the condition after the circuit is setup is described above .
the repelling is another way of saying that owing to the strength of the hydrogen bonding between water molecules , the water molecules are better off with themselves alone as compared to with non-interacting non-polar molecules within . a substance dissolves only in a solvent , where the solvent-solute interaction is as strong ( or stronger ) than the solvent-solvent interaction and therefore the solvent finds it better ( energetically and thermodynamically favourable ) to allow the solute molecules to dissolve , i.e. take up spaces between the molecules . but if the solute-solvent interaction is poor , ( as in the case of non-polar/hydrophobic molecules ) , the solvent finds it better to be among itself and not allow the hydrophobic molecules to take up spaces between the molecules , which is equivalent to having repelled the non-polar substances , i.e. their mixing with water is energetically opposed . this is also the reason why polar substances do not dissolve in non-polar solvent . there are hydrophobic surfaces which depend on the surface energy or the contact angle of water on that surface , but the same argument cannot be extended to molecules where there is no surface to account for the surface energy .
the oscillatory part is nothing but thomas-fermi approximation or more riguresly , this is a version ( someone should correct me if i am wrong ) weyl 's formula regrading on how to obtain the wkb from the trace formula : you can read the 2 papers by berry and tabor on how they derived a trace formula ( like that of gutzwiller ) but to the case of integrable systems . from the derivation there you can see how the ebk pop up . . .
the electric field intensity is usually defined in introductory textbooks as the limit $$\lim_{q\rightarrow0}\frac{\mathbf f}{q} , $$ i.e. the force on a test charge , per unit charge , when that test charge goes to zero . however , you are right in noting that the limit $q\rightarrow 0 $ is impossible to take because charge is quantized , and you can never have charges below one electron charge $q=e$ . because of that , it is easier to define the electric field produced by a point charge $q_1$ at a relative separation $\mathbf r$ as the vector $$\mathbf e:=\frac{1}{4\pi\epsilon_0} \frac{q_1 \hat{\mathbf r}}{r^2} , $$ and then stipulate as a physical law that the force such a field ( or any electric field ) exerts on a second charge $q_2$ is given by $\mathbf f=q_2\mathbf{e}$ , with due stipulation that self-interactions are not allowed . this procedure is both mathematically consistent and concurs with physical , experimental observations .
i think the dominant effect might actually be the fact that the salt you add might not be at boiling temperature . but this is just based on the fact that the boiling-point elevation due to salt in water is actually quite low for typical amounts of salt used in cooking , say . i am not too familiar with the second effect you mention though .
for a perfect full moon , simply change am to pm and vice versa . for other phases , there is a different offset . first and third quarters would be six hour offsets , etc . , though i would have to think a while before stating which was plus and minus . during a total solar eclipse , the times would exactly coincide , and approximately so for a new moon . ( good luck finding the shadow cast by new moonlight . ) technically , for best precision you ought to account for the equation of time and the slight change in moon phase over the course of a moon-day . also , since the moon 's orbit is inclined with respect to the earth 's , you can not use the same latitude setting all the time , but this is a frikking repurposed sundial we are talking about . if you are really after precise timekeeping , use a technology invented in the last thousand years .
i believe it was xiao-gang wen in 1989 , see also this 1994 paper by him and his collaborators http://dao.mit.edu/~wen/pub/ednab.pdf he is at mit . i was once hosting a seminar by him , he is one of the most creative and playful folks in this segment of condensed matter physics . the paper above contains some other relevant references , including a paper by wen and tony zee .
you cannot have a total vorticity with periodic boundary conditions , since if you take a path around all of your vortices , it will have a non-zero circulation . but you have periodic bc , so you can continuously deform that path to a point , and a point has zero circulation . mirror images are not quite the same as in electrostatics . we want periodic boundary conditions . to get periodic boundary conditions you imagine tiling the plane with your system . this will trivially be periodic and so you can just take a tile , and work with that . so you want the mirror image of a vortex to be a vortex . ( in electrostatics you usually use mirror images to enforce not periodic boundary conditions , but to enforce constant voltage . that is why you flip the sign of the mirror charge . here we want periodic b.c. if you flipped the sign of the vortices i believe you would get *anti*periodic boundary conditions , but do not quote me on that . ) this evolving in imaginary time is presumably an " annealing " type of operation . you are free to run the gp equation on any initial condition you want . however , to cleanly see the interaction of the vortices , we want to the vortices to be in their ground state . otherwise when we turn on the time evolution they will get rid of their excess energy by shedding waves and other junk . one way to get to the ground state is to evolve you equation in " imaginary time " . your usual time evolution is $\exp ( i\hat{h}t ) $ . if plug in $t = i\tau$ you get $\exp ( -\hat{h}\tau ) $ . applying this to a state exponentially suppresses the higher-energy components , so you get rid of the high energy stuff . this is related to finite-temperature ( just replace $\tau$ with $\beta$ and you have the partition function ) , but for your purposes you can just consider it a convenient mathematical trick . note that since you are annealing anyway , the specific details of what state you start might not be so important , since you will end in the same place anyway ( hopefully ) . finally , they are a little thin on the details , so if you plan to use this work , you should just send the authors an email asking for details .
at $r=0$ we should have $−v_0 r ( r ) =er ( r ) $ , which implies $e=−v_0$ . ( is this allowed ? ) nope , not allowed . in any case , that ode is not as bad as it looks . change variables from $r ( r ) $ to $u ( r ) $ [ which was defined in the problem as $u ( r ) = r r ( r ) $ ] , and it will wind up looking very familiar . you will come up with a second-order ode that has two linearly independent solutions , $$u_1 ( r ) = \cdots , \quad u_2 ( r ) = \cdots$$ then you can get the two independent solutions to the original schrodinger equation as $r_i ( r ) = \frac{u_i ( r ) }{r}$ . the final physical solution for $r ( r ) $ needs to be well-defined at the origin . ( in fact you realized this in the second part , but you do not need to worry about it there because $r &gt ; r_0$ does not include the origin ; but you do need to worry about it here ) so you need to pick a linear combination of $r_1 ( r ) $ and $r_2 ( r ) $ that is not infinite at $r = 0$ . hint : what needs to be the numerical value of $u ( 0 ) $ ? the other part , with $r &gt ; r_0$ , is extremely similar . again , remember that there are two linearly independent solutions . you only found one of them . also , the solution you found does not blow up if $k$ is negative - but are you sure that $k$ is negative ? what do you know about the value of $e$ ? ( specifically , what does it mean for the particle to be in a bound state ? ) you do not need to care about what the solution to the second part does at the origin , because the region you are solving the equation in does not include $r = 0$ . but it does include $r \to \infty$ , so you will need to pick a linear combination of the solutions that stays finite in that limit .
you might like inward bound by abraham pais . author was a particle physicist . the book is mostly a history of particle physics , but quantum mechanics is heavily intertwined . otherwise it meets your criteria perfectly .
in the equations as you have written them , the constant of proportionality is an outward-pointing vector for the electric field and an inward-pointing vector for the gravitational field . or in other words , if you take the radial component only : it is a positive constant for the electric force and a negative constant for the gravitational force . the details : gauss 's law for electrostatics actually says $$\iint\vec{e}\cdot\mathrm{d}\vec{a} = \frac{q_\text{enc}}{\epsilon_0}$$ and for newtonian gravity , you can write $$\iint\vec{g}\cdot\mathrm{d}\vec{a} = -4\pi g m_\text{enc}$$ for a spherically symmetric surface and mass/charge distribution , letting $\hat{n}$ represent the outward-pointing normal vector at each point on the surface , these simplify to $$\vec{e} = \frac{q_\text{enc}}{4\pi\epsilon_0 r^2}\hat{n}$$ and $$\vec{g} = -\frac{gm_\text{enc}}{r^2}\hat{n}$$ note that the constant of proportionality in the first case is $\hat{n}/4\pi\epsilon_0 r^2$ , which points outward , and in the second case is $-g\hat{n}/r^2$ , which points inwards .
just because the maximum speed is $6\pi\text{ cm/s}$ does not mean that $6\pi = 6\pi \cos ( 3\pi t ) $ . keep in mind that speed is the absolute value of velocity $x&#39 ; $ .
just realize that you can form ordinary dirac spinors from 2-spinors by using charge conjugation , $i\sigma_2\eta^*$ , that gives a right- handed field that can fit in the right-handed slot ( forming a 4 component majorana field ) $$ \psi_1=\left ( \begin{array}{c}\eta \\ i\sigma_2\eta^*\end{array}\right ) $$ and analogous for $\psi_2$ in terms of $\chi$ . then you just look at the ' mass terms ' $\bar\psi_1 \psi_2$ to get your term ( well in fact you need to insert also a $p_l$ ) . i think the textbook by ramond shows this kind of things . actually , if you add also the hermitian conjugate to your expression , you can even fit all in a single dirac spinor $$ \psi=\left ( \begin{array}{c}\eta \\ i\sigma_2\chi^*\end{array}\right ) $$ and look at $\bar{\psi}\psi$ .
the show you watched seems to get two concepts mixed up : supersymmetry and dark matter . the existence of dark matter is strongly hinted at by comsological and astrophysical considerations . it is the easiest explanation for several observations we make in the universe . supersymmetry on the other hand provides a candidate particle . the lightest supersymmetric particle is conjectured to be stable and could therefore constitute dark matter . also , supersymmetry has ( at least ) 5 higgs bosons , which is most surely the context in which that number came up in the show . so , while the ( direct ) observation of dark matter and additional higgs bosons at the lhc would make a strong case for supersymmetry , both concepts are primarily unrelated .
no they will not . space is intrinsically isotropic , so assuming they are not aware of any specific reference points , and they are far enough away from a massive body as to experience an insignificant amount of gravity , there would be no way of knowing their orientation . gravity essentially provides observers with a force field that the body can utilise to establish orientation etc . hope this helps .
the behavior of the non-axial rays is illustrated on the picture below . rays ( red ) falling in direction determined by the vector cd ( in circle ) reflect from the surface of a parabola ( blue ) , forming an intersection at point j ( red dot ) . the intersection point is obviously out-of original focus ( yellow dot a ) . tracking the direction vector shows the tracks of an intersection point ( red and gray dots ) , which form a mustache-like pattern originating from a . more interesting is the following picture - it shows that parallel rays do not even focus in a single point at all ! one pair of rays intersect at j ( red ) , while other pair intersect at n ( green ) . green and red tracks are different , so rays do not focus . they are smeared along a ( probably linear ) path consisting j-n .
it is not really the legendre polynomials you should be calculating against , but more precisely the spherical harmonics $$y_{lm}=\sqrt{\frac{ ( 2l+1 ) ( l-m ) ! }{4\pi ( l+m ) ! }}p_l^m ( \cos ( \theta ) ) e^{im\phi} . $$ exactly what you integrate will depend on what exactly is the object you are studying , but the essentials will be the same . none of the coefficients by themselves will mean much , but in general bigger coefficients mean bigger anisotropies ; the different varieties of spherical harmonics represent different modes of anisotropy . for example , if the coefficients for $l=1$ are nonzero then it probably means you chose the wrong origin , and the cluster centre is in some particular direction . if the coefficients for $l=2$ are nonzero then you will have some sort of ellipsoidal distribution ; the moments then tell you if it is oblate or prolate , how much , and in which direction . ( this is a common problem in nuclear physics , where the quadrupolar moments of nuclei are essential properties . ) nonzero $l=3$ moments could imply some sort of triangular or tetrahedral structure , and so on . a good way to picture these anisotropies is to graph the surface $r=1+\epsilon y_{lm}$: you should also be careful because the different moments are not necessarily comparable , and it could be hard to tell which one is more important . an example of this is in electrostatics . say you have some localized density of electrical charge $\rho ( \mathbf{r} ) $ which you are trying to examine from far away . then you can decompose the far field into its multipolar components as $$ \phi ( \mathbf{r} ) =\sum_{l=0}^\infty\frac{4\pi}{2l+1}\sum_{m=-l}^l q_{lm}\frac{y_{lm} ( \theta , \phi ) }{r^{l+1}} , $$ where $$q_{lm}=\int\mathrm{d}\mathbf{r} \rho ( \mathbf{r} ) r^ly_{lm}^\ast ( \theta , \phi ) $$ are the multipole moments . these measure the different anisotropies of the distribution and imprint it on the different anisotropic components of the far field . however , their relative importance changes with the distance from the distribution , because the different anisotropic ( multipolar ) components decay at different rates . thus the ratio between different coefficients essentially tells you the distance you need to be at for the more anisotropic component to be negligible . in your case , i gather you want to study the cluster boundary as a surface $r=r ( \theta , \phi ) $ . my first shot at that would be to attempt a decomposition of the form $$r ( \theta , \phi ) =\sum_{l=0}^\infty\sum_{m=-l}^l r_{lm}y_{lm} ( \theta , \phi ) , $$ where you can find the coefficients $r_{lm}$ using the orthogonality properties of the harmonics . here the $r_{lm}$ are all distances and are thus comparable , so that if some coefficient is much smaller than another you can probably ignore it . each coefficient then tells you how much of some particular type of anisotropy ( multipolarity ) your surface has . you should be careful when judging them as absolutes because they are distances and therefore carry dimensional information ; to remove that you could try dividing by $r_{00}$ , which is up to constants the radius of the cluster . a useful measure of total anisotropy could then be $$\sum_{l\neq0}\left|\frac{r_{lm}}{r_{00}}\right| , $$ or squaring each term , or some such .
it is easy to check if your calculations are right from this ( rather crude ) drawing : there you see that the size of an object is related to the size of the image according to $$h_o=h_i\frac{d}{f}$$ where $h_o$ and $h_i$ are the sizes of the object and image . $f$ is not a focal distance , but the distance from the pinhole to the sensor or film . so , if you have a 10cm object at 10m from the camera , and you want that to be 0.5mm ( 50px width ) , you need to place the sensor 0.5mm*10m/10cm=5cm behind the pinhole . if you do that , the angle your camera will see is $\tan ( \alpha ) =h_\mathrm{ccd}/ ( 2f ) $ . the field of view is defined as twice this angle ( $\mathrm{fov}=2\alpha$ ) . with the above numbers , the vertical fov is 4.1º and the horizontal fov is 5.5º .
the simple answer is that the sun 's gravity produces the same acceleration on both the earth and the moon . the sun is pulling both of them along , but they are falling together . you may imagine two skydiver jumping out of a plane at the same time ( and we had better ignore air resistance ) . they are subjected to gravitational forces from the earth that vastly larger than the forces between them , but that does not rip them away from each other because they both experience the same acceleration .
it is clear from no signalling--- by entangling $a_1$ and $a_2$ , alice uses local operators which necessarily commute with the spin operators on $b_1$ and $b_2$ , so the reduced density matrix for $b_1$ and $b_2$ stays completely random . it makes no difference what method alice uses , unless it involves mucking around with bob 's electrons .
if we do an interference experiment with a ( charged ) particle coupled to the electromagnetic field or a massive particle coupled to the gravitational field , we can see interference if no information gets stored in the environment about which path the particle followed ( or at least , if the states of the environment corresponding to the two paths through the interferometer have a large overlap --- if the overlap is not 1 the visibility of the interference fringes is reduced ) . the particle is " dressed " by its electromagnetic or gravitational field , but that is not necessarily enough to leave a permanent record behind . for an electron , if it emits no photon during the experiment , the electromagnetic field stays in the vacuum state , and records no " which-way " information . so two possible paths followed by the electron can interfere . but if a single photon gets emitted , and the state of the photon allows us to identify the path taken with high success probability , then there is no interference . what actually happens in an experiment with electrons is kind of interesting . since photons are massless they are easy to excite if they have long wavelength and hence low energy . whenever an electron gets accelerated many " soft " ( i.e. . , long wavelength ) photons get emitted . but if the acceleration is weak , the photons have such long wavelength that they provide little information concerning which path , and interference is possible . it is the same with gravitons . except the probability of emitting a " hard " graviton ( with short enough wavelength to distinguish the paths ) is far , far smaller than for photons , and therefore gravitational decoherence is extremely weak . these soft photons ( or gravitons ) can be well described using classical electromagnetic ( or gravitional ) theory . this helps one to appreciate how the intuitive picture --- the motion of the electron through the interferometer should perturb the electric field at long range --- is reconciled with the survival of interference . yes , it is true that the electric field is affected by the electron 's ( noninertial ) motion , but the very long wavelength radiation detected far away looks essentially the same for either path followed by the electron ; by detecting this radiation we can distinguish the paths only with very poor resolution , i.e. hardly at all . in practice , loss of visibility in decoherence experiments usually occurs due to more mundane processes that cause " which-way " information to be recorded ( e . g . the electron gets scattering by a stray atom , dust grain , or photon ) . decoherence due to entanglement of the particle with its field ( i.e. . the emission of photons or gravitons that are not very soft ) is always present at some level , but typically it is a small effect .
no . i can take a ball and swing it back and forth periodically with my hand . the motion is periodic , but the situation is not conservative - my body generates a lot of heat . a simple mathematical example is a forced , damped harmonic oscillator . it has a steady-state periodic solution that dissipates energy . if you want to know whether a force field is conservative , take its curl . time-independent force fields ( force is a function of position but not time ) are conservative iff their curl is zero .
the path integral in quantum mechanics computes the evolution kernel , which is the matrix element of the evolution operator : $\mathrm{exp} ( ih t ) $ , ( $h$ is the hamiltonian ) , between two position eigenstates . the path integral expresses the evolution kernel as a sum over paths : $u ( x , t , x_0 ) = \int_{x ( 0 ) =x_0}^{x ( t ) =x} \mathrm{exp} ( \frac{is}{\hbar} ) \mathcal{d}x$ . where $u$ is the evolution kernel , $s$ is the classical action . on the other hand , the evolution operator has an expansion as a sum over the energy eigenstates : $u ( x , t , x_0 ) =\sum_n \mathrm{exp} ( \frac{-ie_n t}{\hbar} ) \psi_n ( x ) \psi_n^{*} ( x_0 ) $ where , $\psi_n ( x ) $ are the energy eigenstates . from this expression , it is clear that the evolution kernel has a discrete spectrum , whenever the energies are quantized . in other words , in the case of quantized values of the energy , the fourier transform of the evolution kernel : $\hat{u} ( x , \omega , x_0 ) \equiv \int_{-\infty}^{\infty} u ( x , t , x_0 ) \mathrm{exp} ( i\omega t ) dt$ will be a sum of dirac delta functions centered at the frequencies : $\omega_n = \frac{e_n}{\hbar}$ please observe that the weight of each dirac delta function is just the projection operator on the corresponding discrete eigenstate .
the main idea behind polaroid sunglasses is that reflexion from water , snow and other glary reflectors is mainly polarized in one direction . to understand this , witness the behaviour foretold by the fresnel equations ( the graph below taken from the wikipedia " fresnel equations " page ) : so that you can see for a wide range of scattering angles from these surfaces , the reflected light reaching your eyes is mainly in the $s$-polarized direction ( electric field vector orthogonal ( "senkrecht " in german ) with the plane of polarization ) , so if you quell this polarization , you get rid of most of the glare from these surfaces . why are your lenses twenty degrees off in their polarization axes ? i would say that this is a simple question of production economics . the power through a polaroid varies like $ ( \sin \theta ) ^2$ , where $\theta$ is the angle between the actual polarizing axes and their ideal directions for quelling a given linear polarization . this functional dependence is very flat for a wide angle range around the null , so , if there is a twenty degree error , the attenuation ratio is still 0.1 . so a polarizer that is twenty degrees off is still almost as good as an ideally aligned one for the lower-the-glare-in-human-sight application . therefore , a manufacturer simply will not go to the extra cost of the quality control needed to align the polarisers more accurately : it really would not make the product any better for the application at hand .
in quantum field theory and its extensions including string theory , the electric charge is a generator of a $u ( 1 ) $ symmetry which should be promoted to a local symmetry i.e. gauge symmetry . in string theory , the $u ( 1 ) $ symmetry and the gauge field often appear as parts of the low-energy effective action . this could be enough to answer the question : we reduce the problem to the same problem in the approximate theory - quantum field theory . except that we do not have to end at this point . string theory produces many geometric pictures how to " imagine " or " visualize " the electric charge . those " visualizations " are often dual to each other : it means that even though these ways to present the charges superficially look totally different , one may actually demonstrate that their physical implications are totally equivalent and indistinguishable . kaluza-klein theory the oldest picture embedded in string theory goes back to 1919 and a discovery by theodor kaluza , later refined by brilliant physicist oskar klein . five-dimensional general relativity , with the new dimension compactified on a circle , produces $u ( 1 ) $ electromagnetism aside from the four-dimensional general relativity . the mixed components of the metric , $g_{\mu 5}$ , may be interpreted as the gauge field $a_\mu$ in the large dimensions . the isometry rotating the circle ( compact fifth dimension ) at each point is interpreted as the $u ( 1 ) $ gauge symmetry . and charged particles are particles that carry a momentum in the new , fifth direction . by quantum mechanics , the momentum has to be quantized ( for the wave function to be single-valued ) , $p=q/r$ , where $r$ is the radius of the circle ( $2\pi r$ is the circumference ) and $q$ is an integer that may be identified with the electric charge . a particle with the opposite charge is simply a particle that moves in the opposite direction along the hidden circular dimension . this works not only for strings but even for point-like particles in higher-dimensional spacetimes . windings string theory offers a special , more intrinsically stringy origin of the charges , too . closed strings may wrap around a non-contractible loop in spacetime - such as the circle from the kaluza-klein theory . they obey boundary conditions on the string : $$ x^5 ( \sigma+\pi ) = x^5 ( \sigma ) +2 \pi r w . $$ those $w$ times wound strings would not exist in a theory without strings . the winding number $w$ - how many times the string is wrapped around the circle - is interpreted as another type of charge . $b_{\mu 5}$ , a component of an antisymmetric tensor field , is interpreted as a new gauge field $a_\mu$ for this $u ( 1 ) $ symmetry . the oppositely charged particles are strings wrapped in the opposite direction ; to be distinct , the closed strings have to be oriented ( carry an arrow ) . this winding number origin of the charge is equivalent to the kaluza-klein origin by an equivalence we call t-duality . the gauge groups in the heterotic string theory combine the kaluza-klein-like charges and the winding-like charges and promote them to large non-abelian groups such as $so ( 32 ) $ or $e_8\times e_8$ . generalizations of wound strings exist for higher-dimensional branes : the total " wrapping number " of some membranes or branes around non-contractible cycles in spacetime ( homology ) are also manifesting themselves as electric charges . many non-perturbative dualities exist . some cycles on which the branes may be wrapped may be shrunk to zero size but they still exist : in those cases , the charged objects are localized in space ( the gauge field only lives on a singularity which may be extended just like a brane ) . that is the case of the ade singularities . in all cases , oppositely oriented branes correspond to oppositely charged particles . note that the orientation may be defined for the " whole world volume " so the reversal of the spatial orientation may be mimicked or compensated by the reversal of the world volume in the temporal dimension . open strings and d-branes when open strings are allowed , they can carry charges ( historically known as " chan-paton factors" ) at the end points - these are the points stuck on the d-branes . so the end points behave as quarks : if the string is oriented and carries an arrow from the " beginning " to the " end " , the beginning may be called a quark and the end may be called an antiquark . in this setup , the charges are most analogous to those of point-like particles . the world line of the quark and antiquark ( going backwards in time ) is nothing else than the boundary of the open world sheet as embedded in the spacetime . even this seemingly point-like origin of charges may be dual - exactly equivalent - the purely stringy ways to produce the charges .
as an engineer , i have had to get pricing on stuff i was not going to buy many times in the past . i doubt you will find a website which will have the prices , but you can find them yourself in a few hours work . what you do is this : call them up and ask for a " sales rep " ( abbreviation for " sales representative " but nobody calls them by the full name ) tell him/her that you are working at a physics lab ( i presume you are ) and that you need " budgetary pricing " . this is pricing that includes no discounts . budgetary pricing is not an offer to sell . so if the price suddenly goes through the roof because of a revolution in some obscure country budgetary pricing will not get them into trouble . they should give this to you without a lot of trouble as it is not secret information . i have always assumed that they do not put it on their websites because ( a ) it changes , and ( b ) it might be construed as being an offer to make a contract . and if you can not get information from their website , ask them to give you a " data sheet " . that should be all you need . hey , i will get you started . here 's a company that sells wafers for the valley : http://www.svmi.com/ here 's their extensive non silicon product line : http://www.svmi.com/non-siliconwafers.aspx here 's their contact info : tel : 408.844.7100 fax : 408.844.9470 email : sales@svmi . com now when you talk to them , do not sound like a complete idiot . make sure you know everything about their product before you pick up the phone . make yourself a temporary expert on their product line . know what size you are interested in and what level of quality , etc . decide in advance what year you are asking for ( new stuff gets cheaper with time ) , and the approximate quantity . if possible , use their website to determine the actual product number for the item you are interested in . by giving them a part number you are making their job very easy . sales people are not , by nature , inclined to annoy potential customers . make it easier for them to give you a price than it would be for them to tell you to go away . if you give them a part number it will be very easy to look up a number for you . do not waste their time by asking for them to speculate about exactly how these prices will change in the future . ( but if they are bored they might have time to talk to you about this . ) they should be able to tell you that they " expect that these prices will drop as production ramps up " , or give you a vague idea , and maybe you will get lucky . but remember that you are asking industry to do you a favor for no real good reason .
this question ( v1 ) asks many questions . let us here make some general remarks , which op hopefully will find useful . noether 's theorem only needs infinitesimal transformations to work . hence the important object is not the set $g$ of finite transformations , but rather the set $\mathfrak{g}$ of infinitesimal transformations . in general , the set $\mathfrak{g}$ does not have to constitute a lie algebra or even a lie algebroid . the " lie bracket " of two infinitesimal transformations might only close on-shell , i.e. modulo euler-lagrange equations . ( this is known as an open algebra . ) a horizontal infinitesimal transformation $\delta x^{\mu}$ changes the spacetime point $x^{\mu}$ , while a vertical infinitesimal transformation $\delta_0 \phi^{\alpha} ( x ) $ changes the fields $\phi^{\alpha} ( x ) $ without moving the spacetime point $x$ . a general infinitesimal transformation is a combination of horizontal and vertical infinitesimal transformations . a vertical infinitesimal transformation is typically of the form $$\tag{1} \delta_0 \phi^{\alpha} ( x ) ~=~\varepsilon^a ( x ) ~y^{\alpha}_a ( \phi ( x ) , \partial\phi ( x ) , x ) + d_{\mu}\varepsilon^a ( x ) ~ y^{\alpha , \mu}_a ( \phi ( x ) , \partial\phi ( x ) , x ) , $$ where $\varepsilon^a ( x ) $ are infinitesimal transformation parameters , which are coordinates of a section $\varepsilon ( x ) $ in a vector bundle $e$ over spacetime . to apply noether 's first theorem for a finite subspace of global$^1$ infinitesimal transformations , one identifies a finite-dimensional subspace of sections $\varepsilon_{ ( 1 ) } ( x ) $ , $\ldots , $ $\varepsilon_{ ( m ) } ( x ) $ , in $e$ . thus the global infinitesimal transformations are of the form $$\tag{2} \varepsilon ( x ) ~=~ \sum_{r=1}^m \omega^{ ( r ) }~\varepsilon_{ ( r ) } ( x ) , $$ where the parameters $\omega^{ ( 1 ) }$ , $\ldots$ , $\omega^{ ( m ) }$ , are $x$-independent . in coordinates , $$\tag{3} \varepsilon^a ( x ) ~=~ \sum_{r=1}^m \omega^{ ( r ) }~\varepsilon^a_{ ( r ) } ( x ) . $$ -- $^1$ a global ( local ) transformation refers in this physics context to an $x$-independent ( $x$-dependent ) transformation , respectively . what are $x$-independent are here really the $\omega^{ ( r ) }$ parameters , not necessarily the basis elements $\varepsilon_{ ( r ) } ( x ) $ . thus the notion of global transformations depends in principle on the choice of section basis $\varepsilon_{ ( 1 ) } ( x ) $ , $\ldots , $ $\varepsilon_{ ( m ) } ( x ) $ . [ local and global transformation in physics should not be confused with the mathematical notion of locally and globally defined objects . all transformations in this answer ( local as well as global ) are assumed to be globally defined on the entire spacetime . locally defined transformations take us to the realm of gerbes . ]
the major problem with ultrasound as a mechanism of purification is that it does not break molecules . heat at least denatures proteins and breaks hydrogen bonds , but ultrasound is of a just smaller order of magnitude of energy at the atomic scale , which can be of the order of the adhesive forces holding the liquid together , but not of stronger molecular bonds . but i think you can do it a different way : use a sound waves intensity gradient to move the biological impurities in the water to a part of the container , by having them walk down an effective potential gradient , like optical tweezers move molecules . this requires only that the density/stiffness of the molecules be different from water , so that the sound energy at a given mode is different inside the molecules than in the water . if it is greater , the molecule will move to the regions of greater intensity . if it is less , it will move towards the regions of smaller intensity . by arranging the sound wave to have an intensity gradient , you can make all the molecules segregate towards/away from the microphone , leaving water in the middle with only ionic or small molecule impurities , which are not affected by the sound . you can flush the sides away , and repeat to make a purer water . this might work for getting rid of prions , which are not disinfected by boiling . this article is the only thing i found that might be relevant , but it is paywalled : http://www.annualreviews.org/doi/pdf/10.1146/annurev.bb.20.060191.001541 i think this might be a very useful idea .
your ear is an effective fourier transformer . an ear contains many small hair cells . the hair cells differ in length , tension , and thickness , and therefore respond to different frequencies . different hair cells are mechanically linked to ion channels in different neurons , so different neurons in the brain get activated depending on the fourier transform of the sound you are hearing . a piano is a fourier analyzer for a similar reason . a prism or diffraction grating would be a fourier analyzer for light . it spreads out light of different frequencies , allowing us to analyze how much of each frequency is present in a given source .
i found a good paper that can help you . however , due to copyright issues i cannot put the spectra here . try to get this article : " the distribution of energy in the visible spectrum of daylight " . a . h . taylor and g . p . kerr . j . opt . soc . am . 31 no . 1 , pp . 3-8 ( 1941 ) . also available here ( pdf ) .
the imaging is not being done by focussing transmitted light as would be done in an optical microscope . instead it is detecting light emitted by the nanoparticles as they fluoresce . this means there is no lower limit to the size of the particl ; e detected , except that when the particles get very small they emit too little light , i.e. they are too faint , for the phone camera to distinguish them from background noise . there still remains a limit to detecting structure in the particles . for large particles you had expect to resolve differences in the fluorescence across the particle , while for small particles they will just appear to be a featureless blob . it is a bit like seeing stars with the naked eye . a star is far too small for your eye to resolve but you can still see the light coming from it . the only limitation is that the star appears as a fuzzy blob rather than a disk .
when you have a matrix $\phi = \begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix}$ , with one column and two rows , and its transpose matrix $\phi^t = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}$ , with one row and two columns , the product of the two matrix $\phi^t \phi$ is a matrix $p$ with one column and one row : $p =\phi^t \phi = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}\begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix} = ( \phi_1^2+\phi_2^2 ) $ because this matrix $p$ has one row and one column , it may considered as a scalar . for your particular problem , you have : $ ( \vec{\nabla}\phi ) ^t . ( \vec{\nabla}\phi ) = \sum\limits_{i=1}^n ( {\partial_i}\phi ) ^t ( {\partial_i}\phi ) = \sum\limits_{i=1}^n ( ( \partial_i \phi_1 ) ^2 + ( \partial_i \phi_2 ) ^2 ) $ the first equality comes from the definition of the inner product and the gradient , and the second equality comes from the definition of the transpose operation $t$ , and the manipulation of these matrices , as seen at the beginning of the answer .
i have since found this pdf from cvi melles griot giving a temperature coefficient of 0.016 nm/°c at 400 nm , increasing to 0.027 nm/°c at 820 nm . this will vary between coating types but it is enough to get started .
i am not a physicist either . as i understand it , heat can be lost by conduction , by convection and by radiation , the purpose of the bottle is to reduce all three . if you half the amount of liquid , the question is whether you also half the loss of heat , or do more or less . analysis is difficult because the weak part of the bottle is the cork . if it is full , there is hot liquid near the cork that looses heat faster , and then gets conduction and convection heat fron the rest . there is also a lesser problem with the bottom , since it is an additional surface where heat can be lost . when the bottle is half full , the liquid is further away from the cork . but the air inside will conduct some of the heat ( conduction , and convection ) to the empty part of the bottle , and radiation may internally add some . if the empty part became as hot as the liquid , the heat loss would be the same as before , for a lesser mass of liquid . hence it would cool faster , if it does not get as hot , it means that some heat is lost to keep it cooler . if the bottle were homogenous ( no cork effect , no bottom effect ) , that would mean that , in addition to its normal heat loss through the side , the remaining liquid has to provide for the heat loss in the empty space above it . hence it cools down faster . the bottom is a disadvantage for the half full bottle , since its loss is the same in both case , and thus contributes comparatively more to cooling when the liquid mass is lower . now , i would need more data and/ or knowledge to analyse the effect of the cork . with a very conducting cork , the full bottle would loose heat quickly ( assuming the liquid touches it ) through convection and conduction in the liquid . with a totally insulating cork , it would at worse balance the effect of the bottom of the bottle , so that the analysis without cork or bottom would be valid . so with a reasonnably good cork , my conclusion is that a half full bottle will cool faster .
the only connection between the " wave on a string " antenna and an e/m antenna is the relative position of the wave and the antenna . the way they detect waves is completely different , since " what is waving " is completely different . for e/m waves , antennas are conductors which rely on potential differences to measure the frequency and amplitude modulation of incoming waves . these potential differences come from the oscillation of the electric field , and can be measured in any sized conductor ; it need not be of a specific length . a typical configuration for an antenna ( by which i mean " dipole antenna" ) is formed by two quarter-length conductors for a half-wavelength total length . the ideal position of the conductor to receive , say , plane waves , is exactly what you would expect for the wave on a string ; you want the antenna to be perpendicular to the direction of motion . of course , there is polarization , relative angle between the electric/magnetic field vector , and receiver properties to consider , but that is the basic picture . i intend this post to be editable ; if anyone is interested and is more knowledgeable about simple antennas , please feel free .
the field inside the sphere will not be zero if it is hollow and there is a point charge in the hollowed out part . the field will be zero in the conductor , because the field is always zero in a conductor in electrostatics . what you might be refering to is that the field will be zero inside the hollow sphere if it is charged , because the charges will distribute symmetrically over the sphere .
the parameters $g^2$ and $n$ are independent of each other so it is meaningless to ask whether $g^2$ goes like $1/n$ for large or small coupling " in general " . in general , it may go but it does not have to go . but if it does go , i.e. if $g^2 n$ is kept fixed , then one may say new interesting things . if one introduces a new symbol $\lambda = g^2 n$ for the ' t hooft coupling , the condition simply says that $\lambda$ is kept fixed – which is natural , especially in the dual stringy interpretation of the same physics . i need to dedicate a special paragraph to an error in your question which is probably not just a typo , due to the complicated work you had to go through to write the fractions . what is kept fixed in ' t hooft 's limit is not $g^2/n$ ; it is $g^2 n$ . it is the product , not the ratio ! so $g^2$ indeed goes like $1/n$ and not $n$ if $n$ is sent to infinity .
yes . a recent publication looks at the wave-particle duality of large particles ( buckminsterfullerene , $\mathrm{c}_{60}$ ( actually more of a many-slit experiment ) ) , however the original diffraction experiments with electrons and such were done in the 1920s-1930s and may be hard to access . the authors talk about facile ways to collapse the interference by observation , but only actually present the diffraction pattern ( or lack thereof ) with and without the grating . the paper is quite detailed . be careful , however . whilst the wave-particle duality of electrons is well-characterised , the movie that dr . quantum is from pushes a lot of untenable silliness .
your definition is quite good and works almost always . i am quite sure it is rigorously true in 2d . you will actually find it in some lecture notes . remember that a theory is conformal if the trace of the stress tensor vanishes : $t \equiv t_\mu^{\mu} = 0 . $ indeed there is a folk theorem that states that $t = \sum \beta_i \mathcal{o}^i$ where the sum runs over those operators $o^i$ in the theory with their beta functions $\beta_i$ ( up to terms generating the conformal anomaly in curved space ) . however , this is not completely true , and there are important classes of counterexamples where additional terms appear . recently , these examples have led to some confusion in the literature ( in the search for scale but not conformally invariant theories ) . all of this is well understood now and a good starting point for your studies would be 1204.5221 [ hep-th ] . edit : do not forget that operator dimensions are not protected and change under the rg flow .
i think you generally have the wrong formula : $t = \frac{1}{2}m\vec{a}\cdot\vec{a}+\frac{1}{2}\vec{\omega}\cdot\vec{\omega}i+m\vec{a}\cdot ( \vec{\omega}\times\vec{r} ) $ , where $\vec{a}$ is the linear velocity of the point you are calculating i around . so for the point of contact of a rotating wheel , you have $\vec{a}=0$ , as the instantaneous linear velocity of the point of contact is zero , $t= \frac{1}{2}m\vec{a}\cdot\vec{a}+\frac{1}{2}\vec{\omega}\cdot\vec{\omega}i+m\vec{a}\cdot ( \vec{\omega}\times\vec{r} ) = 0 + \frac{1}{2}\vec{\omega}\cdot\vec{\omega}i + 0 = \frac{1}{2}\omega^2i$ which should give you the same answer as before .
no , elements of $spin ( n ) $ do not obey the clifford algebra . instead , it is the gamma matrices that obey it . and no , the commutator of the $spin ( n ) $ lie algebra is not the commutators of the elements of the group but elements of the lie algebra . now positively . the spinor representation is the representation on which the generators $j_{ij}$ ( the basis of the lie algebra ) act , $s\mapsto j_{ij}\cdot s$ . the elements of the $spin ( n ) $ group may be obtained by exponentiation : $$ g = \exp ( \sum_{i , j} i\omega_{ij}j_{ij} ) $$ where $j_{ij}$ is the basis of the lie algebra . while in the vector representation , $j_{ij}$ is given by a nearly vanishing $n\times n$ matrix with entries $\pm i$ on the $i , j$ and $j , i$ position , respectively , the matrices $j_{ij}$ have a completely different form in the spinor representation of $spin ( n ) $ . they may be written as $$ j_{ij} = \frac{\gamma_i \gamma_j - \gamma_j \gamma_i}{4} $$ where $\gamma_i$ are gamma matrices that do obey the clifford algebra $$ \gamma_i \gamma_j + \gamma_j \gamma_i = 2\delta_{ij}\cdot {\bf 1}$$ so the matrices obeying this algebra may be combined to bilinear expressions , the antisymmetric tensor $j_{ij}$ with two indices , and these $j_{ij}$ obey the $spin ( n ) $ lie algebra , and as with every lie algebra , the elements of the lie groups may be obtained by exponentiating combinations of the lie algebra matrices . ( equivalently , the lie algebra is the tangent space of the lie group manifold in the vicinity of the unit element of the group . )
i happen to have attended the atw ( aerospace thematic workshop ) twice in 3 years , so i can give you some first insight . i do not know precisely when plasma flow control was first experimented on , however this field gathers a quickly increasing amount of researchers around the world . the reason for that is that , before , people could use continuous dc plasmas ( not practical at atmospheric pressure because of filamentary structure of the discharge , and strong thermal instabilities that always lead to an arc , that is a power-consuming and ineffective discharge ) or at best dbds ( dielectric barrier discharges : you have two electrodes , and between them , an insulator . when you switch the voltage on , a discharge will " creep " on top of the dielectric , and ionize the air , without transiting to arc because current cannot pass ) . but today , you have nanosecond high voltage discharges : high voltage rise time allow for high currents , and nanosecond limitation of the discharge allow for avoiding the transition to arc . this way , you deposit a lot of energy in the flow , and you chose which way the energy will be deposited : thermal energy with very fast heating ( fast gas heating . . . actually the topic of my phd thesis ! ) , vibrational energy , or slow gas heating ( as in arc discharges ) . and with very fast energy deposition into thermal energy , you get shock waves , which even in hypersonic flows , can do something to the high-enthalpy flow , while keeping the energy requirement low enough to be embarked on a flying vehicle . and this is a great step taken forward , that used to be really impossible in the past ! one application of plasma actuation is to mitigate the noise produced by reflected shocks in supersonic aircraft air inlets , and some very recent results show you can indeed tamper with instabilities that generate noise , and damp them . but as you asked for an honest answer , i would say this research still needs at least 5-10 years before anything practical can be proposed for industrial purposes . . . and possibly more . i hope this could help you get more insight into this field !
you are right about your understanding of these terms . this terminology appears in extensions of the randall-sundrum type brane world models . the original model contains a single compact extra dimension bounded by two branes and is known as a hard wall model with the " hard wall " referring to the hard cutoff of space by the ir brane . with such a geometry it is found that the kaluza klein ( kk ) masses of particles that live in the bulk scale as $m_n^2 \sim n^2$ ( like the energy levels of a particle in a box ) . attempts were made to use rs type setups to be dual to qcd in order to calculate meson masses etc . this is known as ads/qcd . however the meson mass spectrum is what is called a regge spectrum i.e. $m_n^2 \sim n$ and so the rs type model needed to be adapted . this paper first introduced the idea of a soft wall to solve this problem . one of the branes in the hard wall model is removed and a dilaton field $\phi$ is introduced which dynamically cuts off the space-time $$s= \int d^5x \ , \sqrt{g}\ , e^{-\phi}\mathcal{l} . $$ the profile of the dilaton in the extra dimension then determines the kk spectrum of bulk fields and for a quadratic dilaton profile ( $\phi ( z ) \sim z^2$ ) a regge spectrum is produced . the removal of one of the branes ( hard spacetime cutoff ) and replacement by a smooth dynamical cutoff coming from the dilaton coined the name " soft wall " . following this idea , people decided to model electroweak physics with such a geometry ( see e.g. here ) . all the standard model fields , including the higgs must now propagate in the bulk . the new setup offered unique phenomenology and is far less constrained by electroweak precision observables and fcncs which cause severe tensions in the original rs . note that since the dilaton field is not normally given a kintic term in such models , it is not a true dynamical field and one may simply consider the effect as being a different form of metric than rs . so essentially the difference between hard wall and soft wall is just a different geometry of the extra dimension which produces different phenomenology .
the mass of the black hole only grows by $ ( 1-\epsilon ) m$ , i.e. the mass that has not been radiated away yet . that is guaranteed by the mass conservation . however , one must be careful about dividing mass and energy to " individual places " in general relativity ; in this case , it can be kind of done , but more detailed questions " where the mass/energy resides " could be meaningless . only the total mass/energy is conserved in general relativity ( in asymptotically flat and similar spaces ) . the local physics of electrons moving in magnetic fields etc . is always the same . the electron mass is always the same constant . to describe what electron is doing in a situation like this , go to a freely falling frame , find out what the values of the electromagnetic fields are in this frame , and use exactly the same electron mass etc . as you would use in the absence of any black hole . if you wanted to use a non-freely-falling frame ( or coordinate system ) to describe the behavior of an electron near the event horizon , you must be very careful to do it right . for example , the gravitational field near the event horizon makes the usual static coordinates extremely deformed relatively to the flat metric – a component of the metric tensor goes to zero or infinity near the event horizon – and there is a nonzero curvature etc . so i am sure that all people who think that general relativity is still essentially the same newtonian mechanics – and in between the lines , you make it likely that you belong to this set – would almost certainly make the calculations incorrectly in a curved system . that is why i am urging you to go to a freely falling frame .
your formula for the generating function is wrong in a crucial sense . the formula you are after reads $$ \frac{1}{|\mathbf{r}-\mathbf{r}'|}=\sum_{l=0}^\infty \frac{r_&lt ; ^l}{r_&gt ; ^{l+1}}p_l ( \cos\theta ) . $$ note that the numerator and denominator of each term are powers of the lesser and greater , resp . , of $r$ and $r'$ . for the multipolar expansion your question asks about , you need the point of evaluation to be further away from the centre than the disk radius $r$ , which means that the powers will be in $r/p$ instead of $p/r$ . that will solve the divergence issues on the integrals . i find the key to quickly seeing which way the expansion will go is seeing it as a taylor series ( for fixed $\theta$ ) in the relevant small paramenter .
i think you are on the right track . there are a couple of bits of advice you may follow : you may simply note that if $a \geq b$ , then it follows that $a = b$ is a valid solution , thus $a$ and $b$ must have the same units . therefore $\delta{p}\delta{x}$ has the same units as $h$ which has the same units as $\delta{e}\delta{t}$ . the method you used is called dimensional analysis and it is perfectly correct to use it .
linacs come in several types , but the kind you are talking about are segmented devices . the device is divided into multiple regions , each developing a strong electric field , but to avoid needed million volt potentials ( as in a van de graff accelerator ) the regions have alternating fields at any given moment . then you arrange for a bunch of charged particles to enter the device at a time when the field has the polarity that will accelerator those particles . however , the particles are moving , so if you just left the field that way the beam would enter the next region ( where the field points the other way ) and lose all the energy the gained in the first region . instead , you swap the field as the particles move between regions . that way they pick up more energy in the second region . then you swap feilds again before they move into the third region and they get another boost and so on . all of this happens very fast , of course , so in modern devices the power is provided at radio frequencies . indeed the super conducting klystrons that are all the rage are resonant cavity devices working in the radio band .
the two formulae are dissimilar so the difference is mathematically evident . if you are asking : why are we calling the nuclear force a strong force , the wiki article has the answer in a nuttshell : , calling it a residual strong force : the residual strong force is thus a minor residuum of the strong force which binds quarks together into protons and neutrons . this same force is much weaker between neutrons and protons , because it is mostly neutralized within them , in the same way that electromagnetic forces between neutral atoms ( van der waals forces ) are much weaker than the electromagnetic forces that hold the atoms internally together . qualitatively : the proton and neutron as bound colorless states of three quarks each , to first order are neutral in the strong force , in the same way that a molecule is neutral to the electromagnetic force . first order , because at higher orders there are moments/distortions of the collective many body potential that extend outside the neutral region and create the wan der waals electromagnetic forces for the molecules and the nuclear force for the nuclei .
theoretically ? sure . practically ? no . the primary problem is not lack of knowledge about how to manipulate individual atoms , though this is very tricky and it might not currently possible to manipulate the right kind of atoms in the right kind of way for this sort of task . the central problem is one of scale . for an item like a milky way bar , you are talking about billions and billions of atoms . your problem would not be that the candy bar was melty , it would be that you would die thousands of years before there was enough of it put together to take a bite . also , the techniques for atom manipulation work reasonably well for placing atoms on some sort of substrate like printing circuits on silicone chips . when you start talking about things like assembling complex sugars atom by atom , the situation gets significantly more difficult .
if anyone is interesed , the answer to my question is $$i\mathcal{m}=\frac{-ie^4\epsilon_{\eta} ( k ) \gamma^{\eta}g_{\eta \delta}\gamma^{\delta}\epsilon_{\delta} ( k' ) g_{\delta \beta}\gamma^{\beta}\epsilon^*_{\beta} ( k'' ) g_{\alpha \beta}e^*_{\alpha} ( k''' ) \gamma^{\alpha}g_{\alpha \eta}}{ ( q^2+i\epsilon ) ^4}$$
have a look at the wikipedia article on tests of general relativity . the most recent measurement quoted there was by a group from the university of texas . you can find a copy of the paper here . they measured a deflection of $1.66$ arcseconds $\pm 10\%$ , compared to the prediction from general relativity of $1.75$ arcseconds . respond to comment : the angular deflection of the light at a distance $r$ from an object of mass $m$ is approximately given by : $$ \theta = \frac{4gm}{c^2r} $$ put in the mass and radius of the sun and you will find $\theta = 8.48 \times 10^{-6}$ radians . convert this to degrees and multiply by $3,600$ to convert to arcseconds and you will recover the figure of $1.75$ arcseconds i quoted above .
you can think of it in this way : to find position of any object we use reflected light from that object . for day-to-day life objects there is no problem . but for subatomic particle it means that we are giving them considerable amount of momentum and energy through photons . thus the very moment we measure their position we are also changing their momentum . thus both cannot be known with absolute certainty at the same time . hope this helps .
an electron volt is just the energy acquired when an electron of charge $e$ falls through a potential of 1 volt , which means $$1ev = e \times 1 = 1.6 \times 10^{-19} j$$ when you lift up your $2.5kg$ laptop ( a 15-inch apple macbook pro , for example ) by a foot , you do a work of approximately $2.5 kg \times 10 ms^{-2} \times 0.3 m = 7.5 j$ which is about $4.7 \times 10^{19} ev$ . so an $ev$ is a really low energy scale by everyday standards . one tev ( a tera electron volt ) is about the energy of motion of a flying mosquito .
in nmr , the strong magnets set the frequency of the nuclear resonance , using the constant magnetic field . typically the resonance radio waves are around the mhz frequencies whereas wi-fi is around 2.5ghz . when the frequencies are different , they do not disrupt each other 's signals . electro-magnets would not interfere , as it is a constant field produced rather than an elecromagnetic wave , and even then it would have to be at the same frequency .
i think what would happen is that any water molecule with enough energy to escape the surface tension would escape . because there is no air to provide the water molecule with a way of turning round and going back in , it would permanently leave . this means that the highest energy molecules would selectively evaporate , lowering the average energy of the remaining water . this process is known as evaporative cooling , where you selectively remove the most energetic molecules ( and yes , it is what happens when you blow on a hot drink ) . depending on the size , shape and method of getting it into the vacuum at that temperature ( in a jar , take the jar away , blasted out a tube with gas ) it may or may not freeze on its way to total evaporation . this is assuming zero pressure in outer space and close enough to zero temperature . if you look at the pressure / temperature phase diagram for water ( wikipedia page of generic phase diagram ) it depends if the rapid drop in external pressure causes the evaporative cooling to drop the internal temperature fast enough to briefly solidify on it is way to pure gas . carbon dioxide will go from solid straight to gas at atmospheric pressure , ( smoke machines ) and , as the pressure is so low , if the water did freeze solid , it would not be for long , as it would continue evaporating in such a strong vacuum . edit : fixed spelling typos .
the links you give seem to be arguing about whether the value of $\pi$ changes in a curved spacetime . it does not . $\pi$ is a constant defined as the ratio of a circle 's circumference to diameter on a euclidean plane . however the ratio of a circle 's circumference to diameter does change in a curved 4d spacetime . it will be greater than $\pi$ if the curvature is negative and less than $\pi$ if the curvature is positive ( as in the example of your sphere ) . as lurscher has commented , if you make the region of spacetime you are looking at very small the ratio will always tend to $\pi$ , again just as in the example of your sphere .
it is simply the diameter of the fiber core . in a single-mode fiber , only the lowest-order mode fits physically into the fiber .
a colleague in astronomy had a student a few years ago who did a calculation about the possibility of primordial black holes , created in the big bang . if the size of these was just right , they could be evaporating into nothing due to hawking radiation right " now " ( scare quotes because this would necessarily include distant black holes that evaporated many years ago , whose light is just reaching us now ) . the last burst of hawking radiation for these would look basically like a faint gamma-ray burst , in which case it ought to be directly detectable . i am not sure of the current status of this-- their preliminary result was , if i remember correctly , that you might be able to test this by measuring the probability distribution for gamma-ray bursts of the appropriate size and duration , but we did not have any telescopes capable of picking them up at the time . i am not sure if that is changed or not . anyway , that would give you a direct way to detect black holes of a certain size , though they would not be around after the detection , so it might not really fit the spirit of the question . . .
the velocity calculation was distance* ( change in angle ) . however , this does not take into account the changing time-delay of light : we see it sped-up because the time delay is decreasing , like a tv recording where you are fast-forwarding as you gradually catch up with real time . fortunately , all we need to do to calculate the real speed is to account for the time delay , no weird relativity is necessary . suppose a far away object is approaching at $0.8c$ and has a transverse velocity of $0.25c$ ( total velocity of $0.84c$ ) . it emits a burst of light ( in our frame ) at time $t$ and $t+5$ seconds . it is $d$ light seconds from us at time $t$ and $d-4$ at $t+5$ . accounting for the time delay of light , we see flashes at time $t+d$ and $t+5+ ( d-4 ) = t+d+1$ ; we see them only $1$ second apart . in those $5$ seconds , it moved transversely a distance of $5\times0.25 = 1.25$ light seconds . since it appeared to move $1.25$ light seconds in $1$ second , we see apparent superluminal motion .
the problem is you have the wrong relations between $\{n_\mathrm{h} , n_\mathrm{he}\}$ and $\{n_\mathrm{p} , n_\mathrm{n}\}$ . every hydrogen contains 1 proton , and every helium contains 2 , so $n_\mathrm{p} = n_\mathrm{h} + 2 n_\mathrm{he}$ . the neutrons are only contributed to by helium in the accounting : $n_\mathrm{n} = 2 n_\mathrm{he}$ . inverting these relations yields \begin{align} n_\mathrm{h} and = n_\mathrm{p} - n_\mathrm{n} \\ n_\mathrm{he} and = \frac{1}{2} n_\mathrm{n} . \end{align} it looks like you got the direction wrong , in the sense that there should be fewer helium nuclei than protons or neutrons ( the 2 's are on the wrong side ) . also , " hydrogen " means ${}^1\mathrm{h}$ not ${}^2\mathrm{d}$ unless otherwise stated .
regarding compactification -- perhaps you are looking for the notion of the " unit tangent bundle " , which would be compact provided the manifold is compact . in general , the tangent bundle is a manifold of dimension twice the original and naturally projects onto the original manifold ; in other words , if the original manifold is not compact , even the unit tangent bundle is not compact . you can read more about the unit tangent bundle here . regarding changes of coordinates : to be completely frank , i understood neither the motivation nor the technical details . nevertheless , when dealing with like questions , one has to be careful to not mix the categories that one is working in . from the point of view of bare set theory , all the spaces you described , and in addition $\mathbb{c}^2$ , are equivalent ( in the sense that there exists a one-to-one and onto map from any one of the spaces in question to any other ) . however , in addition to the set structure , our spaces have a much richer structure : a geometric one . thus , as soon as we pass to the smooth category ( think of it , very roughly speaking , as a context of smooth manifolds and smooth maps defined on them ) , then questions arise as to whether changes of coordinates preserve the structure we are interested in . to make matters more complicated , the spaces $\mathbb{r}^k$ , $\mathbb{c}^k$ and the projective spaces all carry an algebraic structure . in short , there is no reason to change the coordinates from , say , $\mathbb{r}^2$ to $\mathbb{c}^1$ , unless one wants to benefit from the additional structures carried by $\mathbb{c}^1$ . in this case , then , one has to make sure that the change of coordinates is canonical ( in the sense that whatever is proven in the new coordinates can be carried back to the old coordinates ) . thus the transformation that manifests this change of coordinates must possess certain structural integrity ( loose language alert ! ! ) . this makes the question difficult , especially when it is not clearl exactly what one is looking for . here is a simple example : let us view $\mathbb{r}^2$ as $\mathbb{c}^1$ via the transformation $\phi : \mathbb{r}^2\rightarrow \mathbb{c}^1$ with $\phi ( a , b ) = a + bi$ . no tricks here . moreover , this transformation is continuous with a continuous inverse , so a homeomorphism . thus all the topological properties are preserved ( i.e. . whatever one can prove about $\mathbb{c}^1$ and the natural topology thereof , as well as continuous maps defined on them , one can formulate analogous results for $\mathbb{r}^2$ ) . notice also that both $\mathbb{r}^2$ and $\mathbb{c}^1$ are manifolds , but here one has to be careful : a manifold is always modeled on a " model manifold " ( for example , a smooth surface embedded in $\mathbb{r}^3$ is modeled on $\mathbb{r}^2$ in the sense that it is locally , around any point , essentially a copy of $\mathbb{r}^2$ . now , when we say that both $\mathbb{r}^2$ and $\mathbb{c}^1$ are manifolds , what do we mean by that ? here one has to make a choice . we can either model both on $\mathbb{r}^2$ , or model $\mathbb{r}^2$ on $\mathbb{r}^2$ , and model $\mathbb{c}^1$ on $\mathbb{c}^1$ . in the former case they are equivalent via the change of coordinates $\phi$ ; in the latter case , they are not , since there are functions , say $f$ , on $\mathbb{r}^2$ which are infinitely differentiable and constant on some open set , while the corresponding " push forward " of this function under $\phi$ , namely $f\circ \phi$ , is not differentiable in $\mathbb{c}^1$ . that is , in the analytic category , $\mathbb{r}^2$ and $\mathbb{c}^1$ are not equivalent ( analytic category is stronger than continuous ; also , being differentiable in $\mathbb{c}^1$ is equivalent to being infinitely differentiable , is equivalent to being analytic ) . the latter complication comes from the fact that in addition to a topological structure of $\mathbb{c}^1$ ( which , as i have commented above , is the same as that of $\mathbb{r}^2$ ) , $\mathbb{c}^1$ carries an algebraic structure ( namely , a field ) which is compatible with its topological ( even smooth ) structure in the sense that the field operations of multiplication and addition are smooth maps ( in fact analytic ) . $\mathbb{r}^2$ , on the other hand , as it is usually defined , does not enjoy these properties . you could argue , of course , that we can " redefine " $\mathbb{r}^2$ to reflect the properties of $\mathbb{c}^1$ . sure we can do that , but then $\mathbb{r}^2$ becomes $\mathbb{c}^1$ that goes by the name of $\mathbb{r}^2$ . when we write $\mathbb{r}^2$ , we specifically mean the two-dimensional vector space with the underlying field being $\mathbb{r}$ . the vector space operations in this case are incompatible with the field operations of $\mathbb{c}^1$ . long story short : whenever a change of coordinates is introduced , it is usually for the sake of transforming a representation into a simpler form , to simplify computations or to relate to a known problem/solution in the other coordinates . you know you are doing something that does not " feel right " when you are changing coordinates for the sake of benefiting from some additional ( or principally different ) mathematical structure of the new coordinates , because whatever you manage to prove in the new coordinates that requires this additional ( or different ) structure , you probably will not be able to pull back to the original coordinates ( there are some exceptions to this rule , of course ! ) . thus , it seems to me , that what you are doing is either ( a ) ultimately incorrect in the sense of making a category error , or ( b ) simply " renaming " your spaces without losing or gaining anything .
this problem has a unique solution if you only allow charges of one sign ; it is known as the moment problem and is one of the central problems of measure theory . the wikipedia article on it should provide a good starting point for reading on it . however , as luboš points out , for a signed measure the moment problem is usually indeterminate . one way to phrase this is that there is a big set of charge distributions for which all moments vanish . ( this includes , for instance , all bounded charge distributions with a conducting shell around them . ) i do not know of results for finding solutions in the indeterminate case but phrasing the problem in these terms might help .
among the base units of the international system , the kilogram is the only one whose name and symbol , for historical reasons , include a prefix . names and symbols for decimal multiples and submultiples of the unit of mass are formed by attaching prefix names to the unit name " gram " , and prefix symbols to the unit symbol " g " ( cipm 1967 , recommendation 2 ) . bipm the reason why " kilogram " is the name of a base unit of the si is an artefact of history . louis xvi charged a group of savants to develop a new system of measurement . their work laid the foundation for the " decimal metric system " , which has evolved into the modern si . the original idea of the king 's commission ( which included such notables as lavoisier ) was to create a unit of mass that would be known as the " grave " . by definition it would be the mass of a litre of water at the ice point ( i.e. . essentially 1 kg ) . the definition was to be embodied in an artefact mass standard . after the revolution , the new republican government took over the idea of the metric system but made some significant changes . for example , since many mass measurements of the time concerned masses much smaller than the kilogram , they decided that the unit of mass should be the " gramme " . however , since a one-gramme standard would have been difficult to use as well as to establish , they also decided that the new definition should be embodied in a one-kilogramme artefact . this artefact became known as the " kilogram of the archives " . by 1875 the unit of mass had been redefined as the " kilogram " , embodied by a new artefact whose mass was essentially the same as the kilogram of the archives . the decision of the republican government may have been politically motivated ; after all , these were the same people who condemned lavoisier to the guillotine . in any case , we are now stuck with the infelicity of a base unit whose name has a " prefix " . bipm the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
think about it physically . how much gas is in the tube ? there are three sources : the gas coming in at $x=0$ which was already in solution . the gas being carried away by the fluid at the other end , $x=l$ diffusion across the walls of the tube . these three terms are represented in the equation $$ \frac{d}{dt} \left ( a \int_{0}^{l} u ( x , t ) dt \right ) = v ( 0 ) au ( 0 , t ) - v ( l ) au ( l , t ) + p \int_{0}^{l} q ( x , t ) dx $$ the first term is the velocity of the liquid , $v$ , times the cross section , $a$ , giving the total volume of liquid coming in at $x=0$ . we multiply this by the concentration of gas in the liquid at $x=0$ to get the velocity that gas enters at $x=0$ . the second term goes in much the same way , but we have a negative sign because it is the gas which is leaving at the other end . you say you understand that the third term as diffusion through the walls , which is correct .
the phenomenon you describe is ferromagnetism not paramagnetism . ferromagnetic materials like iron behave as if they contain many tiny bar magnets ( called magnetic domains if you are interested to pursue this further ) , but because the magnet domains are aligned randomly the fields cancel out and there is no net magnetic field . however if you put a ferromagnetic material in a magnetic field the external field will cause partial alignment of the magnetic domains . this induces a magnetic field in the originally unmagnetised iron , and that is why your paper clip sticks to the ball . however if you remove the external magnetic field the domains will go back to their original alignment , the net magnetic field will go back to zero and the paper clip will fall off again . if you apply a very strong field and/or combine it with heating and cooling you can permanently change the alignment of the magnetic domains so they remain aligned when the external field is removed . this is how you make permanent magnets .
per kittel 's " elementary statistical physics": $$dq=du_a+mdh$$ . where $du_a$ does not include the magnetic field energy as part of the system . an alternative , equivalent , formulation is $$dq=du_b-hdm$$ where $du_b$ does include the field energy : $u_a=u_b-\mathbf{h \cdot m}$ . with $dq=tds$ , i think you are there . . .
i will try to give a simple answer without going too deep into physics details . localization of light is the confinement of a light wave so that it is very intense in one particular location . usually it vanishes rapidly as you get farther away from that location . ( note that by " light wave " i am not talking about the traveling plane waves that you would use to describe a light beam ; this is more of a stationary oscillation mode . ) this crops up in the context of plasmonic nanoparticles because surface plasmons ' wavelengths are smaller than normal light waves of the same frequency , allowing surface plasmons to be confined into even tinier spaces . there are two major advantages that lead to applications . one is that confining the light into a small space allows easy manipulation of it , for example by tailoring the shape of your nanoparticle . the other is that confining the light into a small space also squeezes all the energy it carries into that small space . this is called field enhancement . you can then do processes that require a lot of electric field strength ( e . g . nonlinear things ) without needing a huge amount of light to achieve that field strength . ( i adapted some of this answer from the introduction to my doctoral dissertation . )
the event horizon is a lightlike surface , and so its area is coordinate-invariant . for a schwarzschild black hole , $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2 + \left ( 1-\frac{2m}{r}\right ) ^{-1}dr^2 + r^2 ( d\theta^2+\sin^2\theta\ , d\phi^2 ) $$ the horizon suface is at $r = 2m$ of schwarzschild radial coordinate , and so at any particular schwarzschild time ( $dt = 0$ ) has the metric $$ds^2 = ( 2m ) ^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) , $$ which is just the metric on a standard 2-sphere of radius $2m$ . you can find the square area element explicitly as the determinant of the metric , $da^2 = ( 2m ) ^4\sin^2\theta\ , d\theta^2d\phi^2$ , and integrating : $$a = 16\pi m^2 = \frac{16\pi g^2}{c^4}m^2 . $$ the volume of the black hole is not invariant , as jerry schirmer says . if you try to apply anything the above schwarzschild coordinates above , then since the coefficients of $dt^2$ and $dr^2$ switch signs across the horizon , $t$ is spacelike and $r$ is timelike . therefore , since the black hole is eternal , it could be said to have infinite volume ( classically , but a real astrophysical black hole would have a finite but still extraordinarily high lifetime ) , as you will be integrating $dt$ across its lifetime . technically , the above argument is a bit flawed , because the schwarzschild coordinate chart is not defined across the event horizon , so one should be more careful how they are continued across the horizon ( e . g . , with kruskal-szekeres coordinates ) . but this can be made more rigorous . in another coordinate chart , e.g. , the gullstrand-painlevé coordinates adapted to a family of freely falling observers , $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2-2\sqrt{\frac{2m}{r}}\ , dt\ , dr + \underbrace{dr^2 + r^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) }_{\text{euclidean in spherical coord . }} , $$ at any instant of time ( $dt = 0$ ) , space is precisely euclidean ; since the horizon is still $r = 2m$ in these coordinates , " the " volume is $$v_{\text{gp}} = \frac{4}{3}\pi ( 2m ) ^3 . $$ if you pick yet another coordinate chart , you may get a yet different answer .
yes , that is ok ! you have stumbled upon one of the basic strange phenomena of relativity .
the kinetic theory of gases does in fact say that molecules in a gas move very rapidly ( although some move quite slow , and others even faster ) . however , there is another crucial component to the theory . the idea of mean free path . here , a molecule is moving very fast but does not get very far before hitting another molecule . this is why things like odors travel relatively slow and why gases do not " mix instantaneously . " a given molecule may only move 1e-6 meters before bouncing off of another molecule , and the direction it bounces is random . this slows down the progress of mixing . additionally , in your example of opening the rear window of a truck , it is important to look at the macroscopic effects of the flow , which kinetic theory does not really explain . the truck cabin generates a wake with a recirculation zone behind it . this means that the air immediately behind the window is relatively stagnant , which is also why the air in the cabin does not leave as fast as it could .
the outcomes are not supposed to be the same . there are two ways to interpret your question : 1 . you want to calculate the kinetic energy in different reference frames . let 's think for example of a point-like body moving in a constant velocity $\mathbf{v}$ . it is kinetic energy is $\frac{1}{2}mv^2$ , but if we calculate it in a reference frame that is moving with the body , in that frame the body is at rest and we get zero . so we do not expect the same outcome when calculating kinetic energy in different reference frames . 2 . you want to stay in the laboratory reference frame , but pick different points as the axis of rotation for your calculations . here there is a subtle point we need to be aware of . it is true that some calculations involving the rotation of rigid bodies can be done in several different ways , each time picking a different axis , and still resulting in the correct outcome . for example this works if we want to calculate the linear and angular acceleration of a body when a given force is applied to it . however , if a rigid body has linear movement and rotation simultaneously , and we want to calculate it is kinetic energy , we need to be careful when using the formula : $$ e_k = \frac{1}{2}m v^2 + \frac{1}{2} i \omega^2 , $$ where $\mathbf{v}$ is the velocity of the axis point and $\omega$ is the angular velocity of rotation around that point . this is the formula you used in your calculations , but in fact it is valid only in the following cases ( and i give a proof of this below ) : when we use the center of mass as the axis of rotation . when the axis of rotation is at rest ( i.e. . $\mathbf{v}=0$ ) . when the velocity is parallel to the line connecting the axis point to the center of mass . regarding the calculations you showed in your question : when you used the fixed point of the cylinder as the axis case 2 applied . when you used the center of mass case 1 applied . when you used the moving extreme none of the cases applied , and you cannot use the above formula in this case . proof : we model the rigid body as a collection of point-like masses $m_i$ with their positions relative to the axis of rotation denoted as $\mathbf{r}_i$ . the velocity of mass $i$ is : $$\mathbf{v}_i = \mathbf{v} + \boldsymbol\omega \times\mathbf{r}_i . $$ the total kinetic energy is then : $$e_k = \sum_i \frac{1}{2} m_i v_i^2 = \frac{1}{2} mv^2 + \frac{1}{2} i \omega^2 + m \mathbf{v} \cdot ( \boldsymbol\omega \times \mathbf{r}_{cm} ) , $$ where $m=\sum_i m_i$ is the total mass , $i=\sum_i m_i |\hat{\boldsymbol\omega} \times \mathbf{r}_i|^2$ is the moment of inertia and $\mathbf{r}_{cm} = ( \sum_i m_i \mathbf{r}_i ) /m$ is the center of mass relative to the axis of rotation . we see that we need the last term to vanish in order to get the formula we want to prove , and we can get this if $\mathbf{r}_{cm}=0$ ( case 1 ) , $\mathbf{v}=0$ ( case 2 ) or $\mathbf{v} \cdot ( \boldsymbol\omega \times \mathbf{r}_{cm} ) =0$ ( case 3 ) .
the thermal radiation associated with some object is typically described in terms of the " black-body " spectrum for a given temperature , given by the planck formula . this formula is based on an idealization of an object that absorbs all frequencies of radiation equally , but it works fairly well provided that the object whose thermal spectrum you are interested in studying does not have any transitions with resonant frequencies in the range of interest . as the typical energy scale of atomic and molecular transitions is somewhere around an ev , while the characteristic energy scale for " room temperature " is in the neighborhood of 1/40 ev , this generally is not all that bad an assumption-- if you look in the vicinity of the peak of the blackbody spectrum for an object at room temperature , you generally find that the spectrum looks very much like a black-body spectrum . how does this arise from the interaction between light of whatever frequency and a gas of atoms or molecules having discrete internal states ? the thing to remember is that internal states of atoms and molecules are not the only degree of freedom available to the systems-- there is also the center-of-mass motion of the atoms themselves , or the collective motion of groups of atoms . the central idea involved with thermal radiation is that if you take a gas of atoms and confine it to a region of space containing some radiation field with some characteristic temperature , the atoms and the radiation will eventually come to some equilibrium in which the kinetic energy distribution of the atoms and the frequency spectrum of the radiation will have the same characteristic temperature . ( the internal state distribution of the atoms will also have the same temperature , but if you are talking about room-temperature systems , there is too little thermal energy to make much difference in the thermal state distribution , so we will ignore that . ) this will come about through interactions between the atoms and the light , and most of these interactions will be non-resonant in nature . in terms of microscopic quantum processes , you would think of these as being raman scattering events , where some of the photon energy goes into changing the motional state of the atom-- if you have cold atoms and hot photons , you will get more scattering events that increase the atom 's kinetic energy than ones that decrease it , so the average atomic ke will increase , and the average photon energy will decrease . ( or , in more fully quantum terms , the population of atoms will be moved up to higher-energy quantum states within the box , while the population of higher-energy photon modes will decrease . ) for thermal radiation in the room temperature regime , of course , the transitions in question are so far off-resonance that a raman scattering for any individual atom with any particular photon will be phenomenally unlikely . atoms are plentiful , though , and photons are even cheaper , so the total number of interactions for the sample as a whole can be quite large , and can bring both the atomic gas and the thermal radiation bath to equilibrium in time . i have never seen a full qft treatment of the subject , but that does not mean much . the basic idea of the equilibration of atoms with thermal radiation comes from einstein in 1917 , and there was a really good physics today article ( pdf ) by dan kleppner a few years back , talking about just how much is in those papers .
i will restrict myself to trace-preserving cp-maps . one can rewrite $\mathcal o=\sum_{k , l}o_k|k , l\rangle\langle k , l|$ , where the $o_k$ are in decreasing order . the non-increasing condition $\langle\mathcal o\rangle$ corresponds then to an non-increasing condition on $k$ . writing $\gamma$ in terms of kraus operators , one has $\gamma ( \rho ) =\sum_i b_i\rho b_i^*$ with $\sum_ib_ib_i^*=\mathbb1$ . the condition on $k$ given above is then translated into the following writing of the kraus operators : $$b_i=\sum_{\substack{k , l , k&#39 ; , l&#39 ; \\k\le k&#39 ; }}b_i^{klk&#39 ; l&#39 ; }|k , l\rangle\langle k&#39 ; , l&#39 ; | . $$ another way to say the same thing is the condition $b_i^{klk&#39 ; l&#39 ; }=0$ if $k&gt ; k&#39 ; $ . then , of course , the normalization condition imposes $$ \sum_{\substack{i , k&#39 ; , l&#39 ; //k\le k&#39 ; }}\left|b_i^{klk&#39 ; l&#39 ; }\right|^2=1 , \forall k , l . $$ if you apply the same reasoning with a non-increasing and non-decreasing condition , you find that $k$ has to be conserved , and this is then equivalent to the commutativity condition you give in your question . in the same way , this answer is not general : you have operations which preserve $\langle\mathcal o\rangle$ without commuting with $\mathcal o$ .
spin wave theory simply does not apply for 1d spin system . the starting point of the spin wave theory is a magnetically ordered ground state . but mermin-wagner theorem states that 1d spin system can not order even at zero temperature , due to the strong quantum fluctuation . so 1d heisenberg model does not lead to an antiferromagnetically ordered ground state , and hence the spin wave is not well defined , and the spin fluctuation does not follow the dispersion relation $\omega\sim k$ . it is known [ 1 ] that 1d spin chain is gapped , as conjectured by haldane . [ 1 ] z . -c . gu and x . -g . wen , phys . rev . b 80 , 155131 ( 2009 ) .
it is largely a matter of definition . here are some quotes . from springer reference : definition there is no universally recognized definition of chaotic flows . flows with properties that are neither constant in time nor presenting any regular periodicity are normally referred as chaotic . fluid turbulence is generally found to be chaotic . it is also random , dissipative , and multiple scaled in time and space . it is a complex system of infinite degrees of freedom . a paper full of state diagrams ( but rather heavy on the math ) for transitions to chaotic flow : https://tspace.library.utoronto.ca/bitstream/1807/25484/1/transition%20to%20chaos%20in%20converging-diverging%20channel%20flows.pdf and finally , a nice discussion at quora . com ( registration apparently required ) as piyush grover pointed out , a chaotic flow has to have mixing by definition . the following answer assumes ' chaotic ' to mean ' random ' which is technically incorrect . but i have decided to leave it here anyway , because i think that is what the op meant . however , i still think that you will not observe a k^ ( -5/3 ) spectrum in the case of chaotic advection . i am also adding piyush 's comment as a part of this answer . chaotic but non-turbulent flows can have exponential mixing . there is a whole field of chaotic advection based on this fact . in fact , you can have exponential mixing of mass in stokes flow , which is as far away from turbulence as possible . this is often used to mix fluid efficiently at micro devices ( low re ) , where turbulence is simply not feasible due to energy considerations . a chaotic flow is one in which there seems to be a high irregularity in the behavior of one/all flow variables with time/space . while a turbulent flow certainly exhibits this behavior , there are also other properties that should be present for a flow to be called turbulent , one of which is high levels of mixing , i.e. , mass/momentum/heat transfer . this is a distinct ( and perhaps the most useful ) property of turbulent flows which is frequently exploited . when you try to mix the sugar in a cup of coffee by stirring it , you are essentially making use of this property . this effect can be clearly seen by looking at the velocity profiles of laminar and turbulent flows through a pipe . ( taken from page on flowcontrolnetwork ) the lines show the magnitude of horizontal/streamwise velocities with respect to height along a pipe . you can see that the turbulent flow has a much flatter velocity profile than a laminar flow , i.e. , there are higher velocities close to the wall for a turbulent flow compared to a laminar flow , while there are lower velocities close to the centerline for a turbulent flow compared to a laminar flow . this shows that velocity ( momentum for an incompressible/constant density flow ) is transferred to a greater extent from the fluid elements close to the centerline to the fluid elements close to the walls in case of a turbulent flow . a good example of a flow which is chaotic but is not turbulent is the trail behind an aircraft . though the flow inside the jet trail is highly chaotic , it is not turbulent because it maintains the shape ( diameter ) for very large distances behind the aircraft , which means that there is very low/negligible mixing with the surrounding atmosphere .
$\newcommand{\ket} [ 1 ] {\left|#1\right&gt ; }\newcommand{\bra} [ 1 ] {\left&lt ; #1\right|}$ you actually got the notation slightly wrong and had a typo in $i$ , but you are almost right . the usual convention are $$ \begin{align} \ket0 and =\begin{bmatrix}1 \\ 0\end{bmatrix} and \ket1 and =\begin{bmatrix}0 \\ 1\end{bmatrix}\\ \text{if }\ket\psi and =\begin{bmatrix}\alpha \\ \beta\end{bmatrix} and \text{then }\bra\psi and =\begin{bmatrix}\alpha^* and \beta^*\end{bmatrix} \end{align} $$ so we have $$ \begin{align} i and =\ket0\bra0+\ket1\bra1\\ \sigma_x and =\ket1\bra0+\ket0\bra1\\ \sigma_y and =i\ket1\bra0-i\ket0\bra1\\ \sigma_z and =\ket0\bra0-\ket1\bra1 \end{align} $$
indeed , without assuming it from first principles as in bogoliubov formulation , the invariance property of $s$ operator you mention holds when the interaction lagrangian does not include derivatives of fields like in qed . that is a consequence of dyson 's expansion in interaction picture . when , as said above , the interaction lagrangian does not include derivatives of fields , one has : $${\cal h}_i = -{\cal l}_i$$ so that $$s = \sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) \:d^4x_1\cdots d^4x_n\: . $$ it is worth noticing that $\hat{\cal l}_i ( x ) $ includes only free field operators as we are dealing with the so-called interaction picture , so everything is explicitely known , commutation relations of field operators in particular . since the lagrangian functions are scalars , we have : $$u_\lambda\hat{\cal l}_i ( x ) u^\dagger_\lambda = \hat{\cal l}_i ( \lambda^{-1} x ) \qquad ( 1 ) $$ moreover , in view of free fields commutation relations one also has:$$ [ \hat{\cal l}_i ( x ) , \hat{\cal l}_i ( y ) ] =0 \qquad ( 2 ) $$ if $x$ and $y$ are spacelike separated . the fact that $s$ is invariant under the action of orthochronous lorentz group is quite obvious $$u_\lambda su_\lambda^\dagger = \sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int u_\lambda t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] u_\lambda^\dagger \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ u_\lambda \hat{\cal l}_i ( x_1 ) u_\lambda^\dagger\cdots u_\lambda \hat{\cal l}_i ( x_n ) u_\lambda^\dagger ] \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ \hat{\cal l}_i ( \lambda^{-1} x_1 ) \cdots \hat{\cal l}_i ( \lambda^{-1}x_n ) ] \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] \:d^4x_1\cdots d^4x_n$$ due to the lorentz invariance of the measure $d^4x$ . the identity : $$u_\lambda t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] u_\lambda^\dagger= t [ u_\lambda \hat{\cal l}_i ( x_1 ) u_\lambda^\dagger\cdots u_\lambda \hat{\cal l}_i ( x_n ) u_\lambda^\dagger ] $$ is consequence of the definition of $t$-ordinator , ( 1 ) , and ( 2 ) for spacelike separated arguments , considering all cases concerning the time order of $x_1 , \ldots , x_n$ and the fact that $\lambda$ does not change the temporal order for causally related arguments as it belongs to the orthochronous subgroup . for more complicated theories the result is not obvious and it could be false in its elementary formulation based on canonical quantization , excluding the case of gauge theories , where it can be proved separately . regarding weinberg 's statement about lorentz covariance of the $s$ matrix and lorentz invariance of $s$ operator , if i understood well the definition , i think that it works like this . let us start from the full ( interacting ) theory . there are vectors $\psi^\pm_{\{p_i\}}$ describing states which , at late time ( respectively $t\to +\infty$ and $t\to -\infty$ ) evolve like free particle states with momenta $\{p_i\}$ . the correspondingly associated free states , always evolving in accordance witht he free theory , are indicated by $\phi_{\{p_i\}}$ . the $s$-matrix is the matrix of elements : $$\langle \psi^+_{\{q_i\}}|\psi^-_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: . \qquad ( 3 ) $$ in the rhs the $s$ operator takes place . in view of it , the scattering process is completely described in terms of free states . to say that the $s$ matrix is lorentz covariant should mean ( as far as i understand ) : $$\langle \psi^+_{\{\lambda q_i\}}|\psi^-_{\{\lambda p_i\}}\rangle = \langle \psi^+_{\{q_i\}}|\psi^-_{\{p_i\}}\rangle\quad \forall \lambda \in o ( 3,1 ) \uparrow\: , \forall \{\lambda q_i\}\: , \{\lambda p_i\}\: . $$ from ( 3 ) , it immediately entails : $$ \langle \phi_{\{\lambda q_i\}}|s\phi_{\{\lambda p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: . \qquad ( 4 ) $$ if $u_\lambda$ is the unitary representation of $o ( 3,1 ) \uparrow$ on free states , so that $\phi_{\{\lambda p_i\}}= u_\lambda \phi_{\{p_i\}}$ , we therefore have : $$\langle u_\lambda \phi_{\{ q_i\}}|s u_\lambda\phi_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: , $$ that is $$\langle \phi_{\{ q_i\}}|u^\dagger_\lambda s u_\lambda\phi_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: , $$ and so : $$\langle \phi_{\{ q_i\}}| \left ( u^\dagger_\lambda s u_\lambda - s\right ) \phi_{\{p_i\}}\rangle = 0\: , $$ since the set of vectors $\phi_{\{p_i\}}$ forms a basis of the hilbert space ( of the free theory ) in view of the asymptotic completeness hypotheses , we conclude that : $$u^\dagger_\lambda s u_\lambda - s=0$$ i.e. $$s=u_\lambda s u^\dagger_\lambda\: , \quad \forall \lambda \in o ( 3,1 ) \uparrow\: . $$ in other words if the $s$ matrix is lorentz covariant , then the $s$ operator is lorentz invariant .
it is not quite correct to say that n-type materials are doped with " extra " electrons . to be sure , n-type material is charge neutral . it is more correct to say that n-type material is doped with atoms that " donate " an electron to the conduction band ; n-type material has excess mobile electrons versus intrinsic material . so , naturally , n-type material is a good conductor because there are plenty of mobile electrons to participate in an electron current . p-material is doped with atoms that " accept " an electron from the conduction band so there is a deficit of mobile electrons and thus you would not expect there to be much mobile electron current . however , there are plenty of " holes " that can participate in a " hole " current . now , with respect to your question . when an ehp is generated in either type of material , the material remains charge neutral . however , if the ehp is separated by the intrinsic e-field of the pn junction , there is no longer charge neutrality . when an ehp is generated , it is the minority carrier that will be swept across the junction to become a majority carrier on the " other side " . since there is no longer charge neutrality , charge will flow in an external circuit to restore it .
a " quadripole " is a " four-terminal network " , if the currents through pairs of terminals are equal , it becomes a " two-port network " . if you post a schematic containing a quadripole , i could probably tell you if it is a two-port network or not . the equation for a two-port network is : $$ \begin{bmatrix}v_1\\v_2\end{bmatrix}= \begin{bmatrix}z_{11} z_{12}\\z_{21} z_{22}\end{bmatrix}\begin{bmatrix}i_1i_2\end{bmatrix} $$
to say the same thing david zaslavsky said in slightly different words , the second law implies that entropy cannot be destroyed , but it does not prevent you from moving it around from place to place . when we write the equation $\delta s = \int_a^b \frac{dq}{t}$ , we are assuming that this $dq$ represents a flow of heat into or out of the system from somewhere else . therefore $s$ ( which , by convention , represents only the entropy of some particular system ) can either increase or decrease . since we are talking about a reversible process , the entropy of some other system must change by an equal and opposite amount , in order to keep the total constant . that is , $\delta s + \delta s_\text{surroundings} = 0$ . one other thing : in thermodynamics , " closed " and " isolated " mean different things . " isolated " means neither heat nor matter can be exchanged with the environment , whereas " closed " means that matter cannot be exchanged , but heat can . in your question you say the second law " prohibits a decrease in the entropy of a closed system , " but actually this only applies to isolated systems , not closed ones . when we apply the equations above , we are not talking about an isolated system , which is why its entropy is allowed to change . i mention this because you said you are teaching yourself , and in that case it will be important to make sure you do not get confused by subtleties of terminology .
assuming your circuit is as simple as sounds , then if the voltage across the resistor is zero the steady state current through it must be zero . the negative intercept is likely to be due to experimental error , especially since you are extrapolating quite a long way down to zero . you really need to measure more point between $v = 0$ and $v = 2$ to get any further . there are circumstances in which you need a finite voltage before the current increases from zero , for example if your circuit contains a forward biased diode . this would give you a negative $y$ intercept .
as far as i can tell , despite the recent achievements the experimental toolbox in this field is quite limited . the two main techniques are photon-subtraction from squeezed vaquum and generation from fock states by conditional homodyne detection . first technique is based on the fact , that an odd cat state may be expressed as : $$\left ( \left|\alpha\right&gt ; -\left|-\alpha\right&gt ; \right ) \propto \alpha\left|1\right&gt ; +\frac{\alpha^3}{\sqrt{6}}\left|3\right&gt ; +\ldots , $$ which for small $\alpha$ resembles squeezed vacuum state with one photon removed . experimentally photon subtraction is realized with a low reflectivity beam splitter and single photon detection in the reflected port . detecting a single photon in this port heralds the preparation of the desired state . this is only applicable for states with small $\alpha$ - so called " schrödinger kittens " . these kittens may be later " breeded " on a beamsplitter to increase $\alpha$ . second technique uses homodyne detection of $p$ quadrature of a fock state $\left|n\right&gt ; $ splitted on a 50/50 beamsplitter to conditionally prepare cats . a detection of $p\sim0$ heralds the preparation of a cat state with $\alpha=\sqrt{n}$ . the closest to arbitrary superposition preparation is described here . this is not exactly a cat state , but a superposition of a squeezed vacuum and squeezed single-photon states of the form : $$\left|\psi\right&gt ; =\cos\theta \hat{s} ( r ) \left|0\right&gt ; +e^{i\varphi}\sin\theta \hat{s} ( r ) \left|1\right&gt ; , $$ with $\hat{s} ( r ) $ being the squeezing operator . it is probably the most general continious-variable superposition experimentally generated so far . i have to say that i do not specifically keep track of experiments in this field , that is just those which i heard of . as a good reference on experiemtal techniques i can recommend this review by lvovsky and raymer . this paper also contains a lot of references .
every term contains one $\lambda$ in the superscript and one in the subscript , so you sum over those . the only indices which do not appear in both superscript and subscript in the same term are $\mu$ and $\nu$ . example : $$\gamma_{\lambda\sigma}^\lambda\gamma_{\mu\nu}^\sigma = \gamma_{00}^0\gamma_{\mu\nu}^0 + \gamma_{01}^0\gamma_{\mu\nu}^1 + \cdots + \gamma_{10}^1\gamma_{\mu\nu}^0 + \cdots$$
i would say that you have several regimes that are well defined : the behavior of the fluid as it exits in inlet jets and enters the bulk without interference from the cubes . [ length scale set by the exit aperture ? ] flow of the fluid around isolated cubes when far from the edges of the tank ( far being several times the characteristic size of the cube ) . [ length scale set by the side of the cube . ] flow of the fluid toward , along and away from the sides of the tank away from the jets and without interference from the cubes . [ length scale set by the boundary behavior ? ] which is the good news , unfortunately you also have all the cases that mix and match the various length scales : case with cubes interacting with the jet near the aperture case with cubes in motion near the walls case with cubes in close proximity to one another you can probably find existing treatments for all the former cases , but the latter ones are going to be tricky , and you will note that they feature at least two length scales . yuck . this must be part of why they say cfd is hard .
the principle of least ( stationary ) action ( aka hamilton 's principle ) is derived from newton 's axioms plus d'alembert 's principle of virtual displacements . because d'alembert 's principle allows to account for the ( reactions of the ) bonds between the components of a system in a transparent way , the lagrangian and hamiltonian formulations are possible . note1: newton 's axioms , as given , cannot derive neither the lagrangian form nor the hamiltonian as they would need the reactions of the bonds to be added literally inside the formalism , thus resulting in different dimensionality and equations for the same problem where the ( reactions of the ) constraints would appear as extra unknowns . note2: d'alembert 's principle is more general than the lagrangian or hamiltonian formalisms , as it can account also for non-holonomic bonds ( in a slight generalisation ) . update1: when the forces are conservative , meaning derived from a potential $v ( q_i ) $ i.e. $q_i = -\frac{\partial v}{\partial q_i}$ , and the potential is not depending on velocities $\dot{q_j}$ i.e. $\partial v / \partial \dot{q_j} = 0$ ( or the potential $v ( q_i , \dot{q_i} ) $ can depend on velocities in a specific way i.e. $q_i = \frac{d}{dt} \left ( \frac{\partial v}{\partial \dot{q_i}} \right ) - \frac{\partial v}{\partial q_i}$ , refered to as generalised potetial , like in the case of electromagnetism ) , then the equations of motion become : $$\frac{d}{dt} \left ( \frac{\partial t}{\partial \dot{q_i}} \right ) - \frac{\partial t}{\partial q_i}-q_i = \frac{d}{dt} \left ( \frac{\partial l}{\partial \dot{q_i}} \right ) - \frac{\partial l}{\partial q_i}$$ where $l=t-v$ is the lagrangian . ( ref : theoretical mechanics , vol ii , j . hatzidimitriou , in greek ) update2: one can infact formulate d'alembert 's principle as an " action principle " but this " action " is in general very different from the known hamiltonian/lagrangian action . variational principles of classical mechanics variational principles cheat sheet the generalized d ' alembert-lagrange equation 1.2 prehistory of the lagrangian approach generalized lagrange–d’alembert principle for a further generalisation of d'alembert-lagrange-gauss principle to non-linear ( non-ideal ) constraints see the work of udwadia firdaus ( for example new general principle of mechanics and its application to general nonideal nonholonomic systems )
an electric field is said to be static if it does not change with time , i.e. the the charges that produced that field are stationary . this does not imply any constriction on its spatial dependence . in particular , no spherical symmetry is implicit in the definition of electrostatic field , and that field may not depend only on $r$ , as your example shows . this is common when you consider examples of field produced by more than one point charge , e.g. an electric dipole : $$\mathbf{e} ( \mathbf{r} ) ={3\mathbf{p}\cdot\hat{\mathbf{r}}\over 4\pi\varepsilon_0 r^3}\hat{\mathbf{r}}-{\mathbf{p}\over 4\pi\varepsilon_0 r^3}$$ depends on $\mathbf{r}$ and $\mathbf{p}$ .
it is multiplication . when you turn the wheel at the beginning by $x$ degrees , the axle at the end will turn by $x/100$ degrees . this comparison of lengths/angles of movement directly translates to the mechanical advantage . the work is a constant , so because of $w=x_1 f_1=x_2 f_2=x_1/100 f_2=x_1/100\cdot 100 f_1$ , it follows that $f_2=100 f_1$ , which is exactly the mechanical advantage .
i do not have an answer to the question " why would one want to consider such crazy stuff in physics ? " since i do not know much physics , but as a mathematics student i do have an answer to the question " why would one want to consider such crazy stuff in mathematics ? " what physicists call grassmann numbers are what mathematicians call elements of the exterior algebra $\lambda ( v ) $ over a vector space $v$ . the exterior algebra naturally arises as the solution to the following geometric problem . say that $v$ has dimension $n$ and let $v_1 , . . . v_n$ be a basis of it . we would like a nice natural definition of the $n$-dimensional volume of the paralleletope defined by the vectors $\epsilon_1 v_1 + . . . + \epsilon_n v_n , e_i \in \{ 0 , 1 \}$ . when $n = 2$ this is the standard parallelogram defined by two linearly independent vectors , and when $n = 3$ this is the standard paralellepiped defined by three linearly independent vectors . the thing about the naive definition of volume is that it is very close to having really nice mathematical properties : it is almost multilinear . that is , if we denote the volume we are looking at by $\text{vol} ( v_1 , . . . v_n ) $ , then it is almost true that $\text{vol} ( v_1 , . . . v_i + cw , . . . v_n ) = \text{vol} ( v_1 , . . . v_n ) + c \text{vol} ( v_1 , . . . v_{i-1} , w , v_{i+1} , . . . v_n ) $ . you can draw nice diagrams to see this readily . however , it is not actually completely multilinear : depending on how you vary $w$ you will find that sometimes the volume shrinks to zero and then goes back up in a non-smooth way when really it ought to keep getting more negative . ( you can see this even in two dimensions , by varying one of the vectors until it goes past the other . ) to fix that , we need to look instead at oriented volume , which can be negative , but which has the enormous advantage of being completely multilinear and smooth . the other major property it satisfies is that if any of the two vectors $v_i$ agree ( that is , the vectors are linearly dependent ) then the oriented volume is zero , which makes sense . it turns out ( and this is a nice exercise ) that this is equivalent to oriented volume coming from a " product " operation , the exterior product , which is anticommutative . formally , these two conditions define an element of the top exterior power $\lambda^n ( v ) $ defined by the exterior product $v_1 \wedge v_2 . . . \wedge v_n$ , and choosing an element of this top exterior power ( a volume form ) allows us to associate an actual number to an $n$-tuple of vectors which we can call its oriented volume in the more naive sense . if $v$ is equipped with an inner product , then there are two distinguished elements of $\lambda^n ( v ) $ given by a wedge product of an orthonormal basis in some order , and it is natural to pick one of these as a volume form . alright , so what about the rest of the exterior powers $\lambda^p ( v ) $ that make up the exterior algebra ? the point of these is that if $v_1 , . . . v_p , p &lt ; n$ is a tuple of vectors in $v$ , we can consider the subspace they span and talk about the $p$-dimensional oriented volume of the paralleletope given by the $v_i$ in this subspace . but the result of this computation should not just be a number : we need a way to do this that keeps track of what subspace we are in . it turns out that mathematically the most natural way to do this is to keep in mind the requirements we really want out of this computation ( multilinearity and the fact that if the $v_i$ are not linearly independent then the answer should be zero ) , and then just define the result of the computation to be the universal thing that we get by imposing these requirements and nothing else , and this is nothing more than the exterior power $\lambda^p ( v ) $ . this discussion hopefully motivated for you why the exterior algebra is a natural object from the perspective of geometry . since einstein , physicists have been aware that geometry has a lot to say about physics , so hopefully the concept makes a little more sense now . let me also say something about how modern mathematicians think about " space " in the abstract sense . the inspiration for the modern point of view actually derives at least partially from physics : the only thing you can really know about a space are observables defined on it . in classical physics , observables form a commutative ring , so one might say roughly speaking that the study of commutative rings is the study of " classical spaces . " in mathematics this study , in the abstract , is called algebraic geometry . it is a very sophisticated theory that encompasses classical algebraic geometry , arithmetic geometry , and much more , and it is in large part because of the success of this theory and related commutative ring approaches to geometry ( topological spaces , manifolds , measure spaces ) that mathematicians have gotten used to the slogan that " commutative rings are rings of observables on some space . " of course , quantum mechanics tells us that the actual universe around us does not work this way . the observables we care about do not commute , and this is a big issue . so mathematically what is needed is a way to think about noncommutative rings as " quantum spaces " in some sense . this subject is very broad , but roughly it goes by the name of noncommutative geometry . the idea is simple : if we want to take quantum mechanics completely seriously , our spaces should not have " points " at all because points are classical phenomena that implicitly require a commutative ring of observables , which we know is not what we actually have . so our spaces should be more complicated things coming from noncommutative rings in some way . grassmann numbers satisfy one of the most tractable forms of noncommutativity ( actually they are commutative if one alters the definition of " commutative " very slightly , but never mind that . . . ) , and even better it is a form of noncommutativity that is clearly related to something physicists care about ( the properties of fermions ) , so anticommuting observables are a natural step up from commuting observables in order to get our mathematics to align more closely with reality while still being able to think in an approximately classical way .
first and foremost , the bec systems studied in detail today do not involve the formation of any bonds between atoms . bose-einstein condensation is a quantum statistical phenomenon , and would happen even with noninteracting particles ( though as a technical matter , that is impossible to arrange , but you can make a condensate and then manipulate the interactions so they are effectively non-interacting , and the particles remain a condensate ) . the " high school physics " version of what happens at the bec transition is this : particles with integer intrinsic spin angular momentum are " bosons , " and many of them can occupy the same energy state . this is in contrast to particles with half-integer spin , such as electrons , termed " fermions , " which are unable to be in exactly the same quantum state ( this feature of electrons accounts for all of chemistry , so it is a good thing ) . when we talk about a confined gas of atoms , quantum mechanics tells us that we must describe it in terms of discrete energy states , spaced by a characteristic energy depending on the details of the confinement . because of this , the two classes of particles have very different behaviors in large numbers . the lowest-energy state for a gas of fermions is determined by the number of particles in the gas-- each additional particle fills up whatever energy state it ends up in , so the last particle added goes in at a much higher energy than the first particle added . for this reason , the electrons inside a piece of metal have energies comparable to the hot gas in the sun , because there are so many of them that the last electron in ends up moving very rapidly indeed . the lowest-energy state for a gas of bosons , on the other hand , is just the lowest-energy state available to them in whatever system is confining them . all of the bosons in the gas can happily pile into a single quantum state , leaving you with a very low energy . it turns out that , as you cool a gas of bosons , you will eventually reach a point where the gas suddenly " condenses " into a state with nearly all of the particles occupying a single state , generally the lowest-energy available state . this happens with material particles because the wave-like character of the bosons becomes more and more pronounced as you lower the temperature . the wavelength associated with them , which at room temperature is many times smaller than the radius of the electron orbits eventually becomes comparable to the spacing between particles in the gas . when this happens , the waves associated with the different particles start to overlap , and at some point , the system " realizes " that the lowest-energy state would be for all the particles to occupy a single energy level , triggering the abrupt transition to a bec . this transition is a purely quantum effect , though , and has nothing to do with chemical bonding . in fact , strictly speaking , the dilute alkali metal vapors that are the workhorse system for most bec experiments are actually a metastable state-- at the temperatures of these vapors , a denser gas would be a solid . they form a bec , though , because the density of these gases is something like a million times less than the density of air . the atoms are too dilute to solidify , but dense enough to sense each others ' presence and move into the same energy state . the underlying physics is described in detail in most statistical mechanics texts , though it is often dealt with very briefly and in an abstract way . there are decent and readable descriptions of the underlying physics in the new physics for the twenty-first century edited by gordon fraser , particularly the pieces by bill phillips and chris foot , and subir sachdev .
the key output of the flrw metric is the scale factor $a ( t ) $ as a function of time . from this we can calculate the time derivative $\dot{a} ( t ) $ ( which is what the red shift measures ) then check whether or not it satisfies the equation : $$ \left ( \frac{\dot{a}}{a}\right ) ^2 = \frac{8\pi g}{3} ( \rho_{radiation} + \rho_{matter} + etc ) $$ where the etc includes dark energy and anything else you may wish to throw in . so basically the test is to measure the redshift as a function of distance . the problem is that this is extraordinarily hard to do on the scales where the matter distribution is homogeneous . there are various approaches being tried such as baryon acoustic oscillations and properties of galaxy clusters but it is still early days .
i think it might be this $$mgh=\frac{1}{2}mv_{cm}^2+\frac{1}{2}i_{cm}\omega^2=\frac{1}{2}i\omega^2 . $$ i mean , it seems that you added the kinetic energy due to center of mass velocity to the rotational kinetic energy respect to the rotational point . it should have been respect to the center of mass .
it is because when $$v ( x , y , z ) = v_x ( x ) + v_y ( y ) + v_z ( z ) , $$ ( i guess that your extra identity $v ( x , y , z ) =v ( z ) $ is a mistake ) , we also have $$ h = h_x + h_y + h_z$$ because $h = ( \vec p ) ^2 / 2m + v ( x , y , z ) $ and $ ( \vec p ) ^2 = p_x^2+p_y^2+p_z^2$ decomposes to three pieces as well . one may also see that the terms such as $h_x\equiv p_x^2/2m+v_x ( x ) $ commute with each other , $$ [ h_x , h_y ] =0 $$ and similarly for the $xz$ and $yz$ pairs . that is because the commutators are only nonzero if we consider positions and momenta in the same direction ( $x$ , $y$ , or $z$ ) . at the end , we want to look for the eigenstates of the hamiltonian $$ h|\psi\rangle = e |\psi \rangle$$ and because we have $h = h_x+h_y+h_z$ , a hamiltonian composed of three commuting pieces , we may simultaneously diagonalize them i.e. look for the common eigenstates of $h_x , h_y , h_z$ , and therefore also $h$ . so given the separation condition for the potential , we may also assume $$ h_x |\psi\rangle = e_x |\psi\rangle $$ and similarly for the $y , z$ components . however , the equation above is just a 1-dimensional problem that implies that $|\psi\rangle$ must depend on $x$ as a one-dimensional quantum mechanical energy eigenstate wave function , $$ \psi ( x ) = c\cdot \psi_n ( x ) $$ which is an eigenstate of $h_x$ . this has to hold but the normalization factor is undetermined . we usually say that it is a constant but this statement only means that it is independent of $x$ . in reality , it may depend on all observables that are not $x$ such as $y , z$ . so a more accurate implication of the $h_x$ eigenstate equation is $$ \psi ( x , y , z ) = c_x ( y , z ) \cdot \psi_{n_x} ( x ) . $$ in a similar way , we may show that $$ \psi ( x , y , z ) = c_y ( x , z ) \cdot \psi_{n_y} ( y ) $$ and $$ \psi ( x , y , z ) = c_z ( x , y ) \cdot \psi_{n_z} ( z ) $$ and by combining these three formulae , we see that the whole function must factorize to a product of functions of $x$ and $y$ and $z$ separately . if you need a rigorous proof of the last simple step , take e.g. the complex logarithms of the three forms for $\psi$ above and compare e.g. the first pair : $$\ln\psi = \ln c_x ( y , z ) +\ln\psi_{n_x} ( x ) = \ln c_y ( x , z ) +\ln \psi_{n_y} ( y ) $$ take e.g. the partial derivative of the last equation with respect to $y$: $$ \frac{\partial \ln c_x ( y , z ) }{\partial y} = \frac{\partial \ln\psi_{n_y} ( y ) }{ \partial y }$$ the other two ( 1+1 ) terms are zero because they did not depend on $y$ . the right hand side above only depends on $y$ , so the same must be true for the left hand side . i am going to make a simple conclusion but to make it really transparent , let 's differentiate the latter equation over $z$ , too . the $\psi_{n_y}$ term disappears as well so we have $$\frac{\partial^2 \ln c_x ( y , z ) }{\partial y\ , \partial z} = 0$$ it means that $\ln c_x ( y , z ) $ must have the form $k_x ( y ) +l_x ( z ) $ , and $e^{k_x ( y ) }e^{l_x ( z ) }$ must be the remaining factors in the wave function . we say that the wave function in the product form is a " tensor product " of the three independent one-dimensional wave functions and more " operationally " , as another user mentioned , the method described above is the method of " separation of variables " .
i ) well , gaussian integrals $$\tag{1} \int_{\mathbb{r}^n} \ ! d^n x ~e^{-\frac{1}{2} x^t a x} ~=~ \sqrt{\frac{ ( 2\pi ) ^n}{\det a}}$$ are easy to calculate exactly , where the matrix ${\rm re} ( a ) $ is positive definite . ii ) but if op just wants to confirm that the power $p$ of the determinant $\det a$ on the rhs . of eq . ( 1 ) is $p=-1/2$ ( as opposed to some other power $p$ ) , then indeed one may use dimensional analysis . if the integration variables $x^i$ have dimension of length $ [ x^i ] =l$ , then the matrix elements $a_{ij}$ have dimension $ [ a_{ij} ] =l^{-2}$ to keep the argument of the exponential dimensionless . therefore $\det a$ has dimension $ [ \det a ] =l^{-2n}$ . moreover both sides of eq . ( 1 ) must have dimension $l^n$ . hence the power $p=-1/2$ of the determinant $\det ( a ) $ .
if you start with a finite amount of gas in the inner sphere and then deposit a massive amount of energy , the molecules of the gas begin moving rapidly outwards and piling up , creating the blast wave . however , the rate at which the gas is moving outwards may not be balanced by the amount of gas molecules being created by the explosive . if this is the case , then the pressure must decrease below ambient as the molecules are pushed outwards with the blast wave . you can see this in videos of blast waves . the initial wave continues to move outwards , but the smoke/dirt/debris caused by the explosive will move outwards initially , then inwards as the lower pressure region sucks it back in towards the center . there is actually considerably banging that goes on where the low pressure behind the blast wave moves inwards and outwards until it relaxes back to atmospheric pressure . here is a great video that shows the blast and resulting banging as the pressure relaxes .
let $y_1 , y_2$ $2$ complex vectors and let $&lt ; , &gt ; $ be a complex inner product defined by $&lt ; y_1 , y_2&gt ; = \vec y_1^* . \vec y_2$ . let $\vec a$ and $\vec b$ the real and imaginary part of $\vec y$ : $\vec y = \vec a + i \vec b$ then : $$&lt ; y_1 , y_2&gt ; = ( \vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 ) + i ( \vec a_1 . \vec b_2 - \vec b_1 . \vec a_2 ) = u ( y_1 , y_2 ) + iv ( y_1 , y_2 ) $$ the cauchy-schwartz inequality gives : $$&lt ; y_1 , y_1&gt ; &lt ; y_2 , y_2&gt ; ~~\ge ~~|&lt ; y_1 , y_2&gt ; |^2$$ we note that : $&lt ; y_1 , y_1&gt ; = u ( y_1 , y_1 ) $ , so we have : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{u^2 ( y_1 , y_2 ) + v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }$$ now , fixing a particular $y_1$ , we limit the set of $y_2$ to those which respect $u ( y_1 , y_2 ) =0$ . so , we have now : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ now , take explicitely $y_2$ defined by $ \vec a_2 = - \vec b_1 , \vec b_2 =\vec a_1 , $ , we see that $\vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 = 0$ , that is $u ( y_1 , y_2 ) = 0$ , so this choice is coherent with our previous hyphothesis . morevoer , we have $v ( y_1 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , and $u ( y_2 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , so we have , for this particular $y_2$ . $$u ( y_1 , y_1 ) ~~= ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ so , we see , that the inequality $ ( 1 ) $ is effectively saturated by our choice of this particular $y_2$
the half-life of uranium 238 is about the age of the earth , so only about half of the original supply should have decayed by now . also , there are some radioactive nuclei that get created by interactions with cosmic rays in the upper atmosphere ( carbon-14 ) or decay from more stable nuclei ( all of the daughter nuclei between u-238 and lead , for example ) .
pranav 's comment neatly identifies the problem . the temperature of the wire and the fluid are not the same , and you do not know the temperature of the wire . you need to measure $v$ and $i$ , then calculate $r$ and use the second equation to calculate the temperature of the wire . assuming you have a thermometer in your liquid you can now calculate the $\delta t$ to use in your first equation .
here is the proof that i think you are looking for . as ali remarks in his answer , the results holds true for a rigid body undergoing rotation with constant angular velocity . let $\vec r_i$ denote the position of some particle in a rigid body . suppose this rigid body is undergoing rotation with angular velocity $\vec \omega$ , then $$ \dot {\vec r}_i = \vec \omega\times\vec r_i $$ see the appendix for a proof of this . by taking the derivative of both sides with respect to time and multiplying both sides by $m_i$ , the mass of particle $i$ , we obtain $$ \dot {\vec p}_i = \omega\times \vec p_i $$ now we simply note that if $\vec f_i$ denotes the net force on particle $i$ , then newton 's second law gives $\vec f_i = \dot{\vec p_i}$ so that \begin{align} \vec\tau_i and = \vec r_i\times \vec f_i \\ and = \vec r_i\times\dot{\vec p_i} \\ and = \vec r_i\times ( \vec\omega\times\vec p_i ) \\ and = -\vec p_i\times ( \vec r_i\times \vec\omega ) - \vec\omega\times ( \vec p_i\times\vec r_i ) \\ and = \vec p_i\times ( \vec\omega\times\vec r_i ) + \vec\omega\times ( \vec r_i\times\vec p_i ) \\ and = \vec p_i\times \dot{\vec r}_i + \vec \omega\times \vec l_i \\ and = \vec\omega\times \vec l_i \end{align} this is basically the identity you were looking for . in the fourth equality , i used the so-called jacobi identity . now , by taking the sum over $i$ , the result can readily be seen to also hold for the net torque $\tau$ on the body and the total angular momentum $\vec l$ of the body ; $$ \vec \tau = \vec\omega\times\vec l $$ appendix . the motion of a rigid body undergoing rotation is generated by rotations . in other words , there is some time-dependent rotation $r ( t ) $ for which $$ \vec r ( t ) = r ( t ) \vec r ( 0 ) $$ it follows that $$ \dot{\vec r} ( t ) = \dot r ( t ) \vec r ( 0 ) = \dot r ( t ) r ( t ) ^t\vec r ( t ) = \vec\omega ( t ) \times \vec r ( t ) $$ in the last step , i used the fact that $r ( t ) $ is an orthogonal matrix for each $t$ which implies that $\dot r r^t$ is antisymmetric . it follows that there exists some vector $\vec \omega$ , which we call the angular velocity of the body , for which $\dot r r^t \vec a = \vec \omega\times\vec a$ for any $\vec a$ .
you are right , in this case , scalar means lorentz invariant field . but it is not invariant under the transformations of su ( 2 ) xu ( 1 ) of the electroweak model . and it is a scalar under the su ( 3 ) of qcd . so the four real components of the higgs are indeed invariant under space-time transformations . physicists are usually not very clear in these distinctions , and you have to guess under which transformation the field is a scalar .
some students learning physics for the first time mistakenly think that objects that are accelerating have force . force is not a property possessed by an object , but rather something you do to an object that results in the object accelerating ( changing its speed ) , given by the equation f = ma . that is , forces cause acceleration , not the other way around . this means that if you observe an object accelerating , then it implies a force is acting on the object to cause such an acceleration . in this case , as the object strikes the hand , your hand applies a force to the ball causing it to slow down ( decelerate ) , and the ball applies an equal and opposite force to your hand causing it to accelerate ever so slightly ( newton 's third law ) , which is detected by your sensory neurons .
to find the bound states for the potential $$v ( x ) ~=~\left\{\begin{array}{ccc}ae^{cx} and \text{for} and x&gt ; 0 , \\ \infty and \text{for} and x\leq 0 , \end{array} \right . $$ where $a , c&gt ; 0$ are two positive constants , one should solve the time-independent schrödinger eq . with the two boundary conditions $$ \psi ( x=0 ) ~=~0 \qquad \text{and} \qquad \lim_{x \to \infty}\psi ( x ) ~=~0 . $$ this boundary value problem does only have solutions for certain discrete values of the energy $e$ .
$e^{i\theta} + e^{-i\theta}$ is just $2\cos \theta$ . the superposed wavefunction is $$\psi ( x , t ) = 2n\cos ( ax ) e^{i ( f ( x ) + \omega t ) }$$ then $$\psi^*\psi = 4n^2\cos^2 ( ax ) $$ the average height is $2n^2$ if $x_0a = n\pi/2$ , in which case $n = \frac{1}{2}\sqrt{1/x_0}$ . otherwise you can do this integral .
not sure why the word " phase " is in the name of this function . the intensity of unpolarized light scattered by rayleigh particles as a function of detector angle $\theta$ is proportional to this function . colloidal scientists call this measurement " static light scattering , " but atmospheric scientists and others may have different names for it . you can get some physical intuition for this function by picturing an em wave interacting with a point scatterer . imagine sweeping a detector in a circle of a radius $r$ centered on the scatterer , such that the incident light is in the plane of this circle . then place an analyzer ( polarizer ) in front of the detector so you are detecting only either vertically or horizontally polarized light . if the incident light is vertically polarized , i.e. polarization normal to the plane of the circle , the amplitude ( and intensity ) of scattered light is isotropic . if the light is horizontally polarized , i.e. ( polarization in the plane of the circle ) , the intensity detected depends on detection angle . convention is to call $\theta=0$ degrees the angle of forward scattered ( also unscattered ) light . that way , $\theta=180$ is the angle where we detect backscattered light , and $\theta=90$ and $\theta=270$ are each right in between those two " poles " . imagine horizontally polarized radiation interacting with our point scatterer . this polarization leads to maximum scattered intensity at $\theta=0$ and $\theta=180$ , and minimum of zero scattered intensity at $\theta=90$ ( and $\theta=270$ ) . you can use the right-hand rule and dot products to convince yourself of this . let your fingers point in the direction of the polarization , and your thumb point in $\hat{k}$ , the direction in which the scattered photon is propagating . you can let your thumb point in any direction around the circle ; imagine that your thumb points toward the detector , so it determines which $\theta$ you are analyzing . if your fingers are in the plane of the circle you are looking at horizontally polarized light . try to put your fingers in the plane and point your thumb at $\theta=90$ . you should find that your fingers are pointing in a direction that is orthogonal to the horizontal polarization of the incident light , which means none of such light will scatter in this direction . the $cos^2 ( \theta ) $ term characterizes this angular dependence of horizontal polarization , and the $1$ term states the isotropy of the vertically polarized radiation . we have to add these for unpolarized incident light . this general dependence is true for dilute solutions of rayleigh scatterers , or gases . the intensity measurement comes from counting a large number of photons at each angle . the function comes from classical optics/electromagnetism . i suspect it could also be derived from a quantum mechanical ( qm ) treatment of photons . qm may be better suited for determining the " angles at which the scattering is more likely to happen " as you put it , since we think of the classical picture of as purely deterministic .
extra-dimensional scenarios may be described as " inspired " by string theory but they are independent hypotheses and they may be true even if string theory is not . however , one has to reduce the ambitions and standards of consistency . sociologically , it is surely true that the research of models with extra dimensions has been adopted and pursued by many people who have never take studied proper string theory or taken a course in it . despite the academic independence , a confirmation of experimentally accessible extra dimensions - which is extremely unlikely to occur , due to their likely tiny size - would be a huge evidence supporting string theory because it is the only framework in which the extra dimensions actually have a justification ( many ) .
$e=\hbar \omega$ is the energy of any particle , not just a photon . the terminology eigenvalue comes from linear algebra . given a matrix $m$ , an eigenvector $v$ of $m$ with eigenvalue $\lambda$ is a solution to the equation \begin{equation} mv = \lambda v \end{equation} in quantum mechanics the wave function is to be thought of as a kind of vector . observables are represented by ( hermitian ) operators ( which are morally the same as ( hermitian ) matrices ) , and the eigenvalues of those operators are the possible values the observable can take . so setting $m=i\hbar \frac{\partial}{\partial t}$ , $v=\psi$ , and $\lambda=\hbar \omega$ , you see the equation you wrote is just an eigenvalue equation for the " energy operator " $i\hbar \frac{\partial}{\partial t}$ . the eigenvalue is $\hbar \omega$ . it is called an energy eigenvalue because the observable is the energy . your physical system will be described by a wave function ( i prefer to call it a " state" ) $\psi$ . if $\psi$ satisfies that eigenvalue equation , then you will measure the system to have the energy $e$ with 100% probability . in general $\psi$ will not be an eigenvector of the energy operator . however it can be written as a sum of energy eigenvectors due to the magic properties of hermitian operators . in that case if you measure the energy you will measure it to be one of the eigenvalues of the eigenvectors making up $\psi$ , with probability given by the square of the amplitude of that eigenvector . if the notes you are following did not explain this clearly before diving into talking about energy eigenvalues , i would strongly recommend looking for alternative references to read . there are a million references that explain quantum mechanics at all levels and in many different ways . this point you are asking about is the central concept of all of quantum mechanics , so it is definitely worth reading many sources about this to find the one that has the explanation you find the clearest .
as the formula clearly shows , $\phi ( x ) $ cannot be interpreted as a pure creation operator of any type . it is a combination of creation and annihilation operators . creation operators are those called $a ( k ) ^\dagger$ and annihilation operators are called $a ( k ) $ . so yes , if $\phi ( x ) $ acts on a generic state with a well-defined number of particles $n$ , it produces a linear superposition of states that have $n+1$ and $n-1$ particles , respectively . when it acts on the vacuum , for example , however , the annihilation operator piece drops out and it creates a 1-particle state . it is somewhat hard to understand what you mean by " interpretation " . the only right interpretation is the right calculation . it is an operator that gives something if it acts on a state , and all these answers may be calculated . they should not be interpreted , they should be calculated .
let $f$ be the independent force acting on the object . let $d$ be the velocity dependent force acting in the opposite direction of $f$ . the net force accelerating the object is just the difference . we have : $f - d = ma$ since $d$ is velocity dependent , the equation is a differential equation for the velocity . $\dot v + \dfrac{ca\rho}{2m}v^2 = \dfrac{f}{m}$ this can be solved for $v ( t ) $ and which can then be integrated to find $x ( t ) $ . to solve this equation for constant independent force $f$ , first note that there is a terminal velocity which can be found by setting the acceleration to zero , $\dot v = 0$: $v^2_{term} = \dfrac{2f}{ca\rho}$ we can now rewrite the differential equation : $\dfrac{1}{1-v^2/v^2_{term}}dv = \dfrac{f}{m}dt$ we can now straightforwardly integrate both sides to get : $\tanh^{-1} ( v/v_{term} ) = \dfrac{f}{mv_{term}}t + c$ for zero initial velocity , we can finally write an expression for $v ( t ) $: $v ( t ) = v_{term} \tanh ( \frac{f}{mv_{term}}t ) $ thus , the velocity increases rapidly at first and then much more slowly , asymptotically approaching the terminal velocity which we can see from a plot of $\tanh$:
for the first question , no there can be no such general rule . the reason is the same why there is no intuitive way to understand the difference between your right and left shoe -- they are just reflections of each other but neither one is more fundamental ( assuming you are not a pirate ) . in other words , all the difference ( that really matters ) comes from the given theory . if the theory is $p$-symmetric ( such as electromagnetism ) you have no way to distinguish between these ( that is why you have not heard anyone talking about right- or left-handed electrons in the classical physics courses ; there were just electrons ) . on the other hand , if the theory violates $p$-symmetry you have a chance since one type of particles will interact differently than the other . e.g. weak interactions violate the $p$-symmetry so that only left-handed electrons and left-handed neutrinos can form a weak isospin doublet ( and so can right-handed antiparticles ) . i will leave the experimental part of the question to someone more educated on these matters . but e.g. for neutrinos you might be able to measure helicity ( which is as good as chirality since neutrinos are almost massless ) which is a projection of spin on to the momentum . you know the momentum direction ( since you know where the collision happened and where you registered the particle ) and i suppose there are standard methods for measuring spin projection to arbitrary direction ( but i admit complete ignorance on these experimental matters ) .
negative resistance is not uncommon . you see it in arc lamps , too . for your motor ( used as a generator ) , i would guess that it is most efficient at higher current , possibly having field coils and not permanent magnets . the voltage from a generator comes from moving a wire through a magnetic field . with field coils in series , the magnetic field is generated by the current flowing through the ( generator ) . there is probably a small residual permanent field in the iron , so you get some voltage even when there is no current . i bet if you could run up to higher currents , eventually you would get max power from the generator , and start seeing voltage go down again . i am going on this being a ' universal ' motor you have salvaged from somewhere . read about universal or series wound motors at the motor wiki : http://en.wikipedia.org/wiki/electric_motor i found a discussion group with one sensible ( to my mind ) answer amongst the cruft ( see engineertony 's reply ) here : http://cr4.globalspec.com/thread/77573/how-to-make-a-generator-from-a-universal-motor my own take is , go for it ! it kinda works , it looks like you can get real power from it , though the voltage is all over the map , you will want to regulate it .
impact on water is a very complex topic . your simple calculation just figures out the velocity of a free-falling body after a 50 m drop . that just tells you the initial relative velocity of body and water surface . it does not tell you much about the force at impact , or whether the person survived . there are two things that might kill on impact : high local pressure on the surface of the body - this can cause lacerations , bleeding , fractures ; and a deceleration of the body that causes internal organs ( the brain or the kidneys ) to rip loose , resulting in severe internal bleeding and death . the key to survival , then , is to minimize the local pressure and the deceleration . this is why you see stunt men dive in the most streamlined manner possible - they try to minimize the area with which they enter the water , which limits the force on the body and thus maximizes the time to decelerate . it also helps to fall into " less dense " water - for example , the frothy water right behind a cruise ship will have a lot of bubbles in it , and this will increase the distance you move before slowing down . there is an additional complication which relates to the shape of the contact area - you may be familiar with the " belly flop " , where you fall flat on the water and it hurts a lot . this is not just because you slow down quickly - it is because there is a brief moment when the contact point between your body and the water moves faster than the speed of sound in water , and this results in an " attached shock wave " which can cause the pressure of the water to briefly become very high ( see for example http://www.dtic.mil/dtic/tr/fulltext/u2/a259783.pdf - a bit of a long paper . . . ) . it seems that the world record for a stunt man surviving a fall into water stands at just over 54 m ( olivier favre , https://www.youtube.com/watch?v=mld529gwkj4 ) . and that is " still " water . as i mentioned above , in frothy water it might be possible to survive a higher drop . at the same time , people frequently kill themselves by jumping off the golden gate bridge - about 70 m height . found an interesting link discussing this - the last answer sounds quite credible in its analysis , making some attempt at quantifying the forces during impact . i do not agree with the entire analysis ( in particular the " slamming into a stationary body at half the speed " is too much of an approximation for my taste ) but it makes some good points : http://www.newton.dep.anl.gov/askasci/gen01/gen01790.htm
however , this same black sky effect does not occur when looking out the window of a plane . it does not ? ( image credit : http://www.123rf.com/photo_10994787_view-of-jet-plane-wing.html )
well here 's an easier way to see it : as the universe is a fabric of space and time , the fabric can be rolled up and spread according to our needs . the problem : large amounts of energy is necessary for this to happen . in the starting , the big bang provided the energy necessary to do it . the speed of light limit is on actual objects , not on expansion of space , as observed during the period just after big bang . thus we can bend space behind us and expand it in front of us at speeds faster than light , appearing as if we are travelling faster than light .
strictly speaking , a " particle " is only a quantum notion which must be understood in the context of quantum field theory , as a asymptotic state " in " or " out " , in some interaction . so , classicaly , stictly speaking , there is no " particle " you are speaking of the classical dirac equation , which is a classical field equation . the quantum field version of the dirac equation , is an equation , where the fields are operators , and these operators apply on states . the " particles " are only some particular asymptotic states , in interactions , being on mass-shell . the field operators are mixing creation operators of particles , and destruction operators of anti-particles , because you cannot separate the two . so the classical version of the dirac equation , which is a approximate view of the quantum version of the dirac equation , is mixing too the degrees of freedom of pseudo-particles and pseudo-anti-particles . but keep in mind that only a quantum treatment is correct , where particles are states , and fields are operators .
the primary utility in introducing the generating functional is in using it to compute correlation functions of the given quantum field theory . let 's restrict the discussion to that of a theory of a single , real scalar field on minkowski space , and let $x_1 , \dots , x_n$ denote spacetime points . of central importance are time-ordered vacuum expectation values of field operators evaluated at such points ; \begin{align} \langle0|t [ \phi ( x_1 ) \cdots\phi ( x_n ) ] |0\rangle . \end{align} it can be shown that these objects can be obtained from the generating functional by taking functional derivatives with respect to the $j ( x_i ) $ as follows : \begin{align} \langle0|t [ \phi ( x_1 ) \cdots\phi ( x_n ) ] |0\rangle = \frac{1}{z [ 0 ] }\left ( -i\frac{\delta}{\delta j ( x_1 ) }\right ) \cdots \left ( -i\frac{\delta}{\delta j ( x_n ) }\right ) z [ j ] \bigg|_{j=0} . \end{align} this standard fact is proven in many books on qft . it is often proven using the path integral approach which makes it pretty transparent why it is true . the crux of the argument is that every time you take a functional derivative with respect to the source $j ( x_i ) $ , it pulls down a factor of the field $\phi ( x_i ) $ . dividing by $z [ 0 ] $ is an important normalization relating to vacuum bubbles , and setting $j=0$ after computing the appropriate functional derivatives eliminates terms with more than $n$ factors of the field and renders the final result source-independent as it should be .
in the practice of measurement control , a very good rule of thumb is to have at least 30 measurement points under the same conditions when characterizing an unknown situation . if you have time to make 100 measurements , i would suggest making 33 " repetition " measurements at low , mid , and high values of $t$ . the justification for the number 30 comes from analysis of populations of measured variables using the student 's t distribution . in your case , the degrees of freedom would be one less than the number of " repetition measurements , " i.e. 32 . edit i should probably point out that this rule is used for the characterization of variability in the measurement system , not the measurand . if you are using a well calibrated piece of equipment , e.g. a scale with well known uncertainty , you can make fewer measurements . nonetheless , i would still recommend multiple measurements of the measurand . when adding up the contribution to total uncertainty , the standard deviation of the measuring system can be devided by the root of the number of measurements taken . that is , $$\sigma_m = \frac{\sigma_s}{\sqrt{n}}$$ where $\sigma_m$ is the contribution to the total uncertainty in the measured value $\sigma_s$ is the characterized uncertainty of the measurement system and $n$ is the number of " repitition " measurements taken you should aim to make $n$ sufficiently large that $\sigma_m$ is a small relative to other contributers to the total uncertainty .
what follows is a very rough adaption of chapter 25 in gravitation by misner , thorne , and wheeler . begin with the schwarzschild metric with polar angle $\theta$ fixed at $\pi/2$: $$ ds^2 = -\left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \mathrm{d}t^2 + \frac{1}{1-r_\mathrm{s}/r} \mathrm{d}r^2 + r^2 \mathrm{d}\phi^2 . $$ for a test particle of rest mass $m$ , 1 we know by definition $$ g_{\mu\nu} p^\mu p^\nu + m^2 = 0 , $$ where $\vec{p}$ is the 4-momentum of the particle . for an affine parameter $\lambda$ parametrizing the worldline of the particle , these two equations can be combined to give $$ -\left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{\mathrm{d}t}{\mathrm{d}\lambda}\right ) ^2 + \frac{1}{1-r_\mathrm{s}/r} \left ( \frac{\mathrm{d}r}{\mathrm{d}\lambda}\right ) ^2 + r^2 \left ( \frac{\mathrm{d}\phi}{\mathrm{d}\lambda}\right ) ^2 + m^2 = 0 $$ now the derivative in the first term is simply the energy $e$ , which is related to the conserved energy at infinity $e_\infty$ by $e = e_\infty/ ( 1-r_\mathrm{s}/r ) $ . furthermore , the definition of angular momentum is $l = r^2 ( \mathrm{d}\phi/\mathrm{d}\lambda ) $ , which is also conserved . inserting these definitions gives $$ -\frac{e_\infty^2}{1-r_\mathrm{s}/r} + \frac{1}{1-r_\mathrm{s}/r} \left ( \frac{\mathrm{d}r}{\mathrm{d}\lambda}\right ) ^2 + \frac{l^2}{r^2} + m^2 = 0 , $$ or $$ \left ( \frac{\mathrm{d}r}{\mathrm{d}\lambda}\right ) ^2 = e_\infty^2 - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{l^2}{r^2} + m^2\right ) . $$ using again the definition of $l$ , we can write $$ \left ( \frac{\mathrm{d}r}{\mathrm{d}\phi}\right ) ^2 = \frac{r^4}{l^2} \left ( e_\infty^2 - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{l^2}{r^2} + m^2\right ) \right ) , $$ which we can rewrite to be \begin{align} \left ( \frac{\mathrm{d}r}{\mathrm{d}\phi}\right ) ^2 and = r^4 \frac{e_\infty^2-m^2}{l^2} \left ( \frac{e_\infty^2}{e_\infty^2-m^2} - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{1}{r^2} \left ( \frac{l^2}{e_\infty^2-m^2}\right ) + \frac{m^2}{e_\infty^2-m^2}\right ) \right ) \\ and = \frac{r^4}{b^2} \left ( \frac{e_\infty^2}{e_\infty^2-m^2} - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{b^2}{r^2} + \frac{m^2}{e_\infty^2-m^2}\right ) \right ) , \end{align} where $$ b = \frac{l}{\sqrt{e_\infty^2-m^2}} $$ is the impact parameter , defined to be the ratio of angular to linear momentum . at this point , we have a nice general result , but to apply it to photons we take the limit $m \to 0$ , which gives $$ \left ( \frac{\mathrm{d}r}{\mathrm{d}\phi}\right ) ^2 = \frac{r^4}{b^2} \left ( 1 - \frac{b^2}{r^2} \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \right ) . $$ the radius of closest approach will be the value $r = r_\text{min}$ for which $\mathrm{d}r/\mathrm{d}\phi$ vanishes : \begin{gather} \frac{r_\text{min}^4}{b^2} \left ( 1 - \frac{b^2}{r_\text{min}^2} \left ( 1 - \frac{r_\mathrm{s}}{r_\text{min}}\right ) \right ) = 0 ; \\ \frac{b^2}{r_\text{min}^2} \left ( 1 - \frac{r_\mathrm{s}}{r_\text{min}}\right ) = 1 ; \\ b^2 ( r_\text{min} - r_\mathrm{s} ) = r_\text{min}^3 . \end{gather} the only thing left is to decide what $r_\text{min}$ is allowed to be if the light is to escape . since at this point in the trajectory the photon 's velocity is entirely in the tangential direction by construction , the question becomes how close in can a photon have a circular orbit ? the answer is $r_\text{min} = 3r_\mathrm{s}/2$ . thus $b_\text{max}$ , the maximum impact parameter for a photon to be captured , obeys $$ b_\text{max}^2 \left ( \frac{3}{2} r_\mathrm{s} - r_\mathrm{s}\right ) = \left ( \frac{3}{2} r_\mathrm{s}\right ) ^3 , $$ or $$ b_\text{max}^2 = \frac{27}{4} r_\mathrm{s}^2 . $$ this is exactly what we sought , since it immediately tells us $$ \sigma = \pi b_\text{max}^2 = \frac{27}{4} \pi r_\mathrm{s}^2 . $$ 1 we could set $m = 0$ here , but by holding off we get slightly more general intermediate results .
a push up is a form of lever . the athlete must exert roughly half her body weight ( under some assumptions i will clarify at the end of the post . ) we can solve this problem using the principle of virtual work . assume the athlete raises her body through a small angle $\textrm{d}\theta$ . then her center of mass rises by $l \cos\theta \ \textrm{d}\theta$ , with $l$ the distance from her feet to her center of mass . the work done is $$\textrm{d}w = mgl\cos\theta \ \textrm{d}\theta$$ this work is equal to the force multiplied by the distance over which the force is exerted . if we call the distance from her feet to her shoulders $l$ , then $$mgl\cos\theta \ \textrm{d}\theta = f l \cos\theta \ \textrm{d}\theta$$ $$f = \frac{mgl}{l}$$ your center of mass is roughly half way up your body , so a push up requires you to lift about half your weight . this answer assumes the pushup is infinitely slow ( i.e. . no acceleration ) . this is actually not so bad an approximation as it sounds , but people doing fast push ups will probably exert a higher force at the bottom of the push up , then let gravity do negative work to slow them down as they near the top . this is the basis for " clapping push ups " ( which i am too weak to perform ) . we have only calculated the component of force in the direction of motion , so , we are assuming the athlete pushes directly in the direction of motion of her shoulders . this means she does not push along the red line in your picture , but diagonally ahead of it . in reality , she might push more straight down , increasing the force and decreasing the angle through with her elbows flex . the force may actually change throughout the push up due to this effect . if you lean into the arm of a couch and do a push up off that , you will find it is much easier at this high angle . the reason is you are not pushing straight down any more , so it is a little bit like going up a ramp ( in that the distance over which the force is exerted is lengthened to accomplish the same amount of work ) . i also assumed the athlete 's center of mass does not move , which means i am pretty much thinking of her arms as massless , and the rest of her is a rigid board . finally , i am assuming her hands and feet do not slip , the floor is too big to move , etc . an empirical way to determine the force would simply be to put scales underneath the hands , but the reading could be a little off depending on whether the scales are constructed to measure total force on them , or only the vertical component of the force . finally , an actual human is not simply one force pushing in one direction . there are muscles , bones , ligaments , etc . all sorts of different forces are going on in different places . this calculation gives the force that the arms exert on the main body . the force that the triceps exert on the elbow , for instance could be much higher .
in the many-worlds interpretation of quantum mechanics , there is at all times just one state vector for the entire universe . it is a vector in a certain ( infinite-dimensional ) vector space , and that vector space is always the same . so there is nothing in the theory whose dimension changes when the number of particles in the universe changes . to be a bit more precise , the space in which the state vector of the universe lives is ( something like ) a fock space . vectors in that space include states with all possible different numbers of particles , as well as superpositions containing different numbers of particles . so if a particle-antiparticle pair is created , the state vector simply " wanders " from one part of that space to another ; the space itself need not get any bigger .
there is hardly a book covering all physics , but for particular subjects there is some . for example : jammer : the conceptual development of quantum mechanics . whittaker : a history of the theories of aether and electricity .
this is in addition to my answer posted elsewhere since op wanted a more general answer . that example captured the essence based on the idea of how information can be encoded -- it is a somewhat constructive argument in spirit . another way of thinking about the amount of information is as follows : if an event that is very probable happens , then you cannot get much information out of it . it was going to happen any way . on the other hand , if something unusual happens , then that should give you something to think about . such an event carries more " information " . ( for eg : the occurrence of a certain event conveys no information ) if two independent events happen , then the information you glean from them must " add up " . since their probabilities multiply to give the probability of the combined event , the information gleaned from each event must be proportional to the $\log$ of its probability . in typical treatments , one solves a functional equation for the dependence of the entropy on the probability distribution -- with the two conditions mentioned above . the latter gives the $a \log [ ] + b $ while the former fixes the additive constant $b$ to zero . the scale factor $a$ depends on the base to which you take the logarithm .
i do not know if this is what op is asking ( v1 ) , but consider a single qubit , whose states can be visualized via the bloch sphere . the classical states ( which form a bit ) are the north and the south pole . the pure quantum states are the surface of the 2-sphere $s^2=\partial ( b^3 ) $ , and the mixed quantum states are the interior 3-ball $b^3$ . since there is just a single qubit , it is not entangled .
faddeev has implicitly dropped a total 4-divergence term $d_{\mu} ( a_0 f^{0\mu} ) $ in the lagrangian density ${\cal l}$ . this does not affect the equations of motion , i.e. , maxwell 's equations .
there exists such a model and it is described in a wikipedia article , the ekpyrotic universe . the ekpyrotic model came out of work by neil turok and paul steinhardt and maintains that the universe did not start in a singularity , but came about from the collision of two branes . this collision avoids the primordial singularity and superluminal expansion while preserving nearly scale-free density fluctuations and other features of the observed universe . the ekpyrotic model is cyclic , though collisions between branes are rare on the time scale of the expansion of the universe to a nearly featureless flat expanse . observations that may distinguish between the ekpyrotic and inflationary models include polarization of the cosmic microwave background radiation and frequency distribution of the gravitational wave spectrum .
this is more of a psychology question . after you start seeing things , you notice that a certain side of your vision is the part that will see your finger if you touch your forehead , and the other side will see your finger if you touch your lips . we designate the first as " up " and the second as " down " . the brain just gets a bunch of signals . " up " and " down " are artificial tags we attach , where " up " is the side of our forehead and " down " is the side of our lips . besides , if one was to wear glasses that inverted vision , the mind would indeed slowly adjust .
the correct equation for position at time $t$ : $\vec{r ( t ) } = \vec{p} + \vec{v} . t + {1\over2}\vec{a} . t^2$ where $\vec{v}$ accounts for both initial speed and wind speed , thus : $\vec{v} = 30 . \hat{\vec{d}} + 15 . \hat{\vec{v}} = ( 18,15,24 ) $ and $\vec{a} = ( 0,0 , -9.81 ) $ is gravity acceleration assuming it is pointed toward -z axis . it hits the ground when z equates the radius , so : $2 + 24t - {9.81\over2}t^2 = 0.015$ solving for the above equation give you the time . mass does not play a role because we know the acceleration from the start , also the same for wind velocity as long as it has no z component . edit : we could calculate the drag force for a sphere : $f_d = {1\over2}c_d \rho v^2 a$ in which $c_d$ is a geometrical constant that is equal to 0.47 for sphere , $\rho$ is the density of the medium ( air in this case ) , $v$ speed relative to air , $a$ is the cross section area ( $\pi r^2$ ) . from that you could calculate acceleration due to drag ( that is where mass comes in ) and you need to integrate manually or ( numerically ) ( look at jj fleck solution for this part ) acceleration equation to get position equation as the above equation for $\vec{r ( t ) }$ is valid only for the case of constant acceleration .
conceptually , the reason they are attached to walls is to remove rigid body motion from consideration . when they are tethered to the wall , there is no translation or rotation about the center of mass of the system . so , when you break the connections to the walls and they are now free to drift through space , the number of admissible solutions increases . if you connect one mass to the wall and leave the other end unconnected , then you remove the rigid body modes because it cannot rotate or translate and you are back to a similar , but different , problem as when both are connected to walls . the equations of motion are easily derived for 3 bodies in both conditions when one draws the free body diagram ( if deriving from a newtonian approach ) or through the application of lagrange 's equation or hamilton 's principle , depending on your choice of weapon . but it is not that it is harder or impossible to have the three masses floating in space , it just adds more admissible modes to the solution which may or may not obscure the intent of the example/problem .
i supposed you are in a context of bound states , with normalized eigenfunctions $\psi_n ( x , t ) = \phi_n ( x ) e ^{ie_nt}$ . of course , if you calculate $\langle x ( t ) \rangle_{\psi_n} = \int dx \bar \psi_n x \psi_n$ , you will find a position expectation value which does not depend on time . now , this is not the general case , if you take a linear combination of the $\psi_n$ : $\psi = \sum a_n \psi_n$ , and calculate $\langle x ( t ) \rangle_{\psi} = \int dx \bar \psi x \psi$ , you will find a positition expectation value which depends on time .
in sqcd , you can get " gluino hadrons " , mesons and baryons in which the gluino is one of the constituent fermions . ( also a gluinoball , a glueball with some gluinos mixed in . ) so you could have a " gluino string " , but in the opposite sense to what you want : it is two gluinos , at the ends of a gluon string . another possibility would be a topological defect in a gluino condensate . maybe you could get this in n=2 sqcd , with monopoles at the ends . but what you are looking for is a " fermionic meson " , two quarks connected by the " gluino string " . well , you can define this as an operator , a fermionic wilson line . the only place i can find such an entity playing a role is in holographic approximations to qcd like the sakai-sugimoto model . in holographic qcd , you typically have color branes and flavor branes . a gluon is a string between color branes , a quark is a string between a color brane and a flavor brane , a meson is a string between flavor branes , and a baryon is a brane connected by strings to flavor branes ( these strings are the valence quarks ) . these strings and branes all live in ads space , and qcd lives on the boundary . also , these are usually supersymmetric models - the exact , nonsupersymmetric holographic dual to qcd has yet to be determined . in superstring theory , you do have fermionic strings , and they are extended in space ; the fermi statistics come from the fermionic fields on the worldsheet . and the meson strings in holographic qcd do have mesino superpartners . see section 3.3.2 here . they are discussing mesino operators in the worldvolume theory of a flavor brane . one is a quark-squark composite , but the other is a quark and an antiquark connected by a gluino ( the adjoint fermion ) . so we had to go into anti de sitter space to find it , but there it is , the fabled fermionic gluino string . :- )
there is not that much to remember as far as fundamental particles are concerned . we have matter particles and field particles . matter particles are quarks and leptons and the latter include electrons and neutrinos . both quarks and leptons come in pairs ( up/down quark , electron/neutrino , etc . ) which means they are charged under the weak force . quarks are moreover colored and form rgb triplets which means they are charged under the strong force . that is pretty much it except for unknown reasons there are three copies of each particle with identical properties but distinct masses ( e . g . electron/muon/tauon ) . you do not really need to know much about the two heavier families since these are unstable ( decaying into the lower families which are stable since there is nothing else they could decay into ; at least besides the down quark that can decay into up quark ; this is the famous radioactive beta decay ) and you need expensive equipment ( such as lhc or jets from galaxies ) to produce them . as for the field particles , there is one type for every force although it gets a bit more tricky with weak and strong forces . for em we have the usual photon . weak force will give us three more -- the $w^{\pm}$ and $z$ which are massive -- and finally the strong force has eight massless gluons ( of various colors ) . additionally there may or may not be the higgs particle ( or more of them ) , superpartners , etc . but all of this is as yet unconfirmed . so , there , that is all there is to particle zoo . it is only pain when you try to learn it by heart . but if you instead try to understand the concepts related to these particles ( role of the weak particles and neutrinos in the beta decay , muons in cosmic rays , features of quantum chromodynamics , etc . ) then it should be fun .
real batteries have a finite energy storage capacity . adding additional cells adds additional capacity ( this is why i would add them , you have not really specified any context so it is hard to say what you are looking for ) . also , it is worth noting : i am assuming that all the cells are identical , and internal resistance is negligible . this is an important assumption . in a real circuit , you would lose some power ( $i^2r$ ) to the internal resistance of the battery . you can reduce this power loss by adding cells , reducing the current each cell provides .
so , as noted , we use poynting 's theorem to get : $$\frac{1}{\mu_0} \oint_{\partial v} ( \vec{e} \times \vec{b} ) \cdot d\vec{s} = -\int_v \vec{e} \cdot \vec{j} dv$$ [ the static version of poynting 's theorem is just : divergence theorem , $\nabla \cdot ( \vec{e} \times \vec{b} ) = \vec{b} \cdot ( \nabla \times \vec{e} ) - \vec{e} \cdot ( \nabla \times \vec{b} ) $ , then $\nabla \times \vec{e} = 0$ and $\nabla \times\vec{b} = \mu_0\vec{j}$ ] the electric field is just the negative gradient of electric potential . we can use integration by parts in higher dimensions : $$-\int_v \vec{e}\cdot\vec{j} dv = \int_v ( \nabla \phi ) \cdot \vec{j} dv = \oint_{\partial v} \phi \vec{j} \cdot d\vec{s} - \int_v \phi ( \nabla \cdot \vec{j} ) dv$$ the current density does not diverge . so it is just the first term , basically , the surface integral of voltage times current . of course your boundary condition says there is no current out of the surface , still , the previous statement holds in static situations even without that boundary condition .
in general , the elasticity of a collision is dependent on the properties of the colliding objects . in a perfectly elastic collision , no kinetic energy is dissipated , which means the collision creates no heat , no sound , etc . in a perfectly inelastic collision , the maximum possible amount of kinetic energy is dissipated as heat , sound , etc . this corresponds to the two particles sticking together after the collision . in real life , most collisions are neither perfectly elastic nor perfectly inelastic , but rather somewhere in the middle . ( one major exception to this is gas molecule collisions , which are perfectly elastic . ) some objects collide nearly perfectly elastically , such as billiard balls or steel ball bearings , while others collide nearly perfectly inelastically , such as balls of putty or mud . without knowing the specific properties of the colliding objects ( such as their elasticity , etc . ) , it is impossible to predict how elastic a collision will be .
i do not just mean reactions that require heat to proceed , storing surplus energy in chemical bonds . i wonder about strongly endothermic reactions that suck heat out of environment a reaction that requires heat to proceed , a reaction that sucks heat out of the environment , and an endothermic reaction are all the same . these are all just descriptions of reactions that occur because entropy ( s ) increases , despite that fact that enthalpy ( h ) also increases . $g = h -ts$ a reaction is favorable if gibbs free energy decreases . the reaction can still be favorable , despite enthalpy ( h ) increasing , if entropy ( s ) increases enough . you take some substance a ( e . g . ammonium nitrate ) , and some substance b ( e . g . water ) . . . i wonder what happens on the microscopic scale you have to separate molecules/ions/atoms of a from others of a . this could involve breaking apart ionic interactions in a crystal , or intermolecular interactions for example . this takes energy . you have to seperate molecules/ions/atoms of b from others of b . for a solvent , this would involve breaking apart intermolecular interactions such as dipole-dipole interactions . this takes energy . new interactions between a and b form . this releases energy . if the energy released in step 3 is less than the energy required for steps 1 and 2 , the process is endothermic .
maybe that was just window frost ( http://www.its.caltech.edu/~atomic/snowcrystals/frost/frost.htm - " forms when a pane of glass is exposed to below-freezing temperatures on the outside and moist air on the inside" ) ?
here is the answer that i gathered from months of looking at these boundary conditions : ( 1 ) and ( 2 ) would mean that the slope is zero and the bending moment / curvature at the ends is zero . ( 1 ) and ( 3 ) mean that the slope is zero and the shear stress at the end is zero .
i think the analogy with the em field is a good one with some provisos e.g. gravitational waves require quadrapolar or higher oscillation ( gravitational dipoles do not radiate ) and gravitational waves are self interacting . the electric potential round a point charge does not need continual em waves to sustain it , and likewise the curvature round the earth does not need continual gravity waves to sustain it . for for both gravity and electromagnetism waves are only required to propagate changes in the fields , not static fields . planets , or any form of mass/energy , can not spring in and out of existance any more than electric charges can spring in and out of existance , so it is not useful to use this as an analogy . however if you take any randomly shaped electric field you can decompose it as a fourier series to get the combination of em waves needed to ( momentarily ) create it . i guess the same would be true of gravitational waves , at least in the weak field approximation .
for your second problem , the propagator can be written with its indices as $$ ( s_f ) _{\alpha\beta} ( x-y ) = \langle t\{\psi_{\alpha} ( x ) \bar\psi_{\beta} ( y ) \} \rangle $$ then we have $$ \langle t\{\bar\psi ( x ) \gamma\psi ( y ) \} \rangle = \gamma_{\alpha\beta} \langle t\{\bar\psi_{\alpha} ( x ) \psi_{\beta} ( y ) \} \rangle = -\gamma_{\alpha\beta} ( s_f ) _{\beta\alpha} ( x-y ) = -\mathrm{tr} [ \gamma s_f ( x-y ) ] $$
there are few relevant points : the first is that we very rarely consider the magnetism of nuclear moments in a solid because the nuclear magneton is about 2000 times smaller than the bohr magneton . so we are really stuck with electrons as the only relevant degrees of freedom for magnetism . but , let 's imagine for a minute that we could kick out the electrons . the whole point of a spin liquid is to be driven to paramagnetism at low temperatures by quantum fluctuations . the high ordering temperature scales in typical magnets are set by the " exchange " scale ( of electrons literally , though virtually , exchanging places in the solid ) . dipole-dipole interactions are not strong enough to be the origin of magnetism in solids ( this was a big deal in the early days of quantum mechanics ! ) and dipolar interactions between nuclear spins even more so ; so functionally , it would be impossible to determine experimentally if such a system was a spin liquid or just a thermally disordered paramagnet even at the lowest possible experimental temperatures .
magnetic fields are generated by moving charged particles , and exert forces on moving charged particles . the magnetic field generated by a moving charged particle can be calculated using biot-savart 's law . since a current is a steady stream of charged particles , the magnetic field created by a current-carrying wire can be calculated integrating over the length of the wire . the exact formula in classical electromagnetism is : $$\mathbf{b} =\frac{\mu_0 q \mathbf{v}}{4\pi} \times \frac{\mathbf{r}}{r^2}$$ you want to pay close attention to the two vector quantities , v and r , the velocity of the charged particle and the position in space from it , and to the fact that they are combined with a cross product , $\times$ . the resulting magnetic field , b , is therefore perpendicular to both . so a single electron moving in a straight line basically generates a magnetic field that goes around in circles around the path it is moving along . this picture may help . then you have the force acting on a moving particle when there is a magnetic field . this is known as lorentz 's force , and the equation describing it is : $$\mathbf{f} = q\ \mathbf{v} \times \mathbf{b}$$ you again have a cross product , this time involving the magnetic field , and again v , although this is a different v than before , not the velocity of the moving charged particle generating the field , but of the moving charged particle the field is acting on . because of the cross product , a particle will not have any force acting upon it if it is moving parallel to the magnetic field . that is exactly why there is no force on the closed loop in your picture : because it lays along the direction of the magnetic field . you could combine both equations into a single one , and so the force on particle 2 by particle 1 , $\mathbf{f_{21}}$ , if $\mathbf{r_{21}}$ is the position vector of particle 2 from particle 1 , would then be $$\mathbf{f_{21}} =\frac{\mu_0 q_1 q_2}{4\pi\ r^2}\mathbf{v_2} \times \mathbf{v_1} \times \mathbf{r_{21}}$$
the circuit likely was closed by his body , or by a grounding wire he was holding . at least that is one way the demonstration has been done . i assume the other end was in contact with the generator . another way is to suspend the tube so it is not in electrical contact with anything , and swing it around , so that you observe a momentary discharge when the tube is orthogonal to the field . in this case the discharge shuts itself off because there is no closed circuit path as you observe . yes . think of v as like the height of a hill for a positive charge . e wants to push the charge down the hill , in the direction of lower v . hence the minus sign .
consider the ratio of their shifts in energy . since all the numbers are the same except for the reduced mass you are looking at something like : $$ \frac{ ( e_i -e_f ) _p}{ ( e_i -e_f ) _d}=\frac{\mu_p}{\mu_d}=\frac{m_p m_e ( m_d +m_e ) }{ ( m_p +m_e ) m_d m_e} $$ if we cancel the $m_e$ and then pull a $m_d$ out of the top and a $m_p$ out of the bottom we get $$ \frac{ ( e_i -e_f ) _p}{ ( e_i -e_f ) _d}=\frac{ ( 1+\frac{m_e}{m_d} ) }{ ( 1+\frac{m_e}{m_p} ) } $$ i hope this helps .
i would like to add a bit of mathematical detail the ( correct ) statements by djbunk . let a scalar function $f$ be given ( let 's not restrict ourselves to the electric potential ) . for any unit vector $\mathbf n$ , we can define the directional derivative $d_\mathbf{n}$ of the function $f$ in the direction $\mathbf n$ as follows : $$ d_\mathbf{n}f ( \mathbf x ) = \mathbf n\cdot\nabla f ( x ) . $$ the directional derivative gives the rate of change of the scalar function $f$ in the direction of the unit vector $\mathbf n$ . notice that $$ \mathbf n\cdot \nabla f ( \mathbf x ) = |\nabla f ( \mathbf x ) |\cos\theta $$ where $\theta$ is the angle between $\mathbf n$ and $\nabla f ( \mathbf x ) $ , so the directional derivative is maximized when $\theta = 0$ , and is minimized when $\theta = -\pi$ . in other words ; the the rate of change of a scalar function $f$ at a point $\mathbf x$ is positive and greatest in magnitude in the direction of the gradient of $f$ at $\mathbf x$ . this confirms bjbunk 's statements .
the question is : does the air+alcohol-gas burn creating a big pressure or does the container need of a little hole to burn ? strange " need of a little hole " , what does that mean ? the question was : do we need of hole to start the ratio of oxygen+alcohol another time strange " need of hole " . and on the whole two rather different questions . one thing i can say : in a container with alcohol vapor ( i assume ethyl alcohol is meant ) and air one will have 40 mmhg partial pressure of ethanol at 19 °c 720 mmhg partial pressure of air this makes 40/760 = 5.3 vol-% of ethanol . a view in tables says that ethanol has lower explosive limit ( lel ) 3.3 vol% upper explosive limit ( uel ) 19 vol% answer is : your container will explode , the liquid ethanol will be sprayed around , burning . i would prefer to be several dozens of meters away from such " experiments " , at least .
you need the two initial gluons in your #2 because each one comes from the sea of one of the participating particles ( i.e. . the proton and the anti-proton ) . there is not enough energy in a ( anti- ) proton to produce a top quark , but the energetic collision supplies enough . the vertical line in your #3 is a space-like ( anti- ) quark line . it is meaning is very clear in the connection between quantum field theories and feynman diagrams , but about the best you can say without the math is that it represents a virtual fermion . you may recall that anti-particles are mathematically equivalent to particles moving backward in time , with allows an interpretation in which it takes two interaction with the gluon to " reverse " it and then you have a single particle that comes from the in as an anti-particle , gets turned around and heads back to the future as a particle .
you have misunderstood the nature of a singularity . the singularity at the centre of a black hole is not a point in space . instead it is a place where spacetime becomes infinity curved , and it is not possible to describe what happens there . well , it is not possible using general relativity , but we hope some future theory of quantum gravity will explain what happens at the singularity . if you jumped into a black hole , and timed your fall using your wrist watch , then you would hit the singularity in a finite ( and short ! ) time , but what happens to you at the singularity it is not possible to say . a light ray can not time its travel time to the singularity , but it would also hit the singularity ( still traveling at $c$ ) and once again what happens afterwards we can not say . if you are interested in finding out more about this then the phenomenon is called geodesic incompleteness . a geodesic is the path taken by a freely falling observer , like you and the light ray , and we describe it as incomplete because as far as we can tell the path stops abruptly at the centre of the black hole and there is no space or time beyond it .
" free energy " in the sense of " gibbs free energy " or " helmholtz free energy " is a rigorously defined concept . by contrast , the word " energy " is ill defined and means different things in different contexts . i think you will have to state your question more precisely for us to help . from lagerbaer in a comment : one way to think about energy vs free energy is that the former is a measure of all the work you had have to do to assemble the system , whereas the free energy , which you get from the energy itself by subtracting $t\cdot s$ where $t$ is temperature and $s$ is entropy , tells you how much work you could get out of a system as " useful " work ( in contrast to the " random " energy in heat ) i did a quick search for " what is energy " and what is energy ? where did it come from ? looks an informative thread . have a look at this discussion and see if it casts any light on your question .
i think i worked it out myself . what i found is that trying to arrive at the hartree-fock hamiltonian via this mean-feald procedure is not really that appropriate . instead , one says : given a two-particle operator $\hat o = c_1^\dagger c_2^\dagger c_3 c_4$ , how can i find a single-particle operator o^{mf}$ that is a good approximation ? if we have a complete set of single-particle basis states that include states $1$ to $4$ of the two-particle operator , we would then look at the basis elements of $\hat o$ and $o^{mf}$ and try to find the best possible agreement . now , the two-particle operator can do three different things : 1 . leave a state as is . this is the case if either $1 = 3$ and $2 = 4$ or $1 = 4$ and $2 = 3$ . 2 . move one particle from a previously occupied state into a previously unoccupied state , e.g. if $1 = 3$ but $2 \not = 4$ or $1 = 4$ and $2 \not= 3$ . 3 . move two particles into new states , i.e. if $1$ and $2$ are different from $3$ and $4$ . with our single particle operator , we can only hope to somehow reproduce the behavior of case 1 and 2 , but not case 3 . so , given states of the type $|\psi_0\rangle$ that can be written as $\prod_i c_{n_i}^\dagger |0\rangle$ , we look at all matrix elements of $\hat o$ between $|\psi_0\rangle$ and states of the type $c_i^\dagger c_j |\psi_0\rangle$ , $i \not= j$ , that arise from $|\psi_0\rangle$ by exchanging two particles . it is a bit tedious to work these matrix elements out in detail , but one can then indeed confirm that by chosing $$\hat o^{mf} = \langle c_1^\dagger c_4\rangle c_2^\dagger c_3 + \langle c_2^\dagger c_3\rangle c_1^\dagger c_4 - \langle c_1^\dagger c_3\rangle c_2^\dagger c_4 - \langle c_2^\dagger c_4\rangle c_1^\dagger c_3$$ the matrix elements for the two-particle operator $\hat o$ and the mean-field operator $\hat o^{mf}$ will turn out to be the same ( for the states of the type introduced above ) .
both the continuous spectrum and the characteristic lines are used ; however , in some cases a filter such as aluminum can be used to remove low energy xrays that are not needed , so as to reduce xray exposure , as explained here : http://www.cyberphysics.co.uk/topics/medical/xray.html
from special relativity we know that a lorentz transformation : \begin{equation} x'^\mu = \lambda^\mu {}_\nu x^\nu \end{equation} preserves the distance : \begin{equation} g^{\mu \nu} \delta x_\mu \delta x_\nu = g^{\mu \nu} \delta x_\mu ' \delta x_\nu ' \end{equation} the above two equations imply : \begin{equation} g^{\mu \nu} = g^{\rho \sigma}\lambda_\rho {}^\mu \lambda_\sigma {}^\nu \end{equation} now , let us consider an infinitesimal transformation : \begin{equation} \lambda_\nu {}^\mu = \delta_\nu{}^\mu + \omega_\nu{}^\mu + o ( \omega^2 ) \end{equation} such that we can write : \begin{equation} \begin{aligned} g^{\mu \nu} and = g^{\rho \sigma}\lambda_\rho {}^\mu \lambda_\sigma {}^\nu \\ and = g^{\rho \sigma} \left ( \delta_\rho{}^\mu + \omega_\rho{}^\mu + \cdots \right ) \left ( \delta_\sigma{}^\nu + \omega_\sigma{}^\nu + \cdots \right ) \\ and = g^{\mu \nu} + g^{\mu \sigma} \omega_\sigma{}^\nu + g^{\rho \nu} \omega_\rho{}^\mu + o ( \omega^2 ) \\ and = g^{\mu \nu} + \omega^{\mu\nu} + \omega^{\nu \mu} + o ( \omega^2 ) \end{aligned} \end{equation} and so : \begin{equation} \omega^{\mu\nu} = - \omega^{\nu \mu} \end{equation} thus , the matrix $\omega$ is a $4 \times 4$ antisymmetric matrix , which corresponds to $6$ independent parameters ( i.e. . the $3$ parameters corresponding to boosts and the $3$ parameters corresponding to rotations ) .
i ) two square matrices $a$ and $b$ are similar matrices if they are connected via a relation $$\tag{1}ap~=~pb$$ for some invertible matrix $p$ . ii ) two square matrices $a$ and $b$ are unitarily similar matrices if $p$ in eq . ( 1 ) is a unitary matrix .
for an ( idealized ) perfectly round wheel on a perfectly smooth road , there is only a single point of contact between the wheel and the road at any given time . if you were to plot the motion of a single point on the wheel 's surface as it goes around and then touches the ground , you would see that it follows a curve called a cycloid . the picture in that wikipedia article explains it better than i possibly could . ( * ) as you can see from the image , the point on the wheel 's surface is actually changing directions as it touches the road , so at that point in time its instantaneous velocity is zero . because it is stationary relative to the road , there is no kinetic friction . however , there can still be static friction , such as if you are driving the car around a curve . in that case , it is static friction on the wheel that prevents you from slipping and keeps you following the curved path . ( or static friction plus a contribution from gravity if the curve is banked . ) there is also static friction between the wheels and the road that causes the car to accelerate in the first place . ( i am assuming it starts from rest . ) if friction between wheels and ground were zero , the wheels would spin in place but the car would never go anywhere . ( * ) the picture makes it very clear , but if you prefer a verbal explanation : the wheel as a whole is moving forward ( relative to the road ) , but when the point on the wheel 's surface is at the bottom of its rotation , it is moving backward relative to the center of the wheel . the result is that the point on the surface of the wheel is stationary ( relative to the road ) when it is at the bottom of its rotation .
a plasma sheath is the interaction of a plasma with a boundary . as long as you let enough time for the plasma to be created , you let enough time for the plasma to create a self-consistent electric field in the sheath . in stationary plasmas , the only case when there is no sheath , is when the surface that the plasma is in contact to , is at the plasma potential ( i.e. . same potential as the plasma ) , which means you have to supply current to this surface ( in langmuir probes in dc glow discharges , for example ) in order to compensate for the fact that electrons arrive more quickly - and hence , at a higher rate - than positive ions , to the boundary , producing a net negative charge flux and thus a current . and in this case , there is no electric field either . in all other cases , the sheath is necessarily accompanied by an electric field of its own , because the electric field is what allows the sheath to exist , as an entity with different properties and structure than the plasma bulk . if you increase or decrease the electric field applied to your discharge , the sheath will change , and adapt to that field . basically , if you try to increase the voltage in a dc glow discharge , the cathode sheath will first cover all of the cathode area , then the current across the sheath will increase to allow the potential drop across the sheath to increase . then the cathode will heat up during the transition to arc , and the sheath will change to take into account the increase ion and electron emissivity of the cathode . all these processes are rather complex and depend highly on the situation . so i think one might say : sheath and electric field are bound together by many relations , so it is difficult to answer without knowing a little more about your particular case !
this is sort of the same as anna 's answer , but i would like to put a slightly different spin on it . as anna points out , there are two different co-ordinate systems involved : one for the observer sitting on earth and one for an observer in the freely falling spaceship , and the situation looks very different for the two observers . each observer can ( in principle ) measure the stress energy tensor then solve the einstein equation to give the curvature tensor . the key thing to note is that these tensors are co-ordinate independent i.e. both observers will calculate the same stress energy and curvature tensors . however , although the tensors are co-ordinate independent the representations of them in the two co-ordinate systems will be different . we normally write the tensors as a 4 x 4 matrix , and the two different observers will calculate different values for the elements in the matrices because they are using different bases . so it is not correct for the observer in the spaceship to think the galaxy is somehow being deflected by his gravity . actually strictly speaking it is also incorrect for the observer on earth to think the spaceship is being deflected by the sun 's gravity . the gravity , i.e. the curvature , is not attached to any particular body . the solar system and the spaceship together ( and in principle the rest of the universe ) produce a curvature then both of them move in response to that curvature . the difference seen by the observers is purely down to them using different bases to represent the tensors .
it is an intrinsic property of our universe . there were some alternative interpretations , like the " hidden variables " ( there are a swarm of deterministic random things going on that we do not know or cannot know about that cause the quantum randomness ) but they have been experimentally disproven ( bell 's theorem ) . you have a nice list of the experiments here .
there is one formula relating the speeds of any two " platforms " ( say $p$ and $q$ ) between each other : $$v_{p} [ q ] = v_{q} [ p ] . $$ and there is of course the well known symbol for " speed of light ( in vacuum ) " , as determined of light signals exchanged by members of any one platform between each other : $c$ . the speed of any one platform ( $q$ ) as determined by members of any other platform ( $p$ ) can thereby be expressed ( and this will be found convenient below ) as $$v_{p} [ q ] = c \ , \ , \beta_{p} [ q ] $$ with the appropriate real number value $\beta_{p} [ q ] $ . correspondingly of course : $$\beta_{p} [ q ] = \beta_{q} [ p ] . $$ and then there is one formula relating the ( pairwise ) speeds , or for simplicity rather the corresponding $\beta$-numbers , of any three " platforms " ( say $g$ , $h$ , and $j$ ) between each other : $$1 - \left ( \ , \beta_{h} [ g ] \ , \ , \ , \beta_{h} [ j ] \ , \ , \ , \text{cos} [ \angle_{h} [ g , j ] ] \ , \right ) = \sqrt{ \frac{ ( 1 - \beta^2_{h} [ g ] ) \ , ( 1 - \beta^2_{h} [ j ] ) }{ ( 1 - \beta^2_{g} [ j ] ) } } . $$ in case $\text{cos} [ \angle_{h} [ g , j ] ] = -1$ which should be applicable to the " stack of platforms " described in the question this simplifies to the surely familiar formula $$\beta_{g} [ j ] = \frac{ ( \beta_{h} [ g ] + \beta_{h} [ j ] ) }{ ( 1 + \beta_{h} [ g ] \ , \ , \beta_{h} [ j ] ) } , $$ or likewise : $$\beta_{g} [ j ] = \frac{ ( \beta_{g} [ h ] + \beta_{h} [ j ] ) }{ ( 1 + \beta_{g} [ h ] \ , \ , \beta_{h} [ j ] ) } . $$ now , this formula can be applied to the speed values given in the question $v_1 := \beta_1 \ , c$ , $v_2 := \beta_2 \ , c$ , . . . $v_k := \beta_k \ , c$ and so on ; where $v_1$ is the speed of the bullet wrt . the first platform , $v_2$ the speed of the second platform wrt . the first , and so on ; successively in the proposed " russian doll " setup . of particular interest are surely the resulting speed value $v_{ [ 0 , k ] }$ ( or the corresponding real number $\beta_{ [ 0 , k ] }$ ) of the bullet wrt . the $k$ th platform . with the above formula follows $$ \beta_{ [ 0 , ( k + 1 ) ] } = \frac{ ( \beta_{ [ 0 , k ] } + \beta_k ) }{ ( 1 + \beta_{ [ 0 , k ] } \ , \ , \beta_k ) } $$ noting the similarity of this formula to the addition theorem of the " hyperbolic tangent " function $\text{tanh}$ , $$\text{tanh} [ x + y ] = \frac{ ( \text{tanh} [ x ] + \text{tanh} [ y ] ) }{ ( 1 + \text{tanh} [ x ] \ , \ , \text{tanh} [ y ] ) } , $$ we obtain $$ \beta_{ [ 0 , ( k + 1 ) ] } = \text{tanh} [ \text{arctanh} [ \beta_{ [ 0 , k ] } ] + \text{arctanh} [ \beta_k ] ] $$ . applying this to all ( "$n$" ) given speed values ( or corresponding $\beta$ values ) then $$ \beta_{ [ 0 , n ] } = \text{tanh} [ \sum^n_{j = 1} \text{arctanh} [ \beta_j ] ] , $$ or correspondingly $$ v_{ [ 0 , n ] } = c \ , \ , \text{tanh} [ \sum^n_{j = 1} \text{arctanh} [ \frac{v_j}{c} ] ] . $$ the values of the $\text{tanh}$ function are or course approching $1$ , but do not reach the value $1$ for any argument value ; cmp . http://commons.wikimedia.org/wiki/file:sinh%2bcosh%2btanh.svg therefore the total bullet speed ( wrt . the " final , $n$ th platform" ) that may be reached in the described " russian doll " setup with $n$ successive platforms cannot reach ( or even exceed ) the speed of light , $c$ .
so is it correct to say that magnetic field are ultimately caused by currents ? no , think of a magnetic field as a field that permeates all of space and time , existing independent of anything else . ( however an empty field does not do anything so changes in $\vec b$ field are what matter . ) the ( change in ) magnetic field can be created by currents but also by other stuff too . using the gauss 's law : $$ \nabla \cdot \vec b = 0 $$ we can see that the magnetic field does not have any divergences i.e. no sources or sinks ( mono-poles ) . you can imagine it as an incompressible fluid ( like a tank of water , the water can move but you can not create high density water or vacuum volumes ) . so we have established there are no divergences however there can be curls in the field , i.e. the field can flow , but since there are no divergences these flows must be closed loops . the equation you describe , ampère 's circuital law ( with maxwell 's correction ) , is given by : $$ \nabla \times \vec b = \mu \vec j + \epsilon \mu \frac {\partial \vec e}{\partial t} $$ this states that there are two ways to create a curl in the $\vec b$ field . the first is with a current density i.e. movement of charge which necessarily requires electric charges . so yes ( curls in ) magnetic fields can be created by electric charges . however there is a second part which states that curls in magnetic fields can also be created by $\vec e$ fields changing in time . this usually requires a charge particle but it can also be caused by a changing magnetic field . this is how light propagates , changing $\vec b$ creates a changing $\vec e$ field which creates a changing $\vec b$ etc . if magnetic mono-poles exist then they can be used to create the first changing $\vec e$ field . which eliminates the need for charges ( well they are magnetic charges ) .
what you are looking for is usually called a window , not a filter . you need to check different manufacturers to choose a material that fits your requirements , including not only light transmission but also the pressure differential , chemical inertness , temperature range , etc . checking ars ' website , calcium fluoride ( caf2 ) might be what you are looking for .
temperature is related to average energy per degree of freedom via the equipartition theorem . for example , as the kinetic energy is quadratic in the velocity and corresponds to three degrees of freedom ( the three spatial directions ) , on average each molecule will have a kinetic energy of $\frac32k_bt$ where $k_b$ is the boltzmann constant . this means that temperature and energy are indeed intimately related ( temperature can be considered a measure of average energy ) and thus there are so-called natural systems of units which set $k_b=1$ . this means that temperature will have the same unit as energy , eg electron volt in case of particle physics . while the equipartition theorem probably provides the most intuitive visualization of temperature , it also has its problems : as arnold neumaier points out , it only holds under specific assumptions and in particular breaks down in case of non-ergodic systems or in cases where continuity is no longer a good approximation for quantized energy levels . an example of such a system would be a non-atomic gas , which adds quantized internal degrees of freedom to the mix . the heat capacity of diatomic gases provides a good illustration for this as it can be derived classically via the equipartition theorem . a reasonably good explanation of the measurements is that ' quantum mechanical ' degrees of freedom do not contribute at low temperatures and start approching the classical contribution as temperature increases . rotational degrees of freedom contribute almost fully at room temperature , whereas the vibrational degrees of freedom only contribute for heavier molecules as the spacing of the vibrational energy levels depends on the reduced mass of the system .
incandescent bulbs are tremendously inefficient in producing visible light . if you model the filament as a black body at 3000 c you will see that the majority of light emitted is in the infrared . cfls , on the other hand operate through the principle of fluorescence rather than incandescence , which means that less of the energy they consume is put out as heat . addendum : for some more concrete figures on just how much energy incandescents waste as heat , check these wikipedia page sections : incandescent light bulb efficiency and luminous efficacy examples . it would perhaps be more fitting to call them ' heat bulbs ' . [ edit : implemented mark booth 's comment . ]
i do not have a solution , but i have objections ; ) lets first consider the category given . you work with one object ( lets denote it $x$ ) which is a set with two elements $x = \{\text{on} , \text{off}\}$ . you have some morphisms ( the identity which you call " do nothing " plus the following ) : $$ \text{flip}\colon x\to x $$ so there are two morphisms . but look , we expect $$ \text{flip} ( \text{on} ) =\text{off}\quad\mbox{and}\quad \text{flip} ( \text{off} ) =\text{on}$$ in other words , the flip morphism is an automorphism . so you have the collection of morphisms be precisely the cyclic group with two elements ! we can improve the situation by " vertical categorification " . this sounds scary , but what happens is we promote $x$ to be a category now , and things get a little better . how to categorify the situation ? first we make " on " and " off " objects , lets call them $a$ and $b$ respectively . then we have an invertible morphism which " flips on " the switch : $$ \text{flip}\colon a\to b $$ its inverse would flip off the switch $$ \text{flip}^{-1}\colon b\to a$$ if we add another object $\omega$ which is intuitively a set with two elements ( true or false ) , then we can introduce a morphism that checks if the lights are on . that is , we have one morphism $$ f\colon a\to\omega $$ which checks if the lights are on while the switch is flipped on . we have another morphism $$ g\colon b\to\omega $$ which checks if the lights are on while the switch is flipped off . if we assume that the wiring is correct , and the light is off when the switch is flipped off , then $g$ factors through a terminal object ( $g$ will always have " false " as its output ) . this might allow us to suggest there is a $g^{-1}$ function . can we say this ? no ! if we can , then we have $b$ be isomorphic to $\omega$ , and the flip morphism is an isomorphism . so that would imply that $\omega$ is isomorphic to $a$ . . . if we allow this , then flipping the switch on would be " the same " as the lights being on . this may be too much for your model ( what if the light burns out ? ) .
using gradshteyn and ryzhik ( seventh edition ) 3.876 ( 1 ) $$\int_0^\infty \frac{\sin{ ( p \sqrt{x^2+a^2} ) }}{\sqrt{x^2+a^2}} \cos ( b~x ) dx=\frac{\pi}{2} j_0 ( a\sqrt{p^2-b^2} ) ~~ [ 0&lt ; b&lt ; p ] \\ = 0~~ [ 0&lt ; p&lt ; b ] $$ differential of both sides with respect to $b$ will give the integral you want to calculate .
let 's set $c=1$ for simplicity . using your observations , it suffices to show that ( just combine the second and third equations you write down ) $$ \dot \gamma = \vec u \cdot \frac{d}{dt} ( \gamma \vec u ) . $$ to prove this , the following facts are useful : $$ \dot \gamma = \gamma^3\vec u \cdot\dot{\vec u} , \qquad \gamma^2\vec u^2 +1 = \gamma^2 . $$ now just compute \begin{align} \vec u \cdot \frac{d}{dt} ( \gamma \vec u ) and =\vec u \cdot ( \dot \gamma \vec u + \gamma \dot{\vec u} ) \\ and =\vec u \cdot ( \gamma^3 ( \vec u \cdot \dot{\vec u} ) \vec u + \gamma \dot{\vec u} ) \\ and = \gamma \vec u \cdot \dot{\vec u} ( \gamma^2 \vec u^2 + 1 ) \\ and = \gamma^3\vec u \cdot\dot {\vec u} \\ and = \dot \gamma \end{align}
it seems to me that you are looking for the boltzmann transport equation : $$ \frac{\partial f}{\partial t}+\frac{\mathbf p}{m}\cdot\nabla f+\mathbf f\cdot\frac{\partial f}{\partial\mathbf p}=q+\left ( \frac{df}{dt}\right ) _{\rm coll} $$ with $f$ the distribution in phase-space , $\mathbf p$ the particle momentum , $q$ some source term , and the rhs an interaction term based on collisions . here we can use $\mathbf f=-\nabla\phi$ for some potential $\phi$ . there is also a fokker-planck equation that uses the lorentz force for $\mathbf f$ but is otherwise the same . the fokker-planck equation without the collision term is called the vlasov equation .
wolfram 's early work on cellular automata ( cas ) has been useful in some didactical ways . the 1d cas defined by wolfram can be seen as minimalistic models for systems with many degrees of freedom and a thermodynamic limit . insofar these cas are based on a mixing discrete local dynamics , deterministic chaos results . apart from these didactical achievements , wolfram 's work on cas has not resulted in anything tangible . this statement can be extended to a much broader group of cas , and even holds for lattice gas automata ( lgas ) , dedicated cas for hydrodynamic simulations . lgas have never delivered on their initial promise of providing a method to simulate turbulence . a derivative system ( lattice boltzmann - not a ca - has some applications in flow simulation ) . it is against this background that nks was released with much fanfare . not surprisingly , reception by the scientific community has been negative . the book contains no new results ( the result that the ' rule 110 ca ' is turing complete was proven years earlier by wolfram 's research assistant matthew cook ) , and has had zero impact on other fields of physics . i recently saw a pile of nks copies for sale for less than $ 10 in my local half price books store .
this link shows the massive calculation of the sunset diagram which is the name of the diagram you want to look at . the massless limit is simple . i suspect this question will be closed soon for being too specific and not relating to any physics concepts , though . . .
as lurscher mentioned in a comment , you are using the wrong units for magnetic susceptibility . $\chi$ is actually a dimensionless number that is related to the magnetic permeability of a material relative to that of a vacuum . i think you were mixing it up with the molar magnetic susceptibility , which is $\chi_\text{mol} = \mathcal{m}\chi/\rho$ , where $\mathcal{m}$ is the molar mass of the substance ( units of $\mathrm{kg/mol}$ ) and $\rho$ is the density ( units of $\mathrm{kg/m^3}$ ) . $\chi_\text{mol}$ is the thing with units of $\mathrm{m^3/mol}$ , but it is $\chi$ that actually appears in the magnetic levitation formula . with that cleared up , let 's look at the equation . the left side , naturally , has units of $\mathrm{t^2/m}$ . if you include the magnetic constant on the right side , as wikipedia ( correctly ) does , you have $$\biggl [ \mu_0\frac{ \rho g }{\chi}\biggr ] = [ \mu_0 ] \frac{ [ \rho ] [ g ] }{ [ \chi ] } = \biggl ( \frac{\mathrm{t\ , m}}{\mathrm{a}}\biggr ) \frac{\mathrm{ ( kg/m^3 ) ( m/s^2 ) }}{1} = \frac{\mathrm{t\ , kg}}{\mathrm{m\ , s^2\ , a}}$$ here i am using the notation where putting brackets around a quantity designates the units of that quantity . for example , the units of the magnetic constant are $\mathrm{t\ , m/a}$ , so $ [ \mu_0 ] = \mathrm{t\ , m/a}$ . now you can equate the units of the two sides of the equation : $$\frac{\mathrm{t^2}}{\mathrm{m}} = \frac{\mathrm{t\ , kg}}{\mathrm{m\ , s^2\ , a}}$$ which simplifies to $$\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$$ so if this equivalence is correct , then it shows that the original equation is dimensionally consistent . and if you look on the wikipedia page for the tesla , it does indeed give $\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$ as one of the definitions of that unit . alternatively , you could check it using a formula involving magnetic field and current , such as $\vec{f} = i\mathrm{d}\vec{l}\times\vec{b}$ . the units of this are $\mathrm{n = a\ , m\times t}$ , and since $\mathrm{n} = \mathrm{kg\ , m/s^2}$ , you can set $\mathrm{kg\ , m/s^2 = a\ , m\ , t}$ and find exactly that $\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$ . this is a useful trick that keeps you from having to memorize the definitions of all the si ( or other ) units .
the reflector focuses the light from the bulb which is a point source to a straight line .
the no-go results from algebraic and constructive qft you mention deal with related but slightly different matters . ( edit : the previous version of the following paragraph was slightly misleading - haag 's theorem is actually stronger than i stated before ; see below for details ) haag 's theorem ( which actually slightly predates the inception of algebraic qft ) tells us that we cannot write interaction picture dynamics within hilbert spaces which are free field representations of the ccr 's . this is not the same as to say that interacting dynamics does not exist at all - it simply says that we cannot implement it as unitary operators in the interaction picture . this is done by showing that the possibility to do so in some hilbert space does imply that we are dealing with a free field representation of the ccr 's . the argument is closed by a " soft triviality " result by jost , schroer and pohlmeyer arguing that the latter implies that all truncated $n$-point functions vanish for $n&gt ; 2$ , hence the field is really free - in particular , the " interaction hamiltonian " is zero . this has consequences for both scattering theory and attempts to rigorously construct field theoretical models starting from free fields . in the first case , haag 's theorem is circumvented by either the lsz of haag-ruelle scattering formalisms , which obtain the s-matrix by respectively taking infinite time limits in the weak ( matrix elements ) and strong ( hilbert space vectors ) sense . recall that both setups require the assumption of a mass gap in the joint energy-momentum spectrum ( i.e. . an isolated , non-zero mass shell ) , otherwise we run into the notorious " infrared catastrophe " , which is dealt with using " non-recoil " ( i.e. . bloch-nordsieck ) approximation methods in formal perturbation theory but remains a challenge in a more rigorous setting , save in some non-relativistic models . in the second case , one is led to consider representations of the ccr 's which are inequivalent to free field ones . since field theories living in the whole space-time have infinite degrees of freedom , the stone-von neumann uniqueness theorem no longer holds ( actually , haag 's theorem can be seen as a manifestation of this particular failure mechanism ) , and hence such representations should exist in abundance . motivated by these results , algebraic qft was devised with a focus on structural ( i.e. . " model-independent" ) aspects of qft in a way that does not depend on a particular representation ; on other front , one may also try to explore this abundance of representations to construct models rigorously , which brings us to the realm of constructive qft . the " existence " ( a . k . a " non-triviality" ) and " non-existence " ( a . k.a. " triviality" ) results in constructive qft tell us which interactions survive after non-perturbative renormalization . more precisely , you construct field theoretical models in a mathematically rigorous way by first considering " truncated " interacting theories ( i.e. . with uv and ir cutoffs ) , and then carefully removing the cutoffs in a sequence of controlled operations . the resulting model may be interacting ( i.e. . " non-trivial" ) or not ( i.e. . " trivial" ) , in the sense that its truncated $n$-point correlation functions for $n&gt ; 2$ may be respectively non-vanishing or not . in the first case , any representation of the ccr 's in the hilbert space where the interacting vacuum state vector lives is necessarily inequivalent to a free field one - in particular , one cannot write the interacting dynamics as unitary operators in the interaction picture , in accordance with haag 's theorem . in the second case , you really obtain a free field representation of the ccr 's , but here because renormalization has completely killed the interaction . finally , it is important to notice that triviality of a model may stem from reasons unrelated to the underlying mechanism of haag 's theorem . the latter , once more , is a consequence of having an infinite number of degrees of freedom in infinite volumes ( this theorem does not hold " in a box " , for instance ) , whereas the former usually derives from an interaction which has too singular a short-distance behavior , as argued in the previous paragraph . this can be intuitively be understood by the ( local ) singularity and ( global ) integrability of the free field 's green functions : the lower the space-time dimension , the better the singular ( uv ) behaviour and the worse the integrability ( ir ) behaviour , and vice-versa . that is the underlying reason why $\lambda\phi^4$ scalar models are super-renormalizable in 2 and 3 dimensions ( having only tadpole feynman graphs as divergent in 2 dimensions ) and non-perturbatively trivial in $&gt ; 4$ dimensions . ah , i have almost forgotten about the references : in my opinion , the best discussion of triviality results in qft from a rigorous viewpoint is the book by r . fernández , j . fröhlich and a . d . sokal , " random walks , critical phenomena , and triviality in quantum field theory " ( springer-verlag , 1992 ) , specially chapter 13 . there both the above " hard triviality " results for $\lambda\phi^4$ models and " soft triviality results " such as the jost-schroer-pohlmeyer theorem ( which underlies haag 's theorem , as mentioned at the beginning of my answer ) are discussed . the book is not exactly for the faint of the heart , but the first sections of this chapter provide a good discussion of the statements of the theorems , before proceeding to the proofs of the above " hard triviality " results . for a detailed discussion of jost-schroer-pohlmeyer 's and haag 's theorems , as well as their proofs , i recommend the book of j . t . lopuszanski , " an introduction to symmetry and supersymmetry in quantum field theory " ( world scientific , 1991 ) . the classic book of r . f . streater and a . s . wightman , " pct , spin and statistics , and all that " ( princeton univ . press ) also discusses these two results .
i am on record of having the opinion that there is no real argument against us being a simulation in a general sense , however we frequently find people jumping to quick into the simulation pool and stating there new what-ever-it-is proves the universe is a simulation . the example given above sounds like one of them . first off , quantum error correcting code ( qecc ) are mathematical approaches to allow for stable transfer of quantum information by correcting for decoherence effects . if some version of qecc is apparent in any formulation of quantum mechanics , it is interesting but probably not very meaningful in proving we exists in some sort of emulation . second , just because it shows up in one theory , unless that particular version is shown to have the ability to predict physical effects then it is hard to make the claim about its relevance . whether these things are testable is a matter of debate . however , there are people who are proposing to look for " glitches " in the universe . some would hypothesize that if we lived in a simulation based on lattice quantum chromodynamics ( lqcd ) we should be able to find places where the lattice work becomes apparent . this is clearly far-fetched but who am i to judge ? for the world of warcraft , although i do not play that game , the first evidence of a simulation is along the same lines as the theory to test for lqcd latticework . the pixelation of the characters would be the first indication of potential emulation . the universe as we know it has a continuous spacetime ( versus discrete ) , so any sign of blockiness is a good indicator . generally , anything that is an inconsistency with basic laws of physics ( e . g . perpetual motion , decreasing entropy , etc ) would be the first indicator something was wrong . now , in wow one can assume that they might operate under a slightly different set of physics than our real world . so ultimately inconsistencies in some portion of the world relative to the laws of physics would be a flag . something you should look into is the equivalence principle . in a nutshell it is a statement that the laws of physics should be the same regardless of you location in spacetime . it is very critical to our notion of the world around us , but a similar rule should be applicable wow , and significant inconsistencies would be cause for exploration .
http://www.nature.com/ncomms/journal/v2/n4/full/ncomms1263.html above is a reference with interference of 6 nm molecules achieved here is a proposal to use much larger molecules in space : http://arxiv.org/abs/1201.4756 here is a proposal to use larger molecules in an optical interfereometer : http://arxiv.org/abs/1103.4081
not only is the constant nature of the speed of light guaranteed by theory , it is also shown experimentally . in fact , as you may know , it was the experimental discovery that the speed of light is constant irrespective of the ( inertial ) frame of reference which formed the inspiration for the development of special relativity by albert einstein . mathematically , purely from the relativistic formula for velocity-addition it can be seen that the light would still travel at a speed $c$ in a vacuum . indeed , say the astronaut has a speed of $-v$ with respect to the ground . he observes the light leaving his flashlight with a speed of $u = +c$ ( with respect to him ) . $^1$ the relativistic theory then tells us the light is travelling at a speed $s$ with respect to the ground , given by $$\begin{align} s and = \frac{ ( -v ) + ( +c ) }{1+\frac{ ( -v ) ( +c ) }{c^2}} \\ \\ and = \frac{c-v}{1-\frac{v}{c}} \\ \\ and = \frac{c-v}{\frac{c-v}{c}} \\ \\ and = \frac{c-v}{c-v}c \\ \\ s and = c . \end{align}$$ you can walk at whatever speed you like , you will never get a different result . except if you insert $-c$ instead of $-v$ , then the answer is undefined - however , you do still get $c$ if you calculate it using limits . $^1$ why can we say this ? well , remember that in the reference frame of the astronaut , he himself is not moving at all and he would expect the light to leave his flashlight at a speed of $c$ ( w . r.t. him ) . this becomes clearer when you replace the flashlight with a small cannon and the light with a little ball . suppose you can set the exit speed for this ball out of the cannon . say you set it at $v$ . when you walk around carrying this small cannon , you expect the ball to still leave the cannon at a speed $v$ with respect to you when you fire it . classically , if you’re walking at a speed $u$ with respect to the ground , the ball will leave the cannon at a speed $u+v$ with respect to the ground . relativistically , the only thing that is invalid in all the above is this simple summation of speeds . you need the formula i used in the main text of this post , which reduces to the simple summation if both $u$ and $v$ are much smaller than $c$ . so it’s perfectly alright to say that the light leaves the astronaut’s flashlight at a speed $c$ w . r.t. the astronaut , even without knowing about the constancy of the speed of light in a vacuum .
op is formally correct that $$\frac{\partial}{\partial x^{\mu}}~\in~ \gamma ( tm|_{u} ) $$ is a vector field ( defined in a local coordinate neighborhood $u$ ) , and not a one-form . what weinberg simply means by casually saying that the partial derivative operator $\partial/\partial x^\mu$ is a covariant vector , or in other words a 1-form , [ . . . ] is just that the local basis of vector fields $$\tag{1} \frac{\partial}{\partial x^{\mu}} ~=~\frac{\partial y^{\nu}}{\partial x^{\mu}} \frac{\partial}{\partial y^{\nu}}$$ transforms in the same way as the components $$\tag{2} \eta^{ ( x ) }_{\mu}~=~\frac{\partial y^{\nu}}{\partial x^{\mu}}\eta^{ ( y ) }_{\nu} $$ of a 1-form/covector [ or a covariant ( 0,1 ) tensor ] $$\eta~=~\eta^{ ( x ) }_{\mu} dx^{\mu} ~=~\eta^{ ( y ) }_{\nu} dy^{\nu}$$ under a local coordinate transformation $x^{\mu} ~\to~ y^{\nu}=y^{\nu} ( x ) $ . the point is that the ( traditional ) physicist often thinks of a $ ( r , s ) $ tensor $$t~=~ \frac{\partial}{\partial x^{\mu_1}}\otimes \cdots \otimes \frac{\partial}{\partial x^{\mu_r}} ~t^{\mu_1\ldots\mu_r}{}_{\nu_1\ldots\nu_s} ~ dx^{\nu_1}\otimes \cdots \otimes dx^{\nu_s}$$ merely in terms of its components $t^{\mu_1\ldots\mu_r}{}_{\nu_1\ldots\nu_s}$ , and in particular , the transformation property thereof under local coordinate transformations . local basis elements , such as , e.g. , $\frac{\partial}{\partial x^{\mu}}$ and $dx^{\nu}$ are often viewed as merely bookkeeping devices . in conclusion , ref . 1 is probably not the best textbook to learn differential geometry from . for instance , already in eq . ( 4.11.12 ) on the very next page 116 weinberg claims that the fact that the exterior derivative squares to zero , $d^2=0$ , is known as poincare 's lemma . this is definitely not correct , cf . wikipedia : the identity $d^2=0$ means that exact forms are closed , while poincare 's lemma states that the opposite holds locally : closed forms are locally exact ( except for zero-forms ) . references : s . weinberg , gravitation and cosmology , 1972 .
edited : indeed , the flat moon would provide 50% more illumination than the round moon . i assumed that it is full moon now , so the sun illuminates the moon , and we are on axis $z$ connecting the moon and the sun ( the origin is at the center of the moon ) . if $\theta$ is the angle between axis $z$ and some direction , and illumination ( total light energy per area - both incident and reflected , as there is no absorption ) on the surface of the ( round or flat ) moon equals $a$ for $\theta=0$ . then illumination on the surface of the round moon for some $\theta$ equals $a\cos ( \theta ) $ , as the same sun light energy falls on a larger area . therefore , the lambertian light intensity in the direction $z$ will be proportional to $a\cos^2 ( \theta ) $ . therefore , we need to compare an integral of $a$ over a unit circle with an integral of $a\cos^2 ( \theta ) $ over the surface of the unit hemisphere . ( initially , i integrated $a\cos^2 ( \theta ) $ over the unit circle , rather than the unit hemisphere , and offered a wrong result here ) .
this is a really rough calculation that does not take into account the realistic direction of the bow shock , or calculation of the drag force . i just take the net momentum flow in the solar wind and direct it so as to produce the maximum decceleration and see what happens . apparently the solar wind pressure is of the order of a nanopascal . as i write this it is about $0.5\ \mathrm{npa}$ . you can get real time data from nasa 's ace satellite or spaceweather . com ( click through " more data " under " solar wind" ) . during periods of intense solar activity it can get up to an order or magnitude or so more than this . let 's take this worst case and assume , unrealistically , that all of the pressure is directed retrograde along the earth 's orbit . this will give the maximum deccelerating effect . i get a net force of $\sim 10^6\ \mathrm{n}$ . dividing by the earth 's mass gives a net acceleration $2\times 10^{-19}\ \mathrm{m/s^2}$ . let 's fudge up again and call it $10^{-18}\ \mathrm{m/s^2}$ . the time it would take for this to make a significant dint the the earth 's orbital velocity ( $30\ \mathrm{km/s}$ ) is of the order of $10^{15}\ \mathrm{yr}$ . i think we are safe . for the other planets there is a $1/r^2$ scaling of the solar wind with the distance from the sun ( assuming the solar wind is uniformly distributed ) and an $r^2$ scaling with the size of the planet . so for mercury the former effect gives an order of magnitude increase in drag and the latter effect takes most of that increase away again . there is an additional $r^{-3}$ increase in effect due to the decreased mass of a smaller body ( assuming density is similar to the earth ) . then there is the $r^{-1/2}$ increase in orbit velocity due to being closer to the sun . so the total scaling factor for the time is $ r r^{3/2} $ , which for mercury is about 0.1 . so the end result is not much different for mercury . this site always causes me to learn new mathematica features . it made really quick work of this since it has all sorts of astronomical data built in : note that the number of digits displayed in the final column is ludicrous . : )
i cant speak in the case of full generality , but at least for schwarzschild and reissner-nordstrom ( electrically charged ) non-rotating black holes , there exists a coordinate system called ' isotropic coordinates' ; it turns out that in these coordinate systems the ' interior ' and ' exterior ' regions are actually isomorphic ; this means that representing the solution in these coordinates gives rise to two identical yet causally disconnected regions of space-time . these two space-times share a common boundary , namely the event horizon . as such , the two regions can be thought of representing the same physics and the answer to your question is then a definite ' yes': the information contained within the ' interior ' section also amasses on the horizon .
the quantum number $n$ of the harmonical oscillator runs from $0$ to $\infty$ . ( your sum starts at 1 . ) $\sum_{n=0}^{\infty} e^{-\theta ( n+1/2 ) } = \frac{e^{-\theta/2}}{1-e^{-\theta}} = \frac{e^{\theta/2}}{e^\theta - 1} = \frac{1}{e^{\theta/2}-e^{-\theta/2}}$ . i guess there just is an error in your exercise . ( tas make mistakes , too . ) the faq says no homework questions . let 's hope they do not tar and feather us . ; - )
the rule of thumb for me is how would the solutions of the schrodinger equation look for this potential . when the energy of the electron is above the potential well the solutions can be continuous ( although there can be discrete resonances with an energy width ) . below the lip of the potential , as for example in the hydrogen atom , the solutions will be discrete , quantized . if the walls of the potential go to infinity , as in your example , then all the solutions will be quantized as in this example of the particle in a box . in the limited in energy potential ( not going to infinity ) the energy levels with large energy quantum numbers are very dense and resemble a continuum , although they can be labeled with a discrete number , as for example in the hydrogen atom . above the energy of the well are the continuum solutions , not discrete .
kirchhoff 's loop rule is also called kirchhoff 's voltage law ( kvl ) . which is different from kirchhoff 's current rule which is also called kirchhoff 's current law ( kcl ) . kvl is derived from maxwell–faraday equation for static magnetic field ( i.e. . the derivative of b with respect to time is zero ) . kcl is derived from charge continuity equation which is equation 3 here . a well known case in which kvl does not apply is when having a varying magnetic field enclosed by the circuit being studied . the presence of time varying magnetic field makes the measured voltage non-unique ( depends on the branch used to measure the voltage ) . have a look at page 3 of this presentation . a well known case in which kcl is limited is when having a voltage source with very high frequency such that effects like parasitic capacitance can no longer be ignored . in those cases wires ( or conducors ) are treated as transmission lines . in such a case a current can flow even in an open circuit . . for further information have a look at limitations sections of kcl and kvl here .
there is actually at least one very big clue that is been accessible to skygazers since the earliest times : the first quarter moon at dusk . every child in the northern hemisphere going back to 30,000 bce likely would have been familiar with how 1st-quarter moons always tend to rise at noon , reach its highest point at sunset ( with an azimuth directly south ) , and set at midnight . form a triangle out the observer , the sun and the moon : $\triangle osm . $ the only angle the observer can measure directly is of course the angle between the sun and moon , the observer forming the vertex . the sun is in the direction of the horizon , and the 1st-quarter moon is near zenith , hence $\angle som \approx 90° . $ the angle with vertex at the moon , $\angle oms$ , could not be measured in general , but it does not take too much imagination to infer that the shape of the sunlit portion of a 1st-quarter moon results whenever $\angle oms \approx 90°$ . hence , $\triangle osm$ is an acute , nearly isosceles right triangle , whose legs are practically parallel and much , much greater in length than the base . this small base length is the earth-moon distance $|om|$ is itself much greater than any terrestrial distances we measure on the earth 's surface . thus , with extremely little effort we can be reasonably confident that eratosthenes ' condition of parallel sunlight rays holds to good enough approximation for the purpose of his measurements ( uncertainties in the measurements of distances between cities would have been the limiting factor towards overall precision anyway ) .
the wind is certainly doing work , because it applies a force and the point where the force is applied is displaced . however it is not doing any work on the boat , it is doing the work on the water . the key point is that the net force on the boat is zero . we know the net force on the boat is zero because the boat is moving at constant velocity - if the net force were non-zero the boat would be accelerating . since the net force on the boat is zero no work is being done on the boat . the velocity of the boat is constant because the drag of the water is balancing out the force applied by the wind . so overall the wind is applying a force to the water - the boat is just the instrument through which the force from the wind is communicated to the water . so the wind is doing work on the water , but not on the boat .
you are correct that it is impossible to change the frequency of every component of a waveform while ( a ) preserving all the phase relationships between the frequencies and ( b ) preserving the length of the waveform . however , while it is mathematically impossible to do this precisely , it is possible to do things that sound quite a lot like it to the human ear . the most basic algorithms for this are fairly simple , but doing it well is pretty difficult , and the algorithms are being improved all the time . it is referred to as " pitch shifting " if the goal is to change the pitch but not the time , and " time stretching " if you want to change the time while keeping the pitch . the reason this is possible is that the ear distinguishes rhythm and pitch as quite distinct things . a sequence of clicks will sound like a sequence of clicks until its frequency passses about 20-30 hz , at which point it transitions into a buzzing sound that is perceived as having a pitch . because of this , as dumpsterdoofus said , you can chop the sound up into little segments of short duration , change the pitch of each one individually , and then stitch them back together again , repeating things or leaving gaps where necessary . by itself this does not sound so good , because you get a click at the end of each segment , where the waveforms do not line up . it can be improved a lot by overlapping " windows " of sound , which each have a fade-in and fade-out applied . that way you do not hear the clicks that the ends of the segments , but you do tend to hear phase-cancellation effects where thr windows are added back together again . for this reason this technique requires quite a bit of tuning to get it to sound good on any given input sound . there are also fourier-domain techniques , as brandon enright said . these also usually use overlapping windows ; i guess the advantage of using the fourier domain is that you have more control over the phase relationships . ( i do not know very much about these techniques . ) a fairly recent development is the " pitch synchronous overlap-add " ( psola ) algorithm . this uses overlapping windows as well , but it synchronises the length of the windows to the pitch of the input sound . this makes it much harder for the ear to perceive the individual windows . i call it " the algorithm that ruined music " , because it is responsible for that nasty " autotune " effect that is been overused on the vocals of every pop record for the last ten years or so . however , it does also have peaceful applications , and the pitch dial on your amplifier probably uses some version of it . you can find more details about these algorithms on wikipedia .
have these super-dense states been replicated by third parties ? no , i do not think there has been anything published in any reputable journal claiming to have reproduced holmlid 's supposed experimental discovery of ultradense deuterium . if there had been , it would have been big news . he is active in the cold fusion community , so it would not be surprising if other cold-fusion kooks did similar experiments and presented them at true-believer conferences , etc . holmlid is basically a one-man echo chamber who tirelessly pushes his crackpottery in online venues such as wikipedia and physics . se . although he has managed to get his articles published in journals , a literature search showed that out of 2154 references to his papers , 1863 were self-citations . extraordinary claims require extraordinary evidence . holmlid 's claims , about both " rydberg matter " and " ultradense deuterium , " are extraordinary , and there is no evidence for them from any reputable experimentalist . by the way , he has another , more recent paper claiming laser-induced fusion in ultradense deuterium : http://arxiv.org/abs/1302.2781
if all you have is the data there is not a lot more you can do . i doubt the difference in fit between the $v:d$ and $v:d^2$ fits is significant . i note that neither fit goes through the origin , which makes me suspect that neither fit captures the physics behind the data . you need to have a look at your system and see if you can write down some ( maybe approximate ) equation to model it is behaviour . i would guess that you have some system with friction/drag involved , so the acceleration starts out high and falls with increasing speed . in that case if you can make a guess at the relationship between velocity and drag you can write down an approximate equation of motion and then fit that to the data .
first , i have to correct you : compactifications of string theory clearly do not give you just " supergravity " – which is an inconsistent theory at short distances . they give you the full string theory , a compactified one . type iia and type iib are t-dual to each other . it means that the compactification of one on a circle $s^1$ of radius $r$ is equivalent to the compactification of the other on a circle of radius $\alpha ' / r$ where $\alpha'$ is the squared string length ( the regge slope ) or $1/r$ in some natural " string units " of length . t-duality is covered in every modern textbook of string theory and to reliably explain how it works , one would have to reproduce several chapters of an introduction to string theory here which is counterproductive . it is ultimately a symmetry because it is a parity i.e. a sign flip of $\partial_z x^9$ that is only applied to the left-movers but not to the right-movers $\partial_{\bar z} x^9$ . that is what interchanges $\int d\sigma \partial_\tau x^9$ , a total momentum component , with $\int d_\sigma \partial_\sigma x^9$ , the total winding . the left-movers and right-movers may be treated rather separately in cfts because of the conformal symmetry . you talk about the compactifications on $t^2$ which may be achieved by compactifying one more dimension . because the two $s^1$ compactifications – of type iia and type iib – were already equivalent to each other , they remain equivalent to each other if you compactify one more dimension , of course . the moduli spaces obtained by compactifying m-theory or type iia or type iib on tori are all identical – and they still have a u-duality group identifying different choices of the radii and other parameters . the u-duality group for m-theory on $t^k$ is formally $e_{k ( k ) } ( z ) $ which is only truly exceptional if $k$ is greater than five . there is no conflict with chirality because the theory with 8+1=9 large dimensions that you get by compactifying type iia or type iib on a circle is already non-chiral – after all , in an odd spacetime dimension , almost all theories are non-chiral . for example , there are no weyl ( chiral ) spinors in an odd space or spacetime dimension . the only chiral vacuum among the compactifications of m-theory , type iia , or type iib theories on tori is the uncompactified type iib string theory in 10 dimensions . everything else is non-chiral . you lose the chirality once you compactify a theory on a circle . the left-right symmetry of the lower-dimensional theory is guaranteed because it may be obtained by flipping the sign of one large ( remaining ) dimension as well as one compactified ( circular ) dimension and this flip of two dimensions is just a 180-degree rotation which is always a symmetry . so parity symmetry can not be violated by the circular compactifications .
first , the earth 's capacitance is not " huge " , less than 1f , so it would not take long to charge earth to a significant potential with moderate current . it is important , however , that earth is conductive , so , unless the terminal has the same potential as the earth , the current will be maintained in the circuit , as the circuit will be closed somehow through the earth and the plant equipment maintaining the terminal potential - it is difficult to assume " that no conductive path exists going back to the power plant ground spike " , unless earthing is very bad either at the plant or at the contact with the terminal , in which case some limited area will be charged and the current will stop . how large the current will be , depends on such things as the resistance in the circuit , including the resistance of the earthing at the plant and at the contact with the terminal . edit ( 07/13/2013 ) actually , earth 's capacitance is not just less than 1f , it is much less - about 0.0007f
you start from this $ [ p , f ( x ) ] \psi= ( pf ( x ) -f ( x ) p ) \psi$ knowing that $p=-i\hbar\frac{\partial}{\partial x}$ you will get $ [ p , f ( x ) ] \psi=-i\hbar\frac{\partial}{\partial x} ( f ( x ) \psi ) +i\hbar f ( x ) \frac{\partial }{\partial x}\psi=-i\hbar\psi\frac{\partial}{\partial x}f ( x ) -i\hbar f ( x ) \frac{\partial}{\partial x}\psi+i\hbar f ( x ) \frac{\partial }{\partial x}\psi$ from where you find that $ [ p , f ( x ) ] =-i\hbar\frac{\partial}{\partial x}f ( x ) $
before answering the question more or less directly , i would like to point out that this is a good question that provides an object lesson and opens a foray into the topics of singular integral equations , analytic continuation and dispersion relations . here are some references of these more advanced topics : muskhelishvili , singular integral equations ; courant and hilbert , methods of mathematical physics , vol i , ch 3 ; dispersion theory in high energy physics , queen and violini ; eden et . al . , the analytic s-matrix . there is also a condensed discussion of `invariant functions ' in schweber , an intro to relativistic qft ch13d . the quick answer is that , for $m^2 \in\mathbb{r}$ , there is no " shortcut . " one must choose a path around the singularities in the denominator . the appropriate choice is governed by the boundary conditions of the problem at hand . the $+i\epsilon$ " trick " ( it is not a " trick" ) simply encodes the boundary conditions relevant for causal propagation of particles and antiparticles in field theory . we briefly study the analytic form of $g ( x-y ; m ) $ to demonstrate some of these features . note , first , that for real values of $p^2$ , the singularity in the denominator of the integrand signals the presence of ( a ) branch point ( s ) . in fact , [ huang , quantum field theory : from operators to path integrals , p29 ] the feynman propagator for the scalar field ( your equation ) may be explicitly evaluated : \begin{align} g ( x-y ; m ) and = \lim_{\epsilon \to 0} \frac{1}{ ( 2 \pi ) ^4} \int d^4p \ , \frac{e^{-ip\cdot ( x-y ) }}{p^2 - m^2 + i\epsilon} \nonumber \\ and = \left \{ \begin{matrix} -\frac{1}{4 \pi} \delta ( s ) + \frac{m}{8 \pi \sqrt{s}} h_1^{ ( 1 ) } ( m \sqrt{s} ) and \textrm{ if }\ , s \geq 0 \\ -\frac{i m}{ 4 \pi^2 \sqrt{-s}} k_1 ( m \sqrt{-s} ) and \textrm{if }\ , s &lt ; 0 . \end{matrix} \right . \end{align} where $s= ( x-y ) ^2$ . the first-order hankel function of the first kind $h^{ ( 1 ) }_1$ has a logarithmic branch point at $x=0$ ; so does the modified bessel function of the second kind , $k_1$ . ( look at the small $x$ behavior of these functions to see this . ) a branch point indicates that the cauchy-riemann conditions have broken down at $x=0$ ( or $z=x+iy=0$ ) . and the fact that these singularities are logarithmic is an indication that we have an endpoint singularity [ eg . eden et . al . , ch 2.1 ] . ( to see this , consider $m=0$ , then the integrand , $p^{-2}$ , has a zero at the lower limit of integration in $dp^2$ . ) coming back to the question of boundary conditions , there is a good discussion in sakurai , advanced quantum mechanics , ch4.4 [ nb : " east coast " metric ] . you can see that for large values of $s&gt ; 0$ from the above expression that we have an outgoing wave from the asymptotic form of the hankel function . connecting it back to the original references i cited above , the $+i\epsilon$ form is a version of the plemelj formula [ muskhelishvili ] . and the expression for the propagator is a type of cauchy integral [ musk . ; eden et . al . ] . and this notions lead quickly to the topics i mentioned above -- certainly a rich landscape for research .
conservation laws typically hold only for closed systems . in this case the inner drum is losing mass so it can not be considered a closed system ; instead , you must also include the angular momentum of the outer drum . on the other hand , you are right that there is no torque on the inner drum , and that therefore its angular momentum - that of the drum itself , ignoring the sand - is also constant . together , these two constraints are enough to determine the two final velocities .
it is true that at the speed of sound , you will have a huge amount of drag . the reason is that the air in front of you has to move out of the way , and if you are moving at the speed of sound , the pressure wave that pushes the air out of the way is moving at exactly the same speed as you . so in the continuum mechanics limit , you can not push the air out of the way , and you might as well be plowing into a brick wall . but we do not live in a continuum mechanics universe , we live in a world made of atoms , and the atoms in a gas bounce off your airplane . at the speed of sound , you get a large finite push-back which is a barrier , and above this , you still have to do the work to push a mass of air out of the way equal to your plane 's cross section with ballistic particles . as you go faster , the amount of drag decreases , since the atomic collisions do not lead to a pile-up on the nose-cone . but if you look at wikipedia 's plot here , the maximum drag at the supersonic transition is only a factor of 2 or 3 higher than the drag at higher supersonic speed , so it is possible to travel at mach 1 , it is just not very fuel efficient .
vector spaces because we need superposition . tensor product because this is how one combines smaller systems to obtain a bigger system when the systems are represented by vector space . hermitation operator because this allows for the possibility of having discrete-valued observables . hilbert space because we need scalar products to get probability amplitudes . complex numbers because we need interference ( look up double slit experiment ) . the dimension of the vector space corresponds to the size of the phase space , so to speak . spin of an electron can be either up or down and these are all the possibilities there are , therefore the dimension is 2 . if you have $k$ electrons then each of them can be up or down and consequently the phase space is $2^k$-dimensional ( this relates to the fact that the space of the total system is obtained as a tensor product of the subsystems ) . if one is instead dealing with particle with position that can be any $x \in \mathbb r^3$ then the vector space must be infinite-dimensional to encode all the independent possibilities . edit concerning hermitation operators and eigenvalues . this is actually where the term quantum comes from : classically all observables are commutative functions on the phase space , so there is no way to get purely discrete energy levels ( i.e. . with gaps in-between the neighboring values ) that are required to produce e.g. atomic absorption/emission lines . to get this kind of behavior , some kind of generalization of observable is required and it turns out that representing the energy levels of a system with a spectrum of an operator is the right way to do it . this also falls in neatly with rest of the story , e.g. the heisenberg 's uncertainty principle more or less forces one to have non-commutative observables and for this again operator algebra is required . this procedure of replacing commutative algebra of classical continuous functions with the non-commutative algebra of quantum operators is called quantization . [ note that even on quantum level operators can still have continuous spectrum , which is e.g. required for an operator representing position . so the word " quantum " does not really imply that everything is discrete . it just refers the fact that the quantum theory is able to incorportate this possibility . ]
as i now understand the formulation of this question we have the following situation : ( 1 ) an observer is situated near the event horizon of a black hole ; ( 2 ) a light source is situated a similar radial distance from the bh and is shining light which is reaching the observer ; ( 3 ) the radial distance of both from the bh is approximately x ; ( 4 ) the event horizon expands ( reducing the distance between it and the observer/light source ) . several observations about what is likely to happen here : ( 1 ) the event horizon is at r=2m for a schwarzchild solution , but there are unusual photonic effects nearby outside too . at r=3m the photons will go into orbit around the bh . so perhaps if nearby the observer is picking up a photon from its orbit . even if a little further out the light bending will be quite extreme , and so the observer is likely to " see " the light coming from another region of the sky . ( 2 ) the photons of light from the source may already be in a kind of elliptical orbit which drags them into r=2m . ( 3 ) with an expanded event horizon we are assuming greater gravitational force on both observer and source . at some point one or both may be overwhelmed by the increasing gravitation near the event horizon . thus they would be unable to maintain any fixed distance , and be drawn in simply because they dont have an infinite power source in their engines . if such movement occured the light from source to observer could be distorted further as they try to maintain position , but move in different orbits . ( 4 ) as the event horizon approached both would cross the r=3m region , at this point the light source might no longer reach the observer , depending on exact distances . it should be noted that the " potential curve " for gravitation near a black hole is unlike a standard newtonian one near a massive object . the exact trajectories of objects is also heavily dependent on a quantity related to the angular momentum of the photon . to complicate matters further the use of the term " event horizon " in this expanding example may not be appropriate : other terms like " apparent horizon " may be more appropriate . in a sense the event horizon is theoretical construction best applicable to a black hole at the end of ( its ) time . in short i am expecting the stream of photons to stop reaching the observer somewhat before the horizon meets the source , unless the source and observer are on some very specially constructed orbits , in which neither are at position x . the rays connecting the two might reform later as both head into the black hole , resulting in a light that is intermittent . finally i believe that anything demonstrated here , is a demonstration of the properties of an apparent horizon .
such noise is generated by the so called " line " ( = horizontal deflection or flyback ) transformer . this transformer does several things in a crt or tv at the same time : it couples the horizontal output stage to the horizontal deflection coils and recovers the energy from the coils field during flyback , and generates the high voltage for the crt ( about 20 kv ) . the transformer has a core made from ferrite material . such materials have a small and unwanted magnetostriction effect ( there a varieties of ferrites which have a strong effect ) . in case of failiure the core of such a transformer can be driven into saturation , and the frequency of oscillation comes down to the region where our ears are much more sensitive . then you hear that vibration of the core . btw in the 50ties , when i was much younger ( including my ears ) i could hear the 15 625 hz tone of tv sets out in the street when i passed houses at nighttime ( less other noise ) . whether the transformers were louder in those times , or i just had better ears , i do not know .
unlike john rennie , i think that the problem is not in the efficiency of this system but in the fact , that it will not generate considerable lift . so even if marketing materials are completely true and dyson air multiplier is more energy efficient than conventional fans this efficiency only applies to moving air ( which is its intended use ) but not to the lifting force . the principle behind air multiplyer ( see this video ) is creating the flow around the surface of the duct which induces considerably greater flow through the duct . however the resulting flow would be nearly potential and the net force on the duct would be quite small . somewhat similar effect does occur in helicopters : vortex ring state , where under certain conditions increasing the air flow through the rotor does not produce additional lift . in helicopters this is harmful and could even cause the crash , but the air multiplier effectively creates similar ' vortex ' around the duct for the purpose of moving air .
i would leave special relativity out of the picture . i mean : magnetism is a relativistic effect , in the sense that it pops out from the application of ( special ) relativity to electrostatics , but their relationship is more conceptual and may deserve a new question entirely devoted to the matter . long story short , you do not need special relativity to understand magnetic forces . all you need to know is that when a charged particle $q$ moves at a velocity $\mathbf{v}$ through an electric field $\mathbf{e}$ and a magnetic field $\mathbf{b}$ , it experiences the lorentz force $$ \mathbf{f_l} = q ( \mathbf{e} + \mathbf{v}\times\mathbf{b} ) $$ this really is the fundamental equation for electrodynamics , together with maxwell 's equations . now , a current-carrying wire is usually considered to be neutral overall ( i.e. . the number of electrons and ions are the same so that the net charge is zero ) , so you can take the electric field $\mathbf{e} = 0$ . then , you are dealing with a current , not with a single charge . how to get the above equation in terms of the current $i$ ? the electric current $i$ is defined to be $\frac{dq}{dt}$ where $q$ is the electric charge . the velocity is defined as $\frac{d\mathbf{l}}{dt}$ where $d\mathbf{l}$ is the path element in the direction of the current ( or of the charge 's trajectory ) . so , the infinitesimal force on an infinitesimal charge $dq$ is $d\mathbf{f_l} = dq ( \mathbf{v}\times\mathbf{b} ) $ , where $$ dq\cdot\mathbf{v} = dq\cdot\frac{d\mathbf{l}}{dt} = \frac{dq}{dt} d\mathbf{l} = id\mathbf{l}$$ . to get the total force , you just integrate along the path defined by $d\mathbf{l}$: $$\mathbf{f_l} = \int_l i d\mathbf{l}\times\mathbf{b} $$ ok . but you have two wires . and you know that a current carrying wire generates a magnetic field , given by ( assuming the wire is in a straigh line ) : $$ b_{\phi} = \frac{\mu_0}{2\pi}\frac{i}{r} $$ meaning that it is only in the tangential ( $\neq$ radial ) direction . the direction of the field is given by the right hand rule , line up your right thumb with the current direction and your fingers will tell you the direction of the magnetic field . this formula is obtained via ampère 's law . if the two wires are in a straight line and are separated by a distance $d$ , the value of the magnetic field due to one at the position of the other is $b = \frac{\mu_0}{2\pi}\frac{i}{d}$ . assume the wires carry equal , constant currents $i$ . using this as the magnetic field in the lorentz force , you get a force of $$ |\mathbf{f}| = i\cdot ( \frac{\mu_0}{2\pi}\frac{i}{d} ) \cdot \int_l dl = \frac{\mu_0}{2\pi}\frac{i^2}{d}\cdot l $$ where $l$ is the length of the wires , or a force per unit length of $$ |\mathbf{f}| = \frac{\mu_0}{2\pi}\frac{i^2}{d} $$ . $d\mathbf{l}$ points along the current : use this and the cross product in the lorentz force equation to get the direction of the resulting force . if the currents are parallel , the force is attractive , while if they are opposite , the force is repulsive . the magnitudes of the forces are the same , as they only depend on the wire separation $d$ and on the current $i$ . notice , the other wire will exert the exact same force on the fist , due to newton 's iii law of motion .
however , they must somehow occupy space , as i have read that light waves can collide with one another . that is not true . yes , light waves can " collide " and interact with each other ( rarely ) , but that itself does not imply that they need to occupy space . it is not even entirely clear what it means for a subatomic particle to occupy space . a particle like a photon is a disturbance in a quantum field , and is " spread out " across space in a sense ; it does not have a definite size in the same sense that a macroscopic material object does . but you will probably agree that , if it is possible to make any sensible definition of " occupying space " for a subatomic particle , it should involve preventing other things from also occupying that same space . photons do not do that . they are bosons , and as a consequence of that they are not subject to the pauli exclusion principle , so if you have a photon occupying some space ( whatever that may mean ) , you can in theory pack an unlimited number of additional photons into the same space .
is not it just generally true that in the absence of any potential , the momentum eigenfunctions are also energy eigenfunctions ? in other words , when there is no potential , ( in the right units ) $$ h = p^2/2m + v ( x ) = p^2/2m $$ since the hamiltonian is proportional to the momentum operator squared , it is easy to see that any eigenket of the momentum operator having eigenvalue $p$ will be an eigenket of the momentum operator with eigenvalue $ p^2/2m$ . it turns out that the momentum eigenkets ( and linear combinations thereof ) form the complete solution set of the particle on a ring problem .
we know that we can describe a spin $1/2$ massless particle using only a single weyl field ( lets say left-handed $\psi_{l}$ ) . to introduce a mass term we have to use two spinor fields ( one left-handed and one right-handed ) and this gives the dirac mass term . the question is now that if we can describe a massive particle with a single weyl field . well yes , due to the fact that given a left-handed weyl spinor , it is possible to construct a right-handed spinor $\psi_{r}=i\sigma^{2}\psi_{l}^{*}$ . thus , we can write the dirac equation using $i\sigma^{2}\psi_{l}^{*}$ $$\hspace{43mm} \bar{\sigma}^{\mu}i\partial_{\mu}\psi_{l}=im\sigma^{2}\psi_{l}^{*} \hspace{30mm} ( 1 ) $$ the known algebraic methods performed for the dirac equation to prove that it implies a massive klein-gordon equation can be performed here without any problems . thus , the above equation implies $ ( \box+m^{2} ) \psi_{l}=0$ . here we have constructed a mass term using only $\psi_{l}$ and this is known as majorana mass . the similarity with the dirac mass can be seen by writting $ ( 1 ) $ in terms of the four component majorana spinor $\psi_{m}$ in the chiral representation $$\psi_{m}=\begin{pmatrix} \psi_{l}\\i\sigma^{2}\psi_{l}^{*} \end{pmatrix}$$ now , equation $ ( 1 ) $ becomes $$ ( i\gamma_{\mu}\partial^{\mu}-m ) \psi_{m}=0$$ the majorana mass has a very important physical difference when compared to the dirac mass . we know that the dirac action with a mass term is invariant under a global $u ( 1 ) $ transformations of $\psi_{l}$ and $\psi_{r}$ ( i.e. . $\psi_{l}\rightarrow e^{i\alpha}\psi_{l} , \hspace{2mm}\psi_{r}\rightarrow e^{i\alpha}\psi_{r}$ ) . but for majorana spinors , $\psi_{l}$ and $\psi_{r}$ are not independent , they are related by complex conjugation . so , if $\psi_{l}$ transforms as $\psi_{l}\rightarrow e^{i\alpha}\psi_{l}$then $\psi_{r}$ transforms like $\psi_{r}\rightarrow e^{-i\alpha}\psi_{r}$ . the majorana equation $ ( 1 ) $ is not invariant under global $u ( 1 ) $ symmetries . this fact implies that a spin $1/2$ particle with a $u ( 1 ) $ conserved charge cannot have a majorana mass . all spin $1/2$ particles with an electric charge cannot have a majorana mass . also leptons that have a majorana mass violate the lepton number ( because this is a $u ( 1 ) $ symmetry ) . one possible particle that could have a majorana mass is the neutrino . but this is yet to be determined . ( i did not answer your questions point by point but i hope this clarifies some of them ) .
the premise of the question is not correct , but there is a general shape to rivers . from leopold and langbein , writing in scientific american : a sample of 50 typical meanders on many different rivers and streams has yielded an average value for this ratio of ahout 4.7 to one . the ratio they use in that article is different from the definition you ( and wikipedia , and comments ) are using . i am sure it is simple algebra to convert one ratio to the other . but that article also notes that : for the large majority of meandering rivcrs the value of this ratio ranges between 1.3 to one and four to one . whatever the conversion comes out to , there appears to be quite a range of real-life meander ratios . that scientific american article is a summary of a longer , more technical article by the same authors . the method they use is to fix beginning and end points a and b , and allow the river to random walk from a to b . the most probable shape for such a path is what they call a " sine-generated " curve . at a given point , the angle between the tangent to the river and the mean direction of the river the sine of the distance along the channel . the resulting curve is not quite a semi-circular curve , so the meander ratio is not predicted to be $\pi$ . a more recent study by garret williams confirms leopold and langbein 's results , and reports that the most common value for the ratio of the radius of curvature to the channel width is between 2 and 3 . the other major effect driving river shape is how easily the river can erode the soil that it passes through . the river will tend to flow more directly downhill if the surrounding soil is difficult to erode . in areas where the soils erodes quite easily , the river will assume this sine-generated curvature . as an example of that , you can look at the mississippi river in the united states . it has the classic sine-generated shape all the way from about cairo , il south to new orleans , la . but it is much straighter up along the illinois/iowa border .
the mass of the original polonium atom is 209.9828737 ( 13 ) au , while the mass of the lead atom is 205.9744653 ( 13 ) au and the mass of the helium is 4.00260325415 ( 6 ) au . the mass deficit gives you the amount of energy released .
the short answer to this question is that we do not know . the subject of your question is still in early " speculative " , theorizing , and researching stages . i can say this because collisions of bubble universes under the eternal inflation theory just happens to be my specific area of work . non-colliding bubble universes ( and the local potential minima in the scalar field are considered other universes ) are causally disconnected from each other due to both the potential wall between the false and true vacuum as well as the rate of expansion of each bubble ( it is fast enough that no information from inside could ever escape the bubble and thus non-colliding bubbles could not influence one another ) . as for collisions leaving imprints on the cmb , that is still a matter of research . at the moment , it is considered the most likely outcome of a bubble collision ( at least , the most likely one that would be visible and not completely destroy us ) . however , physicists like myself are still running simulations to determine what these effect would look like . also keep in mind that for an observer in one of the bubbles to observe the effects of the collision , the collision wall would necessarily have to have passed them already ( which is why we believe if we have experienced a visible collision , it likely would not destroy us because it has not ) . it is only in this way that the bubbles can be causally connected , because they come into contact and ( in some theories ) expand through each other . it is impossible for me to offer a non-speculative answer for whether there are probabilities that can be seen no matter the causal structure . as an observer in just one bubble , my view is understandably biased . furthermore , since we allow for the physical constants and laws to vary between different bubbles ( except for the speed of light ) , it is equally impossible for me to comment on the probabilities observed by someone in another bubble . it should be noted that this area of research is still mostly blackboard physics . there have been some papers published concerning bubble collisions , but without hard empirical data , the best we can do is run complicated computer simulations of hypotheticals .
any hamiltonian contains kinetic and potential parts . interaction means the potential part of the hamiltonian . sometimes a part of interaction can be treated exactly together with the kinetic part . they form an approximative or " non perturbed " hamiltonian . the rest of interaction is then treated perturbatively and is called " a perturbation " . interaction of the atomic electron with an external magnetic field is such part of the total hamiltonian .
kinematically , yes . in terms of describing the positions of objects , it is equivalent to say " a is accelerating away from b " and " b is accelerating away from a " . however , it is an observed fact that the universe treats these two situations differently . a and b can check whether they feel artificial gravity in their reference frame . if so , it is accelerating . as far as i know , the " way the universe decides " to break this symmetry is a topic of continuing speculation . check out some related questions : inertia in an empty universe is acceleration an absolute quantity ? is rotational motion relative to space ? acceleration in special relativity newton&#39 ; s bucket what if the universe is rotating as a whole ?
no , this is not possible . it is only possible for $\mathbf{e}$ to be proportional to $\mathrm{r}^{-2}$ .
the general diffeomorphism symmetry in the target space is not a symmetry of the world line theory or , analogously , the world sheet theory ! a general spacetime diffeomorphism changes the metric tensor $g_{\mu\nu} ( x^\alpha ) $ which plays the role of the " coupling constants " ( coefficients defining the action , e.g. your exponent ) in the world line or world sheet theory ! if a transformation changes the values of coupling constants , then it is clearly not a symmetry , not even classically . the isometry , a diffeomorphism that actually preserves the metric at each point , is a symmetry of the world line theory or the world sheet theory both at the classical and quantum level . i guess that the confusion that led to the question were the omnipresent misleading comments about " background independence " . one may be tempted to say that the diff symmetry is there because we may also change the background metric . but if we do , we are changing the rules of the game . the full spacetime dynamics ( at least in string theory ) ultimately allows us to change the spacetime metric by creating condensates of gravitons in a state etc . but in the world line theory or the world sheet theory , this " emergent " process has a different interpretation : the spacetime background metric has to be considered as a fixed collection of coupling constants and it just happens that we may prove that the " full theory " with one metric field configuration is equivalent to another but that is something else than saying that any particular world line or world sheet theory has a diff symmetry ! it does not . sorry i have not mentioned the word " unruh " because i believe that the core of the aforementioned paradox has nothing to do with the unruh effect .
the universe today is believed to be dominated by dark energy . in fact , it is believed that the dark energy may take the form of a cosmological constant , in which case its energy density has been a constant throughout the history of the universe . radiation and matter are also present today and have been present for most of the history of the universe . however , as the universe expands , matter and radiation are diluted ; their densities decrease at later times . if you know about the scale-factor of the universe , $a$ , we can say that $\rho_{\gamma}\propto a^{-4}$ , $\rho_{m}\propto a^{-3}$ , and $\rho_{\lambda} = constant$ , where $\rho_{\gamma}$ is the density of radiation , $\rho_{m}$ is the density of matter , $\rho_{\lambda}$ is the density of the cosmological constant . tracing back from out dark-energy dominated universe , we get to a matter dominated universe , and then further back still , we get radiation domination . however , it is also believed that the very early universe went through a period of accelerated expansion called inflation . here , the universe expands as if it were dark-energy dominated ; its volume increases rapidly and it is supercooled . this process does not last long . also , when inflation comes to an end , the energy is used to " reheat " the universe .
the radiation density has two components : the present-day photon density $\rho_\gamma$ and the neutrino density $\rho_\nu$ . the photon density as a function of frequency can be derived directly from the cmb : the photon number density follows the planck law $$ n ( \nu ) \ , \text{d}\nu = \frac{8\pi\nu^2\ , \text{d}\nu}{e^{h\nu/k_b t_0}-1} , $$ with $k_b$ the stefan-boltzmann constant , and $t_0$ the current cmb temperature . the photon energy density is then $$ \rho_\gamma\ , c^2 = \int_0^{\infty}h\nu\ , n ( \nu ) \ , \text{d}\nu = a_b\ , t_0^4 , $$ where $$ a_b = \frac{8\pi^5 k_b^4}{15h^3c^3} = 7.56577\times 10^{-16}\ ; \text{j}\ , \text{m}^{-3}\ , \text{k}^{-4} $$ is the radiation energy constant . with $t_0=2.7255\ , \text{k}$ , we get $$ \rho_\gamma = \frac{a_b\ , t_0^4}{c^2} = 4.64511\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ the neutrino density is related to the photon density : in eq . ( 1 ) on page 5 in the paper , you see that $$ \rho_\nu = 3.046\frac{7}{8}\left ( \frac{4}{11}\right ) ^{4/3}\rho_\gamma . $$ this relation can be derived from physics in the early universe , when neutrinos and photons were in thermal equilibrium . so $$ \rho_\nu = 3.21334\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} , $$ and the total present-day radiation density is $$ \rho_{r , 0} = \rho_\gamma + \rho_\nu = 7.85846\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ we can also express this relative to the present-day critical density $$ \rho_{c , 0} = \frac{3h_0}{8\pi g} = 1.87847\ , h^2\times 10^{-26}\ ; \text{kg}\ , \text{m}^{-3} , $$ where the hubble constant is expressed in terms of the dimensionless parameter $h$ , as $$ h_0 = 100\ , h\ ; \text{km}\ , \text{s}^{-1}\ , \text{mpc}^{-1} , $$ so we get $$ \begin{align} \omega_{\gamma}\ , h^2 and = \dfrac{\rho_\gamma}{\rho_{c , 0}}h^2 = 1.71061\times 10^{-5} , \\ \omega_{\nu}\ , h^2 and = \dfrac{\rho_\nu}{\rho_{c , 0}}h^2 = 2.47282\times 10^{-5} , \\ \omega_{r , 0}\ , h^2 and = \omega_{\gamma}\ , h^2 + \omega_{\nu}\ , h^2 = 4.18343\times 10^{-5} . \end{align} $$ for a hubble value $h=0.673$ , one finds $\omega_{r , 0} = 9.23640\times 10^{-5}$ .
suppose you want to use real measurements in this equation , then the density is actually the mass $m_0$ contained in a mesh unit volume ( or voxel ) divided by the volume of the voxel , $a^3$ such that $\rho=m_0/a^3$ . as far as i know in the lattice boltzmann method there is a finite number of velocities $\vec c_i$ and $f_i$ is the mass $m_i$ of matter moving with velocity $\vec c_i$ divided by $a^3$ , $f_i=m_i/a^3$ . as $\sum_im_i=m_0$ , this explains why $\sum_f_i=\rho$ . i hope this answers the interpretation part of your question .
let me give you an intuition of what components are and then i will answer your question : i pull a dog on a leash with a tension of 5n at an angle of 53.1 degrees . this is equivalent to pulling the dog simultaneously horizontally with a rope with a force of 3n ( 5cosθ ) and vertically with a rope with a tension force of 4n ( 5sinθ ) in both scenarios , the dog " feels " the same . in other words , the two components are equivalent to the single force of 5n at an angle of 53.1 degrees . in your question , the weight only acts downwards . we can " break " the force down using a coordinate system and find two components which are equivalent to the weight . the vertical weight vector can be divided into components along the slope and perpendicular to it . by replacing the weight vector by these components , and dividing the other forces into the same coordinate system , we can determine the motion of the object along and perpendicular to the plane .
i have found the solution to my problem . first of all , i had a factor 2 discrepancy due to the pixel dimension and , more important from an optical point of view , the laser beam was underfilling the entrance pupil of the objective . this means the focusing was worse !
let $ [ a , a^{\dagger} ] ~=~1 , $ and let $|0\rangle$ be the vacuum state : $a|0\rangle=0$ . define $$|n\rangle~:=~ \frac{1}{\sqrt{n ! }} ( a^{\dagger} ) ^n|0\rangle . $$ then $$ a |n\rangle ~=~ \sqrt{n} |n-1\rangle , \qquad a^{\dagger} |n\rangle ~=~ \sqrt{n+1} |n+1\rangle , \qquad\langle n |m\rangle ~=~ \delta_{n , m} . $$ an arbitrary linear operator is of the form $$t= \sum_{n , m\in\mathbb{n}_0} |n\rangle t_{nm} \langle m| , \qquad t_{nm}~:=~\langle n|t |m\rangle~\in~ \mathbb{c} , $$ so it is enough to study operators of the form $|n\rangle \langle m|$ . it is straightforward to see that $$ |n\rangle \langle m|~=~\sum_{k\in\mathbb{n}_0} c^{nm}_k ( a^{\dagger} ) ^{n+k} a^{m+k} , $$ where there exist unique coefficients $c^{nm}_k\in \mathbb{c}$ , which can be recursively obtained from the relations $$\delta_k^0~=~\sum_{r=0}^k c^{nm}_{k-r} \sqrt{ \frac{ ( n+k ) ! }{r ! } \frac{ ( m+k ) ! }{r ! } } . $$
the photon propagator $d_{\mu\nu} ( x , y ) = \langle 0 | a_\mu ( x ) a_\nu ( y ) |0\rangle$ is a building block for amplitudes , but it is not necessarily an amplitude itself . the source for an electromagnetic field has to be a conserved current , which basically means that you create states from the vacuum using linear combinations of $a_\mu ( x ) $ operators whose coefficients are conserved currents . $$ |j\rangle = \int j^\mu ( x ) a_\mu ( x ) dx |0\rangle $$ where $\partial_\mu j^\mu = 0$ . you can show by direct computation that the amplitude $\langle j_1 | j_2 \rangle = \int\int j_1^\mu ( x ) d_{\mu\nu} ( x , y ) j_2^\nu ( y ) dxdy$ is gauge invariant if the currents $j_1$ and $j_2$ are conserved .
it is simply a matter of notation . the $p_1$ ( and hence $e_1$ and $e_2$ ) in $$\int d\pi_2=\int d\omega\frac{p_1^2}{16\pi^2e_1e_2} ( \frac{p_1}{e_1}+\frac{p_1}{e_2} ) ^{-1}$$ is no longer an integration variable ; it has the fixed value that satisfies the delta function $\delta ( e_{cm}-e_1-e_2 ) $ in the previous integral . the factor $ ( \frac{p_1}{e_1}+\frac{p_1}{e_2} ) ^{-1}$ comes from applying an identity of the delta function : $$\delta ( g ( x ) ) = \frac{\delta ( x-x_0 ) }{|g' ( x_0 ) |} . $$
i am going to take c=1 . the top orange line , which is the t ' axis in your drawing , is drawn with a slope of 1/0.6 ; this follows from the definition of velocity . the bottom orange line , which is the x ' axis , has a slope of 0.6 . what is a little harder is to get the scale on the orange axes . the scale follows from the fact that area is preserved by lorentz transformations . the velocity of 0.6 turns out to correspond to a lorentz transformation in which a square is distorted into a parallelogram with its long axis stretched by a factor of 2 , and its short axis contracted by 1/2 . if you want to see this approached developed in more detail , see ch . 7 of this online book : http://www.lightandmatter.com/area1sn.html . ( i am the author . ) there are a couple of recent commercial textbooks that use similar geometric approaches : mermin , it is about time : understanding einstein 's relativity takeuchi , an illustrated guide to relativity
i think , on would need a model to understand how it works in principle . let us consider the simplified system represented on the figure . you have two metallic spheres with radii $r_1$ and $r_2$ respectively . they are linked via a metallic wire as well . because all these guys constitute a single metallic object , they must have the same potential that i noted $v$ . let us assume for simplicity that the charges are uniformly distributed on the two spheres with two different surface charge densities $\sigma_1$ and $\sigma_2$ . now , that the stage is set , there are two competing effects which have opposite influence on the increase of the charge density : the charge on a metallic object , in the linear regime , is proportional to its potential which is denoted by $q = cv$ where $c$ is the capacitance . the capacitance is such that , for simple convex shapes , it increase with the system size such that for a sphere $c \propto r$ . from this , we can assert that $\frac{q_1}{q_2} = \frac{r_1}{r_2}$ . hence the total charge carried by sphere 1 is bigger than that of sphere 2 . there is a second effect ( which is a simple one ) that consists in noticing that the charge density is inversely proportional to the square of the size of the object essentially $\sigma \sim r^{-2} = \frac{q}{4 \pi r^2}$ for a sphere . hence $\frac{\sigma_1}{\sigma_2}=\frac{q_1}{q_2}\frac{r_2^2}{r_1^2}$ overall , we finally get that $\frac{\sigma_1}{\sigma_2}=\frac{r_2}{r_1}$ and hence " points " have a larger charge density than bigger objects .
i feel i could make sense of this if i first consider the system of only the atmosphere . in that case , one assumes the atmosphere is at constant pressure $p_\text{atm}$ . the change in volume $\delta v$ of the atmosphere could be calculated by knowing the initial and final states of the gas in the cylinder , and first calculating the amount ( $n$ ) of gas entering the atmosphere . one could then calculate work by $w=p_\text{atm}\delta v$ . not sure if this result could apply to the system of gas originally enclosed in the cylinder . someone better than me might chime in .
you have the right ideas , but you do not dare to put them in maths ! momentum conservation is vectorial . here , you have a 2d system , so let 's write it with 2 component vectors . i will denote $\vec{p}_{i/f}$ the initial and final momentum respectively . so the momentum conservation yields $ \vec{p}_i = \vec{p}_f $ i choose to represent the $x$-axis as the first component of my vector , and $y$ the second one . thus , $\begin{pmatrix} m_av_a \\ 0 \end{pmatrix} = \begin{pmatrix} m_bvb_x + m_cvc_x \\ m_bvb_y + mcvc_y \end{pmatrix} \quad \quad \quad ( 1 ) $ now the exercise tells you several things . first of all , you know $m_a , v_a , m_b$ and the angle at which block $b$ takes off . you have to trigonometrically decompose the speeds to find out how to write them in vector form . for instance , $vb_x = v_b\cdot\cos ( 35 ) $ , and $vb_y = vb\cdot\sin ( 35 ) $ . with the same reasoning , $vc_x = 0$ and $vc_y = vc$ because of momentum conservation being vectorial , momentum is conserved on the $x$-axis as well as the $y$-axis independently . from ( 1 ) , you see that $\left\{ \begin{array}{l} m_av_a = m_bv_b\cdot\cos ( 35 ) \\ 0 = mbv_b\sin ( 35 ) + m_cv_c \end{array} \right . $ the only unknowns are $v_b$ and $v_c$ . all you need to do is solve this equation system .
the state for generator a can be written more formally as , $$ | a \rangle = \sqrt{\frac{1}{4}} | 1 \rangle_{a} + \sqrt{\frac{1}{4}} | 2 \rangle_{a} +\sqrt{\frac{1}{4}} | 3 \rangle_{a} +\sqrt{\frac{1}{4}} | 4 \rangle_{a} \ , , $$ where $ | 1 \rangle_{a}$ represents the generator $a$ in " state " number 1 . the probablity of getting , for example , number 3 from generator $a$ is derived as , $$ |\langle 3| a \rangle|^{2} = \left| \sqrt{\frac{1}{4}} \langle 3 | 1 \rangle_{a} + \sqrt{\frac{1}{4}} \langle 3| 2 \rangle_{a} +\sqrt{\frac{1}{4}} \langle 3| 3 \rangle_{a} +\sqrt{\frac{1}{4}} \langle 3| 4 \rangle_{a} \right|^{2} = \frac{1}{4} $$ analogously $b$ is more formally given by $$ | b \rangle = \sqrt{\frac{1}{3}} | 2 \rangle_{b} +\sqrt{\frac{1}{3}} | 3 \rangle_{b} +\sqrt{\frac{1}{3}} | 4 \rangle_{b} \ , . $$ a " superposition " of these generators could be their direct sum , if we are considering the states or numbers $a$ and $b$ generate to be " different " . $$ | a \rangle \oplus | b \rangle = \sqrt{\frac{1}{4}} | 1 \rangle_{a} + \sqrt{\frac{1}{4}} | 2 \rangle_{a} +\sqrt{\frac{1}{4}} | 3 \rangle_{a} +\sqrt{\frac{1}{4}} | 4 \rangle_{a} + \sqrt{\frac{1}{3}} | 2 \rangle_{b} +\sqrt{\frac{1}{3}} | 3 \rangle_{b} +\sqrt{\frac{1}{3}} | 4 \rangle_{b} $$ where it is now understood that the kets $ | \ldots \rangle_{a}$ and $| \ldots \rangle_{b}$ live in the direct sum space $ a \oplus b $ . now the probability that $a$ generates number "2" is given by , $$ | \langle 2_a | a \oplus b \rangle |^{2} = |\langle 2| a \rangle|^{2} = \frac{1}{4} = p ( 2|a ) $$ and the probability that $b$ generates number "2" is given by , $$ | \langle 2_b | a \oplus b \rangle |^{2} = |\langle 2| b \rangle|^{2} = \frac{1}{3} =p ( 2|b ) \ , . $$ just as we had before . if we want the probabilities of each generator given the number "2" we have , $$ p ( a|2 ) = \frac{ p ( 2|a ) p ( a ) }{ p ( 2 ) } \ ; , \ ; p ( b|2 ) = \frac{ p ( 2|b ) p ( b ) }{ p ( 2 ) } $$ now we need $p ( a ) $ , $p ( b ) $ and $p ( 2 ) $ . if we regard the generators to be chosen equally as likely $p ( a ) = p ( b ) $ , we get for the likelihood ratio of the generators , $$ \frac{p ( a|2 ) }{p ( b|2 ) } = \frac{p ( 2|a ) }{p ( 2|b ) } = \frac{3}{4} $$
if you use lenses to refocus a beam , with high quality optics , the limiting factor will most likely be the quality of the anti-reflection coatings of your lenses and not the absorption or scattering from their optical material ( at least for visible/nir light ) . it is not easy to lose less than 0.1% per surface from fresnel reflections . if you are desperate to get the smallest losses possible , then you may want to use a refocusing system that involves mirrors . losses while reflecting of a dielectric mirror can be less than 0.01% , taking into account scattering , absorption and transmission .
you pretty much know it already . " random " is a broad word that we use to mean that we can not predict behavior . each of the three cases of randomness that you cite is unpredictable for a different reason , though - that is the difference . dice are random because they are complicated , chaotic pendulums are random because we are not good enough to measure their initial position perfectly , and quantum systems are random because they are not deterministic . expanding on that : the randomness of a coin toss or a dice roll is based on an imprecise model . in principle , if we knew everything about the coin ( its initial position , the forces applied , the density of air that slows it down , etc ) then you could predict whether it winds up heads or tails with certainty . in the real world , nobody bothers , since constructing this model is very difficult . it depends sensitively on the height you are flipping the coin from , its initial spot on your thumb , and so on . chaotic randomness is due to imprecision in initial measurements alone . different initial conditions do not cause smooth changes in the final outcome . a good example is the chaotic motion of the planets - if we try to predict the position of saturn in 500,000,000 years , we get a certain position based on where it is now and all of the forces acting on saturn . but if we choose a slightly different initial condition - say , 10cm further along in its orbit to start - then we get a totally different answer potentially hundreds of thousands of km off . then we look at an initial condition in between , 5cm further along - and the deviation is even worse - it is now millions of kilometers off ! in other words , it chaotic randomness arises in systems where improving your accuracy of measurement does not help . the only way to get the " true " answer is to have the exact initial value . quantum randomness is due to fundamental laws of nature . quantum particles behave randomly on their own because that is just what they do , axiomatically . there is no initial measurement which could even be exact . the outcome is fundamentally nondeterministic , not a limit based on our models or our measurements . in some sense , quantum randomness guarantees that we can never " beat " chaos by getting a perfect initial measurement . but it arises from a fundamentally different origin .
when two or more sources of light combine incoherently ( not in any fixed phase relation ) , you can only " add " , and that is intensity ( power ) - electromagnetic field " squared " and averaged , in the appropriate sense . when you have control over phase relations between two beams , yeah , sure you " add " the two at some point . to " subtract " all you need to do is shift one beam by 180degrees . ( you are dealing with monochromatic light ? ) just add an optically transparent material sufficient to delay the beam by half a wavelength , and there you go . note that " subtract " and " add " may be meaningful only in a limited sense , in the change of phase relationship of two beams in a stable optical apparatus . that is , it is not in the phase relationship itself , but in how it changes by 180 degrees . this is practical and sufficient in many experiments . you do not really have control over things such as spacing of holes in an optical table down to 100nm accuracy , but whatever the distances are , you can be sure they will not change by much during the course of the experiment ( &lt ; 100nm ) . if you have absolute geometric control to several-nanometer scale accuracy all the way from beam splitter to beam combiner ( or detector ) then you could compare the optical paths and say that the beams add , or subtract , or combine in an in-between phase relation , depending on the difference of optical paths being integer , half-integer or other number of wavelengths .
instead of thinking about one field changing in response to the change of the other , it is more correct to say that whenever the magnetic field is changing , so is the electric field , and vice versa . the way these fields change is governed by maxwell 's equations . this way , we do not arrive at the confusion op has .
i know that this is not quite a " minimal " set of assumptions to add in , but if you are going to interpret $g_{ab} = 8\pi g t_{ab}$ as having something to do with " ordinary matter " , you should be starting with a lagrangian formulation where you have : $$s = \int \sqrt{|g|}d^{4}x \frac{1}{16\pi g}r + 2\lambda + l_{m}$$ where $l_{m}$ represents the lagrangian density of the ordinary matter . then , the calculus of variations gives you einstein 's equation , and the " ordinary matter " interpretation of $t_{ab}$ is trivial . as for the comparison with the newtonian limit , the only real way to do it is the adored and dreaded post newtonian formalism--where you perturbatively expand in relativistic corrections to newton 's laws , as seen in this article in addition to any gr textbook . it quickly gets very ugly , as you start getting effects like self-force showing up in terms that have factors like $\frac{567849}{98478433}$ , but people studying gravitational waves use these techniques pretty regularly .
its questions like this one that keep me coming back to this site ! your first question is : is there an easy way to understand and/or visualize the reciprocal lattice of a two or three dimensional solid-state lattice ? yes ! the reciprocal lattice is simply the dual of the original lattice . and the dual lattice has a simple visual algorithm . given a lattice $l$ , for each unit cell of $l$ find the point corresponding to that cell 's " center of mass " ( see below ) . connect each such " center of mass " to its nearest neighbors . the resulting lattice is the dual of $l$ . to find center of mass of unit cell ( we consider 2d case , generalizes to arbitrary dimension ) : draw the perpendicular bisectors of the edges which bound the unit cell . for regular lattices these lines should intersect at a single point in the interior of the cell . this point is the " center of mass " of the cell . performing these simple steps you find that the dual of a square lattice is also a square lattice , and that the triangular and hexagonal lattices are each others duals ! you can see a nice illustration of this fact here . your second question is : what is the significance of the reciprocal lattice , and why do solid state physicists express things in terms of the reciprocal lattice rather than the real-space lattice ? as mentioned by others this has to do with fourier transforms . in solid-state physics we want to understand the excitations ( waveforms ) that a certain material , whose structure is given by some lattice $l$ , can support . for a lattice only certain momenta are allowed due to its discrete structure . these allowed momenta correspond to the vertices of the dual lattice ! for more see the wikipedia page or check out the first couple of chapters of little kittel or ashcroft and mermin .  Cheers,  edit : this to clarify some doubts about my answer @wsc has expressed in the comments . first of all , it is incorrect that reciprocal lattice vectors in 3d have dimensions $1/l^2$ . consider a 3d lattice with basis vectors $\{a_i\}$ . the reciprocal lattice has basis vectors given by $$ b_i = \frac{1}{2v} \epsilon_i{}^{jk} \ , a_j a_k $$ in index notation , with summation convention . a more familiar way to write this is in vector notation : $$ \mathbf{b}_i = 2\pi \frac{\mathbf{a}_j \times \mathbf{a}_k}{\mathbf{a}_i \cdot ( \mathbf{a}_j \times \mathbf{a}_k ) } $$ where $ ( i , j , k ) $ are cyclic permutations of $ ( 1,2,3 ) $ . we can see that $$ \dim [ \mathbf{b}_i ] = \frac{\dim [ \mathbf{a} ] ^2}{\dim [ \mathbf{a} ] ^3} = \frac{1}{l} $$ and in terms of the lattice spacing $a$ , $\vert\mathbf{b}\vert \sim \frac{1}{a}$ . in fact , this is a basic fact true in any dimension . we can also understand the normalization of the reciprocal lattice vectors by the factor $\mathbf{a}_i \cdot ( \mathbf{a}_j \times \mathbf{a}_k ) $ as being nothing more than $v$ - the volume of the unit cell . why ? so that the transformation between the lattice and reciprocal lattice vector spaces is invertible and the methods of fourier analysis can be put to use . for all regular lattices afaik the " dual " and " reciprocal " lattices are identical . for irregular lattices - with defects and disorder - this correspondence would possibly break down .
both are right . any moving clock is slower than a clock at rest , from the perspective of the frame at rest . maybe this simplified freehand graphic ( apologies for its lack of precision ) helps to see that both a and b feel the same about each other 's time dilation : let 's say that the red axis represents a and its proper time measured in minutes ( first eight minutes are showed ) . green axis and its numbers represents b observer . light or radio signals from a to b , represented in red oblique lines , are fired on a minute basis . six of them are showed , that took six minutes of a proper time . however , these six signals from a to b take some eight minutes in b proper time . b concludes that a clock is slower . the same holds if we invert the situation ( green lines from b to a ) . well , almost the same ( the last green line is intended to go from green 6 to red 8 , blame my trembling fingers ) .
according to this paper , new approach to the classical radiation fields of moving dipoles , the answer is : $p = \dfrac{18d^2a^4}{35c^7} + \dfrac{2d^2\dot a^2}{15c^5}$ here , $d$ is the fixed electric dipole moment and the acceleration , velocity , and dipole moment are along the $z$ axis . from the paper : this formula may be considered as the dipole analogue of the larmor formula of the point charge . and the larmor formula for a point charge is : $p = \dfrac{2q^2a^2}{3c^3}$
for symmetric or antisymmetric tensor product , the most useful definition of an unentangled state is a state of the form $$ |\psi\rangle = \sum_{p} ( -1 ) ^{2j\cdot |p|} \prod^\otimes_i |\psi_{p ( i ) }\rangle $$ which is just a convoluted symbolic expression for the totally symmetrized ( for bosons ) or totally antisymmetrized ( for fermions ) tensor product . the sum goes over all permutations $i\mapsto p ( i ) $ . the power of $ ( -1 ) $ is $1$ for bosons i.e. $j\in {\mathbb z}$ while it is $ ( -1 ) ^{|p|}$ , i.e. the sign of the permutation , for fermions with $j\in {\mathbb z}+1/2$ . the product is a tensor product . all states that can not be written in this form are entangled . if we kept the usual definition of unentangled states as " strict tensor products " , there would be too few unentangled states because most multi-boson states and almost all multi-fermion states refuse to be strict tensor products . so the definition above makes the form of unentangled states more general . however , it may be too general for various purposes . in general , the tensor factors $|\psi_{p ( i ) }\rangle$ should correspond to states where objects are localized in different regions of space , otherwise the state that is unentangled according to the definition above could be entangled " morally " . in particular , in quantum field theory , unentangled states may be obtained as $$ a^\dagger b^\dagger c^\dagger \dots |0\rangle $$ where $a^\dagger$ and others are polynomials in creation operators but $a^\dagger , b^\dagger , c^\dagger$ are only made of creation operators acting on disjoint , non-overlapping regions . the actual wave function of the state will have to be symmetrized or antisymmetrized so it will not be a strict tensor product but the state of the form above will still enjoy many features of unentangled states . the discussion about the non-overlapping regions may get important and subtle because , for example , the simple singlet state of two spins , $|up\rangle |down\rangle - |down\rangle |up\rangle$ , is entangled according to the normal definition but it could end up being unentangled because it is an antisymmetrized tensor product . however , it usually only makes sense to call such a state unentangled if the up-spinning and down-spinning electron are also located at different positions . if they share the same location , the strict singlet state should be called entangled in all conventions .
the quickest way would be to use the right hand grip rule . from symmetry you may conclude that the magnetic field around each wire forms concentric circular loops around the wire . so now it remains to determine whether clockwise or anticlockwise . to do this , just imagine that you are gripping the wire with your right hand in such a way that your thumb points in the direction of current . then the remaining fingers point in the direction of the magnetic field . this follows directly from the convention used for doing path integrals - if the closed line integral is performed in the anti-clockwise direction , then the area vector for doing the corresponding surface integral is taken to point towards you . of course , having thus found the contributions from each individual wire , you would then have to take their vector sum . on a side note , similar conventions are adopted in mechanics for finding the direction of angular velocity corresponding to clockwise or anticlockwise rotation . thankfully , the convention is more or less uniform through all areas of physics !
if you were to surround the atmosphere by an adiabatic envelope and allow it to come to equilibrium , it probably would settle into such a state . however , the atmosphere is not a static place . it is actively mixed due to heating of the ground by the sun , and by cooling of the upper atmosphere by radiation into space . this makes the surface air less dense than the air above it , causing highly turbulent convection cells to form . also significant is the differential heating between the equator and the poles , which also drives convection on a global scale . the mixing effect of all this turbulent convection is much greater than the very slow tendency for the gases to form concentration gradients due to their differing densities .
edit with respect to correction/edit in question : first of all lets assume that $m_a$ is caused by force $f$ applied at distance $x$ from a , and distance from a to b is $l$ . to calculate $m_a$ $xf = m_a$ $m_a = \sqrt [ 2 ] {x^2 + l^2} f \sin\theta$ $m_a = \sqrt [ 2 ] {x^2 + l^2} f x / \sqrt [ 2 ] {x^2 + l^2}$ $m_a = m_a $ $m_a$ is moment of the force which provides moment $m_a$ about point a , about point b . rest of your equations look correct !
your physics is correct : all colors ( wavelengths ) that make up the " blue " sky are " darkened " with altitude in ( approximately ) the same proportion because there is lass air to scatter . however , the brain does not directly perceive how much red , green , and blue there is . the brain transforms this information into something like the " hcl " color space . " h " means " hue" ; different colors on the rainbow are different hues . " c " means " chroma " , low chroma colors are washed out , zero chroma means shades of grey . " l " means luminosity , on a scale of dark vs light . if the red , green , and blue components are all reduced we notice less " l " but the same " h " and " c " ( as long as it stays bright enough for cones to work ) . see : http://tools.medialab.sciences-po.fr/iwanthue/theory.php for more information .
so , whenever i want to find a nice introduction to a concept in physics , i check the american journal of physics , as it is full of articles with clever descriptions of phenomenon appropriate for presentation in university courses . in this case , this yields many results . in particular , i found the following three articles very helpful : the mathematics of brownian motion and johnson noise - daniel gillepsie [ doi ] [ pdf ] two models of brownian motion - david mermin [ doi ] [ pdf ] fluctuation and dissipation in brownian motion - daniel gillepsie [ doi ] [ pdf ] for completeness , i will give my version of a modern approach to describing brownian motion , where i will borrow heavily from the above . if you want to think in terms of newton 's laws , we will take an approach that in spirit is the same that langevin gave three years after einstein 's paper ( a translation of which also appeared in the american journal of physics [ doi ] [ pdf ] ) , that gives the same result . if we imagine a pollen particle suspended in a liquid , we can assume that the forces on the pollen particle are given by a dissipative friction force , and some random jostling by impacts from the water molecules , we will write $$ m \dot v = -\gamma v + f \gamma ( t ) $$ where $\gamma$ is the drag coefficient , $f$ is some constant we will have to determine , and $\gamma ( t ) $ represents a random gaussian process . that is , we will assume the effect of all of the jostling by the water molecules amounts to drawing a random variable at every instant of time . then , the next step of the argument is to make this whole deal consistent with statistical mechanics , namely the equipartition theorem , so in particular , if we look at long times , we should have $$ \frac 12 m \left\langle v^2 ( \infty ) \right \rangle = \frac 12 k t $$ or in words , the average kinetic energy at long times should be a half $kt$ if we are going to be consistent with statistical mechanics . so , we need only compute the average fluctuations in our velocity for long times . you can follow the papers to see a detailed mathematical account , but for just a taste , we can get the answer from dimensional analysis . we are interested in determining $ \langle v^2 ( \infty ) \rangle $ , and this answer should only depend on the parameters in our equation for the forces the particle feels , namely $m$ , $\gamma$ and $f$ . the dimensions of $m$ and $\gamma$ are easy to read off of the equation $$ [ m ] = [ m ] \qquad [ \gamma ] = [ m t^{-1} ] $$ but what about that $f$ ? well , it depends on the dimensions of our random gaussian noise term , which is a bit tricky . but , the way we tried to describe it , the noise was supposed to be completely uncorrelated in time , so though i did not detail it , this means that $$ \langle \gamma ( t ) \gamma ( t' ) \rangle = \delta ( t-t' ) $$ in detail . and since we know that $\int dt\ , \delta ( t ) = 1$ , we have the dimensions $$ [ \delta ( t ) ] = [ t^{-1} ] \qquad [ \gamma ( t ) ] = [ t^{-1/2} ] $$ which tells us that $$ [ f^2 ] = [ m l t^{-3} ] $$ which seems funny , but it enables us to determine that $$ \langle v^2 \rangle \propto \frac{ f^2 }{ \gamma m } $$ and in particular , we will assume that the proportionality constant is 1 , which using equipartition , gives us $$ m \langle v^2 \rangle = f^2/\gamma = k t $$ or $$ f = \sqrt{ \gamma k t } $$ if we had done all of the math properly , the real answer turns out to be $$ \boxed{ f = \sqrt{ 2 \gamma k t } } $$ which is pretty darn close . the point of all of that , and of einstein 's original paper is that we have shown that the fluctuations ( $f$ ) causes by the jostling of the unseen water molecules is directly related to the dissipation ( $\gamma$ ) you can observe in ordinary fluid experiments . this is the major result of einsteins and langevin 's papers . with a bit more work , we can relate this to the diffusion constant , which tells us how the root mean square position increases linearly with time : $$ \langle x^2 ( \infty ) \rangle = d t $$ doing our dimensional analysis again , we discover we need a relation of the form $$ d \propto \frac{f^2}{\gamma^2} $$ or , getting rid of this silly $f$ thing , using our other result above $$ \boxed{ d = \frac{ kt }{\gamma } } $$ which turns out to be right even if you do the math right ( the proportionality constant is 1 ) . this was the actual formula einstein got famous for in his paper , relating the diffusion constant , something you can measure in experiment , to the drag coefficient , something you can also measure from a different set of experiments . giving in the end a quantitative theory of brownian motion that worked to help solidify the atomic hypothesis and some of the early results of statistical mechanics .
your original question was : afaik , the particles vibrate according to their energy level . is this vibration in 3d space ? one has to state whether you are talking classical particles or quantum mechanical elementary particles or quantum mechanical atoms and molecules . classical particles move in three dimensions , any motion . it might be constrained because of boundary conditions but it is three dimensions . elementary particles , i.e. the ones of the standard model move through space with their kinetic energy , in three dimensions . when unbound , they do not vibrate . quarks , which are always bound within nuclei , vibrate in three dimentional space . atoms and molecules have vibrational states which again are three dimensional but following boundary conditions . all the above vibrations are with the normal definition of vibration , the center of mass being displaced from its average position with the energy of vibration , not connected with the de broglie wavelength . the " vibrations " you are asking , i.e. . the de broglie wavelength defined " displacement " is not similar to the above . the wave is a probability wave , in three dimensional space , but the wave nature appears in specially designed experiments in the distribution of the probabilities of finding elementary particles , atoms , molecules . here is what happens when electrons of the same energy are sent through the double slit : electron buildup over time the probability wave pattern , connected with the energy of the electrons , builds up slowly one electron at a time , proving that the electrons can appear as a probability wave , in three dimensions , but the electron itself is intact , hitting one spot at a time . there is no 3d shape to the electron itself . it is its interactions with the slits that show up the wave nature .
the relation $\mathbf{e} = -\nabla v$ holds only in the absence of vector potential , otherwise the electric field changes to $$ \mathbf{e} = -\nabla v-\frac{\partial\mathbf{a}}{\partial t} . $$ the reason for this is that when you introduce vector potential by $\mathbf{b} = \nabla\times\mathbf{a}$ , faraday 's law reads $$ \nabla\times\mathbf{a}+\frac{\partial}{\partial t} ( \nabla\times\mathbf{a} ) = \nabla\times\left ( \mathbf{e}+\frac{\partial\mathbf{a}}{\partial t}\right ) = 0 . $$ this can be solved generally by putting the bracket equal to a gradient of a scalar function $-\nabla v$ which gives the result for electric field in terms of both scalar and vector potentials given above .
there is no simple equation for how a paper airplane flies like there is for a simple projectile because the airplane can interact with the air in complicated ways . the physics of a paper airplane is described by newton 's laws of motion . these laws apply to both the airplane and the air it travels through . the plane is acted on by a constant gravitational force and by contact forces with the air , especially drag and lift . the nature of the force between the air and the plane can be quite complicated , and requires an extremely detailed analysis for accurate simulation . for example , by constructing the plane slightly differently , you can make it fly faster , slower , further , curve left or right , or bob up and down . the basic physical ideas are those of fluid dynamics and the basic equation involved is the navier-stokes equation . modeling something like an airplane accurately is mostly the domain of expertise of aeronautical engineers . to make a simple model for a game , you might want to start with a simple constant gravity force , a drag force proportional to the square of the velocity , and a lift force also proportional to the square of velocity ( which comes from here ) , and then play around with the parameters until you find something pleasing to your eye .
background as has been stated in the answer you linked to , the propagation of light from the pupil of an imaging system to the image plane can be modeled by a very close approximation called fraunhofer diffraction . $$ u ( x , y ) \propto \int\int u ( \xi , \eta ) e^{-i\frac{2\pi}{\lambda z} ( x\xi+y\eta ) } d\xi d\eta $$ where $\xi$ and $\eta$ are the x- and y- coordinates in the pupil plane , $u ( \xi , \eta ) $ is the optical field in the pupil plane , and $u$ is the optical field in the image plane . under the substitutions $$ \begin{eqnarray} \mathcal{f}_x and = and \frac{x}{\lambda z} \\ \mathcal{f}_y and = and \frac{y}{\lambda z} \\ \end{eqnarray} $$ fraunhoffer diffraction is simply a fourier transform ( ft ) . thus it is easier to write : $$ u ( x , y ) \propto \mathcal{f}\left [ u ( \xi , \eta ) \right ] $$ where $\mathcal{f}$ will represent the ft , including the necessary scale factors and substitutions mentioned . a property of the ft important for this discussion is called convolution theorem . in terms relevant to this answer , this theorem states that the multiplication of two patterns in the pupil of the lens is equivalent to the convolution of the ft of those patterns in the image plane . we can write this as : $$ \mathcal{f}\left [ u ( \xi , \eta ) \right ] \otimes \mathcal{f}\left [ p ( \xi , \eta ) \right ] \propto \mathcal{f}\left [ u ( \xi , \eta ) \times p ( \xi , \eta ) \right ] $$ or $$ u ( x , y ) \otimes p ( x , y ) \propto \mathcal{f}\left [ u ( \xi , \eta ) \times p ( \xi , \eta ) \right ] $$ where $p ( x , y ) $ is the ft of the pupil function $p ( \xi , \eta ) $ , and $\otimes$ is the convolution operator . answer so how does this apply in the case of a camera lens ? well , in the pupil of a camera lens , there is some optical field -- i.e. some pattern of light on its way to the image plane . keep in mind that it is not yet a clearly focused image , but an optical field which will become an image by the time it arrives at the image sensor . also in this plane is the aperture stop , which blocks some of the light . this aperture stop can be considered as a function which is multiplied by the light field on its way through the pupil . it multiplies the incoming filed by 1 where there aperture is clear , and by 0 where light is blocked . in other words : $$ p = \left\{ \begin{array}{rl} 1 and \text{where the aperture is clear} \\ 0 and \text{where the aperture blocks light} \end{array} \right . $$ so , based on the background given above , we know that the image we see will be the convolution of the ideal image $u$ with the ft of the pupil function , $p$ . this will make bright light sources in the image take on the shape of the ft of the pupil function , which is why $p$ is typically called the point spread function ( psf ) of the camera lens* . so what does $p$ look like ? the exact computation of the necessary ft is messy and complicated , but there is a basic rule of thumb which provides a good qualitative description of the ft of some basic shapes : sharp edges in the input to the ft are represented by bright streaks in the output , with the orientation of the streak being perpendicular to the edge which produced it . so , a triangular aperture has 3 sharp edges , and therefore the resulting psf has 3 streaks , which produces a 6-pointed star . in the images below you can see an example in which i have produced a triangular shape to represent the lens aperture , and the resulting psf : in the ( more realistic ) case of a hexagonal aperture , there will technically be 6 streaks , but because the opposite sides of a hexagon are parallel to each other , the streaks in the psf will be in parallel pairs , so they will overlap and you will only see 3 distinct streaks in the image . *this is only true in the case where the lens is optically perfect . in reality , aberrations in the optics also contribute to the psf , but a description of that is beyond the scope of this answer .
you can compute the coefficients by virtue of $$ t_{ij} = \mathrm{tr} [ \rho_g ( \sigma_i\otimes\sigma_j ) ] \ , $$ $$ m_i = \mathrm{tr} [ \rho_g ( \sigma_i\otimes i ) ] $$ and $$ n_i = \mathrm{tr} [ \rho_g ( i\otimes\sigma_i ) ] \ . $$ the proof is immediate if you note that all tensor products of paulis ( or of paulis with the identity ) have trace zero , while $\mathrm{tr} [ i\otimes i ] =4$ .
one recent result which provided certain definite constraints came not from particle physics but from molecular physics . using cold molecules , it is possible to achieve very high precision measurements on properties like the electric dipole moment of the electron , which are on a par or better than the constraints provided by particle physics experiments , and beginning to bite into the territory of certain supersymmetric theories . this is nicely explained in this physics world article about the latest result , an experiment on thorium monoxide molecules performed by the acme collaboration and reported in order of magnitude smaller limit on the electric dipole moment of the electron . the acme collaboration . science 1248213 , 2013-12-19 . arxiv:1310.7534 . to borrow an image from physics world , different supersymmetric theories predict different ranges of values for $d_e$: the acme result constrains it to $|d_e|&lt ; 8.7\times 10^{−29}\ e\ \text{cm}$ . it is therefore my impression that it does rule out , or heavily constrain , several susy candidates .
i believe that in that text , $i$ refers to the magnitude of the current ( a scalar ) , which is assumed to be in the same direction as the length vector $\vec{l}$ ( a vector ) . there is no need for both $i$ and $\vec{l}$ to be vectors . think of current flowing through a wire—if $i$ were a vector ( $\vec{i}$ ) , then the direction of $\vec{i}$ would always be the same as the direction of the wire , because current always flows along a wire . the direction of the wire is already captured by $\vec{l}$ , so it is not necessary to make $i$ a vector quantity also .
basically yes . 1 ) acceleration is a change over time in velocity . since velocity has units of distance per unit time ( like meters per second ) , acceleration has units of distance per unit time per unit time ( like $\mathrm{m}/\mathrm{s}^2$ ) . so the accepted answer is mistaken on that point . 2 ) just to make sure everyone is on the same page with notation : from reading the question , and better yet the accepted answer to the original question , one can see that all throughout , $x$ , $y$ , and $z$ are referring to components of acceleration , not speed ( or position , as a physicist would assume given those names ) . i imagine that mistaken answerer just used " speed " and " acceleration " interchangeably - unfortunately . 3 ) but the answer has the right idea . the quantity $$ \sqrt{x^2 + y^2 + z^2} $$ is indeed the magnitude of the vector with components $x$ , $y$ , and $z$ , assuming those components are in orthogonal positions ( which they are ) . if all three are accelerations in $\mathrm{m}/\mathrm{s}^2$ , then the result will be too . if the device is otherwise not moving ( or even if it is moving , but at a constant speed and in an unchanging direction ) , then the only acceleration it will feel will be due to the earth 's gravity , which ( assuming we are not inside the mantle or as far away as the moon or something ) is pretty constant at $9.8~\mathrm{m}/\mathrm{s}^2$ . thus one would hope the square-rooted quantity we calculated comes out to $9.8$ . if it does not , you need to tweak the calibrations more .
since omega centarui is at a declination of -47.5 degrees , you would need to be at a lattitude of 23.5 degrees north ( i.e. . on the tropic of cancer ) for it to get to 20 degrees above the horizon when it crosses the meridian ( due south ) . in theory , if you had a perfectly flat horizon ( and not atmosphere ) , you could see it just on the horizon as it crosses the meridian at a latitude of 42.5 degrees north . to see it from a hill with a negative horizion the lattitude would depend on the amount of negative horizon you had . this assumes that there is no atmosphere . even at 30 degrees above the horizion you are looking through twice as much air as straight up . at 20 degrees you are looking through 3 times as much air and it gets worse as you go to the horizion , this will effectively limit the minimum angle you can look at . omega cen is a 3.7 magnitude object so it is fairly bright but even so , you will loose it in the atmosphere before you get all the way to the horizon .
off the top of my head , the cosmic microwave background radiation was hypothesized as a consequence of big bang theory before it was observed by accident by penzias and wilson . also the light element abundances , also a consequence of bbt , was theoretical and is still being refined today through observations that supported the initial theory . i do not know what your definition of " biggest discoveries " would be , but volcanoes on io were theoretical before observed by voyager 2 . the discovery of the most volcanically active body in the solar system seems kinda big to me , but i am a planetary astronomer . i think you are going to get multiple people posting several different things in response , so one of us responders should try to agglomerate the responses into a single reply . edited to add : since you have accepted my answer over andrew 's i will just append his to mine so it is more likely that people who just read the accepted ones will see it : " neutron stars were predicted in 1934 by baade and zwicky , one year after the discovery of the neutron . they were not observationally confirmed until 1965 by hewish and okoye . it is hard to beat a prediction that sat around for 30 years before being confirmed . "
your first criterion $ \underline{g}^l=\underline{g}^v $ does not apply for a mixture , though possibly not for the reasons you think . with a pure material " a mole " has an unambiguous meaning , but for a mixture you have to choose what you are taking to be a mole . you are taking a weighted average of the components present , but unless the composition of the liquid and vapour phases are the same ( in general they will not be ) " a mole " of the liquid is not the same as " a mole " of the vapour . that is why your equation does not apply . the only conditions under which " a mole " would be the same for the liquid and vapour phases is if the compositions of the liquid and vapour phases were the same , i.e. $x_i^l=x_i^v$ . that is why you have ended up concluding that your criterion could only apply if $x_i^l=x_i^v$ , and as you say that is usually wrong .
the time contour really has nothing to do with renormalization . rather it is something you choose at the outset for the purpose of the calculation you want to do . with any choice of time contour the renormalization theory is pretty much the same . what renormalization does ( understood in terms of kadanoff/wilsonian renormalization group ) is generate higher dimension effective operators in the lagrangian . the addition of operators to the lagrangian has no effect on what is your choice of time contour to integrate them on ! the reason for the choice of time contour is a little more subtle , and you have probably only seen the two most common special cases . exposure to the general case may clarify what is going on with the imaginary time thing , even if you never use the most general case . the general correlation function ( simplifying to a single scalar field ) can be written $$ \langle \phi ( x_1 , t_1 ) \cdots\phi ( x_n , t_n ) \rangle = \mathrm{tr}\left\{ \rho ( t_0 ) u ( t_0 , t_1 ) \phi ( x_1 , t_1 ) u ( t_1 , t_2 ) \cdots u ( t_{n-1} , t_n ) \phi ( x_n , t_n ) u ( t_n , t_0 ) \right\}$$ where the time evolution operators $u ( t_i , t_j ) $ come from working in the heisenberg ( or interaction ) picture and $\rho$ is an arbitrary initial density matrix describing the system at the initial time $t_0$ . this is all standard stuff similar to what you will see in any qft course . here comes a trick ( part 1 ) : you can write any density matrix you like as $\mathrm{e}^{-\beta h^m}$ . completely general . $h^m$ is not necessarily the hamiltonian of your system , though if it is you have a thermal equilibrium state at temperature $\beta^{-1}$ . now the trick ( part 2 ) : notice that $\mathrm{e}^{-\beta h^m} = \mathrm{e}^{-i ( -i\beta ) h^m} = u ( t_0 - i\beta , t_0 ; h^m ) $ . this is just a trick : imaginary time evolution with " hamiltonian " $h^m$ gives you a density matrix . if $h^m = h$ this is just a thermal state . if not it is not . the general formalism can cope with the real time dynamics of an arbitrary non-equilibrium state . now have a look at page 107 of stefanucci and van leeuwen . i reproduce the relevant figure below ( i believe it is fair use , but i heartily recommend you read the whole book if you get the chance ) : the first figure shows the general situation i have described : the time evolution starts at $t_0$ , runs up the real axis to catch any $\phi ( x , t ) $ operators that are there , then back down to $t_0$ to " meet " the initial density matrix , which we make by evolving down the imaginary axis with $h^m$ which may or may not be $h$ . now we can make approximations . if all you care about are thermal equilibrium properties and not non-equilibrium time evolution , you can measure all thermal correlations by taking all times at the initial time and $h^m=h$ . the real time part of the contour collapses and you are just left with the imaginary time contour you know . it is not so much that thermal field theory is defined on an imaginary time contour . it is just that that is what is left when you do not care about anything else . on the other hand you can start with some non-interacting state at $t_0\to -\infty$ and slowly ( adiabatically ) turn on an interaction and watch what happens . this gives the second set of contours ( fig . b ) , known as the schwinger-keldysh contours and often used for studying nonequilibrium situations like electric currents in nanostructures etc . finally if you take the density matrix to be an equilibrium density matrix at zero temperature then you can use the gell-mann-low theorem to remove the backwards time contour completely . this gives you the usual one way real time contour that you probably know from ordinary qft ( fig . c ) . this works because a vacuum state at $t\to -\infty$ adiabatically turns into a vacuum state at $t\to +\infty$ . in a non-equilibrium situation you can not rely on this and you need the full contour .
the comic is making a joke out of reference frames , it is making no such remark about the accuracy of the physical statement that " trains rotate earth . " if trains did rotate earth , then two trains coming at each other at the same time would cause earth to stop rotating . however , in your own personal reference frame , the train appears to rotate earth as you travel from point a to point b ( because you are always the stationary observer ) . and that is the joke the comic is making .
a goldstone boson is a generic type of particle formed when symmetries are spontaneously broken . if you want to suggest that dark matter is a goldstone boson then that says very little unless you suggest a specific model with a symmetry to be broken . when exact symmetries are broken you get a massless goldstone boson ( except in a few special circustances , e.g. in gauge theory the extra mode gives mass to the gauge bosons instead of forming a goldstone boson ) dark matter cannot be formed from massless particles since they would not be gravitationally bound to galaxies and we know that dark matter is . massless particles would fly past on the same trajectories as photons in the microwave background . if the broken symmetry is not perfect you get pseudo-golstone bosons which are light on the scale of the model , but not massless . the pion is an example from flavour chiral symmetry breaking , but it is not stable . any theory that predicted such a particle would predict other new particles that could just as easily be part of dark matter if they are stable . without a specific proposal for such a theory not much has been said . note that it is actually very easy to dream up particle models of dark matter , e.g. you just need a new quantum number to explain stability . the difficulty is to find a theory that is well motivated from other considerations . e . g supersymmetry solves the hierarchy problem etc . , axions solve the strong cp problem . however there is no clear reason why dark matter needs to solve other problems in this way . until we can detect a signature for dark matter interactions it is going to be very hard to settle what it is .
if $\theta$ is the angle between the arms , displaced from the equilibrium $\theta_0$ by $\delta \theta$ and the torque applied is $\tau =-\kappa \delta \theta$ , assuming equal masses of $m$ with initially motionless parts . the first step is the kinematics , whereas the acceleration of 2 and 3 is related to the acceleration of 1 and the common angle . for simplification we have that 1 is not accelerating in the horizontal direction $\ddot{x}_1=0$ ( as seen in figure below ) . $$ \begin{aligned} \ddot{x}_2 and = \ddot{x}_1 - \ell \cos \left ( \frac{\theta}{2} \right ) \frac{ \ddot{\theta}}{2} and \ddot{x}_3 and = \ddot{x}_1 + \ell \cos \left ( \frac{\theta}{2} \right ) \frac{ \ddot{\theta}}{2} \\ \ddot{y}_2 and = \ddot{y}_1 + \ell \sin \left ( \frac{\theta}{2} \right ) \frac{\ddot{\theta}}{2} and \ddot{y}_3 and = \ddot{y}_1 + \ell \sin \left ( \frac{\theta}{2} \right ) \frac{\ddot{\theta}}{2} \end{aligned} $$ now for the equations of motion of each part . we start with free body diagrams in order to sum up the forces on each part . $$\begin{aligned} -fr_2 \sin \left ( \frac{\theta}{2} \right ) + fr_3 \sin \left ( \frac{\theta}{2} \right ) + fn_2 \cos \left ( \frac{\theta}{2} \right ) + fn_3 \cos \left ( \frac{\theta}{2} \right ) and = m \ddot{x}_1 = 0 \\ -fr_2 \cos \left ( \frac{\theta}{2} \right ) - fr_3 \cos \left ( \frac{\theta}{2} \right ) + fn_2 \sin \left ( \frac{\theta}{2} \right ) + fn_3 \sin \left ( \frac{\theta}{2} \right ) and = m \ddot{y}_1 \end{aligned} $$ the eom are done in a direction along the arm $$\begin{aligned} m \ddot{x}_2 \cos \left ( \frac{\theta}{2} \right ) - m \ddot{y}_2 \sin \left ( \frac{\theta}{2} \right ) and = -fn_2 \\ m \ddot{y}_2 \cos \left ( \frac{\theta}{2} \right ) + m \ddot{x}_2 \sin \left ( \frac{\theta}{2} \right ) and = fr_2 \\ 0 and =\ell fn_2 + \tau \end{aligned} $$ with $\rightarrow fn_2 =-\frac{\tau}{\ell}$ $$\begin{aligned} m \ddot{x}_3 \cos \left ( \frac{\theta}{2} \right ) + m \ddot{y}_3 \sin \left ( \frac{\theta}{2} \right ) and = -fn_3 \\ m \ddot{y}_3 \cos \left ( \frac{\theta}{2} \right ) - m \ddot{x}_3 \sin \left ( \frac{\theta}{2} \right ) and = fr_3 \\ 0 and =\ell fn_3 - \tau \end{aligned} $$ with $\rightarrow fn_3 =\frac{\tau}{\ell}$ combined the all of the above equations substituted into the kinematics are $$ \begin{aligned} -fn_2 \cos \left ( \theta \right ) + fr_2 \sin \left ( \theta \right ) - 2 fn3 and = m \ell \frac{\ddot{\theta}}{2} \\ fr_2 \cos \left ( \theta \right ) + fn_2 \sin \left ( \theta \right ) + 2 fr_3 and = 0 \\ - fn_3 \cos \left ( \theta\right ) - fr_3 \sin \left ( \theta \right ) + 2 fn_2 and = - m \ell \frac{\ddot{\theta}}{2} \\ fr_3 \cos \left ( \theta \right ) - fn_3 \sin \left ( \theta \right ) + 2 fr_2 and = 0 \end{aligned} $$ the above is solved with $$\boxed{\frac{3 \tau ( \cos\theta-2 ) }{\ell ( \sin^2\theta+3 ) } = m \ell \frac{\ddot{\theta}}{2}}$$ and $$\begin{aligned} fr_2 and = \frac{\tau \sin\theta ( 2-\cos\theta ) }{\ell ( \sin^2\theta+3 ) } \\ fn_2 and = -\frac{\tau}{\ell} \\ fr_3 and = \frac{\tau \sin\theta ( 2-\cos\theta ) }{\ell ( \sin^2\theta+3 ) } \\ fn_3 and = \frac{\tau}{\ell} \end{aligned}$$
if i understand correctly the intention of your question , the answer you are looking for is not the planck time . if the question is at what time scales a systems behaves as quantum rather than classical , the answer would be equivalent to at what spatial scales a measurement is quantum instead of classical . the answer depends on the system itself . there are macroscopic systems ( for instance , bose-einstein condensates ) that behave in quantum manner , but most classical examples involve small particles or molecules . thus , there is no single answer , the range can vary orders of magnitude if you can prepare carefully the system . the same happens with the time scale . a system behaves in a quantum manner if the measurement is smaller than the decoherence time . as with the case of spatial scale , this length of time varies with the system .
we need to have : $$p_{fusion}&gt ; p_{loss}$$ to do so , we begin by defining a new quantity $\tau$ , called the confinement time , which measures the rate at which a system loses energy to its environment . it is the energy density w ( energy content per unit volume ) divided by the power loss density $p_{loss}$ ( rate of energy loss per unit volume ) : $$\tau=\frac{w}{p_{loss}}$$ a well-known property of gases , which applies equally well to plasmas , is that their energy density w is equal to $$w=3 k_b n t$$ this formula is a standard result in statistical mechanics and holds whenever there are no impurities in the plasma , an assumption that is justified for fusion reactors because they are typically well insulated . $$p_{fusion}&gt ; \frac{w}{\tau}=\frac{3 k_b n t}{\tau}$$ now we can write : $$n_a n_b \langle {\sigma v}_{a , b} \rangle e_{fusion} &gt ; \frac{3 k_b n t}{\tau}$$ putting $ \frac{3 n^2}{n_a n_b}=12$ $$n t \tau &gt ; \frac{12 k_b t^2}{\langle {\sigma v}_{a , b} \rangle e_{fusion} }$$ $\langle {\sigma v}_{a , b} \rangle$ is the averaged cross-section with the maxwell–boltzmann distribution .
the spin referred in condensed matter is the spin of the electrons least bound to the atoms ( usually valance electrons ) . the atoms reside on the lattice sites . a spin half problem means the atoms have only one valance electron . but there are other possibilities like spin 1 , 3/2 and all . as qeb has already mentioned it can also be used for nuclear spins also . i just want mention that any two level quantum system can be represented as a effective spin 1/2 problem .
the definition for point particles is : $t = \frac{p^2}{2m}$ , where $t$ denotes my kinetic energy . if you are dealing with classical physics , the momentum for a point-particle equals $\vec{p} = m\vec{v}$ , which would give you $t = \frac{mv^2}{2}$ . in the theory of special relativity we notice that objects become heavier when they move , the momentum becomes : $\vec{p} = {m\vec{v}\gamma}$ , where $\gamma = \frac{1}{\sqrt{1- ( v/c ) ^2}}$ . hence the kinetic energy becomes : $t={mc^2\gamma}$ , this is derived below . if you try to make a non-relativistic electromagnetic lagrangian , it is not possible to incorporate the magnetic field in terms of a potential energy . so the solution is to define the momentum equal to $\vec{p} = m\vec{v}-\frac{q\vec{a}}{c}$ , where a is the vector potential so that $\vec{b} = \nabla\times\vec{a}$ . this would give an kinetic energy ( or kinetic term in your hamiltonian ) equal to : $t = \frac{ ( m\vec{v}-\frac{q\vec{a}}{c} ) ^2}{2m}$ . one often calls $m\vec{v}$ the kinematic momentum and $\vec{p}$ the canonical momentum ( since this is the momentum that follows from a lagrangian ) . it are the canonical momenta that are called " thé momenta " . and last but not least , in quantum mechanics you work on a different kind of mathematical structure ( in hilbert-space in stead of the classical phase-space ) . by equivalence with wave-theory the momentum is derived as $\hat{p} = \frac{\hbar}{i}\frac{\partial}{\partial x}$ , where we have an operator $\hat{p}$ which works on that hilbertspace . note : from dj_mummy note that the given relations for kinetic energy all assume the particle was originally at rest at a given time $t=t_0$ . the kinetic energy is derived by using the work-energy theorem ( difference in kinetic energy = work done on the point particle ) . using the laws of classical physics ( newton 's second law ) we get : $t = \int\limits_{x_0}^{x_1}\vec{f}\cdot d\vec{x} = \int\limits_{t_0}^{t_1}\vec{f}\cdot \vec{v}\ , dt = \int\limits_{t_0}^{t_1}\left ( m\frac{d\vec{v}}{dt}\right ) \cdot \vec{v}\ , dt= m\int\limits_{t_0}^{t_1} \vec{v}\cdot d\vec{v} = \frac{m}{2}v^2$ . where $\vec{v}$ is the velocity at time $t_1$ . this gives us the kinetic energy in the case of classical physics ! for the relativistic particle we define a 4-momentum $p^\alpha = m\frac{du^\alpha}{d\tau}$ . where $u^\alpha$ is the four-velocity and $\tau$ is the proper time . the derivation is actually done here and we get the formula above !
let 's suppose we have a source particle and and a test particle a distance $r$ from each other . upon measuring the force on the test particle , we find some value $f_\text{one source}$ . by varying the distance , we discover it depends on the inverse distance squared:$$f_\text{one source}=\frac{c}{r^2} , $$ where $c$ is just some constant . ( no ' charge ' appears here because we have not introduced the concept of charge . ) now let 's bring an additional source particle that is identical to the original , and place it right on top of the original . again , we measure the force , but now we call it $f_\text{two sources}$ . how is this new force related to the original one ? we have no idea , because i have not told you anything about the particles or the nature of the forces . suppose now it is an experimental fact that the force doubles:$$f_\text{two sources}=2f_\text{one source}=\frac{2c}{r^2}=\frac{c}{r^2}+\frac{c}{r^2} . $$ here , the possibility of superposition exists . i have made the superposition part ( where you just add up the contribution due to each source alone ) very explicit . now , suppose instead of the force doubling , the force happened to quadruple :$$f_\text{two sources}=4f_\text{one source} = \frac{4c}{r^2}\ne\frac{c}{r^2}+\frac{c}{r^2} . $$ thus , if the force did not double when you added another identical source particle , superposition would not be possible . it does not matter how you end up ' expanding ' $c$: if you try to do $c=c'q$ or $c=c'q^2$ , you can not get the math to work out ( i.e. . , you can not get the net force on your test particle to be the sum of individual forces ) . this second example ( where the force quadrupled ) is an example of what griffiths means when he says " . . . proportional to the square of the source charge . " one could say $f_\text{two sources}= ( c+c ) ^2/r^2$ and get the right expression for the force , but you could not then go and write the force using superposition .
if you want a direct , physical measurement of curvature , here 's a plan that would take lots of money and decades , possibly centuries to set up . perfect for physics ! what you need are three satellites equipped with lasers , light detectors , precision aiming capabilities , and radio communication . these three satellites are launched into space and position themselves far away from each other so that they form the points of a very large triangle . the satellites then turn on two lasers , aiming each one at the other two . each satellite reports to the others when it is receiving the laser light . once the satellites are all reporting that they see the laser light from the others , they measure the angle between their own two laser beams . each satellite transmits this angle back to headquarters on earth . the overall curvature of space can be determined from these angles . if the sum is 180 degrees , like you learned in geometry class , then the space around the satellites is flat . if the sum is more than 180 degrees , then space has positive curvature there , like the surface of a sphere . you can picture the situation on earth by drawing a line from the north pole to the equator , continuing a quarter way around the world along the equator , then heading back to the north pole . you have just drawn a triangle with three 90 degree angles for a sum of 270 . if the sum of the angles is less than 180 , the region of space has negative curvature like a saddle . let 's say the satellites are surrounding a star . since light bends towards masses , the satellites will have to aim away from the star so that the light will bend around the star and hit the other satellites . this means that the angles of the resulting triangle will be larger than normal ( i.e. . , flat space ) , meaning the sum will be greater than 180 degrees . thus , we can conclude that space has positive curvature near a mass . you can probably exactly calculate the curvature from the resulting triangle , but i have not had sufficient math education to do so .
the vacuum dirac equation automatically implies the klein-gordon equation . it means that every solution to the vacuum dirac equation is automatically a solution to the klein-gordon equation . the converse of course does not hold . the most basic reason is that the klein-gordon equation should really act on scalars , a single bosonic field , while the minimum number of components for the $d=4$ dirac equation is four ( and they should be fermionic fields ) . so a general ( or generic ) valid solution to the klein-gordon equation is a valid solution to the klein-gordon equation ( this much is a tautology , but you were asking about it ) , but it is not a solution to the dirac equation . even if you combine 4 solutions to the klein-gordon equation , declare that they are 4 components of a dirac spinor , and ask whether they solve the dirac equation , the answer is no . it is because the dirac equation is really " stronger " than the klein-gordon equations for its components . effectively , the dirac equation is first-order while the klein-gordon equation is second-order . the dirac equation implies certain correlations between the spin ( up/down ) of the particle and the sign of the energy ( positive/negative ) . the quadruplet of klein-gordon equations allows all combinations of spin up/down and the sign of the energy . however , the most general quadruplet of solutions to the klein-gordon equation may be written as a solution of the dirac equation with a positive mass and a solution to the dirac equation with a negative ( opposite ) mass . the dirac equation describes spin-1/2 ( and therefore " fermionic" ) particles such as electrons , other leptons , and quarks , while the klein-gordon equation describes spin-0 " scalar " ( and bosonic ) particles such as the higgs boson . however , before they do the proper job , the " wave functions " have to be promoted to full fields and these fields have to be quantized .
take the trace of the equation by contracting it with $g^{\mu\nu}$: $$ g^{\mu\nu}r_{\mu\nu}-\dfrac{1}{2}g^{\mu\nu}g_{\mu\nu}r=\dfrac{8\pi g}{c^4}g^{\mu\nu}t_{\mu\nu} $$ as $g^{\mu\nu}r_{\mu\nu} = r$ , $g^{\mu\nu}t_{\mu\nu} \equiv t $ and $g^{\mu\nu}g_{\mu\nu} = 4$ , the previous equation gives you $r = -\dfrac{8\pi g}{c^4}t$ . substituting this into einstein 's equation shall give you the result .
okay , i do not quite get the details of what you are doing , but since this is linear algebra , i would advise you to use linear algebra . you can then easily transfer between bra-ket notation and matrices . first , let 's fix what we are talking about : you have one system $a$ containing one spin , so the system is a space $\mathbb{c}^2$ with basis states $|0\rangle , |1\rangle$ ( down and up - you can of course name them down and up , but that does not change anything ) . furthermore , you have a second system $b$ with a state that can be in either $|-1\rangle , |0\rangle , |1\rangle$ , i.e. a $\mathbb{c}^3$ . this means , your states live in $\mathbb{c}^2\otimes \mathbb{c}^3$ and your time evolutions will just be some unitaries of this space . in order to write them down more easily , let 's choose an ordering of the basis $|0\rangle\otimes|-1\rangle , |0\rangle\otimes|0\rangle , |0\rangle\otimes|1\rangle , |1\rangle\otimes|-1\rangle , |1\rangle\otimes|0\rangle , |1\rangle\otimes|1\rangle$ , which means that $|0\rangle\otimes|-1\rangle$ corresponds to the vector $ ( 1,0,0,0,0,0 ) ^{\mathrm{tr}}\in\mathbb{c}^2\otimes \mathbb{c}^3$ . choosing such a basis makes it easier , to write down the corresponding unitaries . let me make two examples : you rotate the first spin pi/2 around the x-axis . a rotation of a spin around the x-axis is nothing but the unitary evolution of the pauli-x-axis . you can show that a rotation of the bloch-sphere around an axis $\boldsymbol n$ by an angle $\theta$ is given by $$ r_{\boldsymbol n} ( \theta ) =e^{-i\theta \boldsymbol \sigma\cdot \boldsymbol n/2} $$ where $\boldsymbol \sigma$ is the vector of pauli-matrices . thus , a simple rotation of the first part of the system by $\pi/2$ around the x-axis is given by the unitary $$ e^{-i\pi x/4}\otimes 1_3 \in \mathcal{b} ( \mathbb{c}^2\otimes \mathbb{c}^3 ) $$ with the identity acting on the second system . now , suppose you want to implement a conditional unitary . well , nothing easier than that , you just make it up . you know where your basis states should end up ( you just write down what the state will look like after the application of the conditional unitary for any of the basis states $|0\rangle\otimes|-1\rangle , |0\rangle\otimes|0\rangle , |0\rangle\otimes|1\rangle , |1\rangle\otimes|-1\rangle , |1\rangle\otimes|0\rangle , |1\rangle\otimes|1\rangle$ . this will give you all the entries of a $6\times 6$ unitary corresponding to the operator . since you have a parameter $t$ , your unitary will be $t$-dependent . this lets you create unitaries for every step of the way . now to get the overall unitary of the whole process , you just have to multiply them all from right to left ( later processes are multiplied from the left ) - like a general quantum circuit . in principle , now , in order to get your final state , you just multiply the matrix and your initial state vector . there might be a caveat here - i am not entirely sure , whether you can actually initialize the whole system ( i.e. . both parts of the system are in some specific state - maybe a superposition , maybe not ) . if you can , then this will correspond to some vector , if you can not you will need to use density matrices .
it depends how good of an approximation you want . if you just want something that looks like starlight to the human eye then it is not too hard - you can buy solar spectrum bulbs at any hardware store . but of course , this is not going to give you a great approximation , and it is only going to be anywhere close in the visible wavelengths . if you want something that is going to be a fairly good approximation across a very wide range of wavelengths then just heat any random object up to between $3600\:\mathrm{k}$ and $50,000\:\mathrm{k}$ , depending on the star . ( those massive blue stars at $50,000\:\mathrm{k}$ will present a challenge , but i think it is within the bounds of experimental possibility . ) this works because stars and other hot object both emit spectra that are close to the ideal black body spectrum . you can get an idea of how close by comparing the curve on this graph to the edge of the yellow region : ( image source . ) if you want to reproduce all those deviations from the ideal black body curve then it is going to be a bit harder , but it is probably doable if you have a good enough reason to bother . i would guess that a good technique would be to surround your black body with gases similar to the star 's corona , in order to reproduce the absorption lines . emission lines will be a bit more tricky , but i guess if there is no other way you could simply heat those gases up to the appropriate temperature . the uniqueness of a star 's spectrum comes mostly from its temperature and its composition , i.e. the gases that make it up , so by using this method you could probably more or less simulate the spectrum of a specific star . this method would simulate the spectrum of light that the star emits , but if you wanted to simulate the spectrum that we actually see it would be much harder . this is because the spectra of distant stars are modified by a redshift , caused by the fact that distant galaxies are moving away from us . the redshift is basically the optical equivalent of the doppler effect , and it causes us to see frequencies lower than what the star emits . if you wanted to simulate this in the laboratory you would have to use a different method than the one i have described , such as the customised diffraction gratings described in rod vance 's answer . of course , if you meant " simulate on a computer " then it is a different question . i think this is probably not too hard - you just need to look up the appropriate emission and absorption spectra and add them up in the right way . i am sure people researching stars ' spectra do this all the time .
take the solutions of the potential problem of an atom and look at the energy levels . between the n=1 energy level and the n=2 energy level there is a forbidden gap in energy , i.e. you will not find the electron of the hydrogen atom there . note the thick line for large n where the energy gaps become so tiny leading to a continuum , i.e. an energy " band " where one can find the electron when measuring its spectrum . one can always model the collective potential of many atoms in bulk . new energy levels appear due to this collective potential in which the electrons see the whole lattice and not just the parent nucleus , depending on the material . instead of having discrete energies as in the case of free atoms , the available energy states form bands . crucial to the conduction process is whether or not there are electrons in the conduction band . in insulators the electrons in the valence band are separated by a large gap from the conduction band , in conductors like metals the valence band overlaps the conduction band , and in semiconductors there is a small enough gap between the valence and conduction bands that thermal or other excitations can bridge the gap . with such a small gap , the presence of a small percentage of a doping material can increase conductivity dramatically . the gaps , where electrons are " forbidden " arise because of the collective potential and solutions it allows . one can intuitively understand why bands form , because of the collective potential of a large number of atoms and the uncertainty principle due to the quantum mechanical nature at this level : the lattice is vibrating due to thermal motion and the levels get smudged : ) .
this equation is called the ray equation and it can indeed be derived from fermat 's principle . i guess you can find more about its derivation in , e.g. , born and wolf 's principles of optics or in fundamentals of photonics by saleh and teich .
as is easily checked , fields linear in creation and annihilation operators ( and hence amenable to a particle interpretation ) have zero vacuum expectation value . thus the $\phi$ field with its nonvanishing vacuum expectation value cannot be given a particle interpretation . but the field $\psi=\phi-v$ has such an interpretation as its vacuum expectation value is zero . this works only if $v$ is the vacuum expectation of $\phi$ . note that the field $\phi$ is and remains massless ; it is the field $\psi$ that had acquired a mass term . the 1-loop approximation to a quantum field theory is given by the saddle-point approximation of the functional integral . for that you have to expand around a stationary point , and for stability reasons this stationary point has to be a local minimizer . if the local minimum is not global , the vacuum state is metastable only ; so one usually expands around the global minimizer . a mass term breaks the scaling symmetry of a previously scale-invariant theory . it may or may not break other symmetries . in the above case , the symmetry $\phi\to-\phi~~~$ of the action is broken in the stable vacuum .
calculating the effect of acceleration in special relativity is straightforward , but i suspect the algebra is a bit much at high school level . see john baez 's article on the relativistic rocket for a summary , or see chapter 6 of gravitation by misner , thorne and wheeler for a more detailed analysis . when you are first introduced to sr you tend to be told about time dilation and length contraction and given formulae to calculate them . however this is at best an oversimplification and at worst actively misleading . when you are looking at some object moving relative to you you do indeed measure the object 's length to be contracted , but what actually happens is that the two end points in the object 's rest frame transform into points at slightly different times in your rest frame . you measure the object to be contracted because you are measuring the end points at slightly different times . there is no sense in which the object is squeezed by it is high velocity . any object has a proper length , which is equal to its length in its rest frame . the proper length is an invariant and all observers will measure the same proper length regardless of their relative velocity . if you consider proper length then the object is not contracted . anyhow , the answer to your question is that when the object comes to a stop relative to you its length has not changed . this is because it never did change - the change you measured was due to the coordinates you were using not matching the coordinates the object was using . when the object comes to rest in your frame you and the object are using the same coordinates ( at worst differing in the position of the origin ) so both of you measure the length to be the proper length .
$\varphi=15°$ ; $m_t=30 kg $ ; $\mu _c =0,25 $ ; $ d=130m$ ; newton 's 1st law : $$\sum f_x = 0 \implies t_x-\mu _c m_tg cos \varphi - m_tgsin \varphi= 0$$ $$\sum f_y = 0 \implies t_y-m_tg cos \varphi = 0$$ $$\begin{pmatrix} t_x \\ t_y \end{pmatrix} = \begin{pmatrix} \mu _c m_tg cos \varphi +m_tgsin \varphi \\ m_tg cos \varphi \end{pmatrix} = \begin{pmatrix} 1.23 \\ 0.96 \end{pmatrix} n $$ after you find $t_x$ and $t_y$ , you can find the resultant vector which will be in the direction of $\vec d$ . the work done then is simply : $$w=\vec t \bullet \vec d = |t||d|$$
it depends on the situation and interpretation , but it certainly can be the case that your shadow is faster than you . if you imagine a single spotlight shining in front of you , you would cast a shadow behind . if the spotlight moves to behind you , even if you stand still , your shadow will move to the other side . depending on the distance of the object onto which the shadow is cast , this " motion " of your shadow can be very fast indeed ; it could even be faster than the speed of light ! by another interpretation , you are only very slightly faster than your shadow . again , imagine the spotlight in front of you . this time the light stays still , but you hold your arm out and sweep it up . as light from the spotlight passes you , your arm reflects or absorbs some of it ; the wall behind you reflects only the light that falls on it which corresponds to the light that did not fall on your arm , this is the shadow . as you sweep your arm upwards , the pattern on the wall follows the movement . however , because it takes some time for light to travel from the point where it passes you to the wall , the shadow will slightly lag your movement . for real life cases , this lag is miniscule , but if the wall were a sufficiently long distance away , it could be significant . when you consider the added distance from the wall to the observer , the apparant lag time increases .
i do not think that local weyl ( conformal ) invariance implies global invariance . the fields are only defined over local open sets $u_i$ , so if the action is invariant under a constant conformal transformation ( which does not depend on $x\in u_i$ ) $\phi\to \omega_i^2 \phi$ this does not imply anything about the global properties of $\omega:m\to \mathbb{r}$ , only that $\omega|u_i$ is constant . there is something called a " conformal connection " which i do not know very much about , but i think the short answer is that if you restrict the fields to satisfy some specific symmetry group $g$ ( i.e. . they are part of a $g$-bundle and $g$ is some group of conformal transformations ) then the fields will always be conformally invariant , locally and globally .
you are definitely taking this picture too seriously . hybridization of atomic orbitals is just an approximation based on an independent particle model created by pauling to rationalize some structural trends in chemistry with quantum mechanics . we can only assign orbitals unambiguously for one-electron systems , though of course independent particle models were widely employed and still are to explain reactivity trends , etc . anyway , if you prepare an electron in an sp3 state , which is an equally weighted linear combination of the px , py pz and s orbitals , the probability to find and electron at a certain angular momentum might be calculated by taking the square of the projection of this angular momentum on the wave function , as qm tells us to do . do not worry too much if you find weak spots in general theories developed to explain molecules , reactivity , etc , since they are highly approximated in most cases ( unless you do an expensive computation for a system , but then it is not general anymore ) , and the guys that developed those such as pauling already knew that . as you know these highly approximated theories ( e . g . , molecular orbital and valence bond theory ) were and still are very successful , even if they are not very rigorous . it gets really hard to be rigorous in chemistry beyond a certain point . . . @tedbunn of course the basic formalism of qm would predict properties of this hypothetical electronic state , i did not say anything that would imply the opposite . still , to think about bonding in terms of sp3 hybrid orbitals such as one does in general chemistry is really a very crude picture , unless you are dealing with molecules that have very special properties such as td symmetry ( methane , for example ) , and still in those cases if you perform a calculation by using vb or mo ( hartree-fock ) theory ( and it does not even need to be with a computer , since symmetry is going to make life very easy here ) with only the sp3 orbitals of carbon and s of hydrogen in your set of basis functions , you will see a very big quantitative error in predicted ionization energies when you compare to photoelectron spectroscopy measurements of ionization energies . another good test would be to perform high level quantum chemistry calculation of methane using a software and employing two different basis sets , one containing only s and p functions centered on carbon and s functions centered on hydrogen atoms , and another in which the basis set has functions of several angular momenta centered on each atom . if you compare your results with experiments you will see that the first method will have a much lower accuracy compared to the second one . s and p functions might still be the ones that contribute mostly to the bonding molecular orbitals in the different orbital configurations ( set of occupied molecular orbitals ) that contribute to a good description of methane , but we can only say that for sure for very special cases . when you go to molecules with very distinct geometries , it is vital to have d functions centered on carbon , for example , in order to determine even qualitatively right chemistry trends . quantum chemistry semi-empirical methods widely used in the past often had this problem of only employing valence basis functions centered on an atom in molecule calculations , not giving enough flexibility for the description of molecular geometries , and therefore providing bad results whenever the geometry deviated too much from what the qualitative models ( such as the hybridization model ) predicted . in fact , i would really be surprised ( but not too much ) if any modern paper that discusses chemical bonding , or that qualitatively discusses bonding orbitals to explain computed trends , do that by employing hybridization theory arguments . those that i have seen almost always do that by looking at molecular orbitals or some refined valence bond treatments . therefore , although hybridization is a deep and important topic that every chemist should dominate , in the way it was developed by pauling it has severe limitations that people are , or should be , aware of .
the interaction of a particle with its own field depends of the position and retardation depends of the distance between the source particle and the test particle . in principle the field is present at the particle position and the retardation is exactly zero in this one-particle case , therefore the particle always interact with its own field with independence of its velocity
for the sorts of vehicles we are used to , like cars and aeroplanes , there are two contributions to drag . there is the drag caused by turbulence , and the drag caused by the effort of pushing the air out of the way . the streamlining in cars and aeroplanes is designed to reduce the drag due to turbulence . the effort of pushing the air out of the way is basically down to the cross sectional area of whatever is pushing it is way through the air . turbulence requires energy transfer between gas molecules , so you can not get turbulence on length scales shorter than the mean free path of the gas molecules . the wikipedia article on mean free paths helpfully lists values of the mean free path for the sort of gas densities you get in space . the gas density is very variable , ranging from $10^6$ molecules per cm$^3$ in nebulae to ( much ) less than one molecule per cm$^3$ in intergalactic space , bt if we take the value of $10^4$ in the table on wikipedia the mean free path is 100,000km . so unless your spaceship is very big indeed we can ignore drag due to turbulence . a sidenote : turbulence is extremely important in nebulae , and a quick glance at any of the hubble pictures of nebulae shows turbulent motion . however the length scale of the turblence is of the order of light years , so it is nothing to worry a spaceship . so your spaceship designer does not have to worry about the sort of streamlining used in aeroplanes , but what about the drag due to hitting gas molecules ? let 's start with a non-relativistic calculation , say at 0.5c , and use the density of $10^4$ i mentioned above , and let 's suppose that the gas is atomic hydrogen . if the mass per cubic metre is $\rho$ and you are travelling at a speed $v$ m/sec then the mass you hit per second is : $$ m = \rho v $$ suppose when you hit the gas molecules you accelerate them to match your speed , then the rate of change of momentum is this mass times your speed , $v$ , and the rate of change of momentum is just the force so : $$ f = \rho v^2 $$ a density of $10^4$ atoms/cm$^3$ is $10^8$ per m$^3$ or about $1.7 \times 10^{-19}$kg and 0.5c is $1.5 \times 10^8$m/sec so $f$ is about 0.004n per square metre . so unless your spaceship is very big the drag from hitting atoms is insignificant as well , so not only do you not worry about streamlining , you do not have to worry about the cross section either . however so far i have only talked about non-relativistic speeds , and at relativistic speeds you get two effects : the gas density goes up due to lorentz contraction the relativistic mass of the hydrogen atoms goes up so it gets increasingly harder to accelerate them to match your speed these two effects add a factor of $\gamma^2$ to the equation for the force : $$ f = \rho v^2 \gamma^2 $$ so if you take v = 0.999c then you get $f$ is about 7.5n/m$^2$ , which is still pretty small . however $\gamma$ increases without limit as you approach the speed of light so eventually the drag will be enough to stop you accelerating any more . incidentally , if you have a friendly university library to hand have a look at powell , c . ( 1975 ) heating and drag at relativistic speeds . j . british interplanetary soc . , 28 , 546-552 . annoyingly , i have googled in vain for an online copy .
$$ \delta \rho \cong -\frac{\int_v \ ! \mathrm{d}^3r \phi ( \mathbf{r} ) \sigma_a ( \mathbf{r} ) \phi ( \mathbf{r} ) } {\int_v \ ! \mathrm{d}^3r \phi ( \mathbf{r} ) \sigma_f ( \mathbf{r} ) \phi ( \mathbf{r} ) } $$ it seems like because of a result of one group perturbation theory . page 223 of j . duderstadt nuclear reactor analysis . since the temperature feedback is changing the thermal utilisation , which is the ratio between the absorbed and utilised thermal neutrons .
there is such a theorem about product spaces , it is called the künneth formula . for de rham cohomology , $$h^n ( y \times x ) \cong \bigoplus_{i+j=n} h^j ( x ) \otimes h^i ( y ) . $$ since the cohomology of $s^n$ and $\mathbb r^n$ are both well known , the calculation is now simple . you can find this theorem in the book by bott and tu , differential forms in algebraic topology . you could of course also use a mayer-vietoris sequence with $u$ as a spherical cap slightly south of the equator , and $v$ as a spherical cap slightly north of the equator , both extended for all times . then both $u$ and $v$ have the topology of $\mathbb r^4$ ( because a hemisphere has the topology of $\mathbb r^2$ ) and $u \cap v$ has the topology of $s^1 \times \mathbb r^3$ ( because a band around sphere has the topology of $s^1 \times \mathbb r$ ) . then you need to find the cohomology of $s^1 \times \mathbb r^3$ . you can do this also with the mayer-vietoris sequence . bott and tu use the m-v sequence on $s^1$ as an example , this calculation will be essentially the same . there is a good reason that the calculation will be essentially the same , namely that $s^1$ is a deformation retract of $s^1 \times \mathbb r^3$ . cohomology is invariant under deformation retracts , so we really just need to know the cohomology of $s^1$ . in fact for the original problem of $s^2 \times \mathbb r^2$ has $s^2$ has a deformation retract , so we could have used this from the beginning . however calculating the cohomology of $s^n$ is rather simple with the m-v sequence . you just need to know it for $s^1$ and then you can proceed by induction .
my assumption from hearing that problem is that you do not need to worry about fluid flow and the problem is only asking you to calculate how the mass of water is distributed . if the tank were cylindrical or a right prism , you could simply assume that all the mass was at the midpoint of the height of the tank , and you have to raise it in the gravitational field to get it out of the tank . the work to empty the tank would be $\frac{1}{2} m g h$ , where m is the total mass of water in the tank . but this tank does not have the same cross section from top to bottom . you will need to include the cross section at each height in your integration from the top of the tank to the bottom . is that enough information for you , or do you have some other clues in the problem that suggests that you do need to worry about fluid flow ?
first off , physics tends to provide a very good background for people who move on to study problems in other areas , which is perhaps why there is a lot of cross-over to computer science . however , there are also a number of areas at the interface of computer science and physics which attract people from both sides : computer hardware ( which is generally based on semiconductor physics ) . large scale simulations physics of computation ( quantum computing , reversible computing , etc . ) theoretical computer science etc . of these , perhaps the last one ( tcs ) seems the most surprising . however , in recent years , there has been significant success in applying ideas from thermodynamics and statistical mechanics to problems in computational complexity . an example of this would be the simulated annealing algorithm which works extremely well for optimization problems , as well as work done on phase transitions in 3sat .
yes , spring elongation is $\delta= \frac{f}{k}$ regardless of where $f$ comes from .
electricity , understood as movement of electrical charges , can generate an em-wave , if the geometric conditions of the circuit and frequency conditions in the current flow are given . as for the means to travel , it is interesting to refer to " skin effect . " as the frequency of the current passing through a conductor is increased , the flow of electric charge moves to the outer surface of the conductor . when the wavelength of the frequency associated with the movement of electric charges , it becomes comparable to the conductor length , the phenomenon of " radiation " is produced . ultimately , the movement of electric charge is normally performed in a conductive medium . if the specified conditions are met , this movement can generate an electromagnetic wave , which can propagate even in a vacuum . so , electricity is not an electromagnetic wave , but can generate a disturbance in the associated electromagnetic field that has the ability to spread like a wave . something interesting to note is that an electromagnetic wave can generate electricity ( photoelectric effect ) .
dirac 's derivation of the existence of positrons that you described was a totally legitimate and solid argument and dirac rightfully received a nobel prize for this derivation . as you correctly say , the same " sea " argument depending on pauli 's exclusion principle is not really working for bosons . modern qft textbooks want to present fermions and bosons in a unified language which is why they mostly avoid the " dirac sea " argument . but this fact does not make it invalid . the infinite potential charge of the dirac sea is unphysical . in reality , one should admit that he does not know what the charge of the " true vacuum " is . so there is an unknown additive shift in the quantity $q$ and of course that the right additive choice is the choice that implies that the physical vacuum $|0\rangle$ ( with the dirac sea , i.e. with the negative-energy electron states fully occupied ) carries $q=0$ . the right choice of the additive shift is a part of renormalization and the choice $q=0$ is also one that respects the ${\mathbb z}_2$ symmetry between electrons and positrons . it is bizarre to say that dirac missed the lagrangian formalism . dirac was the main founding father of quantum mechanics who emphasized the role of the lagrangian in quantum mechanics . that is also why dirac was the author of the first steps that ultimately led to feynman 's path integrals , the approach to quantum mechanics that makes the importance of the lagrangian in quantum mechanics manifest . it would be more accurate to say that dirac did not understand ( and opposed ) renormalization so he could not possibly formulate the right proof of the existence of the positrons etc . that would also correctly deal with the counterterms and similar things . still , he had everything he needed to define a consistent theory at the level of precision that was available to him ( ignoring renormalization of loop corrections ) : he just subtracted the right ( infinite ) additive constant from $q$ by hand . your sentence the decay of electrons to positrons is then supressed by the u ( 1 ) gauge symmetry of the lagrangian forcing conservation of electrical charge . is strange . since the beginning – in fact , since the 19th century – the u ( 1 ) gauge symmetry was a part of all formulations of electromagnetic theories . it has been a working part of dirac 's theory from the very beginning , too . the additive shift in $q$ , $q=q_0+\dots$ , does not change anything about the u ( 1 ) transformation rules for any fields because they are given by commutators of the fields with $q$ and the commutator of a $c$-number such as $q_0$ with anything vanishes : $q_0$ is completely inconsequential for the u ( 1 ) transformation rules . all these facts were known to dirac , too . the fact that the u ( 1 ) gauge symmetry was respected was the reason that there has never been anything such as a " decay of electrons to positrons " in dirac 's theory , not even in its earliest versions . an electron can not decay to a positron because that would violate charge conservation while the charge has always been conserved . for historical reasons , one could mention that unlike dirac , some other physicists were confused about these elementary facts such as the separation of 1-electron state and 1-positron states in different superselection sectors . in particular , schrödinger proposed a completely wrong theory of " zitterbewegung " ( trembling motion ) which was supposed to be a very fast vibration caused by the interference between the positive-energy and negative-energy solutions . however , there is never such interference in the reality because the actual states corresponding to these solutions carry different values of the electric charge . their belonging to different superselection sectors is the reason why the interference between them can not ever be physically observed . the " zitterbewegung " is completely unphysical .
your equation for the wave is really a vector equation : $$ \psi ( {\bf x} , t ) = a \sin ( \omega t - {\bf k . x} ) $$ this tends to be glossed over when students are first taught the equation , and to be fair in 1d the dot product $\bf k . x$ is simply $kx$ or $-kx$ depending on whether $\bf k$ and $\bf x$ point in the same or opposite directions . anyhow , $\bf k$ is the wave vector and like all vectors has direction and magnitude . its magnitude is the wave number $k$ , and the wave number is equal to $2\pi/\lambda$ . because $k$ is the magnitude of a vector it is always positive , and therefore $\lambda$ is always positive . when you have a wave travelling in the $-x$ direction it is not $\lambda$ that changes sign , it is the direction of $\bf k$ .
both approaches are equally correct in this case . $f = mv^2/r $ is just a consequence of the law for rotational motion , which says $ \tau = i\alpha$ ( torque = moment of inertia * angular acceleration ) . the former formula may be used in case the objects in consideration are point masses . but the latter , more general version of the formula is applicable for any rotating body .
that is exactly the case . if you look at the trajectory of any given spacecraft , you will see that it has a few burns of the rocket engines punctuating very long periods just coasting along in orbit around some other body . for example , the flight path of apollo 8 has something like eight different rocket burns : launch , translunar and transearth injection ( to get out of orbit and go towards the other body ) , three course correction burns , lunar orbit insertion to catch up with the moon , and one orbit correction burn on the moon . image source : wikipedia the rocket engines spend most of their time turned off , and carry just enough fuel for all of this plus a little extra for safety . this still means that the initial rocket needs to be huge , because the translunar injection requires quite a bit of fuel and that fuel needs a huge other load of fuel to get into orbit .
no , a massive body is able to bend light around it , which is called gravitational lensing . this has been observed multiple times . edit photons are massless . otherwise , they would not travel at the maximum speed , which is called speed of light . keep in mind , that gravitational lensing is not a part of newtonian mechanics . you need general relativity for that . and in the context of general relativity , it is not mass , which exerts gravitation , but energy . and photons clearly carry energy . so , you could produce a gravitational well ( even a black hole ) entirely without massive particles .
neglecting air resistance ( whether this is a good idea or not is besides the point ) , suppose you drop the egg from height 10.0m , and it fell to a height of 0.10m , and then rebounded to a height of 9.0m . what can you deduce from this fact ? edit : since you want something a bit more along the modelling side , if you know how the tension changes with temperature , you can deduce the change in entropy with change in length at fixed temperature using maxwell relations . you can assume the rubber band obeys $du = \delta q + t dl$ where $t$ is the tension and $l$ is it is length . you can then use more thermodynamic cleverness to determine the change in temperature with length for adiabatic stretching , which allows you to figure out the irreversible component ( lost as heat ) . google for thermodynamics+rubber+elasticity should get you started . more links : http://physics.oregonstate.edu/~roundyd/courses/ph423/lab-2.pdf ftp://ftp . ccmr . cornell . edu/tmp/mse-4020/4020-notes-7-text-book-rubber-elasticity . pdf
with a potential $v ( x ) = - \frac{\alpha}{|x|}$ , with the notation $a = \large \frac{\hbar^2}{m \alpha}$ , solutions are : $$u^+_n ( x , t ) \sim x e^{ - \large \frac{x}{na}} ~l_{n -1}^1 ( \frac{2x }{na} ) e^{ -\frac{1}{\hbar} \large e_nt}~~for~~ x&gt ; 0$$ $$u^+_n ( x , t ) = 0~for~~ x\le0$$ and : $$u^-_n ( x , t ) \sim x e^{ + \large \frac{x}{na}} ~l_{n -1}^1 ( \frac{2x }{na} ) e^{ -\frac{1}{\hbar} \large e_nt}~~for~~ x&lt ; 0$$ $$u^-_n ( x , t ) = 0~for~~ x\ge0$$ whose energy is : $$e_n = - \frac{1}{n^2} ( \frac{m \alpha^2}{2 \hbar^2} ) $$ $l_n^\gamma$ is the generalized laguerre polynomial [ edit ] there are 2 different set of basis functions , see this reference page $192$ formulae $20a$ and $20b$
the wedge is tangent to the sphere . using that it touches at height r/5 you can easily work out the slope of the wedge . the velocity of the sphere follows directly ( 20m/s times slope ) .
perhaps my ensuing answer will be a little too simple to be satisfying to you , but i only have a firm grasp of things when i keep it simple . let 's just consider statistical mechanics and look at the canonical ensemble ( where there is a constant number of particles [ n ] , volume [ v ] , and temperature [ t ] ) . the components that make up this system want to reach equilibrium because that is how thermodynamic systems behave . the equilibrium point in the canonical ensemble is defined as the point at which the helmholtz free energy , a , is minimized , and it is defined as : a = u - ts where u is potential energy ( a negative value ) and s is entropy . entropy is also defined in stat mech as s = -k ln ( w ) where k is the boltzmann constant and w is the number of microstates in a system . what does this mean ? a system in equilibrium does not always increase its entropy , rather it minimizes its free energy and one way to minimize free energy is to maximize entropy . let 's consider the three different states of h2o : at low temperatures , hydrogen bonds ( i.e. . potential energy ) dominate free energy and the system exists as a low entropy solid . at intermediate temperatures , neither potential energy or entropy dominate and its a liquid . at high temperatures , the entropy term dominates and the water molecules exist far away from each other so as to have an extremely high number of microstates , and very little potential energy .
there are 3 observations that support the big bang theory , i.e. origin of the universe in a singularity : the redshift of galaxies , as you already mentioned . the cosmic background radiation . the amounts of different nuclei in the universe , notably the preponderance of light elements like hydrogen and helium . each of these alone would probably not be sufficient to support the big bang theory . the redshift of galaxies could be explained by some other theory , some have been suggested by hoyle and narlikar in the past . probably the other two phenomena could be explained independently as well , but it is the conjunction that fits so well with the big bang hypothesis . does that settle the matter once and for all ? short answer is no . since these 3 observations have been made and confirmed , more detailed observations have been added to the mix and this has complicated the story for the big bang model . but that would take us into a longer post . the current model which is the most widely accepted is the so-called lambda-cdm model . as for the problem of the universe starting in a real singularity , instead of a very dense state , this is still an open problem related to a yet to be invented ( or completed ) theory of quantum gravity . our current understanding of singularities in general relativity is going back to the penrose-hawking singularity theorems . they are of the kind " here be dragons ! " in that they delineate the conditions for singularities to form and point where our knowledge ends . more can not be done , because a singularity is basically a failure of the theory .
this question is quite a common one for those first learning about capacitors . first , let 's remember that an electric field caused by stationary charges is conservative--this can easily be explained since a single charge creates a conservative field , and superposition of two conservative fields creates another conservative field . so , the field generated by a floating capacitor has to be conservative . the universe is not crazy , so it is probably us missing something ? are there any assumptions that we made while calculating the field ? yes , there are : we assumed that the capacitor was infinite in size , and thus the field became uniform . but , here , we are dealing with the edges of the capacitor . the field is not uniform here , it is more like ( second half of image ) : or : when it comes back out , the x-component of the field will be against the velocity of the particle , slowing it down back to the initial speed . for example , for a positively charged particle , the trajectory is as follows : the green indicates the force on the particle at various points . once the particle exits , it is " pulled back " . the net effect is that the speed stays the same but the direction does not . perfectly in accordance with conservation of energy . ignoring fringe fields can lead to some interesting apparent paradoxi , like the origin of the force that pulls a dipole slab into a capacitor .
starting without the beam splitter , just to establish notation , label the slits 1 and 2 , the laser l and a point on the screen x . then $\langle 1| l \rangle$ is the ( complex ) amplitude to go from the laser to slit 1 . $\langle x| 1 \rangle$ is the amplitude to go from slit 1 to a point on the x on the screen . the amplitude to do that path combination is $\langle x| 1 \rangle\langle 1| l \rangle$ . given that it must land somewhere on the screen we know $$ \sum_x ( |\langle x| 1 \rangle\langle 1| l \rangle + \langle x| 2 \rangle\langle 2| l \rangle |^2 ) =1$$ ( or integral . . . ) if we now place a beamsplitter b right after slit 2 , then instead we have$$|\langle c| b \rangle \langle b| 2 \rangle \langle 2| l \rangle|^2+\sum_x ( |\langle x| 1 \rangle\langle 1| l \rangle + \langle x| b \rangle \langle b| 2 \rangle\langle 2| l \rangle |^2 ) =1$$ now we know a few things about the constituents : $$|\langle 1|l \rangle |^2=0.5 $$ $$|\langle 2|l \rangle |^2=0.5 $$ $$|\langle b|2 \rangle |^2=1 $$ $$|\langle c|b \rangle |^2=0.5 $$ so the first term is $\frac{1}{4}$ ( propagation into the camera ) and the sum of the other terms must be $\frac{3}{4}$ ( propagation to somewhere on the screen ) .
inside the event horizon all timelike paths lead to the singularity . a timelike path is one along which you never travel at a speed greater than c . a static black hole is spherically symmetric ; any asymmetries are radiated away as gravitational waves as the black hole forms . therefore the singularity must be at the centre or it would break the spherical symmetry .
the central point of the question is somewhat ambiguous , but here is an effort to answer it . i am sorry in advance if i have misunderstood it . does light/photons travel ? the question whether light travels from place a to place b or not , can be answered mainly by experience and experiment/observation . when you hold a torch in the dark and you aim it at some point in the background where it is dark , you can see its effects almost immediately . from having being dark , now it is bright and you can see the objects that exist there . that means that light not only travelled there and illuminated the area , it also came back to your eye to give you the information about the objects . this means that light has not always been there , suspended in the air , waiting for you to turn the torch on and make it become reality . i don’t think this is how you envision it . does light “feel” the existence of space ? this type of questions touch on the borders of ontology , somewhat . it is not very easy to formulate answers because one has to talk in terms of metaphysical notions and concepts which , unfortunately , fall outside the scientific method of thinking . but let us take a look at it from this point of view : imagine we send a laser beam from one side of our room to the other . watching it without an apparatus it looks as if light did not have to travel at all , it looks as if the event evolved instantly . a very sensitive apparatus , however , can sense that light has actually taken some time to go there and back . the situation can become more obvious if we try to send the laser beam to the moon and back ( this has been done . ) even we , without any apparatus , can tell that the distance involved must be huge . so space becomes important and even light “feels” the vastness of it . in the experiments you mentioned , the extremely sensitive detectors can distinguish photons arriving with a time difference just a few nanoseconds or less , due to the slightly different paths they take ( space becomes very important in less obvious ways ) light can even “feel” the geometry of space-time , as is demonstrated by the deflection of light-rays passing near the surface of the sun , during a total solar eclipse . light can “feel” the immense density of a bose-einstein condensate by slowing down to incredibly low speed . you can run fast enough and catch up with it ! ! the question whether or not light takes a well defined path to go from a to b involves quantum mechanics , and from your comment i read that it does not interest you at the moment ( ? )
in special relativity , hertz per dioptre is an excellent unit for showing the joint invariance of electromagnetic phenomena in the behavior of all types of lenses , reflective or refractive , under the effects of the lorentz transformation along the axis of motion . i am not aware of any other unit that links those two domains in quite that way . in the case of refractive lenses with chromatic dispersion , the invariance turns out to be non-trivial and a bit surprising , since it asserts that the atomic materials in a lorentz compressed lens must maintain a very specific relationship in how they interact with a spectrum of gamma-shifted light frequencies . here 's how it works for the easier reflective-lens case . first , imagine a sphere 4 meters across with an $f=280$ thz resonant infrared light wave inside . why 4 meters ? well , i am trying to use the correct definition of dioptre . that is the focal length of a refractive or reflective lens , which means the distance it requires to converge parallel light down to a single focal point . in this case , the lens is reflective and has spherical curvature . looking only at a region small enough ( e . g . 2 cm across ) to avoid spherical aberration , the focal length of the $d=4$ m sphere is $l=\frac{1}{2}r=\frac{1}{4}d=\frac{1}{4}4=1$ m . so , a 4 m diameter sphere thus correctly gives a dioptre ( curvature ) of $\delta=1/l=1/1=1$ d , where d $=m^{-1}$ . next , accelerate the sphere along it x axis to a velocity of $v=\sqrt{\frac{3}{4}}$ c , which gives a lorentz factor of $\gamma=2$ . that means that both the sphere and the resonant light pattern within it will be compressed to $\frac{1}{2}$ their original lengths along the x axis , from the perspective of a viewer " at rest " relative to the moving sphere . for the small reflective lens regions around either end of where the x axis crosses the sphere , the pre-acceleration curvature was $\delta_0=1d$ ( the zero subscript indicates the rest frame ) . after acceleration to $\gamma=2$ the sphere becomes an oblate spheroid , and the curvatures of the two reflective lens areas have been reduced to $\delta_1=2d$ , where higher dioptre numbers indicate flatter curves . ( the proof of that is left as an exercise for the reader , but it is not difficult . ) now let 's examine what happens to the frequency of the light within the sphere . the neat thing about special relativity is that physics must remain invariant for both the observer and the observed system . so , if there were n wavelengths of resonant light crossing the sphere along the x axis prior to it being accelerated , there must also be n wavelengths along that same length after the compression . in other words , the wavelengths of the radiation must also be cut in half along x ( only ) , resulting in twice the frequency as before . that transforms the original x-axis $f_0=280$ thz light of the at-rest sphere into $f_1=560$ thz light in the moving sphere . an observer in the rest frame would see this as bright green . observant readers may now be saying " hey , that can not be right ! the lorentz factor also slows time . . . so should not the light in the moving sphere be slower and thus less energetic ? " while it is true that time will pass more slowly within the moving sphere , it is not correct to think that this same light will be slower when viewed from the rest frame . for that situation the geometry of the wavelengths wins , and the light looks green . however , a simpler way to think of it is that since the light is being emitted and reflected by an object traveling at $\gamma=2$ ( or equivalently $v=\sqrt{\frac{3}{4}}$ c ) , the ordinary doppler effect will double its frequency . ( @colink has correctly noted that the above explanation glosses over some important complications . please see his excellent comment for more info . i may try to address that soon . ) now it is time to put this all together . the original light and sphere had an eta factor of : $\eta_0=f_0/\delta_0 = ( 280 thz ) / ( 1 d ) = 280\times{10}^{12}$ hpd where 1 hpd = 1 hz/d ( hertz per dioptre ) . the moving light and sphere has an eta factor of : $\eta_1=f_1/\delta_1 = ( 560 thz ) / ( 2 d ) = 280\times{10}^{12}$ hpd . in other words , the eta factor $\eta$ , which relates the lorentz-transformed electromagnetic waves to the lorentz-contracted physical mirrors from which they reflect , has remained invariant for this example of $\gamma=2$ . it is not an isolated case . it is easy to show that $\eta$ is a universal invariant of special relativity : $\forall{v_i} ( \eta_i=\frac{f_i}{\delta_i}= c ) $ where c is a constant in units of hpd = hz/d = hertz per dioptre . now the remarkable generalization of all of this is that by the same kinds of geometric arguments and application of the " physics must be preserved in both frames " principle , refractive lenses must also fall under the above argument . if a refractive lens has chromatic dispersion ( the colored fringes seen in cheap lenses ) , then the constant c in the above equation will become a frequency-dependent value $c ( f ) $ . yet the eta invariance remains intact ! that is surprising because light dispersion is a pretty complicated phenomenon , yet from the rest frame these messy compressed atoms must nonetheless maintain eta invariance . that is . . . unexpected . thus hpd units not only have real physical meaning , but a meaning that relates directly to the original intent of both the hertz and dioptre units ( versus just being $m/s$ in disguise ) . this meaning in turn provides an easy way to express an invariant relationship in special relativity that links together the electromagnetic and mechanical lorentz transformations in an unexpected and non-intuitive fashion . and finally , despite all the above unexpectedly interesting ( to me at least ! ) sr relationships involved , the hpd unit really did originate as a bit of humor in ( as best i could uncover ) this xkcd discussion posting back in 2007 . so , shrodingersduck from the people 's democratic republic of leodensia , wherever you are six years later , i thank you for inadvertently creating an interesting and quite fun opportunity to explore special relativity in a rather unusual context . addendum 2013-01-31 the generality of the hpd unit in special relativity can i think be stated even more broadly . so , here goes : light frequency , geometric forms , and frequency-dependent refractive indices all change when systems undergo lorentz transformation , so they are not individually lorentz invariant . theorem : if the optical characteristics of an optical system are instead described using hpd ( hertz per dioptre ) and/or its inverse unit dph ( dioptres per hertz ) , the resulting description of its optical properties will remain constant ( "eta invariance" ) regardless of relativistic frame or orientation from which the optical system is analyzed . that is a theorem only . @colink 's excellent observation that the doppler argument i made could be bogus because the shift works differently depending on whether the light is moving with or against the velocity still concerns me . so , i want to look at that a lot more closely and see if i can disprove my own theorem . still , would not it be delightful if a unit defined as a joke turned out to be relativistically invariant when the common units for the same phenomena are not ? the other obvious generalization question is this : does eta invariance ( if it exists ) apply to other wave phenomena ? and finally , @joezeng , i think i misunderstood your question about whether the eta factors ( descriptions of optical components using hpd units ) are related to the velocity of light . well , hpd does have dimensional equivalence to a velocity ( $m/s$ ) , but if there is a meaningful way to re-interpret an hpd value as a velocity , i sure do not see it . intriguing question , though . . .
as i mentioned in the comments , p and s are working in the schrodinger picture which means that the operator fields are time-independent . of course , in the heisenberg picture , the solution of the klein-gordon equation is dependent on time ( and then it will have four-vectors ) . in order to see this , let us write down the klein-gordon equation : \begin{equation} \left ( \partial^2 + m^2 \right ) \phi ( x ) =0 \end{equation} where $g=\mathrm{diag} ( +1 , -1 , -1 , -1 ) $ . then the solutions in the heisenberg picture can be written as : \begin{equation} \phi ( x ) = e^{\pm i p_\mu x^\mu} \end{equation} which can be easily verified : \begin{equation} \begin{aligned} \partial^2 \phi and = \partial_\mu \partial^\mu \left ( e^{\pm i p_\nu x^\nu}\right ) \\ and = \partial_\mu \left ( \pm i p^\mu\right ) \left ( e^{\pm i p_\nu x^\nu}\right ) \\ and = \left ( \pm i p^\mu\right ) \left ( \pm i p_\mu\right ) e^{\pm i p_\nu x^\nu} \\ and = - p_\mu p^\mu e^{\pm i p_\nu x^\nu} \\ and = - ( e^2 - \mathbf{p}^2 ) e^{\pm i p_\nu x^\nu} \\ and = -m^2 e^{\pm i p_\nu x^\nu} \\ and = -m^2 \phi \end{aligned} \end{equation} and so : \begin{equation} \left ( \partial^2 +m^2\right ) \phi = \left ( -m^2 +m^2\right ) \phi = 0 \end{equation} it is normal to write the solution in terms of positive frequency solutions and negative frequency solutions : \begin{equation} \phi ( x ) =\phi_+ ( x ) + \phi_- ( x ) = a e^{- i p_\nu x^\nu} + b e^{+ i p_\nu x^\nu} \tag{1} \end{equation} of course , we also need to sum over all energy-momentum values $p_\mu$ ( because equation $ ( 1 ) $ is a solution for any value of $p_\mu$ ) . hence , the general solution is : \begin{equation} \phi ( \mathbf{x} , t ) = \int \frac{\mathrm{d}^3 \mathbf{p}}{n} \ ; \left [ a ( \mathbf{p} ) e^{- i e_{\mathbf{p}} t + i \mathbf{p} \cdot \mathbf{x}} + b ( \mathbf{p} ) e^{i e_{\mathbf{p}} t - i \mathbf{p} \cdot \mathbf{x}}\right ] \end{equation} where $n$ is a normalization constant . in order to see how to switch between the dirac and schrodinger picture , i refer you to section $2.4$ of p and s . edit i could not help my self and will quickly add this : p and s are discussing the real klein-gordon field , which means : $$ \phi = \phi^* $$ and so : \begin{equation} \int \frac{\mathrm{d}^3 \mathbf{p}}{n} \ ; \left [ a ( \mathbf{p} ) e^{- i p^\mu x_\mu} + b ( \mathbf{p} ) e^{ip^\mu x_\mu}\right ] = \int \frac{\mathrm{d}^3 \mathbf{p}}{n^*} \ ; \left [ a^* ( \mathbf{p} ) e^{ ip^\mu x_\mu} + b^* ( \mathbf{p} ) e^{-i p^\mu x_\mu}\right ] \end{equation} which implies : \begin{equation} \begin{array}{cc} a ( \mathbf{p} ) = b^* ( \mathbf{p} ) \ ; , and b ( \mathbf{p} ) = a^* ( \mathbf{p} ) \end{array} \end{equation} and $n$ must be a real . so the real field can be written as : \begin{equation} \phi ( \mathbf{x} , t ) = \int \frac{\mathrm{d}^3 \mathbf{p}}{n} \ ; \left [ a ( \mathbf{p} ) e^{- i e_{\mathbf{p}} t + i \mathbf{p} \cdot \mathbf{x}} + a^* ( \mathbf{p} ) e^{i e_{\mathbf{p}} t - i \mathbf{p} \cdot \mathbf{x}}\right ] \end{equation}
there is another question on this site about whether the laws of physics change over time . i think that the answers to that one ( including mine ) apply pretty much perfectly to this question about whether the laws change in space . we expect the fundamental laws of physics to be the same throughout space . in fact , if we found that they were not , we would strongly expect that that meant that the laws we had discovered were not the fundamental ones . it is very sensible to ask whether the laws as we currently understand them vary with respect to position . people do try to test these things experimentally from time to time . for instance , some experiments to test whether fundamental constants change with time are also sensitive variations in the fundamental constants with position . some cosmological theories , especially some of those that come under the heading of " multiverse " theories do allow for the possibility that the laws are different in different regions of space , although generally only on scales much larger than what we can observe . in general , in such theories , the truly fundamental laws are the same everywhere , but the way the evolution of the universe played out in different regions is so different that the laws appear quite different . one way this can happen is by the mechanism of spontaneous symmetry breaking . when the universe cooled down from very high temperatures , it probably underwent various transitions , more or less like phase transitions , in which an initially symmetric state turns into a less-symmetric state . in those transitions , there may be different ways that the final state can come out , and they may be quite dramatically different -- completely different sorts of particles may exist , for instance . there could be different regions of the universe in which the symmetry breaking went different ways , in which case the " apparent " laws would be utterly different in different regions , but probably only on scales many orders of magnitude larger than what we can see .
yes , absolutely . in other words , the magnetic field also obeys the principle of superposition . this does break down if you consider back-reaction ( i.e. . the currents feel lorentz forces from the magnetic fields ) ; but it should always be fine for static systems .
if the universe is spatially infinite , it always had to be spatially infinite , even though the distances were shortened by an arbitrary factor right after the big bang . in the case of a spatially infinite universe , one has to be careful that the singularity does not necessarily mean a single point in space . it is a place - the whole universe - where quantities such as the density of matter diverge . in general relativity , people use the so-called penrose ( causal ) diagrams of spacetime in which the light rays always propagate along diagonal lines tilted by 45 degrees . if you draw the penrose diagram for an old-fashioned big bang cosmology , the big bang itself is a horizontal line - suggesting that the big bang was a " whole space worth of points " and not just a point . this is true whether or not the space is spatially infinite . at the popular level - and slightly beyond - these issues are nicely explained in brian greene 's new book , the hidden reality .
there are a lot of comments . the end result is that your first proposition is correct as far as the physics models we have validated with experimental evidence go . even classically , when one has an extended body whose trajectory is assumed by the center of mass , the paradox becomes trivial , even if the center of mass never reaches the goal , half of the solid body minus an infinitesimal essentially does . the heisenberg uncertainty principle is involved at the elementary particle state , and yes the classical motion is no longer relevant and does not describe the behavior of nature at the microscopic scale . as for your second proposition , what disallows discreteness of space and time is not lack of imagination . it is at the moment incompatibility of known and validated physics and mathematical models proposed with such discreteness incorporated . at the moment as far as i know , locality for lorenz invariance , which has been validated an innumerable number of times , is the main obstacle for such theories , but also i am not aware if there exist proposals that can also incorporate the standard model of particle physics , which is an encapsulation of all the experimental measurements we have up to now . thus discreteness in time exists in some models but is not validated by any data and is not a proposition accepted by the mainstream physics community . in any case even if you take time as a variable the argument with the heisenberg uncertainty principle is sufficient as there also exist a delta ( e ) delta ( t ) > h_bar form of it .
the radar method is a general approach that works for non-inertial observers and curved spacetime . two co-ordinates of an event are given by your clock time at which the event intersects your future and past light cone , called retarded time and advanced time , ( $\tau^+ , \tau^-$ , resp . ) . or use a diagonal combination thereof : $\tau^\star = \frac{\tau^+ + \tau^-}{2}$ , called radar time , and $\rho = c\frac{\tau^+ - \tau^-}{2}$ , called radar distance . this diagonal combination has the property that , in the case of an unaccelerated observer in flat spacetime , $\tau^\star$ and $\rho$ are equal to the usual measures ( the " infinite grid of rulers and clocks " business ) . two other co-ordinates can be given by the incoming angles ( $\omega^+$ ) of the null geodesic from the event to you . this is the reception or retarded co-ordinate system . the dual system , the trasmission or advanced co-ordinate system , would use the outgoing angles ( $\omega^-$ ) of the null geodesic from you to the event . for a non-rotating unaccelerated observer in flat spacetime , the two pairs of angles are equal to each other and to the usual polar co-ordinate angles . in flat spacetime this will assign a unique co-ordinate to every reachable event , that is , every event in the observer 's causal diamond . in curved spacetime it will assign at least one co-ordinate to every reachable event , however there may be duplicates . one can restrict to the boundary of the causal past and future , as described in answer i linked to above . then , under certain causality assumptions , every reachable event gets a unique $\tau^\star$ and $\rho$ . the surfaces of constant $\tau^\star$ and $\rho$ are then 2-d globally spacelike surfaces , but not always topologically $\mathcal{s}^2$ , rather , they will be some subquotient of $\mathcal{s}^2$ . that is , for a given $\tau^\star$ and $\rho$ some angle pairs $\omega^+$ will not be valid ( corresponding to parts of the light cone that have " fallen behind" ) , and some events on the boundary of validity will have more than one angle pair .
the necessary and sufficient condition for $db = 0$ ( more commonly written $\nabla \cdot \mathbf b = 0$ ) to imply $b = da$ is the vanishing of the second de rham cohomology $h^2 ( m ) $ . this is guaranteed for a contractible manifold since cohomology is homotopy invariant . however this is for a 2-form $b$ defined on all of $m$ . consider instead the restriction of $b$ to some open set $u$ , $b|_u$ . we can take $u$ to be contractible , for instance , $u$ could be the image of a coordinate ball . then $h^2 ( u ) $ vanishes and we can find $a_u$ such that on $u$ , $da_u = b|_u$ . therefore there always exists a local vector potential . the aharonov-bohm effect is related to the nonvanishing of $$\phi = \oint a$$ even when $da = 0$ . when the first de rham cohomology $h^1 ( m ) $ vanishes , we have that $da = 0$ is equivalent to $a = df$ , and so by stokes 's theorem $\phi$ is always $0$ . in particular $\phi$ can never be non-zero for a contractible space . when a global $a$ cannot be found , one can still make sense of the quantity $\phi$ , but the best way to do this is with the tools of gauge theory . the book gauge fields , knots , and gravity by baez and muniain discusses much of the concepts mentioned in this answer .
$y = y_0 + ut + 0.5at^2$ since $y_0$ and $u$ are $0$ , we have $y = 0.5~at^2$ . in your calculation $t = 2.95 \times 10^{-9}$ , which is correct . so , $y = 0.5\times5.30×10^{17}\times ( 2.95 \times 10^{-9} ) ^2$ therefore , $y = 2.3~\text{cm}$
yes , indeed . there is a working group called the ' particle data group ' operating the particle data listing where they sum up all experimental results , have short reviews about the physics behind it , also explaining the implicit assumptions made and give bounds on the most popular extensions of the standard model .
it is difficult to answer this question . an em wave is generated by vibrating charges and nuclear reactions . sun is full of vibrating charges and nuclear fusions . because of this full range of frequencies are emitted . at distances close to sun we observe the directions of waves to be random . but at far away distances the direction of waves seem parallel . since only parallel waves can have constant separation between them . converging and diverging waves become distant at longer distances .
consider two bodies a and b . with respect to an inertial coordinate system with origin at point o , the coords of the particles in a are vectors $x_{a}\in v_{3}$ with $a=1,2 , \ldots , n_{a}$ and similarly the coords of the particles of b are $x_{b}\in v_{3}$ with $b=1,2 , \ldots , n_{b}$ . the momenta wrt the inertial frame with origin at point o of the particles of a are $p_{a}\in v_{3}$ and the momenta of the particles of b are $p_{b}\in v_{3}$ . the total angular momentum of the system wrt point o is , $$ j=\sum_{a}x_{a}\times p_{a}+\sum_{b}x_{b}\times p_{b} \ . $$ let 's introduce the centre of mass of body a as $x_{a}\in v_{3}$ , $$ x_{a}=\frac{\sum_{a}m_{a}x_{a}}{\sum_{a}m_{a}}=\frac{\sum_{a}m_{a}x_{a}}{m_{a}} $$ and the centre of mass of body b as , $$ x_{b}=\frac{\sum_{b}m_{b}x_{b}}{\sum_{b}m_{b}}=\frac{\sum_{b}m_{b}x_{b}}{m_{b}} $$ adding and subtracting the centre of mass coords , $$ j=\sum_{a} ( x_{a}-x_{a} ) \times p_{a}+\sum_{b} ( x_{b}-x_{b} ) \times p_{b}+x_{a}\times \sum_{a}p_{a}+x_{b}\times \sum_{b}p_{b} \ . $$ the first two terms on the rhs are the angular momenta of the bodies about their respective centres of mass $x_{a}$ and $x_{b}$ . let 's write these contributions as $j_{a}\in v_{3}$ and $j_{b}\in v_{3}$ . the total angular momentum is now , $$ j=j_{a}+j_{b}+x_{a}\times \sum_{a}p_{a}+x_{b}\times \sum_{b}p_{b} \ . $$ let the linear momentum of the particles of body a be , $$ p_{a}=\sum_{a}p_{a} $$ with a similar formula for the sum of the momenta of the particles of body b . put these formulae into the equation for the total angular momentum , $$ j=j_{a}+j_{b}+x_{a}\times p_{a}+x_{b}\times p_{b} \ . $$ this is precisely the splitting of the total angular momentum that harold wrote in his question . $j_{a}$ and $j_{b}$ are the angular momenta of a and b about their own centres of mass . $x_{a}\times p_{a}$ is the orbital angular momentum of a about the origin o of the inertial coords and $x_{b}\times p_{b}$ is the orbital angular momentum of b about o . the point o could be taken as the centre of mass of the complete system , but it does not matter for the above result .
helium nuclei behave like bosons only in phenomena where their integrity is preserved and they can be assumed as point-like particles . when you compress them a lot this is not the case any more . single protons and neutrons will start to interact with each other and they are fermions . you have no chance to condensate them in a black hole , indeed what you get are triple-alpha processes in which helium is fused to $^{12}$c .
where did you get that figure for the speed of the earth 's surface ? the circumference of the earth at the equator ( it is greatest circumference ) is 40,075km and it rotates once in 24 hours , so the speed is 40,075/24 or about 1,670 km/hour or 464 m/sec . if you calculate the time dilation at 464 m/sec it is insignificant , as you had expect given that this speed is 0.0000015$c$ . response to comment : the velocity of rotation around the sun is about 29,800 m/sec ( 0.0001$c$ ) , and at this speed the time dilation factor is about 0.999999995 . this gives an error of about 23 years over the 4.54 billion lifespan of the solar system . but in any case you need to define what you mean by the time . if a physicist uses radioactive decay to measure a date then because the physicist is moving at the same speed as the radionuclide the physicist and the radionuclide experience time flowing at the same rate . that means that by definition there can not be any dating errors . response to response to comment : suppose a block or uranium-238 split in two at the beginning of the solar system , 4.54 billion years ago , and one half ended up on earth while another half ended up on pluto . on the earth a physicist dated the formation of the solar system by measuring how much of the u-238 had decayed , while on pluto an alien physicist did the same experiment . the two physicists would come up with different ages i.e. they would measure that different amounts of the u-238 had decayed . this is a real difference : time really does flow at a slightly different rate on earth and on pluto , though the difference is so small that you had never actually be able to measure it . some of the difference would be due to the different orbital velocities of earth and pluto , and some would be because earth is deeper into the sun 's potential well and gravity affects time as well . i would have to sit down with a pen and paper to work out which effect was stronger . although the time dilation is insignificant for the solar system gravitational time dilation can be significant for neutron stars and especially for black holes . in fact if you sat some distance from a black hole and watched someone falling into it you had see time for the falling astronaut slow to a stop as they approached the black hole event horizon . one last comment : you might be interested in this article . i have already mentioned that the gravitational field of the earth affects time , and this has been used to measure the earth 's geoid by measuring the differences in the rate atomic clocks run .
there is no point in space where the big bang took place . it happened everywhere , simultaneously . centered on earth , since everything is moving away from us with a uniform velocity ( or is stationary with respect to the cmb , if you prefer ) , the net momentum of the universe is approximately zero .
in reality , the definition of incompressible fluid is not what you listed . physically speaking , incompressible means : $$ \frac{\partial \rho}{\partial p} = 0 $$ or the change in density with pressure is zero . this in turn implies the speed of sound is infinite . this also , technically , allows for changes in density with time if those changes are only due to temperature changes and not pressure changes , ie . $$ \frac{\partial \rho}{\partial t} \neq 0$$
here 's an example . let 's consider the case of a free particle in a force field given by a potential $u$ . if $$l=t-u , $$ then the newton 's equations of motion $$\dot {\mathbf p}=-\dfrac{\partial u}{\partial \mathbf r}$$ are equivalent to lagrange 's equation in cartesian coordinates : $$\dot {\mathbf p} = \frac{\partial l}{{\partial \mathbf q}} , $$ with $\mathbf q = \mathbf r$ . the principle of least action states that motion $\gamma$ beetween two points of the generalized configurations space , $ ( \mathbf r _1 , t_1 ) $ and $ ( \mathbf r _2 , t_2 ) $ , is the one that makes the action $s ( \gamma ) =\int _{t_1}^{t_2} l ( \gamma ( t ) , \dot\gamma ( t ) ) \text d t$ stationary , meaning that the linear part ( the so called differential ) of the variation $$\delta s ( \gamma , h ) =s ( \gamma+h ) -s ( \gamma ) $$ is zero in $\gamma $ . this two things are linked by the fact that ( for a sufficiently nice $l$ ) this condition is equivalent to the lagrange 's equation above mentioned . an interesting feature of this principle is that it does not depend on the system of coordinates chosen , which gives a lot of freedom in choosing the more appropriate set of coordinates for the problems . to give an application , suppose that $\mathbf r= ( x , y ) $ are the cartesian coordinates of a point in the plane and let $\pi= ( r , \theta ) :\mathbb r ^2 \to \mathbb r ^2$ be the polar coordinates . suppose that $$\mathbf r ( t ) = ( x ( t ) , y ( t ) ) $$ is a solution of the equations of motion . then we can show easily that $$\mathbf q ( t ) =\pi ( \mathbf r ( t ) ) , $$ which is the projection onto the polar plane of this solution , solves the equations of motion $$\dfrac{\text d }{\text d t} \dfrac{\partial \tilde l}{\partial \dot {\mathbf q}}=\dfrac{ \partial \tilde l}{\partial \mathbf q} , $$ that is , lagrange 's equation are still valid in the new system of coordinates , with a new lagrangian given by $$\tilde l ( \mathbf q , \dot {\mathbf q} ) =l ( \mathbf r , \dot {\mathbf r} ) . $$ in fact by definition of $\tilde l$ , we have $$\int _{t_1} ^{t_2} \tilde l ( \mathbf q ( t ) , \dot { \mathbf q} ( t ) ) \text d t=\int _{t_1} ^{t_2} l ( \mathbf r ( t ) , \dot { \mathbf r} ( t ) ) \text d t . $$ since $\mathbf r$ satisfies lagrange 's equations , it minimizes the action associated to $l$ . so $\mathbf q$ minimizes the action associated to $\tilde l . $ so $\mathbf q$ satisfies lagrange 's equations with the new lagrangian .
it is very similar to the first newton 's law : he says that in a body at rest or moving with a linear uniform motion there is no force acting on it or the vector sum of all forces acting on it is zero . however , this law ignores galilean relativity , because if one object and an observer are falling at the same acceleration with same initial velocity , so the object is at rest in the observer 's reference frame , so he can say that no force is acting on it .
your mistake here is to assume that the multiplication $\vec v\cdot \vec \nabla$ is commutative . it is not ; the dot product here is just a convenient mathematical notation . this part of the wikipedia article on navier-stokes equations explains how to interpret this term .
the question says : no standing waves are observed for any other mass between these values so if you increase the mass smoothly from 16 to 25kg no standing waves are seen . if there was more than two nodes difference between 16 and 25kg you would have seen one or more standing waves as the mass was increased .
the infinite length is not really infinite but it is infinite relative to the very small radius of loops . if we consider radius to be relatively comparable , then the field will depend upon the radius and the point where the field is to be calculated . ( can be done by adding field due to all loops by integration ) then field inside at a point comes out to be $$\dfrac12 \mu_0ni\bigg [ \dfrac{b}{\sqrt{b^2+r^2}}+\dfrac{a}{\sqrt{a^2+r^2}}\bigg ] $$ where $a$ and $b$ are distances from the point to the ends of solenoid . we can see if $r&lt ; &lt ; a , b$ then field comes out to be$$\mu_0ni$$ and this condition is called infinite length .
from that you have $\nabla_r t^r_r=t'$ , but there is also $\nabla_t t^t_r=\frac{\partial}{\partial t}t^t_r+\gamma^t_{\alpha t}t^\alpha_r-\gamma^\alpha_{r t}t^t_\alpha=\gamma^t_{r t}t^r_r-\gamma^t_{r t}t^t_t=-\gamma^t_{rt} ( t-\mu ) $ . so $\nabla_k t^k_r=\nabla_rt^r_r+\nabla_tt^t_r=-t'- ( f'/f ) ( t-\mu ) . $ this should be a comment , but the symbols did not work . my guess is that it is called equation of equilibrium because the t is the stress energy of the rope , not a stress energy that affects the space time geometry . the background is fixed and the rope lives on it .
the quantity that determines day-to-day gravity we feel is the difference is clock-rate at different places , but yes , it is all in the time coordinate , in the sense that the space we see is flat , and only the time coordinate is mismatched from point to point . gravity describes spaces that curve too , distances that change from place to place , but this effect does not matter since it is very small . the reason we see time-component of metric and nothing else is simply because we are so much longer in time than in any other direction--- when you sit still for a few seconds , you extend over a meter or so , but your time extent is one light-second , or 300,000 meters . so on our scales , thing are very very long in time , and short in space , and we can see a slight curvature between different places in the time coordinate , because we go by so much time coordinate .
i am not completely sure what op is asking ( v1 ) . however here is my interpretation . op asks : how do i know which group elements [ . . . ] to assign to each link ? the group element $u_{\ell}\in su ( 2 ) $ affiliated with a link $\ell\in l$ is not fixed . one is supposed to integrate over all possible group values of $u_{\ell}\in su ( 2 ) $ . phrased differently , the link variables $ ( u_{\ell} ) _{\ell\in l}$ are the dynamical variables of the model . the integration measure in the integral for $z$ reads $$ [ du ] ~=~ \prod_{\ell\in l} du_{\ell}$$ here $du_{\ell}$ typically denotes the haar measure for $su ( 2 ) $ .
no , this is not a golden-ratio spiral . its closest relative is the archimedean spiral , whose fundamental equation is $$r=a+b\ , \theta . $$ this is the spiral traced out by the water thrown out by a horizontal sprinkler as it rotates : because its horizontal velocity is constant , the radius $r ( t ) $ of a given drop at time $t$ increases linearly with $t$ , whereas the angle it propagates on is the direction of the sprinkler when it was fired , which also increases linearly with $t$ ; hence , there is a linear relation between $r$ and $\theta$ . image credit : anton croos . i can not find a picture taken from above the sprinkler - apparently people are more careful with their cameras than you had think . in the case of your image , there is the additional action of gravity to deflect the raindrops , so the spiral will not be perfect , but the principle is the same . it is important to note that fibonacci and golden spirals operate on a different principle and they are very hard to sustain over multiple turns , as the radius grows exponentially . this is easy to do with , say , a mollusk that eats more as it grows , but it is hard to accomplish with purely kinematical phenomena . kinematical phenomena do , on the other hand , more or less routinely produce archimedean , or archimedean-like spirals . my favourite is this one , which is produced by shock waves propagating at constant speed through a planetary nebula , and produced by the gas emitted by one of the stars in a closely-orbiting binary pair :
no . in the proper big bang cosmology , the beginning of the world is a spacelike singularity – a horizontal " wiggly " line in the penrose causal diagram – so quite the opposite to your claim holds : the big bang does not allow any correlations between particles in different regions to start with . a correlation or entanglement must always be a result of the subsystems ' contact in the past – of their common ancestry , if you wish – and there are no points in the big bang spacetime that would belong to the intersection of the past light cones of two points at the beginning which makes any correlations and entanglement impossible . indeed , the cosmic microwave background suggests that different regions were correlated – they seem to have the same temperature , to say the least – and cosmic inflation is the major solution to this puzzle . inflation extends the penrose diagram in the temporal direction and allows the particles to communicate in the past and get entangled and correlated . but cosmic inflation is " beyond the big bang theory " . i also want to stress that independently of these " causal technicalities " in cosmology , there is a deeply misleading suggestion in your question . you seem to say that particles are " objectively entangled " and they remain " objectively entangled " forever . but that is not the case . entanglement is just a correlation in the predicted properties that will be measured , expressed in the most general quantum way . once we actually perform the measurement of at least one subsystem , the entanglement disappears . for example , if the initial state is a ( maximally entangled ) singlet state of two spins $$\frac{1}{\sqrt{2}} \left ( |\rm up\rangle |\rm down\rangle - |\rm down\rangle |\rm up\rangle \right ) , $$ the measurement of the first spin yields either " up " or " down " which means that our knowledge of the spins is changed to either " up down " or " down up " ( that is the " collapse " of the wave function ) . the states " up down " and " down up " describe just factual values of two spins that are perfectly known and there is no longer any reason to talk about correlations or entanglements when we know the actual values . it only makes sense to talk about correlations or entanglements of particular particles before they are actually measured . because the properties of particles in the universe have already been " measured " gazillion of times , the hypothetical entanglement created by the big bang – more precisely , by inflation , as discussed at the beginning – has been washed away many times and has almost no detectable traces anymore .
0 ) your guess about the hilbert space is on the right track , but not correct . the space of gauge invariant operators is much too big ; you have to mod out by the equations of motion in an appropriate sense . ( think about the case of 1d quantum mechanics , where the gauge symmetry is trivial . the hilbert space is $l^2 ( \mathbb{r} ) $ , generated by time zero position observables , not $l^2 ( \mathbb{r}^{\mathbb{r}} ) $ , which is what you had get if you used all observables . ) 1 ) in pure yang-mills , the wilson loops are a complete set of observables . ( credit , iirc , goes to migdal . ) 2 ) you can recover the local observables by taking the limit of small loops . 3 ) it is not always true that observables are gauge invariant polynomials in the fields . wilson loops are not polynomials ! 4 ) if there is enough matter fields , the gauge theory may not confine , in which case , baryons and mesons are not the only observables . how many matter fields are required depends on the gauge group . 5 ) not sure what you are asking here . why do you think they should be baryons or mesons ?
yep , average force $\langle f \rangle=\frac{p_f-p_i}{t}$ there are two things going on here : impulse for any collision , it is convenient to define a quantity called " impulse " . most collisions consist of large , varying forces acting in a short time . these are hard to calculate , so they can be encoded into the " impulse " . the impulse is the change of momentum on a body during a collision . due to the identity $\vec f=\frac{\rm d\vec p}{\rm dt}$ , we get : $$\vec j=\delta \vec p=\int\rm{d}\vec p=\int\vec f\rm{d}t$$ impulses are pretty useful in multi-body problems . especially when string/friction/etc are present ; since we can use them in place of forces and conserve momentum . time-averaging for any quantity $x$ dependant on time , the time average of the quantity is : $$\langle x\rangle=\frac{\int x\rm{d}t}{\int \rm{d}t}$$ with limits of integration as the time you want it to be averaged over . in this case , the numerator becomes the impulse ( change in momentum ) . note that for any quantity $x=\frac{\rm{d}y}{\rm{d}t}$ , $\langle x\rangle=\frac{\delta y}{\delta t}$ this is useful for average speed ( total distance traveled by total time ) , average acceleration , and , of course , average force .
yes . . . your supervisors formula is incorrect . smart people make mistakes -- it happens . if your supervisor is not open to criticism , then i suggest that you give him or her a few simple examples that illustrate the problem , and then immediately present a solution . your example of a mirror coated over a non-mirror is a great place to start . in order to solve your multiple reflection problem , you can use the transfer-matrix method . unfortunately you will need to invert the result , because you want to calculate the properties of one layer given the properties of several , which is the opposite of what the method is usually set up to do . whether or not you can do this depends on if you have enough information to separate the effects of the different layers . also , note that the reflectance of one layer is actually undefined , because reflection is a property of an interface between two materials , not a property of a single material . water in water does not reflect , and air in air does not reflect , but there is reflection at air-water interfaces . your coatings will reflect different amounts if they are in contact with one another as compared to if they are in contact with air .
you would like to state the källen-lehman representation through the fermonic field commutator , that is $\{\bar\psi ( x ) , \psi ( y ) \}$ but , as far as the proof goes , you can only use this in a time-ordered way . i mean , you should use $\theta ( x_0-y_0 ) \psi ( x ) \bar\psi ( y ) -\theta ( y_0-x_0 ) \bar\psi ( y ) \psi ( x ) $ . this will grant the due appearance of the klein-gordon propagator in the final formula . when you will do that , the standard view , seen through states and bound states , holds true . about adiabatic continuity , you will always get a weighted sum of free propagators with all the spectrum of the theory , free and bound states , that is in agreement with such a hypothesis . the effect of the interaction will be coded in the weights and the spectrum itself . finally , positivity of the spectral function can only be granted , and a proof holds , when the states behave in a proper way . this is not exactly the case for a gauge theory and some of the difficulties arising in proving the existence of a mass gap can be tracked back to a problem like this . e.g. see this book by franco strocchi . further clarification : when you insert the operator generating a translation in the bosonic field , the same is somewhat different for the spinorial case . you will get $$u^\dagger\psi u=s\psi $$ with $s$ the one i think you studied in the proof of lorentz invariance of the dirac equation . now , you are almost done . this will give for you matrix element $$\langle 0|\psi ( 0 ) |\alpha\rangle=\sqrt{z}u ( \alpha ) $$ being $\alpha$ running both on momenta and spin . you are practically done as , using the known relations $\sum_s u\bar u= \gamma\cdot p+m$ and $\sum_s v\bar v= \gamma\cdot p-m$ , you will get back källen-lehman representation .
the ordinary bessel functions are perfectly well defined for complex arguments . for example , here is a plot of $\re [ j_2 ( x + i y ) ] $: the difference between the ordinary and modified bessel functions is that they satisfy different equations : $$ z^2 y'' + z y ' + ( z^2 - n^2 ) y = 0 , $$ for the ordinary bessel functions and $$ z^2 y'' + z y ' - ( z^2 + n^2 ) y = 0 , $$ for the modified bessel functions . note that there is a relationship between them : $$ j_{\nu } ( z ) =\frac{z^{\nu } i_{\nu } ( i z ) }{ ( i z ) ^{\nu }} $$ with similar identities going the other way . it is all very similar to the relationship between the trig functions $\sin ( z ) , \cos ( z ) $ with the hyperbolic functions $\sinh ( z ) , \cosh ( z ) $ .
answer expected by following author 's hints . \begin{align} \mathbf{l}_{eg} and = \dfrac{1}{4\pi}\int \mathbf{r'} \times \left [ \mathbf{e} \times \mathbf{b} \right ] d^3r'\\ and = \dfrac{1}{4\pi} \int \left [ \left ( \mathbf{b . r'} \right ) \mathbf{e} - \left ( \mathbf{e . r'} \right ) \mathbf{b} \right ] d^3r'\\ and = \dfrac{1}{4\pi} \int \left [ \left ( \dfrac{g}{r'^3}\mathbf{r ' . r'} \right ) \mathbf{e} - \left ( \mathbf{e . r'} \right ) \dfrac{g}{r'^3} \mathbf{r'} \right ] d^3r'\\ and = \dfrac{g}{4\pi} \int \dfrac{1}{r'} \left [ \mathbf{e} - \left ( \mathbf{e . \hat{r}'} \right ) \mathbf{\hat{r}'} \right ] d^3r ' \\ and = \dfrac{g}{4\pi} \int \left [ \mathbf{e . \nabla'}\right ] \mathbf{\hat{r}'} d^3r ' . \end{align} or , let $\mathbf{u}$ and $\mathbf{v}$ be arbitrary vectors : \begin{equation} \left [ \mathbf{u . \nabla}\right ] \mathbf{v} = \left [ \mathbf{u . \nabla}v^i\right ] \mathbf{e}_i , \end{equation} where $ ( \mathbf{e}_i ) _{1\leq i \leq 3}$ denotes the cartesian basis . by integrating by parts we have : \begin{align} \mathbf{l}_{eg} and = \dfrac{g}{4\pi} \int \left [ \mathbf{e . \nabla'}\right ] \mathbf{\hat{r}'} d^3r'\\ and = \dfrac{g}{4\pi} \int \mathbf{e . \nabla'} ( \hat{r}'^i ) d^3r ' \mathbf{e}_i \\ and = \dfrac{g}{4\pi} \int \left [ \mathbf{\nabla ' . } \left ( \mathbf{e}\hat{r}'^i \right ) \mathbf{e}_i - \left ( \mathbf{\nabla ' . e} \right ) \mathbf{\hat{r}}'\right ] d^3r'\\ and = \dfrac{g}{4\pi}\ \left [ \oint \mathbf{\hat{r}'} \left ( \mathbf{e . da}\right ) - \int \left ( \mathbf{\nabla ' . e}\right ) \mathbf{\hat{r}'} d^3r ' \right ] . \end{align} but the field $\mathbf{e}$ vanishes at infinty so it comes : \begin{equation} \mathbf{l}_{eg} = -\dfrac{g}{4\pi} \int \left ( \mathbf{\nabla ' . e}\right ) \mathbf{\hat{r}'} d^3r ' . \end{equation} and finally , using the maxwell equation :$\mathbf{\nabla ' . e} = 4\pi e \delta^{ ( 3 ) } ( \mathbf{r} - \mathbf{r'} ) $ , we get the result : \begin{equation} \mathbf{l}_{eg} = -eg\mathbf{\hat{r}} . \end{equation}
precisely- angular momentum is very difficult to radiate efficiently , while energy is very easy . the net result of minimizing energy while mostly maintaining angular momentum is inevitably a disc . i doubt there will be much of a metallicity effect , since the overall flattening is so pronounced . i expect elliptical galaxies have not become planar because they do not radiate well . the spiral density wave pattern of a spiral galaxy probably " stirs " them very efficiently , so bulk kinetic energy of stars gets dissipated well . likewise , i think the kuiper belt is less coplanar and the oort cloud even less than that because of the lack of perturbations . they are relatively dynamically frozen , as well as the usual sense . on the subject of different solar systems , i would expect tidal disturbances from close passes with neighboring stars to be the most dominant effect in determining how closely planets ' orbital planes coincide . so . . . " urban " star areas would have more close passes than " rural " ones , and also more metal pollution . ergo , if anything i would expect systems with higher metals to be less coplanar . caveat : these kind of dynamics are not my specialty . i am less confident about these speculations than most of my typical answers .
heuristically i would say yes , it always must be curved if you are starting from rest . think of it this way ; suppose the graph is not curved , that is , it is a straight line , then either the straight line is horizontal , or not horizontal . if the graph ( straight line ) is horizontal , then the distance is not changing and you remain at rest . if the graph is a straight line and not horizontal then you would be moving at a constant velocity , but you started from rest , which is a contradiction . i will admit this argument does not really account for pathological examples . maybe one could construct some graph , which in some sort of piece-wise consideration , is made up of straight lines . but i feel like so long as the graph is smooth , then the above argument holds up . again , at least with the above logic ( i will admit , quite hand-wavey ) , it seems like there should be some curvature . regarding your example , specifically , if the car has constant velocity then the distance/time graph must be a straight line .
i would guess that the article is referring to solitons . i am not sure if every non-linear system gives soliton solutions , but many do . the wikipedia article i have linked gives lots of examples of classical solitons , but i am not sure to what extent ( if at all ) they are important in the standard model . perhaps one of the qft specialists hereabouts could comment .
work done by the electric force is positive and not negative . the change in the potential energy of the electric field $\delta u$ is equal to the negative of the work done $w$ by the electric force . you have $$\delta u = -w$$ $$u_1=u_0-w&lt ; u_0$$ $$\therefore w&gt ; 0$$ and as $$w=\int f\cdot dl&gt ; 0$$ , we can say that $f$ acts in the same direction as $dl$ , i.e. it is attractive .
$\partial_i \frac{f}{1-f^2}=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1-f^2}{f} ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1}{f}-f ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( \partial_i ( \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( - \frac{1}{f}\partial_i f \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $= ( \frac{1}{1-f^2} ) \partial_i f ( \frac{1}{1-f^2} ) + ( \frac{f}{1-f^2} ) \partial_i f ( \frac{f}{1-f^2} ) $
i will try to address your question , though , as david says in the comments , it is evident that you have very little background in elementary particle physics . i will bring over an event much simpler than a display of an event that could show a higgs particle decay . here is a simple antiproton annihilation event whose end particles are recorded by their passage through a bubble chamber which also has a magnetic field perpendicular to the picture . the antiproton enters from below and hits a proton which is at rest , so not visible , in the bubble chamber liquid . it annihilates and eight pions come out , their momentum measured by the curvature , their mass by the ionisation track . where is the higgs field in this picture ? it permeates everything and at the point of interaction when the pions materialize it has supplied the masses to the quarks and antiquarks that they are made up of . the simulated higgs event display you have attached shows the decay products of the higgs boson . this particle is predicted by the standard model and it is necessary to find and confirm it in order to validate the sm . it appears because a higgs field exists , but it is a particle in the data set of particles predicted and mostly found by the sm . in the real experiment , a number of events with two photons , for example , have been accumulated so that the claim of seeing a higgs like particle has been established statistically . a lot of work remains to make sure that the bump seen has really the decay branching ratios and spin and statistics expected from the sm before the discovery of the higgs boson is established unequivocally . then we could state with some certainty that the standard model which depends on the existence of a higgs field is validated . so it should be clear that each individual event is not like a spider that can be dissected . it is an instant of the materialization of the fields and the experiment has to accumulate enough events to statistically establish an observation that validates a hypothesis .
there seem to be a lot of human body mechanical models , such as this one : as for applications , i have heard that sub-audio frequency vibrations have been considered as nonlethal weapons for riot control .
the short answer is probably " yes we can " , and possibly " we have already seen supernovae from the first galaxies " , in the form of long-duration gamma-ray bursts . grb 090429b has been given a redshift z=9.4 , beating the previous record-holder grb 090423 at z=8.2 . as we continue to watch the skies , we are seeing more and more of these objects , and we will gradually push back the boundary . i do not know what sets the upper limit on how far away we can see grbs but i do not think it is a coverage problem : swift covers something like a tenth of the sky . note that despite the high redshifts , they are probably not quite high enough to be the first stars . if someone reported a long grb at z=15 , then i would think it more likely . i am not an observer , so i am not sure about what colours and magnitudes are usually associated with supernova , but i can try . with a bit of googling , daniel kasen 's page suggests that they are relatively bright in most bands . off the top of my head , i think we see them most in optical , but that might just be a selection effect . i think , until now , we have been finding type ia supernova up to about z=1.5 . that boundary is being pushed , but i am not sure how . ( possibly improved ir spectroscopy from a hubble servicing ? ) the overall brightness of supernovae is of the order of 10$^{51}$ ergs . type ia 's have typical absolute visual magnitude -19.3 , according to wiki . as for other types of supernova , i suspect that as you move further out , isolating a single supernova in a low-resolution galaxy image is a major obstacle , but that is pure speculation on my part . that is , the supernova needs to be significantly brighter than the surrounding galaxy . fortunately , i think this is the case for grbs , but i am doubt it for other supernova . i am much less knowledgable on this than my previous answer , so i welcome corrections .
the confusion is coming form the fact that you are thinking in terms of the bra-ket physics notation without understanding how the underlying vector spaces are constructed . " kets " are vectors in a vector space , i.e. a set of objects on which vector-vector addition and vector-scalar multiplication is defined ( for some field of scalars ) . " bra"s are covectors , aka one-forms , defined as linear functions from a vector space to its field of scalars . they also form a vector space , and they exist even if we do not define an inner product on the set of kets . since the space of bras is a vector space , it can be tensored with another vector space such as the space of kets . this is defined just like any other tensor product of two vector spaces ( which is the cartesian product , equipped with an intuitive definition of addition and multiplication ) . you could write this down in two equivalent ways : $\vert \uparrow \rangle \otimes \langle \downarrow \vert$ or $\langle \downarrow \vert \otimes \vert \uparrow \rangle$ . they are structurally the same thing , just with a different convention for the ordering . ( likewise , coordinates in 3d can be ordered $ ( x , y , z ) $ or $ ( z , y , x ) $ without changing anything . ) the first case is often abbreviated $\vert \uparrow \rangle \langle \downarrow \vert$ since there is no risk of confusing it with anything else . ( of course , that notation is a bit dangerous since it suggest we have a way of associating a bra $\langle x \vert$ with every ket $\vert x \rangle $ , but that would be a mistake because we have not defined an inner product . ) on the other hand , if you tried to write the second case as $\langle \downarrow \vert \vert \uparrow \rangle$ or $\langle \downarrow \vert \uparrow \rangle$ , it could be misinterpreted as letting the bra ( which is a linear functional from the space of kets to the scalars ) act on the ket , which produces a scalar . that is a very different mathematical object . if the bras and kets have vector space dimension $n$ , then the object $\langle \downarrow \vert \uparrow \rangle$ has dimension 1 while the object $\langle \downarrow \vert \otimes \vert \uparrow \rangle$ has dimension $n^2$ .
the final condition for a realistic theory in physics is that its predictions ( of scattering amplitudes or correlators etc . ) have to agree with observations . this implies that the predictions have to be consistent and obey some general consistency conditions ( unitarity , non-negativity of probabilities , some symmetries , locality or approximate locality , and so on ) . for theoretical theories , it is just the general consistencies that hold . now , the task is to classify all possible theories and learn how to calculate with them . it turns out that the euclidean spacetime or world sheet is simply a simpler , more straightforward , more free-of-subtleties approach to produce a machine that calculates some scattering amplitudes or other observables . at least formally , the euclidean theories may be continued to analytic ones and vice versa . for nontrivial spacetime topologies , the euclidean objects are likely to be more manageable . for example , the world sheets in string theory ( think about a torus or pants diagrams etc . ) are much more well-behaved in the euclidean signature so we may consider this approach " primary " . covariant calculations in string theory are almost always done with the euclidean ones . the result may be continued to the minkowski momenta etc . and some of the consistency conditions above are still guaranteed to hold because of some properties of the complex calculus . in the light-cone gauge , we may work directly with the minkowski-signature world sheets . but we pay the price that the interaction points where strings split or join are singular and the direction of the " future time " is ambiguous . we must also include contact terms , higher-order interaction terms , to deal with some divergences caused by the singular world sheets , but when these things are summed over , we may prove that the resulting amplitudes agree with the covariantly computed ones ( in the euclidean signature ) . gravity in $d\geq 4$ ( and maybe 3 ) suffers from the " negative norm conformal factor " . the euclideanized einstein-hilbert action $\int r \sqrt{g}$ is no longer positively definite . in particular , if you consider scalar waves that scale the metric by an overall number , $g_{\mu\nu}=e^f\eta_{\mu\nu}$ , and derive the kinetic term for $f$ , it will have the opposite sign than the kinetic term for other components of the metric tensor ( the physical polarizations of the gravitational waves , like $g_{xy}$ ) . it follows that the action will be bounded neither from below nor from above , and $\exp ( -s_e ) $ in the euclidean path integral will diverge in some region of the configuration space . in this sense , people believe that the minkowskian path integral must be the " more kosher one " for higher-dimensional gravity . but this is a bit empty statement because at the quantum level , higher-dimensional gravity obtained as a direct quantization of einstein 's equations is inconsistent , anyway . and string theory which is consistent and contains gravity does not give us any tool to directly rewrite the path integral in terms of spacetime fields including the metric ; it is not a field theory in the ordinary sense . so the preference for the " minkowski signature " is a bit vacuous . after all , the minkowskian action is not bounded from either side , either . this is considered " not to be a problem " because the integrand is $\exp ( is ) $ which still has the absolute value equal to one , so it does not diverge . but i would personally say that the unboundedness of the euclidean action is the " same " problem for the minkowskian path integral . quite generally , the wick rotation is extremely important in quantum field theory and it is actually even more important in quantum gravity or places with many spacetime ( or world sheet ) topologies , i.e. in situations where one has many different " time variables " in which we might try to expand things . one should not be afraid but at the end , whatever theory he deals with , he gets some amplitudes whose self-consistency ( and/or consistency with observations ) must be verified . with some " good rules of behavior " while wick-rotating , one may be " pretty sure " that some tests will be passed .
these are selection rules for the electric dipole radiation . the transition from the excited state to a lower state of an atom is governed by the following matrix element : $$c \cdot \langle f | \hat{p} | i\rangle$$ you could also write the sum of positions of electrons $\sigma\hat{r}$ instead of their total momentum $\hat{p}$ in the middle . this simple form of the operator in the middle is because at low enough frequencies , i.e. long enough wavelength , the atom simply finds itself in a uniform field , anf the electric potential $\phi$ for a uniform field is linear in $\hat{r}$ . equivalently , the matrix element may be converted from $\hat{r}$ to $\hat{p}$ and vice versa by realizing that the commutator of the hamiltonian with $\hat{x}$ is proportional to $\hat{p}$ . at any rate , the operator in the middle is a 3-dimensional vector that only acts on the positions or momenta of the electrons , not their spins . so it has to commute with the total spin operator $\hat{s}$ of the electrons , and $\delta s=0$ as a consequence . on the other hand , it is a vector , i.e. a $j=1$ object as far as the so ( 3 ) transformations go , and by combining its $j=1$ angular momentum with the orbital angular momentum $l_i$ of the initial state , you may get the final $l_f$ being $l_i\pm 1$ or $l_i$ according to the basic rules of the addition of angular momentum . you may imagine that you are just adding two vectors of specified lengths , $l_i$ and $1$ , and depending on their relative angle , the length of the sum may go from $l_i-1$ to $l_i+1$ . your list did not allow $l_f=l_i$ and i think you are right that it is typically forbidden as well because it violates parity . the angular momentum to parity goes like $ ( -1 ) ^l$ , i guess , but because the vector operator is parity-odd , the initial and final states have to differ by their parity as well , which means that only $\delta l=\pm 1$ is allowed . everyone , please correct me if i am saying something incorrectly . best wishes lubos
daylight does not contain equal amounts of all colours . the spectrum of daylight peaks around yellow/green , and the amount of energy falls off quite sharply as you move to the blue end of the spectrum . that means even if the width of the blue filter is the same as a green filter , you will measure less light coming through the blue filter . if you are using artificial light the effect will be even more pronounced as artificial light generally has a lower colour temperature than sunlight . incidentally , the response of your solar cell will also vary with the colour of the light , and this may well also be affecting your results . i can not lay my hands on a typical frequency response for a silicon solar cell , but i think the response falls off at the blue end of the spectrum .
the resolution is controlled by diffraction at the smallest part of the lens system . the wikipedia article on angular resolution goes into this in some detail . to quote the headline from this article , for a camera the spatial resolution at the detector ( or film ) is given by : $$ \delta \ell = 1.22 \frac{f\lambda}{d} $$ where $f$ is the distance from the plane of the lens to the detector , $\lambda$ is the wavelength of the light and $d$ is the camera aperture . making the pixel size smaller than $ \delta \ell$ will not do any harm , but it will not make the pictures any sharper . i do not know if smartphone cameras contain a variable aperture . with conventional cameras larger apertures produce less diffraction so the picture quality should actually improve in low light . however larger apertures expose a larger area of lens and optical aberration dominates the quality . the end result is that there is an optimum aperture below which diffraction dominates and above which optical aberration dominates . incidentally , the poor performance at low light probably is not due to diffraction . i would guess it is just that the signal to noise ratio of the detected light falls so far the pictures get very noisy .
you ask if the region where the field is defined has holes in it , what happens ? well , in that case you can define the vector potential on simply-connected sub-regions $r_i$ whose intersection is the whole non-simply connected region $r$ and such that they differ by only by a gauge transformation on the regions of overlap . this is a physically well-motivated thing to do , because it means that up to gauge transformation , the vector potential can be defined on $r$ . here 's a simple example . let $\ell=\{ ( x , y , z ) \ , |\ , x=0 , y=0\}$ denote the $z$-axis , then the region $r=\mathbb r^2\setminus\ell$ is not simply connected . to see this , simply consider a closed loop enclosing the axis ; there is no way to continuously shrink it down to a point while staying in $r$ . because of this , the there is no $\mathbf a$ defined on all of $r$ . however , let $\ell_+$ denote the positive $z$-axis , and let $\ell_-$ denote the negative $z$-axis , then the regions $r_- = \mathbb r^3\setminus \ell_+$ and $r_+ = \mathbb r^3\setminus \ell_-$ have the property that they are each simply connected and $r = r_+\cap r_-$ . moreover , we can define a vector potential $\mathbf a_+$ on $r_+$ and $\mathbf a_-$ on $r_-$ such that there exists a scalar function $\lambda$ for which \begin{align} \mathbf a_+ ( \mathbf x ) - \mathbf a_- ( \mathbf x ) = \nabla\lambda ( \mathbf x ) , \qquad \text{for all $\mathbf x\in r$} \end{align} in fact , here are the explicit expressions in spherical coordinates $ ( r , \theta , \phi ) $: \begin{align} \mathbf a_{\pm} and = -g\frac{\cos\theta\mp 1}{r\sin\theta}\hat{\boldsymbol \phi} \end{align} i will leave it to you to determine $\lambda$ ; it is a fun exercise . what are the physical consequences of that ? well , in the context of quantum mechanics , these sorts of topological issues are physically relevant ( i am unsure if there are examples in which they are relevant at the classical level , but i do not think so ) . i will not go into the details here ( unless perhaps there is some demand ) , but the very vector potentials i wrote down in the example above come up when discussing magnetic monopoles and the quantization of electric charge ( see dirac quantization ) . these topological issues also become significant in discussing the famous aharonov-bohm effect .
yes , and the formula you already have still works . take z to be the optical path length : refractive index n times physical distance . a gaussian beam in glass diverges in exactly the same way as in free space , only ' squeezed ' in the z direction by a factor of n .
the answer is $d/2$ . since this is such a nice answer , there might be a really simple way to obtain it that does not involve any differential equations . i do not know one , though . here 's my way of solving it . let 's adjust the problem so that the turtle at the origin swims in the $-y$ direction , and the other turtle swims straight towards him ( i did this because i thought it would make the signs easier for me ) . now , let 's switch the rest frame so that there is a current flowing in the $+y$ direction , the turtle at the origin sits on a little island , and the other turtle always swims towards the origin , but is pulled off-course by the current . we can now use geometry to figure out which direction the turtle moves at any point in the plane . at point $ ( x , y ) $ , the turtle swims with velocity $$\left ( \frac{-x}{\sqrt{x^2+y^2}} , \frac{-y}{\sqrt{x^2+y^2}} \right ) . $$ . taking the current into account , the turtle 's velocity is $$\left ( \frac{-x}{\sqrt{x^2+y^2}} , 1-\frac{y}{\sqrt{x^2+y^2}} \right ) . $$ this gives us the differential equation for the turtle 's path $$\frac{dy}{dx} = \frac{y - \sqrt{x^2+y^2}}{x} . $$ maple gives us the solution to this : $$ y = \frac{1}{2} \left ( c-\frac{x^2}{c} \right ) , $$ which can easily be checked to be a solution for $c&gt ; 0$ ( although i would like to see how to solve it without a computer algebra system ) . now , we know that when $y=0$ , $x=d$ . this gives $c=d$ . when $x=0$ , we have $y=d/2$ .
with no resistance , the full voltage is applied to the fan , and you get mechanical work done , at whatever efficiency the fan itself is capable of . never minding the fan itself , so far as the electrical aspect goes , you could say it is 100% efficient . with resistance in the system , for example about equal to the resistance of the fan , you have less current flowing through the system . half as much . same voltage . so the system is using half the power . the fan , now one part of a voltage divider circuit , is getting half the voltage , and getting half the current . it runs slower , doing less , doing about 1/4 the work . 1/4 of the fan action divided by 1/2 power going into the system means that the system is 50% efficient . imagine a lot of resistance in the system , megohms . you will have only a trickle of current , say a microamp . the fan barely moves . the system will be using very little power , but most of that little power is just making the resistor hot . or rather , a small fraction of a degree warmer than ambient temperature . the system efficiency is close to zero .
$-i\hbar\vec{\nabla}|\psi\rangle$ is not a valid notation . the nabla operator is defined in three-dimensional euclidean space , not the in the hilbert space of quantum states . when the author says $\hat{\boldsymbol{p}}=-i\hbar\vec{\nabla}$ he does not mean the momentum operator defined in the state space , but the space of wavefunctions . then $\hat{\boldsymbol{p}}\psi ( \boldsymbol{r} ) =-i\hbar\vec{\nabla}\psi ( \boldsymbol{r} ) $ .
$j$ can be determined experimentally by knowing , for example , $t_c$ . through a theoretical model including the structure and the species involved you can estimate the value for $j$ through the overlap integrals of the electrons involved . here you can read more about exchange interaction , which is the basics behind magnetism ( which is a quantum phenomena ) .
i get the impression that op is referring to normalized ricci flow ( nrf ) : $$ \frac{1}{2} \partial_t g_{\mu\nu} ~=~ -r_{\mu\nu} + \frac{\langle r \rangle}{n} g_{\mu\nu}~ . $$ here $\langle r \rangle$ is the average scalar curvature over the full space-time $m$ . the average procedure is often weighted with an einstein-hilbert boltzmann factor . it is just a number ( as opposed to a space-time dependent scalar quantity ) . also $n$ is the space-time dimension , which is fixed , and hence cannot be easily varied as op suggests .
the answer to your question is very wide and include many phenomena . there is no single mechanism that converts absorbed energy into heat . i will give you a general overview on this topic . heat is transferred by radiation which is in general an electromagnetic wave ( not light only ) . for example , ir radiation alone provides 49% of the heat provided to earth from sun radiation . have a look at heat section of this page . in electromagnetic spectrum , the wavelength varies significantly , which means that materials respond differently to different ranges of wavelengths . for example ( listing from long wavelength to short wavelength ) : radio waves : the wavelength here is very long such that the materials constituents ( atoms and molecules ) do not sense the waves . thus most solids are transparent to radio waves , which means there is no wave-material interaction . there is no heat generated . the transparency is clear as you can get cellphone reception in your home as the wave simply travel through walls and our bodies . microwaves : the wavelength here is shorter , the photons energy in this level are comparable to the energy levels of the molecular rotational levels , which means that a molecule could absorb a microwave photon and start to rotate . this rotation is a form of kinetic energy of molecules which is translated as heat when you look at the whole ensemble of molecules . this mechanism is the mechanism known for microwave heating in your kitchen . ir : the photon energy in this range is comparable to molecular vibrational levels . so if a molecule absorbed an ir photon it will vibrate , which is translated into heat when you look at the whole ensemble of molecules . visible light : things get complicated here , the photon energy here is comparable to electronic levels within an atom . there is no simple direct explanation of how photon energy is transformed into heat . one can argue for visible light that an electron can absorb a photon and radiate it without increasing kinetic energy of atoms ( no increase oh heat ) . that is true but that is a very simplistic picture of what happens in reality . first because the visible light is continuous spectrum , while the photons that can be absorbed by electrons are discrete . so there are many visible light photons with wavelengths that electrons can not absorb , which could be absorbed by ions or affect polar bonds somehow if they existed in the material . second , some materials like solids ( where heat by radiation is efficient ) have energy bands rather than the simplistic electron levels concept . so the reaction of materials to visible light in this case is complex . but one of the mechanism of transforming photon energy to heat that i can think of here is the absorption of visible light by ion lattice in some crystals . uv : the photons in this range have enough energy to ionize the atoms they hit ( by liberating an electron from its orbital ) . again , there is no simple direct explanation of how photon energy is transformed into heat . one possible route is the inelastic scattering of electrons with materials atoms . for example , have a look at page 5 of this report where it is mentioned that inelastic electron scattering can induce phonons ( lattice vibration , which is a form of kinetic energy of atoms ) x rays : the photon energy in this range is much larger than the energy required to move electron from one level to other . x rays photons can ionize an atom and become lower energy photon through compton scattering . there is no simple direct explanation of how photon energy is transformed into heat . briefly , heating by radiation in general can not be attributed to a simple explanation . the mechanisms through which photons energy is transformed into heat depend on the energy of the photon and the properties of the material . the simplistic picture of a photon being absorbed by electron is narrow to explain conversion to heat because it only describes a single electron in a free atom . that is a narrow view of material properties . have a look here and the nice figure of non-ionizing radiation section of this page hopefully that was helpful
the solution is to realize that the steady-state solution of a harmonically driven system must also oscillate harmonically . ( as regards your solution , this means that spectrally the $c_i ( \omega ) $ are delta functions , which resolves your contradiction . ) thus one usually begins by postulating the oscillatory ansatz $$c_a ( t ) =c_a e^{-i\omega_a t} , \ , \ , c_b ( t ) =c_b e^{-i\omega_b t} , $$ where the $c_a$ and $c_b$ are now constants . as an ansatz this is harmless and if it turns out to not be a solution you can drop it ( but as it happens it will ) . your schrödinger equation thus reads $$ \begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v e^{i t \omega _d} \\ \frac{1}{2} v e^{-i t \omega _d} and \frac{\omega _0}{2} \end{pmatrix} \begin{pmatrix} c_a ( t ) \\ c_b ( t ) \end{pmatrix} = i\frac d{dt} \begin{pmatrix} c_a ( t ) \\ c_b ( t ) \end{pmatrix} $$ so $$ \begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v e^{i t \omega _d} \\ \frac{1}{2} v e^{-i t \omega _d} and \frac{\omega _0}{2} \end{pmatrix} \begin{pmatrix} c_a e^{-i\omega_a t}\\ c_b e^{-i\omega_b t} \end{pmatrix} = \begin{pmatrix} \omega_a c_a e^{-i\omega_a t} \\ \omega_b c_b e^{-i\omega_b t} \end{pmatrix} $$ or $$ \left\{ \begin{array}{ccc} -\frac{\omega _0}{2} c_a e^{-i\omega_a t} and +\frac{1}{2} v e^{i t \omega _d}c_b e^{-i\omega_b t} and = \omega_a c_a e^{-i\omega_a t} , \\ \frac{1}{2} v e^{-i t \omega _d} c_a e^{-i\omega_a t} and + \frac{\omega _0}{2} c_b e^{-i\omega_b t} and = \omega_b c_b e^{-i\omega_b t} . \end{array} \right . $$ this needs you to set $\omega_a+\omega_d=\omega_b$ , after which you can eliminate the time dependence . that leaves you with the simple linear system $$ \left\{ \begin{array}{ccc} -\frac{\omega _0}{2} c_a +\frac{1}{2} v c_b = \omega_a c_a , \\ \phantom+ \frac{1}{2} v c_a + \frac{\omega _0}{2} c_b = ( \omega_a+\omega_d ) c_b , \end{array} \right . $$ which is an eigenvalue system for the hamiltonian $h=\begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v \\ \frac{1}{2} v and \frac{\omega _0}{2} -\omega_d \end{pmatrix}$ . since you tagged this as homework , i will leave the calculation here , as i am sure you are better off calculating eigenvectors and eigenvalues on your own .
you are correct - the force is constant in all four cases . since each of the situations describes a " uniform spherical shell of matter , " you can assume that the mass is concentrated at the center of that shell , as per the shell theorem cited . if you have learned gauss 's law for electric fields , it can be applied to this problem . gravitational force , following the same inverse square relationship as the coulomb force , also obeys gauss 's law . set up a spherical gaussian surface concentric with the spherical shells and passing through the particle . the total gravitational flux through this surface is constant in all four cases , since the total mass enclosed is constant . moreover , since each sphere is uniform , the gravitational force is evenly distributed across the surface . therefore , the gravitational force on the particle is the same in all four cases .
i have changed my answer substantially after further thought . in the frame at rest with respect to the slits , interference is determined by how many wavelengths fit between one slit and the screen compared to how many wavelengths fit between the other slit and the screen . in the frame in motion with respect to the screen , three things are different : the wavelength is altered by the doppler effect the screen-slit distance is altered by a lorentz contraction , and the observer claims the light hitting the screen does not come from where the slits are at that instant but from where they were located a short time ago . the path length for the light is different than the instantaneous separation between the slit and screen . in this frame the length $l$ that the light travels is given by $l = \frac{z}{\gamma} - vt $ $l = \frac{z}{\gamma} - vl/c$ $l = \frac{z}{\gamma ( 1+\frac{v}{c} ) }$ this is the same factor by which the doppler effect alters the wavelength , so observers in both reference frames agree on the number of wavelengths that lie along the path that the light travelled .
there is a nice reason for this , which witten often explains . imagine that your three dimensional space is the boundary of a four-dimensional space , for example , you can imagine that space is the surface z=0 of regular four dimensional space x , y , z , t . further , you can imagine that space is closed into a sphere , which does not affect things except for some boundary conditions at infinity ( the physics should not care about such things , also note that this is implicitly euclidean ) . if you close the three dimensional space-time into a sphere , the interior of the sphere is like the rest of the values of z for the plane case . you can extend any 3 dimensional gauge field configuration to the imaginary fourth dimension arbitrarily , so that any gauge field on the surface of the sphere can be extended to many different gauge fields on the interior . on the interior , you can construct the manifestly gauge invariant operator : $$ \epsilon_{\mu\nu\lambda\sigma} f^{\mu\nu}f^{\lambda\sigma} = f\tilde{f}$$ it is important to note that this quantity is a perfect divergence : $$ f\tilde{f} = \partial_\mu j^\mu_\mathrm{cs} $$ where j is the chern-simons current in 4-dimensions . using stokes theorem , for any four-dimensional gauge field configuration $$ \int f\tilde{f} = \int d ( *j ) = \int_\partial *j $$ where the last equality is stoke 's theorem , and the previous equality is writing the diverence of a current as the poincare dual of a three-form . so the manifestly gauge invariant $f\tilde{f}$ integral on any gauge field on the interior of the sphere is equal to the integral of the three form *j on the boundary of the sphere . so the integral of *j must be gauge invariant . i did not work out the actual form of *j , but it is the quantity you are trying to prove gauge invariant . although witten 's argument is conceptually illuminating , so it is the correct argument , verifying gauge invariance explicitly is not much more difficult than understanding all parts of the argument . still , it is good to know the conceptual reason , because the reason the chern-simons style things are important is exactly because they are the boundary terms of integrals of those gauge invariant field tensor combinations which are perfect derivatives .
under the constraints of the problem , then yes , what you are doing is correct . if you were not required to use conservation of energy , then it would probably be easier to calculate the vertical component of the initial velocity and use 1d kinematics .
like prahar had said , the problem reduces fairly simply in momentum-space . we note that , in such space : $\hat x = i\hbar\frac{\partial}{\partial p}$ and $\hat p=p$ , thus , using some auxiliary function $f$: $$ [ \hat x , \hat g ( \hat p ) ] f=i\hbar\frac{\partial ( \hat gf ) }{\partial p}-i\hbar\ , \hat g\frac{\partial f}{\partial p}=i\hbar\frac{\partial \hat g}{\partial p}f $$ by applying the product rule and reducing , this yields the correct result .
reading this may help you out : http://en.wikipedia.org/wiki/fresnel_equations what you are describing seems to be some sort of light pipe , where you are counting on internal reflection to transport the light along the strip . whether yor pipe is surrounded by a higher refractive index material , as in optic fiber , or not , as in surrounded by air , to keep all , or at least most , of the light from refracting out of the pipe , you need the incidence angle to be shallow enough . on a matte surface , rather than having a well defined incidence angle for a ray of light , this will be spread over a range of different angles , due to the irregularities of the surface . this may help understanding that . it will depend on your exact configuration , but if you look at the graphs of the fresnel formulas above , that is almost certainly going to mean that in a light pipe configuration more light will refract out of a frosted surface than a smooth one . the new kindle paperwhite uses a smooth light pipe , with little protubeances at selected points to guide light out of the pipe , they used to have a nice video explaining it .
per wikipedia , the electromagnetic tensor $f^{\mu \nu}$ contributes to the stress energy tensor $t^{\mu \nu}$ by $$t^{\mu \nu} = \frac{1}{\mu_0} \left ( f^{\mu \alpha} g_{\alpha \beta} f^{\nu \beta} - \frac{1}{4} g^{\mu \nu} f^{\gamma \delta} f_{\gamma \delta} \right ) $$ the einstein equations govern how the stress-energy tensor is coupled to spacetime curvature . since the magnetic field is entirely captured by the electromagnetic tensor , the answer is yes , magnetic fields contribute to gravitation .
there is quite a big controversy these days about the correct definition of the entropy in the microcanonical ensemble ( the debate between the gibbs and boltzmann entropy ) , which is closely related to the question . everyone agrees that the correct definition of the density matrix is given by $$\rho ( e ) =\frac{\delta ( e-h ) }{\omega ( e ) } , $$ where $h$ is the hamiltonian and $$ \omega ( e ) =tr\ , \delta ( e-h ) . $$ then the question is the correct definition of the entropy . boltzmann says $s_b=\ln \omega ( e ) $ , whereas gibbs argued $s_g=\ln \omega ( e ) $ where $$ \omega ( e ) =\int_0^e\omega ( e ) de . $$ in the text quoted by the op , the partition function corresponds to $\omega ( e ) $ . note that in most cases , in the thermodynamics limit , both entropies gives the same result . the question arises in the case of small systems and special cases with bounded from above spectra . hilbert et al . ( arxiv:1408.5382 and arxiv:1304.2066 ) argue that only the gibbs entropy is thermodynamically consistent . i must say that i find their arguments compelling , and that of their opponents , given in at least two comments of their papers , not at all .
maybe a way for you to be not confused is to imagine a time dependence . for instance , let suppose three times $t_i , t , t_f$ with $t_i &lt ; t &lt ; t_f$ . one may suppose that the particle is in the initial state $|a\rangle$ at time $t_i$ , is in the final state $|z\rangle$ at time $t_f$ , and , at the intermediary time $t$ is in one of the $2$ states $|s_1\rangle$ or $|s_2\rangle$ . the law of composition of amplitudes say that : $\langle z , t_f|a , t_i\rangle =\langle z , t_f|s_1 , t\rangle\langle s_1 , t|a , t_i\rangle + \langle z , t_f|s_2 , t\rangle\langle s_2 , t|a , t_i\rangle$ this is true for all $z$ , so we have : $|a , t_i\rangle = |s_1 , t\rangle\langle s_1 , t|a , t_i\rangle + |s_2 , t\rangle\langle s_2 , t|a , t_i\rangle$ now , you may interpret this equation as follows : given that the particle is in the state $|a\rangle$ at time $t_i$ , the probability amplitude to find the particle in the state $|s_1\rangle$ , at time $t$ , is : $\langle s_1 , t|a , t_i\rangle$
you may check the wikipedia page for neutrino oscillations https://en.wikipedia.org/wiki/neutrino_oscillations where you may also find the derivation of the formula for the neutrino oscillations in the case of two flavors ( in which case the oscillations are harmonic and simple ) $$ p_{\alpha\rightarrow\beta , \alpha\neq\beta} = \sin^{2} ( 2\theta ) \ , \sin^{2} \left ( \frac{\delta m^2 l}{4e}\right ) \quad \mathrm{ ( natural\ , units ) } $$ numerically , in si units , that is $$ p_{\alpha\rightarrow\beta , \alpha\neq\beta} = \sin^{2} ( 2\theta ) \ , \sin^{2}\left ( 1.267 \frac{\delta m^2 l}{e} \frac{\rm gev}{\rm ev^{2}\ , \rm km}\right ) . $$ you may see that the probability depends on the length $l$ in kilometers – you may write $l=ct$ using the time $t$ as well , if you wish , $c$ is the speed of light ( their speed is undetectably different from $c$ ) . however , the frequency also depends on the neutrino energy $e$ expressed in ${\rm gev}$ above . the factor $\delta m^2$ is the difference between the eigenvalues of $m^2$ . for the three species , these are $$\delta m_{12}^2 = 7.59\times 10^{-5} {\rm ev}^2$$ $$\delta m_{32}^2 \approx \delta m_{13}^2 = 2.32 \times 10^{-3}{\rm ev}^2$$ two of the neutrino mass eigenvalues are very close to each other . this results in very slow oscillations ( solar oscillations , $12$ ) . the remaining third eigenvalue is further from them and it results in faster oscillations ( seen in atmospheric oscillations , $23$ ) . at any rate , the units in the formula above are designed so that $e$ is of order one ( i.e. . gev ) . because $\delta m^2$ is 3-5 orders of magnitudes smaller than 1 , $l$ of the wavelength is of order between thousands and millions of kilometers which translates roughly to something between milliseconds and seconds . you may calculate the precise frequency for yourself – the periodicity is proportional to the neutrino energy . most of these things have been known for a few decades . what was observed in the last year was a matrix element that allows the $13$ transmutation " directly " . the previous solar and atmospheric oscillations de facto measured the angles and mass differences $12$ and $23$ only . the accuracy of all the parameters is rather close to one percent these days .
the answer is yes in some unintersting senses : take two gravitational attracting point particles and set them at rest . they will attract each other and their velocity will go to $\infty$ in finite time . note this does not contradict conservation of energy since the gravitational potential energy is proportional to $-1/r$ . this is not so interesting since it is just telling you that things under gravity collide . but its technically important in dealing with the problem of gravitationally attracting bodies . now a more intersting question : is there a situation where the speed of a particle goes to infinity without it just being a collision of two bodies ? suprisingly , the answer to this question is yes , even in a very natural setting . the great example is given xia in 1995 ( z . xia , “the existence of noncollision singularities in newtonian systems , ” annals math . 135 , 411-468 , 1992 ) . his example is five bodies gravitational interacting . with the right initial conditions one of the bodies can be made to oscillate faster with the frequency and amplitude going to infinity in finite amount of time . added here is an image . the four masses $m$ are paired into two binary systems which rotate in opposite directions . the little mass $m$ oscillates up and down faster . it is behavior becomes singular in finite time .
actually , most research suggests that interactions of two galaxies ( i.e. . , " mergers " or collisions ) is what determines the shapes of galaxies , and not necessarily gravity alone . this astronomy now article from a few years ago goes a bit more into the details . it is true that there is a strong force pulling us inwards ( on the order of a billion newtons ) , so you indeed could say that a galaxy is collapsing , but it would take something like $10^{100}$ years before the stars would fall into the supermassive black hole in the center . on that time-scale , it really is not a statement worth making , imo .
i will add a small disclaimer as well , i am a mathy with little to no physics background , so if any of the below needs expanding , feel free to ask ! ( though i will mention that i felt a lot more comfortable with this stuff when i first computed the induced actions i will mention below and really got my hands dirty verifying everything . ) to see that there is only one $ ( 1,1 ) $-form coming from each fixed point in the first example , you need to get a little into how the blowing up actually adds anything to the cohomology . for this , there are two steps . ( 1 ) what is the blowup ? ( 2 ) how are we adding classes in cohomology ? for ( 1 ) , note we are taking the quotient of $t^4$ by $\mathbb{z}_2$ and the problem is this generates 16 singularities . you can indeed blowup these singularities , but there is no guarantee this actually resolves them , so after one simple blowup you may not have a smooth object yet ( i.e. . , a resolution ! ) and you may need to continue blowing up to actually resolve the singularities . ( this is why example 2 has a more exotic resolution . ) to see that the resolution is complete after the first blowup , you have to somehow note the action of $\mathbb{z}_2$ , induced on the $\mathbb{p}^1$s lying above each fixed point is trivial ( so the blownup quotient is smooth ) . a simple way for this is to note the involution acts as $-1$ on the differentials , ( cotangent space ) so acts as $-1$ on the tangent space at each singular point , and the blowup replaces each point with the projectivization of the normal bundle at that point , but the projectivation of the action by $-1$ is trivial and the $\mathbb{p}^1$s are indeed fixed by the action of $\mathbb{z}_2$ . for problem 2 , we are looking at the same situation except the action is not $-1$ , and does not projectivize to something trivial . hence , after the first blowup , the $\mathbb{p}^1$s are not fixed and you will need to blowup the fixed points ( there are two ) on each $\mathbb{p}^1$ lying above a fixed point . if you carry the action up on the $\mathbb{p}^1$ and once more on the blowup of the fixed points on the $\mathbb{p}^1$ you see that the action is trivial on the second blowup , so you have got 2 fixed $\mathbb{p}^1$s lying above each original fixed point . now , the fun comes in with part ( 2 ) . why does this add $ ( 1,1 ) $-forms and nothing else ? for this , perhaps the original source is grothendieck 's sga v , in which ( if i recall correctly ) section vii is largely devoted to proving the following result ( which i have simplified greatly for the case with dimensional examples ) : let $x$ be a two dimensional smooth projective irreducible variety with $y$ a finite set of points on $x$ . let $\widetilde{x}$ be the blowup of $x$ along $y$ . then $$ h^k\left ( \widetilde{x}\right ) \simeq h^{k-2} ( y ) \oplus h^k ( x ) . $$ i will be present the full version below , but it is far easier for the two-folds , and just follows from topological poincare duality , so lets finish up the examples first ! with duality , we need only work out $$ h^0\left ( \widetilde{x}\right ) \simeq h^{-2} ( y ) \oplus h^0 ( x ) , $$ $$ h^1\left ( \widetilde{x}\right ) \simeq h^{-1} ( y ) \oplus h^1 ( x ) , $$ and $$ h^2\left ( \widetilde{x}\right ) \simeq h^{0} ( y ) \oplus h^2 ( x ) . $$ now $h^{-2} ( y ) = h^{-1} ( y ) = 0$ , so the only change the blowup has in cohomology is on $h^2$ . ( as per your question , note that $h^{2,2}\left ( \widetilde{x}\right ) \simeq h^{0,0}\left ( \widetilde{x}\right ) $ so this shows there are no $ ( 2,2 ) $-classes coming from the blowup . ) for $h^0 ( y ) $ , note that each point has a $ ( 0,0 ) $-class , so each point contributes 1 class in $h^{1,1}\left ( \widetilde{x}\right ) $ as desired . in example two , recall we had two fixed points to blowup , lying above the original fixed points , so there are 18 $ ( 0,0 ) $-classes coming in to the final numbers . the full sga result is as follows : let $x$ be a smooth projective irreducible variety of dimension $n$ , and let $y_1 , \ldots , y_r \subset x$ be mutually disjoint closed irreducible subvarieties of $x$ of codimension $d\geq 2$ . let $y$ be the union of the $y_i$ and let $\widetilde{x}\to x$ be the blowup of $x$ along $y$ . then $$ h^k\left ( \widetilde{x}\right ) \simeq h^{k-2} ( y ) \oplus \cdots \oplus h^{k-2 ( d-2 ) } ( y ) \xi^{d-2} \oplus h^k ( x ) , $$ where $\xi$ represents the line bundle $\mathcal{o} ( 1 ) $ . ( this is how you can define chern classes , using the $\xi^i$ as a basis for the cohomology of the blowup - or any vector bundle in general - if you have seen these before . ) so above , with the nice two-fold examples , the $\xi$ terms were not necessary as the exponents are not big enough . note though , that as any $n$-fold product of elliptic curves ( or curves in general ) only has fixed points in the singular locus , so you will only get $ ( 1,1 ) $-classes in the resolution . you have to look at more exotic construction , like $e\times s/\mathbb{z}_2$ where $e$ is an elliptic curve and $s$ is a surface , where the fixed locus on $s$ may now have curves , so you will find $ ( 2,1 ) $-classes in the resolution , and more and more complicated fixed loci ( can ) give higher classes in the cohomology . ( because of your tags , i will mention the $e\times s$ case is the classic borcea-voisin construction when $s$ is a k3 surface and the involutions are non-symplectic . ) added : let 's look at the second blowup explicitly . we know the action of $\mathbb{z}_3$ on the two tori induces an action on the respective $1$-forms of multiplication by $\omega$ and $\omega^{-1}$ . to find how this affects the blowup , we need to see what the action on the tangent space of a fixed point is . note these 1-forms give us a basis for the cotangent space of each torus , which is isomorphic to the tangent space ( which in general takes the transpose action , but in our one dimensional case that means it is the same action ) so the $\mathbb{z}_3$ action induces multiplication by $\omega$ and $\omega^{-1}$ on the respective tangent spaces . to make things a little simpler , lets give our ( complex ) two-fold a name , let $$ x = t^2\times t^2/\mathbb{z}_3 . $$ thus , the blowup of an affine chart about a fixed point ( which we can set as the origin ) can be written as $$ \widetilde{x} = \left\{\big ( ( x , y ) , [ u:v ] \big ) \mid xu = vy\right\} \subset \mathbb{a}^2\times \mathbb{p}^1 , $$ the $\mathbb{p}^1$ being what we have added in the blowup . to double check this is correct , note that any point $ ( x , y ) \neq ( 0,0 ) $ has a single pair $ [ u:v ] $ satisfying the condition $xu=vy$ , whereas the origin $ ( x , y ) = ( 0,0 ) $ has the entire $\mathbb{p}^1$ satisfying $xu=vy$ , so we really do have a manifold that corresponds one-to-one with $x$ , away from the singularity , and the $\mathbb{p}^1$ lies above the fixed point . aside : this $\mathbb{p}^1$ arises because it is the ( projectivization of the ) normal bundle of the singular point on $x$ . in general , if you blowup a $d$-dimensional submanifold on an $n$-dimensional manifold , you get a $\mathbb{p}^{d-1}$ lying above the locus you are blowing up . ( e . g . , on a surface , blowing up a curve gives a $\mathbb{p}^1$ lying above each point on the curve , whereas blowing up a point gives a $\mathbb{p}^2$ lying above the point . ) the idea of blowing up in general is ( loosely ) you want to add all the information `coliding ' at the singularity . if you have never seen much about the resolution of singularities , i recommend glossing the wiki page as it gives a nice description of blowing up projective space , affine space , complex manifolds , and ( perhaps best to ignore ) the heavy schemes way . back to work : but this means we know the action on the $\mathbb{p}^1$ that lies over the fixed point , since we know the action of $\mathbb{z}_3$ on the tangent space of the fixed point above . we have $$ [ u:v ] \mapsto [ \omega u:\omega^{-1}v ] . $$ this action is not trivial , so the $\mathbb{p}^1$ is not fixed by the action , so we have simply added a $\mathbb{p}^1$ but not changed the fixed locus at the point , so the quotient still has a singularity here ! things get worse though , as this $\mathbb{p}^1$ actually has two fixed points ! the origin $ [ 0:v ] $ and the point at infinity $ [ u:0 ] $ . ( our manifold has 18 fixed points now , but they are all isolated fixed points , so they are all the same type of singularity as before . ) we must now try to resolve both . we can take the chart $v\neq 0$ and note that we can write $$ \widetilde{x} = \left\{\left ( ( x , \frac{xu}{v} ) , \left [ \frac{u}{v}:1\right ] \right ) \right\} , $$ i.e. , there are only two degrees of freedom . this means we can write our affine chart ( still on a two-fold ) in the coordinates $$ y := \{ ( x , z ) \} $$ where $z = u/v$ . note that action of $\mathbb{z}_3$ on $z$ is $$ z = \frac{u}{v} \mapsto \frac{\omega u}{\omega^{-1} v} = \omega^{-1}\frac{u}{v} = \omega^{-1}z . $$ blowing up this we have the same story as above , and $$ \widetilde{y} = \left\{\big ( ( x , z ) , [ s:t ] \big ) \mid xs = zt\right\} . $$ this time we do not have the action on the tangent space about the fixed point $ ( 0,0 ) $ in $\widetilde{x}$ , but we can actually decipher it from above . we know the action on $ [ u:v ] $ in the first blowup , so the condition $xu = vy$ tells us $$ xu \mapsto \omega xu $$ and $$ vy \mapsto \omega^{-1}vy , $$ so we have $ ( x , y ) \mapsto ( \omega^{-1}x , \omega y ) $ . thus , our action on the $\mathbb{a}^2$ in the second blowup is $ ( x , z ) \mapsto ( \omega^{-1} x , \omega^{-1} z ) $ . hence , the condition $xs = ut$ in the second blowup means the ( projective ! ) action of $\mathbb{z}_3$ on this $\mathbb{p}^1$ is trivial , and our fixed locus now has a whole $\mathbb{p}^1$ instead of just a point . thus , at least at this point , $\widetilde{y}$ is smooth . a similar calculation shows the $\mathbb{p}^1$ lying above the point at infinity , say in the chart $u\neq 0$ , is also fixed . this means every fixed point on $x$ is only resolved after the two blowups , where we have actually resolved 18 fixed points all in all .
a generalized free field is one for which ( modulo field redefinitions ) the connected $n$-point functions $g_n ( x_1 , . . . , x_n ) $ vanish whenever $n &gt ; 2$ . this means , basically , that a generalized free field is one for which the euclidean functional measure is gaussian . the field is completely specified by its 2-point functions $g_2 ( x , y ) $ . generalized free fields are usually discussed using the parametrization given by the kallen-lehmann decomposition of the 2-point function $g_2 ( x , y ) $ , which says that ( for scalar fields ) $g_2 ( x , y ) = \int_0^\infty d\rho ( m ) \delta_m ( x-y ) $ , where $\delta_m ( x-y ) $ is the real-space propagator for a free real field of mass $m$ and $\rho$ is a positive measure . this parametrization makes it easy to write down examples of generalized free scalar fields . just pick a positive measure $\rho$ on the mass line $ [ 0 , \infty ) $ . the simplest example is the free field of mass $m$ , which corresponds to $\rho ( m ) = \delta_m ( m ) $ , the dirac delta function supported at $m$ . you get other examples by picking other measures . for example , you can take a purely continuous measure , like $d\rho ( m ) = \theta_m ( m ) dm$ or $d\rho ( m ) = m^2dm$ . ( here , $\theta_m ( m ) = 0$ if $m &lt ; m$ and $1$ otherwise . ) these examples where the measure has continuos support are somewhat difficult to think of in the usual lagrangian formalism ; it is as if you have a continuum of fields of different mass which are all constrained to move together . but giving the correlation functions is enough to define a field theory ; you can use wightman 's reconstruction theorem to recover the hilbert space and the field operators . if you are working in axiomatic field theory , then you usually impose some sort of growth conditions on your correlation functions . in the case of generalized free fields , these growth conditions translate into conditions on $g_2$ , or equivalently on $rho$ . for example , if the free field 's euclidean measure obeys the osterwalder-schrader axioms , then $\rho$ must be a tempered measure of polynomial growth at $\infty$ ( and not too unreasonable behavior near $0$ ) .
typically one is introduced to the spectral theorem for hermitian operators . recall : if $a$ is hermitian then $$a = \sum_k a_k | k\rangle\langle k| , $$ where each $a_k$ is real and $\{| k \rangle\}$ is an orthonormal basis . if we have a function $f:\mathbb r\to \mathbb r$ ( i.e. . a real valued function ) , then we define ( overloading the definition ) $f:$hermitian operators $\to$ hermitian operators as $$f ( a ) :=\sum_k f ( a_k ) | k\rangle\langle k| . $$ but we need not restrict ourselves to real valued functions . we could have a complex valued function $f:\mathbb r\to\mathbb c$ . now , however , the function defined on hermitian operators will have a more general range , i.e. $f:$hermitian operators $\to$ linear operators . consider the specific function $f ( a ) =e^{i a}$ applied to $a$ . by definition $$f ( a ) = e^{ia} = \sum_k e^{i a_k} | k\rangle\langle k| , $$ which you can prove to yourself is unitary . it turns out that every unitary $u$ can be obtained by applying this function to a ( non-unique ) hermitian operator ( the canonical one being $-i\log u ) $ . ( stone 's theorem generalizes this a bit to parameterized groups of unitaries with the upshot that the hermitian operator is uniquely determined . )
the blades are at an angle . as the blade moves down it hits an air molecule and the air molecule " bounces off " toward you . it is just like hitting a ball in tennis/baseball
in general emulsifiers do not work by lowering the surface tension . they do lower the surface tension , but it is still thermodynamically favourable for droplets to coalesce . given the above i am sure you have already guessed that the emulsifier creates a kinetic barrier . if you take an oil in water emulsion stabilised with a typical anionic surfactant , to make two drops coalesce you have to displace surfactant from the oil/water interface into either the water or the oil . there is an energy barrier associated with this process , and this prevents coalescence . you can still break emulsions given enogh shear . for example oil in water emulsions are used in some hydraulic systems . the extreme shear involved , in e.g. pumps , overcomes the kinetic barrier and breaks the emulsion , and this is used to provide oil for lubrication where the shear is highest .
because of the proliferation of notation across multiple physical specialties , i do not know if there is a concise reference . i do not know what level you are at , so i do not know where you feel comfortable . i would start : a ) with a table of mathematical symbols then move to : b ) who is fourier ? as an introduction and desk reference on fourier analysis ( and there are phds i know who keep this on their desk as well , especially once they discover who the lead advisor for the english edition was ) . next are : c ) quantum mechanics demystified by mcmahon d ) relativity demysitified by mcmahon do not be fooled by the cover art , granted these are not textbooks , but they will give you the basic training needed to step into more rigorous books . i would then take time to understand what the action is and what it means to vary it . one quick reference is : e ) quantum field theory demystifed by mcmahon followed by : f ) quantum field theory in a nutshell by zee i would recommend starting with just the first few chapters of zee before hitting the best kept dirty little secret paper on arxiv that i have found to date : g ) a simple introduction to particle physics by robinson , bland , cleaver and dittman garanteed to turn you into a bed wetter . from here you will have some interesting choices , but for recreation i would spend 4 weeks and watch and take copious notes ( quite seriously ) on all the lectures : h ) the fourier transform and its applications by brad osgood at stanford you could probabaly actually just start with this , but it is important to make sure you take it seriously before doing so . after all that , then i would start in on supersymmetry , which has its own host of deep dark notation , as discussed in : i ) supersymmetry demystified by labelle and then you need to tie up everything you know and start in on string theory : j ) string theory demystified by mcmahon from here you should be able to read things directly in the arxiv , or dig into more rigorous textbooks .
i meanwhile found the constant by intensive googling ( for example in this article ) . it is ( depending on the source ) around 144 k .
the random potential is a model for small fluctuations in the local energy of an electron , when there are defects or impurities that raise the energy of an electron at certain spots by a little bit , and lower it at other spots . this is a good model , although it does not look like one at first , because the quantum mechanical solution is not sensitive to the finest details of the random ptential v ( in low dimensions ) , rather the quantum mechanical electron only notices the slowly varying average value of v from region to region . this is a huge field , and i am only giving a very superficial overview . it is one of the most studied problems of the past half-century , and it deserves the attention it gets . the random potential looks superficially intimidating , because the usual definition is that the potential v is gaussian random at every point , and generated by the probability distribution $$ e^{-{1\over 2}\int v^2 d^dx} $$ in d-dimensions . this is an independent gaussian at every point x of d-dimensional space , and the statistical correlation function of v for different picks from this probability distributions is $$ \langle v ( x ) v ( y ) \rangle = \delta^d ( x-y ) $$ you should think of this as follows : the space is made a lattice of size $\epsilon$ , the integral is replaced by a sum , and the delta function by a discrete bump of size $1\over \epsilon^d$ . this is probably the discrete approximation you saw . then the statement is that in the limit that $\epsilon$ goes to zero , you get a sensible theory . the theory is the eigenvalue of the schrodinger operator $$ ( \delta + \lambda v ) \psi = e\psi $$ where $\lambda$ is the disorder strength--- it tells you how strong the randomness is . the point is that the potential energy in each lattice site is a gaussian of width $\lambda\over \epsilon^{d\over 2}$ . the gaussian has a huge variance , and the potential swings from a huge positive value to a huge negative value every few neighboring cells . it does not look like a sensible notion of potential superficially . potentials that are this crazy make no sense classically , since you have turning points everywhere , and classically , you just oscillate around the deepest minimum , and you can not , because there are bumps everywhere blocking your path . for this problem , you have to stop thinking classically completely . in quantum mechanics the particle is tunneling around the minima , going through to other minima , and getting suppressed some whenever it has to go through a high potential region . the main question here is whether the particle can wind its way to infinity , so that the wavefunction is extended , or if the wavefunction of every state decays exponentially at long distances . this is the question of whether the random material localizes the electrons , so that it is an insulator , or delocalizes the electrons , so that it is a conductor . the classic reference for this is the nobel prize winning paper " absence of diffusion in random lattices " by p.w. anderson . despite the title , it is not talking about statistical diffusion , it is talking about this quantum mechanical thing . self-consistency in dimensions &lt ; 4 the first thing to do is to make sure that the problem has a self-consistent small-spacing limit . this is probably what you are interested in doing rigorously , but the heuristics are instructive . consider placing a small wavefunction bump of width a on top of the random potential . the thing to check is that you ( almost surely ) can not gain an infinite amount of energy by placing the bump in the right place , at a point where the average of v over the bump is going to be very negative . dividing the region of size $a^d$ in to $\epsilon^d$ cubes , you find that there are $n= ( a/\epsilon ) ^d$ of them , each random , and so by the square-root law , the typical negative value of v on this bump will have the size $1\over \sqrt{n}$ , while the fluctuations in v on the scale $\epsilon$ blow up as $1\over \epsilon^{d\over 2}$ . the $\epsilon$ parts cancel out , and you find the a scaling of the typical potential energy in a region of size a : $$ v_\mathrm{typ} = - {\lambda\over\epsilon^{d\over 2}} ( {\epsilon\over a} ) ^{d\over 2} = {\lambda\over a^{d\over 2}}$$ as a gets bigger , you average over more independent random v 's , and you get a smaller average , and it shrinks as this power law . as a gets smaller , you can make a more and more negative potential energy without bound , because the fluctuations get bigger ( and you look for the most negative region ) . but there is a cost to making a smaller , the small width means there is a momentum uncertainty of order $1/a$ , and this makes a kinetic enegy which goes as $1/a^2$ . so so long as the kinetic energy beats the potential energy in how it blows up , there will be a good limit . otherwise , the particle 's states will collapse to a point . you probably are asked to prove this rigorously . the condition is that d&lt ; 4 . so the problem makes sense in 1,2,3 dimensions , and maybe in 4 . it also makes sense on fractal shapes of the appropriate fractal dimension strictly less than 4 . the case of 4 dimensions is marginal , and i am not sure if the problem is well posed there , if it depends on the lattice details , or if there are different 4 dimensional fractals where it works and others where it does not . this is the boundary case . to make this rigorous might be somewhat difficult , because you need to prove that with probability 1 , one cannot gain energy by finding very special locations in a configuration of v where the potential energy is much much smaller than the typical energy flucuation size . there might not be good methods in mathematics for doing this rigorously right now , but it is completely obvious on physical grounds ( the number of positions is only growing polynomially with the mesh shrinking , while the number of configurations on the shrinking mesh grows exponentially . the much greater number of possibilities for the microscopic v values makes it obvious that searching through polynomially many positions is not going to get you more than a small factor over the naive estimate above . ) 1 dimension is exactly solvable the one dimensional problem can be solved exactly , and this was done by bert halperin in http://prola.aps.org/abstract/pr/v139/i1a/pa104_1 . one way to treat this is to use the so-called " r-matrix " , which is a 1-dimensional reflection/transmission matrix that tells you how plane-waves scatter/reflect off a bump . the product of the r matrices in a series is the r-matrix for the random potential problem , and in this case , you can analyze the product numerically , and see that you get attenuation for all v 's . this means that all wavefunctions in 1d are localized , they all fall off exponentially at long distances . one surprising ( but true ) prediction is that all wires are eventually insulating . since all materials have a little bit of random potential , all one-dimensional wires will localize eventually . the reason you do not see this is because long metal wires both have a relatively small disorder , and a large thickness , so that the localization length is much larger than you can measure . 2 dimensions is critical the two dimensional problem is fascinating , because it is the critical point for a renormalization group analysis of the problem of localization . at two dimensions , the random potential problem is marginally localizing . this is an involved analysis , and i am not prepared to write about it right now . one thing that this suggested is that 2d is like 1d , in that every $\lambda$ localizes . there were simulations , however , that suggested that this is not so , that there is a localization transition in 2d . this debate went on for 20 years in condensed matter physics , and i have heard that it is considered resolved now , although which way , and with what methods ( numerical or rg ) i would not be able to say . 3 dimensions : anderson transition and weak localization the three dimensional case is very interesting , since it has a second order phase transition between localizing and delocalized states as you tune the parameter $\lambda$ . at $\lambda=0$ , you have complete de-localization , all the states are momentum states , they are spread out and do not decay at infinity . when $\lambda$ is weak , you can treat the random potential as a small perturbation , and calculate the corrections to material properties from a few orders of perturbation theory . this is not so great from a mathematical perspective , because the perturbation is not localized at one spot , so the eigenfunctions are completely different to a mathematician , but for a physicist , the current transmission in the conductor is only altered by a sequence of scatterings from the randomness . these scatterings have the property that if you scatter k-times , and you scatter exactly backwards k-times , you have the same phase for both processes . this leads the particle to want to stay put more than you expect , because of the constructive interference between the paths and their time-reverse . if you break the time-reversal symmetry , by introducing a magnetic field , you then allow the electrons to flow better , because you remove the constructive interference . this effect is called weak localization , and it leads to materials having a resistivity peak at 0 applied magnetic field . this magneto-resistance was a very active subject in the 1980s and 1990s , as was weak-localization in general . one of the nice aspects of this is that it lets you figure out how long electrons maintain phase-coherence in a material , since the effect requires constructive interference of paths that are extended a significant way inside the metal . but the standard story is just the localization . at extremely high $\lambda$ , we know from the localization scaling that the wavefunctions will scruch up small , but we know from the dimensional scaling that they can not go down to $\epsilon$ , but must occupy some intermediate scale . anderson suggested studying this starting from completely localized states . the approaches disorder problems are interesting because they require you to average over the disorder , but not like a dynamical variable that is thermally fluctuating , but like a static thing . physicists call this " quenched " disorder , since it is analogous to quickly quenching a hot material like steel in water and freezing the impurities and disorder in place , without allowing them to come to thermal equilibrium and iron themselves out . for quenched disorder , you want to compute correlation functions , and then average over the disorder , $$ \langle \phi \phi \rangle_v = \sum_v p ( v ) \langle \phi\phi\rangle = \sum_v p ( v ) {\partial\over \partial j}{\partial\over \partial j} \log [ z ( j ) ] |_{j=0} $$ the quantity $z$ is also a sum over configurations , it is the quantum path-integral ( or the quantum statistical path integral ) . the difference is that the average over $v$ has to be done after you take the log of $z$ , since you do not want $v$ to become a dynamical quantum field , or a classical statistically fluctuating field , but you just want to average the results of the calculation over all values of $v$ . traditionally , there are two ways to do the average over $v$ . the first is parisi 's replica trick : the idea here is to perform the sum over $v$ with $n$ copies of the system : $$ \sum_{v\phi_1 . . . \phi_n} e^{-s ( \phi_1 ) - s ( \phi_2 ) . . . - s ( \phi_n ) } $$ this gives $$ \langle z^n\rangle $$ then you take the formal limit $n\rightarrow 0$ , in which the leading scaling is $$\langle \log ( z ) \rangle $$ . this idea is probably the hardest thing to imagine making rigorous , but this replica idea has been immensely fruitful for physics . it bears a resemblance to the notion of renyi entropy in mathematics , but it is more formal , since the actual partition functions are only fully defined for integer $n$ greater than 1 . this is a vast field , and you can google " replica trick " and " replica symmetry breaking " to learn more . this method is indispensible to modern condensed matter physics . the second method is the supersymmetry approach , and it relies on the following fact : in a susy system , the partition function is exactly 1 ! this might look surprising , but it is obvious in a stochastic system with a nicolai map ( see this answer : a certain $\cal{n}=2$ superconformal theory ( or is it ? ) ) . when you have a stochastic system , the partition function is constant--- it only depends on selecting the noise variable , not on the values of the field . if you use this fact , together with the fact that the derivatives of $\log ( z ) $ with respect to sources is the reciprocal of z times the derivative of $z$ with respect to the sources , you get that for a susy system , you can average $\log ( z ) $ just by averaging $z$ . this is the other way of dealing with disorder . the susy method and the replica method are complementary , and have given insight into different problems . the replica method has been more general although less rigorous , and more fraught with worries about whether it works .
it is not an uncommon interpretation that the alcubierre drive acts as a multiplier of an existing subluminal velocity . the trouble is that the alcubierre metric describes the drive in constant motion that does not change with time . it tells us nothing about how the drive accelerated to that speed or decelerated from it . the drive is moving in whatever direction the metric says because that is how the metric was constructed . i have seen very few papers on the alcubierre metric that even mention the question of acceleration , and none that treat it in detail . i would guess the ratio of hardness of the problem to interest in the outcome is too high for most researchers . the paper the alcubierre warp drive : on the matter of matter examines it briefly although the authors ' interest is really on the effect of matter caught up in the drive . so i am afraid there is no answer to your question .
i think it is bizarre that a particle does not have a definite composition . yeah , it is . as qftme said , that is quantum mechanics for you . it really does not make sense until you immerse yourself in the subject for long enough ( and even then , only somewhat ) . but it does appear to be the way the universe works . anyway , just so everyone is on the same page , let me start from the basics . if you are familiar with linear algebra , you know that a vector in a 2-dimensional vector space , for example , can be written as a linear combination $\alpha|0\rangle + \beta|1\rangle$ of two basis elements $|0\rangle$ and $|1\rangle$ . for example , a direction vector of length 1 that points northeast can be written as $$\frac{|\text{north}\rangle + |\text{east}\rangle}{\sqrt{2}}$$ or it could be written as $$|\text{northeast}\rangle$$ or $$\alpha|\text{north-northeast}\rangle + \beta|\text{east-southeast}\rangle$$ etc . you could figure out what the coefficients $\alpha$ and $\beta$ are in that last case , but it does not matter . the point is , there are an infinite number of ways to decompose any vector . the pion state is an example of such a vector . it is often considered to be a member of a three-dimensional vector space . one possible basis for that vector space is $u\bar{u}$ , $d\bar{d}$ , and $s\bar{s}$ . but another possible basis is $$\pi^0 = \frac{u\bar{u} - d\bar{d}}{\sqrt{2}}$$ $$\eta = \frac{u\bar{u} + d\bar{d} - 2s\bar{s}}{\sqrt{6}}$$ $$\eta&#39 ; = \frac{u\bar{u} + d\bar{d} + s\bar{s}}{\sqrt{3}}$$ this basis is useful because these particular combinations happen to be relatively stable ; in other words , when a particle consisting of any combination of $u\bar{u}$ , $d\bar{d}$ , and $s\bar{s}$ is detected in a cloud chamber ( if you are old-school ) or a calorimeter or something like that , it will behave like one of these three particles . it is possible that what was actually emitted was the quantum state $u\bar{u}$ , but in terms of the " stable " states , that is $$u\bar{u} = \frac{1}{\sqrt{2}}\pi^0 + \frac{1}{\sqrt{6}}\eta + \frac{1}{\sqrt{3}}\eta&#39 ; $$ ( hopefully i did the math right ) . so you would have a probability of $\frac{1}{2}$ that it acts like ( or technically , collapses to ) a pion , $\frac{1}{6}$ that it collapses to an eta meson , and $\frac{1}{3}$ that it collapses to an eta prime meson . one of those three possibilities is what you had actually observe in your detector . you can do this the other way around , too : suppose that instead of $u\bar{u}$ , you started with a pion , and instead of measuring the " stable " meson type , you were able to directly measure the quark content . since the pion state contains equal components of $u\bar{u}$ and $d\bar{d}$ , your hypothetical quark flavor measurement would give you one of those outcomes with 50% probability each : half the time you had find that you had an up quark and an anti-up quark , and the other half of the time you had find a down and anti-down quark . that is what the state $\frac{u\bar{u} - d\bar{d}}{\sqrt{2}}$ actually means : it governs the probabilities that the pion will interact with a quark flavor measurement as each particular quark type .
i think this is related to elastic fatigue . here 's the wikipedia page .
first of all , to have standing waves you must be talking about a wave carrying system with spatial extent : something like a guitar string . such a system has a set of possible vibrational modes . the first two modes are shown in the figure . we can describe each mode shape with a function $\phi_n ( x ) $ where $n$ is a label that indexes the modes , ie . $n$ is an integer going from 0 to $\infty$ . note that for each mode , $n$ is the number of nodes other than the endpoints . figure : first two normal modes of a string . the black line indicates the rest position . the blue solid line is the zeroth ( "fundamental" ) mode . the broken red line is the first mode . suppose the string is in some arbitrary state , where the deflection from rest at each point $x$ is given by a function $y ( x ) $ . it turns out that you can write any $y ( x ) $ like this : $$y ( x ) = \sum_{n=0}^{\infty}c_n \phi_n ( x ) . $$ that sum is called a fourier series , and the point is that you can write the state of a string as a linear sum of the normal modes . what is special about the modes is that if you start the string in exactly one of those modes , it will keep that shape forever ; only the deflection amplitude will oscillate . for example , if we start the string in $\phi_0$ the string 's deflection for all time is $$y ( x , t ) = \phi_0 ( x ) \cos ( \omega_0 t ) $$ where $\omega_0$ is a frequency associated to the $0^{\text{th}}$ normal mode . the fact that this shape is maintained forever means that the mode is a standing wave , by definition . if you start the string in the generic shape given in our first equation above , the string 's shape for the rest of time will be $$y ( x , t ) = \sum_{n=0}^{\infty}c_n\phi_n ( x ) \cos ( \omega_n t ) . $$ to recap : you can always write the shape of a string as s sum of normal modes ( aka standing waves ) each with its own shape and vibrational frequency . now on to your question . . . if you pluck the string in some way such that the initial shape is not one of the normal modes , or in your words , not one of the usual harmonic standing waves , you can still write it as a sum of other modes ( sum of other standing waves ) . the future evolution of the string 's shape is then given by our third equation . so , the answer to your question is basically that if you excite the string in a non-standing wave way , it will vibrate as a sum of standing waves , and that sum is just whatever set of standing waves superimpose to match how you are exciting the string . note : everything said here pertains to linear systems . most things in nature are at least approximately linear as long as the amplitude of the waves is not too big . there are exceptions to that , however . if that does not make sense , please post a comment : )
the traveller can travel 10 billion light years arbitrarily fast in his/her own experienced time . however , to the observer who stays at home on earth , the traveller 's speed will simply get asymptotically closer to the speed of light . this does , if you think about it , line up very fine with the twin " paradox " ( which is not really a paradox at all ) . as the traveller keeps accelerating the engine , when $v \rightarrow c$ , the time his travel takes will not be shortened to the twin who stays at home . a journey of , say , 40 light years will mean that the twin who stays home will always have aged at least 80 years when the traveller returns home . the traveller , however , can experience an arbitrarily short travel time , tending asymptotically to zero as $v$ tends to $c$ .
the geometry in your picture is too classical . once you pass the event horizon , it does not look like a sphere surrounding you anymore , and you do not see it as a special surface anyway . if you look back along a radial direction , you will see the same horizon point ahead of you ( in the past ) and behind you ( also in the past ) , at different affine parameter along the horizon ( this is clear in a penrose diagram ) . but you will not see the horizon as a sphere . when you approach a schwarzschild singularity , there is no way to avoid getting compressed to oblivion , because all the volume you carry is compressed to a tiny volume near r=0 . the radial area is r , and the area of a sphere is $4\pi r^2$ always , and r is time inside the horizon , and you are necessarily drawn to r=0 , which is the singularity . you can not save yourself by conformal mapping , because the actual physical distances are shrunk--- even if you were to conformally get shrunk to zero size , your matter is not conformally invariant , the atoms set a scale . the dr component of the metric does not vanish at the singularity , it is limiting value is ${1\over 2m}$ . this means that you are losing a certain unit of r per unit time as you fall in , which means your radial volume is shrinking to zero quadratically with time . the time part of the metric ( which is spatial now ) goes to ${2m\over r}$ , and so you gain a linearly diverging space in exchange , but the quadratic compression does not make up in volume for the quadratic sphere shrinking . further , this is not a conformal transformation in any reasonable sense , it is spaghettification . the real caveat about black holes is that this whole story assumes the black hole is neutral and nonspinning . for spinning or charged black holes , the interior structure is altered in radical ways , and there is nothing classically wrong with going in and coming out , except for some dubious arguments about what happens when you hit the cauchy horizon in the interior .
neither of those statements are true . it is an easy approximation to make : a neutron star has all of that ' space ' removed from between nucleons --- so we just need to know how big a neutron star of mass equal to the solar system would be . well , the only significant mass is the sun ( jupiter is about 1% the mass of the sun---negligible ) . if the sun were compressed into a neutron star , it would have a radius of about 10km ( up to 50% or so accuracy ) . see this nice talk about neutron star radii . solar system : so if you removed all of the ' space ' between all of the atoms in the solar systems , it would form an object about the size of a large town , or small city . universe : obviously collecting all of this mass would yield a black-hole . but conceptually , using some very order of magnitude estimates for the universe as a whole , if we assume there are roughly $10^{20}$ - $10^{22}$ stars ( i think this estimate is quite high ) , then the radius would be something like a 1-100 mpc or roughly 10 million to 1 billion light-years . edit ( to address the question itself ) : the concept of ' size ' for atoms and nuclei has some grey area , but you can define the size of a hydrogen atom , or the size of a proton/neutron to an order of magnitude . a statement like ' remove all of the empty space ' is much more nebulous , and ends up being largely a question of semantics . a more accurate way of phrasing the underlying concept being addressed might be something like : ' roughly how much volume do the dominant mass-constituents of matter take up ? ' the idea is that nucleons ( protons and/or neutrons ) are 2000 times more massive than electrons , and thus the important component of mass . at the same time , the electrons are the dominant volume-fillers ( by a factor of about $10^{15}$ ) .
it may not be the answer you are looking for but i recommend you get a thermos or a well insulated flask . these are what mountaineers use and you do not have to change the chemical composition of water this way .
pipes are damaged when ice forms a complete blockage , and the expansion of water trapped by it puts too much pressure on them . now , ice is a pretty good thermal insulator , so once a little ice forms on the inside of the pipe further freezing proceeds slowly . if the water is flowing there will not be enough time for it to freeze between leaving the ( relatively warm and protected ) underground run and exiting the pipe at the dripping faucet . thus the pipe never freezes all the way through , no blockage forms and excess pressure is never applied .
for a geometrical argument , you are looking for basically what ron posted . but you can also argue this one mathematically : as you may know , the difference between two spacetime events is represented by a time difference $\delta t$ and a spatial difference $\delta x$ . under a lorentz boost , these quantities transform like this : $$\begin{align}c\delta t&#39 ; and = \gamma ( c\delta t - \beta\delta x ) \\ \delta x&#39 ; and = \gamma ( \delta x - \beta c\delta t ) \end{align}$$ now , the spacetime interval is $\delta s^2 = c^2\delta t^2 - \delta x^2$ . for a timelike interval , $\delta s^2 &gt ; 0$ , this means $c\delta t &gt ; \delta x$ , assuming that both differences are positive ( and you can always arrange for that to be the case ) . using the lorentz boost equations , you can see that in this case , $c\delta t&#39 ; $ has to be positive . so for two events separated by a timelike interval , if one observer ( in the unprimed reference frame ) sees event 2 later than event 1 , any other observer ( in the primed reference frame ) will also see event 2 later than event 1 . on the other hand , suppose you have a spacelike interval , $\delta s^2 &lt ; 0$ . in this case , $\delta x &gt ; c\delta t$ , so it is possible to get $c\delta t&#39 ; &lt ; 0$ for a specific velocity ( namely $\beta > \frac{c\delta t}{\delta x}$ ) . so if one observer ( in the unprimed reference frame ) sees event 2 later than event 1 , it is still possible for another observer ( in the primed reference frame ) to see them in the reverse order .
total momentum is always conserved , in both elastic and inelastic collisions , but total kinetic energy is only conserved in elastic collisions . this example seems to be a completely inelastic collision , because at the end the objects merge . there is a formula to calculate the final velocity $v$ of two object with speed $u_1$ and $u_2$ and mass $m_1$ and $m_2$ in a completely inelastic collision , which is : $$v=\frac{m_1u_1+m_2u_2}{m_1+m_2}$$ here 's a simple derivation : since momentum is always conserved , the sum of momenta at the beginning is the same as the end : $$p_{i1}+p_{i2}=p_{f1}+p_{f2}$$ however , since this is a completely inelastic collision , at the end the two objects will merge , and so there will be only one final momentum . the final momentum is simply the sum of initial momenta , like final mass is the sum of initial masses : $$p_{1}+p_{2}=p_f\qquad m_1+m_2=m_f$$ then : $$v=\frac{p_f}{m_f}\qquad v=\frac{p_1+p_2}{m_1+m_2}\qquad v=\frac{m_1u_1+m_2u_2}{m_1+m_2}$$ total kinetic energy however is not conserved , as you can see summing initial kinetic energies and comparing with the final kinetic energy .
i will address the part of the question which discusses gravitational singularities . there are many types of singularities on manifolds , and more generally topological spaces . in general relativity , you are likely to encounter : coordinate singularity : these arise because we have used inappropriate coordinates ; for example the schwarzschild black hole metric may be singular at $r=2gm$ . however , these are not true physical singularities , and may be removed with diffeomorphisms . curvature singularity : a true physical singularity which arises when a curvature scalar ( as it is invariant under diffeomorphisms ) is singular . for example , at the center of a black hole . conical singularity : a singularity which occurs , for example , when we encounter a point at the tip of a cone , which is taken to be infinitesimally small . to understand why it is problematic , consider a geodesic at that point ; when you arrive at the tip which way do you continue ? at curvature singularities , or regions of high curvature , we cannot resort only to general relativity ; at this point quantum gravity becomes important . as professor tong states , the question of singularities is morally equivalent to high energy scattering . the short-distance ( planck scale ) phenomena of spacetime , after fourier transform , is governed by high energy gravitons . other singularities : another example of a singularity arises when studying orbifolds . given a smooth manifold $x$ and a discrete isometry group $g$ , an orbifold is the quotient space , $x/g$ and a point in the orbifold corresponds to an orbit in $x$ consisting of a point and all its images under the $g$ group action . if certain elements of $g$ leave a point in $x$ invariant , then the orbifold has an orbifold singularity , which is a type not really encountered in general relativity , but rather string theory . it may be removed with the use of algebraic geometry , for example by replacing certain regions with eguchi-hanson spaces for the case of $t^4/\mathbb{z}_4$ .
i think this is a typo in morse and feshbach methods of theoretical physics . the correct expression is $-\frac{1}{6}r^2\nabla ^2\psi$ or $-\frac{1}{6} ( dx^2+dy^2+dz^2 ) \nabla^2 \psi$ .
even a normal planet does not permanently lock its atmosphere : a little bit of it is creeping out all the time . the air molecules are distributed according to a maxwell-boltzmann distribution , which falls off to zero exponentially . a small fraction of that air will always be above escape velocity and will disappear into space . the distribution of air re-thermalizes , and thus another fraction is lost to space . the fraction that is above escape velocity depends on the mass of the molecule : it is appreciable for helium on earth ( popped balloons are gone forever ) . for your deep well , you had have to consider the shape of the maxwell-boltzmann distribution and the variation of pressure with altitude ( and include a non-earth " g" ) . frame the problem in terms of the amount of loss that you are comfortable with--- something so small that it will not be missed or can easily be replenished . someone who is actually engineering this might also want to chill the upper layer of gas with some kind of large-scale air conditioning . that would reduce the loss so that the hole would not need to be as deep . maybe a greenhouse effect could be useful to keep the upper layer cold and the lower layer warm . after all , who needs to see the sun ?
it seems you have done the hard part already , which is to evolve the object 's position as a function of time . and moreover , the simulation seems stable over a number of orbits . ( but eventually things start to go wrong ; you may want to look at an answer i wrote to what is the correct way of integrating in astronomy simulations ? ) so my understanding is all you really need is the full orbit plotted . in that case , there is no need to make it a function of actual time ( that is hard ) . instead , we can fall back to kepler 's first law , which says that the separation $r$ between the bodies obeys $$ r = \frac{r_\mathrm{max} ( 1-e ) }{1+e\cos ( \theta ) } . $$ $r_\mathrm{max}$ is the maximum separation , which i believe is the initial separation in your particular simulation . the eccentricity $e$ ( formula here ) is given by $$ e^2 = 1 + \frac{2er^4\dot{\theta}^2}{g^2m^2m} , $$ where $e$ is the orbital energy , $g$ is the gravitational constant , $m$ is the mass that is so heavy it essentially does not move , and $m$ is the lighter mass . this can more conveniently be rewritten $$ e^2 = 1 + 2 \left ( \frac{rv}{gm}\right ) ^2 \left ( \frac{v^2}{2} - \frac{gm}{r}\right ) , $$ where $v$ is the velocity of the moving particle . note that the convention is for $\theta$ to measure the angle from closest approach ( i.e. . , $\theta = 0$ is the negative $y$-axis in your simulation ) . if you want $\theta$ to increase with time , then the ( nonstandard ) transformation to cartesian coordinates is \begin{align} x and = -r \sin ( \theta ) \\ y and = -r \cos ( \theta ) , \end{align} assuming $r = 0$ corresponds to the massive object . in any event , to plot the orbit all you need to do is calculate the constant $e$ at one point in the orbit , sample $\theta$ with as many points as you like , calculate the separations $r ( \theta ) $ , and convert the $ ( r , \theta ) $ pairs to $ ( x , y ) $ .
the attraction does happen at all temperature , but it is negligible if the temperature is too high . so the electrons do attract each other , but the thermal fluctuations do not allow for cooper pairs to be stable . to give an heuristic example : imagine a lot of hydrogen atoms . the electrons are bound to their protons at zero temperature . if now you put the hydrogen atoms in a medium with a temperature high compare to 13ev ( the binding energy ) , the electrons can take some energy from environment to leave their protons , and you do not have hydrogen atoms anymore , but a plasma ( free protons and free electrons , interacting without forming bound states ) . the same thing happens with cooper pairs : the temperature needs to be small enough to allow the physics to be dominated by these very weakly bounded pairs .
i think that you are confused . when you rotate something by 360 degrees , you will not change the direction in space of anything . you will only change the wave function to minus itself - if there is an odd number of fermions in the object ( which is usually hard to count for large objects ) . if you have electrons with spins pointing up and you rotate them around the vertical axis by any angle , whether it is 360 degrees or anything else , you will still get electrons with spin pointing up . this is about common sense - many spins with spin up give you a totally normal , " classical " angular momentum that can be seen and measured in many ways . the flip of the sign of the wave function can not be observed by itself because it is a change of phase and all observable probabilities only depend on the density matrix $\rho=|\psi\rangle \langle\psi|$ in which the phase ( or minus sign ) cancels . the phase - or minus sign - has nothing to do with directions in space . it is just a number . in particular , it is incorrect to imagine that complex numbers are " vectors " , especially if it leads you to think that they are related to directions in spacetime . they are not . you would have to prepare an interference experiment of an object that has not rotated with the " same " object rotated by 360 degrees - and it is hard for macroscopic objects because the " same " object quickly decoheres and you must know whether it has rotated or not , so no superpositions can be produced . ; - ) however , all detailed measurements of the spin with respect to any axis indirectly prove that the fermions transform as the fundamental representation of $su ( 2 ) $ . in particular , if you create a spin-up electron and measure whether it is spin is up with respect to another axis tilted by angle $\alpha$ , the probability will be $\cos^2 ( \alpha/2 ) $ . the only sensible way to obtain it from the amplitude is that the amplitude goes like $\cos ( \alpha/2 ) $ and indeed , this function equals $-1$ for $\alpha$ equal to 360 degrees .
assuming there is more flux coming into the open end of the tube than the thermal radiation in the closed part of the piston then yes . radiation pressure doing work is perfectly feasible eg https://en.wikipedia.org/wiki/solar_sail
a band , in itself , does not have charge . a band is just a collection of possible states where electons might be found . the electrons have charge but the states themselves do not . if the valence band is fully occupied , the charge of the electrons in the band will be balanced by the charge of the protons in the lattice , and the overall charge of the material will be neutral . as an aside , since the protons are well-localized in position space , they are very spread out in k-space . if there is an unoccupied state in the valence band , there will be a local ( in k-space ) net positive charge because the nuclear positive charges are not compensated by valence band electrons . this positve charge behaves as if it were attached to a particle , and we call that particle a hole . often the electron removed from the valence band is only moved up to the conduction band , so the net charge in the material may still be neutral , even though there is a hole present . the conduction band particle ( electron ) and valence band particle ( hole ) may move apart in k-space ( and in position space ) so we must keep track of them seperately .
a rigid body has 6 configuration degrees of freedom because its most general configuration can be obtained by translating ( 3 degrees of freedom ) and rotating ( 3 degrees of freedom ) its initial configuration . a mathy way of saying this is that its configuration manifold is $\mathbb r^3\times \mathrm{so} ( 3 ) $ . however , you are right that the phase space of a rigid body is 12-dimensional because each independent configuration degree of freedom corresponds to an independent momentum degree of freedom .
no . the law of the conservation of mass-energy is a purely local law ( i.e. . the divergence of the stress-energy tensor must be zero at any point in spacetime ) . you will find that , as long as your wormhole is a solution to einstein’s equations , this law will hold anywhere in the vicinity of the wormhole or within it . in other words , there is no point at which the mass suddenly disappears . it enters the mouth of the wormhole , pass through its throat , and emerges from its other mouth .
i was wondering if there is an intuitive way to understand the motion of a body influenced by two other massive bodies ( say the moon being influenced by the earth and the sun . no , intuition is not of real use in a three body gravitational problem , more so in many body . in 1887 , mathematicians ernst bruns [ 4 ] and henri poincaré showed that there is no general analytical solution for the three-body problem given by algebraic expressions and integrals . the motion of three bodies is generally non-repeating , except in special cases . [ 5 ] actually the motions of the planets are studied as possible dynamic chaos . planetaria solve the many body problem with numerical approximations .
i would add a humidity sensor , as water vapor is the strongest green house gas . this graph may suggest other gases > breakdown of the anthropic greenhouse gas emissions by gas . source : ipcc , 2007 here is an article on halocarbons .
( i try to answer to my own question , after some reflections made with the help of luboš . ) for an incompressible and irrotational flow , the conditions $\nabla\times \boldsymbol u=\boldsymbol 0$ and $\nabla\cdot \boldsymbol u = 0$ imply , $\nabla^2\boldsymbol u =\boldsymbol 0$ . indeed : $$\nabla^2\boldsymbol u = \nabla ( \nabla\cdot\boldsymbol u ) -\nabla\times ( \nabla\times \boldsymbol u ) = \boldsymbol 0$$ this forces us to write down the navier-stokes equation for the motion of the fluid without the viscous term $\mu\nabla^2\boldsymbol u$ , no matter the viscosity : $$ \rho ( \partial_t \boldsymbol u + u\cdot\nabla \boldsymbol u ) = -\nabla p \ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ now , it could seem that this implies the flow is automatically a high-reynolds number flow ( for which we could have wrote down the same equation , but for a different reason : $\mu=\rho\nu\simeq 0$ , and this would have been an approximation ) . but , even if the viscosity is far from neglectable , we can make another kind of approximation , saying that the inertia , represented by the left-hand-side terms in n-s equation , can be neglected because of $re=ul\rho/\mu\ll 1$ ( this can happen in a lot of situations : microobjects , extra-slow flows , and - of course - high viscosity . in this case , the equations of motion become : $$-\nabla p = \boldsymbol 0\ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ which are , in fact , the equations we would arrive at if we started by the stokes equation ( for inertia-less flows ) for irrotational flows . then , an irrotational flow is not necessarily governed by the euler equation , i.e. , it is not necessarily inviscid .
for a path integral , the integral $$ \int e^{i\int \lambda ( x ) f ( x ) } d\lambda = \prod_x ( 2\pi ) \delta ( f ( x ) ) $$ where the product is over all points x ( imagine a lattice ) . so that the result is not just a saddle point identity ( saddle points and perturbative approximations are not the same , perturbative is by expanding the exponential in a series ) , it is an exact path integration identity . while the result is not fully rigorous , that is only because of the issue of path integration definition , the limit of continuous space as the lattice gets finer . ignoring this mathematical point , there is no way this identity is avoided in any proper definition of path integration , since it holds without any regulator ambiguity .
first , note that in physics , we consider unitary representations $u$ of the poincare group acting on the hilbert space $\mathcal h$ of the theory because we are interested in a precise formulation of the concept of poincare transformations acting on the quantum mechanical states of the theory as symmetries ( since the laws of physics should be inertial frame-invariant ) ; and by wigner 's theorem , we choose these symmetries to be realized by unitary operators . these observations are related to your #1 and #3 and i think they should be kept conceptually distinct from the notion of a state that represents a single particle state . second , since such quantum field theories are supposed to allow for the emergence of states of particles , and in particular should account for states in which there is a single elementary particle , we expect that there is some subset $\mathcal h_1$ of the hilbert space of the theory corresponding to states " containing " a single elementary particle . given these observations , let 's rephrase your question as follows : what properties do we expect that the action of the representation $u$ will have when its domain is restricted to the subspace $\mathcal h_1$ ? in particular , we would like to justify the following statement the restriction of the unitary representation $u$ acting on $\mathcal h$ to the single-particle subspace $\mathcal h_1$ is an irreducible representation of the poincare group acting on $\mathcal h_1$ . this requires justifying two things : the restriction maps $\mathcal h_1$ into itself . the restriction is irreducible . i think that the justification of the first property is pretty intuitive . if all we are doing is applying a poincare transformation to the state of the system , namely we are just changing frames , then the number of particles in the state should not change . it would be pretty strange if you were to , for example , boost or rotate from one inertial frame into another and find that there are suddenly more particles in our system . the irredicibility requirement means that the only invariant subspace of the single particle subspace $\mathcal h_1$ is itself and $\{0\}$ . the physical intuition here is that since we are considering a subspace of the hilbert space in which there is a single elementary particle , expect that there is no non-trivial subspace of $\mathcal h_1$ in which vectors of this subspace are simply " rotated " into one another . if there were , then the particle would not be " elementary " in the sense that the non-trivial invariant subspace would represent the states of some " more elementary " particle . when it really comes down to it , however , i am not sure if there is some more fundamental justification for why the restriction of $u$ to $\mathcal h_1$ is irreducible aside from the decades of experience we have now had with particle physics and quantum field theory .
that is exactly right . a fundamental tenet of physics is that all inertial reference frames are equivalent and indistinguishable . 1 furthermore , given one inertial frame ( standing at rest 2 ) , any other frame moving with respect to it with a constant velocity is also inertial . the frame " moving at terminal velocity " is just as inertial as " sitting still " and so you would not even be able to tell you were moving . by definition you feel no acceleration at constant velocity . thus the acceleration due to gravity must be exactly balanced by some other force . by construction that force is not air resistance for you ( as would be the case of a sky diver at terminal velocity ) but simply the normal force of the elevator floor , which would make the experience feel exactly like standing in a non-moving elevator in the same gravitational field . 1 at least locally , meaning that any experimental apparatus and things you measure are confined to objects also in that frame . 2 to be pedantic , standing " still " in a gravitational field is considered inertial in newtonian mechanics but not general relativity . i am speaking in newtonian terms here , but the conclusion would be just the same if analyzed with the machinery of gr .
being bulk neutral neutrons participate only weakly in electromagnetic interactions which is the dominate interaction for charged particles . instead neutron scattering can be thought of as primarily a contact interaction with the nuclei of atoms in the way . light atoms ( and hydrogen in particular ) have a larger cross-sectional area per nucleon than heavy ones take up more of the energy of the interaction in recoil than heavy ones making them much more effective at reducing the kinetic energy of non-thermal neutrons per unit areal mass density . historically waxes , water and plastics have been the neutron shielding materials of choice , though concrete or rammed earth are cheap and not too bad . once down to thermal energies neutrons get as much kinetic energy as they lose on average and you just have to wait for them to decay or capture . doping your absorber with boron , chlorine or even gadolinium will help to capture the thermalized neutrons faster . pvc gets you the chlorine for free in your plastic , and boron can be added easily to concrete or to a number of plastics . it should not be overlooked that it takes a lot of space to slow , thermalize and capture neutrons ( that contact interaction thing means they go through more material before interacting than charged particles ) ; especially if you need to get them all . they are notorious for penetrating large quantities of shielding , and distance is one of your best friends when it comes to neutron shielding .
no , it does not propagate . a low frequency electric field is a near field - it may depend on time but its distance-dependence is such that this field decays as $1/r^2$ at best . the propagating part of a low-frequency field ( $\propto 1/r$ ) has a magnetic component too . edit : the time-dependent field $e ( t ) $ has two terms : one is a near field and another is a propagating field $$e ( t ) =a_1/r^2+a_2\omega/r . $$ the propagating field strength is proportional to the frequency $\omega$ and is accompanied with a propagating magnetic field of the same strength . they can be small . a near magnetic field is smaller than a near electric field due to low frequency . so it is possible to have a variable electric field without variable magnetic field , but it does not propagate in reality . light is a propagating emf .
two books to get you started in general computational radiation transport : computational methods of neutron transport , written by e.e. lewis , edited by w.f. , jr . miller , isbn 0-89448-452-4 monte carlo particle transport methods : neutron and photon calculations , written by ivan lux and laszlo koblinger , isbn 0-8493-6074-9 the supporting materials for the mcnp ( monte carlo n-particle ) transport code maintained by los alamos national laboratory may also be useful : http://mcnp.lanl.gov/ these references are not targeted at nuclear rockets , instead they focus on the basic principles of computational radiation transport . application to nuclear rockets , or any other particular system , is just a matter of constructing the right set of assumptions and constraints to answer your specific question .
my problem appears to be with the initial state of the system , which i have written as $$\left| \psi\right\rangle=\left|\psi_1\right\rangle\left|\psi_2\right\rangle+\left|\psi_2\right\rangle\left|\psi_1\right\rangle , $$ where $\left|\psi_1\right\rangle$ is packet from one source , and $\left|\psi_2\right\rangle$ is packet from another one . this state says that the system is in a superposition of states , in each of which one of the particles comes from one source , and another necessarily from another source . i.e. the system is highly entangled . such system could be created e.g. by some generator of pairs of particles with opposite momenta . but two independent sources are clearly not such a source of entangled pairs . as the particles are indistinguishable , and there is no symmetry which would allow us to determine that the particles come from different sources , we can not say which source the particle has come from . if we watch single particle emitting from the sources , they might come one after another from different sources , or they could repeatedly come from single source , then several times from another one . i.e. there is no rule that if one particle is from source a , next detected one is from source b . so , the initial state must be in the following form : $$\left|\psi\right\rangle=\left ( \left|\psi_1\right\rangle+e^{i\phi}\left|\psi_2\right\rangle\right ) \otimes\left ( e^{i\psi}\left|\psi_1\right\rangle+e^{i\chi}\left|\psi_2\right\rangle\right ) +\\ +\left ( e^{i\psi}\left|\psi_1\right\rangle+e^{i\chi}\left|\psi_2\right\rangle\right ) \otimes\left ( \left|\psi_1\right\rangle+e^{i\phi}\left|\psi_2\right\rangle\right ) , $$ where $\phi , \psi , \chi$ are constants , which depend on the experimental setup . now from the form of the initial state it is obvious that the interference pattern will be present , and it is confirmed by numerical simulation . thus , it appears that even in this multiparticle experiment particles interfere with themselves , rather than with each other , to produce visible pattern on the screen .
1 ) the first derivation is correct . when you make the second one for $\dot x$ written as $\dot x = \dfrac{d x}{d t}$ you make an error when using full derivatives instead of partial ones . in the latter case you would have obtained : $$ \dot x=\dfrac{d x}{d t}=\dfrac{\partial x}{\partial \theta}\dfrac{d \theta}{d t} +\dfrac{\partial x}{\partial \phi}\dfrac{d \phi}{d t}+\dfrac{\partial x}{\partial r}\dfrac{d r}{d t} $$ then , as $x=r \cos \theta$ , you can insert $\dfrac{\partial x}{\partial \phi}=0 , \dfrac{\partial x}{\partial \theta}=-r \sin \theta$ , $\dfrac{\partial x}{\partial r}=\cos \theta$ into the last formula to obtain the same result you had before . 2 ) the derivatives you are using here are not covariant derivatives , they are always partial . the derivative $\dfrac{d}{d t}$ is expressed through partial derivatives , in contrast to full derivative $\dfrac{d}{d t}$ , which is expressed through covariant derivatives . as you are actually not using covariant derivatives in your derivation , you do not have to worry about the basis vectors . 3 ) you could make you derivation alternatively by rewriting your original equations in a covariant form $m\dfrac{d^2 x^i}{d t^2}+c\dfrac{d x^i}{d t}+kx^i=0$ , where $x^i$ stands for $ ( x , y , z ) $ , stating that the equations of motion do not depend on the choice of coordinates , rewriting the equation in spherical coordinates as $m\dfrac{d^2 \tilde x^i}{d t^2}+c\dfrac{d \tilde x^i}{d t}+k \tilde x^i=0$ , where $\tilde x = ( r , \theta , \phi ) $ , and expanding all the derivatives $\dfrac{d}{d t}$ through partial derivatives and christoffel symbols in spherical coordinates . you would have to take care of the basis , as the result would be in natural basis , whereas the equations of motion are usually written in orthonormal basis . 4 ) another powerful way to transform coordinates would be by using lagrangian form of the equations of motion . this would be possible for the case of conservative systems only ( no damping terms ) . in this case you could just rewrite the lagrangian of your system in spherical coordinates and write lagrangian equations from it in a more or less straightforward manner .
if i had to give a one sentence answer to this question , it would be as follows : *that the phase and amplitude alone on one plane is enough to wholly define a three-dimensional light field arises from the various uniqueness theorems for maxwell 's equations within a connected volume given the solution on the volume 's boundary ; otherwise put : once you know a solution on a boundary , then the values within must follow from " reasonable " physical assumptions . for simplicity let 's sit with the scalar diffraction theory , so now we are essentially talking about uniqueness theorems for the helmholtz equation $ ( \nabla^2 + k^2 ) \psi = 0$ . uniqueness theorems when $k^2 &gt ; 0$ or when $k^2 \in \mathbb{c}-\mathbb{r}$ are much more complicated than when $k^2\leq0$ . the latter case corresponds to static solutions of the klein gordon equation or to static solutions of the maxwell equations with or without an assumption of a massive photon ; see my answer here for more details . such cases have very strong uniqueness theorems : once a solution 's values are set on a compact volume 's boundary , there is only one possible solution within the volume . this situation even extends to semi-infinite volumes . however the former situation includes $k^2&gt ; 0$ , the case for scalar diffraction in freespace or a lossless dielectric : uniqueness theorems need further strong assumptions about the field to make them work . thankfully , some of these assumptions are reasonable physically . we can restore simplicity to the solutions of the freespace helmholtz equation ( i.e. to the situation we have with a hologram ) by making reasonable physical assumptions such as the sommerfeld radiation condition or that the field is a tempered distribution ; for more information on the latter condition , see my answers here or here . given these assumptions , together with the assumption that the field is propagating purely left-to-right , we can reconstruct a field from the hologram as follows . you begin with the helmholtz equation in a homogeneous medium $ ( \nabla^2 + k^2 ) \psi = 0$ . if the field comprises only plane waves in the positive $z$ direction then we can represent the diffraction of any scalar field on any transverse ( of the form $z=c$ ) plane by : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}$$ to understand this , let 's put carefully into words the algorithmic steps encoded in these two equations : take the fourier transform of the scalar field over a transverse plane to express it as a superposition of scalar plane waves $\psi_{k_x , k_y} ( x , y , 0 ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) $ with superposition weights $\psi ( k_x , k_y ) $ ; note that plane waves propagating in the $+z$ direction fulfilling the helmholtz equation vary as $\psi_{k_x , k_y} ( x , y , z ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) $ ; propagate each such plane wave from the $z=0$ plane to the general $z$ plane using the plane wave solution noted in step 2 ; inverse fourier transform the propagated waves to reassemble the field at the general $z$ plane . if you can understand these steps you should be other see how the solution to helmholtz 's equation , i.e. the full three-dimensional scalar light field , is reconstructed from its values on a plane . the latter of course is what a phase and intensity mask hologram encodes . what hinders holography ? i am not up with the latest hologram production techniques , but essentially making a hologram is a kind of interferometry and as such calls for low vibration and building of an interferogram between transmitted and reference light . one can not simply " snap " a hologram like one can with a digital camera ( or even with an older style film camera ) . moreover , the phase masking needed to make the equations above work is highly colour-dependent , so that any kind of colour holography is even more restrictive than the making of one-colour holograms . the holography wiki page gives you a good overview ; the " rainbow " holographic technique is the nearest i know of to colour holography . aside from this technique , most holograms need high coherence in the light source for reconstruction . another interesting technique is the manipulation of light by computer generated holography , where one computes by solving maxwell 's equation the phase and amplitude mask needed for e.g. nulling out the mean aberration from a lens before analysis by an interferometer .
quantum field theory is the framework that we normally use to study particle physics . it is the idea that the world is described by fields and each field can move into excited states which correspond to particles . since we are working with fields , you not constricted to a fixed particle number as in quantum mechanics . this is very useful since due to $e=mc^2$ , particles constantly appear and disappear in the real world . particle physics on the other hand , is the study of the particles that make up our world . since qft 's are terrific at describing nature , we use them to describe particle physics ; they are tool we need to study particle physics . however , some people study qft for the sake of qft since its a very deep and subtle subject .
the following is a passage from diffusion in the standard map ( pdf ) by itzhack dana and shmuel fishman : a major difficulty in the analysis of chaotic behavior of hamiltonian systems is the proximity of chaotic and regular orbits on various scales [ 2 ] . thus , for $k \approx 1$ , the phase plane of ( 1 ) is an intricate mixture of regular and chaotic orbits [ 3 ] . for $k \gg 1$ chaotic orbits fill almost the entire phase plane , but " islets of stability " , in particular accelerator modes [ 2 , 7 ] , are known to exist for $k$ arbitrarily large . when a chaotic orbit approaches such an islet , it may wander near it in a " regular " fashion . the area of each islet generally decreases when $k$ increases . distinctive features of a stochastic or random motion , which allow a statistical description of it , are the rapid decay of correlations and diffusion . the decay of correlations in the chaotic region is related to the local instability , measure lyapunov characteristic exponent [ 1 , 2 , 8 ] . for $k \gg 1$ , the decay of correlations is actually exponential , with the decay exponent proportional to the lyapunov exponent [ 8 ] . the existence of a definite characteristic time for the decay of correlations and the resulting statistical independence generally imply diffusion in the unbounded direction of the action $i$ for $k &gt ; k_\mathrm{c}$ [ . . . ] . therefore a definite diffusion coefficient $d$ can be associated with the chaotic motion $$ d = \lim_{n\to\infty} \frac{\big\langle ( i_n-i_0 ) ^2\big\rangle}{n} , $$ where the average is taken over some ensemble of initial positions $ ( i_0 , \theta_0 ) $ within the chaotic region . for large $k$ , where the lyapunov exponent is large , diffusion was verified unambiguously [ 2 ] .
what we call " laws of physics " have an evolutionary path . it really started with newton and the falling apple and slowly it evolved into complete mathematical models of experimental observations , called theories . from the observations and the theories conservation laws emerged . these laws are strictly obeyed within the framework that they have been validated . take the thermodynamic law " entropy remains the same or increases in closed systems " . the region of validity of the law was transformed when the atomic nature of matter became understood , and statistical mechanics became the underlying framework . there , from an absolute law it became an estimate of probability outcomes , which to all intents and purposes recreates the law for macroscopic systems . if the universe could emerge from nothing , what about physical laws ? as our observations and experiments advance , new mathematical frameworks appear which transmute the laws of the overlying frameworks : conservation of energy became the relativistic four vector energy which blended mass in the mixture . conservation of relativistically defined energy and momentum also became fuzzy instead of absolute due to the heisenberg uncertainty principle of quantum mechanics . so when we come to cosmology where there exist theoretical models of solutions of general relativity it is not surprising that apparent inconsistencies with conservation laws developed for different frameworks . at the moment there does not exist a theory of everything which quantizes gravity and includes the other three forces , weak , strong , electromagnetic that has been validated through all relevant observations , even though string theory offers such possibilities . it is therefore premature to be definitive of how the known conservation laws validated by our laboratory experiments will evolve in a cosmological setting . something and nothing have to be mathematically defined within the appropriate theoretical models .
ah , this gives me a chance to give a proper home to an analysis i first posted on reddit . ( i would much rather have first posted it here :-p ) mathematical derivation it all starts with a blog post i have written that comes very close to addressing the exact question you are asking . in the post , i calculated how fast an object would be moving after falling a given distance , assuming quadratic drag . but one of the formulas i used to get to that result is the time it takes for an object to fall a certain distance . here 's the argument from my post . if you write out newton 's second law for an object falling through the air , you get $$\frac{1}{2}ca\rho \dot{y}^2 - mg = m\ddot{y}$$ i.e. drag force minus gravitational force equals mass times acceleration . in this equation , $m$ is the object 's mass , $a$ is the cross-sectional area it presents , $c$ is the object 's drag coefficient , $\rho$ is the density of the fluid it is falling through , $g$ is the acceleration of gravity , and $y$ is its height at any given moment . solving this equation for $\dot{y}$ gives $$\dot{y} = -\sqrt{\frac{2mg}{ca\rho}}\tanh\biggl ( \sqrt{\frac{gca\rho}{2m}}t\biggr ) $$ you can then integrate this with respect to time and solve it for $t$ to get $$t = \sqrt{\frac{2m}{gca\rho}}\cosh^{-1}\exp\biggl [ \frac{ca\rho}{2m} ( h - y ) \biggr ] $$ a couple more steps are shown in my blog post , but they are not really important . the point is that this formula gives the time $t$ it takes for an object to fall a distance $h - y$ . you will notice that the properties of the falling object occur in this formula only as part of the particular combination $\frac{ca\rho}{2m}$ . so the behavior of a falling object can be entirely characterized by that ratio . if you call this ratio $r$ , then the formula becomes $$t = \sqrt{\frac{1}{rg}}\cosh^{-1}\exp [ r ( h-y ) ] $$ for a few sample values of $h - y$ , here 's what this looks like as a function of $r$: you will notice that the time to fall a given distance always increases with increasing values of $r$ . so the larger an object 's value of $\frac{ca\rho}{2m}$ , the longer it takes to fall . conversely , an object with a smaller ratio of cross-sectional area to mass ( i.e. . smaller $r$ , assuming the same shape ) will fall faster . now , roughly speaking , a fat person tends to be larger than a skinny person in all three dimensions . so their mass will roughly be bigger by a factor of $k^3$ for some $k$ , whereas their cross-sectional area will only be bigger by $k^2$ . ( this is a huge approximation , of course , but it should still work for the question of " faster " vs . " slower . " ) accordingly , $r$ for a fat person will be smaller ( by a factor of $k$ ) , which means they take less time to fall . physical interpretation that is all well and good , but just going through the math does not necessarily make it clear why ( physically ) fat people fall faster . the crux of the explanation is in that last paragraph : a fat person has a larger mass in proportion to their surface area . since the drag force is proportional to area , but weight is proportional to mass , as a person gets fatter , the weight ( going down ) increases more than the drag force ( going up ) , which means the person accelerates more . more math : slowdown factor now what about this " slowdown factor " that wolfram alpha is coming up with ? if you look down toward the bottom of the results , it tells you that the slowdown factor is just the ratio of the time actually taken to fall , which i showed you how to calculate above , to the time that would be taken without air resistance . you can get the latter time by setting $c$ , $a$ , or $\rho$ to zero , or taking the limit as $m\to\infty$ . ( does it make sense why all of these assignments correspond to making the effect of the air insignificant ? ) or , of course , you could just as well take $r\to 0$ . now , before you start wondering how you are going to get away taking the square root of $\frac{1}{0}$ , you actually need to take a limit , and the limit of $t$ as $r\to 0$ is well defined : $$t_0 = \lim_{r\to 0}\sqrt{\frac{1}{rg}}\cosh^{-1}\exp [ r ( h-y ) ] = \sqrt{\frac{2 ( h - y ) }{g}}$$ the formula for the slowdown factor is then $$\text{slowdown factor} = \frac{t}{t_0} = \sqrt{\frac{1}{2r ( h - y ) }}\cosh^{-1}\exp [ r ( h-y ) ] $$ this depends only on the product of the " drag ratio " $r$ and the height fallen $h - y$ . essentially it is a way of characterizing how much the presence of air resistance affects the time of flight .
the question is at least a little bit indeterminate because of " naturally occurring " . for any given nucleus , the more highly ionized it becomes the greater the binding energy of the remaining electrons , culminating with getting the last one off of the hydrogen-like core , which runs roughly $13.6\frac{z^2}{n^2}\text{ ev}$ . unfortunately for significant $z$ those kinds of energies may be big enough to disrupt heavy nuclei which may come apart before they lose that last electron . for a list of theoretical and experimentally observed lines , ask nist . the highest value i find tabulated is a $\mathrm{fm}$ k-edge at around 141 kev .
as your calculations show , yes , it does . the reason for this is that the work performed by a force $f$ on an object is proportional to the displacement through which it is applied , $$w=f \delta x . $$ if an object is going faster , then for an given time interval $\delta t$ ( and thus a given impulse $i=f\delta t$ ) the displacement $\delta x=v\delta t$ is bigger , and you must perform more work .
i think what is happening in rough qualitative terms us that the water freezes around the sides and the top first leaving a hole in the centre . ice expands by 4%-9% when freezing so as the water below freezes it forces the remaining water up through the hole where is freezes around the edge . the hole shrinks as the water freezes and rises around its edge forming the base of the spike . the spike is hollow so water is pushed up to the top where it freezes at the edge making the spoke grow longer update : if you search for ice spike on youtube you will find some good timelapse videos showing these spikes forming . i especially like this one because you can see the unfrozen water pushing up inside the spike . sometimes in a larger water container you can get an inverted pyramid shape . the explanation is the same and the shape is due to the crystaline nature of the ice . this video shows the phenomena
imagine $\theta$ to be quite large , about 80$^\circ$ . any normal car would fall down ( rather slip down against the friction force from tires . ) but if the speed of the car is very high , the centrifugal force would prevent it from slipping ( imagine a horizontal roller coaster loop-the-loop ) . the frictional force would have to act up the incline in this case to counter a component of the weight down the incline . also , consider static friction , not kinetic . the friction is the sideways dragging of the tires and not related to their kinetic/rolling motion .
it is well-known that x-rays are blocked by metal . [ ref : kid 's science ] obviously the doctor wants to look at your internal organs , unobscured by a fuzzy outline of your house keys and pendant . so , the sign is requesting that you removing metal items from the external parts of your body , to allow visibility to the internal parts . ( mri is a totally different story . )
i found a compact version of the course notes , re-written in english by the same professor who gave the original lectures . you can find them at this link .
background ( skip this if you know it all ) ! i too worried about this when i first learned it . basically i think it is easiest to think of the eightfold way quantum mechanically first and worry about qft later . so that is what i will do in this answer . in quantum mechanics ( at least according to wigner ) a particle is a basis vector in some representation of the full symmetry group of the theory ( poincare $\times$ internal ) . fundamental particles are defined to be in the ( anti ) fundamental representation of the internal symmetry group . in the eightfold way we hypothesize that the relevant hamiltonian for our qm theory has an $su ( 3 ) $ symmetry and look at the consequences . we also restrict our attention to just spin 1/2 fermions . this means that by definition there are three fundamental particles ( the up , down and strange quarks ) together with three fundamental antiparticles . now we know from basic qm that multiparticle states are constructed from tensor products of single particle states . a useful mathematical way of enumerating the possible particles is to find all tensor products of the fundamental and antifundamental representations . these decompose into irreducible representations allowing you to easily count the number of degrees of freedom and their properties . how do i decompose a tensor product into a sum of irreps the general procedure is known as clebsch-gordan decomposition . it is completely analogous to the process you go through when adding angular momenta in qm . you can even compute coefficients which tell you exactly how any given tensor product state decomposes for a general symmetry group $su ( n ) $ , see here . of course , in reality this complexity is often not necessary to determine the particle content of the theory . instead you can do the following . to determine the irrep decomposition of $m\otimes n$ plot the weight diagrams of $m$ and $n$ plot the weight diagram of $m\otimes n$ which is obtained by ( vector ) adding the weights in the first two diagrams in all possible ways . check : you should get $mn$ weights find the " highest " weight ( usually the one with largest distance from the origin ) and identify which irrep it belongs to . this involves calculating the higher irreps , or looking up their weight diagrams . note down this irrep . remove all other weights on the diagram which correspond to the irrep for the highest weight you have found . repeat steps 3 and 4 until there are no weights left . the reason this works is fairly transparent - on each iteration of the algorithm you are just identifying an invariant subspace . remembering that irreps are labelled by their highest weights completes the argument . if you want any more detail i recommend you read jan gutowski 's notes particularly section 4.3 . p.s. just read your profile - hope you are having a good start at imperial ! i will be a phd student at queen mary from january , so maybe i will see you around at a london triangle meeting .
thanks to the references in jamie 's link i have found a wealth of arguments in the following publication : is classical mechanics really time-reversible and deterministic ? the full article can be found on jstor here .
my guess will be that it is mainly the effect of capillarity forces due to the porous quality of the stone surface . this might also be a good read .
you are correct that the " normal " formula $y = h - gt^2/2$ does not work when the gravitational acceleration changes , so you need a different formula . the mathematical expressions are a little ugly , though . steven laid the groundwork for this , but i am going to point you to an earlier answer of mine where i did the calculation . the result comes out to be $$t_f - t_i = \frac{1}{\sqrt{2g ( m_1 + m_2 ) }}\biggl ( \sqrt{r_i r_f ( r_i - r_f ) } + r_i^{3/2}\cos^{-1}\sqrt{\frac{r_f}{r_i}}\biggr ) $$ $r_i$ and $t_i$ are the initial position ( height ) and time , respectively , and $r_f$ and $t_f$ are the corresponding final values . this equation is a little " backwards " in the sense that instead of expressing position as a function of time , it expresses time as a function of position . you can invert it to express position as a function of time , but you will not find a single nice function for it . you would have to do the inversion numerically , by plugging it into a computer , or by computing a power series or something like that .
yes . there is an even easier notation for dispersion , or standard deviation which is $$d ( q ) = \sqrt{ \langle q^2\rangle - \langle q\rangle^2 }$$ both those terms are the same . so $d ( q ) $ is zero .
if you look at reflectances of common materials used to make mirrors with ( for example , the topmost graph found on this wiki page ) , you will see that not 100% of the light is reflected , especially at the shorter wavelengths . i am still looking for a somewhat better source for similar curves for household mirrors , but i know that the idea is roughly the same -- it is actually pretty difficult to make a mirror that reflects 100% of all incident light . the light that does not get reflected gets either absorbed ( mostly ) or transmitted ( usually only for very thin film mirrors ) . the portion that gets absorbed is transformed into heat , which is transferred into the mirror material . therefore , if you shine a short-wavelength , high-power laser beam directly onto a mirror with the intention of damaging the mirror , the power of the laser must be great enough to ensure that the portion of the light that gets absorbed is great enough to heat up the material sufficiently ( and fast enough ) to melt it . this makes most laser weapons only really effective on surfaces with low reflectance ( plastics , certain composites , human skin , etc . ) thin-walled structures ( fuel tanks , etc . ) sensitive electronics ( camera 's , targeting systems , etc . ) etc . laser weapons ( at least , currently ) serve a different purpose than ballistic weapons ; they are more a tool for precision work at large distance . these are the same sorts of problems a laser cutting tool encounters . most of these sorts of machines can cut effortlessly through plastics , wood , ect . with high precision . however , when cutting through metal , they can only cut through relatively thin sheets ( 1cm aluminum already presents too much of a challenge for most machines ) because of the high reflectance of most metals . the efficiency of the machine is also not too great with sheet metal -- a 50 kw machine will normally transfer only a handful of watts of heat to the focal point .
always start with a nice clear diagram/sketch of the problem . it all follows from there . here is a free body diagram i made for you . then you have ( the long detailed way ) : sum of the forces on body equals mass times acceleration at the center of gravity . $\sum_i \vec{f}_i = m \vec{a}_c $ $$ a_x = m a_x \\ a_y - m g = m a_y $$ sum of torques about center of gravity equals moment of inertia times angular acceleration . $\sum_i \left ( \vec{m}_i + ( \vec{r}_i-\vec{r}_c ) \times\vec{f}_i\right ) = i_c \vec{\alpha} $ $$ a_x \frac{l}{2} \sin ( \theta ) - a_y \frac{l}{2} \cos ( \theta ) = i_c \ddot \theta $$ acceleration of point a must be zero . $\vec{a}_a = \vec{a}_c + \vec{\alpha}\times ( \vec{r}_a-\vec{r}_c ) + \vec{\omega}\times ( \vec{v}_a-\vec{v}_c ) $ $$ a_x + \frac{l}{2} \sin ( \theta ) \ddot\theta + \frac{l}{2} {\dot\theta}^2 \cos ( \theta ) =0 \\ a_y - \frac{l}{2} \cos ( \theta ) \ddot\theta + \frac{l}{2} {\dot\theta}^2 \sin ( \theta ) =0 $$ now you can solve for $a_x$ , $a_y$ from 3 . and use those in 1 . to get $a_x$ , $a_y$ . finally use 2 . to solve for $\ddot\theta$ or do the shortcut of finding the applied torque on a and applying it to the effective moment of inertia about the pivot $i_a = i_c + m \left ( \frac{l}{2}\right ) ^2 $ to get $$ \ddot\theta = \frac{m g \frac{l}{2} \cos\theta }{ i_c + m \left ( \frac{l}{2}\right ) ^2 } $$
define the operators $\hat a ( f ) =\sum f_j\hat a_j$ and $ |f_1 , . . . , f_n\rangle:=\hat a ( f_1 ) . . . \hat a ( f_n ) |vac\rangle$ . then $\langle g_1 , . . . , g_m|f_1 , . . . , f_n\rangle$ vanishes for $m\ne n$ and is a sum of the products $\langle g_1|f_{j_1}\rangle . . . \langle g_n|f_{j_n}\rangle$ for all possible permutations $ ( j_1 , . . . , j_n ) $ of $1 , . . . , n$ . note that the $\langle g|f\rangle$ are easy to compute . the formula you requested is a special case of this .
i will be substantially lazy and drop all quadratic terms . keep in mind that i am doing everything to linear order . also , i will suppose we are finding normal coordinates around the point $x^\mu = 0$ . it is trivial to modify the technique for use anywhere else . i will follow your mostly minus metric convention but use $c=1$ . we have the metric $$ \mathrm{d}\tau^{2}=g_{\mu\nu}\mathrm{d}x^{\mu}\mathrm{d}x^{\nu} , \ ( 1 ) $$ and we want coordinates $\tilde{x}^\mu ( x ) $ such that $$ \mathrm{d}\tau^{2}=\eta_{\mu\nu} \mathrm{d}\tilde{x}^{\mu}\mathrm{d}\tilde{x}^{\nu} , \ ( 2 ) $$ ( to linear order ) in a neighbourhood of zero . we write the coordinate transformation as $$ \tilde{x}^{\mu}=ax^{\mu}+\frac{1}{2}b_{\nu\rho}^{\mu}x^{\nu}x^{\rho}+\cdots , \ ( 3 ) $$ where $a$ and $b^\mu_{\nu\rho}$ are constants to be determined . the higher order terms do not influence the construction . without loss of generality we take $b^\mu_{\nu\rho}=b^\mu_{\rho\nu}$ . subbing ( 3 ) in ( 2 ) i get ( exercise ) $$ \mathrm{d}\tau^{2} = \left [ a^{2}\eta_{\rho\sigma}+a\left ( b_{\sigma\lambda\rho}+b_{\rho\lambda\sigma}\right ) x^{\lambda}+\cdots \right ] \mathrm{d}x^{\rho}\mathrm{d}x^{\sigma} . $$ matching this onto ( 1 ) order by order gives the conditions ( exercise ) $$\begin{array}{rcl} a^{2}\eta_{\rho\sigma} and = and g_{\rho\sigma}\left ( x=0\right ) , \\ a\left ( b_{\sigma\lambda\rho}+b_{\rho\lambda\sigma}\right ) and = and g_{\rho\sigma , \lambda}\left ( x=0\right ) . \end{array}$$ in your case this gives the conditions ( exercise ) $$\begin{array}{rcl} a^{2} and = and 1-2\phi^{0} , \\ ab_{t\lambda t} and = and -\phi_{ , \lambda}^{0} , \\ ab_{i\lambda i} and = and \phi_{ , \lambda}^{0} , \text{all the other}\ b^\mu_{\nu\rho}\ \text{vanish} , \end{array}$$ ( where for shorthand $\phi^0 \equiv \phi ( x=0 ) $ and $i=x , y , z$ ) which i am sure you can solve . : ) i get $$ \tilde{x}^{\mu}=\sqrt{1-2\phi^{0}}x^{\mu}-\frac{\phi_{ , \lambda}^{0}x^{\lambda}}{2\sqrt{1-2\phi^{0}}}x^{\mu}+\cdots , $$ though you should check this yourself ( cause i do not feel like it ) in case i made any index/sign mistakes . : )
voltage is the unit of electric potential , the electric potential difference ( in your case , the potential difference between the two ends of resistor in a circuit ) can be called the voltage drop . the potential difference produces an electric field $\vec{e}$ , and the direction of $\vec{e}$ points from high potential to low potential . the electric field applies a force on charged particles ( i.e. . electrons in circuits ) such that the electrons are driven by this force and move , thereby producing a current . so you can see the potential ( voltage ) difference is the reason why there is a current . by the way , you cannot say the " current 's voltage " , since the current is defined as $i = dq/dt$ . that is , it only describes the flow of charge per unit time . when electrons move through a resistor they are scattered by the other electrons and nuclei , causing the electrons lose some of their kinetic energy . but the presence of the electric field will then accelerate the electrons again . we can calculate the average kinetic energy statistically , and assume the electrons are moving at a single average velocity . thus after each collision there is a loss of kinetic energy ( it is converted to heat ) but which is recovered due to the work done by the electric field . and this work is equal to the potential energy difference . you can see that the electrons have the same kinetic energies both when they enter and when they leave the resistor , but different potential energies . so we can say the voltage drop at the two ends of a resistor is caused by the potential energy difference .
i only know one particular reason : the transition responsible for the h1 line is highly forbidden and shows an extreme lifetime ( 10^7 years ) , so the absorption rate in interstellar clouds , which can be very opaque for every other radiation , is very small . looking at the h1 line allows you to see objects which are for example hidden behind dust clouds that absorb a lot of the radiation of the object behind .
you are incorrect to suppose that this spacetime is curved . in fact , up to some conditions on the coordinate ranges , this is simply a piece of minkowski spacetime . let me put it in this form : $$ds^2 = dt^2 - t^2 ( d\psi^2 + \sinh^2\psi\ , d\omega^2 ) \text{ , }$$ where $d\omega^2 = d\theta^2 + \sin^2\theta\ , d\phi^2$ is the metric for a unit $2$-sphere , and we can go from spherical minkowski form $dt^2 - dr^2 - r^2\ , d\omega^2$ to yours by the substitution $$\begin{eqnarray*}r = t\sinh\psi and \quad\quad and t = t\cosh\psi\end{eqnarray*}$$ simply , the ricci tensor is zero because $r_{\mu\nu} = 0$ is a coordinate-invariant condition : if a tensor zero in some coordinate chart ( e . g . , minkowski ) , then it is zero in every coordinate chart ( e . g . , yours ) . naturally , the riemann tensor is also zero . interestingly , this is the also the hyperbolic flrw ( "big bang" ) metric in the limit of zero density , where a galaxies do not interact gravitationally but every galaxy sees itself a center of cosmological expansion . for details , see the milne universe model . . . . the article at the link below says that this interval corresponds to an open flat ( space curvature = -1/r^2 &lt ; 0 ) expanding universe ( look at the picture in the question ) . can you explain that ? sure : spacetime curvature is not the same thing as space curvature , or more generally the curvature of a manifold is not the same thing as the curvature of a submanifold of lower dimensionality . take euclidean space $e^3$ in spherical coordinates , $ds^2 = dr^2 + r^2d\omega^2$ , and look at submanifolds of constant radius : $$d\sigma^2 = r^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) \text{ , }\quad r = \text{constant}$$ one can verify by direct computation that the scalar curvature is positive , as one might expect from them being spheres . obviously , if you ' slice up ' $e^3$ by planes , each will have zero curvature instead . in your case , what is going on is just the same thing , except we are ' slicing up'/'foliating ' the minkowski spacetime $e^{1,3}$ by spacelike hypersurfaces of the form $$d\sigma^2 = t^2\left [ d\psi^2 + \sinh^2\psi\ , d\omega^2\right ] \text{ , }\quad t = \text{constant}$$ in this form , it is not hard to guess that the curvature for each hypersurface is same as above except negative . ( let me know if you need help calculating it explicitly , but for the moment i think intuitively motivating the fact that this makes sense would be more fruitful . ) since coordinate changes do not change the scalar curvature , we can let $\rho = t\sinh\psi$ to get $d\rho = t\cosh\psi\ , d\psi$ and $$t^2\ , d\psi^2 = \frac{d\rho^2}{\cosh^2\psi} = \frac{d\rho^2}{1 + \sinh^2\psi} = \frac{d\rho^2}{1+\rho^2/t^2}\text{ , }$$ putting it in the same form as ( 80 ) except for variable names ; the other part of the metric should be more obvious .
temperature is not a concept that has a lot of utility at the level of single atoms because it represents the mean kinetic energy of a group of particles ( to within a coefficient ) . you can define it , it just does not help much . at the level of two atoms you revert to a more fundamental model such as the forces between them . one atom transfers energy to another through electromagnetic forces between them . when that energy manifests as randomized kinetic energy at the microscopic scale we refer to it as " heat " at the macroscopic scale . in a solid it is usually reasonable to treat the forces between individual pairs of atoms as being spring-like ( i.e. . they obey $f_{i , j} = -k ( r_{i , j} - r_0 ) $ ) . starting from there you can build various models of solid behavior . for instance the einstein model of a crystal .
afer the discussions in the comments this would be my solution : the l.h. s in the equations of motion is always simply $m\ddot{x}$ , the r.h. s contains the sum of all acting forces . so you have gravity and the force of the spring . thats it . $m\ddot{x}=mg-k ( x-x_{0} ) $
let me here just derive the equation ( 6.11 ) that follows the sentence , you mention . the navier-stokes equation ( 6.6a ) reads $$\partial_t v_i + v_j\partial_j v_i = -\partial_i p + f_i + \nu~\partial_j\partial_j v_i . $$ the incompressibility condition ( 6.6b ) reads $$\partial_jv_j=0 . $$ hence we have in the unprimed and the primed point that $$\partial_t v_i = -\partial_j ( v_j v_i ) -\partial_i p + f_i + \nu~\partial_j\partial_j v_i , $$ and $$\partial_t v&#39 ; _i =-\partial&#39 ; _j ( v&#39 ; _j v&#39 ; _i ) -\partial&#39 ; _i p&#39 ; + f&#39 ; _i + \nu~\partial&#39 ; _j\partial&#39 ; _j v&#39 ; _i , $$ respectively . therefore , averaging yields $$\begin{align} \partial_t \langle v_i v&#39 ; _i \rangle and = \langle v&#39 ; _i \partial_t v_i \rangle + \langle v_i \partial_t v&#39 ; _i \rangle \\ and = - \langle v&#39 ; _i\partial_j ( v_j v_i ) \rangle - \langle v_i \partial_j&#39 ; ( v&#39 ; _j v&#39 ; _i ) \rangle \\ and -\langle v&#39 ; _i \partial_i p \rangle -\langle v_i \partial&#39 ; _i p&#39 ; \rangle\\ and +\langle v&#39 ; _i f_i \rangle +\langle v_i f&#39 ; _i \rangle \\ and +\nu \langle v&#39 ; _i\partial_j\partial_jv_i\rangle + \nu \langle v_i\partial&#39 ; _j\partial&#39 ; _j v&#39 ; _i\rangle \\ and = -\partial_j \langle v_i v_j v&#39 ; _i\rangle -\partial_j&#39 ; \langle v&#39 ; _i v&#39 ; _j v_i\rangle \\ and -\langle v&#39 ; _i \partial_i p \rangle -\langle v_i \partial&#39 ; _i p&#39 ; \rangle\\ and +\langle v&#39 ; _i f_i \rangle +\langle v_i f&#39 ; _i \rangle \\ and +\nu\left ( \partial_j\partial_j + \partial&#39 ; _j\partial&#39 ; _j\right ) \langle v_iv&#39 ; _i\rangle , \end{align} $$ where we have used that averaging and differentiation commute , and also that primed velocities are independent of unprimed derivatives , and vice-versa . the full karman-howarth-monin relation is derived by marc brachet on p . 8-9 here , essentially following the book by uriel frisch ( 1995 ) .
hooke 's law is frequently used to model multi-dimensional materials because the stress tensor is simple ( linear ) . the full expression can be found on wikipedia . the simplification for 2d is straight forward ( drop any terms with a 3 in the subscript ) . note that whether deformation in one dimension affects the others is a property of the material and shows up through poisson 's ratio ( $\nu$ ) . independence between deformations in x and y imply $\nu = 0$ if you imagine two perpendicular springs only , then the terms with $\gamma$ ( or different subscripts like 12 , 23 , 31 , depending on the form of the equation ) drop out of the expression as those are shear terms . the shear terms can be thought of as a spring across the diagonal . the stress tensor $\sigma$ is defined as the force per unit area .
i think you may be misguided by the concept that we associate $\textbf{observables}$ to self-adjoint operators . they do operate on the hilbert space , but to see them as entities that transform states or prepare them is a little bit tricky . i will describe here self-adjoint operators and preparation of states . 1 ) the true ( physical ) power of self-adjoint operators for describing observables lies in the spectral theorem , and not in its $\psi \mapsto a\psi$ action . physically , what it means ? there is a set called spectrum of an observable , and it is the set of possible outcomes on its measure for given states . for example , a spin observable $s$ on a 1/2-spin system has spectrum $\sigma ( s ) = \{-1/2 , +1/2\}$ , and decomposes as a sum of its spectral projections , $s = +1/2 p_{+} -1/2 p_{-}$ . in general , there is a spectral resolution $e$ , that is , a bunch of projection related to the spectrum , such that the operator can be written as $a = \int_{\sigma ( a ) }\lambda de ( \lambda ) $ . and what are the spectral projections ? those are again ( self-adjoint ) operators , but the whole collection of spectral projections will give you a probability measure when coupled with a state . in the spin system example , if you take a state $\psi$ , then $\langle\psi , p_+\psi\rangle$ would give you the probability of measuring a +1/2 spin , and likewise for -1/2 . now suppose you had a 1/2 spin system with prepared state $\psi$ , and you measure the spin , and get +1/2 . after the measurement , your state collapses to a $|+1/2\rangle$ state . in a more detailed formalism , suppose you have prepared a state $\psi$ and you are going to make a measurement of an observable expressed as $a = \int_{\sigma ( a ) } \lambda de ( \lambda ) $ ( where the $e$ is the spectral resolution of your operator , just think of the 1/2-spin example intuitively ) . then suppose your measurement is on a subset $\lambda \subset \sigma ( a ) $ ( you may think of the set $\{+1/2\} \subset \{-1/2 , +1/2\}$ . your state $\psi$ then collapses to the following state $\phi$: $\psi \rightarrow \phi = \frac{e ( \lambda ) \psi}{\|e ( \lambda ) \psi\|} . $ ( notice that $\phi$ is normalized and well defined , since $e ( \lambda ) \psi=0$ then the probability of the outcome being in $\lambda$ would be zero to start ) . summing up , you do not simply apply a self-adjoint operator on a state , since , as you have seen , it does not have much meaning . this is a point most introductory qm books do not stress as much as i would want . what happens with measurements and collapses and whatsoever uses , as i tried to point out , the spectral projections more than the operator itself . so , as you said about your hamiltonian operator , it does not act like your syrup machine , which we will try to cover up next . 2 ) now what you describe as " tools " , in your example , the putting syrup , is not a measurement per se , it is a preparation of states , which would grab a state without syrup and put syrup in it . the modeling of such procedure is usually ignored , at least to my knowledge . one choice would be just saying " my state now is syrup" , end of discussion . other option is using unitary operators ( $u$ such that $uu^* = u^*u = 1$ ) . those transform state vectors in state vectors . if you would like more sophisticated examples , it starts to get tricky , and i will shut up before i say something very wrong about it . but rest assured this is not easy at all , and your question is really nice . hope to see some other inspiring aswers .
the formula you have specified $$ \delta k = \sqrt{ ( \delta k_1 ) ^2 + ( \delta k_2 ) ^2} $$ is the formula to obtain error of quantity $k$ , as being dependent on $k_1$ and $k_2$ according to the following expression $$ k = k_1 + k_2 . $$ generally , to obtain experimental error of a dependent quantity ( and the expression stated in your question ) , you start with the expression for dependent quantity $$k = f ( k_1 , k_2 , . . . ) $$ and use statistical expression $$\delta k = \sqrt{\sum_i \left ( \frac{\partial f}{\partial k_i} \delta k_i \right ) ^2} . $$ if $$k = \frac{k_1 + k_2}{2}$$ then $$ \delta k = \frac{\sqrt{ ( \delta k_1 ) ^2 + ( \delta k_2 ) ^2}}{2} $$ so the generalized answer might be : you have to divide with $n$ and not $\sqrt{n}$ . however , bare in your mind that the statistical expression above might be used when measured quantities are " independent " of each other . if $k_1$ and $k_2$ are the same quantity measured in two measurements , this is not exactly true , so the exact statistical expression is much more complicated .
the experiment detected more than just solar neutrinos , it also detected those produced by interactions with muons from cosmic rays , radioactive decays in the rocks surrounding the mine , internal radioactive contaminants in their tetracholorethylene fluid , and atmospheric argon decay production , but like any good experiment they controlled and subtracted all of these backgrounds . the full paper is actually available for free here and in section 6 of the paper ( nonsolar production of ${}^{37}$ar ) there is a full discussion of all of their background subtraction methods and calibration measurements . in particular , section 6.1 ( cosmic rays ) discusses how they subtracted the cosmic ray background from direct depth intensity measurements .
i have an answer to establish expectations from first principles . i have not looked up real values , nor am i hopeful of finding such values . all i am doing here is setting a general expectation for the difference in cross sections of ion-ion fusion and ion-atom or atom-atom fusion . i should note that the one glaring piece of information missing from the question is the energy of the reaction . as such , my answer will be somewhat non-specific on that point , but my position is that it does not affect my overall conclusion that no major reaction rate benefit could be obtained in fusion machines by this proposed mechanism . i used a number of simplifications to craft a simple algebraic form for the difference in cross sections . i should first specify though , i used a " radius " for the cross section defined as $a=\sqrt{\sigma / \pi}$ . yes , it is true that cross-sections are not really areas , but for the relative scales here it can be treated as such . now , here are the assumptions i used : fusion cross section &lt ; &lt ; atomic radius physics are identical to ion-ion interaction after atomic radii meet only the impact of cross section broadening considered , the increase in energy due to atom charge interaction not considered target nuclei does not move , you could try to eliminate this with reduced mass or a adjustment to reaction energy , i will leave that for someone else to try . this assumption is not really necessary but i just did not see a quick way around it . the basic proposition we have is that the ion-ion cross sections of the reactions are already known . this , naturally , depends on the energy . my proposition is that , whatever the ion-ion reaction trajectory is , it will be different from the ion-atom interaction by the electrostatic force operating at distances greater than the atomic radius . i hope my method id already starting to seem clear , but i will use an illustration . the proposition is that we know $a_{++}$ we know the profile of the $y-y_{++}$ with simple electrostatic physics . using the assumption that the atomic radius is small compared to the fusion cross section radius , i can claim $\sin{ \theta} = a/s$ . the electrostatic force from the net charges on the atom/ion will operate along the line between the moving atom/ion and the target atom/ion . the distance between these can be approximated to be equal to $s$ . if we do not bother with the speedup from this force , then we will just consider the component inward . $$f_{in} = \sin{\theta} \frac{ k q_1 q_2 }{ s^2 } = \frac{ k e^2 z_1 z_2 a }{ s^3 } $$ the speed of the incoming atom/ion is roughly constant , so i can give this approximate expression to get the above math in terms of time . this applies from $-\infty$ to $-r/v$ . the $v$ follows from the energy of the reaction . $$s ( t ) = - v t$$ then the basic kinematics follow logically . $$v_{in} ( t ) = \int_{-\infty}^{t} \frac{f_{in} ( t' ) }{m} dt ' = \frac{ k e^2 z_1 z_2 a }{2 v^3 m t^3}$$ $$ y ( r ) - y_{++} ( r ) = \int_{-\infty}^{-r/v} v_{in} ( t ) dt $$ $$ a - a_{++} = v_{in} ( r ) \frac{r}{v} + ( y ( r ) - y_{++} ( r ) ) = \frac{ k e^2 z_1 z_2 /r }{m v^2} a$$ this gives the difference in cross section radius due to atom-ion or atom-atom interaction beyond that atomic radius . this is a particularly useful form if we introduce $e_c=k e^2 / r$ and the kinetic energy of the incoming atom/ion . the difference in cross sections will be about twice this difference in radii . $$\sigma - \sigma_{++} = \pi ( a^2 - a_{++}^2 ) \approx - 2 \pi a^2 z_1 z_2 \frac{e_c}{e_k}$$ $$\frac{ \sigma - \sigma_{++} }{ \sigma_{++} } \approx 2 z_1 z_2 \frac{e_c}{e_k}$$ $$e_c = 0.027 kev$$ this makes intuitive sense . the cross section will be affected by an amount proportional to the electrostatic energy of the two touching atoms and inversely proportional to the kinetic energy of the interaction . the interaction energy in iter will be about $8 kev$ , and the optimal energy for dt fusion is $80 kev$ ( previous graph ) . if you had two neutral atoms interacting , the cross sections will be greater than the ion-ion interaction by 0.6 % at iter energies . it would be an improvement of 0.06 % at optimal dt energies . the best you could ever hope for would be the pb interaction with the b fully ionized ( +5 ) and the hydrogen with ( -1 ) . even this is likely unattainable as some comments have pointed out . if we look at this at iter energies ( impossible but this is best-case ) , that would increase the cross section by about 5% . pretty much all other scenarios would have less of an improvement than this . the bottom line is that the coulomb attraction at distances beyond the atomic radius is very very small compared with the coulomb repulsion that the nuclei have to overcome as they get closer than that . any adjustments to the cross section from this atom-ion or atom-atom interaction will really be &lt ; 1% for most conceivable fusion reactions .
you should already be familiar with damping . it simply refers to the fact that if you set a spring going , it eventually stops . the wikipedia article should cover most of what you want to know . any particular spring may be damped for all sorts of reasons . any way the spring can lose energy contributes to damping , so it could be lost internally to heat due to stressing the material , or externally to heat via friction , or externally to an electromagnetic field , to some sort of mechanical dashpot , etc . if you were designing an experiment to study damping , you could be interested in a number of different particular things . you will have to make your own choice about what the most interesting thing to study is . for example , would you like measure the damping ratio ? the overall magnitude of the damping effect ? the time it takes your spring to reduce its amplitude to 1/2 its previous value ? if you have a particular effect picked out , do you want to know how it depends on the load on the spring , or the material of the spring , or whether the spring is in a vacuum , etc . there are many possible things to study , so your question has no definite answer . i am sure you can find something particular you find worthwhile .
you say : by setting γ to 1 , we obtain the result in galilean relativity ( ie : " the time between the lightning strikes is $\tfrac{vd}{c^2}$" ) , which is the theory of space time before einstein came out with special relativity . but remember that $\gamma$ is not some independant parameter . it is just shorthand for : $$ \gamma = \frac{1}{\sqrt{1 - \tfrac{v^2}{c^2}}} $$ so you can not just set $\gamma = 1$ without changing either $v$ or $c$ or both . if you set $v = 0$ then $t = t ' = 0$ . in this case the events are simultaneous in both frames , but that is not surprising because if $v = 0$ both frames are the same inertial frame . if you want to use the galilean limit but keep $v$ non-zero the way to do this is to increase the speed of light to infinity ( obviously this is a thought experiment ) . in that case $\gamma\tfrac{vd}{c^2} = 0$ and again $t = t ' = 0$ so the lightning strikes are simultaneous in both frames .
see the wiki article on polarized 3d glasses . most likely , you have a pair of circularly polarized glasses . the mirror reverses the circular polarization . the article on circular polarization does it better than i would be likely to achieve in less than an hour or two . or hyperphysics , or google .
let us start by summarising the governing equations of mhd . we have the reduced form of the maxwell equations $$\nabla \times \mathbf{b} = \mu \mathbf{j} , $$ $$\nabla . \mathbf{j} = 0 , $$ $$\nabla \times \mathbf{e} = - \partial \mathbf{b} / \partial t , $$ $$\nabla . \mathbf{b} = 0$$ and the auxillary equations $$\mathbf{j} = \sigma ( \mathbf{e} + u \times \mathbf{b} ) , $$ $$\mathbf{f} = \mathbf{j} \times \mathbf{b} . $$ from the last two equations you get your result , namely that $$\mathbf{f} = \sigma ( \mathbf{e} + u \times \mathbf{b} ) \times \mathbf{b} . $$ note that the above combine to give the induction equation . i hope this helps .
as far as i have understood from this paper , they have given some observational limits to the value of $\omega_{k , 0}$ , but this article concludes asserting that " there is no evidence from planck for any departure from a spatially flat geometry " . taking $\omega_{k , 0}=0$ and the value for $\omega_{r , 0}$ given at this post , one can compute the above integral obtaining $t_0 = 0.947797 \ , h_0^{-1}$ , which , taking $h_0 = 67.3 \ , \text{km} \ , \text{s}^{-1} \text{mpc}^{-1}$ , gives $t_0 = 13.78 \times 10^9$ years .
that way of thinking about this is a nice one . from “the elegant universe” ? anyway , mathematically , you can define this four-velocity like the normal velocity , the time derivative of the position . however , in special relativity , the position is space and time $ ( t , x , y , z ) $ . and the derivative has to be with respect to the proper time ( or curve parameter ) $\tau$: $$ u = \frac{\mathrm d}{\mathrm d\tau} x $$ the velocities that you normally measure are calculated with respect to $t$ . so we need the chain rule here : $$ \frac{\mathrm d}{\mathrm d\tau} = \gamma \frac{\mathrm d}{\mathrm dt}$$ so the four-velocity is , taking $c = 1$: $$ u = \gamma \frac{\mathrm d}{\mathrm dt} ( t , x , y , z ) = \gamma ( 1 , v_x , v_y , v_z ) $$ as you said : all objects travel at the speed of light in space time . that means that $|u| = 1$ . with the minkowsi metric , this is : $$ 1 = \gamma^2 - \gamma^2 ( v_x^2 + v_y^2 + v_z^2 ) $$ this means that you travel less through time when you travel through space . but since the equation are for the squares of the velocities , you can indeed have $v_x &lt ; 0$ . so the twin that is on the journey can have $v_x &gt ; 0$ for the first half and then $v_x &lt ; 0$ for the second part and go back to earth . that is not a problem . if you look at a space time diagram of this , you can construct the time intervals for each party . the observer on the earth will have gone through a couple of time steps . when you draw in the planes of simultaneity of the observer on earth , you will see the time intervals for the traveling observer . there , you can see that the earth time intervals are much longer for the traveler . he will only experience a bit more than two of those intervals during his journey . therefore , he is younger . so , when they meet at a space time point again , their age is different . they cannot meet at the space time point where earth is the same age as the traveler , though .
you can totally transfer charge using protons . or using na+ or any kind of charged particle . it happens all the time - if you look at how a wet cell battery works , you will find that while charge across the wire is carried by electrons , current flows along the salt bridge via charged ions ( theoretically protons could be among these ) . i bet if you could somehow make a superfluid out of he+ or something you could use that to carry charge like a superconducting wire . i think our intuition that electrons are always the charge carriers comes from the fact that : 1 . ) electrons are very light compared to protons , so if you imagine putting a proton in an electric field and an electron in the same electric field , the electron will be accelerated 1000x more ( same charge , 1/1000th the mass ) . this means that when there is a charge imbalance and either proton or electron flow could alleviate it , the electrons will flow way before the protons are impelled to move . 2 . ) electrons are the mobile charged particles in a solid - all the protons are bound in nuclei . since almost all of our intuition about current flow comes from wires or other solid conductors , we are almost exclusively worrying about electron flow .
at constant 1 g acceleration half-way through , then constant 1 g deceleration the remaining half , it takes 7 years in rocket time , 38 years in earth time : http://www.cthreepo.com/lab/math1.shtml scroll down to long relativistic journeys and enter your data . to the andromeda galaxy ( 2.5 mil ly ) it is 29 years in rocket time ! : )
the doppler shift causes a shift in wavelength at the origin of the wave ( the frequency of the source never changes ) . this results in a shift in frequency for the observer . in the link below you can see the emission of the wave for a moving source causes the wavelength to be shorter in front and longer behind . the actual source is not changing in frequency . so it is a matter of relativity . to the traveling observer ( in the source ) , only the wavelength is changing , to the stationary observer ( experiencing the doppler shift ) both frequency and wavelength have changed . lookang , wikimedia commons . more simulations and applets here .
as your read suggestion suggested , you can approximate physical reality with " no collisions are simultaneous " . the reason is that the physical world is full of indeterminacies ( aka errors ) due to thermal fluctuations and many other sources . what this means is that , even if the strict mathematical solutions that you will find by assuming simultaneous collisions versus assuming random collisions ( treating all collisions that your algorithm thinks are simultaneous as happening in random order ) could differ , the " physical " behavior of the system will not change in any meaningful way . no human could tell the difference between the two . in conclusion , you are safe in treating the system of collisions as non-simultaneous , and the consequences will be undetectable for the human eye ( including any real physicist 's real world measurements )
entanglement is a quantum correlation between two ( or many ) objects - a correlation means that these two objects ' properties are not independent of each other - which was created in the objects ' common past when they were close to one another i.e. when they were two parts of the same physical system . quantum mechanics changes the character of possible " properties " that objects may have ( the quantities describing objects 's properties are usually called " observables " and they are represented by hermitian operators on the hilbert space ) , as well as the way how these properties are measured and predicted ( just probabilistically ) , so it also changes the character and magnitude of correlations that the objects may exhibit . in particular , quantum correlations may often be stronger - and affecting a large fraction of measurable properties of the objects - than what would be possible according to classical ( i.e. . non-quantum ) physics . in classical physics , correlations have to satisfy e.g. the so-called bell 's inequalities in various situations but quantum mechanics - and the real world - can easily surpass these bounds . technically , objects and their properties in quantum mechanics are described by wave functions . to describe the state of two mostly independent objects , one has to take a wave function from the tensor product $h_1\otimes h_2$ of the hilbert spaces describing the individual objects . the wave function in the tensor product implies probabilistic predictions for any pairs of properties of the first and second object ; in general , they are not independent , and for each combination of the objects ' properties , quantum mechanics ( and the wave function ) may remember an independent probability . any vector in the tensor product that can not be written as a tensor product of vectors from $h_1$ and $h_2$ ( instead , it can only be written as a linear combination of such tensor products of vectors ) is called entangled . in other words , it is non-entangled if it is a simple tensor product of two simpler vectors . if it is a simple tensor product , all probabilities of " coupled properties " of the pair of objects simply factorize to the probability of the first object , and probability of the second object , as you know from probabilities of independent phenomena . the simplest pedagogical example of an entangled state ( as well as the entangled state that is most often found in literature ) is $$\frac{x_1\otimes y_1 + x_2\otimes y_2}{\sqrt{2}}$$ because there are two terms with four different factors , you can not use the distributive law in any way that would allow you to rewrite it as a simple product . the letters $x , y$ refer to the two objects and the labels $1,2$ refer to two different states of each of the two objects . in this state , if the property "1 or 2" is measured on $x$ , one obtains the answers 1 or 2 with 50% probability for each : the coefficient of the wave function is $1/\sqrt{2}$ because these complex coefficients have to be squared to obtain the probability . however , because $x_1$ is " coupled " to $y_1$ and $x_2$ is coupled to $y_2$ , the state and the machinery of quantum mechanics predict that the object $y$ will be measured to have the same property : if $x$ is in 1 , $y$ is in 1 , and the same for the state 2 . linear algebra - which is crucially important for quantum mechanics - allows one to reinterpret the state above as an " identity operator " so the correlation will exist regardless of the type of measurement that we perform both on $x$ and $y$ . for example , if the two states represent spins , the two particles will be correlated so that you will find out that they are polarized with respect to the same axis , if you measure both particles ' polarizations with respect to the same , particular , but arbitrary axis . this would be kind of impossible for two separated particles in classical physics that could only be perfectly correlated for one choice of the axis - but not another axis rotated by 45 degrees , for example - without a communication in between them . however , quantum mechanics predicts that such a 100% correlation " regardless of the axis " is not only possible but guaranteed by the state above and it requires no communication . indeed , one can prove that relativistic theories in quantum mechanics - especially quantum field theory - do not allow one to transmit a single bit of information faster than light even though this would be needed in classical physics to guarantee the perfect correlation that quantum mechanics predicts for these experiments ( and that the experiments confirm ) .
your notation is misleading . certainly , the eigenvector of the $s_x$ operator : \begin{equation} s_x = \frac{\hbar}{2}\begin{pmatrix}0 and 1 \cr 1 and 0\end{pmatrix} \end{equation} is : \begin{equation} \mid s_x , +\rangle = \frac{1}{\sqrt{2}}\begin{pmatrix}1\cr 1\end{pmatrix} \end{equation} with an arbitrary choice of global phase . ok , so this is what i guess you meant by $\mid s_x , +\rangle$ . now , the most general state for this system is given by two numbers ( the two original complex numbers need four real numbers , but one you remove by the normalization requirement and the second by the freedom of choosing a global phase ) . these are usually represented by the bloch sphere . your state is just a set of states in a circle on this sphere , but not necessarily an eigenvector of $s_x$ . edit ( 01/27/2010 ) : in view of the claim that i am not answering the question , let 's try to fully understand the origin of this phase . let 's say you want to measure the spin in a direction given by the unit vector : \begin{equation} \vec{n} = ( \sin\theta\cos\varphi , \sin\theta\sin\varphi , \cos\theta ) \end{equation} it is a common exercise in qm to show that the eigenvalues of $\vec{s}\cdot\vec{n}$ are $\pm 1$ and that the eigenvector associated to the eigenvalue $+1$ is : \begin{equation} \mid\vec{s}\cdot\vec{n} , +\rangle = \cos ( \theta /2 ) \mid +\rangle + \sin ( \theta /2 ) e^{i\varphi}\mid -\rangle \end{equation} where the basis are eigenvectors of $s_z$ and with the state written up to an arbitrary global phase , as i explained before . if you have have trouble finding this answer , look it up in sakurai 's book , chapter 3 . he derives this result over and over again . therefore , by looking at the state in the question , we immediately identify $\theta=\pi/2$ and $\varphi$ as the phase you left undetermined $\varphi = \delta$ . so , this state is a spin 1/2 system which , if you measure $\cos\varphi\ ; s_x + \sin\varphi\ ; s_y$ ( meaning , if you align your stern-gerlach device in the direction $\vec{n}= ( \cos\varphi , \sin\varphi , 0 ) $ ) , you will get always the result $+1$ . and that is the origin of your phase . but your notation is misleading , indeed .
you should get them started whenever they are interested . answer questions , take them out to do astronomy related activities when they want , and so forth . the main thing for me is not to push them faster than they want to go . that causes them to lose interest . as for getting their own telescope , i got mine when i was eight . my neighbor down the street just got one for their eight year old son as well . that is only two data points so you can not really draw many conclusions . however , that seems to be the age when they are responsible enough to take care of it on their own . since you already have a 10" , it may not be as critical as they will probably grow up learning to use yours .
first , check this reference on wikipedia . now , it is generally true that the " speed " ( or , more accurately , the dispersion relation ) of any particle is affected by a medium , where it travels . well , of course , if the particle interacts with the medium . for neutrinos the " slowing down " itself is absolutely negligible even in very dense media . what is important is that interaction of the electron neutrino with ordinary matter is much stronger , so it affects the patterns of neutrino oscillations -- the effect is known as msw effect . finally , this is not particularly related to supernovae . the idea behind interest in supernovae is that during an explosion there are a lot of neutrinos so one must account for the " neutrino matter " and its effect on oscillations as well .
it can not be from the moisture in the air . if there was enough moisture in the air to produce condensation then it would be condensing on everything . there would actually be less of it condensing on the tailpipe , because the tailpipe is quite warm . in fact the water is generated by the combustion of the fuel in the car . it comes from the hydrogen in the fuel , plus some of the oxygen from the air . for example , the combustion of octane is $$ \mathrm{2c_8h_{18}+25o_2 \to 16co_2 + 18h_2o + \text{heat}} . $$ this is just the net result of an extremely complex series of reactions , and motor fuel is not just octane , but ultimately burning fuel in a car will produce carbon dioxide and water in roughly equal amounts , plus much smaller amounts of a whole bunch of other things . usually the $\mathrm{h_2o}$ will be in the form of water vapour , but if it is cold then this will condense , and this is the liquid water you see coming out of the tailpipe .
there exists a huge gap in the strength of the four forces that we have observed in nature between gravity and the other three : in the following image we see that the radiation decouples from the " soup " at energy densities of 0.25ev . that is the snapshot of cmb , cosmic microwave background radiation . cbr in this plot is the cmb , when we have a snapshot of what happened before then . the gap you are talking about is covered by known physics up to 10^-11 seconds . the unification of the three stronger forces takes us to the 10^-35 seconds of your question . a unified theory of the three forces with gravity will fill the in between times up to 10^-43 seconds , a huge gap due to the smallness of the gravitational constant . speculations are listed in the image , and from what i have read the axions are gaining points after the discovery of the imprint of gravitational waves on the cmb . a lot of observational and theoretical research is still to be done before any standard model for the universe can be proposed with some confidence .
yes , there is investigation . some random names on the field ( more on the physics side , no specific order ) : carl dettmann , tamás tél , ott , ying-cheng lai , adilson motter , celso grebogi , holger kantz , alessandro moura , eduardo g . altmann , etc , etc , etc . a quick search on some of these names should help you to find some recent papers on what is being done ( not restricted to ! ) . some specific topics on the subject with some activity : billiards , transient chaos , hamiltonian systems , quantum chaos , control theory . do not be biased by this information , use it has a shortcut to search more and more . it is not meant to represent anything in terms of importance , quantity or quality of the research .
i am not quite sure what you mean about " keeping the entropy below maximum " , but most basic application of entropy is to determine , which processes are reversible . as for reversible processes , you actually can add or remove heat in a way that the entropy of the whole universe remains the same . this is possible when you add the heat to the system by isothermal process ( constant temperature ) . in fact , in isothermal process the change in entropy of the closed system will be opposite to the change of entropy to the rest of the universe . all other types of heat exchange increase the entropy of the whole universe . that is the point of the carnot process , in which you have two adiabatic processes ( no exchange of heat ) and two isothermal processes ( constant temperature ) . carnot process is fully reversible , thus it does not change the entropy of the universe . if this is not exactly the answer you are looking for , please be more specific .
see e.g. page 3 of http://arxiv.org/abs/0707.3400 it is nonsensical to attribute this simple particular insight to a " discoverer" ; all these considerations should be associated with ludwig boltzmann who knew the answer even though the information in physics was considered continuous at that time . one may easily derive the result . for example , put one molecule of an ideal gas in a vessel , learn in which half of the vessel the molecule is ( one bit of information ) , and put a barrier in the middle . you will be able to allow the molecule to do the work and expand from $v/2$ to the original volume $v$ . the molecule will do the work $$ w = \int_{v/2}^v p\ , dv = \int_{v/2}^v \frac{kt}{v}dv = kt \ln \frac{v}{v/2} = kt\ln 2 $$ where i used $pv = nkt$ for $n=1$ molecule of an ideal gas . more generally , you do not have to consider ideal gas . just recall how work is related to the free energy , $e-ts$ . to reduce the entropy of a subsystem by one bit , i.e. by $k\ln 2$ ( look at boltzmann 's tomb formula to know why it is this value ) , we have to do change $e-ts$ by $kt\ln 2$ .
the first issue i see : your speed is in miles/hour , and your time is in seconds .
since hamiltonian vector fields generate symplectomorphisms in $\mathbb{r}^{2n}$ ( with the canonical symplectic form ) , one can pick any hamiltonian , solve the hamilton equations of motion to get a symplectomorphism . since , a nonlinear symplectomorphism is seeked , free particle and harmonic oscillator hamiltonians will not be good examples , as they give linear dependence on the initial conditions . but the hydrogen atom hamiltonian is a good example ( for $\mathbb{r}^{12}$ ( two bodies in three dimensions subject to an inverse square attractive cental force ) ) . the explicitely known solutions of its equations of motion are nonlinear symplectomorphisms .
electrons in the $n$-type conduction band diffuse across the boundary into the empty conduction band of the $p$-type semiconductor . once there they recombine with holes in the $p$-type valence band . so the net motion of electrons is from the $n$-type conduction band to the $p$-type valence band . this means that near the boundary there are no electrons in the conduction band and no holes in the valence band i.e. the material becomes insulating . the transfer of electrons causes the $p$ side to become negatively charged and the $n$ side positively charged . as electrons move , the charge separation creates an electric field that opposes the electron diffusion .
for incoherent light , yes , though you would do well to measure the transmission spectrum to protect yourself from nasty surprises . for a laser , you probably need professional filters with optics-quality surface roughness .
a quantum mechanical model in which the magnetic translation operators are observables is a charged particle moving on a two dimensional torus in the background of a uniform magnetic field perpendicular to the torus surface . please , see for example the following article by e . onofri . the hamiltonian is the magnetic schrodinger operator and the ground state is the lowest landau level . the full solution shows that the degeneracy of the lowest landau level is equal to the magnetic flux through the torus surface area . therefore , the magnetic flux must be quantized . this is the dirac quantization condition . ( there are many other ways to prove this result without the need of the full solution because dirac quantization condition is a particular case of the index theorem ) . a basis of wave functions of the lowest landau level can be taken as the jacobi theta functions , $\theta_{\nu} ( z , \tau ) $ where $z=x+iy$ is the complex coordinate on the torus , $\tau$ is proportional to the ratio between the torus generators and $\nu$ takes integer values between $1$ and the magnetic charge $n$ . the main difference of the landau problem on the torus case from the plane is that the infinitesimal magnetic translation operators $\mathbf{p}-e\mathbf{a}$ are not observables because their action on the wave functions lies outside the lowest landau level . however , finite translations $e^{ ( \mathbf{p}-e\mathbf{a} ) . \mathbf{r}}$ are well defined if $e^{i|r|}$ is an $n$-th root of unity . this particular setting , however , cannot be easily implemented in lab at all , because it would require a net magnetic charge inside the torus and free magnetic charges have not been produced until now . however , this model can be translated into momentum space . here the torus is a brillouin zone of a $2d$ rectangular lattice . the restricted dynamics in one band can be described by an effective theory in which a berry connection term is added to the hamiltonian . thus , this problem becomes analogous to the motion on the torus , but in momentum space . here , in contrast to the real space , berry 's connections have nontrivial ( fictitious ) magnetic charges . physical observables in the real space have analogous observables in momentum space . the hall conductance is proportional to the magnetic charge of the berry curvature , thus the dirac quantization condition is responsible for the quantization of the hall conductance .
what you are describing is the particle in a box system , and for a 3d box the energy levels of this system are given by : $$ e_{ijk} = \frac{\hbar^2\pi^2}{2ml^2} ( n_i^2 + n_j^2 + n_k^2 ) \tag{1} $$ where $l$ is the size of the box . the numbers $n$ label the energy levels with $ ( 1 , 1 , 1 ) $ being the lowest level and larger values of $n_i$ , $n_j$ and $n_k$ giving higher energy levels . the interesting thing about this system is that even in the lowest energy state , $ ( 1 , 1 , 1 ) $ , the energy is not zero . this is the phenomenon known as zero point energy , and a quick look at equation ( 1 ) shows that the magnitude of the zero point energy is inversely proportional to the size of the box squared : $$ e_{111} = \frac{3\hbar^2\pi^2}{2m}\frac{1}{l^2} $$ so to you shrink the size of the box you have to put energy in by doing work on it . an electron is ( as far as we know ) a point particle and has no volume so to perfectly confine the electron would mean taking $l$ down to zero , and that would mean putting in an infinite amount of energy i.e. it can not be done . the uncertainty principle comes in because the uncertainty in the electron 's momentum scales roughly as the energy , so as $l \rightarrow 0$ the momentum uncertainty $\delta p \rightarrow \infty$ . in fact even in principle we can not get $l$ smaller than about the planck length , because at that point the energy density is so high that the system would form a black hole .
i originally had something about the constellations changing in the sky to show that the earth orbits the sun , but that would still be the case if the sun orbited the earth instead . now that i think about it , there is one thing that conclusively proves that the earth orbits the sun : parallax . over the course of one year many of the stars will move relative to each other . at the end of the year they will be back where they started . this is because the earth moves around in a 2au diameter circle , so that six months from your first observation , you will be standing 2au away from where you were then , and are viewing the stars from a ( slightly , but observably ) different angle . to show that the moon orbits the earth you could observe its location at the same time every night , and see that it moves , and is always nearly the same distance from earth . it never goes into a retrograde motion . assuming the earth is spherical , the only way this could be true is if the moon orbits the earth . you might also take the phases of the moon into account and model the sun-earth-moon system to explain it .
if the star and the shell are spherically symmetric , you can use the fact that the field of a symmetric distribution at any radius is the same as the field created by a mass point at the center that has the same mass as is closer to the center than the point of interest . in particular , when you are between the star and the shell you can ignore the shell-it makes no field ( though it does change the potential . )
you have to first derive the time-dependent current $i ( t ) $ which runs through the wire . the capacitor is described by the relation $$q ( t ) = c v ( t ) \qquad ( 1 ) $$ with $c= \epsilon \pi l^2/d$ and $\epsilon$ the dielectric constant of the medium between the two plates . the wire is described by $$v ( t ) = r i ( t ) . \qquad ( 2 ) $$ charge conservation yields $$ \dot q ( t ) = i ( t ) . \qquad ( 3 ) $$ taking the time derivative of ( 1 ) and plugging in ( 3 ) and then ( 2 ) , we obtain $$ rc \ , \dot{i} ( t ) = i ( t ) $$ with the solution $$ i ( t ) = i_0 e^{-t/rc} , \qquad i_0 = \frac{q}{cr} = \frac{qd}{\epsilon\pi l^2 r} . $$ the electric field $e ( t ) $ in the capacitor is constant throughout the capacitor ( in the limit of $l/d \gg 1$ ) and points along the axis of the capacitor . is is given by $e ( t ) = v ( t ) /d =r i ( t ) /d$ . similarly , the magnetic field $b ( t ) $ is always perpendicular to the electric field and curls around the axis of the capacitor . its size is given by $b ( t ) = \mu_0 i ( t ) /2\pi r$ with $r$ the ( radial ) distance to the axis . together , the magnitude of the poynting vector is given by $$|p ( t ) | = e ( t ) b ( t ) /\mu_0 = \frac{r i ( t ) ^2}{2\pi r d} . $$
yes , cavitation creates a " vacuum " . but the liquid will start evaporating the moment the void is created , so it is not a vacuum in the strict sense of " nothing there " . when a cavitation bubble collapses ( which usually happens very shortly after it forms ) , the temperature inside can rise very quickly - due to adiabatic compression of the little gas that has diffused into the bubble in the short time that it existed . the collapse can be so violent that the back side of the bubble , much like a shaped charge , creates a " jet " that can impinge the object that initially created the bubble ( image from http://authors.library.caltech.edu/25017/4/chap3.htm ) . this is roughly the mechanism that is responsible for cavitation damage on say the propellers of a ship : ( image from http://upload.wikimedia.org/wikipedia/commons/e/e6/cavitation_propeller_damage.jpg ) but cavitation is cause by " tearing " at the liquid - you are literally pulling it apart . when a liquid is boiling , you are creating bubbles by forcing the ( water ) vapor out of the liquid phase into the solid phase . definitely not a vacuum there . of course it is only a matter of degree . how low of a pressure do you need before you consider something a " vacuum " ?
in general , the energy is a quantity conserved due to the translational invariance in time or , equivalently , the generator of these translations in time . so depending on what we mean by time , we also have different meanings of the word energy . in the usual treatment of the spacetime , the energy $e=p^0$ is the generator of translations in the usual time $t=x^0$ . in the spacetime light cone gauge , the energy is often meant to be $e= p^-$ and generate translations in $x^+$ , the light-cone-gauge counterpart of " time " that defines the slices in such a gauge . a single string has mass going like $\sqrt{n}$ so this is also its energy $p^0$ in the rest frame where $\vec p=0$ . however , in the light cone gauge , $$p^- = \frac{ ( p^i ) ^2+m^2}{2p^+} $$ note that this is just the usual $p_\mu p^\mu = m^2$ solved for $p^-$ in the light cone gauge , so that the $m^2$ in the numerator goes like $n$ ( the total excitation of the string ) rather than $\sqrt{n}$ . also , the energy may mean the world sheet energy , the generator of translations in the world sheet temporal coordinate $\tau$ . then , for an excited string , the energy goes like $h\sim l_0+\tilde l_0\sim n$ much like in the light cone gauge . however , there are different coefficients in these two interpretations of the energy and one has to treat the nonzero modes and ghosts differently . i am confident that all the standard textbooks make it clear from the context which " energy " they are talking about . concerning the second problem , what was clearly meant was the " long/classical string " and its energy $p^0$ in the rest frame of the whole string . such a long string has the contribution to its rest mass $\delta m^2 = ( t l ) ^2$ where $t$ is the string tension ( the linear energy density ) and $l$ is the length . there are also contributions to $m^2$ going like $n/ \alpha'$ etc . from the string excitations . the world sheet energy or the light-cone-gauge energy have contributions from long wound strings that go like $l^2$ ( because they are linear functions of $m^2$ , not $m$ ) . when you wrote $p^0= ( p^++p^- ) /\sqrt{2}$ , you missed the usual $1/\sqrt{2}$ normalization but this relationship holds in general ( definition of the light cone gauge ) . yes , the energy that was scaling linearly in the length ( and tension ) was $p^0$ . however , you do not gain anything by rewriting $p^0$ in terms of the light cone $p^\pm$ because the energy that is proportional to the length of the string has to be measured in the rest frame of the long string in the spacetime where $\vec p=0$ which also implies $p^+=p^-$ . when you impose this equality between $p^+$ and $p^-$ and the assumption about the right squared mass of the long string , you will also derive $p^0\sim tl$ . but there is no point in setting $p^+=p^-$ etc . because the formula for $p^-$ in the light cone gauge always explicitly contains $1/p^+$ and the formula for $p^-$ may always be seen – once you multiply it by $p^+$ and subtract $ ( p^i ) ^2$ from the result – to be equivalent for the formula for $m^2$ . so in the light cone gauge , to derive the " linear density of the string " , you must still know that $m^2$ goes like $ ( tl ) ^2$ , so you effectively assume ( or precalculate ) what you want to derive , anyway , $m_0\sim tl$ .
the explanation is using an energy argument . that for the normal case of a submerged piece of wood , you can assume that if the wood and a parcel of water above it switch places , then the water ( which is heavier/more massive ) drops in the gravitational field releasing potential energy . this release is not offset by the rising wood since it is not as massive . this energy imbalance goes into the kinetic energy to move the log ( and the water ) . this argument says that in the case of the rotating log , none of the water can move downward to release potential energy . since the water stays in the same place , no energy is released , and there is no energy to move the log . why does not the object just stay where it is ? because forces will combine to move it to a position with less energy . you can do a free-body diagram of a ball rolling downhill to see that the net force is down the hill . but you can also simply say that the ball is going to move in a way that lowers its potential energy ( which is downward in the gravitational field ) . using that argument for the cylinder says that there is no position it is free to move to that lowers the potential energy , so no movement will happen .
it is very important to distinguish whether the symmetry is broken explicitly or spontaneously . i think that the sentence " now when i break this symmetry spontaneously ( or explicitly ) " indicates that its author is not quite distinguishing these things . an explicit symmetry breaking generally lifts the degeneracy because the different parts of the multiplets no longer have the same energy . however , spontaneous symmetry breaking increases the degeneracy , particularly of the ground state . it is really how spontaneous symmmetry is defined . it is a fate of the symmetry that remains the symmetry of the laws of physics , but in practice , the environment we encounter , starting from the ground state , is no longer invariant under the symmetry . its not being invariant means nothing else than the fact that if we act with a generator $g$ of the continuous symmetry on the ground state , we get $$ g|0\rangle \neq 0 $$ we would get zero if the symmetry were not spontaneously broken . if it is broken , we get a nonzero vector that is independent from the original $|0\rangle$ , so we get another " copy " of the ground state . here , $g$ still commutes with the hamiltonian $h$ so these copies have the same energy – we have degeneracy . for example , if the electroweak symmetry were global , just to simplify things , the ground state of the higgs field could have vev $ ( 0,246 ) $ in the units of gev , but it could have any other vev with the same magnitude , too . so there are infinitely many vacua . ( in gauge theory , they are made equivalent , but if we spontaneously break a global symmetry , they are states related by the symmetry and therefore having the same energy , but distinct elements of the hilbert space . ) so the claim that the op is dissatisfied with really holds completely generally , by definition of the spontaneous symmetry breaking ! misunderstanding that the ground state of a spontaneously broken symmetric theory is degenerate is the misunderstanding of the basic idea of the spontaneous symmetry breaking . note that spontaneous symmetry breaking still effectively means that the " symmetry is broken for most practical purposes " because the " copies " mentioned above may be imagined to be physically identified and we split the hilbert space into " superselection sectors " each of which is built upon one copy of the ground state . the action of the symmetry generator on any excited state gives us a vector from another superselection sector which can not be identified with zero so from the perspective of a single superselection sector , the symmetry is simply broken . none of these questions and discussions about explicit vs spontaneous symmetry breaking has anything to do with the time-reversal symmetry ( or the absence of it ) which is just another symmetry ( one given by an antiunitary transformation , so some of the comments above would not apply to this symmetry ) . both systems with and without time-reversal symmetry satisfy the claim that the ground state is degenerate if a symmetry is spontaneously broken .
the short answer is no ! more about imagination how space of our universe looks ( and how expanding ) today and in past check here : wmap
a mixed state is mathematically represented by a bounded , positive trace-class operator with unit trace $\rho : \cal h \to \cal h$ . here $\cal h$ denotes the complex hilbert space of the system ( it may be nonseparable ) . the set of mixed states $s ( \cal h ) $ is a convex body in the complex linear space of trace class operators $b_1 ( \cal h ) $ which is a two-side $*$-ideal of the $c^*$-algebra of bounded operators $b ( \cal h ) $ . convex means that if $\rho_1 , \rho_2 \in s ( \cal h ) $ then a convex combination of them , i.e. $p\rho_1 + q\rho_2$ if $p , q\in [ 0,1 ] $ with $p+q=1$ , satisfies $p\rho_1 + q\rho_2 \in s ( \cal h ) $ . two-side $*$-ideal means that linear combinations of elements of $b_1 ( \cal h ) $ belong to that space ( the set is a subspace ) , the adjoint of an element of $b_1 ( \cal h ) $ stays in that space as well and $ab , ba \in b_1 ( \cal h ) $ if $a\in b_1 ( \cal h ) $ and $b \in b ( \cal h ) $ . i stress that , instead , the subset of states $s ( \cal h ) \subset b_1 ( \cal h ) $ is not a vector space since only convex combinations are allowed therein . the extremal elements of $s ( \cal h ) $ , namely the elements which cannot be decomposed as a nontrivial convex combinations of other elements , are all of the pure states . they are of the form $|\psi \rangle \langle \psi|$ for some unit vector of $\cal h$ . ( notice that , since phases are physically irrelevant the operators $|\psi \rangle \langle \psi|$ biunivocally determine the pure states , i.e. $|\psi\rangle$ up to a phase . ) the space $b_1 ( \cal h ) $ and thus the set $s ( \cal h ) $ admits at least three relevant normed topologies induced by corresponding norms . one is the standard operator norm $||t||= \sup_{||x||=1}||tx||$ and the remaining ones are : $$||t||_1 = || \sqrt{t^*t} ||\qquad \mbox{the trace norm}$$ $$||t||_2 = \sqrt{||t^*t||} \qquad \mbox{the hilbert-schmidt norm}\: . $$ it is possible to prove that : $$||t|| \leq ||t||_2 \leq ||t||_1 \quad \mbox{if $t\in b_1 ( \cal h ) $ . }$$ moreover , it turns out that $b_1 ( \cal h ) $ is a banach space with respect to $||\cdot||_1$ ( it is not closed with respect the other two topologies , in particular , the closure with respect to $||\cdot||$ coincides to the ideal of compact operators $b_\infty ( \cal h ) $ ) . as $s ( \cal h ) $ is closed with respect to $||\cdot ||_1$ , it is a complete metric space with respect to the distance $d_1 ( \rho , \rho' ) := ||\rho-\rho'||_1$ . when $dim ( \cal h ) $ is finite the three topologies coincide ( though the norms do not ) , as a general result on finite dimensional banach spaces . concerning your last question , there are many viewpoints . my opinion is that a density matrix is physical exactly as pure states are . it is disputable whether or not a mixed state encompasses a sort of physical ignorance , since there is no way to distinguish between " classical probability " and " quantum probability " in a quantum mixture as soon as the mixture is created . see my question classical and quantum probabilities in density matrices and , in particular luboš motl 's answer . see also my answer to why is the application of probability in qm fundamentally different from application of probability in other areas ? addendum . in finite dimension , barring the trivial case $dim ( {\cal h} ) =2$ where the structure of the space of the states is pictured by the poincaré-bloch ball as a manifold with boundary , $s ( \cal h ) $ has a structure which generalizes that of a manifold with boundary . a stratified space . roughly speaking , it is not a manifold but is the union of ( riemannian ) manifolds with different dimension ( depending on the range of the operators ) and the intersections are not smooth . when the dimension of $\cal h$ is infinite , one should deal with the notion of infinite dimensional manifold and things become much more complicated .
aeroplanes fly by thrusting air downwards and by thus being borne up by the newton 's-third-law begotten upwards reaction force of the downthrusted air on the aeroplane . there are many excellent answers to the physics se question " what really allows airplanes to fly ? " that you should read . but basically the simplest estimates arise from calculating the ram pressure thrust upwards on the aeroplane given the above principle . the variables you need to know are density of the air at the height , the relative speed of the aeroplane to the air , the angle of attack that the wing makes with the velocity vector of the air relative to a frame comoving with the aeroplane and the scale factor that yields the effective surface area of the wing - which at subsonic speeds is considerably larger than the wing itself because the disturbance to the fluid flow pattern that arises from the wing is felt over a region that is considerably bigger than the wing . the last variable - effective area - can also be expressed as the wing 's coefficient of lift . to illustrate these points , we can do a back of the envelope estimation of ram pressure in this case : see my drawing below of a simple aerofoil with significant angle of attack being held stationary in a wind tunnel . this is the kind of analysis you should do to get an idea of your specific situation . your air density is going to be rather less than that for the following calculation ( commercial jetliners reach their top speed at heights of about 8000m ) : lets suppose the airflow is deflected through some angle $\theta$ radians to model an aeroplane 's attitude ( not altitude ! ) on its last approach to landing or as it takes off , flying at $300\mathrm{km\ , h^{-1}}$ airspeed or roughly $80\mathrm{m\ , s^{-1}}$ . i have drawn it with a steep angle of attack . air near sea-level atmospheric pressure has a density of about $1.25\mathrm{kg\ , m^{-3}}$ ( molar volume of $0.0224\mathrm{m^{-3}} ) $ . the change in momentum diagram is shown , whence the change in vertical and horizontal momentum components are ( assuming the speed of flow stays roughly constant ) : $$\delta p_v = p_b \sin\theta ; \quad\quad\delta p_h = p_b \ , ( 1-\cos\theta ) $$ at the same time , the deflecting wing presents an effective blocking area to the fluid of $\alpha\ , a\ , \sin\theta$ where $a$ is the wing 's actual area and $\alpha$ the scale factor to account for the fact that in the steady state not only fluid right next to the wing is distrubed so that the wing 's effective area will be bigger than its actual area . therefore , the mass of air deflected each second is $\rho\ , \alpha\ , a\ , v\ , \sin\theta$ and the lift $l$ and drag $d$ ( which force the engines must afford on takeoff ) must be : $$l = \rho\ , \alpha\ , a\ , v^2\ , ( \sin\theta ) ^2 ; \quad\quad d = \rho\ , \alpha\ , a\ , v^2\ , ( 1-\cos\theta ) \ , \sin\theta$$ if we plug in an angle of attack of 30 degrees , assume $\alpha = 1$ and use $a = 1000\mathrm{m^3}$ ( roughly the figure for an airbus a380 wing area ) , we get a lifting force $l$ for $\rho = 1.25\mathrm{kg\ , m^{-3}}$ and $v = 80\mathrm{m\ , s^{-1}}$ of 200 tonne weight . this is rather less than the takeoff weight of a fully laden a380 airbus ( which is 592 tonnes , according to the a380 wikipedia page ) but it is an astonishingly high weight just the same and within the right order of magnitude . we see that the wing 's effective vertical cross section is bigger than the actual wing by a factor of 2 to 3 . this is not surprising at steady state , well below speed of sound flow : the fluid bunches up and the disturbance is much bigger than just around the wing 's neighbourhood . so , plugging in an $\alpha = 3$ ( given the experimental fact that the a380 can lift off at 592 tonnes gross laden weight ) , we get a drag $d$ of 54 tonne weight ( 538kn ) - about half of the airbus 's full thrust of 1.2mn , so this ties in well with the airbus 's actual specifications , given there must be a comfortable margin to lift the aeroplane out of difficulty when needed .
i would like to know why two forces ( of different magnitudes but same direction ) that we had use sequentially on an object to move it by the same amount twice , will produce two different values for the work quantity . [ . . . ] why is there a mismatch between the maths and the observed physical change ? in $w=fd$ , the distance $d$ should not be interpreted as the motion caused by the force . in general , force does not cause motion , it causes acceleration . the physical change that is described by work is the change in the object 's energy , not the change in its position . this explains the apparent mismatch . in your example , the weaker force could be a zero force . the observed physical change in the object is zero , because the object 's energy does not change . the object continues in the same state of motion . if possible , please avoid circular definitions like " work is the energy produced by a force " and " energy is the ability to do work " see the answers to this question : intuitively understanding work and energy
the standard model yukawa interactions must be $su ( 3 ) \times su ( 2 ) \times u ( 1 ) _y$ gauge invariant . the down-type yukawa interaction is $$ \mathcal{l} \supset -y_d \bar q \phi d_r + \text{h . c . } . $$ this is indeed gauge invariant . the $\bar q d_r$ form a colour singlet ( $3^* \times 3$ ) , the $\bar q \phi$ form an $su ( 2 ) $ singlet ( $2^*\times2 ) $ , and the whole thing is neutral under $u ( 1 ) _y$ , because the quantum numbers are $-\frac13$ , $1$ and $-\frac23$ , for $\bar q$ , $\phi$ and $d_r$ , which sum to zero . let 's try writing a similar up-type yukawa $$ \mathcal{l} \supset^ ? _ ? -y_u \bar q \phi u_r + \text{h . c . } $$ is it gauge invariant ? no - it breaks $u ( 1 ) _y$ , because the quantum numbers are $-\frac13$ , $1$ and $\frac43$ , for $\bar q$ , $\phi$ and $d_r$ , such that $y=2$ . to fix this problem , we might try $$ \mathcal{l} \supset^ ? _ ? -y_u \bar q \phi^* u_r + \text{h . c . } $$ this is $u ( 1 ) $ invariant because the hypercharge of $\phi^*$ is $-1$ , so we have $-\frac13-1+\frac43=0$ , but now it is no longer $su ( 2 ) $ invariant ( $2^*\times2^*$ ) . now we use the property that $i\tau_2\phi^*$ trasforms in the same way under $su ( 2 ) $ as $\phi$ finally , we can write $$ \mathcal{l} \supset -y_u \bar q i\tau_2 \phi^* u_r + \text{h . c . } $$ which is indeed fully gauge invariant .
the two resistors are in parallel . this means that at $a$ the current splits between them relating to their reistance . so the current throw the top resistor is $3\ , a$ and throw the bottom resistor is $1 \ , a$ . if we use kirchhoff 's current law which states , that in any node ( like $a$ ) the current flowing into the node is equal to the current flowing out of it . you have $1+3=4\ , a$ flowing out of node $a$ and thus must have $4\ , a$ flowing into the node .
let the minkowski metric $\eta_{\mu\nu}$ in $d+1$ space-time dimensions be $$\tag{1}\eta_{\mu\nu}~=~{\rm diag} ( 1 , -1 , \ldots , -1 ) . $$ let the lie group of lorentz transformations be denoted as $o ( 1 , d ; \mathbb{r} ) =o ( d , 1 ; \mathbb{r} ) $ . a lorentz matrix $\lambda$ satisfies ( in matrix notation ) $$\tag{2} \lambda^t \eta \lambda~=~ \eta . $$ here the superscript "$t$" denotes matrix transposition . note that the eq . ( 2 ) does not depend on whether we use east-coast or west-coast convention for the metric $\eta_{\mu\nu}$ . let us decompose a lorentz matrix $\lambda$ into 4 blocks $$\tag{3} \lambda ~=~ \left [ \begin{array}{cc}a and b^t \cr c and r \end{array} \right ] , $$ where $a=\lambda^0{}_0$ is a real number ; $b$ and $c$ are real $d\times 1$ column vectors ; and $r$ is a real $d\times d$ matrix . now define the set of orthochronous lorentz transformations as $$\tag{4} o^{+} ( 1 , d ; \mathbb{r} ) ~:=~\{\lambda\in o ( 1 , d ; \mathbb{r} ) | \lambda^0{}_0 &gt ; 0 \} . $$ the proof that this is a subgroup can be deduced from the following string of exercises . exercise 1: prove that $$\tag{5} |c|^2~:= ~c^t c~ = ~a^2 -1 . $$ exercise 2: deduce that $$\tag{6} |a|~\geq~ 1 . $$ exercise 3: use eq . ( 2 ) to prove that $$\tag{7} \lambda \eta^{-1} \lambda^t~=~ \eta^{-1} . $$ exercise 4: prove that $$\tag{8} |b|^2~:= ~b^t b~ = ~a^2 -1 . $$ next let us consider a product $$\tag{9} \lambda_3~:=~\lambda_1\lambda_2$$ of two lorentz matrices $\lambda_1$ and $\lambda_2$ . exercise 5: show that $$\tag{10} b_1\cdot c_2~:=~b_1^t c_2~=~a_3-a_1a_2 . $$ exercise 6: prove the double inequality $$\tag{11} -\sqrt{a_1^2-1}\sqrt{a_2^2-1} ~\leq~ a_3-a_1a_2~\leq~ \sqrt{a_1^2-1}\sqrt{a_2^2-1} , $$ which may compactly be written as $| a_3-a_1a_2|~\leq~\sqrt{a_1^2-1}\sqrt{a_2^2-1}$ . exercise 7: deduce from the double inequality ( 11 ) that $$\tag{12} a_1\neq 0 ~\text{and}~ a_2\neq 0~\text{have same signs} \quad\rightarrow\quad a_3&gt ; 0 . $$ $$\tag{13} a_1 \neq 0~\text{and}~ a_2\neq 0~\text{have opposite signs} \quad\rightarrow\quad a_3&lt ; 0 . $$ exercise 8: use eq . ( 12 ) to prove that $o^{+} ( 1 , d ; \mathbb{r} ) $ is stabile/closed under the multiplication map . exercise 9: use eq . ( 13 ) to prove that $o^{+} ( 1 , d ; \mathbb{r} ) $ is stabile/closed under the inversion map . the exercises 1-9 show that the set $o^{+} ( 1 , d ; \mathbb{r} ) $ of orthochronous lorentz transformations form a subgroup . $^1$ $^1$a mathematician would probably say that eqs . ( 12 ) and ( 13 ) show that the map $$o ( 1 , d ; \mathbb{r} ) \quad \stackrel{\phi}{\longrightarrow}\quad \{\pm 1\}~\cong~\mathbb{z}_2$$ given by $$\phi ( \lambda ) ~:=~{\rm sgn} ( \lambda^0{}_0 ) $$ is a group homomorphism between the lorentz group $o ( 1 , d ; \mathbb{r} ) $ and the cyclic group $\mathbb{z}_2$ , and a kernel $$ {\rm ker} ( \phi ) ~:=~\phi^{-1} ( 1 ) ~=~o^{+} ( 1 , d ; \mathbb{r} ) $$ is always a normal subgroup .
neither of the interaction terms would appear were the atom not present , and you cannot simply set $\hat{p}=0$ as it is a dynamically fluctuating quantum variable . therefore , both of the terms must describe scattering of the light from the atom . roughly speaking , the term $\sim \hat{p}\hat{a}$ encodes inelastic scattering , while the term $\sim \hat{a}^2$ encodes elastic scattering . in first order perturbation theory , transitions due to the first term are controlled by the matrix elements $$ \mathcal{m}_1 = \langle m , e_f| \hat{p}\hat{a} |n , e_i\rangle = \langle m|\hat{a}|n\rangle\langle e_f|\hat{p}|e_i\rangle . $$ here , $n , m$ are the initial and final number of photons in the light field ( assume for simplicity there is only one mode , obviously in a real calculation you will also have to consider different wavevectors and polarisation states of the field ) , while $e_{i , f}$ are the initial and final atomic eigenstates . since the operator $\hat{a}$ is linear in annihilation/creation operators , the matrix element is zero unless $m = n\pm 1$ . meanwhile , since atomic eigenstates have definite parity , the matrix element is zero unless $|e_f\rangle \neq |e_i\rangle$ ( or more precisely , it is zero unless the two states have opposite parity ) . so to first order , the interaction vertex $\hat{p}\hat{a}$ describes processes in which a photon is absorbed or emitted by the atom , changing its internal state . at higher orders in perturbation theory you will see processes where the atom 's internal state changes multiple times . this includes , for example , an inelastic scattering process where the atom absorbs a photon at one frequency and an electron becomes excited , then the electron drops in energy by emitting a photon at another frequency . ( you could also have elastic scattering processes where the atom ends up in the same state , hence the " roughly speaking " disclaimer above . ) matrix elements of the second interaction vertex look like $$\mathcal{m}_2 = \langle m , e_f|\hat{a}^2 |n , e_i\rangle = \langle m| \hat{a}^2 |n\rangle \langle e_f| e_i\rangle . $$ therefore , these matrix elements vanish unless the initial and final state of the atom is the same . however , the vertex is now quadratic in annihilation/creation operators , so you will see , for example , elastic scattering processes where a photon is absorbed and then re-emitted at the same frequency ( but different direction , in general ) . at higher orders you will start to see really interesting optical non-linearities where , for example , two photons are absorbed , and then two photons are re-emitted at different frequencies . hopefully this explains your idea about " perturbing the hamiltonian of the free em field " . the strong interaction between light and electrons can produce an effective interaction in the presence of matter ( aka optical non-linearity ) between otherwise non-interacting photons . finally , do not forget that the full interaction operator contains both contributions so at higher orders in perturbation theory you will also get cross terms between $\hat{p}\hat{a}$ and $\hat{a}^2$ .
the origin of blue and orange lines ( in your wiki image ) represent north and south poles of earth . indeed , earth could be imagined to be a magnetized spherical shell . those are the magnetic field lines ( a concept introduced by faraday ) . field lines are introduced as an aid of visualizing electric and magnetic fields . they are commonly referred to as " lines of force " . they have several characteristics associated with them - like beginning at north pole and ending at south pole ; do not intersect , etc . you could draw infinite number of magnetic field lines within a given region of space . in magnetism , these curved lines simply imply that a compass placed in any of these lines would deflect tangentially to the curves indicating magnetic poles of earth . these lines could be drawn using a bar magnet itself . . ! sadly , i must also add that your question is possibly related to magnetic field lines already asked here . . .
john rennie 's answer is good , but i will try to explain intuitively why the symmetry breaking leaves some symmetry unbroken . start with a sphere . you can rotate a sphere in three independent ways—around the x axis , around the y axis , and around the z axis , if you like . all of these are symmetries of the sphere , i.e. , they leave the sphere unchanged . these rotations are called $su ( 2 ) $ [ almost—there is a technicality that i will ignore ] , and saying that an $su ( 2 ) $ gauge theory has three gauge bosons ( which it does ) is the same as saying that a sphere can be rotated in three independent ways . draw a dot somewhere on the sphere . all three of the above rotations will probably move the dot , which means they are not symmetries of the sphere plus dot . but there is still a rotational direction that leaves the dot in place , and there was never any reason to prefer those other axes that do not . so forget about those axes and instead pick the axis of symmetry ( an axis going through the dot and its antipodal point ) , and two other axes perpendicular to that one and each other . the axis of symmetry is the " photon " , and i suppose you could call the other two the w + and w − . this analogy is inaccurate in that the real electroweak gauge group is $su ( 2 ) \times u ( 1 ) $ , not $su ( 2 ) $ , and has four bosons , not three . but a sphere is much easier to visualize , and the basic principle is the same . before the symmetry is broken ( by drawing the dot ) , you can not be sure that any particular boson ( rotational axis ) will remain a symmetry ( remain massless ) , but there will always be some combination of those bosons ( some other axis ) that remains a symmetry , and we call that the photon .
is there any physical significance to this matrix the physical ( geometric ) relevance to the matrix $$\left| \begin{matrix} \vec{i} and \vec{j} and \vec{k} \\ a_i and a_j and a_k \\ b_i and b_j and b_k \end{matrix}\right|$$ with regard to the cross product $\vec{a} \times \vec{b}$ is 1: that the three vectors $\vec{i}$ , $\vec{j}$ , and $\vec{k}$ constitute a vector basis that spans a space which is either also spanned by $\vec{a}$ , $\vec{b}$ , and one additional vector which is perpendicular to $\vec{a}$ as well as $\vec{b}$ ; or , in case that vectors $\vec{a}$ and $\vec{b}$ are parallel to each other , also spanned by $\vec{a}$ and two additional ( non-parallel ) vectors . 2: the three basis vectors $\vec{i}$ , $\vec{j}$ , and $\vec{k}$ are pairwise orthogonal ( perpendicular ) to each other . therefore vectors $\vec{a}$ and $\vec{b}$ as well as the cross product vector $\vec{a} \times \vec{b}$ can be completely and uniquely expressed in terms of the corresponding components : $\vec{a} := a_i \vec{i} + a_j \vec{j} + a_k \vec{k}$ , $\vec{b} := b_i \vec{i} + b_j \vec{j} + b_k \vec{k}$ , and $\vec{a} \times \vec{b} := \{ab\}_i \vec{i} + \{ab\}_j \vec{j} + \{ab\}_k \vec{k}$ . finally : 3: the three basis vectors $\vec{i}$ , $\vec{j}$ , and $\vec{k}$ have equal magnitudes : $| \vec{i} | = | \vec{j} | = | \vec{k} |$ . as a consequence , the " mathematical trick " of expressing the cross product vector $\vec{a} \times \vec{b}$ as the above determinant " works": the component of cross product vector $\vec{a} \times \vec{b}$ " along/parallel to " vector $\vec{a}$ vanishes explicitly : $$\left ( a_i ( a_j b_k - a_k b_j ) \frac{ ( | \vec{i} | ) ^2}{| \vec{a} \times \vec{b} |} \right ) + \left ( a_j ( a_k b_i - a_i b_k ) \frac{ ( | \vec{j} | ) ^2}{| \vec{a} \times \vec{b} |} \right ) + \left ( a_k ( a_i b_j - a_j b_i ) \frac{ ( | \vec{k} | ) ^2}{| \vec{a} \times \vec{b} |} \right ) = $$ $$\frac{ ( | \vec{i} | ) ^2}{| \vec{a} \times \vec{b} |} \left ( a_i ( a_j b_k - a_k b_j ) + a_j ( a_k b_i - a_i b_k ) + a_k ( a_i b_j - a_j b_i ) \right ) = 0 , $$ and likewise the component of cross product vector $\vec{a} \times \vec{b}$ " along/parallel to " vector $\vec{b}$ vanishes explicitly ; i.e. cross product vector $\vec{a} \times \vec{b}$ is expressed explicitly orthogonal to both vectors $\vec{a}$ and $\vec{b}$ . and , no less important the magnitude of cross product vector $\vec{a} \times \vec{b}$ " comes out correctly " , i.e. such that $$ \big ( | \vec{a} \times \vec{b} | \big ) ^2 := $$ $$ \left ( ( a_j b_k - a_k b_j ) ^2 + ( a_k b_i - a_i b_k ) ^2 + ( a_i b_j - a_j b_i ) ^2 \right ) ~ ( | \vec{i} | ) ^4 = $$ $$ \left ( ( a_i^2 + a_j^2 + a_k^2 ) ~ ( b_i^2 + b_j^2 + b_k^2 ) \right ) ~ ( | \vec{i} | ) ^4 - \left ( ( a_i b_i + a_j b_j + a_k b_k ) ~ ( | \vec{i} | ) ^2 \right ) ^2 := $$ $$ \big ( | \vec{a} | \big ) ^2 \big ( | \vec{b} | \big ) ^2 - | \vec{a} | ~ | \vec{b} | ~ a_b ~ b_a , $$ where $b_a$ denotes the component of vector $\vec{b}$ " along/parallel to " vector $\vec{a}$ , and $a_b$ denotes the component of vector $\vec{a}$ " along/parallel to " vector $\vec{b}$ .
the operation is the tensor product , an operation producing many components ( products of every component from the left vector and every component from the right vector is remembered ) . in this tensor notation , the inner product ( which involves the summation of 3 products ) would have to be explicitly indicated by a dot , $a \cdot b $ . the whole expression ${\bf \hat r\hat r - 1}$ is supposed to be a matrix , i.e. object with two vector-like indices $m_{ij}$ . in terms of components , $$ m_{ij} = \hat r_{i} \hat r_{j} - \delta_{ij}$$ where the $\delta$ term is the kronecker delta – a representation of the unit matrix that is equal to $1$ for $i=j$ and $0$ otherwise . also , $\hat r_i \equiv r_i / r$ . if one looks what $m_{ij}$ means e.g. for $r= ( 0,0 , z ) $ for $z\gt 0$ , in the positive $z$-direction , he finds out that $m_{ij}={\rm diag} ( -1 , -1,0 ) $ . just to be sure , the indices $j , k$ in the equation 6 of the paper the op mentioned are not vector indices . they are labels identifying different objects/sources at different locations of space .
neglecting friction , the force experienced is the centrifugal force $f=\frac{mv^2}{r}$ ( it would be less if you included friction since the car actually slips ) vectorially added to the orthogonal gravitational force $f_g=mg$ , i.e. $f = m\sqrt{\left ( \frac{v^2}r\right ) ^2 + g^2}$ where $g = 9.81 \frac{m}{s^2}$ 1 . divide this by $f_g$ to obtain a result in gs . remember to use si-units , i.e. divide a km/h speed by 3.6 ( 1000 km/m / 3600 s/h ) to obtain m/s . for the 350 m curve and 285 km/h that yields about 2.08 gs only , to obtain the 5.5 gs mentioned a radius of about 118 m is required , or some higher velocity ( 490 km/h for the 350 m curve ) . 1 ) thanks j.h. for this important correction !
ok , i will start . definition and list ( filled with mistakes ) . please , comment/add/edit/etc . definition ${lim}_{x\to\infty} p ( x ) = p ( x+t ) , $ for $\forall\ ; t &gt ; 0$ $p ( x ) $ is finite i.e. $p ( x ) = \int p ( x ) dx = $ const . clarifying definitions $p ( x ) :=$ " probability density function of measurable $x \in \re_+$" $p ( x ) :=$ " cumulative distribution function " = $\int_{0}^x p ( x ) \ ; dx$ $a &gt ; 0$ $0 &lt ; f ( x ) &lt ; const $ $g ( x ) \ne 0$ long tailed all exponential distributions : $p = f ( x ) exp ( -ax ) $ all power law distributions : $p = x^{-\alpha}$ , where $\alpha &lt ; -1$ all " stretched exponentials": $p = f ( x ) exp ( -ax^{g ( x ) } ) $ non long tailed uniform distribution : p ( x ) = const all " increasing or equal " distributions : $|p ( x ) | \le |p ( x+a ) | $ for $\forall a$ power law distributions with $p ( x ) = x^\alpha$ , where $-1 \le \alpha \le 0$ ( integral not finite )
i believe that evaporative ionization is the opposite effect of what you are looking for , per the general discussion on ion processes in evaporated droplets . this constrains the range of charge-transfer in droplets to the triboelectric effect , as you mentioned . the kelvin generator , including discussion about charge formation that exceed models , may be more than triboelectric , due to ground effects - which is to say that charge may come from other sources that the system of droplet and atmosphere . the only way i could see a kelvin generator involve electrospray ionization is through corrosion or dissolving of the conductive cups that collect the water . the amsci group suggests that the charge arrives from slowing the water droplets ' fall . their model could align with the idea that ion-transfer from droplet to system occurs via coulomb fission .
yes , there are rigorous ways of defining locality in such contexts , but the precise terminology used unfortunately depends on both the context , and who is making the definition . let me give an example context and definition . example context/definition . for conceptual simplicity , let $\mathcal f$ denote a set of smooth , rapidly decaying functions $f:\mathbb r\to \mathbb r$ . a functional $\phi$ on $\mathcal f$ is a function $\phi:\mathcal f\to \mathbb r$ . a function ( not yet a functional on $\mathcal f$ ) $\phi:\mathcal f\to\mathcal f$ is called local provided there exists a positive integer $n$ , and a function $\bar\phi:\mathbb r^{n+1}\to\mathbb r$ for which \begin{align} \phi [ f ] ( x ) = \bar\phi\big ( x , f ( x ) , f' ( x ) , f'' ( x ) , \dots , f^{ ( n ) } ( x ) \big ) \tag{1} \end{align} for all $f\in \mathcal f$ and for all $x\in\mathbb r$ . in other words , such a function is local provided it depends only on $x$ , the value of the function $f$ at $x$ , and the value of any finite number of derivatives of $f$ at $x$ . a functional $\phi$ is called an integral functional provided there exists a function $\phi:\mathcal f\to\mathcal f$ such that \begin{align} \phi [ f ] = \int_{\mathbb r} dx \ , \phi [ f ] ( x ) . \tag{2} \end{align} an integral functional $\phi$ is called local provided there exists some local function $\phi:\mathcal f\to\mathcal f$ for which $ ( 2 ) $ holds . what could we have defined differently ? some authors might not allow for derivatives in the definition $ ( 1 ) $ , or might call something with derivatives semi-local . this makes intuitive sense because if you think of taylor expanding a function , say , in single-variable calculus , you get \begin{align} f ( x+a ) = f ( x ) + f' ( x ) a + f'' ( x ) \frac{a^2}{2} + \cdots , \end{align} and if you want $a$ to be large , namely if you want information about what the function is doing far from $x$ ( non-local behavior ) , then you need more an more derivative terms to sense that . the more derivatives you consider , the more you sense the " non-local " behavior of the function . one can also generalize to situations in which the functions involved are on manifolds , or are not smooth but perhaps only differentiable a finite number of times etc . , but these are just details and i do not think illuminate the concept . example 1 - a local functional . suppose that we define a function $\phi_0:\mathcal f\to \mathcal f$ as follows : \begin{align} \phi_0 [ f ] ( x ) = f ( x ) , \end{align} then $\phi_0$ is a local function $\mathcal f\to\mathcal f$ , and it yields a local integral functional $\phi_0$ given by \begin{align} \phi_0 [ f ] = \int_{\mathbb r} dx\ , \phi_0 [ f ] ( x ) = \int_{\mathbb r} dx\ , f ( x ) , \end{align} which simply integrates the function over the real line . example 2 - another local functional . consider the function $\phi_a:\mathcal f\to\mathcal f$ defined as follows : \begin{align} \phi_a [ f ] ( x ) = f ( x+a ) . \end{align} is this $\phi_a$ local ? well , for $a=0$ it certainly is since it agrees with $\phi_0$ . what about for $a\neq 0$ ? well for such a case $\phi_a$ certainly is not because $f ( x+a ) $ depends both on $f ( x ) $ and on an infinite number of derivatives of $f$ at $x$ . what about the functional $\phi_a$ obtained by integrating $\phi_a$ ? notice that \begin{align} \phi_a [ f ] and = \int_{\mathbb r} dx\ , \phi_a [ f ] ( x ) \\ and = \int_{\mathbb r} dx\ , f ( x+a ) \\ and = \int_{\mathbb r} dx\ , f ( x ) \\ and = \int_{\mathbb r} dx\ , \phi_0 [ f ] ( x ) \\ and = \phi_0 [ f ] . \end{align} so $\phi_a [ f ] $ is local even though $\phi_a$ is not for $a\neq 0$ . the lesson of this example is this : you may encounter an integral functional $\phi:\mathcal f\to\mathbb r$ that is defined by integrating over a non-local function $\phi:\mathcal f\to\mathcal f$ . however , there might still be a way of writing the functional $\phi$ as the integral over a different function , say $\phi'$ , that is local , in which case we can assert that $\phi$ is local as well because to verify that a functional is local , you just need to find one way of writing it as the integral of a local function .
it is the result of a dependence of the pressure with growing depth , due to the gravitational field ( i.e. . the weight of the water ) . you may do an easy calculation with some simple geometrical form , e.g. a cylinder totally submerged in water , to quickly understand how it works . the force due to pressure in each surface element of the curved wall of the cylinder is proportional to the depth of that element , and has the normal direction to the wall , i.e. towards the axis of the cylinder . after an easy integration in polar coordinates , you can see that the resultant force points upwards . that is because the forces in the upper parts are smaller that the ones near the more deeply submerged part of the cylinder . a surprising conclusion is that a golf ball submerged in a tank of water in the space station , would not go upwards . . . or that the bubbles in a coke in the hands of an astronaut remain where they origin . . . i would love to see that .
yes , applying an electric field does create a ph gradient and in fact you can observe this simply by adding a suitable indicator to your system . for example see the section demonstration of ph gradient formation in this article .
dear rajesh , the goal as well as main achievement of string theory is not to " achieve spacetime quantization " whatever this phrase is supposed to mean but to allow calculations about spacetime that are compatible with the postulates of quantum mechanics . it is not the same thing . the statement that " spacetime is quantized " , whatever it means , is just a working hypothesis , not a holy grail that should be " achieved " . instead , what is needed is to have a theory that is a quantum theory as a whole , i.e. one that agrees with the uncertainty principle , probabilistic character of predictions , observables ' being expressed by linear operators on the hilbert space , and so on . on the other hand , the spacetime is an approximate concept in string theory . at distances much longer than the characteristic distance scale of string theory , the string scale ( or the related gravitational planck scale ) , classical general relativity is a good approximation - and some of its aspects remain exact at all distance scales . at distances comparable to the string scale ( or planck scale ) , entirely new set of physical phenomena take over . it is not true that the only difference of these new phenomena from classical general relativity is that " spacetime is quantized " . and in fact , it is not true in any sense and it cannot be true in a consistent theory of quantum gravity that spacetime becomes discrete . and in fact , geometric quantities - while remaining continuous whenever it makes sense to use them - may be shown in string theory to be invalid variables to describe physics at very short distances . to say the least , they are incomplete . so if your question is how string theory confirms the prejudices and beliefs that the spacetime quantities survive as good variables up to arbitrarily short distance scales and/or that they become discrete , the answer is that string theory is a way to prove that both of these prejudices are dead wrong . if your question is which quantum phenomena affecting spacetime are predicted by string theory , it is a valid question but it is too broad and a proper answer would require to review all of string theory - because all important insights of string theory , in some sense , show the consequences of the co-existence of quantum mechanics with a dynamical spacetime .
just write the transformation in its differential form and then rearrange . $$ d\eta = \frac{u_e}{\sqrt{\int_0^x\rho_e u_e \mu_e dx}}\rho dy $$ rearranging and integrating yields : $$ y = \frac{\sqrt{\int_0^x\rho_e u_e \mu_e dx}}{u_e}\int_0^\eta \frac{1}{\rho}d\eta $$
a vibrating shoelace is a poor analogy for a sound wave , because that would be a transverse wave whereas sound is a longitudinal wave . what this means is your example of molecules hitting each other is perfect . a longitudinal wave is described by the variation of the density of the particles . the wikipedia page has some nice animations to help visualize what that means . the wavelength in this context is the distance between two periodic fluctuations in the density of the material . so in the case of the question you linked to , because the sound emitted has a massive wavelength ( and consequently an extremely low frequency ) that means the distance between the density fluctuations are large . so the particle literally moves and collides with another particle and so on , propagating the wave .
constant pull of gravity there is a " constant frictional force " as well . in fact , the magnet stays stuck due to friction , not directly magnetic force . the magnetic force will exert a normal force $n$ on the fridge ( and vice versa by action reaction ) . the fridge will exert an upwards static frictional force $\leq \mu_sn$ , where $\mu_s$ is the coefficient of static friction for the magnet-fridge interface . $\mu_s$ should be large enough so that $\mu_sn\geq mg$ , so the frictional force will be sufficient to keep the magnet steady . now let 's talk about it falling off . there are two ways this can happen : either $\mu_s$ reduces due to smoothing of the surface , or $n$ reduces due to demagnetization . demagnetization can slowly happen whenever the magnet undergoes some change , like rusting , being heated , or being jarred . it can also happen if the magnetic field in the area abruptly changes . in this case , the abrupt change is the one that happens in the earth 's magnetic field , but before that happens , the magnet will most probably have rusted or flaked off . i had a normal ( iron ) magnet which became extremely weak after ten-ish years . so most probably your fridge magnet will fall off before the earth gets swallowed up . i would give it a few decades , since it is a neodymium magnet and it is not really exposed to the air .
the molecules $o_2$ and $n_2$ are symmetric and have no dipole momentum . that is why they can not interact with emw ( at least within dipole approximation ) . one can say that transitions between the oscillator levels in these molecules are forbidden by symmetry in electrodipole approximation . the molecule $co$ consists of two different atoms . the average positions of positive and negative charges are not the same . this molecule is polar .
the ball is deformed while bouncing off . in theory , this can be modelled as an entirely elastic process as a relatively good approximation , however , it actually is not , as some energy is lost in the process and radiated away as heat ( try deforming a ball a few hundred times , it will heat up ) . the process is therefore not entirely elastic , which reduces the kinetic energy of the ball . additionally , a number of other forces affect the ball , listing those mentioned above again for completenes and ordered roughly by the magnitude of the effect : energy lost due to inelasticity of the ball-earth interaction ( ball heats up ) friction of the ball with the air , causing it to slow down friction of the ball with the ground ( "stuck to the ground" ) roughness of ground causing the ball to start spinning or change direction forces stemming from the fact that the earth rotates , although this should mostly affect horizontal velocity momentum transferred to earth
using the notation : $ h = h_0 + v$ and $ \mathbf{k} = \mathbf{k_0} + \mathbf{z}$ the requirement that both free and interacting generators satisfy the poincaré algebra commutaion relations , lead to the following requirements : ( please see the following book by eugene stefanovich ( published in the arxiv ) equations 6.22-6.26 ( page 179 ) : $ [ \mathbf{j} , v ] = [ \mathbf{h_0} , v ] = 0$ $ [ p_i , z_j ] = i \delta_{ij} v , [ j_i , z_j ] = i \epsilon_{ijk} z_k$ $ [ k_{0 [ i} , z_{j ] } ] + [ z_i , z_j ] =0 $ , $ [ \mathbf{z} , h_0 ] + [ \mathbf{k_0} , v ] + [ \mathbf{z} , v ] = 0$ ; now , to verify that these relations are satisfied in the present case , please observe that due to the first given realtion expressing the transformation properties of the interaction hamiltonian density that the action of the free poincaré generators on the hamiltonian density is by means of the well known differential operator realization : [ $h_0 , \mathcal{h} ( \mathbf{x} , 0 ) ] = ( \frac{\partial}{\partial t} \mathcal{h} ) ( \mathbf{x} , 0 ) $ $ [ p_i , \mathcal{h} ( \mathbf{x} , 0 ) ] = i \frac{\partial}{\partial x_i} \mathcal{h} ( \mathbf{x} , 0 ) $ $ [ j_i , \mathcal{h} ( \mathbf{x} , 0 ) ] = i\epsilon_{ijk} x_i \frac{\partial}{\partial x_j} \mathcal{h} ( \mathbf{x} , 0 ) $ $ [ k_{0i} , \mathcal{h} ( \mathbf{x} , 0 ) ] = ( ( t \frac{\partial}{\partial x_i} - x_i\frac{\partial}{\partial t} ) \mathcal{h} ) ( \mathbf{x} , 0 ) = -x_i ( \frac{\partial}{\partial t} \mathcal{h} ) ( \mathbf{x} , 0 ) $ what is left is to perform the substitutions . but in the addition of the given requirements , one must assume that the interaction hamiltonian density vanishes sufficiently rapidly at infinity , and surface terms in integration by parts can be ignored . ( of course these are operators and one must specify the strength of convergence ) . here is a sample calculation of one of the required commutation relations : $ [ j_i , z_j ] = \int x_j i \epsilon_{ilm} x_l \frac{\partial}{\partial x_m} ( \mathcal{h} ( \mathbf{x} , 0 ) ) d^3x$ please observe that the poincaré generators do not act on the free $x_i$ 's in the integrand , because they are only dummy integration variables . thus after integration by parts we get : $ [ j_i , z_j ] = -\int i \epsilon_{ilm} ( \delta_{jm} x_l + \delta_{lm} x_j ) \mathcal{h} ( \mathbf{x} , 0 ) d^3x = + i \epsilon_{ijl} x_l \mathcal{h} ( \mathbf{x} , 0 ) d^3x = i \epsilon_{ijk} z_k$ finally , please let me remark that finding an exact representation of the interacting poincaré algebra would require knowing the exact solution ( hilbert space and operator eigenvalues ) of the interacting quantum field model , which is not known outside perturbation theory , however , the interacting poincaré generators can be deduced form the lagrangian by means of noether 's theorem + canonical quantization . the forms of interacting poincare algebra were already studied by dirac in 1949 .
a knot always requires the rope in the knot to be curved . this increases the stress on the outside of the curved bit of rope , and decreases the stress in the inside . this increase in the stress in a knot means the rope breaks at a lower overall stress than a straight rope would .
the bottom line of my entire question is whether the quantum field theory of an electron is a direct consequence of the fact that the particle producing the field is a quantum particle ( and not a classical one ) or does it involve much more than that ? it involves " much more than that": if i understand correctly , you are taking the classical expression for , say , the coulomb field resulting from a source charge , or a magnetic field resulting from a current element and then saying that , since the position/momentum of the sources are quantized , they become operators and in this way the field becomes an operator since it is a function of those positions/momenta . in qed , it is possible to describe a freely propagating field quantum ( e . g . a photon ) . freely propagating means that once it is been produced , its existence is now independent of any source . i do not see how this is possible in the scheme where you just quantize the source . any time dependence of the source would always be immediately transferred to the electromagnetic field . in qed , you quantize the electromagnetic field and the electron/positron fields independently . neither is in any sense more fundamental than the other . one can act as a source for the other only after you have introduced an interaction term in the theory . so the source/field relationship is not the basis of the quantization . also one of the key features of quantum field theory which distinguishes it from quantum mechanics is that it offers a mechanism to create and destroy particles . this would not be possible with a prescription such as the one you describe . even your electron description is still a single particle one .
well the real question should be why is there a °c ( celsius ) . the celsius scale is a " centigrade " scale in that it uniformly divides the temperature range between the boiling point of water , and the freezing point of water into 100 equal parts , and then it arbitrarily calls the freezing point zero °c , and the boiling point becomes 100°c . the kelvin scale is referenced to the triple point of water , not the freezing point , and that temperature is about 0.1°c ( it might be 0.098°c but i am not sure about that ) . quite arbitrarily , it was decided that degrees on the kelvin scale , should be identical in size to celsius degrees , and experimentally the zero on the kelvin scale ( zero kelvins ) is 273.16 celsius degrees below the triple point of water , which makes it also -273.15°c
first let us see the impact of gravity on plants : gravitropism ( also known as geotropism ) is a turning or growth movement by a plant in response to gravity . roots show positive gravitropism and stems show negative gravitropism . that is , roots grow in the direction of gravitational pull ( i.e. . , downward ) and stems grow in the opposite direction ( i.e. . , upwards ) . gravity is sensed in the root tip and this information is relayed to the elongation zone so as to maintain growth direction and mount an effective growth responses to changes in orientation and continue to grow its roots in the same direction as gravity . roots bend in response to gravity due to a regulated movement of the plant hormone auxin known as polar auxin transport . the differential sensitivity to auxin helps explaining why stems and roots respond in the opposite way to the gravity vector . in both roots and stems auxin accumulates towards the gravity vector on the lower side . in roots , this results in the inhibition of cell expansion on the lower side and the concomitant curvature of the roots towards gravity ( positive gravitropism ) . in stems , the auxin also accumulates on the lower side , however in this tissue it increases cell expansion on the upper side and results in the shoot curving up . now lets see what will possibly happen if gravity changes . more upward gravitational force : roots will tend to be longer and stem shorter . more downward gravitational force : roots will tend to be shorter and stem longer . this can make tall trees unstable . this is contrary to what our common sense might tell . on the hind sight , i am just beginning to wonder if earth has just the right gravity for plants to grow .
well at the most fundamental level , the index of refraction of a material is defined as $ n = \sqrt {\epsilon \mu} $ where $\epsilon $ is the electric permittivity and $\mu$ is the magnetic permeability of the material . this arises from the solution of maxwell 's equations in a medium . also arising from the wave equation , which can be derived from maxwell 's equations , is that the index of refraction is the speed of light in vacuum , $c$ divided by the speed of light in the material $c_m$ . $ n ={ c \over c_m} $ worth noting is that since $c_m$ is always less than $c$ , the index of refraction is always greater than 1 . now the phase velocity for an electromagnetic wave of angular frequency $\omega$ is given by $v_p = {\omega \over k}$ where $k= {2\pi \over\lambda_m}$ is the magnitude of the wave vector and $\lambda_m$ is the wavelength in the medium . so after a little algebra , we find that the wave vector and index of refraction are related by $k ={ n\omega \over c}$ where does all this come into play in refraction and snell 's law ? well , it is the wave vector that comes into play in satisfying the boundary conditions on the electric ( and magnetic ) fields at the interface between two media . to see this , let 's look at the simple case of a plane wave of monochromatic light incident on the interface between two media with indices of refraction $n_1$ and $n_2$: in this simple case , considering the boundary conditions on the electric field at the interface is sufficient to derive snell 's law . the boundary condition is given by equation ( 1 ) in the diagram , namely that the incident and reflected electric fields minus the transmitted electric field must be zero , or equivalently that the total electric field at the interface must be continuous . since we defined y=0 as the plane of the interface , this boundary condition must hold for any value of x . this leads after a little algebra to equation ( 2 ) in the diagram . this equation depends only on the angles of incidence $\phi_{inc}$ and refraction $\phi_{tr}$ , the wave vector magnitudes in both media $k_1$ and $k_2$ , and the transmission and reflection coefficients $t$ and $r$ ( the fraction of energy that is transmitted into the new medium and reflected into the old medium respectively ) . by symmetry , $\phi_{inc} = \phi_{refl}$ . because the left side of equation 2 is independent of angle , so must the right side be . this leads to the term in the exponential being zero , which leads directly to snell 's law , using the relation between wave vector and index of refraction shown above . the group velocity never comes into play in the boundary conditions of refraction . it does come into play in the propagation of energy in the media ( as opposed to the fields ) , but that is another question .
the $\frac{e^2}{4\pi\epsilon_0}\frac{1}{r_0}$ term appears in the potential for the electron motion , as luboš and vijay point out , to keep the whole energy accounting in place so that the nuclear motion can be properly quantized . the key point is that this potential does not involve the electron coordinates , so that as far as the electronic wavefunction is concerned it acts like a constant and therefore does not affect the solution to the electronic eigenvalue equation . if you do include that term , and write the potential energy of the molecule as $$e_p=-\frac{e^2}{4\pi\epsilon_0}\left ( \frac{1}{r_1}+\frac{1}{r_2}-\frac{1}{r_0}\right ) $$ then you can write the potential for the nuclear coordinates directly as $$e_n=\langle\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) |e_p|\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) \rangle$$ where the total wavefunction is split into electronic and nuclear parts as $$\langle \mathbf{r} , \mathbf{r}_1 , \mathbf{r}_2|\psi\rangle=\langle\mathbf{r}|\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) \rangle\langle\mathbf{r}_1 , \mathbf{r}_2|\phi\rangle . $$ the schrödinger equation for the nuclear coordinates is then $$\left ( \frac{\mathbf{p}_1^2}{2m}+\frac{\mathbf{p}_2^2}{2m}+e_n\right ) |\phi\rangle=e|\phi\rangle . $$
remember that the theta term appears in an exponential $e^{i\theta n}$ inside the path integral . if $\theta n$ shifts by $2\pi n$ , for any integer $n$ , the exponential is unchanged , and all path integrals have the same value . the integral $n = \frac{1}{32\pi^2} \int f \wedge f$ is not arbitrary either . it is a topological invariant , and it is normalized so that it will be an integer . indeed , on flat $\mathbb{r}^4$ with your normalization , it is equal to $2 \nu$ , where $\nu \in \mathbb{z}$ is the instanton number . ( you can find the argument in weinberg , vol ii , p 450-2 . ) but for now let 's just take it as granted that that $n$ is always integer-valued . it follows then that shifting $\theta \to \theta + 2\pi m$ for any integer $m$ sends $e^{i\theta n}$ to $e^{i\theta n + i 2\pi m n} = e^{i\theta n}$ . which means that only the value of $\theta$ mod $2\pi$ affects physics . so when someone says that $\theta$ is small , they mean that $\theta$ mod $2\pi$ is small , as you guessed .
you need to be a bit ( pardon the pun ) more strict about the size of the ( hilbert ) space you are playing with . a qu_ bit can be in a superposition of two ( pure ) states , but not more . for this reason , " real values 2 , 3 and 4 in superposition " does not make sense . to draw an analogy to the binary system you mentioned , it is as if you are trying to stuff large numbers into a bit . this restriction appears more clearly in the visual representation of a pure state , the bloch sphere . secondly , you need to be careful when drawing analogies between bits and qubits . for example , your statement " all bits are completely isolated from one another and have no qu-knowledge of any other " is wrong in the general case when there may be entanglement between qubits . i started learning quantum computing with some cs knowledge , and this was a very helpful reference . i think it'll get you started on the right path .
why do we gauge-fix the path integral in the first place ? if we were doing lattice gauge theory , we did not need to gauge-fix . but in the continuum case , ( the hessian of ) the action for a generalized$^1$ gauge theory has zero-directions that lead to infinite factors when performing the path integral over gauge orbits . in a brst formulation ( such as , e.g. , the batalin-vilkovisky formulation ) of a generalized gauge theory , the gauge-fixing conditions can in principle depend on gauge fields , matter fields , ghost fields , anti-ghost fields , lagrange multipliers , etc . perturbatively , a necessary condition for a good gauge-fixing procedure is that the gauge-fixed hessian is non-degenerate ( in the extended field-configuration space ) . generically , the number of gauge-fixing conditions should match the number of gauge symmetries . for yang-mills theory with lie group $g$ , one needs ${\rm dim} ( g ) $ gauge-fixing conditions . one may check that for various standard gauges that only involve the gauge fields , it is not necessary to gauge-fix matter fields to achieve a non-degenerate hessian . -- $^1$ by the word generalized gauge theories , we mean gauge theories that are not necessarily of yang-mills type .
exactly how and to what extent diamond shows up in x-rays depends on factors such as type of x-ray apparatus , size of diamond , orientation and so on . carbon has an atomic mass of 12 . that is fairly low . diamond exhibits a bunch of unique properties such as extreme hardness , high thermal conductivity and chemical inertness . in terms of x-ray windows another property of diamond is of crucial importance : diamond consists of carbon i.e. diamond is a low z material which is transparent to x-rays . . . . high transmission coefficients even at low x-ray energies can be achieved by using thin diamond membranes mounted on circular silicon support disks . the thickness of these membranes can be as low as 1 µm from http://www.diamond-materials.com/en/products/cvd_for_xray/xray_windows.htm however you can use x-rays to make useful images of diamonds x-ray absorption is related to density , diamond is about 3.5 times as dense as water . 3,500 kg/m3 or 3.5 g/cm3 .
$e$ = $mc^2$ a much better expression is $e^2 = ( mc^2 ) ^2 + ( pc ) ^2$ , where $m$ is the " mass " ( also known as " intrinsic mass " , also known " rest mass " , but most physicists nowadays just use " mass" ) of the particle and $p$ is the particle 's momentum . this reduces to $e=mc^2$ in the special case of a particle with zero momentum , but it also reduces to $e=pc$ in the case of a particle such as a photon with zero mass . using $e=mc^2$ as a general expression implies a rather different concept of mass , that of relativistic mass . many physicists did indeed use the concept of relativistic mass early on in the development of relativity theory . at least initially , even einstein was in that camp . most of those physicists , einstein included , abandoned that concept for the concept of " rest mass . " there are just too many problems with the concept of relativistic mass . the concept of " rest mass " ( or " intrinsic mass " or just " mass" ) makes much more sense than does the concept of relativistic mass . note that the term " rest mass " is a bit contradictory for massless particles such as photons . there is no frame in which a photon is at rest . this apparent contradiction vanishes if you use the phrase " intrinsic mass " ( or just " mass" ) in lieu of " rest mass . " there are a few hangers-on amongst professional physicists who still prefer the concept of relativistic mass over rest mass . these physicists are now few and far between . eventually they will die , and the concept of relativistic mass will eventually die with them .
it is indeed the case that due to gravitational time dilation a person on top of a hill would age slightly faster than a person at sea level . you do not need to climb hills to measure this effect . gravitational time dilations due to height differences of a few feet have been measured in the laboratory . the effect is tiny though . in the neighborhood of a large spherically-symmetric massive object such as earth , and compared to being infinitely far away from the object , your aging slows down by a factor $$\sqrt{1\ -\ \frac{v_{esc}^2}{c^2}}$$ here $v_{esc}$ is the velocity needed at your particular position to escape from the gravitational pull of the object , and $c$ the speed of light . when comparing two positions near to each other and close to the object , we can derive a simple equation that describes the relative time dilation . for two positions close to earth with height differences much smaller than the radius of earth , the fractional time dilation is given by $g h / c^2$ , where $g$ denotes the local gravitational acceleration and $h$ the height difference . for a hill on earth that peaks 90 m ( 300 ft ) high , and using $g=10\ m/s^2$ and $c=3\ 10^8 m/s$ , we find $g h / c^2 \approx 10^{-14}$ . in other words , over a period of 3 years , a person on top of the hill would age one millionth of a second ( one microsecond ) more compared to a person at the foot of the hill .
what you did is a good proof , though as you are finding for it to be convincing you have to think carefully about what everything means . the really hard thing is $p=vi$ , which i am actually surprised is not a given formula . once you have that , as you showed , getting to $p=i^2 r$ is easy , so let 's focus on getting $p=vi$ . it is much cleaner to use calculus if you know how to use that : for example you can be able to use the definition of power $p=\frac{de}{dt}$ as well as a formula for the energy ( and also the definition of current ) . however if you do not know calculus , then you have to think through what the time interval $t$ and the energy $w$ are , as you are rightly worried about . ( however you are going above and beyond by thinking carefully through what these things mean , and that is great ! ) i would start with $t$ . it does not really matter what you take for $t$ , but you do have to be consistent about it . let 's say you want $t$ to be the time it takes the electron to make one complete circuit . ok , now we go through the other formulas . can we find an interpretation for all of them with this definition of $t$ ? first compute the energy gained/lost by one electron . that would be $e=qv$ , where $v$ is the voltage gained/lost by an electron in that period of time . actually . . . to be very precise we only want the energy lost by the electrons as they moved through the appliance , we do not want the energy gained by the electron as it went through the battery . with that caveat , a single electron loses $e v_0$ of energy , where $v_0$ is the voltage of the battery . then , how much energy does the device use in that time ? call it $w$ . then use conservation of energy . all the energy the device used came from electrons . so $w=nqv_0$ , where $n$ is the number of electrons that are in the wire . what was the power ? it was the energy that was used in the time $t$ . so it is $p=\frac{nq}{t}v_0$ . i will leave the rest to your imagination .
when we measure a red shift , what we are actually measuring is the absorption spectra of elements in the stars and dust clouds in the target galaxy . these absorption spectra have well known patterns of lines , and when red shifted the entire pattern moves to lower energy/longer wavelength . so you are correct that you can not measure the red shift from just one line , because you do not know where that line was originally , but you can do it when you measure a known pattern of many lines .
the objects such as $\hat \phi ( x , y , z , t ) $ in a qft are strictly speaking " operator distributions " . they differ from " ordinary operators " in the same way how distributions differ from functions . only if you integrate such operator distributions over some region with some weight $\rho$ , $$\int d^3 x\ , \hat\phi ( x , y , z , t ) \rho ( x , y , z , t ) =\hat o , $$ you obtain something that is a genuine " operator " . in a free qft , the state vectors may be built as combinations of states in the fock space – an infinite-dimensional harmonic oscillator . but you may also represent them via " wave functional " . much like the wave function in non-relativistic quantum mechanics $\psi ( x , y , z ) $ depends on 3 spatial coordinates , a wave functional depends on a whole function , $\psi [ \phi ( x , y , z ) ] $ . for each allowed configuration of $\phi ( x , y , z ) $ , there is a complex number . yes , one may also integrate over all classical functions $\phi ( x , y , z ) $ . there also exists a dirac delta-like object , the dirac " delta-functional " , and it is usually denoted $\delta$ , $$\int {\mathcal d}\phi ( x , y , z ) f [ \phi ( x , y , z ) ] \delta [ \phi ( x , y , z ) ] = f [ 0 ( x , y , z ) ] $$ i wrote the zero as a function of $x , y , z$ to stress that the argument of $f$ is still a function . the functional integration is a sort of infinite-dimensional integration and the delta-functional is an infinite-dimensional delta-function . one must be careful about these objects , especially if we integrate amplitudes that may have amplitudes and especially if we integrate over curved infinite-dimensional objects such as infinite-dimensional gauge groups etc . – there may be subtleties such as anomalies . yes , the hilbert space of a free qft is still isomorphic to the usual hilbert space : there is a countable basis . but we are talking about the finite-energy excitations only . there are lots of " highly excited states " that are not elements of the fock space – one would need infinite occupation numbers for all one-particle states . physically , such states are inaccessible because the energy can not be infinite . however , when one is changing the energy from one hamiltonian to another ( e . g . by simple operations such as adding the interaction hamiltonian ) , finite-energy states of the former $h_1$ may be infinite-energy states of the latter $h_2$ and vice versa . so one must be careful : the physically relevant finite-energy hilbert space may be obtained from some infinite-occupation-number states in a different , e.g. approximate , hamiltonian . it is still true that the relevant hilbert space is as large as a fock space and it has a countable basis . the " totally inaccessible " states that are too strong deformations have an important example or name – they are " different superselection sectors " . rigor is a strong word . people tried to define a qft rigorously – by aqft , the algebraic/axiomatic quantum field theory . these attempts have largely failed . it does not mean that there is not any " totally set of rules " that qft obeys . instead , it means that it is not helpful to be a nitpicker when it comes to the new issues that arise in qft relatively to more ordinary models of quantum mechanics ; it is neither fully appropriate to think that a qft is " exactly just like a simpler qm model " but it is equally inappropriate to forget that it is formally an object of the same kind . formally , many things proceed exactly in the same way and there are also new issues ( unexpected surprises that contradict a " formal treatment" ) that have some physical explanation and one should understand this explanation . some of these new subtleties are " ir " , connected with long distances , some of them are " uv " , connected with ever shorter distances . the fact that a qft has infinitely many degrees of freedom is both an ir and uv issue . so even if you put a qft into a box , you will not change the fact that you need wave functionals , delta-functionals , and that there are superselection sectors and states inaccessible from the fock space . by the box , you only regulate the ir subtleties but there are still the uv subtleties ( momenta , even in a box , may be arbitrarily large ) . those may be regulated by putting the qft on a lattice . this has some advantages but some limitations , too .
the electric field assigns a single vector quantity to each point in space ( specifically , the direction in which a positive test charge would accelerate if it popped into existence at that point , assuming it did not perturb the setup creating the field in the first place ) . i believe that the difficulty of this question arises from an ambiguity in the problem statement : to which point ( s ) do the two vectors you have drawn assign electric fields ? they cannot both correspond to the single point p_lr at which the drawings of the left and right arrows intersect ( the head of one touching the tail of the other ) because this would imply that a test charge dropped at p_lr would accelerate simultaneously in two different directions ! you could adopt the convention of summing the accelerations a la newton ii , since two forces f1 , f2 that would have caused accelerations a1 , a2 if applied separately actually cause acceleration a1+a2 if applied simultaneously . but this would force you to admit , by the definition of the electric field , that e ( p_lr ) =e_l+e_r which points along the wire and does not violate the rule that the drawing was intended to be a counterexample of . we could adopt a different convention for deciding which points the arrows assign electric fields to ( perhaps taking the point to be the tail or the tip of each arrow ) . but this arrangement does not imply anything about current building up or not building up in either place . let me propose a slightly more formal version of your question : what prohibits the inside of a wire from sustaining a solenoidal ( circulatory ) vector field ? solenoidal vector fields do not converge or diverge ( lead to charge buildup ) anywhere , but they are not necessarily equal to zero . an example would be the velocity field of a 2d whirlpool . the answer : nothing ! you can generate one of these fields by accelerating a magnet through a pipe . the electrons are dragged around the pipe in circles , which produces a magnetic field opposing the field of the falling magnet . if you use an everyday conductive material like copper , a constant acceleration force like gravity will inevitably " win " , since resistance takes away some of the electrons ' ability to build a perfectly opposing magnetic field . as they rotate around the pipe in circles , their potential decreases to 0 , which means that there must have been a circulatory electric field in the wire . however , if you use a superconducting material , circulatory currents are able to perfectly oppose impinging magnetic fields , leading to the meissner effect ( superconducting levitation ) . if you made a superconducting wire and placed a magnet nearby , the wire would have all sorts of circulatory currents on the inside that did not point along the length of the wire , but these would not necessarily be due to electric fields since superconductors do not require electric fields to sustain currents . another possible way of obtaining circulatory fields in the wire would be a high-frequency alternating current ( google " skin effect " if you are interested ) , although the elementary description of the physics is a bit more complicated in that case . i would hazard a guess that the context of this question implicitly forbade arrangements consisting of moving magnets and alternating currents . mathematically , the condition of " no changing magnetic fields " implies that there are no circulatory electric fields . by the helmholtz decomposition ( vector fields can be written as a sum of an irrotational and a solenoidal field ) , the earlier argument that the current ( and therefore e-field , in a non-superconductor ) must be divergence-free suffices to prove that the field is constant , and the fact that it must have no component normal to the wire surface then suffices to prove that the field must be constant and point along the direction of the wire . i do not know what mathematical background you have , so if the talk about helmholtz decompositions did not ring a bell then you will have to wait for vector calculus . summary : the confusion was due either to a misunderstanding of the definition of " vector field " or due to implicit assumptions ( no changing magnetic fields , no changing electric fields ) in the problem statement . hope this helped : )
taking your question literally , you can see a single barium ion : the triµp group has achieved capturing a single barium ion in a paul trap . the images show coulomb crystals formed by a decreasing number of laser-cooled ions as detected with an emccd camera . this forms an important step towards the planned experiments on single radium ions to measure atomic parity violation and build an ultra-stable optical clock . they are in traps like this one : also , warren nagourney from washingtong university took a picture of a single barium atom scattering light from a laser : single trapped atom , glowing blue photo credit : warren nagourney at the university of washington , c . 2000 what is this ? believe it or not , this is a color photograph of a single trapped barium ion held in a radio-frequency paul trap . resonant blue and red lasers enter from the left and are focused to the center of the trap , where the single ion is constrained to orbit a region of space about 1 millionth of a meter in size . what is the red/blue mess on the sides ? low level out-of-focus laser scatter off of metal trap electrodes and accessories ( atom ovens , electron filaments , etc . ) as seen in this photo . how do we know the dot really is an atom ? when one turns off the red laser , the blue dot vanishes . this is because the scattering process requires both laser colors due to a metastable state in the barium ion . if the blue dot stayed around with the red laser off , we might excuse it as being additional laser scatter off some surface . how was the photo taken ? this is a scanned photo ; the camera was a 35mm nikon ( i believe ) with a wide open 50mm f/1.8 lens . the exposure time was two minutes . several shots were taken at different camera positions and this one caught the ion in the very narrow depth of field . is this how you normally " view " the ion ? no , we use a 50 mm f/1.8 camera lens to image the blue dot onto a photomultiplier tube . we do not require the focus to be so good when using the pmt . where can i see more ? lots of ccd images of one and several trapped ions are found on the monroe group site . only two minutes exposure time , so probably in a dark enough room , someone with good sensitivity could actually see it .
not quite like in the photo above , which shows more than what the naked eye can see , but yes , absolutely ! our galaxy ( well , the chunk of it visible from these parts ) is a naked-eye object . the fact that your question even exists shows how much time is now spent by people under light-polluted skies . it will not be visible from the city , however . you need to drive an hour ( or two , if you live in a huge urban area ) to the country side , far from city lights . stay outside in full darkness for a few minutes , then look up . there will be a faint " river " of light crossing the sky . that is the milky way . full dark adaptation occurs after 30 minutes of not seeing any source of light , but this is not required for seeing our galaxy . while you are in a dark sky area , also look up the andromeda galaxy , a.k.a. m31 . http://www.physics.ucla.edu/~huffman/m31.html i mean , if you can see m31 with the naked eye , at 2 mil light-years away , then of course you can see milky way , which is basically in our backyard . here 's a light pollution map , not very recent , but still useful : http://www.jshine.net/astronomy/dark_sky/
i would not say the ignorance interpretation is a relic of the early days of statistical mechanics . it was first proposed by edwin jaynes in 1957 ( see http://bayes.wustl.edu/etj/node1.html, papers 9 and 10 , and also number 36 for a more detailed version of the argument ) and proved controversial up until fairly recently . ( jaynes argued that the ignorance interpretation was implicit in the work of gibbs , but gibbs himself never spelt it out . ) until recently , most authors preferred an interpretation in which ( for a classical system at least ) the probabilities in statistical mechanics represented the fraction of time the system spends in each state , rather than the probability of it being in a particular state at the present time . this old interpretation makes it impossible to reason about transient behaviour using statistical mechanics , and this is ultimately what makes switching to the ignorance interpretation useful . in response to your numbered points : ( i ) i will answer the " whose ignorance ? " part first . the answer to this is " an experimenter with access to macroscopic measuring instruments that can measure , for example , pressure and temperature , but cannot determine the precise microscopic state of the system . " if you knew precisely the underlying wavefunction of the system ( together with the complete wavefunction of all the particles in the heat bath if there is one , along with the hamiltonian for the combined system ) then there would be no need to use statistical mechanics at all , because you could simply integrate the schrödinger equation instead . the ignorance interpretation of statistical mechanics does not claim that nature changes her behaviour depending on our ignorance ; rather , it claims that statistical mechanics is a tool that is only useful in those cases where we have some ignorance about the underlying state or its time evolution . given this , it does not really make sense to ask whether the ignorance interpretation can be confirmed experimentally . ( ii ) i guess this depends on what you mean by " consistent with . " if two people have different knowledge about a system then there is no reason in principle that they should agree on their predictions about its future behaviour . however , i can see one way in which to approach this question . i do not know how to express it in terms of density matrices ( quantum mechanics is not really my thing ) , so let 's switch to a classical system . alice and bob both express their knowledge about the system as a probability density function over $x$ , the set of possible states of the system ( i.e. . the vector of positions and velocities of each particle ) at some particular time . now , if there is no value of $x$ for which both alice and bob assign a positive probability density then they can be said to be inconsistent , since every state that alice accepts the system might be in bob says it is not , and vice versa . if any such value of $x$ does exist then alice and bob can both be " correct " in their state of knowledge if the system turns out to be in that particular state . i will continue this idea below . ( iii ) again i do not really know how to convert this into the density matrix formalism , but in the classical version of statistical mechanics , a macroscopic ensemble assigns a probability ( or a probability density ) to every possible microscopic state , and this is what you use to determine how heavily represented a particular microstate is in a given ensemble . in the density matrix formalism the pure states are analogous to the microscopic states in the classical one . i guess you have to do something with projection operators to get the probability of a particular pure state out of a density matrix ( i did learn it once but it was too long ago ) , and i am sure the principles are similar in both formalisms . i agree that the measure you are looking for is $d_\textrm{kl} ( a||b ) = \sum_i p_a ( i ) \log \frac{p_a ( i ) }{p_b ( i ) }$ . ( i guess this is $\mathrm{tr} ( \rho_a ( \log \rho_a - \log \rho_b ) ) $ in the density matrix case , which looks like what you wrote apart from a change of sign . ) in the case where a is a pure state , this just gives $-\log p_b ( i ) $ , the negative logarithm of the probability that bob assigns to that particular pure state . in information theory terms , this can be interpreted as the " surprisal " of state $i$ , i.e. the amount of information that must be supplied to bob in order to convince him that state $i$ is indeed the correct one . if bob considers state $i$ to be unlikely then he will be very surprised to discover it is the correct one . if b assigns zero probability to state $i$ then the measure will diverge to infinity , meaning that bob would take an infinite amount of convincing in order to accept something that he was absolutely certain was false . if a is a mixed state , this will happen as long as a assigns a positive probability to any state to which b assigns zero probability . if a and b are the same then this measure will be 0 . therefore the measure $d_\textrm{kl} ( a||b ) $ can be seen as a measure of how " incompatible " two states of knowledge are . since the kl divergence is asymmetric i guess you also have to consider $d_\textrm{kl} ( b||a ) $ , which is something like the degree of implausibility of b from a 's perspective . i am aware that i have skipped over some things , as there was quite a lot to write and i do not have much time to do it . i will be happy to expand it if any of it is unclear . edit ( in reply to the edit at the end of the question ) : the answer to the question " when may ( or may not ) a microstate $\phi$ be regarded as a macrostate $\rho_0$ without affecting the predictability of the macroscopic observations ? " is " basically never . " i will address this is classical mechanics terms because it is easier for me to write in that language . macrostates are probability distributions over microstates , so the only time a macrostate can behave in the same way as a microstate is if the macrostate happens to be a fully peaked probability distribution ( with entropy 0 , assigning $p=1$ to one microstate and $p=0$ to the rest ) , and to remain that way throughout the time evolution . you write in a comment " if i have a definite penny on my desk with a definite temperature , how can it have several different pure states ? " but ( at least in jaynes ' version of the maxent interpretation of statistical mechanics ) , the temperature is not a property of the microstate but of the macrostate . it is the partial differential of the entropy with respect to the internal energy . essentially what you are doing is ( 1 ) finding the macrostate with the maximum ( information ) entropy compatible with the internal energy being equal to $u$ , then ( 2 ) finding the macrostate with the maximum entropy compatible with the internal energy being equal to $u+du$ , then ( 3 ) taking the difference and dividing by $du$ . when you are talking about microstates instead of macrostates the entropy is always 0 ( precisely because you have no ignorance ) and so it makes no sense to do this . now you might want to say something like " but if my penny does have a definite pure state that i happen to be ignorant of , then surely it would behave in exactly the same way if i did know that pure state . " this is true , but if you knew precisely the pure state then you would ( in principle ) no longer have any need to use temperature in your calculations , because you would ( in principle ) be able to calculate precisely the fluxes in and out of the penny , and hence you had be able to give exact answers to the questions that statistical mechanics can only answer statistically . of course , you would only be able to calculate the penny 's future behaviour over very short time scales , because the penny is in contact with your desk , whose precise quantum state you ( presumably ) do not know . you will therefore have to replace your pure-state-macrostate of the penny with a mixed one pretty rapidly . the fact that this happens is one reason why you can not in general simply replace the mixed state with a single " most representative " pure state and use the evolution of that pure state to predict the future evolution of the system . edit 2: the classical versus quantum cases . ( this edit is the result of a long conversation with arnold neumaier in chat , linked in the question . ) in most of the above i have been talking about the classical case , in which a microstate is something like a big vector containing the positions and velocities of every particle , and a macrostate is simply a probability distribution over a set of possible microstates . systems are conceived of as having a definite microstate , but the practicalities of macroscopic measurements mean that for all but the simplest systems we cannot know what it is , and hence we model it statistically . in this classical case , jaynes ' arguments are ( to my mind ) pretty much unassailable : if we lived in a classical world , we would have no practical way to know precisely the position and velocity of every particle in a system like a penny on a desk , and so we would need some kind of calculus to allow us to make predictions about the system 's behaviour in spite of our ignorance . when one examines what an optimal such calculus would look like , one arrives precisely at the mathematical framework of statistical mechanics ( boltzmann distributions and all the rest ) . by considering how one 's ignorance about a system can change over time one arrives at results that ( it seems to me at least ) would be impossible to state , let alone derive , in the traditional frequentist interpretation . the fluctuation theorem is an example of such a result . in a classical world there would be no reason in principle why we could not know the precise microstate of a penny ( along with that of anything it is in contact with ) . the only reasons for not knowing it are practical ones . if we could overcome such issues then we could predict the microstate 's time-evolution precisely . such predictions could be made without reference to concepts such as entropy and temperature . in jaynes ' view at least , these are purely macroscopic concepts and do not strictly have meaning on the microscopic level . the temperature of your penny is determined both by nature and by what you are able to measure about nature ( which depends on the equipment you have available ) . if you could measure the ( classical ) microstate in enough detail then you would be able to see which particles had the highest velocities and thus be able to extract work via a maxwell 's demon type of apparatus . effectively you would be partitioning the penny into two subsystems , one containing the high-energy particles and one containing the lower-energy ones ; these two systems would effectively have different temperatures . my feeling is that all of this should carry over on to the quantum level without difficulty , and indeed jaynes presented much of his work in terms of the density matrix rather than classical probability distributions . however there is a large and ( i think it is fair to say ) unresolved subtlety involved in the quantum case , which is the question of what really counts as a microstate for a quantum system . one possibility is to say that the microstate of a quantum system is a pure state . this has a certain amount of appeal : pure states evolve deterministically like classical microstates , and the density matrix can be derived by considering probability distributions over pure states . however the problem with this is distinguishability : some information is lost when going from a probability distribution over pure states to a density matrix . for example , there is no experimentally distinguishable difference between the mixed states $\frac{1}{2} ( \mid \uparrow \rangle \langle \uparrow \mid + \mid \downarrow \rangle \langle \downarrow \mid ) $ and $\frac{1}{2} ( \mid \leftarrow \rangle \langle \leftarrow \mid + \mid \rightarrow \rangle \langle \rightarrow \mid ) $ for a spin-$\frac{1}{2}$ system . if one considers the microstate of a quantum system to be a pure state then one is committed to saying there is a difference between these two states , it is just that it is impossible to measure . this is a philosophically difficult position to maintain , as it is open to being attacked with occam 's razor . however , this is not the only possibility . another possibility is to say that even pure quantum states represent our ignorance about some underlying , deeper level of physical reality . if one is willing to sacrifice locality then one can arrive at such a view by interpreting quantum states in terms of a non-local hidden variable theory . another possibility is to say that the probabilities one obtains from the density matrix do not represent our ignorance about any underlying microstate at all , but instead they represent our ignorance about the results of future measurements we might make on the system . i am not sure which of these possibilities i prefer . the point is just that on the philosophical level the ignorance interpretation is trickier in the quantum case than in the classical one . but in practical terms it makes very little difference - the results derived from the much clearer classical case can almost always be re-stated in terms of the density matrix with very little modification .
this is the distinction between continuous and discrete spectrum , but only considering the low energy excitations . for an hamiltonian with gapped spectrum , the lowest eigenvalue is separated by a gap from $e=0$ . for example dispersion relation of the form $e=|k|$ is an example of gapless spectrum , and $e=|k+m|$ is an example of gapped one , where $k$ is the wave vector ( which can be any real number ) and $m$ is the mass ( = gap ) . this distinction leads to qualitative difference in the physics - for example the difference between a material being a conductor or an insulator . many times also , the gap is generated by interesting physics ( like the mass gap in yang-mills theory , or the gap in bcs superconductivity ) .
there is a classic treatise on " relativity , thermodynamics and cosmology " from r . tolmann from the 1930s - it is still referenced in papers today . this generalises thermodynamics to special relativity and then general relativity . as a simple example the transformation law for temperature is stated as : $t=\sqrt ( 1-v^2/c^2 ) t_0$ when changing to a lorentz moving frame . another example is that " entropy density " $\phi$ is introduced , which is also subject to a lorentz transformation . finally this becomes a scalar with an associated " entropy 4-vector " in gr . the second law is expressed using these constructs by tolmann . there is some discussion in misner , thorne and wheeler too . of course both these texts also include lots of regular general relativity theory which you may not need .
dbrane , aside from " beauty " , the electroweak unification is actually needed for a finite theory of weak interactions . the need for all the fields found in the electroweak theory may be explained step by step , requiring the " tree unitarity " . this is explained e.g. in this book by jiří hořejší: http://www.amazon.com/dp/9810218575/ google books : http://books.google.com/books?id=mnnagd7otlicprintsec=frontcoverhl=cs#v=onepageqf=false the sketch of the algorithm is as follows : beta-decay changes the neutron to a proton , electron , and an antineutrino ; or a down-quark to an up-quark , electron , and an anti-neutrino . this requires a direct four-fermion interaction , originally sketched by fermi in the 1930s , and improved - including the right vector indices and gamma matrices - by gell-mann and feynman in the 1960s . however , this 4-fermion interaction is immediately in trouble . it is non-renormalizable . you may see the problem by noticing that the tree-level probability instantly exceeds 100% when the energies of the four interacting fermions go above hundreds of gev or so . the only way to fix it is to regulate the theory at higher energies , and the only consistent way to regulate a contact interaction is to explain it as an exchange of another particle . the only right particle that can be exchanged to match basic experimental tests is a vector boson . well , they could also exchange a massive scalar but that is not what nature chose for the weak interactions . so there has to be a massive gauge boson , the w boson . one finds out inconsistency in other processes , and has to include the z-bosons as well . one also has to add the partner quarks and leptons - to complete the doublets - otherwise there are problems with other processes ( probabilities of interactions , calculated at the tree level , exceed 100 percent ) . it goes on and on . at the end , one studies the scattering of two longitudinally polarized w-bosons at high energies , and again , it surpasses 100 percent . the only way to subtract the unwanted term is to add new diagrams where the w-bosons exchange a higgs boson . that is how one completes the standard model , including the higgs sector . of course , the final result is physically equivalent to one that assumes the " beautiful " electroweak gauge symmetry to start with . it is a matter of taste which approach is more fundamental and more logical . but it is certainly true that the form of the standard model is not justified just by aesthetic criteria ; it can be justified by the need for it to be consistent , too . by the way , 3 generations of quarks are needed for cp-violation - if this were needed . there is not much other explanation why there are 3 generations . however , the form of the generations is tightly constrained , too - by anomalies . for example , a standard model with quarks and no leptons , or vice versa , would also be inconsistent ( it would suffer from gauge anomalies ) .
with a downwardly $x$-axis , you have the equations of movement : $m_1 \ddot x_1 = m_1g - k_1x_1 + k_2x_2 \tag{1}$ $m_2 \ddot x_2 = m_2g - k_2x_2 \tag{2}$ equilibrium position means that $\ddot x_1 = \ddot x_2 = 0$ , so you have , naming $x_1$ and $x_2$ the equilibrium positions : $0 = m_1g - k_1x_1 + k_2x_2 \tag{3}$ $0 = m_2g - k_2x_2 \tag{4}$ the second equation gives $x_2 = \frac{m_2g}{k_2}$ , and replacing $x_2$ in the first equation gives $x_1 = \frac{ ( m_1 + m_2 ) g}{k_1}$ now , we are interested on oscillations around the equilibrium positions , so we make a change of variables : $ y_1 = x_1 - x_1 , \quad y_2 = x_2 - x_2\tag{5}$ now , we rewrite equations $ ( 1 ) $ and $ ( 2 ) $ using $y_1 , y_2$ : $m_1 \ddot y_1 = - k_1y_1 + k_2y_2 \tag{6}$ $m_2 \ddot y_2 = - k_2y_2 \tag{7}$ we see , that the equilibrium positions correspond to $y_1=y_2=0$ , as wished , and the terms in "$g$" ( gravity ) have disappeared . now , the equation $ ( 7 ) $ has the solution : $y_2 = y_2 ~\cos ( \omega_2 t + \phi_2 ) \tag{8}$ , with $ \omega_2 = \large \sqrt{\frac{k_2}{m_2}}$ knowing $y_2$ , we may search a solution of the equation $ ( 6 ) $ , as : $y_1 = y_1 ~\cos ( \omega_1 t + \phi_1 ) + z ~\cos ( \omega_2 t + \phi_2 ) \tag{9}$ one find : $\omega_1 = \large \sqrt{\frac{k_1}{m_1}}$ and $z = \large \frac{k_2 y_2}{k_1 - m_1 \omega_2^2}$ .
there are three processes to take into account : the warming of ice towards the melting point if it was originally below $0^{\circ} c$ . the melting of ice itself the warming of the resulting water the 1 . and 3 . part is addressed by heat capacity of ice and water respectively and the amount of heat will be directly proportional to temperature difference and weight of the water/ice . the proportionality constant ( actually it also depends on the temperature but not very strongly so let 's just ignore that ) is called specific heat . for water it is about twice as large as that of ice at temperatures around $0^{\circ} c$ . as for the 2 . part , this has to do with latent heat . simply put , this is an amount of heat you need to change phases without changing temperature . less simply put , when warming you are just converting the heat into greater wiggling of water molecules around their stable positions in the crystal thereby increasing their temperature . but at the melting point that heat will instead go into breaking chemical bonds between molecules in the ice lattice . now , latent heat is really big ( you need lots of energy to break those bonds ) . to get a hang on it : you would need the same amount of heat to warm water from $0^{\circ} c$ to $80^{\circ} c$ as you would need to melt the same amount of ice . now , presumably you want your drink cold in the end so that temperature for 3 . will be close to $0^{\circ} c$ and also the ice cubes should be pretty warm ( no use in producing ice cubes of e.g. $-50^{\circ} c$ , right ? ) . this means that these processes will not contribute much cooling . it is fair to say that melting of the ice takes care of everything . note : we can also quickly estimate how much ice you need by neglecting the processes 1 . and 3 . say you are starting with a warm drink of $25^{\circ} c$ and you want to get it to $5^{\circ} c$ . so , reusing the argument about the $80^{\circ} c$ difference being equivalent to a latent heat of the same mass , we see that you need four times less ice than water to get the job done .
there are numerous distance indicators used for within the galaxy . the most common way is by using intrinsic magnitude . by knowing how bright an object would be if we were close , we can determine how far away it is by how dim it is . there are many types of stars where we have a rough idea of how bright they should be due to characteristics of the star : cephied variables : the original type of variable star that was used by hubble to determine the distance to the andromeda galaxy . rr lyrae variable : like the cephied variable , but usually dimmer . type 1a supernova : these guys , unlike the first two , are cataclismic variables . essentially a binary white dwarf slowly accretes matter from its binary till it reaches the chandrashankar limit , after which point it explodes in a very characteristic way ( since the mass at the time of explosion is roughly constant ) . main sequence stars : generally less accurate than the first 3 , there are some types of main sequence stars which are used to find distances in a similar way . there are a few other ways we can measure distances : perpendicular movement : for example there is a " light echo " from sn 1987a which is essentially light from the supernova interacting with dust around the old star . since this echo should be expanding at the speed of light , we can tell how far away the nova is by the angular velocity of the light . relative velocity in a moving cluster : ( see dmckee 's answer ) tulley-fisher relation : a relationship between the luminosity of the galaxy and it is apparent width . can be used as a decent distance calculator . faber-jackson relation : similar to tulley-fisher , relates luminosity with radial velocity dispersion rate . edit : some more information about redshifts . the whole relationship between redshift and distance was in fact established by hubble by relating distance to cephied variables ( i believe ) with redshift . later on it was made more precise using supernova , which are brighter and can be seen from much father away ( i think recent supernova can be occasionally seen around z=2 , while cephieds are all z&lt ; 1 ) . within a galaxy , redshift cannot be used directs since the " peculiar velocity , " the velocity within the galaxy , completely overshadows the effects of universe expansion on which hubble 's law is based . redshift within the galaxy is useful for certain other techniques . edit : corrected a few minor errors .
for low current applications that are sensitive to voltage , like modern electronics , the resistance of the contacts can be significant . so when you reinsert the batteries you clean off any dirt , moisture or corrosion on the contacts , the resistance drops . so when a current flows the voltage drop accross the contacts is reduced and more of the battery voltage is available to the product . remember that the multiple ( 2or3 aa/aaa ) batteries are in series , so there are two contact for each battery and so 4 or 6 contact resistances ( of perhaps 0.1 or 0.2ohms ) in series . if your electronics needs 3v or 4.5v and takes 50-100ma then you could lose 0.5v !
if you want to generalize a potential to a class that is broader than the simple $\frac12 k_2 x^2$ , it is tempting as a first step to include a small perturbation of the form $\frac13k_3x^3$ . unfortunately , this drastically changes the structure of the potential , because it becomes unbounded from below . thus , you might get a slightly perturbed behaviour from a harmonic oscillator at low amplitudes , but if you drive it hard enough , then the system will cross some hill and then just run away . this behaviour is simply not what one is trying to model , which is the back-and-forth motion about a potential minimum . it is indeed possible ( or i do not see why it would not ) to do a proper analysis of this case in the limit of small enough oscillations that the system never sees the hill , while the harmonic oscillation is still perturbed . however , the existence of the hill and its other side makes the general solution very complicated and quite different to what you really want . for a real potential , of course , there will be a term in $x^4$ that kicks in way before the system sees the hill . thus , after the harmonic oscillator , the next class of models that really capture the type of back-and-forth behaviour one is trying to model , for all amplitudes and all driving forces , is of the form $$v ( x ) =\frac12 k_2 x^2+\frac13 k_3 x^3+\frac14 k_4 x^4 . $$ this succeeds in fixing the problems with a pure $x^3$ perturbation , but in solving those problems we have made the model quite a bit more complicated than we were really hoping for . ( for instance , we now have two dimensional constants , the lengths $k_2/k_3$ and $k_3/k_4$ , instead of just one , and the interplay between those two will affect the behaviour . for example , if $k_3^2&gt ; \frac92k_2k_4$ the potential develops a second dip , which you do not really want . ) thus , i think the useful classification is the duffing potential , $v ( x ) =\frac12 k_2 x^2+\frac14 k_4 x^4$ , is the simplest potential which generalizes the harmonic oscillator to the anharmonic case while avoiding runaway solutions for all starting positions . that said , if you start generalizing this model , your first step should be to put the cubic term back in . even then , the requirement that $k_3^2$ be bounded by $k_4$ means that the cubic term must be a perturbation on top of the quartic behaviour , instead of the other way around . there is an additional physical reason why odd-order perturbations to the potential are not very common , and it is due to the type of symmetry - or lack thereof - your system has . specifically , even-order terms in the potential are symmetric around the equilibrium point , but including odd-order terms will make for a lopsided potential that is not symmetric any more . for all oscillating systems , if you drive them hard enough they will become anharmonic . however , for odd-order nonlinearities to come into play , the system itself needs to be asymmetric , and that need not be the case . indeed , depending on the setting , it can be quite hard to make a system that is asymmetric enough for those effects to be noticeable . a good example of this is in nonlinear optics , where the relevant perturbation expansion is that of the polarization 's response to the driving electric field : $$p=\chi e+\chi^{ ( 2 ) }e^2+\chi^{ ( 3 ) }e^3+\cdots , $$ ignoring the vector/tensor character of these quantities . it generally requires pretty high intensities to drive materials into those regimes , so using $\chi^{ ( 2 ) }$ nonlinearities seems a lot better than using $\chi^{ ( 3 ) }$ ones . however , for most materials , the $\chi^{ ( 2 ) }$ susceptibility is zero because of symmetry considerations : the medium needs to be asymmetric on the scale of a few atoms . this is possible , for example , in non-centrosymmetric crystals , but without such a material you can not do any three-wave mixing process , which rules out second-harmonic generation , sum- and difference-frequency generation , optical parametric amplification , and a host of other toolkit essentials . so , the bottom line on this is : the symmetry of your system matters . you can only get odd-order perturbations to the potential if your system itself is asymmetric . even orders , on the other hand , you get for free by simply driving it hard enough . regarding the last part of your question , " mixing " does have a specific meaning in this context . if you have a harmonic driving that is also being driven harmonically , $$m\ddot x+m\gamma\dot x+m\omega_0^2 x=f\cos ( \omega t ) , $$ then the solution will also oscillate at the driving frequency $\omega$ , with possibly a phase delay which is not really important . however , when you include a cubic or quartic term perturbatively , the first step is to treat those terms as external forces with the position given by the unperturbed solution , say $x ( t ) =x_0\sin ( \omega t ) $ . this means that you have a harmonic system that is got an additional driving of the form $$k_3 x_0^3 \sin^3 ( \omega t ) \quad\text{or}\quad k_4 x_0^4 \sin^4 ( \omega t ) . $$ these are hard to deal with in that specific form , but they become quite a bit easier to handle if you use the appropriate trigonometric identities to reduce them to a finite sum of harmonic drivings . thus , the forces above reduce to $$ \begin{cases} k_3 x_0^3 \sin^3 ( \omega t ) = \frac{3\sin ( \omega t ) -\sin ( 3\omega t ) }{4}\\ k_4 x_0^4 \sin^4 ( \omega t ) =\frac{3-4\cos ( 2\omega t ) +\cos ( 4\omega t ) }{8} . \end{cases} $$ this is then a harmonic driving on a harmonic system , and that we can solve . ( it is only the first term in a perturbation series , but that is another story . ) the mixing you read about is the fact that the final solution will contain terms that oscillate at the original frequency , but it may also contain other harmonics . as you can see from the above , even harmonics lead to a dc perturbation plus a $2\omega$ contribution . there will also be additional terms at higher harmonics , but those are typically harder to detect . an odd-power perturbation , on the other hand , will result only in odd harmonics , including a component at the driving frequency ( "operating frequency band" ) . in terms of the time dependence of the solution $x ( t ) $ , this is probably not that important . however , there are large classes of systems which are easier to interact with on the frequency domain , and then these harmonics are the best way to physically understand the solutions . for example , if you are driving some slightly nonlinear electric oscillator , you are probably doing so at rf or microwave frequencies , and then it is very easy to study the output of the system via the fourier transform capacity of an oscilloscope . once you are there , detecting peaks at $2\omega$ or $3\omega$ can be relatively easy , and it speaks directly as to the type of nonlinearity that is present in the system .
the boiling point of liquid oxygen is 90k , so it is easily condensed by liquid nitrogen . i have personally made lox by pumping air through a glass u tube immersed in liquid nitrogen , so i can confirm it works . later : as discussed in the comments , what condenses is a mixture of liquid oxygen and nitrogen rather than pure liquid oxygen . the dew point for air is about 82k , far enough above the boiling point of liquid nitrogen for a condensate to form , and the condensate is about 50% liquid oxygen . this article includes the relevant phase diagram . from personal experience i can say the condensate is blue , though i did not test its magnetic properties or the violence of its reaction with organic materials .
if you have ever swum to the bottom of a swimming pool you will know that in water the pressure increases as you go deeper . at a depth of about 10 metres the pressure is twice what it is at the surface , but the water 10 metres down does not burst up to the surface because it is held down by the weight of water above it . in fact the increase of pressure with depth is exactly the weight of water above . exactly the same is true of the atmosphere . the pressure at ground level is 101,325 pa because each square metre of the ground has about 10,329 kg of air above it ( 10329 kg times the acceleration due to gravity 9.81 m/sec$^2$ = 101325 pa ) . if you could magically remove the 100 km or so of atmosphere that is above some patch of air at ground level that air would indeed immediately expand upwards . incidentally , bernoulli 's principle is unrelated to the problem .
no , you cannot , since you have not specified the voltages of the two batteries . ignoring converter losses , the relevant quantity is not amp-hours but watt-hours . so let 's say the laptop battery is 18 volts and 2.5 amp-hours , while the ups is 12 volts and 7.5 amp-hours . the energy available from the laptop battery is 18 x 2.5 , or 45 watt-hours . the energy available from the ups battery is 12 x 7.5 , or 90 watt-hours . all else being equal , the ups will provide twice the duration of the laptop battery . all else , of course , is not equal . the laptop battery power goes through a set of dc-dc converters to provide the actual voltages used by the circuits , which have an efficiency less than one . the ups battery power goes through an inverter with its own inefficiency , and the resulting ac goes to the laptop where it is converted to the required internal voltages . consequently , you had expect the overall efficiency of the ups battery to be less than the efficiency of the laptop battery . exactly how much less this is , and its effect on the ratio of the two durations , is not something which can be figured out from first principles .
part of the confusion may be that , in your proposed experiment , there are two object--> optical element--> image processes going on . in the first , a plane mirror forms a virtual image of a real object . as explained well by others , this means that there are rays of light reflecting from the mirror that appear , ( by assuming straight-line geometry ) to come from an object behind the mirror . this image is virtual ; the rays never actually get there ; the mirror could be mounted on the face of a granite cliff ! so we have a virtual image . now , you look " in " the mirror , and begin to process those rays of light . as far as you are concerned , the rays are coming from a real object ; that is what a virtual image does ! so the lens in your eye forms a real image on the retina of your eye , and the rods and cones start to send electrical signals to your brain
i ) yes , e.g. all three mandelstam variables $$ s~:=~ ( p_1+p_2 ) ^2~=~m_1^2+m_2^2+2 p_1\cdot p_2 ~\approx~ ( m_1+m_2 ) ^2 + m_1m_2 ( {\bf v}_1-{\bf v}_2 ) ^2 ~&gt ; ~0 , $$ $$ t~:=~ ( p_1-p_3 ) ^2~=~m_1^2+m_3^2-2 p_1\cdot p_3~\approx~ ( m_1-m_3 ) ^2 - m_1m_3 ( {\bf v}_1-{\bf v}_3 ) ^2 ~&gt ; ~0 , $$ $$ u~:=~ ( p_1-p_4 ) ^2~=~m_1^2+m_4^2-2 p_1\cdot p_4~\approx~ ( m_1-m_4 ) ^2 - m_1m_4 ( {\bf v}_1-{\bf v}_4 ) ^2 ~&gt ; ~0 , $$ are strictly positive in the non-relativistic limit $$|{\bf v}_i|~\ll~ c , \qquad i~\in~\{1,2,3,4\} , $$ of massive particles $$m_i~&gt ; ~ 0 , \qquad i~\in~\{1,2,3,4\} , $$ with unequal ( rest ) masses $$i~\neq~j~~\rightarrow~~ m_i~\neq~ m_j , \qquad i , j~\in~\{1,2,3,4\} . $$ here we have used units where $c=1$ , and the non-relativistic formulas $$ {\bf p}_i~\approx~m_i{\bf v}_i , \qquad e_i~=~\sqrt{m_i^2+{\bf p}_i^2}~\approx~m_i\left ( 1+ \frac{{\bf v}_i^2 }{2}\right ) , \qquad i~\in~\{1,2,3,4\} , $$ and $$ p_i\cdot p_j~=~e_i e_j - {\bf p}_i\cdot {\bf p}_j~\approx~~m_i m_j \left ( 1+ \frac{1}{2}\left ( {\bf v}_i-{\bf v}_j\right ) ^2 \right ) , \qquad i , j~\in~\{1,2,3,4\} . $$ ii ) by the way $s+t+u=\sum_{i=1}^4 m_i^2 \geq 0$ implies that it is impossible to have all mandelstam variables $s , t , u&lt ; 0$ negative . so at least one of the three sectors are physical .
what does it mean to integrate $\frac{d\mathbf p}{dt}dt$ ? first , and in scalar form , recall from elementary calculus that $$\int_{x_1}^{x_2} dx = x_2 - x_1 $$ second , recall that $$f ( x + dx ) = f ( x ) + f' ( x ) dx$$ where $$f' ( x ) = \frac{df ( x ) }{dx} $$ denoting the differential of $f$ as $$df = f ( x + dx ) - f ( x ) $$ we have $$df = f' ( x ) dx$$ since $$\int_{x_1}^{x_2} f' ( x ) dx = f ( x_2 ) - f ( x_1 ) = f_2 - f_1$$ it follows that $$\int_{f_1}^{f_2} df = f_2 - f_1 = f ( x_2 ) - f ( x_1 ) = \int_{x_1}^{x_2} f' ( x ) dx$$
in principle of course you try something like that . but there are three issues that will kill you : $q$ . every resonance has a quality factor which represents how quickly the energy in the mode drains away by assorted dissipative processes . i do not know what it is for the schumann resonances , but i will give you long odds that it is not good : much of the energy you put into the field will just dribble away into space . power density . whatever energy you pump into these modes will spread out over the whole cavity , and you will only be able to draw as much as there is in the region covered by your antenna , which will be effective nothing even with gigawatts driven into the resonance . not only could not you power a iphone , you could not power the little shoplifting-prevention tag that retailers put onto bits of mobile merchandise . antenna dimensions . the naive way to design an antennas to use at frequency $f$ requires conductors of length on order of $c f$ . bit of a problem for frequencies of a few or few tens of hertz .
there is a lot of ambiguity in the definition of the stress-energy tensor . the stress-energy tensor is a conserved current , and like all conserved currents it is only defined up to a total divergence . i assume this $t_{\mu \nu}$ was calculated using the canonical prescription $ t^\mu_\nu=\frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi^i ) }\partial_\nu \phi^i-\mathcal{l}\delta^\mu_\nu $ ( you seem to be missing the second piece , or you are dealing with a massless field ) . the canonical tensor is not symmetric for fields with spin . essentially , the intrinsic angular momentum is also contributing to t . so you find a term $s^\lambda_{\mu \nu}$ satisfying $\partial_\lambda s^\lambda_{\mu \nu}\approx t_{ [ \mu \nu ] }$ ( s is antisymmetric in its first two indices , and thus has vanishing divergence ) and add it to the canonical tensor . see this worked out in detail here http://en.wikipedia.org/wiki/belinfante%e2%80%93rosenfeld_stress%e2%80%93energy_tensor this procedure might seem a little random , but of course what you really should be doing is obtaining t from $t^{\mu \nu}=\frac{\delta s}{\delta g_{\mu \nu}}$ as in general relativity . this $t$ will always be symmetric , and is in fact the same as the belinfante tensor . however , there is still ambiguity in this procedure . in order to obtain t this way , you have to " covariantize " the theory , promoting the metric to a dynamical field . this covariantization is ambiguous : you may couple the metric to the curvature non-minimally . these couplings vanish in the flat space limit , but can still affect the expression for t . but at least this expression will always be symmetric . hope this helps !
1 ) " in what situations is it okay to separate the complete wavefunction like that ? " as a simple product , only for distinguishable non-interacting particles . if you add antisymmetry , as ruslan notes , you get a slater determinant wavefunction which is exact for indistinguishable non-interacting fermions . since electrons interact through the coulomb operator , this is indeed never exact for many-electron wavefunctions , as dan mentions . the simple product wavefunction was first used by douglas hartree ; vladimir fock added antisymmetry resulting in the hartree-fock method ( and i guess the professor in your class is taking you in that direction ) . now you say that you heard this in a graduate chemical engineering course , and hartree-fock is still very useful in chemistry . but , as you note , this wavefunction completely ignores entanglement . i will try to explain why is the product wavefunction still useful despite this shortcoming with an example of chemistry . consider a hydrogen molecule at equilibrium bond length . this is a two-electron wavefunction and we can write a wavefunction which considers entanglement as \begin{equation} | \psi \rangle = c_1 | \sigma_\alpha\sigma_\beta \rangle + c_2 | \sigma_\alpha^*\sigma_\beta^* \rangle \end{equation} where $| \sigma_\alpha\sigma_\beta \rangle$ is the slater determinant formed from the bonding orbitals $ \sigma_\alpha$ and $\sigma_\beta$ , whereas $| \sigma_\alpha^*\sigma_\beta^* \rangle $ is a slater determinant made with the antibonding orbitals $\sigma_\alpha^*$ and $\sigma_\beta^*$ ( $\alpha$ and $\beta$ represent the spin of the orbital of course , and i am assuming that you are familiar with molecular orbitals ) . by the variational principle , $c_1$ and $c_2$ must minimize the energy of $| \psi \rangle $ . however , at equilibrium bond length the energy of $| \sigma_\alpha\sigma_\beta \rangle$ is much lower than that of $| \sigma_\alpha^*\sigma_\beta^* \rangle $ resulting in $c_1 &gt ; &gt ; c_2$ and hence $| \psi \rangle \sim | \sigma_\alpha\sigma_\beta \rangle$ . thus , this is a case in which hartree-fock ( i.e. . a single slater determinant ) can yield a qualitatively correct result . if you add perturbation theory to $| \sigma_\alpha\sigma_\beta \rangle$ , you get very accurate ( quantitative ) results . however , hartree-fock may fail terribly in some cases . suppose now that we separate the atoms that compose the hydrogen molecule at such a large distance that their atomic orbitals do not interact anymore . at this point , $| \sigma_\alpha\sigma_\beta \rangle$ and $| \sigma_\alpha^*\sigma_\beta^* \rangle $ become degenerate and $c_1 = c_2$ . thus , in this case we have a multideterminant wavefunction which can not be described by hartree-fock . indeed , in this case the hartree-fock energy is unphysical and much larger than that of two hydrogen atoms . the lesson of this example is that ( antisymmetrized ) product wavefunctions are useful in cases where the entanglement is small , but fail terribly if the entanglement is strong . in general in chemistry , this failure occurs when one has degenerate or near degenerate orbitals , which occurs mostly in molecules at dissociation , transition metals , and singlet diradicals . however , simple closed shell species from the first two rows of the periodic table can often be well described by antisymmetrized product wavefunctions ( single slater determinants ) . 2 ) " how do you know that an isolated electron in an experiment is not influenced by the rest of the particles in the universe ? " the electron interacts with the rest of the particles in the universe , but the interaction is negligible . recall that this interaction must be through the coulomb operator $1/|r_1 - r_2|$ ( in atomic units ) and usually $|r_1 - r_2|$ will be large enough so that we can ignore interactions of ( say ) a molecule in gas phase with the rest of the universe ( we can not ignore interactions of molecule with a solvent when dealing with solutions though , but there are approximate methods for dealing with those ) .
the article you refer to is about the electrolytic splitting of water . a 100% efficient electrolytic cell would require a voltage of about 1.23v to split water , but for various reasons a simple electrolytic cell requires about 1.48v . the difference between the voltages is called the overpotential , and it increases the amount of power needed to split the water because the power required per unit of hydrogen produced is proportional to the cell voltage . the excess power goes into heating the hydrogen and oxygen produced , and in this case it means that simple cells are about 83% efficient at converting electricity into hydrogen . catalysts can be used to increase the efficiency , and indeed platinum based catalysts can be used to reduce the overpotential and make cells with near 100% efficiency . the problem is that platinum is expensive . the result from the stanford team is that a much cheaper nickel based catalyst can achieve the same efficiency as platinum . the paper is here , but note that it is behind a paywall . if the catalyst proves to be stable enough then it will be useful for electrolytic production of hydrogen , but the improvement in efficiency is not going to change the world overnight . it still takes a lot of power to electrolyse water so it is only feasible when cheap electricity is available .
momentum is conserved in magnitude and direction . so in order to analyze any situation of momentum conservation , you should always start with $$ \sum \mathbf p_{i}=\sum\mathbf p_f $$ where the subscripts denote the initial and final momenta . as to the ball and wall , you are correct that momentum is not conserved if you are only looking at the ball . if you consider that the system includes the wall , then the momentum conservation holds . this does mean that the wall contains a momentum of $2mv$ ( for mass $m$ and velocity $v$ ) . but note that since the mass of the wall is incredible compared to the ball , the velocity is notably imperceptible !
olber 's paradox assume infinite and static universe ( infinite life of universe and stars too ) . cosmological redshift due to universe 's expansion shift visible light to infrared or microwave region of electromagnetic spectrum . the cmb radiation is the most clear effect . observable universe is finite ( more or less 93 billion light years ) . this limit the number of galaxies from which we receive radiation . stars have finite life . in addition also some part of universe with recession velocity greater than c could enter ( in the future ) in the event horizon , but they will red shifted at z more than 1.8 , away from visible region . reference : arxiv:astro-ph/0310808v2 13 nov 2003 peacock cosmological physics cambridge press 2010 ( section 12.1 ) davis linewear misconceptions about big bang scientific american march 2005
" before gravity stopped holding it together " is the same ( pretty much ) as " so that apparent gravity at the surface is zero " . this means that $$m \omega^2 r = \frac{gm_{moon}m}{r^2}$$ with $g=6.7\cdot 10^{-11}$ , $m_{moon}=7.3\cdot 10^{22} kg$ , $r_{moon}=1740 km$ , we find $$\omega=\sqrt{\frac{gm_{moon}}{r^3}}=0.0092 rev/min = 0.55 rev/hour$$ it is interesting to see that the result has the ratio of mass and the third power of the radius - in other words it depends on the density and not the size . whether the moon consists of loose dust that will just fly apart when you reach this speed is another question - not one you asked . you asked for rev/minute but that is a hard number to grok . note that one lap per two hours is what you would have to do in order to remain " in orbit " at the surface of the moon . interestingly , the apollo 11 command module ( piloted by michael collins , the least known of the three astronauts that formed the crew of apollo 11 ) was orbiting at about 60 nautical miles , and going around once in just about 2 hours . pretty similar . . . and that was no coincidence . also note that this speed would start the moon ripping apart at the equator - where the centrifugal force appears strongest . at other points along the surface , you have a force of gravity pointing towards the center ; the component of gravity that counters the centrifugal force ( normal to the axis of rotation ) scales with $\cos ( \text{latitude} ) $ which is also how the centrifugal force scales . this means that matter will start drifting from higher latitudes towards the equator . i have not even begun addressing the other questions you had - about the impact on earth if the moon was spinning at half that speed ( 4 hours / revolution ) . i guess we would get to see the back of it , and watching the moon at night would be a less peaceful experience - but i can not think of any real physical effects ( tides etc ) on earth that would affect life . chances are great that if the moon is being hit by asteroids to make it spin faster , that this would affect its orbit around the earth : that would impact tides , and that would really wreck coastal ecology and possibly climates ; but i am assuming that the moon would accelerate from glancing impacts with " twin asteroids " coming from opposite directions , leaving no net linear momentum , and no net mass increase / decrease - just make it spin faster . the impact on a moon base would be more substantial . every point on the moon would experience days and nights on a four hour cycle , with the temperatures cycling violently . communication with earth would be disturbed - you would want to place relay transmitters on the poles to maintain contact . and of course gravity would be halved again - instead of 1/6th of the earth 's pull , you would appear to have 1/12th . coriolis forces would be wicked too - although you could drive a golf ball a very long way with such low gravity , you had get a horrible hook / slice , depending on whether you are on the northern or southern hemisphere of the moon ( and whether you are left- or right-handed ) . finally a word on the energy of the moon if it spun that fast . we know that $$e = \frac12 i \omega^2$$ and $$i = 2/5 m r^2 = 8.3\cdot 10^{28} j$$ if you took all the power of the sunlight that hits the earth ( assuming 1 kw/m^2 over the side that is lit ) for 20,000 years - you would have the amount of energy needed to give the moon that kind of energy . i imagine that if a sufficient density of meteorites came near enough to the moon to impart that kind of energy , we would not be worrying on earth about building moon bases . . . footnote about collins ' orbit . it was said that while he was on the far side of the moon , he was the " loneliest human since adam " - because he was literally further from any human beings ( and with no means of contacting them ) than any other human , anywhere on earth ( this was before major tom , of course ) . the nasa log of the flight contains the tidbits needed to reconstruct his orbital speed : hidden by the moon for 47 minutes per orbit ( july 21 , 9:44 am ) altitude of 60 nautical miles orbital velocity was 5329 ft/sec ( july 22 , 12:56 am ) after converting to sensible units ( really - they put a man on the moon with feet and nautical miles ? what are we doing , trying to force si units on a nation that was capable of such a feat , with those units… ) and drawing a little diagram , i convince myself that the fraction of the orbit when the command module was " dark " is computed as $$f = \frac{\pi - 2 \cos^{-1}\left ( \frac{r_m}{r_m + h}\right ) }{2\pi} \approx 0.35$$ and if that took 47 minutes , then the orbit took 121 minutes .
you want the time . simply put , for that the minimum requirement is position ( and hence the distance ) and velocity . to know the position you need to detect it . once you detect it , you can calculate the trajectory and thus the time you have to settle your issues ( assuming it is on a collision course ) . i got this from cnn : the b612 foundation is building the sentinel space telescope , the world 's most powerful asteroid detection and tracking system , to see the millions of asteroids we can not see today and could pose threats to our planet . also , neossat , the near earth object surveillance satellite , is a micro-satellite launched in february 2013 by the canadian space agency ( csa ) that will hunt for neos in space . tracking systems are recording asteroids even as large as 140 meters . any asteroid with a radius more than 300 meters means an assured global catastrophe . check this out . the size that you are asking about is so big that it will create noticeable gravitational effects ( like perturbation in orbit ) and so we will know about it .
there is a critical current density for every superconductor where the superconductor acts as an ordinary conductor and a voltage difference can be measured between its ends .
if this were computer science , we might say $\psi$ takes a $d$-tuple of reals ( $r$ ) and another real ( $t$ ) and returns a complex number with the attached unit of $l^{-d/2}$ in $d$ dimensions ( with $l$ being the unit of length ) . 1 if you want any more of an interpretation , well then you have already given it : $\psi ( r , t ) $ is the thing such that $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ is the probability of the particle being observed in the region $r$ at time $t$ . you can loosely think of it as a " square root " of a probability distribution . the reason the " square root " interpretation is not quite right , and probably the reason you are not satisfied with the $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ definition , is that any particular instance of $\psi ( r , t ) $ carries extraneous information beyond what is needed to fully specify the physics . in particular , if we have $\psi_1$ describing a situation , then the wavefunction defined by $\psi_2 ( r , t ) = \mathrm{e}^{i\phi} \psi_1 ( r , t ) $ gives identical physics for any real phase $\phi$ . so the return value of the wavefunction itself is not a physical observable -- one always takes a square magnitude or does some other such thing that projects many mathematically distinct functions onto the same physical state . even once you have taken the square magnitude , $\lvert \psi ( r , t ) \rvert^2$ arguably is not directly observable , as all we can measure is $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ ( though admittedly for arbitrary regions $r$ ) . 1 you can check that $-d/2$ is necessarily the exponent . we need some unit such that squaring it and multiplying by the $d$-dimensional volume becomes a probability ( i.e. . is unitless ) . that is , we are solving $x^2 l^d = 1$ , from which we conclude $x = l^{-d/2}$ .
newton 's second law $f=ma$ does not depend on the point of application of force because this law is valid only for point particles . now to apply it to rigid bodies we must consider them as a system of particles . let a rigid body be made up of $n$ particles of mass $m_1 , m_2 , \cdots , m_n$ . now apply a force $f$ to some $i_{th}$ particle . all other particles will also exert internal forces on each other . therefore , the second law for all particles is \begin{align}f_1^{int} and =\frac{dp_1}{dt}\\ f_2^{int} and =\frac{dp_2}{dt}\\ \cdots\\ f+f_i^{int} and =\frac{dp_i}{dt}\\ \cdots\\ f_n^{int} and =\frac{dp_n}{dt}\end{align} adding all these $$\sum_j f_j^{int}+f=\sum_j\frac{dp_j}{dt}$$ by the third law $$\sum_j f_j^{int}=0$$ thus $$f=\frac{dp}{dt} \text{where } p=\sum_jp_j$$ now if you apply the same force $f$ to the center of mass of the body , you get the same equation for total momentum . $$f=\frac{dp}{dt}$$ therefore both situations will have identical solution for the total momentum and hence for the linear velocity of the center of mass of the rigid body .
the expectation value $$\tag{1} \langle \psi | v| \psi \rangle~=~0$$ of the potential energy operator $v$ is indeed zero , but the expectation value $$\tag{2} \langle \psi |k| \psi \rangle~=\frac{\hbar^2}{2m} \int_{\mathbb{r}}\ ! dx~ |\psi^{\prime} ( x ) |^2 ~=~+\infty$$ of the kinetic energy operator $k$ is actually infinite for the wave function $$\tag{3} \psi ( x ) ~=~ \sqrt{\frac{2}{l}}\left ( \theta ( x-\frac{l}{2} ) -\theta ( x-l ) \right ) , \qquad x\in \mathbb{r} . $$ here $\theta$ is the heaviside step function . the kinetic energy operator $k:=-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}$ is an example of an unbounded operator , which only make sense on its domain ${\cal d}_k\subsetneq {\cal h}$ inside the hilbert space ${\cal h}:=l^{2} ( \mathbb{r} ) $ of square lebesgue integrable functions . in particular , it is a non-trivial mathematical problem how to apply the differential operator $k$ to the non-differentiable wave function ( 3 ) . the infinite result ( 2 ) can be seen ( at the physical level of rigor ) in at least three ways ( ordered with the computationally simplest calculation first ) : plug the heaviside step function into eq . ( 2 ) to get an integral over the square of a pair of dirac delta function situated at $x=\frac{l}{2}$ and $x=l$ . this is strictly speaking mathematically ill-defined . physically , it makes sense to assign the integral the value infinite , cf . this phys . se post . calculate the overlaps $c_n=\langle \phi_n | \psi \rangle$ , and show than the sum $$\tag{4} \langle \psi |h| \psi \rangle=\sum_{n=1}^{\infty}|c_n|^2 e_n ~=~+\infty $$ diverges . this infinite conclusion seems physically robust , since all terms in the series ( 4 ) are non-negative . by regularization , as emilio pisanty suggests in a comment . define a regularized wavefunction $\psi_{\varepsilon} \in c^1 ( \mathbb{r} ) $ in such a way that ( i ) it converges $\psi_{\varepsilon}\to \psi$ for $\varepsilon\to 0^{+}$ , ( ii ) the expectation value $\langle \psi_{\varepsilon} |k| \psi_{\varepsilon} \rangle$ is easy to compute and finite for $\varepsilon&gt ; 0$ . show that $\langle \psi_{\varepsilon} |k|\psi_{\varepsilon} \rangle\to +\infty$ diverges for $\varepsilon\to 0^{+}$ .
1 ) if there is an error $e_j$ , the new states $e_j|0\rangle_l$ and $e_j|1\rangle_l$ are eigenvectors , with eigenvalue $-1$ , of all the stabilizers $s_j$ belonging to some set subset $s_j$ of $s$ . ( the elements of $s_j$ anticommute with $e_j$ ) . this subset $s_j$ identifies uniquely the error $e_j$ . 2 ) $|0\rangle_l$ and $|1\rangle_l$ are eigenvectors , with eigenvalue $1$ , of all the stabilizers $s$ belonging to $s$ ( this is not true for the " components " of $|0\rangle_l$ and $|1\rangle_l$ like , for instance , $|1010101\rangle$ ) . for a stabilizer $s$ , you just calculate $s|0\rangle_l$ and $s|1\rangle_l$ , and you check that the result is $|0\rangle_l$ or $|1\rangle_l$ . for instance : $k^1\left|0\right\rangle_l = ( iiixxxx ) \\\frac{1}{\sqrt{8}} ( \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle + \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle ) = \\ \frac{1}{\sqrt{8}} ( \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle + \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle ) \\ =\left|0\right\rangle_l$ 3 ) $k^4 \left|1010101\right\rangle = iiizzzz |1010101\rangle$ . with $z |0\rangle = |0\rangle$ , and $z |1\rangle = -|1\rangle$ , you get : $k^4 \left|1010101\right\rangle = |1010101\rangle$
i am not going to provide a full answer here , because i do not know the answer , but i want to give some statements that illustrate quite nicely the kind of problems one would face when determining topology of anything : we know spacetime is a manifold . that means , locally , it looks just like $\mathbb{r}^4$ . that is already a bummer . we can not do jack at one place to find out anything about topology . but , as soon as we move , we get into all the complications of reference frames and whatnot . so , experimentally , whether or not we can principally detect topology , it is going to be one hell of a challenge . but it gets worse . you know how we always suppose that fields fall off at infinity ? that is one of the natural reasons principal bundles arise in gauge theories . if we want to make precise the notion of a field $a$ falling off at infinity , we say it has to be a smooth function and have a well-defined value $a ( \infty ) $ . and what is $\mathbb{r}^n$ together with $\infty$ ? the one-point compactification , also known as the sphere $s^n$ . but it is not quite feasible to find global solutions to the equations of motion of a gauge theory on $s^n$ , thanks to the hairy ball theorem and others . so we say : alright , let 's solve the e.o.m. locally on some open sets $u_\alpha , u_\beta$ homeomorphic to the disk ( think of the hemispheres overlapping a bit at the equator ) , and patch the solutions $a_\alpha , a_\beta$ together on the overlap by a gauge transformation on $u_\alpha \cap u_\beta$ . now , we have got our field living naturally on the sphere $s^n$ if we want a global solution . does this mean that we actually live on an $s^n$ , or just that we are inept to find a coherent description of the physics on $\mathbb{r}^n$ ? what would that even mean ? i can hear the people saying " we can always examine what the curvature is - $s^n$ has non-vanishing one , $\mathbb{r}^n$ has vanishing one . " . that is alright , but the above gauge argument forces us the either accept that there is no globally well-defined gauge potential $a$ on $\mathbb{r}^n$ or to think of some $s^n$ on which a patched-together solution lives . what is more real ? what would it even mean to say one of these views is more meaningful than the other ? so , you might be inclined to say : " screw these weird gauge potentials , we are living on a spacetime , and not some bundle ! " but there are topological effects of these bundles such as instantons or the aharonov-bohm effect . spacetime alone is not enough . and what would be a meaningful distinction between " these bundles are not where we live , they are ' above ' spacetime " and " we live on the bundles , and most often only experience the projection on spacetime " ? what i am trying to say is that it is not even clear what we should regard as the universe we live in . the ordinary , 4d spacetime is not enough to account for all the strange things that might happen . and as i said in the beginning , do not take this as an answer . i am biased from being immersed in gauge theories , and only having superficial knowledge of the intricacies of gr . but from what i see , all " non-trivial topology " can also be seen as arising from patching together local solutions to the physical laws that otherwise do not match well .
according to our current scientific knowledge we know that we do not know what the 70% of the energy of the universe is . also , a comprehensive description of the thermodynamics of the universe is impossible with the current standard cosmological model and einstein 's general relativity . in particular it is very complicated , and incomplete , as i said , to talk about what is now the energy of the universe . hence , its thermodynamical evolution has very low sense , with the current knowledge . it is only a speculative subject . for a more satisfactory answer , i can give you a brief naive explanation of why one can not talk about the energy of an evolving universe . as you know and as we currently believe , we live in an expanding universe . this is not a steady state , there is a precise time-line and a temporal asymmetry in its evolution , hence you can not define a globally a well defined conserved charge called energy , in the sense of the noether 's theorem . you can define energy only locally , according to the equivalence principle . also , there is another problem called quantum vacuum whose energy is non-null and may even increase with the expansion of the universe .
the problem with the phase space flow in hamiltonian mechanics is that the flow itself is non-dynamical , that is , the flow is immediately defined for a given hamiltonian , so there is no independent equation governing its evolution . thus , liouville equation is simply a transport of a scalar variable in a given flow . so , dimensional analysis of the flow would be simply subset of dimensional analysis of underlying hamiltonian structure . similarly , i do not think there is any sense of attempting to find turbulence in the phase space flows . sure , time dependence of hamiltonian can introduce changes in the phase space , including the type of changes associated with transitions to chaos : such as bifurcations , tori destruction . . . but again , the flow itself is not the fundamental object in such transitions . if we are talking about the phase space of kinetic equations , the same arguments apply . even though the flow is ' more dynamical ' especially if taken in the context of self-interacting system of equations such as vlasov-maxwell , in these equations the flow itself again is not a fundamental object , so rarely it is analyzed independently . however , most methods of ( numerical ) solutions for such equations like particle-in-cell method and its many variations do use approaches quite similar to that of hydrodynamics .
by using an arbitrary test function you have not included any information about the state of the electron . this means you are considering all possible states of an electron in a coulomb potential , so that formula you derived is true for all bound states of the hydrogen atom and all the unbound states of an electron scattering off the proton . it is true in the classical limit , where it should give you back the kepler orbits , but it also true for states which are behaving distinctly quantum mechanically , notably the eigenstates for the hydrogen atom , which are stationary . given that the expression is so general , its not that surprising that you get a result that does not immediately resemble the classical result . normally to take the classical limit of a system you have to think a bit about what is physically happening as you take that limit . in this case you would probably need to consider a superposition of a large number of very high $n$ states , i.e. states with a large energy and a large uncertainty in the energy . ( you would probably need similar conditions on $l$ as well )
in renormalization , one considers a family of lagrangian densities , with arbitrary factors for each renormalizable monomial in fields and derivatives ; in case of symmetries only of the symmetric ones . thus it is determined by the field and symmetry content only . for perturbation theory , these factors are then written as the renormalized finite term plus the diverging counterterm . the particular way of generating these factors ( by scaling fields or coupling constants ) is immaterial . as the regularization scale is sent to infinity , the counterterms are left to diverge in such a way that the renormalization conditions give finite results . the family of renormalized theories is now parameterizied by the parameters used in the renormalization prescription .
degenerate usually means that the gas is in a quantum regime , that is the thermal de broglie wave length $\lambda_{\rm db}\propto t^{-1/2}$ is much larger that the interparticle distance $l=n^{-1/d}$ , where $n$ is the density and $d$ is the dimension of space . one then has $l\propto k_f^{-1}$ where $k_f$ is the fermi momentum . this regime is the opposite of that of the classical ( dilute ) gas . a degenerate fermi gas is thus such that $t\ll e_f=\frac{k_f^2}{2m}$ . this corresponds to the limit where the fermions form a well defined fermi sphere , etc described in text books ( usually in a chapter about the fermi gas ) . note that electrons in metals also form a degenerate fermi gas .
a quasi-one-dimensional fermi surface is a fermi surface whose topology is the same as the topology of a surface defined by an equation that only depends on one dimension . in the picture above , taken from this paper , you see that the surface is effectively given by $$|k_x|\leq 1 $$ for the first picture . ( well , it should really be an equality if we talk about the surface itself but i wanted to discuss where the states are located , too . ) it looks like $k_y$ does not really qualitatively matter . that is different from ordinary two- or three-dimensional fermi surfaces given by inequalities such as $$\sqrt{k_x^2+k_y^2+k_z^2}\leq 1$$ because $k_y$ and perhaps $k_z$ play no qualitative role in the quasi-one-dimensional fermi surfaces , one may use various parts of intuition and formalism that are relevant for one-dimensional problems and just add the $y , z$ directions as dummy extra variables that do not affect anything qualitative . i believe – but i may be wrong – that it is confused to talk about " characters of fermi surfaces " . in the context of orbital hybridization , " character " refers to the bonds themselves ( the shape at the level of a single molecule ) , for example $sp^3$ has 25% s-character and 75% p-character . but when one talks about " character " of fermi surfaces , the word " character " just means some more general properties – any properties , unspecified properties , not something particular .
from skimming a few articles and patents on e-ink driver technology , my impression is that the primary reason is that each microcapsule acts as a capacitor . once voltage is applied , the particles move to one electrode or the other and remain there because there is no drain path for the charge . the ' gooiness ' of the fluid helps , as evidenced by the typical approach of applying a " shaking pulse " sequence after a certain number of image transitions . this pulse sequence helps ensure that all the microparticles are freed up to be driven to the appropriate state . this may or may not be helpful : http://patents.justia.com/patent/20060170648
no , because the uncertainty principle operates between position and momentum rather than position and velocity . for speeds much less than $c$ , momentum is just proportional to velocity : $p = mv$ . but at relativistic speeds we have to use the relativistic version , $$ p = \gamma mv , $$ where $\gamma = 1/\sqrt{1-v^2/c^2}$ . substituting this in and squaring both sides we get $$ p^2 = \frac{m^2v^2}{1-{v^2}/{c^2}} , $$ which we can rearrange a little to get $$ v^2 = \frac{p^2}{ m^2 + p^2/c^2 } , $$ or $$ v = \frac{p}{\sqrt{ m^2 + p^2/c^2 }} . $$ now , the limit of this as $p \to \infty$ is just $$ v = \frac{p}{\sqrt{p^2/c^2 }} = c . $$ the momentum $p$ can fluctuate due to the uncertainty principle , but now you can see that now matter how big $p$ gets , $v$ will always be less than $c$ .
think about this with an example : the sine and cosine functions . they both average individually to zero over an interval . you can multiply those averages and still obtain zero . but if you multiply sin by itself and then average , you get a very distinct non-zero result . when the functions are arbitrary , the average of the product quantifies statistical correlation between the two functions/variables . this correlation gives a rough measure of how causally connected are both of the variables . this correlation information is completely lost when one takes the averages of the functions individually .
new answer what you have done here is just dimensional analysis . but you have gone a little too far . in particular , just because two things have the same dimensions does not mean that they are equal . if you want to expand $f/a$ a little more , you can choose your favorite from \begin{equation} f = \frac{d p}{dt} = \frac{d}{dt} ( m\ , v ) = m\ , \frac{d}{dt} v = m\ , a \end{equation} here , $p$ is the momentum , $m$ is some amount of mass you are keeping track of , $v$ is its velocity , and $a$ the acceleration . now , pressure is really defined as being the force on a unit of area . so if you do not choose an area , you can not define pressure . there might be situations where there is some momentum change in a volume without the matter directly hitting a surfce . for example , with an electromagnetic force . so there could be a net force on a volume . but again , you can not call anything a pressure unless you select an area to measure it . old answer ( from before reading your clarifications ) pressure is the force per unit area applied perpendicular to the surface of an object . ( or at least the area over which you are imagining measuring the pressure . ) if i understand you correctly , you are interested in the pressure on the surface of the tube . is this correct ? but you are referring to the mass flowing past the tube . that is , it is flowing parallel to the surface . so , in your simplified model where the matter is flowing straight , there would be zero pressure . of course , in real life , the matter would probably having some internal motions as well , which means it would bounce off the walls of the tube , exerting pressure . in short , we would need more information to calculate the pressure . for your particular setup , you still have not told us about any interactions between particles of the matter . from what you have told me , i can still assume that the matter particles all have the same velocity ( but are scattered at different positions ) . now , as you know , force is proportional to acceleration . since the velocity is constant , there is no acceleration , so there must be no force , which means there is no pressure .
a spatial fourier transform means a fourier transform in the spatial variable ( $x\rightarrow k$ ) , while a temporal fourier transform is the same transformation , but in terms of the time variable ( $t\rightarrow \omega$ ) . the equation you have written is the ( asymmetric ) temporal fourier transform of $f ( k , t ) $ . the spatial transform looks like some variation of \begin{equation} f ( k , t ) = \frac{1}{2\pi} \int g ( x , t ) e^{i k x} dx \end{equation} where $g ( x , t ) $ is the ( one-dimensional ) van hove function .
for inflation the potential energy of the field dominates the kinetic energy $\dot{\phi} \ll v ( \phi ) $ this limit is referred as slow roll and under such conditions the universe expands quasi exponentially $a ( t ) \propto \exp \left ( h dt\right ) = e^{-n} $ where we define the number of e-folds $n$ as : $dn = -h dt$ so that $n$ is large in the far past and decreases as we go forward in time and as the scale factor $a$ increases . with this we have : $\epsilon = -\frac{\dot{h}}{h^{2}} = \frac{1}{h}\frac{dh}{dn}$ accelerated expansion will only be sustained for a sufficiently long period of time if the second time derivative of $\phi$ is small enough : $|\ddot{\phi}| \ll |3h\dot{\phi}| , |v' ( \phi ) |$ so that the equation of motion for the scalar field is approximately : $3h\dot{\phi} + v' ( \phi ) \simeq 0$ this condition can be expressed in terms of a second dimensionless parameter , defined as : $\eta \cong -\frac{\ddot{\phi}}{h\dot{\phi}} \cong \epsilon + \frac{1}{2\epsilon}\frac{d\epsilon}{dn}$ then $\eta \simeq \frac{1}{8\pi g} \left ( \frac{v'' ( \phi ) }{v ( \phi ) } \right ) $ in the slow regime $\epsilon , |\eta|\ll 1$ , where the last condition ensures that the change of $\epsilon$ per e-fold is small . notice that $\eta$ need not be small for inflation to take place . inflation takes place when $\epsilon &lt ; 1$ , regardless of the value of $\eta$
yes , the photons actually reach you , like rain falling on you , not like watching rain from a distance . when you see a star , photons from the star actually enter your eye . in for example rods of your eye , the photon causes a molecule of retinal to react by change from cis to trans isomer .
friction force changes direction during the motion . when the body is going uphill the parallel component of gravity with respect to the plane is pulling in one direction ( downhill ) and friction does the same . viceversa , when the body is going downhill , gravity still pulls in that direction ( downhill ) , while this time friction points uphill . that is why you have different acceleration . anyway , when resolving basic mechanics problem , my advice is to think practically : for example , when riding a bike on a hill , can you slow down faster when you are going uphill or downhill ?
it is not possible that susy particles are hiding in kev or mev range . in particular , there can not be any new charged particles ( and similarly new color-charged particles ) that would be this light because they would be easily pair-produced and easily detected . the first ( february 2012 ) claims by different authors ( the original ones , rupp and van beveren , who made the conjecture ) were refused by the compass collaboration ( which was used as one of the main pieces of " evidence" ) here : http://arxiv.org/abs/1204.2349 compass says that the patterns that attracted the attention or rupp and van beveren are due to $\pi^0$ , $\eta$ , and secondary interactions in the compass spectrometer . rupp and van beveren responded that the compass critique is internally inconsistent . it seems more likely to me that compass is right . the newest russian experimental paper looks strange to me . for example , it never quotes any confidence levels , as far as i can see , and instead says that there are " almost no errors " in their measurement , a claim that it easily refuted by looking at their chaotic wiggly charts . an extended discussion may be found on my blog .
i would suggest ' single- and double-slit diffraction of neutrons " by zeilinger , gahler , shull , treimer , and mampe , reviews of modern physics 60 ( 4 ) , 1067-1073 ( 1988 ) . if i might quote the abstract : the authors report detailed experiments and comparison with first-principle theoretical calculation of the diffraction of cold neutrons ( $\lambda \approx$ 2 nm ) at single- and double-slit assemblies of dimensions in the 20—100 $\mu$m range . their experimental results show all predicted features of the diffraction patterns in great detail . particularly , their double-slit diffraction experiment is its most precise realization hitherto for matter waves . so , single and double slit experiments with neutrons have indeed been done , and indeed show just what would be expected . here 's a brief description of how the experiment worked . they used neutrons from a reactor , which were brought down to room-temperature thermal energies using heavy water . they let those out through a collimator to make a beam . to make the beam monochromatic , they bent it through a quartz prism and selected one wavelength using a a slit . they verified the energies using time of flight ( i think using a beam chopper ) . to produce diffraction , they used a tiny double slit with a spacing of 0.1 mm . at a distance of 5 m from the double slit , the spacing of the fringes was about 0.1 mm , and they had to make the graph by slowly moving the detector across the fringes and measuring a count rate at each position .
the following reactions take place : $o_2 + 4 e^− + 2 h_2o → 4 oh^−$ $fe → fe^{2+} + 2 e^-$ $4 fe^{2+} + o_2 → 4 fe^{3+} + 2 o^{2−}$ $fe^{2+} + 2 h_2o ⇌ fe ( oh ) _2 + 2 h^+$ $fe^{3+} + 3 h_2o ⇌ fe ( oh ) _3 + 3 h^+$ $fe ( oh ) _2 ⇌ feo + h_2o$ $fe ( oh ) _3 ⇌ feo ( oh ) + h_2o$ $2 feo ( oh ) ⇌ fe_2o_3 + h_2o$ ( source : wikipedia ) now since initially only $fe$ was present and finally its oxides are present in the sample , there is definitely an increase in the mass of the sample .
( sorry i have to split this up because stackexchange will not let me post more than two links ) even helium atoms ( at reasonable temperatures ) do not tunnel through graphene , as the potential energy barrier is far too high . ( source : leenaerts et al . 2008 )
the electromagnetic processes between atoms and molecules in all phases , solid , liquid , gas , depend on what is generically called " van der waals " fields and subsequent forces . it is well known that the atoms/molecules are neutral , nevertheless there exist for all matter dipole and quadrupole and higher order fields which are mainly attractive and form the chemical bonds which is the way neutral atoms and molecules can bind into solids and liquids and interact as gases . these bonds are quantum mechanical , that means that there exist solutions of the schrodinger equation with energy levels from ground state to continuum , one can model them as repeated over all the mass of the solid , liquid and gas . the unfilled energy levels are close to each other in energy and the continuum of n=infinity ( the radial quantum number ) . at the same time the atoms and solids have pure kinetic degrees of freedom : they can vibrate and rotate in solids , they can move in two dimensions in liquids and in all three dimensions in gases . in gases simple scattering of the molecules transfers the kinetic energy of one molecule to the potential energy of another , i.e. raises an electron to a higher level . the electron goes back to its ground state releasing a specific photon , or a cascade of photons , depending on the energy . remember that the higher levels with respect to n , the radial quantum number , are closely packed . these photons are the ones emitted as black body radiation , and they are a continuum because of the 10^23 molecules per mole and the almost continuous energy levels . the temperature is a function of the average kinetic energy in the gas , the higher the temperature the more energetic the kinetic scattering and the higher the average photon energy . in a solid there are also vibrational and rotational kinematic degrees of freedom that are contributing to the average kinetic energy , i.e. temperature . the kinetic energy of the molecules becomes potential energy for an electron in the lattice which then decays to its ground state or through cascades . the logic is the same as for gases and the same holds for liquids that have some extra kinematic degree of freedom with respect to solids . so it is the quantum mechanica behavior of matter at the micro level which is responsible for the black body radiation , and the infrared catastrophy problem of the classical extrapolations was solved . it is the energy levels that make a difference between infinity and well behaved property in the electromagnetic emmissions . thus the average kinetic energy ( proportional to t ) diminishes by turning into electromagnetic emissions through the stepping of energy levels . does that mean in rarefied conditions where the mean-free-path is relatively large , the rate of ir emissions decreases ( while the intensity is still only dependent on the temperature ) ? when the mean free path is large , the temperature is lower , the average kinetic energy of the atoms is lower and thus the photons produced by the transformation of kinetic to exciting electrons to higher potential energy levels and consequent decay to ground energy levels are all lower and will keep getting cooler if the energy is not replenished . i do not know what you mean by the intensity . this is the black body radiation spectrum . as the temperature decreases , the peak of the black-body radiation curve moves to lower intensities and longer wavelengths . the black-body radiation graph is also compared with the classical model of rayleigh and jeans . if you mean gases in low pressure as at the top of the atmosphere etc , one has to study them separately according to the boundary conditions . there can be gases with very high temperatures as in the atmosphere of the sun .
i interpreted your question differently , more like a mathematics question . in quantum mechanics , we basically have an equation , the schrödinger equation , which is a differential equation on the space of square-integrable complex valued functions . this space is a hilbert space , which means that it is a vector space , and it also has a nice topological structure , basically all cauchy-sequences of vectors converge in that space . in newtonian mechanics , the equations are defined on phase space , which is basically a $6n$-dimensional space , $n$ is the total number of particles , on which coordinates for a point consist of the positions and momenta of each particle you want to describe . the solution of the equations induces a flow on this phase space . the structure of phase space is usually that of a symplectic manifold . in general relativity , the equations are einstein 's field equations . they link the riemann tensor to the energy-momentum tensor . they are difficult to solve in the sense that they are nonlinear and you have to specify an energy-momentum tensor , but this tensor will also depend on the geometry of space-time , thus the riemann tensor . so you have to solve in one go for the geometry and energy-matter distribution . in practice , many simplifying assumptions will be made . but the " space " of solutions is the space of geometries and energy-matter distributions compatible with the field equations .
the container on earth will be cooled by convection currents i.e. it transfers heat to the air around it , and also by black body radiation . by contrast the container in space can only cool by black body radiation , and obviously it will cool down more slowly . you can calculate the cooling in space using the stefan-boltzmann law assuming you know the emissivity ( if you paint the container black the emissivity will be close to unity ) . calculating the cooling in air is harder ; typically you had use newton 's law with empirically derived constants . the final temperature in air is obviously just the temperature of the air around your container . the final temperature in space depends on where your container is . just as the container can lose heat by emitting radiation it can gain heat by absorbing radiation , and space is full of radiation . for example the moon is just a lump of inert rock with little or no internal generation of heat , however by absorbing sunlight the daytime temperature can rise to over 100ºc . however at night , when there is no sunlight the temperature can fall to -150ºc . so the final temperature of your container would be different during the lunar night and day , even though it is in a vacuum in both cases . if you took your container into intergalactic space , well away from any radiation sources , then it would indeed cool to the 2.7k of the cosmic microwave background .
dear leandro , it is because the pauli matrices , together with the $2\times 2$ identity matrix , form a full real basis of all the hermitian $2\times 2$ matrices ( note that $2\times 2$ hermitian matrices depend on four real parameters ) , and the identity matrix is irrelevant in a hamiltonian because it is just a conventional energy shift that acts on all vectors equally and does not create any subtleties such as line crossing so one may omit it in any discussion of interesting physical effects . so any $2\times 2$ hermitian matrix that " matters " - and yes , hamiltonians have to be hermitian operators - is a real linear combination of the three pauli matrices . they do not have to be interpreted as a spin of any kind . but they are still a more natural basis than any other basis of the hermitean matrices because the product of any pair of the matrices generates a multiple of another matrix in the basis , thus simplifying all the calculations . ( analogy with the conventional $so ( 3 ) $ generators is a more transparent way to see the power of this basis . ) but even if you chose a less clever basis than the pauli matrices , you could derive the same results . the equations could just become a bit more cumbersome . line crossing etc . is first observed at degeneracy 2 . however , one may also study $3\times 3$ or $n\times n$ matrices . conventional bases of the hermitian matrices are called the gell-mann matrices in the case of $su ( 3 ) $ - because gell-mann generalized the pauli matrices when he studied the strong force where $su ( 3 ) $ enters in two ways . the $n\times n$ hermitian matrices depend on $n^2$ real parameters . one usually takes the identity matrix to be one of the basis vectors ; the remaining $n^2-1$ vectors are " generators of $su ( n ) $" .
if i am not wrong the product is well defined , because the 2 point funtion is boundary value of an analytic function in some tube ( see streater/wightman ) with certain bounds . more genral there is 1-1 correspondence between translation invariant tempered distributions which satisfy the spectrum condition and analytic functions in this mentioned tube and an easy way to define the product of the distribution is the product of the analytic functions which lies in the same class and therefore gives a well defined distribution on the boundary . really easy is the massless case in let 's say 4d . $w ( x-y ) \sim \frac1{ ( x-y+i\epsilon ( x_0-y_0 ) ) ^2}$ $w^2 ( x-y ) \sim \frac1{ ( x-y+i\epsilon ( x_0-y_0 ) ) ^4}$ where the two-point function is just the boundary value of the analytic function $1/z^2$ . in 1 dimension , that means a chiral field on a light ray , it is even more drastic . there the wick ( =normal ordered ) square of the free fermion is a free boson . edit the answer to the question " is a normal-ordered product of free fields at a point a wightman field ? " is : yes ! for convinience i give you a reference , which is " general principles of quantum field theory " by bogolubov , logunov , oksak , todorov ( 1990 ) p . 344 ex . 8.16 prove that the wick monomials satisfy the wightman axioms edit tim : i can not comment on your yet . i did not say you can multiply general distribution which are boundary values by multiplying the function , just for a certain class but maybe i was sloppy because i thought this was standard . the $\delta$ is not in this class , because it does not fullfill a " spectrum contion " , i.e. the fourier transform is supported in some convex cone with non-empty dual cone . the fourier transform of $\delta ( x ) $ is constant so does not fall into this class . but distributions which have this property on their fourier transformation you can multiply : see the standard textbook reed simon volume 2 - page 92 ex 4 . i hope my references clear the confusion .
please do however remember the following line from the link you yourself have cited : in general , if the behaviour of a system of more than two objects cannot be described by the two-body interactions between all possible pairs , as a first approximation , the deviation is mainly due to a three-body force . hence , it can be seen that , initially , when people were thinking of many-body problems , they encountered terms in the mathematical formulation which they later termed as many body forces . incidentally , they were discovered in strong interactions and were a result of gluon mediation . in the celestial scale hence , you would need to have a similar mediating phenomena/theory to explain any physically valid three body force . also related : n-body forces in classical mechanics
if a crystal has a discrete group of point symmetries then the electronic eigenfunctions will be suitably invariant under that group . formally , the symmetry requires that the eigenfunctions of a hamiltonian with symmetry group $g$ belong to the various representations of that group . in the abstract , a representation of a group $g$ is a vector space ( in this case a subspace of degenerate energy ) $v$ and a " recipe " for unitarily transforming the vectors in $v$ with the transformations from $g$ , in the form of a group homomorphism $r:g\rightarrow u ( v ) $ . if one knows the structure of a group ( in the form of its multiplication table ) then there is a lot that can be said about its possible representations , which are typically denoted by some standard notation ( e . g . $e$ , $a$ , $b$ , etc . ) . the wavefunctions are then labelled by the representation type of the subspace they belong to . to bring this back down to angular momentum , the subspaces with different $l$ are the different representation subspaces . the group in question is the rotation group $\text{so} ( 3 ) $ . it has an infinite family of representations of increasing finite dimension , and the index $l$ that labels them is precisely the angular momentum quantum number of those wavefunctions . in group theoretic terms , then , " having definite angular momentum " simply means " belonging to a suitable representation of $\text{so} ( 3 ) $" . thus a crystal with a point symmetry will have electronic eigenfunctions that do have " definite crystal angular momentum " , in the sense that they belong to a certain representation of the point group . added in response to comment : unfortunately , there is no physical quantity that corresponds to this symmetry . this is due to the general fact that discrete symmetries have no generators . while you can write rotations , for example , in the form $e^{i\mathbf{j}\cdot\hat{\mathbf{n}}\theta}$ , where $\mathbf{j}$ is the generator , this is not really meaningful for discrete symmetries . a good comparison for this is parity : if $\pi$ commutes with $h$ then we say parity is conserved , in the sense that the transformation itself is a constant of the motion . for a more general discrete group $g$ ( instead of $g=\{-1,1\}$ for parity ) then the labels $+$ and $-$ are replaced by the group representation . similarly the labels $l$ and $m$ correspond to the group representation and to the eigenvalues of some particular group transformation . both are conserved under $h$ , but there is no generator .
method 1 is universal , method 2 works only for dc . method 2 is for dc supply , whereas the method 1 is for ac supply . in ac circuit the inductor ( a coil ) and a capacitor pose resistance which is calculated as impedence of circuit . . whereas in dc , capacitor fully blocks the supply and thus capacitive reactance is infinite and the inductive reactance is zero . thus the current in a capacitive circuit is zero . z^2=r^2 + ( xl -xc ) ^2 ; z ( dc ) =r ; also the difference of xl and xc is small and thus answer is approximately the same by the method 2 in ac circuits too .
firstly , $tan ( \theta ) = \frac{v^2}{rg}$ means that as you increase the centrepital velocity , $v$ , of the pendulum , the angle between the rope and the vertical increases . now as the pendulum gets faster and faster , the angle gets larger and larger but never exceeds 90 degrees . i will not provide supporting data , as the above equation is all the data you need .
you may be misinterpreting what is meant by " slow down " in special relativity . it is a fundamentally different sort of slowing down compared to what happens during doppler shift . it is true that the frequency of a periodic signal will tend to be lower as the source is moving away , and higher than would otherwise be expected as it approaches . this works with both sound and light . it seems you know this . however , in special relativity , the thing that is producing the signal is actually producing them less frequently . actually , it does not have to be a signal in the usual sense . in the twin paradox , the space-traveling twin 's heart beats slower , both on the outgoing and incoming trips . time does not speed back up on the return trip to make up for the loss . to apply this same idea to light , if you want to determine what frequency of light you had detect on ( stationary ) earth from a fast-moving light source , you need to first take this time dilation , as it is called , into account . only after doing this would you then consider how the doppler effect would affect your reception . actually , the light source could be moving perpendicularly to our line of sight . in that case , there would still be time dilation , but no doppler effect . that is , you had still detect a lower frequency , but it would not be due to the doppler effect .
the $k$ notation is generally used to describe friedmann robertson walker cosmological models . these are built on the assumptions of homogeneity and isotropy . the spacetime can be described as being foliated by spatial slices of constant curvature . the k value is the sign of this spatial curvature if the {-1 , 0 , +1} convention is adopted . as the curvature is a constant , it makes sense to talk of its sign . further details here .
a small attachment ? no , not possible . think of it this way : if em waves are emitted from the phone 's antenna at point a , the only way they can be stopped from reaching point b is if either you place something directly between a and b that absorbs or reflects them , or you create an equal but opposite wave at b from somewhere else , destructively interfering with the signal from a so that there is no em wave at b . an attachment like the one linked can not shield all directions simultaneously , so option ( 1 ) does not work in general . with option ( 2 ) , at best you can neutralize the wave at one point , but you can not generally neutralize it elsewhere at the same time . for the exact same reason there are noise cancelling headphones ( neutralize sound at your eardrum ) but not noise-cancelling devices fields that can make a whole airplane go silent . something that makes destructive interference at one point will make constructive interference nearby . of course if you want to block em waves , that is easy and you can do it for cheap . surround your phone in metal ( it can even have holes in it ) , creating a faraday cage . it should be noted that shielding of em waves also makes the phone not work - phones are not magic , and that " radiation " is what they use to communicate . block it and of course they stop working . if you want to see this effect on a large scale , just walk inside an earthquake-proof building that has lots of rebar ( and does not have fancy cellular repeaters ) - you will see your signal strength die pretty rapidly .
the energy formula $$\tag{39.11} e~=~\frac{1}{2}mv^2 -\frac{1}{2} m ( {\bf \omega} \times {\bf r} ) ^2 + u $$ in ref . 1 ( of a point particle , as seen in a rotating reference frame $k$ ) consists of three terms : kinetic energy : $\frac{1}{2}mv^2$ . centrifugal potential energy : $-\frac{1}{2} m ( {\bf \omega} \times {\bf r} ) ^2$ . other potential energies $u$ . in particular , the minus sign in front of the second term is the correct one . it is a centrifugal potential , so it encourages the system to increase its radial coordinate $r$ . phrased equivalently , it costs work ( against the centrifugal potential ) to reduce the radial coordinate $r$ . references : l.d. landau and e.m. lifshitz , mechanics , vol . 1 , 1976 .
i am not sure if this is exactly what you are looking for or perhaps you already know what i am about to say . there is a geometric notion of a twistor spinor ( or conformal killing spinor ) : one which is in the kernel of the penrose operator ( see below ) . then one defines the twistor space as the projectivisation of the space of twistor spinors . doing this for minkowski spacetime recovers the usual twistor space . let $ ( m , g ) $ be a riemannian spin manifold . ( when i say riemannian i include also the case of a metric with indefinite signature . ) let $s$ denote the complex spinor bundle . the spin connection defines a map $$ \nabla : \gamma ( s ) \to \omega^1 ( s ) $$ from spinor fields to one-forms with values in $s$ . now $\omega^1 ( s ) = \gamma ( t^*m \otimes s ) $ and clifford action of one-forms on spinors gives a map $$ \omega^1 ( s ) \to \gamma ( s ) $$ the composition of the previous two maps is the dirac operator . the penrose operator is in some sense the complement of the dirac operator $d$ . the kernel of the clifford map $t^*m \otimes s \to s$ defines a subbundle $w$ , say , of $t^*m \otimes s$ . composing the covariant derivative with the projection $\omega^1 ( s ) \to \gamma ( w ) $ defines the penrose operator $p : \gamma ( s ) \to \gamma ( w ) $: explicitly , $$ p_x \psi = \nabla_x \psi + \frac1n x \cdot d\psi $$ for all vector fields $x$ and spinor fields $\psi$ , and where $n = \dim m$ . ( my clifford algebra conventions are $x^2 = - |x|^2$ . ) notice that the " gamma trace " of the penrose operator vanishes . there is a sizeable literature on twistor spinors mostly in riemannian and lorentzian signatures . this is the work of helga baum and collaborators in berlin . a search for " twistor spinors " in mathscinet should give you many links . one important property of the twistor spinor equation is that it is conformally invariant , whence the twistor spinors of conformally related riemannian spin manifolds correspond in a simple way . since you mention maximally symmetric lorentzian manifolds , this observation might be of use because such spaces are conformally flat , hence you can write down the twistor spinors simply by rescaling the twistor spinors in minkowski spacetime . in riemannian signature ( hence for round spheres and hyperbolic spaces ) this is described in the 1990 humboldt university seminarberichte twistor and killing spinors on riemannian manifolds by baum , friedrich , grunewald and kath , later published by teubner .
a backlit sensor is not really a new kind of sensor , it is just a different arrangement of the imaging elements to allow more light into the sensor . from wikipedia : a traditional digital camera sensor consists of a matrix of individual picture elements . each element is constructed in a fashion similar to the human eye , with a lens at the front , sensors at the back , and wiring in between . the front of the detectors require an active matrix to be placed on their front surface . the matrix and its wiring reflects or absorbs some of the incoming light , thereby reducing the signal that is available to be captured . a back-illuminated sensor moves this wiring behind the sensors , similar to a cephalopod eye . they are not without issues though : moving the active matrix transistors to the back of the photosensitive layer normally leads to a host of problems , such as cross-talk , which causes noise , dark current , and color mixing between adjacent pixels . backlit sensors improve the low-light performance of a ccd chip , reducing the iso needed for low-light conditions ( and thus reducing noise ) . this is a clear benefit to astrophotographers , allowing them to use shorter exposures with less noise to get equivalent shots .
there are a lot of factors that go into whether or not a planet has an atmosphere . first , the mass and size of the planet . really what it comes down to is the escape velocity . the higher the escape velocity ( v e ) , the easier it is for a planet ( or moon ) to retain any atmosphere it gets as the gases that make up the atmosphere have to be moving faster to escape . now v e is proportional to the square root of mass divided by radius and mass is proportional to density times radius cubed . since the density of the rocky bodies in the solar system are basically the same ( and the icey bodies only differ by a factor of 2-3 ) density is roughly constant and that means that v e is basically just proportional to the radius of the object . so larger objects have a higher escape velocity and are more likely to retain an atmosphere . next is temperature which is just a measure of the average energy of the particles . so the gas molecules that make up the atmopherers of hot planets have a higher average energy than the molecules in a planet that is cooler . since the average energy translates into the speed of the gas particles , gasses on warmer planets are more likely to be close to or over the escape velocity of the planet and as such can fly off into space . so cooler planets have a higher chance of holding on to their atmospheres since the particles are moving slower . next we have atmopheric composition . at a given temperature more massive particles are going to be moving slower . thus if you have hydrogen gas , with a molecular weight of 2 , compared to oxygen gas , molecular weight 32 , the hydrogen will be moving on average 16 times faster and is much more likely to escape . co 2 , with a molecular weight of 44 , will be moving even slower still . thus for a given planet with a fixed mass and temperature , the lighter gasses will escape first . finally we have availability of materials . if there are no gasses there to start with and no way to acquire them , it does not matter how massive or how cold the planet is , there is nothing there to hold on to .
those are drainage dendrites around the central clear area , much like what you see with rivers as they branch into smaller tributaries upstream . so , at least part of the answer is that the water melting from the snow is joining together into a generally downhill flow towards the central region . furthermore , it is quite likely the central region is a lot warmer because there is no longer any snow there to reflect sunlight . once such a spot forms -- and it has to eventually if there is too much energy coming in -- it will become both a radiating heat source that encourages further melting around it , and also a lower level ( no snow ) that encourages melt water to drain into it , drainage-style . so , some predictions : ( 1 ) this should happen mostly on dark , flat surfaces with little or no slope , such as asphalt parking lots , and ( 2 ) it should happen only when the sun is out .
i did some hunting and followed the paper trail to cook et al . ( 1995 ) . in section 4 , they identify a class of stars that brighten aperiodically . they reckon that these " blue bumpers " are be stars : b-type stars that show strong emission lines . then again , this is off one paper and i am really not sure if this is a widely accepted view , but i imagine there would've been more fanfare if this was an exciting new class of object ? i am not massively learned on the subject of be stars , but the current consensus seems to be that they are rotating very rapidly , to the extent that there is a substantial circumstellar disk of material around the equator . be stars are themselves a type of shell star , all of which are known to show aperiodic variations , presumably because of the unstable nature of the system . in fact , the whole class seems to form an observational problem because of their variable nature and light from the star getting mixed up with the disk .
a square has its diagonals at right angles . so , find the forces along each diag . i.e. , m1 and m3 on m5 for one direction and m2 and m4 on m5 for other . you could actually simpy find difference between , " m1 and m3" and " m2 and m4" and the corresponding charges and use them as resultant mass and charge .
they are looking at s-wave solutions , ie zero angular momentum . they are also looking at modes that are pure tensor with respect to the schwarzchild geometry ( ie they set the vector and scalar parts to zero ) . so the angular parts of the perturbation are just proportional to the angular parts of the background metric ( in a sense the angular parts are proportional to the identity , because the angular momentum is zero ) . the interesting parts of the perturbation then are only allowed to involve $r$ and $t$ , and this is the most general form for the perturbation that mixes $r$ and $t$ but leaves the angular parts alone . the instability comes from the fact that there are solutions that are regular at infinity that blow up at the horizon . see the discussion after equation 11 . there are many tricks to find instabilities . the methods used in the gregory-laflamme paper are certainly valid , but their methods are by no means the only nor the most common ones . generally speaking , the question is whether the split into background + perturbations is a good one in the sense that the perturbations remain small . the perturbations might fail to remain small for many reasons--you might have a runaway potential that becomes infinitely negative ( hamiltonian unbounded below ) or at least pushes you away from your chosen background ( tachyon instability ) , you might have negative kinetic energy ( ghost instability ) , you might have negative gradient energy ( gradient instability ) --there are lots of different ways for instabilities to manifest themselves . here , the method is essentially brute force : numerically construct solutions and show explicitly that there are valid solutions to the perturbation equations where the perturbations become infinitely large .
you have several questions . let 's take them one by one : what is teleported ? in continuous variable ( theoretical ) papers , position and momentum are just convenient denominations for any pair of conjugate continuous variables , in the same way than a qubit can be a 2-level atom , a polarized single photon or a spin-1/2 particle . if a quantum object has continuous unbounded degree of freedom , it behaves like the position ( along a given coordinate axis ) , and there exists a complementary observable which behave like momentum . current experimental realizations include the quadrature of the light , the polarization of a bright beam , collective spin of atomic clouds , etc . therefore , i am not sure that focussing on the position of the particle itself helps in understanding these papers . however , since , as told above , everything is equivalent to a position , the paper should apply to the position of a particle . so in the following , i will assume we are dealing with the position $x$ along the $x$ axis , and $p$ will be the momentum along the same axis . what origin ? the origin does not matter , as long as it is fixed . suppose alice and bob have two different labs , and alice wants to teleport the position and momentum of a particle from her lab to bob 's lab . that means that the initial position of particle a , relative to alice 's apparatus is the same as the final position of particle b relative to bob 's apparatus . in that sense , they can have the " same " position , even if they are in different rooms . of course , if one want to add details one should add that : since we are in the quantum world , " the same position " means that every position measurement would lead to the same measurement statistics . thank to galilean relativity , if alice and bob are moving in respect to each other , the " same momentum " is also to be understood as relative to the movement of each lab . actually , teleporting both position and momentum allows to ' completely ' teleport this degree of freedom , and the statistics of any measurement ( e . g fock-state projection ) will be the same . teleportation procedure i tend to think that continuous variable quantum information is often easier to understand in the heisenberg picture , where the observable operators behave almost like in classical physics initial state alice 's initial particle 's position and momentum are described by the operators $\hat x_a$ and $\hat p_a$ . the entangled pair is described by $\hat x'_a , \hat p'_a$ and $\hat x'_b , \hat p'_b$ . for convenience , i will define $\hat x_\pm=\frac1{\sqrt2} ( \hat x'_a\pm x'_b ) $ and $\hat p_\pm=\frac1{\sqrt2} ( \hat p'_a\pm p'_b ) $ . the entanglement of the pair means that , initially $\hat x_-$ and $\hat p_+$ are both small . it is allowed only if $\hat x_-$ and $\hat p_+$ both have big fluctuations . the bell measurement the bell measurement is a simultaneous measure of $\hat x_m=\hat x_a-\hat x_a'$ and $\hat p_m=\hat p_a+\hat p_a'$ . this measurement is possible because both observables commute . but after the measurement induces a back action on the complementary observables ( $\hat x_a + \hat x_a'$ and $\hat p_a + \hat p_a'$ ) , which become very noisy and cannot be measured later . in particular , the state of the particle a after the measurement does not contain information about the initial state anymore the teleportation itself alice tells bob the values of $x_m$ and $p_m$ , and he moves/accelerates his particle accordingly particle . after this step , we have $$ \hat x_b'^{\text{final}}=\hat x'_b + x_m = \hat x'_b + \hat x_a - \hat x'_a=\hat x_a-\sqrt2 \hat x_- $$ $$ \hat p_b'^{\text{final}}=\hat p'_b + \hat p_m = \hat p'_b + \hat p_a + \hat p'_a=p_a+\sqrt2 \hat p_+ $$ since both $\hat x_-$ and $\hat p_+$ are small , the final observables $\hat x'_b$ and $\hat p'_b$ correspond to the initial observables $\hat x_a$ and $\hat p_a$ . the position and momentum of the first particle have been teleported onto another particle . so if the initial particle was moving ( $\hat p_a\neq0$ ) , then the " target " particle is indeed moving at the end of the process .
you can use a negative charge to test an electric field . you just have to remember that the electric field points antiparallel ( opposite ) to the force on the charge , rather than parallel to it ( in the same direction ) . that is just a convention , though ; we could have defined the electric field to point with the force on a negative charge , and physics would work the same , except for a couple of negative signs in some formulas .
why are you looking for a radial surface . . ? look it as an equipotential surface ( a surface where all points are at same constant electric potential ) as it comes with sphere . hence , you can assume the points a to b as radial to find the potential difference .
a rapid change of volume means that there is little time for heat to escape ( adiabatic process ) . when you compress a gas , you are doing $p \delta v$ work on the gas , and the internal energy of the gas must change . there is nowhere for this energy to go and since the internal energy of the gas is proportional to temperature ( equipartition theorem / ideal gas properties ) the temperature must change . if you compress said gas and then it reach thermal equilibrium with its surroundings , then you can again extract some of this energy in the form of heat , so that it reaches room temperature . now , if you let it expand again , it is temperature will further drop . repeating this process you eventually reach a point in its phase diagram where the gas is a liquid .
the earth receives approximately $6.8\text{mw/m}^2$ of reflected sunlight from the moon ( see below for details of how i calculated that ) . however , the sunlight is also absorbed by the moon and this raise the surface temperature . so the moon also emits thermal radiation towards the earth ( assuming the highest day time temperature of 400k , see comments below for more information ) , $\epsilon_{\text{moon}} ( 1-a ) \sigma ( 400k ) ^4 = 89\text{mw/m}^2$ so the total power received from the moon ( reflected + thermal ) is 10,438 times weaker than sunlight , i.e. $$ \frac{6.8\text{mw/m}^2 + 89\text{mw/m}^2}{1000\text{w/m}^2} = \frac{1}{10438} $$ to answer your question about how much that heats the earth , let 's assume that the average daytime temperature of the earth is 20$^\circ$c and the average nighttime temperature is 10$^\circ$c ( these estimates could be improved , but it does not really change the answer significantly ) . therefore the incident solar energy causes a temperature difference $\delta t=10^\circ$c between night and day . so we know that 1000 $\text{w/m}^2$ ( solar irradiance on the earth surface ) cause a temperature increase of around $10^\circ$c . let 's assume that moonlight will also cause a temperature difference but one that is scaled proportionally by its intensity . moonlight is 10,438 ( reflected and thermal energy ) times weaker than sunlight , the change in temperature of the earth from absorbing moonlight is , $$ \frac{10^\circ c}{10,438} = 958 \mu k $$ good luck measuring that . . . assumptions and method solar irradiance is 1000 $\text{w/m}^2$ at the surface of the moon and the earth . the reflectivity of the moon is about $a=$10% . the solid angle of subtended by the moon in the sky is the same as that subtended by the sun $\epsilon_{\text{moon}} = 6.8\times10^{-5} \text{sr}$ . i say this because during an eclipse they appear to the same size so it is probably quite a good assumption . from 1 and 2 we know that $100\text{w/m}^2$ is reflected at the surface of the moon . from 3 , let 's multiply that by the solid angle subtended by the moon as viewed from the earth as this will give us the amount of the reflected energy that hits the earth . so , $100\text{w/m}^2 \times 6.5\times10^{-5} = 6.5\text{mw/m}^2$ .
the key is the coriolis force . the coriolis force is $f_c = -2m\omega \times v $ . here $\omega$ is the rotation of the frame of reference and $v$ is the linear speed of the satellite . if you do the calculations , left as an exercies for the reader , you will get the missing force . in case 2 the coriolis force is 0 , because the velocity $v$ has to be used in the local frame of reference . and there $ v = 0 $ .
matter is held together by the electrical attraction between the electrons and the nuclei . within the bulk of a solid or liquid , an electron feels these attractions from all directions equally , and therefore the force it experiences equals zero on the average . but if an electron finds itself at the surface , this isotropy is broken . the electron feels attractive forces toward the interior , which are not canceled out by any forces from the outside . normally this causes any electron that impinges on the surface to be reflected back in like a pool ball hitting a cushion . to extract the electron out beyond the surface , you have to do a certain amount of work , called the work function $w$ . the work function for a metal is typically about 5 ev . why does a cathode have to be heated to emit electrons ? it is not actually true that it has to be heated -- cold-cathode devices do exist , and thermionic emission does occur at all temperatures . however , at room temperature , $kt\approx 0.03\ ev$ , which is much smaller than $w$ . that means that only a very tiny fraction of the electrons have more energy than $w$ . the probability of having an energy $w$ at temperature $kt$ goes like $e^{-w/kt}$ , and richardson found in 1901 that the current from a cathode , in the absence of an externally applied electric field , was proportional to $t^2e^{-w/kt}$ .
the is a theorem that says hermitian operator are associated to real eigenvalues . since eigenvalues correspond with our measure values and they are real , this means that it makes sense to have hermitian operators as observables . also , what type of observables are we talking about here ? particles ? observables are magnitudes we can measure : position , momentum , angular momentum , charge , etc .
use the product rule : $$ \begin{align*} \partial_\lambda ( g_{\mu\nu} x^\mu x^\nu ) and = g_{\mu\nu , \lambda} x^\mu x^\nu + g_{\mu\nu} x^\mu_{ , \lambda} x^\nu + g_{\mu\nu} x^\mu x^\nu_{ , \lambda} \\ and = g_{\mu\nu , \lambda} x^\mu x^\nu + g_{\mu\nu} \delta^\mu_\lambda x^\nu + g_{\mu\nu} x^\mu \delta^\nu_\lambda \\ and = g_{\mu\nu , \lambda} x^\mu x^\nu + g_{\lambda\nu} x^\nu + g_{\mu\lambda} x^\mu \\ and = g_{\mu\nu , \lambda} x^\mu x^\nu + 2 g_{\lambda\nu} x^\nu \end{align*} $$ where i have used the convention $t^{\mu . . . }_{\nu . . . , \lambda} \equiv \partial_\lambda t^{\mu . . . }_{\nu . . . }$ , the fact that $x^\mu_{ , \lambda}=\delta^\mu_\lambda$ and the symmetry of $g_{\mu\nu}$ .
if i am only allowed to use one single word to give an oversimplified intuitive reason for the discreteness in quantum mechanics , i would choose the word ' compactness ' . examples : the finite number of states in a compact region of phase space . see e.g. this phys . se post . the discrete spectrum for lie algebra generators of a compact lie group , e.g. angular momentum operators . see also this phys . se post . on the other hand , the position space $\mathbb{r}^3$ in elementary non-relativistic quantum mechanics is not compact , in agreement that we in principle can find the point particle in any continuous position $\vec{r}\in\mathbb{r}^3$ . see also this phys . se post .
nowadays , rockets use a gimbaled thrust system . the rocket nozzles are gimbaled ( an appliance that allows an object such as a ship 's compass , to remain horizontal even as its support tips ) so they can vector the thrust to direct the rocket . in a gimbaled thrust system , the exhaust nozzle of the rocket can be swivelled from side to side . as the nozzle is moved , the direction of the thrust is changed relative to the center of gravity of the rocket . early rockets had vernier thrusters which uses small rocket engines on either sides , to control the altitude of a rocket . nowadays , they are common in most satellites . in this image , the middle rocket shows the normal flight configuration in which the direction of thrust is along the center line of the rocket and through the center of gravity of the rocket . on the left one , the nozzle has been deflected to the left and the thrust line is now inclined to the center line at a gimbal angle $a$ . as the thrust no longer passes through the center of gravity , a torque is generated about the center of gravity and the nose of the rocket turns to the left . if the nozzle is gimbaled back along the center line , the rocket will move to the left . on the right one , the nozzle has been deflected to the right and the nose is moved to the right . wikipedia says , in spacecraft propulsion , rocket engines are generally mounted on a pair of gimbals to allow a single engine to vector thrust about both the pitch and yaw axes ; or sometimes just one axis is provided per engine . to control roll , twin engines with differential pitch or yaw control signals are used to provide torque about the vehicle 's roll axis . the right and left gimbaling is necessary to direct the rocket to its original path , thereby maintaining its stability . . . this link gives a good explanation regarding the stability of rockets . this essay is also good , but it is somewhat big . . .
you need to define an appropriate " order parameter " for your system , one that takes into account the symmetries in the configuration as well ( rotational , transnational , etc ) . there are many ways you could define such " correlator " as you call it , it depends on the system . for example the nematic order parameter in liquid crystals is taken with respect to a vector denoted " the director $\mathbf{n}$" , which can e.g. be the average orientation of the anisotropic rods in the nematic phase ( to simplify here ) , this way the order parameter for each arbitrary rod can be correlated to the director as : $$s \propto \left&lt ; cos^2\theta^{\alpha}-1 \right&gt ; $$ where $\theta$ would be the angle between the direction of the main axis of the rod denoted $\alpha$ and the director . now this was an example for a uniaxial degree of order , but it can be biaxial as well and so on . . . more rigourosly you had define a distribution function for the orientation of rods in your system , and from which you had calculate the correlation function ( usually an average ) that you look for . finally based on the symmetries in the system ( inversion symmetry etc ) the tensor degree of the order parameter is defined ( order 0 a scalar , 1 a vector etc . ) . but this was for liquid crystals , which give a very straightforward idea to how order parameters are defined . now for crystals , you would be more interested in the local deviation of each atom from its lattice position . so the correlation function you are looking for in this case could be the average displacement needed in each degree of freedom of the system to bring back the atom to its lattice position defined by the crystalline structure . in contrast to the earlier example , here we work with the center of mass vectors of each rod or atom instead of the vector along their main axis ( if they are anisotropic i.e. ) . for more references , look into any textbook of solid state physics if you are studying crystals , and for example de gennes ' book for liquid crystals . finally keep in mind that there can be many different forms of order in a system , so the order parameter is defined accordingly ( system specific ) , but they are all referred to as order parameters .
part of the problem is you are trying to take a derivative of the delta function . you can do this inside an integral using a test function . also , have you tried $\delta ( x ) = \lim_{\epsilon\to 0^+}\frac{1}{2\pi\epsilon} e^{-\frac{x^2}{2\epsilon}}$ also , do not drag around the $t'$ and $r'$ , because $\frac{d}{dt} = \frac{d}{d ( t-t' ) }$ , so you are just slowing yourself down giving opportunity for mistakes to creep in . when you are done , just restore the $t'$ and $r'$ . finally , note that $\vec\nabla \delta ( t-|x| ) = -\delta' ( t-|x| ) \frac{\vec{x}}{|x|}$ $\nabla^2 \delta ( t-|x| ) = \delta'' ( t-|x| ) -\delta' ( t-|x| ) \frac{2}{|x|}$ this should be sufficient .
1 ) as rotations in euclidean geometry moves points along circular arcs , boosts in relativity move points along hyperbolic arcs . $\delta s$ is only invariant in the sense that any given point lies on only one hyperbola ( only one level curve of $x^2 - c^2 t^2$ ) which has a unique $\delta s$ with respect to the origin . in other words , comparing between two hyperbolas is not meaningful . 2 ) the quantity is invariant by definition along any single , given hyperbola . i suspect this question was directed more toward comparing between hyperbolas , which as i said is not meaningful . 3 ) no , $\varphi = \tanh^{-1} \beta$ . 4 ) yes , $\varphi = \kappa$ as given . lorentz boosts are indeed the hyperbolic analogues to euclidean rotations . it is thus not uncommon to refer to them as rotations also . you can see the relationship between boosts and rotations through a power series expansion of the exponential function . let $\epsilon^2 =1$ but $\epsilon \neq \pm 1$ . $\epsilon$ is the hyperbolic counterpart to the imaginary number $i$ . the term for this number system is the split-complex numbers . then , the exponential of $\epsilon$ is , $$\begin{align*}e^{\epsilon \varphi} and = 1 + \epsilon \varphi + \frac{\varphi^2}{2 ! } + \frac{\epsilon \varphi^3}{3 ! } \ldots \\ and = \left ( 1 + \frac{\varphi^2}{2 ! } + \ldots \right ) + \epsilon \left ( \varphi + \frac{\varphi^3}{3 ! } + \ldots \right ) \\ and = \cosh \varphi + \epsilon \sinh \varphi\end{align*}$$ this is similar to $e^{i\theta} = \cos \theta + i \sin \theta$ .
it is just a simplifying assumption . it is in fact rarely true . the main problem is that the navier-stokes equations are just tremendously more complex than the maxwell equations--they have chaotic solutions , they couple nonlinearly , they behave wildly differently in the viscous and inviscid limits , and most strikingly , they have no proven existence and uniqueness theorem . so , when learning them , it is best to work out simplified , specialized cases , like the steady flow , irrrotational case ( which can then be solved using methods more analogous to e and m ) , and then to work from there .
what i think is being asked here is about the alternatives theoretically ( and experimentally ) consistent with the cosmological solutions in gr . also some history of terminology . first it would be useful to distinguish between the observable universe and the universe . there are so many theories around that this distinction will help clarify communication ( and understanding ) . of course the observable universe is what is observed and deductions about what is true outside of that ( from some theory ) are not going to be astronomically measurable . now to parts of the question : people say that " the universe is expanding " rather than " observable matter is separating " . if the hubble galactic expansion had been discovered in the decades before einstein 's general relativity ( gr ) , then perhaps the second phrase would have been more common initially . however gr also contains cosmological solutions intended therefore to describe the universe ( and not just the observable one ) . at around the time of hubble 's discoveries the lemaitre-friedmann solution was gaining ground in which gr is seen to provide a specific expansion of the universe from a t=0 singularity . so the early and essentially continual match between theory and astronomical observation has resulted in the first phrase being used as well . how do we know that bb was not : t=0 , v ( universe ) > 0 but v ( matter ) =0 , e=very big ? strictly what describes the t=0 solution of gr is outside of gr . however leaving that point aside there are some comments . v ( universe ) =0 or > 0 this really asks what the singularity can be modelled as in gr . in the days when the closed universe model was prominently discussed it was natural to talk about universe radius and origin at a point for the singularity . however , and especially when the flat or open models seem more observationally accurate , those earlier assumptions do need to be re-considered in accounts . for a flat model say , then the singularity could be an e ^3 - much different from the " point " often discussed ( although other geometries may be possible too ) . however the density of the observable universe 's matter at this point would still tend toward infinity at t=0 and thus occupy v ( matter ) =0 as you have written it . so this leaves a big question about what might be true of the universe as a whole . obviously unknown , but a range of theories abound , including some based on inflation ideas . e= very big ? surprisingly perhaps this need not be true . the reason is that gravitational energy ( a kind of binding energy ) counts negatively . so many theorists prefer the idea that e= small , with e=0 also being popular because then there is no net energy in the universe at t=0 .
waves on strings combine linearly . this means that you can split up a string 's motion into two ( or more ) superimposed waves . the two superimposed waves behave independently , as if the other one was not there . so if you have a standing wave set up on a string , and then you also introduce a travelling pulse , you get something like the following . ( the arrows represent the direction of movement , and the node is marked with a blue dot . ) now to answer your question . i wish i had a way to make the picture animated , but i think you can see it from still pictures . i am going to draw what happens after a short time , when the pulse reaches the node . the standing wave has also moved , and is now swinging back in the other direction . as you can see , the standing wave component still passes through zero at the node , as it always must , but the combined wave ( pulse + standing wave ) does not . because the pulse and the standing wave do not interact , the pulse just passes straight through the node as if it was not there , and the standing wave just keeps waving as if the pulse was not there . note that not interacting is not the same as not interfering . interference happens when two waves get added together and sum to zero , but neither of the two waves is affected by being added in this way , so even when waves interfere , they do not interact .
yep . yep , perfectly fine . this will work . the general method is that the expansion coefficients are given by the inner product $\int \psi^\star_{lm} \psi_0$ where $\psi_{lm}$ are the orthonormal basis functions of your expansion and the integral is over the full space with the natural measure . in your case the basis functions are the spherical harmonics and the measure is $\mathrm{d}\omega \equiv \sin\theta \mathrm{d}\theta \mathrm{d}\phi$ . you can plug in a general spherical harmonic and by doing the integrals get the answer , though in your simple case your method is easier and equivalent . by the way , mathematica knows about spherical harmonics already ( sphericalharmonicy , but make sure it uses the same conventions as you ) . the general method is like finding the fourier series by doing a bunch of integrals . the method you are using is like reading off the fourier series coefficients by rearranging the function itself to look like a fourier series . when you can do it you get the same answer as by doing the integrals , but it is not always easy to do the rearrangement . re edits to the question . q4 . yep , again . : ) q5 . it is as simple as noting that $l^2 y_{lm} = l ( l+1 ) y_{lm}$ and $l_z y_{lm}= m y_{lm}$ ( i am dropping factors $\hbar$ which you can put back yourself if need be ) . just plug your $\psi_0$ in $h\psi_0$ and use the fact that all the operators are linear . it is easy to do term by term and you will see that you get just the answer you have expected . that is because all of these operators $h , l^2 , l_z$ are simultaneously diagonalizable ( because they commute ) and we are working in the basis of their common eigenfunctions .
so if every object is polarized ( positive and negative separation ) , then on a bigger level should not galaxy 's have this too ? in general macroscopic objects are not polarised . for example a single water molecule is polarised - the hydrogen atoms are more positive than the oxygen aton - so the molecule has an electric dipole moment . however in a bowl of water or a block of ice the dipoles from all the $10^{23}$ molecules cancel out and the water/ice has no net dipole moment . so we would not expect an object as big as a galaxy to have a significant dipole moment and this cannot be an explanation for why galaxies are moving away from each other .
snell 's law still applies to the curved surface , but you have to measure the angles of incidence and refraction relative to the surface where the light hits . the image is my attempt to show parallel rays of light falling on a curved surface . even though the rays are parallel , the angle of incidence is different for the two rays because it has to be measured relative to the normal at the point the light strikes the surface . hence the angle $i$ is not the same as the angle $i'$ . response to comment : it has become clear from the comments that the problem is that the value of $n$ depends on whether the light is passing from the air to glass or from glass to air . to be precise the two refractive indices are reciprocals of each other i.e. $$ n_{air-glass} = \frac{1}{n_{glass-air}} $$ the refraction of the light ray happens because the speed of light , and therefore the wavelength , changes when the light enters and leaves the glass . the refractive index when a light ray passes from a medium 1 to a medium 2 is : $$ n_{1-2} = \frac{v_1}{v_2} $$ where $v_1$ is the speed of light in medium 1 and $v_2$ is the speed of light in medium 2 . so in our example the refractive index when passing from medium 2 to medium 1 is : $$ n_{2-1} = \frac{v_2}{v_1} $$ i.e. $$ n_{1-2} = \frac{1}{n_{2-1}} $$
( i will assume in my answer that people have read the discussion on the old question , linked to by the op . ) no , it is not like the aether . it is still true that locally , there is no preferred reference frame . you do not even really need to think about spacetime to see what is going on . consider a two-dimensional plane , parametrised by $ ( x , y ) $ , and roll it into a cylinder by identifying $ ( x , y ) \sim ( x + nl , y ) ~\forall~ n$ , where $l$ is some constant . locally , this space is still perfectly isotropic , but globally , the $x$ direction has been picked out by the identification . to see what this means , let 's imagine drawing two straight line segments , each beginning at $ ( x , y ) = ( 0,0 ) $ and ending at $ ( 0 , l ) $ . the first will just be $ ( 0 , t ) ~ , ~ 0\leq t\leq l$ , and the other will be $ ( t , t ) ~ , ~0\leq t\leq l$ ( which ends at a point equivalent to $ ( 0 , l ) $ under the identification , and therefore the same point on the cylinder ) . obviously the length of the first line is just $l$ , but the length of the second line is $\sqrt{2}l$ , by pythagoras . although any small patch of the cylinder is perfectly isotropic , we see here that the rotational symmetry is broken globally by the identification . in spacetime , a similar thing happens , replacing rotational symmetry by boost symmetry . short answer : generally speaking , there is never a twin paradox : in any spacetime , just write down the metric in any coordinate system you like , and calculate the proper times for the two trajectories of interest . this tells you unambiguously which twin is older and which younger .
you can add a gauss-bonnet term $r^2 - 4r^{\mu\nu}r_{\mu\nu} + r^{\mu\nu\rho\sigma}r_{\mu\nu\rho\sigma}$ . it is purely topological .
a good example for gravity along these lines is the relativistic cosmic string , or the equivalent point particle in 2+1 dimensions . the exact solution of gr in the presence of a cosmic string is a deficit angle--- meaning that spacetime is flat except for a wedge cut out of it and the two sides identified . this can be found by solving the 2+1 einstein equations , but it is easier by just noting that in three dimensions , the riemann tensor is determined by the ricci tensor ( it has the same number of independent components , six ) , so wherever the ricci tensor is zero , spacetime is flat . now you can ask what is the phase for a particle to go around the cosmic string . if the particle goes two ways around the string , with momentum perpendicular to the string axis , and interferes with itself , you get an extra relative phase equal to the momentum times the impact parameter ( distance to the string ) times the cutout angle , which is the mass density of the string . this gives an aharonov-bohm like phase , equal to the 2+1 mass contained inside the interference region . if you superpose strings , you add up the deficit angle , and the deficit angle is determined by the phase independent of the location of the particle inside the loop ( much like the magnetic flux ) . i think this is the best analog of aharanov bohm in gravity . the best reference is t'hooft 's article on 2+1 gravity , which is one of these papers ( i read it in a reprint collection called " under the spell of the gauge principle " , and i do not remember the title , but the third article listed below is available online , and the relations inside are the ones i am referring to ) causality in ( 2+1 ) -dimensional gravity . class . quantum grav . 9 ( 1992 ) 1335-1348 classical n-particle cosmology in 2+1 dimensions . class . quantum grav . 10 ( 1993 ) s79-s91 the evolution of gravitating point particles in 2+1 dimensions . class . quantum grav . 10 ( 1993 ) 1023-1038 . http://www.staff.science.uu.nl/~hooft101/gthpub/evolution_2plus1_dim.pdf this is the basis of later work on the quantum version by t'hooft , witten and many others . it is a very active topic in matheamtical physics now , because of the intersection with topological field theory , quantum groups , and knots .
your assumption is very wrong . gravity is everywhere . or simply , the box has mass . it exerts its very own gravity . if your 1st location has the field , then the 2nd location is also influenced by it unless it is at $\infty$ . you can not just shield anything from gravitational field . so , energy is still conserved when you are moving the box from one place to another . . .
a hint $$ dxf ( x ) = f ( x ) +xdf ( x ) $$ $$ xdf ( x ) $$ take the diference and you get $ f ( x ) $ or $ 1 . f ( x ) $
waves on a string are transverse waves not longitudinal waves . they are not variations in pressure , but variations in the displacement of the string . the ( average ) displacement is greatest at the anti-nodes and zero at the nodes .
note : $r_{ab}= ( -i-3j+3k ) m$ . so , $$ v_{ab}=- ( 2i+3j+4k ) \cdot ( -i-3j+3k ) $$ or , $v_{ab}= - ( -2-9+12 ) $ volt ( s ) or , $v_{ab}= -1$ volt .
conservation of energy does not apply to this situation because the energy you measure when at rest with respect to the source and the energy you measure when moving with respect to the source are in different reference frames . energy is not conserved between different reference frames ; in other words , if you are going to use conservation of energy , you have to make all your measurements without changing velocity . in fact , it is kind of misleading to say that energy increases or decreases due to a doppler shift , because that would imply that there is some physical process changing the energy of the photon . that is really not the case here , it is simply that energy is a quantity for which the value you measure depends on how you measure it . for more information , have a look at is kinetic energy a relative quantity ? will it make inconsistent equations when applying it to the conservation of energy equations ? .
some energy densities for easily compactified substances : purely electric storage : electric field in a capacitor , 0.000 36 mj/kg electric field in a supercapacitor , up to 0.004 2 mj/kg 1 t magnetic field has energy density $\frac{1}{2\mu_0}b^2 = 0.4\ , \mathrm{mj/m^3} $ , estimate mass for superconducting magnet enclosure to decide where to put this on the list ( pretty low ) chemical storage ( atoms are allowed to trade electrons ) : lead-acid batteries , 0.15 mj/kg nickel metal-hydride batteries , 0.30 mj/kg alkaline batteries , 0.40 mj/kg lithium-ion batteries , 0.46 mj/kg theoretical maximum for li/cucl batteries [ via ] , 4.2 mj/kg carbohydrates or proteins , 17 mj/kg coal , 24 mj/kg fats , 38 mj/kg combustion of pure lithium , 40 mj/kg crude oil or gasoline , 45 mj/kg methane , 55 mj/kg ( caveat : gaseous at standard temperature and pressure ) hydrogen for combustion , 140 mj/kg ( also gaseous at stp ) nuclear storage ( atoms are allowed to trade nucleons ) : uranium , typical reactor , 384 000 mj/kg thorium , uranium , ( breeder reactors ) , 80 000 000 mj/kg = 80 x 10 6  mj/kg . this astounding number seems to assume that all of the fuel is consumed , which is not how real reactors work . antimatter , avogadro 's number per gram * 1 gev = 96 x 10 9  mj/kg
your reasoning seems to be that because $\frac{t}{\sqrt{1-v^2}}\to \infty$ when $v\to 1$ , it must be the case that $\frac{t}{\sqrt{1-v^2}}\to 0$ as $v\to 0$ . but in fact when $v=0$ we have $$ \frac{t}{\sqrt{1-v^2}} = \frac{t}{\sqrt{1-0^2}} = \frac{t}{\sqrt{1}} = \frac{t}{{1}} = t . $$ so time does not pass infinitely rapidly but instead passes at exactly its normal rate , as measured by a clock attached to the stationary observer . one should hope that this would be the case !
in your solution you seem to make the assumption that the terminal velocity in the y-direction is zero . this produces the wrong answer . this is how i would solve the problem : first , let 's note that the initial velocity in both x- and y-direction are the same ( due to the $45^{\circ}$ angle ) . let 's call it $v$ . the distance traveled in the x-direction , $d$ , when the ball hit is the ground is given by : $$ d=vt $$ where $t$ is the time of flight . when the ball hits the ground , its velocity in the y-direction will be $-v$ . this means that its velocity has changed by $2v$ ( or rather by $-2v$ ) . hence we also have : $$ 2v=gt $$ substituting for $v$ gives : $$ d=\frac{gt^2}{2} $$ which solved for $t$ gives : $$ t=\sqrt{\frac{2d}{g}}=\sqrt{\frac{2\cdot 180}{9.8}}\approx 6.06\ , \rm{s} $$
the depletion region forms due to the equilibrium between drift ( field driven ) and diffusion ( concentration gradient driven ) currents . if you have very low doping , the depletion region will be large because a large volume of depleted semiconductor is needed to generate enough electric field to balance the diffusion current . on the other hand , if you have very high doping a much smaller region is required to balance the diffusion of carriers . i was actually doing this calculation last week , here are some results from my simulation of the poisson-boltzmann equation for a ge/gaas pn-junction for different doping levels . i have indicated the approximate width of the depletion layer with the blue background . the blue and greens lines are the conduction and valance bands , the red and light blue lines are the intrinsic fermi-level and the fermi-level , respectively . the top plot is for light doping $10^{16}\text{cm}^{-3}$ , the bottom is for higher doping $10^{17}\text{cm}^{-3}$ . this width of the depletion with applied bias was discussed here in an avalanche breakdown , where are the electrons that break free from ? , so that might also be interesting .
as the comments explained , you need to know a few properties of the $\gamma$ matrices . first of all , from $$ \{\gamma_\mu , \gamma_\nu\} = 2 \eta_{\mu \nu} \mathbf{1}_4$$ you can infer that ( depending on the metric but not on the representation of the dirac algebra ! ) in ( +--- ) metric $\gamma_0$ is hermitian ( hint : look at the $\mu = 0 , \nu = 0$ component of the above equation ) , while the $\gamma_i$ ( $i = 1 , 2 , 3$ ) are anti-hermitian . ( in -+++ metric , this would be interchanged ) . and this should allow you to solve the problem . the hermicity properties can be condensed into $$ \gamma_\mu^\dagger = \gamma_0 \gamma_\mu \gamma_0$$ which just reproduces the above if you take the commutation properties into account .
the moment of a vectorfield $\vec{v}$ at a position $\vec{r}$ is equal to $$\vec{r}\times\vec{v} . $$ so torque is simply a special case where the vectorfield we look at is the force field , $\vec{v} = \vec{f}$ . another way of saying this is that torque is the moment of force .
the difference between fermi energy and fermi level is described in a previously asked question . to summarize , you are correct that the fermi level is the energy at which ( in a 0k system ) all of the lower energy states are occupied while all of the higher energy states are unoccupied . for a system not at 0k , the fermi energy is the energy state that has a 50% likelihood of occupation . with nonzero temperature we may only talk about the probability distribution of electrons in energy levels . the likelyhood of an electron to be in an energy state $\mathcal{e}$ is $f ( \mathcal{e} ) $ , where $f$ is the fermi function . the work function is defined as the energy necessary to remove an electron from the surface of a metal : $w = -e\phi - e_f$ . here , $e\phi$ is the energy of the electron just outside of the surface of the metal ( $\phi$ is the electric potential just outside the surface ) and $e_f$ is the fermi energy . recall that there are electrons with energies higher than $e_f$ and there are electrons with lower energy than $e_f$-- for these electrons it is easier ( harder ) to remove them from the metal . by only taking $e_f$ into account the work function is effectively averaging over all possible energy states weighted by the probability of the state being occupied . getting to your first question , the equation $w = |fe + fl|$ does not make sense to me , because in many systems the fermi level and energies are similar if not identical , i do not see what adding them would accomplish and why this would yield the work function . also , the work function is very heavily dependent on the surface properties of the metal , which are taken into account via the potential $\phi$ just outside the surface . i do not see how surface properties are taken into account in the equation your ta wrote . now , because for non-0k systems you can only talk about the probability of an energy level being occupied , your band diagram is a bit oversimplified for our discussion . see instead the band diagram in the " fermi function " section here . this band diagram is that of a semiconductor , which you can tell because the band gap is small enough for the fermi function ( read : probability of energy state occupation ) to be nonzero in the conduction band . however , the point at which the probability of occupation is 50% ( read : the fermi energy ) is the same as the fermi level for all temperatures . notice how the fermi function stretches out with increasing temperature , causing an increase in likelihood of electrons occupying the semiconducting band -- this is the picture of a semiconductor that i carry in my head ( and also clearly explains why heat increases conductivity ) . another minor correction to your picture is that the energy of an electron just outside of the surface of the metal is not necessarily zero ( it is $e\phi$ ) , the work function is the energy difference between being just outside of the metal and being at the fermi level , and is not the difference between being infinitely far away ( 0 energy ) and being at the fermi level .
with the exception of stopping targets , there is going to be a portion of the beam which continues down the beam pipe ( i.e. . misses even the innermost elements of the detector ) , which would usually be counted as " unscattered " for that purpose . in that case the integration is often over the solid angle covered by active detector elements .
the energy needed to remove an electron from a solid is called the work function . for most metals you would need uv photons ( 300 nm for aluminium ) that rarely reach the earth 's surface . visible light can eject electrons from alkali metals , but the quantum yield ( the probability of electron emission per incident photon ) for pure metals is low ( probably less than 1% ) . materials like cste that are used in photocathodes have efficiency up to 40% ( at certain wavelength ) but they are expensive and difficult to handle in open air . silicon solar cells also utilize photoelectric effect and compared to metals they are efficient and inexpensive .
the answer is that $\rho$ must be spherically symmetric ( necessary and sufficient condition ) . to show it , let me change notation . now $r$ ad $r'$ are the absolute values of the vectors $\vec{r}$ , $\vec{r'}$ and we know that $$\phi ( \vec{r} ) := \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||\vec{r}-\vec{r'}||_2} dr ' . \tag{0}$$ where $$||\vec{r}-\vec{r'}||_2 = \sqrt{r^2+{r'}^2-2rr'\cos ( \theta' ) }\tag{1}\: , $$ $\theta'$ being the polar angle of $\vec{r'}$ . from the general theory we know that it also hold $$\phi ( \vec{r} ) := \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||\vec{r}-\vec{r'}||} dr'\tag{2}$$ where $$||\vec{r}-\vec{r'}|| = \sqrt{r^2+{r'}^2-2rr'\cos ( \alpha ) }\tag{3}\: , $$ $\alpha$ being the angle between $\vec{r'}$ and $\vec{r}$ . comparing ( 1 ) and ( 3 ) , we conclude that $$\phi ( \vec{r} ) = \phi ( r \vec{e}_z ) = f ( r ) $$ consequently we have that $$\phi ( \vec{r} ) = f ( r ) \: . $$ since , for some constant depending on the unit system , $\kappa \delta \phi ( \vec{r} ) = \rho ( \vec{r} ) $ , we have that $$\rho ( \vec{r} ) = \kappa \delta f ( r ) \: . $$ in other words $\rho$ must necessarily be a spherically symmetric function . the found condition is also sufficient . indeed , if $\rho$ is spherically symmetric , using the rotational invariance of the measure and the standard distance , it easily arises that the right-hand side of ( 2 ) can be re-written as the right-hand side of ( 0 ) : $$\phi ( \vec{r} ) := \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||\vec{r}-\vec{r'}||} dr ' = \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{rr'} ) }{||\vec{r}-\vec{rr'}||} drr'= \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||\vec{r}-\vec{rr'}||} dr'= \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||\vec{r^{-1}r}-\vec{r'}||} dr ' = \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||r\vec{e}_z-\vec{r'}||} dr ' = \frac{1}{4 \pi \varepsilon_0}\int_{\mathbb{r}^3} \frac{\rho ( \vec{r'} ) }{||\vec{r}-\vec{r'}||_2} dr'\: . $$
i assume you mean the critical exponents of the velocity correlation functions ? first of all , i do not think they have been derived using fluid/gravity ( this is a very difficult problem ) , at best the problem was mapped onto a different problem . there was a series of papers by oz and others about incompressible ( and compressible ) navier-stokes some years ago , see for example http://arxiv.org/abs/0905.3638 , http://arxiv.org/abs/0906.4999 , http://arxiv.org/abs/0909.3574 . there are also somewhat more handwaving efforts like http://arxiv.org/abs/1005.3254 .
start with a plane-wave at rest with its spin pointing in the z-direction : $\psi_l=\psi_r=\left ( \begin{array}{c} 1\\ 0\end{array}\right ) $ first rotate the spin to the required direction . $\psi_l \rightarrow \exp\left ( -\tfrac12 i\theta_i\ , \sigma^i\right ) \psi_l ~~~~~~~~~~~~ \psi_r \rightarrow \exp\left ( -\tfrac12 i\theta_i\ , \sigma^i\right ) \psi_r$ ( find the euler angles $\theta_i$ by transforming the spin-vector to the rest-frame ) now boost the rotated spinors in the right direction . $\psi_l \rightarrow \exp\left ( -\tfrac12\vartheta_i\ , \sigma^i\right ) \psi_l ~~~~~~~~~~~~ \psi_r \rightarrow \exp\left ( +\tfrac12\vartheta_i\ , \sigma^i\right ) \psi_r$ ( calculate the rapidity $\vartheta_i$ from the momentum ) finally : note that you can expand : $\exp ( i\phi_i\sigma^i ) \longrightarrow \cos|\phi|+i \phi_i\sigma^i \sin|\phi|\ , /\ , |\phi|$ to get the matrices out of the exponent 's argument hans
addressing just the last two paragraphs of your question : if $t$ is " non-degenerate " ( at every point as a linear transformation from $t_pm \to t_p^*m$ ) , then modulo some issues with the constraint equations $g$ can be solved from $t$ , at least locally in time , after prescribing it in a compatible way on a space-like hypersurface . see this article by de turck . presumeably the issue with degeneracy has more to do with the method involved in solving the problem , then with it being a critical condition , since the evolution for ricci-flat metrics ( einstein vacuum equations ) is also well-posed . the main difficulty is about the " univocal " part : due to diffeomorphism invariance , the " local " problem for prescribing ricci curvature is under-determined . ( the global problem may have topological constraints ; that i am not sure about . ) so one expects that for the local problem the issue is more likely the overabundance of solutions . on the other hand , the prescription of initial/boundary data may be crucial . roughly speaking the prescribing ricci curvature equation is analogous to the inhomogeneous wave equation with a source term . in which case the hyperbolic nature of the equations shows that for any initial configuration there exists a different evolution . so it may be that this will lead to multiple different metrics compatible with your $t$ ( indeed consider the case where you restrict your manifold to be $m = \mathbb{r}^4$ ; the global nonlinear stability of minkowski space , combined with classical results on classical results on the existence of solutions to the vacuum constraint equations shows that on $m = \mathbb{r}^4$ there are many incongruent ( since for these other solutions the weyl tensor is non-vanishing ) solutions to einstein 's equation when $t = 0$ ; i would expect something similar for the case of prescribed $t$ ) .
yes , for example heat energy is transferred through three different means : radiation ( which is wave like ) conduction ( which is not wave like ) convection ( which is not wave like ) of course , according to the standard model , everything has wave-like properties , so in a way there is no escape from waves , but i believe that your question is best answered with classical phenomena .
it can be used to measure speed - that is how police radar guns and speed cameras work . radar waves from the gun/ camera are reflected off the moving vehicle , and the wavelength is shifted according to the speed of the vehicle relative to the gun/ camera . in astrophysics , looking at light from distant galaxies , we notice that certain characteristics of the light are shifted in wavelength due to the doppler effect . this is known as red-shift , as we notice the light is mostly shifted to the longer-wavelength ( red ) end of the spectrum . this tells us that distant galaxies are moving away from us , which is the primary piece of information that led to the development of the big bang theory .
when applying f = m*a , the mass used is simply the mass of the object that the forces are acting on . in this case , it is indeed the mass of the person . you are right in that the weight of the person ( 650 n ) is equal to the mass of the person times to force felt by him due to gravity . dividing by gravitational acceleration will give you his mass . since the normal force on the passenger from the floor is 620 n ( lower than the normal force he would feel from the floor in a stationary elevator , which would be equal to his weight ) , intuition says that the floor of the elevator is accelerating in the opposite direction of the normal force . yeah , everything looks good to me .
$p=iv$ where , $p$ is power , $v$ is voltage , and $i$ is current in amps . $e=pt$ where $e$ is energy and $t$ is time .
it appears to me the issue is understanding momentum conservation . an even cruder example would be to shine a bright torch out the back of your vehicle . even though the photons have no mass , would not the vehicle move forward ? you also refer to mass in this manner in the paraphrasing of newton 's third law " proportional opposite mass/acceleration ratio in the opposite direction " . it is not the mass that matters here . newton 's third law can be thought of as a statement of conservation of momentum . despite having no invariant mass , photons do have momentum . so a light sail , or a flash light in your case , works because to conserve momentum if light is reflected off your craft or emitted from your craft , your momentum must change to compensate for the change in momentum of the light or the momentum the light carried away . noether 's theorem is an even stronger statement , which shows that if we can describe the physics in a manner that does not depend on position ( ie . you could redo this experiment 1km to the right and it would not effect the results ) , then momentum must be conserved . so this forbids a reactionless drive in special relativity as well . spatial translation symmetry becomes a bit messy in gr , since the spacetime itself is dynamic . so your gravity wave idea could work in principle ( alcubierre drive ) . however it is possible to formulate a type of energy and momentum conservation with pseudo-tensors in gr ( this is how einstein discussed it ) . in this view we can keep track of the energy and momentum of gravity waves as well , and so we can still use momentum conservation for these scenarios as well . so your question : why is it impossible to move a mass in a given direction without a proportional change in the opposite direction ? the answer is : momentum conservation forbids a reactionless drive , and momentum conservation itself follows from the translational symmetry of the physics describing a closed system . so like free energy machines , we do not even need to see the details to know something is a misrepresentation or a scam . we can reject such " inventions " on very general grounds . and indeed , the u.s. patent office explicitly will not review a patent on a perpetual motion machine , or a device that could be used to build such a machine . they may have a similar restriction on " reactionless drives " , but i am not sure of that .
is there always a frame in which spatially separated events are simultaneous ? the answer is no . two events that are spatially separated in one frame of reference ( 1 ) will be co-located in another frame of reference and not simultaneous in any frame if the interval is time-like ( 2 ) will be simultaneous in another frame of reference and not co-located in any frame if the interval is space-like . ( 3 ) will be neither co-located nor simultaneous in any other frame if the interval is light-like . time-like interval if the interval is time-like , the separation in time , $|c\delta t|$ , is larger than the separation in space $|\delta x|$: $$|c\delta t| \gt |\delta x|$$ thus , there is a frame of reference in which $\delta x ' = 0$ ; the two events are co-located in this frame . space-like interval if the interval is space-like , the separation in time is less than the separation in space : $$|c\delta t| \lt |\delta x|$$ thus , there is a frame of reference in which $c\delta t ' = 0$ ; the two events are simultaneous in this frame . light-like interval if the interval is light-like the separation in time equals the separation in space : $$|c\delta t| = |\delta x|$$ thus , in all frames of reference , the events are neither co-located nor simultaneous , i.e. , $$|c\delta t'| = |\delta x'|$$ all of this follows directly from the lorentz transformation . let 's take your example of two events with spatial separation of a tennis court so $$|\delta x| = 78\mathrm m$$ light travels this distance in $\delta t_c = \frac{78}{300 \cdot 10^6} = 260\mathrm{ns}$ thus , if the two events occur within 260ns in this frame of reference , the events have space-like interval and are thus simultaneous in another , relatively moving reference frame of reference . since , in your example , the events occur 1 day apart , the events have time-like interval and cannot be simultaneous in any reference frame .
the operational macrosopic definition of pressure in a solid is the same as that for a gas : when the solid 's wall is in mechanical equilibrium with its surroundings , that means the force exerted on the wall by the surroundings is exactly compensated by the pressure within the solid times the area of the wall .
the casimir effect is analogous to gravity in only one way--- it has a negative energy which varies as a power of the distance . in all other ways , it is different . the power-law is different , the cause is different , it is a quantum effect , not a classical effect , and the mediator is the electromagnetic field , not the gravitational field . negative energy is not surprising--- it just means that there is an attractive force ( and zero potential at infinity ) , so that the potential energy is less than when the objects are far apart . in this respect , both casimir and gravity have negative energies . this energy is an energy difference between two configurations . the total energy in the casimir system is still positive , it is just less than if you take the two plates to infinity . similarly , the total energy in a gravitating system is also positive , but when objects are close , it is less than the energy than when they are far . this is the negative energy in gravity . for the case of casimir forces , there are two ways to view it . the more primitive way is to consider the fluctuating polarizations in a quantum system that lead to attractive forces . these attractions come in two types , depending on the ratio of the distance to ( c times ) the orbital period of the electrons ( the difference in energy levels converted into a time ) : london forces : when the atoms are significantly closer together than the wavelength of light that they would emit between the first excited state and the ground state , the electromagnetic interaction is instantaneous coulomb interaction . the coulomb interaction correlates the charge fluctuations on the two atoms so that they tend to be polarized in the same direction , and this correlated polarization leads to a $1/r^6$ attractive force between points separated by a distance r , or a $1/r^4$ force between two sheets separated by distance r . van-der-waals forces : when the atoms are further than the typical wavelength of light , the electrostatic force is no longer instantaneous , and the correlated force is weaker than you get from the statically correlated system . the force falls off as $1/r^7$ for points , and $1/r^5$ for sheets . this calculation is harder than the previous one , but when casimir calculated the effect , it revealed itself to be more universal than the london-forces , it looked like it does not care as much about the energy levels , that it only depends on polarizability of each object separately . bohr suggested to casimir that the universality could be understood because the attraction was due to the correlations in the quantum field surrounding the materials , and that he should calculate the force only by the vacuum energy in the surrounding field modes . this calculation works , and produces the correct form of the van-der-waals force , and explains the universality . but the effect is not hard to understand--- the casimir effect is the van-der-waals attraction between two plates separated by a distance large enough so that retardation effects are important . in dirac gauge , you have electrostatic forces and photons , and the electrostatic force is not important , and the photon vacuum energy reduction is just due to the different photon modes that are allowed at different distances . this is not mysterious , and it should not be presented as mysterious . but it is a source of endless pseudoscience about machines that extract vacuum energy . the casimir force does not allow you to extract any more energy than any other attractive effective interaction .
you should use the fact that if it is a lens , all the paths to the focal point from a plane perpendicular to the x-axis at some distance behind the lens will take the same time to cross . you can approximate the paths as straight for small deflections , and the extra time from the varying index has to compensate for the extra length from the pythagorean theorem to the focal point .
no . the integral $\int r^2 dv$ . it is not $v$ . $\int dv$=$v$ .
a very high energy gamma ray spontaneously pair-produces a particle and anti-particle , the idea being that the gamma ray has enough energy that a decay into matter is feasible . the particle and anti-particle which are created are still very high energy - they have velocities near the speed of light in a vacuum . whenever a particle flies through a substance at a velocity higher than the speed of light in that substance , it emits cherenkov radiation . the typical analogy given is that this is like a sonic boom : in a sonic boom , distinctive waves are produced when something flies through a substance at higher than the speed of sound in that substance ; with cherenkov radiation , the waves are produced by flying through at more than the speed of light . the particle and antiparticle might collide with stuff in the atmosphere , producing high-energy photons ; these high-energy photons can pair-produce again . this way , the cherenkov radiation amplifies ; a small burst of cherenkov light is produced whenever a gamma ray enters the atmosphere . the cherenkov radiation produced by gamma rays has a distinctive pattern that can be detected using a photomultiplier tube . with an array of these detectors , you can observe the shower from several points . then , you work backwards to figure out where the original gamma ray came from and what energy it had ( much easier said than done ! ) .
you asked about the second equation . see below : $e^{ix}{}= 1 + ix + \frac{ ( ix ) ^2}{2 ! } + \frac{ ( ix ) ^3}{3 ! } + \frac{ ( ix ) ^4}{4 ! } + \frac{ ( ix ) ^5}{5 ! } + \frac{ ( ix ) ^6}{6 ! } + \frac{ ( ix ) ^7}{7 ! } + \frac{ ( ix ) ^8}{8 ! } + \cdots \\ [ 8pt ] {}= 1 + ix - \frac{x^2}{2 ! } - \frac{ix^3}{3 ! } + \frac{x^4}{4 ! } + \frac{ix^5}{5 ! } - \frac{x^6}{6 ! } - \frac{ix^7}{7 ! } + \frac{x^8}{8 ! } + \cdots \\ [ 8pt ] {}= \left ( 1 - \frac{x^2}{2 ! } + \frac{x^4}{4 ! } - \frac{x^6}{6 ! } + \frac{x^8}{8 ! } - \cdots \right ) + i\left ( x - \frac{x^3}{3 ! } + \frac{x^5}{5 ! } - \frac{x^7}{7 ! } + \cdots \right ) \\ [ 8pt ] {}= \cos x + i\sin x \ . $ to calculate the expansions i have used in the above equation , you need to understand the procedure for finding taylor expansions of functions . this youtube video teaches the procedure : http://www.youtube.com/watch?v=gutltrdox3c
the reason for having two prongs is that they oscillate in antiphase . that is , instead of both moving to the left , then both moving to the right , and so on , they oscillate " in and out " - they move towards each other then move away from each other , then towards , etc . that means that the bit you hold does not vibrate at all , even though the prongs do . you might ask why it is that they do that , instead of oscillating in the same direction as one another . the answer is that at first they oscillate in both ways at the same time , but the side-to-side oscillations are rapidly damped by your hand , so they die out quickly , whereas the in-and-out ones are not damped this way , so they ring on long enough to hear them . an excellent illustration of this can be seen in this video of a fem model of a two-pronged fork , which shows you all the vibrational modes separately . ( hat tip to ghoppe , who posted this video in a comment . ) having a third prong would not help very much with reducing damping . there are ( at least ) three different ways a three-pronged fork could vibrate : one with all three vibrating side-to-side in phase with one another , and two where one of the prongs stays still and the other two vibrate in and out . ( at first i thought there would be three of this latter type of mode , but the third can be formed from a linear combination of the other two : $ ( 1,0 , -1 ) - ( 1 , -1,0 ) = ( 0,1 , -1 ) $ . ) the vibrational mode in which everything moves in the same direction would be damped by your hand , and some combination of the other two would continue to sound for a while . as ilmari karonen pointed out in a comment , there would also be a " transverse " $ ( 1 , -2,1 ) $ mode , where prongs vibrate out of the plane of the fork . this mode would not necessarily have the same frequency as the primary in-and-out mode , so it is probably something we had want to avoid . but ultimately there is little reason why , in a three-pronged fork , the vibrations would ring on longer than just two prongs - there would be the same amount of vibrational energy as in a two-pronged fork , but just shared between two or three modes of vibration instead of one .
this is getting complicated . : ) you have to make a lot of assumptions to make progress ; you have listed most of them . i will explicitly add an assumption that we consider only rayleigh scattering ( more-or-less consistent with " clear sky" ) , and that the atmosphere is pure n${}_2$ . since this is homework , i will not spill all the beans . what you need is the cross section for rayleigh scattering : how much light does each molecule remove from the incident light . given that , then you have to figure out what to do with it . this will involve figuring out how many molecules are in the way between the sun and your detector . i will say that there are web sites that present the results you need without your having to do the calculations yourself . of course , this will not give you any of the absorption bands that show up in your chart . and , of course , it will not account for the fact that the light hitting the top of the atmosphere only approximately follows the black body curve . i have not done the exercise myself , but i suspect you will end up reproducing one of the main features of those data . update since we are beyond homework : this wikipedia page gives the rayleigh scattering cross section for n${}_2$ , and a value for molecular density , but it does not say if it is the average value or the value at the earth 's surface . nonetheless , that number can help in making order-of-magnitude estimates . at any rate , rayleigh scattering goes as $\lambda^{-4}$ , so when you take that into account you will multiply your result by $a\lambda^{-4}$ where a is some pre-factor that you do not know yet . you can try to make an estimate of the pre-factor using the data on that wikipedia page , and perhaps some knowledges or guesses about the thickness of the atmosphere . ( or you can simply try different values for $a$ until you find one that works . ) you might be able to model the fact that the solar data matches the b.b. curve for high frequencies , but not at lower frequencies .
hawking radiation is a very robust prediction . it comes simply from applying quantum field theory in the curved space-time near the event horizon . it is also part of the synthesis called " black hole thermodynamics " , for which string theory provides an explanation in terms of the statistical mechanics of microstates . in the s-matrix of quantum gravity , if black holes did not evaporate , they had show up as asymptotic states , but they do not . ( there are eternal black holes in anti de sitter space , but they still evaporate , they just do not get to evaporate completely ; the particles produced by the evaporation can not escape to infinity because of the peculiarities of ads geometry , and fall in again . ) so denying the existence of hawking radiation would screw up many other things . you could say that hawking radiation is real but that it falls back in , like in ads space , but there is no reason for it to do so . the paper featured at arxivblog is a " what if " paper which ignores all these problems and proceeds to calculate some of the consequences . you could compare it to an engineering study of one of m.c. escher 's impossible structures : if you ignore the contradictions in its design , maybe you can calculate some of its properties , but it only has recreational value to do so . we do not quite know that a nonevaporating quantum black hole is logically impossible , in the way that we know the impossible staircase is impossible , but in the future a genuine proof may be available . but empirical confirmation of black hole evaporation is rather unlikely . if we could produce mini black holes in colliders , then we had see it , but those models are not especially favored ; they are a " what if " of a different sort , one in which there is at least a consistent fundamental picture behind the hypothesis ( particular braneworld models ) , but it is just one of many possibilities about what happens at the next frontier of physics and those models are not significantly favored . ( these models are also the ones which predict a detectable signature in grb data . ) if we could send a probe to the edge of an astrophysical black hole , maybe the radiation could be detected , but that is a job for interstellar civilizations , if they exist . maybe you could find indirect evidence for hawking evaporation of primordial black holes in the cosmic microwave background . but i do not know how likely that is - again , it would be highly model-dependent .
although i have not researched it , you can learn some physics and try to get into computational physics . there is some industry for it out there ( e . g the materials sector , though i would doubt they hire anything but phds ) . however , if you are a really great programmer and bring in your ideas and experience creating software , with the additional knowledge of being able to conduct simulations of physical systems and conduct solid quantitative analysis , then why not . there is also a market for physics specific software . for example , you can learn electrodynamics and create a ( hopefully open source or atleast free ; ) ) counterpart to simion . if developing independent software alternative is too much , you can contribute by creating physics modules , writing patches etc . to products like sage . there are more possible places where you can develop , off the top of my mind the root develeopment team at cern has two non-physicists working for them . the best strategy i would recommend is to start learning basic physics , and simultaneously research what is going on at the interface of physics and computation . one guide would be to look at the conferences and seminars that are held on the subject and find out what currently engages physicists . for example , have a look at : physics and computation 2010 and conference of computational physics . look at the titles of talks and submitted papers , the workshops , tutorials etc . find more on the web ( keyword search on the arxiv ) etc , and you will get a rough idea of the status of the field . start working on something , make some contributions to open source initiatives or get your own results , basically get some credentials so that employers might pay attention to you and you might find yourself working along with phds .
yes , infrared radiation which is invisible to human eyes but still radiates heat . this is how they make thermal cameras , they are using your body 's infrared radiation which is detected by the camera and forms an image based on visible light .
here are real events relating to the last page of the pdf link you gave : fig . 1 this bubble chamber picture shows some electromagnetic events such as pair creation or materialization of high energy photon into an electron-positron pair ( green tracks ) , the compton effect ( red tracks ) , the emission of electromagnetic radiation by accelerating charges ( violet tracks ) ( bremsstrahlung ) and the knock-on electrons or delta ray ( blue tracks ) photons are invisible in bubble chambers as they interact only with direct collisions with electrons , called compton scattering , or pair production in the field of a nucleus . charge particles turn in the perpendicular to the plane magnetic field ; this allow us to measure their momentum and charge and the ionisation of the tracks allows the identification of masses . at the lower left of the picture , there is an electron ( identified by its ionisation ) which loses energy into a photon , and the photon pair produces some centimeters away , an electron positron pair . in the middle right side , we see a positron that loses energy into a photon and the photon kicks an electron from the atoms of the chamber , this is a compton scatter . this corresponds to the diagram in the last page of your link , except it has been reduced to one dimension . in reality there are two dimensions , because the photon gives part of its momentum/energy kicking at an angle . the following is the correct diagram kinematically : it should not be surprising that classical scatters and particle scatters kinematically are the same , because momentum and energy conservation hold both classically and quantum mechanically . it is the probability of interaction that is different in the microcosm of elementary particles to the billiard ball particle scattering . in simple scattering experiments the kinematics are not different ( except that special relativity holds in the microcosm ) . edit after edits in question . i think the image , from a real experiment , answers whether a photon can hit an electron or not . now you ask : what if i put the electrons behind a double slit apparatus , and treat individual photons as particles ? based on this " compton scattering , " it is possible for the photon to be deflected any which way . i could claim that the diffraction pattern observed in the double-slit experiment is due to compton scattering , among other factors . prove me wrong ! the difraction pattern of individual photons , even when sent one at a time , is a direct result of the quantum mechanical nature of the photon . the solution of the boundary conditions imposed by the two slits gives a probability distribution that displays an interference pattern . even though electromagnetic interactions viewed as feynman diagrams are similar , it is the boundary conditions that determine the probability of scatter , and two slits is different than two particle scatter , the fields are different and the solutions are different .
good question . the rate of temperature increase scales as the power absorbed by the food divided by mass of the food . so to understand your question , you need to understand how power is absorbed . there is a finite amount of power in the microwaves being produced . these microwaves bounce around in the metal cage where you put your food , until they come into contact with the food . ( well , some of them will get absorbed in the metal by imperfect reflection , but let 's ignore that at first . ) once they get absorbed by the food , they turn into heat . because they bounce around until they hit some food , the efficiency of a microwave is pretty high , in the sense that most of the power generated in the form of microwaves goes into heating the food , regardless of how much food you have . so , at lowest-order , increasing the mass will increase the amount of water , but will not increase the amount of power being absorbed by the food . but now , that thing about absorption by the metal comes in . the power absorption will be slightly greater with a lot of food , since the food will be more likely to absorb the microwave before it gets absorbed by the metal . this is a lower-order effect , but it is there . of course , then the issue of skin depth comes in . microwaves only penetrate a certain distance into the food . ( of order an inch , depending on the food . ) so increasing the mass is not really what you want ; you want to increase the mass that is within the skin depth . for example , a wide dish of water that is one inch deep will absorb better than a jug of water with the same volume . this is why you want to split apart chicken breasts when defrosting them , for example . to answer your question , then , the more food you put in , the more efficiently your food will capture the power being produced by the microwave oven . so you will capture the microwave 's power better , but you will still heat slower because you have more mass . but this is not a dominant effect , and you might be better off redistributing your food to maximize the surface area .
no . particles with spin will feel the torsion not only through their spin precession , since the equations of motion for them ( mathisson-papapetrou equations ) will contain the asymmetric part of the connection . one source for the question is the review hehl , f . w . , von der heyde , p . , kerlick , g . d . , and nester , j . m . ( 1976 ) . general relativity with spin and torsion : foundations and prospects . rev . mod . phys . , 48 ( 3 ) , 393 . ( it has an online version ) . from there we learn : we have already indicated that photon and spinless test particles sense no torsion . a test particle in $u_4$ theory , one which could sense torsion , is a particle with dynamical spin like the electron . its equation of motion can be obtained by integrating the conservation law of energy-momentum ( 3.12 ) . in so doing , we obtain directly the mathisson-papapetrou type equation${}^{20}$ for the motion of a spinning test particle ( hehl , 1971 ; trautman , 1972c ) ${}^{21}$ adamowicz and trautman ( 1975 ) have studied the precession of such a test particle in a torsion background . all these considerations seem to be of only academic interest , however , since torsion only arises inside matter . there , the very notion of a spinning test particle becomes obscure ( h . gollisch , 1974 , unpublished ) . only neutrinos , whose spin self interaction vanishes , seem to be possible candidates for $u_4$ test particles . ( hehl , 1971 ) reference here is apparently original result on the motion of test particle with spin : hehl , f . w . ( 1971 ) . how does one measure torsion of space-time ? . phys . lett . a , 36 ( 3 ) , 225-226 . ( http://dx.doi.org/10.1016/0375-9601(71)90433-6 ) for a more modern notation ( tetrad formalism ) for the mentioned mathisson-papapetrou equations you can use the thesis : laskoś-grabowski , p . ( 2009 ) . the einstein–cartan theory : the meaning and consequences of torsion . master 's thesis pp . 17-19 the references there should provide all further information .
some 3d glasses use narrow-band filters rather than polarization , which is quite clever . see http://en.wikipedia.org/wiki/dolby_3d for details . the reason the light looked different through each eye is that the light spectrum is not uniform across the visible wavelength . thus , when different parts of that spectrum are viewed ( through the left or right lenses ) , you get light that is not quite white , and not quite the same .
there are two problems to deal with which must be disentangled to solve problems like these . both angular momentum operators are vector operators , so in some sense they " take values " in $\mathbb r^3$ ; you are being asked for their dot product , which should be taken within that copy of $\mathbb r^3$ . you would have the same problem if you were asked to calculate the dot product $\mathbf r\cdot\mathbf p$ for a single particle without spin . the orbital and spin angular momentum operators act on the two different factors of a tensor product of hilbet spaces . thus any ( operator ) product of a scalar orbital operator with a scalar spin operator should be interpreted as a tensor product . you would have the same problem if you were asked to calculate the product $l^2s^2$ , which would need to be interpreted as $l^2\otimes s^2$ . thus , in your case , you must read $l\cdot s$ as $$ \mathbf{l}\cdot \mathbf{s}=\sum_{i=1}^3l_is_i=\sum_{i=1}^3l_i\otimes s_i . $$ to compute the matrix representation of this , you should begin with the matrix representation of each $l_i$ and $s_i$ . you then compute the tensor product matrices $l_i\otimes s_i$ . finally , you add all of those matrices together to get the final result . this is all much clearer with an example . the $z$ component , for example , is easy , since each matrix is given by $$ l_z=\hbar\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \quad\text{and}\quad s_z=\frac\hbar 2 \begin{pmatrix}1 and 0\\0 and -1\end{pmatrix} , $$ in the bases $\{|1\rangle , |0\rangle , |-1\rangle\}$ and $\{|\tfrac12\rangle , |-\tfrac12\rangle\}$ respectively . the tensor product matrix , then , in the basis $\{|1\rangle\otimes|\tfrac12\rangle , |0\rangle\otimes|\tfrac12\rangle , |-1\rangle\otimes|\tfrac12\rangle , |1\rangle\otimes|-\tfrac12\rangle , |0\rangle\otimes|-\tfrac12\rangle , |-1\rangle\otimes|-\tfrac12\rangle \}$ , is given by $$ l_z\otimes s_z=\frac{\hbar^2} 2 \begin{pmatrix} 1\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} and 0\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \\ 0\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} and -1\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \end{pmatrix} =\frac{\hbar^2} 2 \begin{pmatrix} 1 and 0 and 0 and 0 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 0\\0 and 0 and -1 and 0 and 0 and 0\\ 0 and 0 and 0 and -1 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 1 \end{pmatrix} . $$ this procedure should be repeated with both the $x$ and the $y$ components . each of those will yield a six-by-six matrix ( in this case ) . to get your final answer you should add all three matrices .
the link you posted in your question contains a link to a wikipedia page on the triboelectric effect , which in turns contains the answer to your question . from the " cause " section : although the word comes from the greek for " rubbing " , τρίβω ( τριβή: friction ) , the two materials only need to come into contact and then separate for electrons to be exchanged . after coming into contact , a chemical bond is formed between some parts of the two surfaces , called adhesion , and charges move from one material to the other to equalize their electrochemical potential . this is what creates the net charge imbalance between the objects . when separated , some of the bonded atoms have a tendency to keep extra electrons , and some a tendency to give them away , though the imbalance will be partially destroyed by tunneling or electrical breakdown ( usually corona discharge ) . in addition , some materials may exchange ions of differing mobility , or exchange charged fragments of larger molecules . the triboelectric effect is related to friction only because they both involve adhesion . however , the effect is greatly enhanced by rubbing the materials together , as they touch and separate many times .
atoms in a solid are usually stuck to each other in some sort of a rigid lattice , which gives the solid its structure and shape . this is not the only possible state of affairs : they can also slide past each other , if the temperature is high enough , but then they are likely to do so endlessly , all of them , and the material becomes a liquid . and , indeed , in a liquid you can have both negative and positive charge carriers , where the positive ones are entire atoms with one electron stripped off .
consider a theory of fields $\phi:m\to t$ where $m$ is a manifold , and $t$ is a set . in physics , $t$ is often either a vector space or a manifold . we call $m$ the domain of the theory , and we call $t$ the target space . of the theory . we call a function from $m$ to $t$ a field configuration , and the set of all field configurations is denoted $\mathcal f$ . let 's consider two situations : case 1 . let groups $g_m$ and $g_t$ be given . let $\rho_m$ be an action of $g_m$ on $m$ , and let $\rho_t$ be an action of $g_t$ on $t$ , then there is a " natural " action of $\rho_\mathcal f$ of $g_m\times g_t$ on $\mathcal f$ given by \begin{align} \rho_\mathcal f ( g_m , g_t ) ( \phi ) ( x ) = \rho_t ( g_t ) \big ( \phi\big ( \rho_m ( g_m ) ^{-1} ( x ) \big ) \big ) \end{align} i will leave it to you to prove that this is a group action . case 2 . let a groups $g$ be given . let $\rho_m$ be an action of $g$ on $m$ , and let $\rho_t$ be an action of $g$ on $t$ , then there is a " natural " action of $\rho_\mathcal f$ of $g$ on $\mathcal f$ given by \begin{align} \rho_\mathcal f ( g ) ( \phi ) ( x ) = \rho_t ( g ) \big ( \phi\big ( \rho_m ( g ) ^{-1} ( x ) \big ) \big ) \end{align} i will leave it to you to prove that this is in fact a group action . now , let us phrase your question in the following way : in either of the above cases , is their a sense in which $\rho_m$ induces $\rho_t$ ? the answer , as far as i am aware , is that it depends on the context and on what you mean by " induced . " let 's consider the example you give in your statement of the question . example . an $\mathrm{o} ( 2 ) $ vector field on $\mathbb r^{3,1}$ . we have \begin{align} m = \mathbb r^{3,1} , \qquad t = \mathbb r^2 , \qquad g_m = \mathrm{p} ( 3,1 ) , \qquad g_t = \mathrm{o} ( 2 ) \end{align} where $\mathrm{p} ( 3,1 ) $ is the poincare group in four dimensions . in this case , one often takes $\rho_m$ and $\rho_t$ to be \begin{align} \rho_m ( \lambda , a ) ( x ) = \lambda x+a , \qquad \rho_t ( r ) ( v ) = rv \end{align} notice that this falls under case 1 above . in this case , there is no " canonical " relationship ( as far as i am aware ) between the poincare group and $\mathrm{o} ( 2 ) $ , so there is no canonical sense in which $\rho_m$ induces $\rho_t$ . however , consider the following example : example . an $\mathrm{so} ( 3 ) $ $2$-tensor field on $\mathbb r^{3}$ . we have \begin{align} m = \mathbb r^{3} , \qquad t = t_2 ( \mathbb r^3 ) , \qquad g = \mathrm{so} ( 3 ) \end{align} where $t_2 ( \mathbb r^3 ) $ is the vector space of $2$-tensors on $\mathbb r^3$ . then there is a natural action of $g$ on $m$ given by \begin{align} \rho_m ( r ) x = rx \end{align} since $2$-tensors on $\mathbb r^3$ can be described as bilinear maps functions $s:\mathbb r^3\times \mathbb r^3 \to \mathbb r$ , there is also a natural action of $g$ on $t$ given by \begin{align} \rho_t ( s ) ( x_1 , x_2 ) = s ( r^{-1}x_1 , r^{-1} x_2 ) \end{align} which can also be written as \begin{align} \rho_t ( s ) ( x_1 , x_2 ) = s ( \rho_m ( r ) ^{-1}x_1 , \rho_m ( r ) ^{-1} x_2 ) \end{align} so , in this case , there is a sense in which $\rho_m$ has induced $\rho_t$ .
when you use the reduced mass , what you have first done is to go from the variables $ ( r_1 , p_1 ) $ and $ ( r_2 , p_2 ) $ to $ ( r=r_1-r_2 , p/\mu=p_1/m_1-p_2/m_2 ) $ and $ ( m r=m_1 r_1+m_2 r_2 , p=p_1+p_2 ) $ , where $m=m_1+m_2$ is the total mass and $1/\mu=1/m_1+1/m_2$ is the ( inverse of the ) reduced mass . as you said , this is usually introduced in classical mechanics to simplify the two-body problem , and it is not a priori valid in quantum mechanics . but in fact , it is . you just have to show that $\hat r , \hat r , \hat p , \hat p$ have all the expected commutation relations of independent position and momentum operators . this then allows to separate the two-body hamiltonian in two parts $\hat h ( \hat r_1 , \hat r_2 , \hat p_1 , \hat p_2 ) =\hat h_{red} ( \hat r , \hat p ) +\hat h_{cm} ( \hat r , \hat p ) $ .
there are no unique representations , so we will assume that op is mostly interested in normal-ordered expressions between the three operators $a^{\dagger}$ , $a$ and $n:=a^{\dagger}a$ . here $ [ a , a^{\dagger} ] =1$ . the underlying identities are $$ \tag{1} [ a , n ] ~=~a \qquad\text{and}\qquad [ n , a^{\dagger} ] ~=~a^{\dagger} , $$ which lead to $$ \tag{2} f ( a ) e^{zn}~=~ e^{zn} f ( e^{z} a ) \qquad\text{and}\qquad e^{zn}f ( a^{\dagger} ) ~=~ f ( e^{z} a^{\dagger} ) e^{zn} , $$ respectively . here $z\in \mathbb{c}$ is a complex number , and $f:\mathbb{c}\to \mathbb{c}$ is a sufficiently well-behaved function , e.g. an exponential function . so the sought-for commutators read in normal-order form $$ \tag{3} [ f ( a ) , e^{zn} ] ~=~ e^{zn} \{f ( e^{z} a ) -f ( a ) \} , $$ and $$ \tag{4} [ e^{zn} , f ( a^{\dagger} ) ] ~=~ \{f ( e^{z} a^{\dagger} ) -f ( a^{\dagger} ) \}e^{zn} , $$ respectively . more generally , one has $$ \tag{5} e^{zn}f ( a^{\dagger} , a ) ~=~ f ( e^{z} a^{\dagger} , e^{-z} a ) e^{zn} , $$ with corresponding commutator $$ \tag{6} [ e^{zn} , f ( a^{\dagger} , a ) ] ~=~ \{f ( e^{z} a^{\dagger} , e^{-z} a ) -f ( a^{\dagger} , a ) \}e^{zn} . $$
spontaneous emission occurs in all directions , and is akin to a glow . to have a concentrated beam with low divergence ( that is the idea of a laser ) , the photons must all be moving in the same direction--that is where a pair of mirrors come in . the high concentration of photons on this axis directly increases the rate of stimulated emission too . but spontaneous emission cannot be increased so simply--that is why it constitutes only about 1/10th of the output radiation of a typical laser .
i can not be sure what the source meant without seeing the context , however i suspect the author meant the following . a $ u ( 1 ) $ gauge transformation acting on a charged scalar field gives : \begin{equation} \phi ( x ) \rightarrow e ^{ i \alpha ( x ) } \phi ( x ) \end{equation} under such a transformation the normalization is invariant since $\phi$ simply gains a phase . this is just the definition of a $u ( 1 ) $ rotation .
there is a identity for the derivative of the cross-product of two vector functions $\mathbf a ( t ) $ and $\mathbf b ( t ) $ ; \begin{align} \frac{d}{dt} ( \mathbf a \times \mathbf b ) = \frac{d\mathbf a}{dt}\times \mathbf b + \mathbf a\times \frac{d\mathbf b}{dt} \end{align} using this rule with the computation you are considering , we obtain \begin{align} \frac{d}{dt}\left ( m_i\mathbf r_i\times \frac{d\mathbf r_i}{dt}\right ) = m_i \frac{d\mathbf r_i}{dt}\times \frac{d\mathbf r_i}{dt} + m_i\mathbf r_i\times \frac{d^2\mathbf r_i}{dt^2} = m_i\mathbf r_i\times \frac{d^2\mathbf r_i}{dt^2} \end{align} where in the last step , we have used the fact that the cross product of any vector with itself is zero .
colour theory has a lot to do with how the brain processes the signals from the retina , as well as the physics of how light is detected in the eyes . but broadly speaking , the additive and subtractive properties of colour result from the physics of light and its interaction with pigments , so if we were tetrachromatic we would experience them similarly . the main difference is that a tetrochromat would experience four primary colours rather than three . this would change some aspects of colour theory ( i guess you had have something more like a " colour sphere " than the colour wheel , for example ) but the basics would be the same . imagining we had perfect rgb screen technology , capable of reproducing all the colours within our visible spectrum range ( 400nm—700nm ) . could you , if you had tetrachromatic vision , use the same rgb screen and still see all the colours in your " visible spectrum " of the same range ? such a screen is actually kind of impossible . there are colours that we trichromats can see that cannot be reproduced by an rgb screen . the range of colours a three-colour screen can produce are only a subset of the colours we can see . this is due to the way our cone cells respond to more than one light frequency at once . it is a fundamental thing , it is not just due to deficiencies in the screen . but the subset of colours that an rgb screen can produce is a pretty big one , so we generally do not notice . but aside from that point , no , a tetrochromat could not use a three-colour screen and still see most of the colours in its visible range . there would simply be a primary colour missing . it would be like trying to reproduce all the colours a trichromat can see using only two light frequencies . if you use red and cyan for example , then you can get red , white , cyan and black , but you can not get blue or green . could you use this perfect rgb screen technology if , while trichromatic , your cone cells were " tuned " to different frequencies ( but still covered the identical " visible light " range ) ? ! no , you could not , for similar reasons . most likely , it would still cover a large proportion of the colours you can perceive , but there would certainly be some colours you can see that it would not be able to produce .
it seems they are using the law of cosine http://en.wikipedia.org/wiki/law_of_cosines and $\theta$ is really an angle between $r$ and $r'$ . in this problem you can always choose the coordinate system in the way that the $r$ is directed along the axis z in cartesian coordinates , because only relative distance between points designed by $r$ and $r'$ matters
first of all , sean carroll is a relativist so his treatment of the diffeomorphism symmetry as a gauge symmetry should be applauded because it is the standard modern view preferred by particle physicists – its origin is linked to names such as steven weinberg , it is promoted by physicists like nima arkani-hamed , and naturally incorporated in string theory so seen as " obvious " by all string theorists . in this sense , carroll throws away the obsolete " culture " of the relativists . there are some other " relativists " who irrationally whine that it should not be allowed to call the metric tensor " just another gauge field " and the diffeomorphism group as " just another gauge symmetry " even though this is exactly what these concepts are . second of all , a symmetry expressed by a lie algebra can not be " discrete " , by definition : it is continuous . lie groups are continuous groups ; it is their definition . and only continuous groups are able to make whole polarizations of particles unphysical . it is plausible that a popular book replaces the continuous groups by discrete ones that are easier to imagine by the laymen but this server is not supposed to be " popular " in this sense . third , when you say that if $u$ is unitary , the generator has to be hermitian and traceless , is partly wrong . unitarity of $u$ means the hermiticity of the generators $t^a$ but the tracelessness of these generators is a different condition , namely the property that $u$ is " special " ( having the determinant equal to one ) . the tracelessness is what reduces $u ( n ) $ to $su ( n ) $ , unitary to special unitary . fourth , and it is related to the second point above , " charge conjugation " is not any gauge principle of electromagnetism in any way . electromagnetism is based on the continuous $u ( 1 ) $ gauge group . this group has an outer automorphism – a group of automorphisms is ${\mathbb z}_2$ – but we are never putting these elements of the discrete group into an exponent . fifth , similarly , qcd is not based on the discrete symmetry of permutations of the colors but on the continuous $su ( 3 ) $ group of special unitary transformations of the 3-dimensional space of colors . because none of the things you wrote about the non-gravitational case was quite right , it should not be surprising that you have to encounter lots of apparent contradictions in the case of gravity as well because gravity is indeed more difficult in some sense . sixth , $so ( 3,1 ) $ is not related to the diffeomorphism in any direct way . it is surely not the same thing . this group is the lorentz group and in the gr , you may choose a formalism based on tetrads/vielbeins/vierbeins where it becomes a local symmetry because the orientation of the tetrad may be rotated by a lorentz transformation independently at each point of the space . but this is just an extra gauge symmetry that one must add if he works with tetrads – it is a symmetry that exists on top of the diffeomorphism symmetry and this symmetry is different and " non-local " because it changes the spacetime coordinates of objects or fields while all the yang-mills symmetries above and even the local lorentz group at the beginning of this paragraph are acting locally , inside the field space associated with a fixed point of the spacetime . ( the fact that diffeomorphisms in no way " boil down " to the local lorentz group is a rudimentary insight that is misunderstood by all the people who talk about the " graviweak unification " and similar physically flawed projects . ) i will not use with tetrads in the next paragraph so the gauge symmetry will be just diffeomorphisms and there will not be any local lorentz group as a part of the gauge symmetry . the diffeomorphism symmetry is locally generated by the translations , not lorentz transformations , and the parameters of these 4-translations depend on the position in the 4-dimensional spacetime . this is how a general infinitesimal diffeomorphism may be written down . if there were no gauge symmetries , $g_{\mu\nu}$ would have 10 off-shell degrees of freedom , like 10 scalar fields . however , each generator makes two polarizations unphysical , just like in the case of qed or qcd above ( where the 4 polarizations of a vector were reduced down to 2 ; in qcd , all these numbers were multiplied by 8 , the dimension of the adjoint representation of the gauge group , $su ( 3 ) $ etc . ) . because the general translation per point has 4 parameters , one removes $2\times 4 = 8$ polarizations and he is left with $10-8=2$ physical polarizations of the gravitational wave ( or graviton ) . the usual bases chosen in this 2-dimensional physical space is a right-handed circular plus left-handed circular polarized wave ; or the " linear " polarizations that stretch and shrink the space in the horizontal/vertical direction plus the wave doing the same in directions rotated by 45 degrees : this counting was actually a bit cheating but it does work in the general dimension . to do the counting properly and controllably , one has to distinguish constraints from dynamical equations and see how many of the modes of a plane wave ( gravitational wave ) are affected by a diffeomorphism . in the general dimension of $d$ , it may be seen that the tensor $\delta g_{\mu\nu}$ may be described , after making the right diffeomorphism , by $h_{ij}$ in $d-2$ dimensions and moreover the trace $h_{ii}$ may be set to zero . this gives us $ ( d-2 ) ( d-1 ) /2-1$ physical polarizations of the graviton . in $d=4$ , this yields 2 physical polarizations of the graviton . a gravitational wave moving in the 3rd direction is described by $h_{11}=-h_{22}$ and $h_{12}=h_{21}$ while other components of $h_{\mu\nu}$ may be either made to vanish by a gauge transformation ( diffeomorphism ) , or they are required to vanish by the equations of motion or constraints linked to the same diffeomorphism . morally speaking , it is true that we eliminate two groups of 4 degrees of freedom , as i indicated in the sloppy calculation that happened to lead to the right result . note that $$\frac{d ( d+1 ) }2 -2d = \frac{ ( d-2 ) ( d-1 ) }2-1 $$ i have to emphasize that these is a standard counting of the " linearized gravity " and it is the same procedure to count as the counting of physical polarizations after the diffeomorphism " gauge symmetry " – just the language involving " gauge symmetries " is more particle-physics-oriented .
to answer your question let me start with the most basic constituents of the universe , i.e. elementary particles . standard model of particle physics contains matter particles ( quarks and leptons ) , force carriers ( w/z , photon , gluon ) and the higgs particle . photon and gluon are massless , and the rest of the particles have non-zero masses . in a non-interacting situation , all those particles have their masses fixed , independent of their speed ( so no relativistic mass which is an old way of thinking about relativistic kinematics anyways ) . when there are interactions between particles and enough energy to produce new particles ( through $e=mc^2$ ) , weird things happen . at this point we need quantum field theory to fully understand what is going on . without going much into details , the way we think about the interactions is through force carrying particles i mentioned in the previous paragraph . when two particles interact , they can simply scatter off each other or the interaction can create a complete different set of particles . we understand the process of going from an " initial " set of particles to a " final " set of particles in terms of all the " paths " connecting them with the interactions allowed by the standard model . this situation is very similar to what happens in the double slit experiment . during this process virtual particles ( from the above set i mentioned ) are created . these intermediate particles have the exact same properties as the real ones , except their mass ( or invariant mass ) can be different than their actual mass . as in the double slit experiment , nature takes all the allowed paths between initial and final states . at this point we can talk about the energy of our particles . according to special relativity there is a rest frame for any massive particle and in this special frame even though our particle is at rest it will have an energy given by $e_0 = mc^2$ . so for a massless particle like photon there is no rest frame and hence no rest energy . but all the particles ( massive or massless ) have momentum , and a total energy given by $e = \sqrt{ ( mc^2 ) ^2 + ( pc ) ^2}$ . note that this reduces to $e = pc$ for a massless particle like photon . if you think in terms quantum physics , an electromagnetic wave contains photons . so the energy of the electromagnetic wave comes from the energy of the photons i.e. the momentum carried by the photons ( which is also related to their frequency or wavelength ) . so far we talked about mass and energy of fundamental particles . what happens when we have bound states like protons/neutrons or nuclei or atoms or molecules ? the mass of these compound objects depend on the bounding energy ( or potential energy ) that keeps them together . so for example the mass of a proton is not equal to the total mass of the quarks that makes up the proton ( two up , one down quark ) but is mostly created by the interaction between these quarks explained by quantum chromodynamics ( qcd ) . since there is more than one fundamental particle making up our bound states , there can also be excited states with slightly different masses . an atom for example can absorb a photon and switch to an excited state for a very brief time which has a different mass . this mass difference is usually very small . this only happens when the energy of the photon matches the difference between the discrete energy levels of the atom . most of the time they will only scatter like billiard balls . for an even larger bound system like a solid object , an absorbed electromagnetic wave usually changes the temperature of the object . in this larger system , we can think of atoms organized in a geometric pattern . on the average they sit at fixed points on a 3d lattice but they all individually vibrate around their equilibrium points . so an electromagnetic wave absorbed by the object usually changes the amplitude of these vibrations or the average vibration energy i.e. the temperature of the object .
yes . g2 shows up often , starting with atomic physics ( perhaps racah is the first ; see r . e . behrends , j . dreitlein , c . fronsdal , and b . w . lee , “simple groups and strong interaction symmetries , ” rev . mod . phys . 34 , 1 ( 1962 ) . ) . you will find some refences in my 1976 phys rev paper on cns . physics . gatech . edu/grouptheory/refs . i have whole folder of physics g2 papers , but now i see i did not bother to enter g2 history into www.birdtracks.eu. nobody 's perfect . sorry predrag ( for responses , email to dasgroup [ snail ] gatech . edu , i sometimes look at those . pure accident i saw this question . . . )
no . boiling itself does not mean that the water will cook anything . if you have boiling water at 30°c you could touch it ( if we forget that it is at really low pressure ) and nothing would happen . boiling is not what cooks , but temperature . in fact , if you want to purify water at high altitudes , you need to boil water for a longer time because it will be at a lower temperature .
there is a sense in which this is right . if we model the laser as a stream of photons , hitting some surface , then it is the change in the momentum of the photons due to their interaction with the surface that causes the pressure . for example , if the laser shines on a mirror , then the photons will bounce back after hitting the mirror , and there momenta will change by an amount equal to twice the magnitude of their original momentum . however , it is also possible for the photons to be absorbed by the material . in this case , the photons do not bounce back , but their momentum change by an amount equal to the initial magnitude of the their momenta , and this momentum change due to interaction with the surface is what causes the pressure . on the other hand , if you are referring to the object that emits the laser , then it will certainly be the case that this object will feel a force in a direction opposite that of the beam travel direction . by conservation of momentum , if the laser emits a photon of momentum $p$ , then its momentum must increase in the opposite direction by that same amount $p$ as well . the change in momentum of the object then leads to propulsion . see also the wiki and especially the second paragraph in the quantum theory argument section .
the answer to your question is , sometimes , but it depends on the source and your hypothesised relationship approximately holds in some cases . in three dimensions , your relationship does not hold . if the sound source is small , then its pressure field outside the source 's hardware will be a general multipole scalar field , i.e. a general superposition of spherical waves each fulfilling helmholtz 's equation $ ( \nabla^2 + k^2 ) \psi = 0$ where $k = 2\pi/\lambda$ is the wavenumber at the frequency in question in the medium in question : $$\psi ( r , \theta , \phi ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu= -\ell}^\ell \psi_{\ell , \nu}\ , j_\ell ( k r ) p^{|\nu|}_\ell ( \cos ( \theta ) ) e^{i\ , \nu\ , \phi}$$ where $ ( r , \theta , \phi ) $ are the spherical co-ordinates of the point in question , $p^{|\nu|}_\ell$ are the associated legendre functions and $j_\ell$ are the spherical bessel functions of the first kind and order $\ell$ . in the farfield , i.e. when $k r \gg \ell$ we have $j_\ell ( k\ , r ) \approx \sin ( k\ , r - \ell \pi/2 ) / r$ so that the wave 's intensity varies like $1/r^2$ in the farfield . therefore , a factor of 10 increase in the distance from the source leads to a factor of 100 decrease in the intensity ( radiated power per unit area ) , which in decibel terms is a loss of $10\log_{10}100 = 20{\rm db}$ . therefore , to match the intensity of a 50db source at 1m distance , you are going to need a 70db source at 10m and a 90db source at 100m . you can get this result imagining the source is like an isotropic radiator with some , say dipole , radiation pattern . there will be an inverse square intensity dependence on dustance . however , suppose your source is somehow cylindrical . maybe it is like a paper cone loudspeaker but the cone is replaced by a very long paper cylinder radiating in and out . if you are near enough to the cylinder that it can be approximated as being infinitely long , then you have essentially gotten yourself a two dimensional problem , the waves are now cylindrical waves : $$\psi ( r , \theta ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu=-\infty}^\infty \psi_{\nu , \ell}\ , j_\nu ( k_\ell r ) e^{i\ , \nu\ , \phi}$$ where now $j_\nu$ is the bessel function of the first kind and order $\nu$ . now in the farfield , $j_\nu ( k_\ell r ) \approx\sqrt{2/\pi}\cos ( k_\ell r -\nu\pi/2 -\pi/4 ) /\sqrt{r}$ so that the wave 's intensity varies like $1/r$ in the farfield and now a factor of ten increase in distance corresponds to a loss of 10db . therefore , to match the intensity of a 50db source at 1m , you would need a 60db source at 10m way or a 70db source at 100m away .
there might be several reasons , some more obvious than others . the quality of the cell ( including its orthogonal quality , aspect ratio , and skewness ) also has a significant impact on the accuracy of the numerical solution . orthogonal quality is computed for cells using the vector from the cell centroid to each of its faces , the corresponding face area vector , and the vector from the cell centroid to the centroids of each of the adjacent cells ( see equation 5–1 , equation 5–2 , and figure 5.22: the vectors used to compute orthogonal quality ) . the worst cells will have an orthogonal quality closer to 0 , with the best cells closer to 1 . the minimum orthogonal quality for all types of cells should be more than 0.01 , with an average value that is significantly higher . aspect ratio is a measure of the stretching of the cell . as discussed in computational expense , for highly anisotropic flows , extreme aspect ratios may yield accurate results with fewer cells . generally , it is best to avoid sudden and large changes in cell aspect ratios in areas where the flow field exhibit large changes or strong gradients . skewness is defined as the difference between the shape of the cell and the shape of an equilateral cell of equivalent volume . highly skewed cells can decrease accuracy and destabilize the solution . for example , optimal quadrilateral meshes will have vertex angles close to 90 degrees , while triangular meshes should preferably have angles of close to 60 degrees and have all angles less than 90 degrees . a general rule is that the maximum skewness for a triangular/tetrahedral mesh in most flows should be kept below 0.95 , with an average value that is significantly lower . a maximum value above 0.95 may lead to convergence difficulties and may require changing the solver controls , such as reducing under-relaxation factors and/or switching to the pressure-based coupled solver . source url
strictly speaking it is a unit of energy . but using $m=\frac{e}{c^2}$ , you can convert energy into mass . operating , we get $1{\rm\ , ev}/c^2 =1.78\cdot 10^{-36}\rm{\ , kg}$ . ( the $c^2$ is usually ommited . )
the mistake you made is in the way you stated coloumb 's law . it is either $$ \vec{f} = k \frac{q_1 q_2}{r^\color{red}3} \color{red}{\vec{r}} $$ or $$ \vec{f} = k \frac{q_1 q_2}{r^\color{red}2} \color{red}{\hat{r}} $$ but definitely not $$ \vec{f} = k \frac{q_1 q_2}{r^\color{red}3} \color{red}{\hat{r}} $$
if the mass of the penny is negligible comparing to the hammer , the speed of the hammer at the end of the track should be the same in both cases . with the collision in the first scenario , the speed of the penny should be twice of the hammer if the collision is complete elastic . but , for the second move-along scenario , the speed of the penny will be just the same as the hammer at the end of the track .
if $z \mapsto \mathrm{e}^{\mathrm{i}\delta}$ , then $z^p \mapsto ( \mathrm{e}^{\mathrm{i}\delta} z ) ^p = \mathrm{e}^{\mathrm{i}\delta p} z^p$ . what we are actually looking at is that the " transformation of the $p$-th power of $z$" is a way to speak of the representation of the circle group $\mathrm{u} ( 1 ) $ labeled by $p$ .
( 1 ) since $u ( \textbf{r} ) = u ( \textbf{r}+\textbf{r} ) $ , we can expand this part in terms of reciprocal lattice vectors , $u_k ( \textbf{r} ) = \sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_\textbf{k-g}}$ . we can therefore write : \begin{equation} \psi_{\textbf k+\textbf k} = e^{i ( \textbf k + \textbf k ) \cdot \textbf r}\sum_\textbf{g'}{e^{i\textbf{g'}\cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}} = e^{i\textbf k \cdot \textbf r}\sum_\textbf{g'}{e^{i ( \textbf{g'}+\textbf k ) \cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}}=e^{i\textbf k \cdot \textbf r}\sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_{\textbf k-\textbf g}} = \psi_\textbf k \end{equation} where $\textbf g = \textbf k+\textbf g'$ . ( 2 ) you can interpret $\textbf p'$ as being equal to $\textbf p$ . this is true because the real space lattice is periodic ; $\textbf k$ is always equal to $\textbf k + \textbf k$ . ( 3 ) the conserved quantity is $\textbf k$ $\textit mod$ $\textbf k$ . you can see that i used this fact in the answer to ( 2 ) . you can read just about any solid state physics textbooks for complete justification though my personal favorite is ziman 's theory of solids .
i can think of at least four things going on in this experiment that need pointing out : when you inflate a balloon by mouth , the air is warm : this makes the air inside the inflated balloon slightly lighter than the air it displaced the air inside the balloon has 100% relative humidity at 37c , and condensation will quickly form on the inside of the balloon as the air inside cools down . the air inside the balloon contains carbon dioxide , which has higher density than room air ( molecular mass of 12+16+16 = 44 amu , vs oxygen at 32 amu and nitrogen at 28 amu - ignoring small isotopic effects , and ignoring argon ) . the pressure inside the balloon is larger than outside - this increases the density so how large are each of these effects ? warm air : 37c vs 20c results in drop in density of 0.945x ( 293 / 310 ) or -5.5% moisture : partial pressure of water at 37c is 47.1 mm hg source which is about 0.061 atmospheres . assuming that pressure is constant , this water ( mass 18 amu ) displaces air ( mean mass 29 amu ) , so the density of the air decreases by 0.061 * ( 29 - 18 ) / 29 = 2.3% . if we allow the air outside the balloon to have 60% relative humidity ( with saturated vapor pressure of 10.5 mm hg ) , it would be slightly less dense than dry air ( 10.5*0.6/760* ( 29-18 ) /29 = 0.3% ) making the net difference -2.0% . note that much of this moisture will condense when the balloon cools down - little droplets will form on the inside of the balloon . with the air inside still saturated , its density will be 0.1% lower than on the outside ; the net result amounts to 2.9% of the mass of the air in the balloon . carbon dioxide : the exhaled air has 4 - 5 % carbon dioxide source : wikipedia , with an equivalent drop in oxygen . the density of exhaled air is therefore higher than that of inhaled air by 0.045 * ( 44 - 32 ) / 29 = +1.9% pressure in the balloon : from this youtube video - time point 3:43 i estimate the pressure increase in the balloon at 23 mm hg , resulting in an increase in density of 2.9% summarizing in a table : a freshly inflated balloon will thus have only a slightly lower density than the air it displaced , because the temperature + moisture effect is greater than the other two . after you wait a little while , the temperature will equalize and the density of the air inside the balloon will be greater - by 7.7% , with more than half of that not caused by the pressure in the balloon . . . in summary : the experiment described in your link measures the difference in density between air in a balloon , and ambient air . since the density of the air inside the balloon is higher than the density outside the balloon , one may conclude that the air inside the balloon has finite density . one may not conclude that the medium outside the balloon ( which we believe to be " dry air" ) has any density at all - since nothing in this measurement tells us about the air outside the balloon . if you did the experiment carefully with a balloon initially filled with warm air , and you allowed the air to cool down , you might be able to tell that the balance shifts - in other words , that there must be a change in the buoyancy experienced by the balloon as it cools down . that would be an experiment to demonstrate " air has mass " ( volume of balloon decreases , and it experiences less buoyancy ) . from the experiment as described ( popping the balloon ) , we learn that " exhaled air has mass " . that is not the same thing . if you used an air pump ( balloon pump ) to inflate the balloon , the first three components would go away and you are left with the difference due to the pressure only - 2.9% of the mass of the air in the balloon .
the article looks indeed wrong . in fact , there are two mistakes . first , you are right that the acceptance-rejection method has to be applied to $\rho ( r ) $ , and not to $m ( r ) $ . to understand how this idea works , suppose we want to generate a one-dimensional normalized distribution function $p ( y ) $ . now , let 's assume we can rewrite this distribution function in terms of a variable $x$ , such that it takes the form of a uniform distribution . that is , $$ p ( x ) = \begin{cases} 1 and \text{for $0\leqslant x \leqslant 1 , $}\\ 0 and \text{elsewhere} . \end{cases} $$ given $p ( y ) $ , what is $x$ ? we have the jacobian transformation $$ p ( y ) dy = p ( x ( y ) ) \left|\frac{dx}{dy}\right|dy = \left|\frac{dx}{dy}\right|dy , $$ which implies $$ p ( y ) = \frac{dx}{dy} , $$ assuming that $x ( y ) $ is an increasing function . thus $$ x = \int_0^y p ( y' ) dy ' = f ( y ) . $$ in other words , the integral of $p ( y ) $ ( or equivalently , the area under the curve ) follows a uniform distribution . with this in mind , there are essentially two ways to perform a monte-carlo simulation . the first way is the acceptance-rejection method : plot the curve $p ( y ) $ and uniformly generate a pair of numbers $ ( a , b ) $ in the interval $ ( [ 0 , y_\max ] , [ 0 , p_\max ] ) $ , where $y_\max$ and $p_\max$ are the upper bounds of $y$ and $p ( y ) $ . if the coordinate $ ( a , b ) $ lies under the curve $p ( y ) $ , accept it ; otherwise , reject it . if the coordinate is accepted , $y=a$ is generated point . there are major drawbacks to this method : $y_\max$ and $p_\max$ can be infinite , so one would need a cut-off . and if $p ( y ) $ has a sharp peak , one ends up rejecting a lot of points . a far more efficient method is to uniformly generate $x$ , and calculate the corresponding $y$ by inverting $x=f ( y ) $: $$ y = f^{-1} ( x ) . $$ this automatically fills up the area under the curve , without rejecting points . if the calculation of $f^{-1} ( x ) $ is too numerically involved , one can use a combination of both methods : introduce another ( simpler ) function $f ( y ) $ that lies everywhere above $p ( y ) $ . apply the inversion method to $f ( y ) $ , generating a point $y$ . then uniformly generate a value $b$ in the interval $ [ 0 , f ( y ) ] $ . if $b\leqslant p ( y ) $ , accept $y$ ; otherwise , reject it . now , consider the hernquist distribution . since it has a cusp at the origin , and the cumulative mass $m ( r ) $ is a simple function $$ m ( r ) = m\frac{r^2}{ ( a+r ) ^2} , $$ i would definitely recommend the inversion method . but there is an important caveat here , and that is the second mistake in the article : $\rho ( r ) $ is not really a one-dimensional distribution . instead it is a distribution in 3-dimensional space , and it is only a function of one variable due to spherical symmetry . in order to apply the monte-carlo method , we have to express $\rho$ as a truly one-dimensional distribution function , which we can do by expressing it in terms of the volume $$ y = \frac{4\pi}{3}r^3 . $$ now we have $$ p ( y ) = \rho ( y ) = \frac{m}{2\pi}\frac{a\ , ( 3y/4\pi ) ^{-1/3}}{\left [ a + ( 3y/4\pi ) ^{1/3}\right ] ^3} , \\ f ( y ) = m ( y ) = \int_0^y\rho ( y' ) dy ' = m\frac{ ( 3y/4\pi ) ^{2/3}}{\left [ a + ( 3y/4\pi ) ^{1/3}\right ] ^2} . $$ once we generated a point $y$ , the corresponding radius is $$r=\left ( \frac{3y}{4\pi}\right ) ^{1/3} . $$ there is an important consequence : there are likely more particles at large radii than around the centre , even though $\rho ( r ) $ is much larger at small radii . the reason is that particles between two radii $r$ and $ ( r+\delta r ) $ occupy a shell with volume $$v = \frac{4\pi}{3}\left [ ( r+\delta r ) ^3-r^3\right ] . $$ the larger the radius $r$ , the larger the volume of the shell , which means you need more particles to fill it and get $\rho ( r ) $ . this is obvious in the case of a constant density , but it is also true for general densities .
it should be obvious that the linear speed is inversely proportional to the distance from the center to that point on that groove . yes , speed varies over a record , so the wavelength of the wiggles in the groove gets shorter for the same frequency as you get further into the record . however , the master record was created with this same phenomenon in reverse , so it all works out . the resolution that the vynil can be pressed with exceeds what is required even for encoding the high frequencies near the end of the track ( closest to the center ) . since dust and dirt particles will be the same size regardless of where they land on the record , the noise from this dirt will have a overall lower spectrum at the end of the track than the beginning . since the standard riaa playback profile is heavily low pass filtered , this argues that there should be on average relatively more noise from dirt at the end of a track . there are other sources of noise too that do not scale the same way . overall , the fact that linear speed changes over 2:1 from the beginning to the end of a track is one of the engineering tradeoffs that went into records . this is also true of floppy disks and hard drives . it is interesting to note that it is not true for cds . the cd standard specifies constant linear track speed for the same data rate . cd drives ( and their decendents like dvd drives ) have variable speed motors that compensate for the diameter effect to keep linear track speed constant .
the unique thing about sno was that it was simultaneously sensitive to charged-current and neutral-current interactions , because they used deuterated water . the three main interactions are neutrino capture on deuterium , $\nu + n \to e + p$ , which generates a fast electron and a slow proton . the lepton and the baryon exchange a $w$ boson ( the " charged weak current" ) . only electron-type neutrinos may participate in this interaction ; $\nu_\mu$ and $\nu_\tau$ would have to generate heavier leptons , but solar neutrinos do not carry enough energy to make those more massive particles . deuterium dissociation due to neutrino scattering , $\nu + ( np ) \to \nu + n + p$ . the free neutron will wander around for a while before getting captured on another deuteron and emitting a gamma ray . because the neutrino 's charge does not change this reaction is mediated by the " neutral current " ( the $z$ boson ) and all neutrinos contribute equally . elastic scattering from electrons , $\nu + e \to \nu + e$ . this interaction has both charged- and neutral-current contributions , so neutrinos of all flavors may contribute , but electron neutrinos contribute more heavily than the other flavors . these different interaction channels gave independent measurements of the total neutrino flux and the electron neutrino flux . it is worth noting that the neutral current had only just been predicted in 1967 , and was not discovered until the early 1970s . for the most part the solar neutrino community believed that there was some misunderstood property of neutrino detection that caused everybody to measure one-third the predicted solar neutrino flux . it took many years before the possibility that the misunderstood bit was a property of the neutrino itself was really taken seriously . i do not know for certain , but i would expect that the design discussions for sno began in the early 1990s . there are many technical challenges associated with the detector — not least that they have many tons of heavy water suspended in many tons of light water in a thin , transparent membrane . the heavy water is on loan from the canadian nuclear power industry ; sno has a hefty insurance policy to pay to replace it if the membrane ruptures and the heavy water mixes with the light water and is ruined .
to answer your question , you should first understand when is a system most stable . firstly it should not have a tendency to move or change state , thus it should be under equilibrium conditions , i.e. the net force should be zero . we know that $$f = - \frac{du}{dx}$$ putting $f=0$ , we get $$\frac{du}{dx}=0 \tag{1}$$ secondly , it should be able to maintain that equilibrium condition by itself . this can be tested by displacing the system by a small distance $dx$ . if the force on the system then becomes opposite to direction of $dx$ , we can say that the system has a tendency to restore back to its original equilibrium position . an example of this would be a ball kept at the bottom of a spherical valley . displace the ball a little towards the right , and the net force on it acts towards left , bringing it back to its original position . you will realise that i just described a stable equilibrium condition . what this proves is that it is the stable equilibrium condition in which the system is most stable . from the above description we have that the small displacement $dx$ and net extra force $df$ should be in opposite directions . $$\frac{df}{dx} &lt ; 0$$ $$-\frac{d^2u}{dx^2}&lt ; 0$$ $$\frac{d^2u}{dx^2}&gt ; 0\tag{2}$$ from $ ( 1 ) $ and $ ( 2 ) $ it is evident that the graph of $u$ should have a minima at stable equilibrium condition , i.e. the potential energy should be minimum when a system attains maximum stability .
the explanation does not lie in the vectorial nature of the quantities at hand , but rather in the fact that they can be viewed as functions of several variables . just as a scalar function $f$ can depend on a variable $x$ and be denoted $f ( x ) $ , or it can depend on two variables $x$ and $y$ and be denoted $f ( x , y ) $ , and similar for more variables , a vector can be a function in just the same way and depend on as many quantities as you like . what they are in your concrete example depends on the physical system you want to describe . the lorentz force $\mathbf{f}$ depends on the position $\mathbf{r}$ ( which is not the unit vector ) , the velocity $\mathbf{v}=\mathbf{\dot{r}}$ , time $t$ and charge $q$ . thus , in order to make this explicit , we denote it $\mathbf{f} ( \mathbf{r} , \mathbf{\dot{r}} , t , q ) $ . since we have an equation for this force , we expect all these arguments to occur on the right hand side as well . velocity and charge appear as multiplicative quantities , position and time as arguments of two other ( vectorial ) functions : the electric field $\mathbf{e} ( \mathbf{r} , t ) $ and the magnetic field $\mathbf{b} ( \mathbf{r} , t ) $ .
the reference you link to is for objects orbiting sun ( i.e. . comets or asteroids ) . if you are wanting to deal with satellites in low earth orbit , you will need a good book on orbital dynamics or astrodynamics . objects in solar orbit are typically dealt with in celestial mechanics . in both cases , the physics is the same ( two body gravitational interaction ) but the terminology is different . for satellites , one does not speak of " perihelion " or " aphelion " but rather " perigee " and " apogee . " some of the orbital parameters have slightly different names too . for satellites , orbital elements are disseminated in a highly standardized form called tle , which stands for two line element . there are also highly standardized algorithms for taking elements in tle form and turning them into topocentric ra and dec values as a function of time . your best bet , aside from an appropriate textbook on orbital dynamics , is to look at the source code for the algorithms themselves . the free satellite program predict http://www.qsl.net/kd2bd/predict.html stands out as being reliable and well documented . of course , the source code is included in the download . if you google around you may even find the nasa papers that document the sgp4/sdp4 algorithms . here are some c++ and c# implementations of the norad sgp4/sdp4 algorithms ( i have not tested or otherwise evaluated them ) . http://www.zeptomoby.com/satellites/ here 's another site i just found via google that looks potentially useful . http://satelliteorbitdetermination.com
the fundamental representation of a lie group $g$ , as commonly used in this context , is the smallest faithful ( i.e. . injective ) representation of the group . we do not require fermions to belong to the fundamental rep , it is just the case that , in the standard model , they always either belong to the fundamental or the trivial representation ( as that is thoroughly indicated by experimental data ) , so there is rarely a need to look at other representations . to belong to a certain representation $v_\rho$ of $g$ means that the field is a section of the associated vector bundle $p \times_g v_\rho$ , where $p$ is the principal bundle belonging to our gauge theory . equivalently , the field is a $g$-equivariant function $p \to v_\rho$ fulfilling $f ( pg ) = \rho ( g^{-1} ) f ( g ) $ for all $p \in p$ and $g \in g$ . if principal bundles are not talked about , the field is often just taken to be a function $\sigma \to v_\rho$ , though this is , strictly speaking , not the way to do it . every field must belong to a representation of the gauge group ( even if it is the trivial one ) since the gauge transformations must have a defined action upon everything in our theory . it is not required that fields belong to irreducible representations ( again , it is simply often just the case ) , but since every reducible representation can be split up into irreducible ones , it is enough to look at the behaviour of the irreducible representations . ( note though that there are fields transforming in reducible representations - the usual $\frac{1}{2}$-spinors ( not weyl spinors ! ) transform as members of the $ ( \frac{1}{2} , 0 ) \oplus ( 0 , \frac{1}{2} ) $-representation of the lorentz group )
http://en.wikipedia.org/wiki/containment_building by " nuclear reactor " i take it you mean the containment building that is visible looking at a nuclear reactor site . the pressure vessel which houses the nuclear reactor itself is not quite bell shape , and the containment building itself is not always bell shape either . here is an image from that article . you can browse through wikipedia for actual pictures of examples . i typically refer to these containment buildings being " can " shape or " sphere " shape . the best way to explain the reason for these two extreme shapes is to consider the two extremes from a design perspective . that is : limiting factor is the internal steam pressure in an accident limiting factor is material strength to hold up the containment itself if #1 is the case , the objective is to keep the containment from leaking or blowing apart , and you will make the containment out of steel . steel has good tensile strength and you will also build it spherical . this should probably go without saying , but the strongest structure to hold a pressure inside is a sphere . but that is not always the restricting engineering limit . consider the case that a massive massive volume is needed . large volume , lower maximum pressure . in that case , the ideal material selection will change , and you will objectively use a material that is cheaper and still has good specific compressive strength . trying to build a large volume , lower maximum pressure , containment will lead to a more " can " like shape , where you basically build a cylinder . this lets you build tall , keeping the forces compressive , and still not sacrifice too much in terms of strength against an internal pressure . of course , you still need a roof . generally , containment buildings exhibit a compromise between these two cases , which is what i was intending to demonstrate in that sketch . really , it is incorrect to say they are always dome/bell shape . the shape varies , and varies strongly country to country , and generation to generation , reflecting different engineering design choices .
this answer is very similar to adam 's , though i come to the opposite conclusion i.e. that earth loses mass over time . according to the scientific american article the earth loses about 3kg of hydrogen per second , and i make that about $10^8$ kg per year . according to this article the earth gains about $3 \times 10^7$ kg per year from meteors ( mostly extremely small ones ) . so unless there are other sources of weight loss or gain that i have not thought of , i reckon the earth gets lighter by about $7 \times 10^7$ kg per year .
this is not correct ( dissipated power is depending on operation conditions , it is not invariant ) . correct is the value of the current through lamp2 ( it is 1.13a ) . you need to look up what voltage this corresponds to from the lamp2 chart . the voltage v1 is then 5v plus this voltage .
it seems most experts think the abiotic theory is nonsense see http://www.theoildrum.com/story/2005/11/4/15537/8056 for example dr . jon clarke : . the fact remains that the abiotic theory of petroleum genesis has zero credibility for economically interesting accumulations . 99.9999% of the world 's liquid hydrocarbons are produced by maturation of organic matter derived from organisms . to deny this means you have to come up with good explanations for the following observations . the almost universal association of petroleum with sedimentary rocks . the close link between petroleum reservoirs and source rocks as shown by biomarkers ( the source rocks contain the same organic markers as the petroleum , essentially chemically fingerprinting the two ) . the consistent variation of biomarkers in petroleum in accordance with the history of life on earth ( biomarkers indicative of land plants are found only in devonian and younger rocks , that formed by marine plankton only in neoproterozoic and younger rocks , the oldest oils containing only biomarkers of bacteria ) . the close link between the biomarkers in source rock and depositional environment ( source rocks containing biomarkers of land plants are found only in terrestrial and shallow marine sediments , those indicating marine conditions only in marine sediments , those from hypersaline lakes containing only bacterial biomarkers ) . progressive destruction of oil when heated to over 100 degrees ( precluding formation and/or migration at high temperatures as implied by the abiogenic postulate ) . the generation of petroleum from kerogen on heating in the laboratory ( complete with biomarkers ) , as suggested by the biogenic theory . the strong enrichment in c12 of petroleum indicative of biological fractionation ( no inorganic process can cause anything like the fractionation of light carbon that is seen in petroleum ) . the location of petroleum reservoirs down the hydraulic gradient from the source rocks in many cases ( those which are not are in areas where there is clear evidence of post migration tectonism ) . the almost complete absence of significant petroleum occurrences in igneous and metamorphic rocks ( the rare exceptions discussed below ) . the evidence usually cited in favour of abiogenic petroleum can all be better explained by the biogenic hypothesis e.g. : rare traces of cooked pyrobitumens in igneous rocks ( better explained by reaction with organic rich country rocks , with which the pyrobitumens can usually be tied ) . rare traces of cooked pyrobitumens in metamorphic rocks ( better explained by metamorphism of residual hydrocarbons in the protolith ) . the very rare occurrence of small hydrocarbon accumulations in igneous or metamorphic rocks ( in every case these are adjacent to organic rich sedimentary rocks to which the hydrocarbons can be tied via biomarkers ) . the presence of undoubted mantle derived gases ( such as he and some co2 ) in some natural gas ( there is no reason why gas accumulations must be all from one source , given that some petroleum fields are of mixed provenance it is inevitable that some mantle gas contamination of biogenic hydrocarbons will occur under some circumstances ) . the presence of traces of hydrocarbons in deep wells in crystalline rock ( these can be formed by a range of processes , including metamorphic synthesis by the fischer-tropsch reaction , or from residual organic matter as in 10 ) . traces of hydrocarbon gases in magma volatiles ( in most cases magmas ascend through sedimentary succession , any organic matter present will be thermally cracked and some will be incorporated into the volatile phase , some fischer-tropsch synthesis can also occur ) . traces of hydrocarbon gases at mid ocean ridges ( such traces are not surprising given that the upper mantle has been contaminated with biogenic organic matter through several billion years of subduction , the answer to 14 may be applicable also ) . the geological evidence is utterly against the abiogenic postulate . also abiogenic origin of hydrocarbons : an historical overview by dr . geoffrey lasby abstract : the two theories of abiogenic formation of hydrocarbons , the russian-ukrainian theory of deep , abiotic petroleum origins and thomas gold 's deep gas theory , have been considered in some detail . whilst the russian-ukrainian theorywas portrayed as being scientifically rigorous in contrast to the biogenic theory which was thought to be littered with invalid assumptions , this applies only to the formation of the higher hydrocarbons from methane in the upper mantle . in most other aspects , in particular the influence of the oxidation state of the mantle on the abundance of methane , this rigour is lacking especially when judged against modern criteria as opposed to the level of understanding in the 1950s to 1980s when this theory was at its peak . thomas gold 's theory involves degassing of methane from the mantle and the formation of higher hydrocarbons from methane in the upper layers of the earth 's crust . however , formation of higher hydrocarbons in the upper layers of the earth 's crust occurs only as a result of fischer-tropsch-type reactions in the presence of hydrogen gas but is otherwise not possible on thermodynamic grounds . this theory is therefore invalid . both theories have been overtaken by the increasingly sophisticated understanding of the modes of formation of hydrocarbon deposits in nature .
look at around 0:34 in the video . i want to point out something relevant to the question here . the end of the tube is narrowed . that is , in technical terms , a nozzle . nozzles are extremely common in engineering and they work as a form of mechanical leverage just like a lever . i should also note that the straw itself is already a form of nozzle and allows ( i think ) a more potent blow than would otherwise be possible with the lips . a human mouth has limitations . the most accurate way to frame this would be to say that one 's mouth can only produce a given flow rate , $\dot{m}$ , at a certain pressure above atmosphere , $\delta p$ . combined , these give an energy rate , or power , that can be produced by the mouth . the comment by steve melvin does apply - that the balls can not move faster than the fluid that is passing by it . however , the small outlet of the straw he uses is a way to make a tradeoff , getting high fluid velocity by sacrificing volume of flow . this would , in fact , be rather more difficult to do as accurately and gracefully with mechanical forces . this type of easy conversion ability of forms of fluid mechanical work is a major reason that hydraulics is such a useful science .
both rotation and translation . it will execute neither pure rotation nor pure translation
this is a heisenberg picture/schrodinger picture confusion . the operator " r " is not time dependent in the schrodinger picture , but in the heisenberg picture , it is . the time dependent version of the operator is the operator r ( t ) which has the property that $$\langle\psi ( 0 ) |r ( t ) |psi ( 0 ) \rangle = \langle\psi ( t ) | r ( 0 ) |\psi ( t ) \rangle $$ for all choice of $\psi ( 0 ) $ . it is a unitary transformation which shifts the time dependence from the state to the matrix . $$ |\psi ( t ) \rangle = e^{-iht} |\psi ( 0 ) \rangle $$ so that $$ r ( t ) = e^{iht} r ( 0 ) e^{-iht} $$ and you can check that everything works formally . whenever you hear somebody talking about the time derivative of an operator , they are always talking in the heisenberg picture , because otherwise you need explicit time dependence in the operator for this to be meaningful . most modern people talk in the path integral , where you have both pictures simultaneously . schrodinger picture is moving the boundary condition forward in time , heisenberg picture is moving the insertion point of a local object forward in time .
if the symmetry axis of the cone lies along the direction of travel , and if you are using the drag equation \begin{align} f_\mathrm{drag} = \frac{1}{2}\rho v^2a \end{align} to compute the drag force , then you should take $a$ to be the area of the base , namely the full area that would be obstructing your vision if you were looking at the object coming toward you . this assertion is confirmed by wikipedia when it defines the drag coefficient which uses the nice term " projected frontal area ; " the reference area depends on what type of drag coefficient is being measured . for automobiles and many other objects , the reference area is the projected frontal area of the vehicle . this may not necessarily be the cross sectional area of the vehicle , depending on where the cross section is taken . for example , for a sphere $a=\pi r^2$ .
i would like to add a bit of detail to mpv 's answer . my take on the different optical microscopy techniques are : intensity imaging methods which suffer from a divergent noise contribution from out-of-focus information , in exactly the same way as the night sky should be uniformly bright given an infinite universe as described by olber 's paradox . these methods thus lack depth sectioning capability and can only be used with thinly sliced sections to forestall noise buildup from out-of-focus scatterers . this category is basically bright and dark field microscopy and can image a whole field of view at once ( rather than by scanning , as in category 2 ) ; intensity imaging methods which either structure the illumination light or make use of nonlinear processes to control the noise buildup that happens in category 1 . this category includes confocal and multiphoton imaging and all such methods must build images up through scanning , one pixel at a time ; interferometric methods where the phase delay through a thinly sliced specimen is converted to intensity variations by interferometry : this is the classic zernike phase contrast imaging technique ; the sensationally named " superresolution " methods which make use of both optical information together with further information about the sample such that the further information allows one to locate a point source more finely than the diffraction limit of light would otherwise allow . these methods are often wrongly described as " beating " or " outdoing " or " overcoming " the diffraction limit ; they do nothing of the kind and must always use further information for the final location of a point source . the most developed of these methods are sted , storm and structured illumination ( sim ) , which uses periodically modulated illumination to downconvert high spatial frequency information in images to the spatial frequency band where this information can be imaged by a diffraction limited micrcroscope . this is exactly the same mechanism that begets moiré fringe patterns . category 1: brightfield , darkfield , microscope slides , pathology labs and olber 's paradox my drawing below tries to explain the signal-to-noise calculation for microscopy with uniform sample lighting . i first put this question to myself many years ago as : " why can not i put my hand under the objective of a powerful microscope , light my hand strongly ( with , say , a fibre bundle lightsource ) and see the cells in my hand by bringing the microscope into focus on my skin ? " i have drawn the illumination light field in blue and the light scattered from an object in green ( most often i design systems for fluorescence ) , but the argument works just as well in reflexion mode . the main point is that we have out collector lens system imaging the object onto , say , a ccd array and the aberration free field is such that the photon coupling amplitude from scatterer to ccd pixel varies as $\exp ( i\ , k\ , r ) / ( 1+ ( i r/r_0 ) ) $ , where $r$ is the distance of the object from the focus of the collector system . here i have drawn the object we want to see at the focus . but there are also out-of-focus scatterers lit just as brightly as the object and the formula $\exp ( i\ , k\ , r ) / ( 1+ ( i r/r_0 ) ) $ means that , in the farfield , each scatterer contributes a power proportional to $1/r^2$ to the pixel . so far so good : it seems the sensitivity of the instrument to out-of-focus information drops off very swiftly with distance from focus . not swiftly enough , for we must sum up the noise contribution from the whole out of focus volume . if we assume scatterers ( noise objects ) are roughly equally responsive and are uniformly distributed in the large , then we can do this summation in spherical shells as shown . so each scatterer in the spherical shell a distance $r$ from the focus contributes noise power proportional to $1/r^2$ but each spherical shell volume varies like $r^2$ so that roughly each spherical shell of thickness $\delta r$ , no matter how far from focus , contributes a roughly equal noise power . therefore , if the sample is infinitely thick , the noise power diverges . this is exactly the reason why one would expect a uniformly bright night sky in an infinitely long living , infinitely wide universe as in olber 's paradox . one cannot see the cells in one/s own hand with a brightfield microscope because the out-of-focus noise levels are divergent . this is precisely the reason we must prepare microscope slides in a pathology laboratory : the divergent noise figure is controlled by physically " gating " the out-of-focus information : we simply slash it off with a knife ! category 2: confocal and multiphoton imaging : controlling the out-of-focus noise my drawing below repeats this calculation for confocal microscopy . here we structure the lighting by focusing it one pixel at a time on the point we want to image . a system i have worked with lights the sample with the image of the tip of a single mode optical fibre : this begets a convergent lightfield in the sample as shown in my drawing by the blue lightfield . the fluorescence ( or reflected light ) is also gathered by the single mode of the same optical fibre . the fibre 's mode works as a sending and receiving antenna , with its directivity and gain reciprocally related as for any other antenna . the upshot of all this is now that , not only is there a $\exp ( i\ , k\ , r ) / ( 1+ ( i r/r_0 ) ) $ coupling amplitude for light returning to the optical fibre 's single mode , there is another $\exp ( i\ , k\ , r ) / ( 1+ ( i r/r_0 ) ) $ excitation amplitude as well because the illumination field is focussed . therefore , the probability that a noise scatterer is raised to a fluoresciung state varies like $1/r^2$ and , given that this happens , the probability that the photon couples back into the fibre also varies like $1/r^2$ . this means that the noise power contributed by a spherical shell of radius $r$ centred on the focus and of thickness $\delta r$ is proportional to $r^2 \times r^{-2}\times r^{-2} \delta r = r^{-2} \delta r$ . therefore , the noise gathered from an infinite medium is now a finite number and the object tissue does not have to be sliced to be imaged by a confocal microscope . another way to achieve the same noise probabilities is through multiphoton excitation . here , a fluorophore is raised into an extremely short lingering virtual state and then is raised again to the first excited state by a second excitation photon . so the probability for this to happen is proportional to the square of the intensity , not simply the intensity as for one-photon fluorescence . so now the probability that a noise object will be raised to fluorescence varies like $1/r^4$ an , even if we gather all the fluorescence and take no steps to localize it ( by focussing it through a pinhole or single mode fibre tip ) the total noise converges . often in two photon imaging one does just that : the return path does not have to be an imaging system at all to form the image : all we need to do is register the fact of fluorescence and we know where it is coming from since we know where the illumination system is focussed . if we work in this so-called non-descanned mode , the each out of focus spherical shell contributes noise proportional to $r^{-2} \delta r$ as for the confocal microscope ; if we image the fluorescence as we did for the confocal microscope , we get even better noise rejection , for the noise from the same shell now varies like $r^{-4} \delta r$ . for $n$ photon fluorescence , we can likewise achieve $r^{- ( 2\ , n ) } \delta r$ noise performance . below is a comparison between the " tightness of localization " achieved by one and two photon fluorescence . the top trail shows the $1/ ( r_0^2+r^2 ) $ fluorescence probability for one photon fluorescence : the bottom dot shown by the arrow shows the $1/ ( r_0^2+r^2 ) ^2$ two-photon fluorescence probability . category 4: superresolution see the wikipedia page " super resolution microscopy " superresolution is typified by techniques such as stimulated emission depletion microscopy , ground state depletion microscopy and like ideas pioneered for example by stefan hell . this is often sensationally described as " breaking the diffraction limit of light " but this is misleading . the essential idea is that some further information is used to localise where a detected photon has come from and this further information is what gets one below the diffraction limit . for example , sted microscopy uses a high powered " fore-pulse " focussed as a first order ( one with a null at its centre and whose intensity varies like $r^2 e^{-\alpha\ , r}$ ) gaussian mode on excited fluorophores to relax all those fluorophores aside from those at the very centre and within a distance that is much below the diffraction limit . then the system reads the fluorescence arising from a second pulse following very soon ( nanoseconds ) after the first and with the same focus , but this time in the zeroth order gaussian ( the one that varies like $e^{-\alpha\ , r}$ ) mode . now only fluorophores well within a radius that is well below the diffraction limit from the central focal point can contribute to this second reading , and this knowledge lets one infer to within tens of nanometres where the light has come from . another technique called storm ( also on the " super resolution microscopy " wikipedia page ) uses advanced fluorophore chemistry and nonlinear interaction to switch fluorophores on and off quickly so that only the fluorophores near the focus are excited at once . with so few fluorophores lit , the out of focus noise is almost nonexistent and so the signal is clean enough that the point spread function can be deconvolved by fourier techniques from the image . depending on the signal received , the deconvolution achieves an improvement of resolution length proportional to $1/\sqrt{n}$ where $n$ is the number of photons gathered . one can get down to tens of nanometers resolution in optimal conditions with this technique , however , the sample has to be meticulously prepared and sectioned . other techniques include modulation of the optical image to achieve the spatial analogue of syncrhonous downconversion ( the same phenomenon that yields moiré patterns ) of the high spatial frequency content of an image down to the spatial frequency range where it can be imaged by conventional , diffraction-limited light microscopy .
toby , i agree that this is really counter intuitive and i was also quite surprised as well when i first saw this very demonstration . i am an undergraduate ta and this is how i explained it in my lab section . i hope this helps . i see two parts to a full explanation : ( 1 ) why is the electric field constant and ( 2 ) why does the potential difference ( or voltage ) increase ? why is the electric field constant as the plates are separated ? the reason why the electric field is a constant is the same reason why an infinite charged plate’s field is a constant . imagine yourself as a point charge looking at the positively charge plate . your field-of-view will enclose a fixed density of field lines . as you move away from the circular plate , your field-of-view increases in size and simultaneously there is also an increase in the number of field lines such that the density of field lines remains constant . that is , the electric field remains constant . however , as you continue moving away your field-of-view will larger than the finite size of the circular plates . that is , the density of field lines decreases and therefore , the electric field decreases as well as the potential field . to show this mathematically , the easiest way to show this for e = constant is using the relation between the electric and potential fields : $$e = -\frac{\delta v}{\delta d} \longrightarrow \delta v =-e \delta d$$ i would expect the voltage to increase linearly as long as the field is constant . when the electric field starts decreasing , the voltage also decreases and the fields behave as finite charged plates . although i’ve only talked about one plate , this idea immediately applies to two plates as well . why does the work increase the electrical potential energy of the plates ? one way to interpret why the voltage increases is to view the electric potential ( not the electrical potential energy ) in a completely different manner . i think of the potential function as representing the “landscape” that the source ( of the field ) sets up . let me explain what the gravitational potential acts like when a ball is thrown upwards ( of course , you know what happens in terms of the force of gravity or in the conservation of energy scenario ) . i claim that the potential function is related to the “gravitational landscape” that the earth sets up , which is derived from the potential energy and is equal to the potential energy per mass : $$ {\delta u = mg\delta y} \longrightarrow \frac{\delta u}{m} = \delta v = g\delta y$$ plotting these functions , the constant gravity field sets up a gravitational potential ramp ( linear behavior ) that looks like in terms of energy , the ball moves up this gravitational ramp were the ball is converting its kinetic energy into potential energy until the ball reaches it maximum height . however , the gravitational ramp exists whether the ball is thrown up or not . that is , gravity sets up a gravitational ramp ( the landscape ) and this is what the ball “sees” before it is thrown up . if we now apply the above thinking to a constant electric field between the parallel plates , the electric potential function is derived in a similar manner : $$ {\delta u = qe\delta r} \longrightarrow \frac{\delta u}{q} = \delta v = e\delta r$$ if we look at the electric potential of the negative plate ( it’s easier than the positive plate ) , it has a negative electrical ramp that starts at 0v . so as your ta pulls the plates apart , the work she does moves the positive plate up the electrical ramp and increases the potential of the positive plate . so this interpretation of the electric potential is what you intuitively already think about in terms of mechanical situations like riding your bike up a hill . there is no difference in the electrical situation .
superfluids can " climb walls " and whatnot . http://en.wikipedia.org/wiki/superfluid superfluids have zero viscosity , but may have surface tension . those with surface tension creep up walls in a capillary-like fashion ( except here , they can creep up a single wall , whereas capillary action requires a tube ) . i do not know about the immiscible liquids , though . what you are saying may be true for certain anisotropic liquids . but i am not sure of this . or maybe weird stuff will happen if you mix immiscible superfluids . ( speculation follows ) they may actually form alternating bands , when one liquid climbs up the wall , the other liquid climbs up the film of the first liquid on the wall . this may happen for two superfluids with different densities and surface tensions .
different wave functions with the same $|\psi ( x ) |^2$ represent different physical states ( unless they are proportional ) . different states means that one gets different measurable results on at least one kind of measurements . the same $|\psi ( x ) |^2$ gives the same probability density for position measurements ( only ) , but generally not for measurements of other observables such as momentum . for the momentum probability density , the absolute squares of the fourier transform counts , and this is usually different if only the $|\psi ( x ) |^2$ are the same . the mathematical content of the wave function is the following ( from which the above follows ) : the inner product of $\psi$ with $a\psi$ gives the expectation value of the operator $a$ for a system in state $\psi$ . for example , if you take $a$ to be multiplication by the characteristic function of a region in $r^3$ you get the probability for being in that region . the position operator is simply multiplication by $x$ , while the momentum operator is a multiple of differentiation . for going deeper , try my online book http://lanl.arxiv.org/abs/0810.1019 , written for mathematicians without any background knowledge in physics .
construction of the helicity formula using 3-vector notation the zero component of the pauli lubanski vector $w^0 = \epsilon^{0 ijk}j_{ij}p_k = \epsilon^{ijk}j_{ij}p_k $ the angular momentum genrerators $ j^k = \epsilon^{ijk}j_{ij}$ thus $w^0 = j^k p_k = \vec{j} . \vec{p} $ the orbital angular momentum $ \vec{l} = \vec{x} \times \vec{p}$ is orthogonal to the momentum : $ \vec{l} . \vec{p} = 0$ and since the total angular momentum is the vector sum of the orbirtal and the spin angular momenta $ \vec{j} = \vec{l} + \vec{\sigma}$ thus $w^0 = j^k p_k = \vec{j} . \vec{p} = ( \vec{j-l} ) . \vec{p} = \vec{\sigma} . \vec{p} $ now , since $w^0 = \hat{h} p_0$ and for a massless particle $ p_0 = p$ we obtain : $ \hat{h} = \frac{\vec{\sigma} . \vec{p}}{p_0} = \vec{\sigma} . \hat{p}$ the helicity operator $$\hat{h} = \sigma . \hat{p}$$ where $\sigma$ is the spin operator and $\hat{p}$ is the momentum unit vector is a projection along the axis $\hat{p}$ of a spin operator , thus one might expect it to have for a helicity $\lambda$ the eigenvalues $\lambda$ , $\lambda-1$ , . . . , $-\lambda$ . however , the eigenvectors corresponding to all eigenvalues except $\pm \lambda$ are not physical , because they describe longitudinal polarizations which do not exist in free massless particles . here is an example of the massless spin-1 case ( photon ) . in this case , we may choose the spin operators as : $ \sigma_x = \begin{bmatrix} 0 and 0 and 0\\ 0 and 0 and -i\\ 0 and i and 0 \end{bmatrix}$ $\sigma_y = \begin{bmatrix} 0 and 0 and i\\ 0 and 0 and 0\\ -i and 0 and 0 \end{bmatrix}$ $\sigma_z = \begin{bmatrix} 0 and -i and 0\\ i and 0 and 0\\ 0 and 0 and 0 \end{bmatrix}$ the action of the helicity operator on ( say ) , the electric field in the momentum representation is : $$\hat{h} \vec{e} = i\begin{bmatrix} 0 and -\hat{p}_z and \hat{p}_y\\ \hat{p}_z and 0 and -\hat{p}_x\\ -\hat{p}_x and \hat{p}_x and 0 \end{bmatrix}\begin{bmatrix} e_x\\ e_y\\ e_z \end{bmatrix} = i \hat{p}\times \vec{e}$$ thus : $$\hat{h}^2 \vec{e} = - \hat{p} \times ( \hat{p}\times \vec{e} ) = \vec{e} -\hat{p} ( \hat{p} . \vec{e} ) $$ but , since for a free electromagnetic field : $$\hat{p} . \vec{e} = 0$$ we get : $$\hat{h}^2 = 1$$ , and the only admissible eigenvalues are $\pm 1$
there are several different levels of advanced in quantum mechanics . i will try to answer using these levels of quantum mechanics : basic : single particle or single particles interacting with a single atom/nucleus , or classical field picture--- anything einstein would have been comfortable with . advanced : highly entangled many-body quantum mechanics , involving many-body effects that cannot be understood from single-body or single-field picture . inscrutable : quantum computing--- actual exponential computation as compared to classical behavior . i will give an off-the-cuff list of things that were predicted theoretically at each of the three levels , that was hard to understand from just seat-of-the-pants non-quantum intuition . at level 1 , there are essentially as many examples as you care to list : electron diffraction : the diffraction of electrons from crystals was one of the early predictions of quantum mechanics which was confirmed experimentally . knowing that electrons diffract is important for the construction of electron microscopes , and you need to know the relation between wavelength and momentum . lasers come from spontaneous emission theory , and einstein 's prediction that a coherent collection of bosons will make other bosons be created with the same momentum preferentially was very surprising . this is the basic idea behind lasers , and bec 's , and these were only found because the theoretical principles were known in advance . slow neutrons are dangerous : if you classically estimate the neutron scattering cross section off a nucleus , at low momenta , you are completely wrong , because of resonance effects . these effects can blow up the nucleus to look ( to the neutron ) as it it were the size of a barn , when it is normally the size of a kernel of corn . you can look up the unit " barn " for a more accurate etymology . qualitative chemistry : if you use simple quantum mechanical orbitals , and the notion of superposition ( which is called resonance in chemistry ) , you can get an idea of which molecules will make dyes , what shapes will be preferred , and so on . these types of things were worked out by linus pauling , and led to the discovery of the alpha-helix , and later , to the structure of dna . there are too many class-1 examples to list , so consider class 2 . here , one is looking for a theoretical insight in a many-body system , with a highly entangled wavefunction , which leads to practical predictions . the easiest example that comes to mind is bcs theory . bcs theory : this predicts that any very cold fermi system with the weakest of attractive interactions will produce a strange vacuum state , where it is like a bose einstein condensate of paired-fermions , even when the force is too weak to bind two individual fermions into actual pairs . the presence of other fermions in the sea is essential , it makes a condensate of particles which do not exist really . one of the most striking prediction of bcs theory was the prediction that he3 should become superfluid at ultra-cold temperatures . there is no reason you would suspect this from experiments on superconductors , without the detailed theory of cooper pairing . this was spectacularly confirmed by difficult experimental work of lee , osheroff , and richardson , work which was awarded the 1996 nobel prize . the theory of renormalization is quantum , class 2--- many body . but it is equally applicable to statistical systems , where simple models allow one to predict all sorts of phenomena that were not suspected experimentally . here is an example : anderson localization in 1d and 2d : any sufficiently long wire is insulating . any sufficiently large sheet of conductor is also insulating . you would never guess even the 1d business from experiment , but it is true , and needs to be considered when you make very thin wires . anderson localization itself is in class 1 , but the renormalization analysis which allows you to say things like this is class 2 . quantum field theory has made contact with experiment , most elegantly through 2d conformal field theory : rational 2d critical exponents : this was predicted from sophisticated quantum field theory considerations , relying on the conformal algebra from string theory , relying on the 2d conformal field theory of belavin , polyakov zamolodchikov . that in itself was an extension of 1960s work on operator product expansion , by zimmermann , wilson , kadanoff and polyakov , which defined the correct algebra for renormalized fields . the rational critical exponents are confirmed experimentally using systems as divers as polymers , 2d fluids , but also using exact solutions , and conputer simulations . you would have a hard time guessing an exponent is rational from an experiment . but by far the most spectacular type 2 quantum theoretical application is : semiconductor physics : the qualitative ideas of semiconductor physics , including the existence of " p-type " charge carriers , were understood theoretically in tandem with the experimental production of these materials . the theory of doping is not so sophisticated--- you need to know which are donors and which are acceptors , but the theory of p-type semiconductors relies crucially on many-body effects , so that you have particle hole symmetry . this is the central technological advance of the late 20th century , and made possible the computer revolution . in class 3 , there are several potential applications : simulating quantum systems : as feynman noted , a quantum computer will be able to simulate other quantum systems efficiently . this is impossible on a classical computer . factoring : given a quantum computer , peter shor showed how to factor numbers , which will make current cryptographic systems insecure . grover 's database search : this allows you to search a database with n items in $\sqrt{n}$ steps . guaranteed secure communications : you can make a channel in which you can ensure that you and your communication partner are not eavesdropped on . for the other applications in this class , i defer to neilson and chuang . the problem with class 3 applications ( at least the full blown computational ones ) is that we are not going to be 100% sure they will work until we build them . the other option is that quantum mechanics will fail for these .
to make it fall you need a torque . this torque is provided by the weight force acting on the center of mass of the object and by the offset between the center of mass and the edge of the object . imagine your domino standing upright then tilt it . you are moving the center of mass . when the center of mass ( blue ) is on the right of the edge ( red ) then you have a torque , represented by the triangle . the torque is $\tau = m g d$ so to make it fall you need $d$ greater than zero . if the domino ( of uniform constant density ) as base of width l the center of mass is located at l/2 . for an height h , the center of mass is located at height h/2 . that his other sketch : taking this one in the limit case where $d=0$ you obtain the last sketch . solving the trigonometry you obtain $\alpha = atan \frac{l/2}{h/2}$ . the angle of the domino with respect to the " table " is $90-\alpha$ degrees .
colin 's comment is spot on , but to expand a bit on the " lots of details " he mentioned , heat radiated from the earth 's surface is partially absorbed by greenhouse gases in the troposphere , and because the troposphere is turbulent this heat gets redistributed throughout the troposphere instead of escaping into space . if you e.g. double the co$_2$ content of the troposphere it will intercept and redistribute more of the heat radiated from the earth , so the earth will overall radiate less heat into space . because the earth is now radiating less heat than it receives , it gets hotter . but as it gets hotter more heat is radiated from the surface and more escapes into space . eventually the temperature rises until the heat radiated once again matches the heat received , and the temperature stabilises .
they are the same , because integration is linear : $$\int _{t_1}^{t_2} \left ( f ( t ) - g ( t ) \right ) \ , dt = \int_{t_1}^{t_2}f ( t ) \ , dt - \int_{t_1}^{t_2}g ( t ) \ , dt$$ addendum : consider two paths . let the system trajectory in the first path be denoted $x_1 ( t ) $ and the trajectory for the second path be denoted $x_2 ( t ) $ . let the lagrangian be denoted $l$ . then the action for the first trajectory is $$s_1 = \int_{t_1}^{t_2}l ( x_1 ( t ) ) \ , dt$$ and the action for the second trajectory is $$s_2 = \int_{t_1}^{t_2}l ( x_2 ( t ) ) \ , dt . $$ the difference in the action between the two paths is \begin{eqnarray} \delta s \equiv s_1 - s_2 and = and \int_{t_1}^{t_2}l ( x_1 ( t ) ) \ , dt - \int_{t_1}^{t_2}l ( x_2 ( t ) ) \ , dt \\ and = and \int_{t_1}^{t_2} \left [ l ( x_1 ( t ) ) -l ( x_2 ( t ) ) \right ] \ , dt \\ and = and \int_{t_1}^{t_2}\delta l ( t ) \ , dt \end{eqnarray} in the last equation we wrote $\delta l$ to denote $l ( x_1 ) - l ( x_2 ) $ . this is not quite what we are going for : we want to get $\delta l$ inside the integral , where $\delta l$ is the variation of the lagrangian . the variation is the derivative of a function with respect to something . when you think about the definition of a derivative , you have something like $$\frac{df}{dx} ( x_0 ) \equiv \lim_{h\rightarrow 0}\frac{f ( x_0+h ) -f ( x_0 ) }{h} . $$ so really , taking the $\delta$ inside the integral requires the linearity property we already demonstrated and the ability to move the limit inside the integral . this is ok in almost every case you will ever encounter . to prove why you can move the limit inside the integral you have to do some analysis which i really do not want to recall and type here . you can find that sort of thing in analysis books .
as said in the comments , this is a very broad question , so instead of writing a very long post , i point you to a good article titled " superconductivity and the environment : a roadmap": http://iopscience.iop.org/0953-2048/26/11/113001 . the article lists a lot of emerging technologies that make use of superconductors . the applications of room temperature superconductors would be the same as the applications of normal superconductors , but these applications would just be much easier to realize if cryogenic environment is not needed . many items listed in the article would become preferred over non-superconducting way of doing things if an easy-to-use material with room temperature superconductivity was found . since there is no complete theory as for what causes superconductivity in high temperatures , it is impossible to guess when ( if ever ) a rts is found . finding these materials is basically educated guessing an a lot of trial-and-error . it could be that someone stumbles upon such material tomorrow or it could be that room temperature superconductors do not even exist . there is no way to know .
they are different because in a transmission line we have distributed resistance , capacitance , conductance and inductance ( meaning that each tiny segment of transmission line has its own tiny resistance , capacitance , conductance and inductance ) while in rlc circuits we have lumped resistance , inductance and capacitance . also rlgc does not model a transmission line with the circuit you have shown above but with infinite number of them in series . we know well how to deal with lumped elements and circuits containing them , but dealing with distributed elements and circuits ( e . g transmission lines ) is often much harder and we have to resort to solving maxwell equations directly . so i think not only there is no point in approaching rlc circuits from rlgc model but also it is impractical .
if you see it staying in one spot you can infer that it is moving directly along your line of sight , as you say . but in the more likely event that you see it move across the sky you can determine that it is moving in a given plane only . unless you have some independent way of judging its size or distance you can not tell any more . this actually happens all the time when a bug flies in front of someone 's camera and they think they have seen an interstellar visitor .
what you need is the euler-bernoulli beam theory . the last three pages of this pdf explain the eigenmodes .
the atmospheric pressure at stp is 101325 n/m$^2$ , so 100 times this is 1.01325 $\times$ 10$^7$ n/m$^2$ . you just have to work out the height of a column of water with a 1 m$^2$ base and weighing 1.01325 $\times$ 10$^7 /g$ kg , where $g$ is the acceleration due to gravity . i make it about 1.03 kilometers , though note that it will vary slightly with temperature because the density of water varies with temperature .
in a new paper , rodejohann and zhang write ( pages 13 to 14 ) that in the standard model ( with massless neutrinos ) , the top yukawa can never rg-evolve to exactly 1 , but that this becomes possible once you have massive neutrinos . then it will grow beyond 1 as you continue to still higher energies . but they also write that attaining the exact value 1 could indicate " the restoration of certain kinds of yukawa unifications or flavor symmetries " . so if you can find a form of symmetry breaking which sometimes occurs when a coupling is exactly unity , and then use it appropriately in a gut or other model of new physics . . . then you will have an explanation .
the stefan-boltzmann law governs the irradiance ( radiant power per unit area ) . the total energy is not sigma*t^4 , but rather the total power $p$ of a body with surface area $a$ and temperature $t$ is given by \begin{equation} p=a\sigma t^4 \end{equation} this result may be surprising , but it is correct . the reason irradiance rises so quickly is because the wavelengths of light decrease with increasing temperature , carrying away more energy with each photon ( on average ) . at the same time , higher temperatures cause the photon emission rate from the surface to increase . it is the increase in the energy of the photons coupled with the increase in photon production rate that gives the $t^4$ dependence .
firstly , definition of torque is $\vec{r}\times \vec{f}$ and angular momentum $\vec{r}\times \vec{p}$ . and now w.r.t. your frame $\vec{f}$ and $\vec{p}$ and $\vec{r}$ are all relative . but newton 's second law of rotation holds for all frames . . because all points are just frames and to maintain the distances in frame , you have to move with that frame , and as force and momentum both are relative to your frame , so will be torque and angular momentum , but the thing is they will all give the correct angular accelerations and angular velocities and linear velocities relative to them as newton 's laws can be made valid in all frames ( by applying pseudo force in some ) . and answer in your book must be given in absolute terms , you can find correct answer by then applying gallilean relativity to your frame .
when you calculate work , you do so along a given path . here , that path has tangent vector $d\mathbf s$ . this is a vector with direction ; the minus sign will ultimately come from choosing the path 's orientation--inward or outward . edit : aha , i think i have found the unintuitive part . the key is in the use of the coordinate $r$ to parameterize the path , in that $r$ is larger at the start of the path and smaller at the end . this runs counter to what you would usually do when parameterizing such a path with an arbitrary parameter . let $\mathbf s_0$ and $\mathbf s_1$ be the starting and ending points of a path $\mathbf s ( \lambda ) = \mathbf s_0 + ( \mathbf s_1 -\mathbf s_0 ) \lambda$ . the work integral is then $$w = \int_{\mathbf s_0}^{\mathbf s_1} \mathbf f ( \mathbf s ) \cdot d\mathbf s= \int_0^1 \mathbf f ( \mathbf s ( \lambda ) ) \cdot \frac{d\mathbf s}{d\lambda} \ , d\lambda = \int_0^1 \mathbf f ( \mathbf s ( \lambda ) ) \cdot ( \mathbf s_1 - \mathbf s_0 ) \ , d\lambda$$ for two finite points , the basic approach is sound , but it breaks down when you have a point at infinity involved . this is the reason that the problem of assembling a configuration is usually attacked with a different basic parameterization . instead , set $\mathbf s ( \lambda ) = \lambda \hat{\mathbf a}$ for some unit vector $\hat{\mathbf a}$ and set the bounds of the integral as being from $ [ \infty , r ) $ . this is the important point : even though the path is being traversed coming in from infinity , the parameterization means that $d\mathbf s/d\lambda = + \hat{\mathbf a}$ , not minus as i originally thought . the path 's still oriented outward ; we are just traversing it backwards . here 's how that integral looks : $$w = \int_{\infty}^r \mathbf f ( \lambda \hat{\mathbf a} ) \cdot \hat{\mathbf a} \ , d\lambda$$ of course , we know the expression for the electric force : $$\mathbf f ( \mathbf r ) = k\frac{qq_0 \mathbf r}{|r|^3}$$ plug in $\mathbf r = \lambda \hat{\mathbf a}$ to get $$\mathbf f ( \lambda \hat{\mathbf a} ) = k \frac{qq_0 \lambda \hat{\mathbf a}}{\lambda^3} = k \frac{q q_0 \hat{\mathbf a}}{\lambda^2}$$ we find that the integrand is then $$w = \int_{\infty}^r k \frac{q q_0}{\lambda^2} \hat{\mathbf a} \cdot \hat{\mathbf a} \ , d\lambda = \int_\infty^r k \frac{q q_0}{\lambda^2} \ , d\lambda = - k \frac{q q_0}{r} &lt ; 0$$ the work is negative , so the change in potential energy $\delta u = - w$ is positive as required . so where is the problem then ? as we have seen , there actually should not be an extra negative sign coming in on line 4 ( as posted in the op 's question ) . this is somewhat obscured because an explicit parameterization of the path is never written down in the first place--usually , you do not have to , but this problem is tricky enough that it helps immensely .
the momentum eigenstate is not normalizable . suppose $x$ is a periodic variable with period $2\pi r$ -- the periodicity is important as momentum is then a `good ' operator . then one has the allowed values of momenta are quantized i.e. , $p_n = \frac{n \hbar}{ ( r ) }$ with $n\in \mathbb{z}$ . for the values of $p$ mentioned above , the momentum eigenstates are normalizable with $c=1/\sqrt{2\pi r}$ . let us call this normalized state $\langle x|n\rangle$ in the coordinate basis . explicitly , one has $$\langle x|n\rangle = \tfrac1{\sqrt{2\pi r}}\ e^{ip_n x/\hbar}\ . $$ ( you should treat the remark in the ucsd page about one particle to mean that your normalize your state to one . ) the eigenstates are orthogonal to each other as one can check explicitly . $$ \langle n | m\rangle =\delta_{n , m}\ . $$ the next step is to go from the box normalizable states to the delta function normalizable states . one needs to take $r\rightarrow\infty$ and convert kronecker delta into the dirac delta function . one uses the following identification which follows from the properties of the dirac delta function and standard integration : $$ \sum_m = \frac{r}{\hbar}\int dp \quad \mathrm{and}\quad \delta_{m , n} = \frac{\hbar}{r} \delta ( p_n-p_m ) \ . $$ with these identifications , we define $$ |p_n\rangle =\sqrt{\tfrac{r}{\hbar}}\ |n\rangle\ . $$ it is now easy to see that the dirac delta normalization implies that in the $x$-basis , one has $$u_p ( x ) :=\langle x|p_n\rangle = \tfrac1{\sqrt{2\pi \hbar}}\ e^{ip_n x/\hbar}\ . $$ this leads to a simple mnemonic : replace $r$ by $\hbar$ to go from box normalizable states to delta function normalization . of course , momentum which was discrete in a box now takes values in the continuum .
for questions about resonances and particles the particle data group is the best reference . one can find the whole delta resonance family and remind oneself what each number is standing for , and thus know how to pronounce the symbol . the number in parenthesis is the mass in mev . the superscript is the charge of the particular resonance displayed on the plot , presumably . s , p , d , . . . are by convention the labels of the angular momentum quantum number " l " , and the two numbers are the numerators of the isospin and j quantum number ( j is the total angular momentum quantum number ) . so the first one is read as : delta zero seventeen fifty ( pee three one ) or ( pee three halves one half ) . etc . ( the superscript of parity is missing in your information . ) the bar over a symbol denotes an antiparticle , antidelta ( 1910 ) zero in the second line . i would try and put the charge next to the main symbol , your first option but the other way is clear also . for similar questions the naming scheme for hadrons would be a help in comprehension as well as pronunciation .
heating almost any material will cause it to expand . that is , its density will go down , as the same mass of material takes up more space . or , alternately , the same volume of the material weighs less . if you take your bucket and fill it to the brim with cool water then heat the water ( not to boiling ) , some of it will spill out of the top . ps : water is one of a very few materials that does not follow this rule perfectly . water is actually at its densest ( and heaviest for a given size container ) at 4c . if you compare a bucket of 4c water to a bucket of 1c water , the warmer bucket will be heavier .
first regarding : is there any appreciation for how the incompleteness theorems might apply to physics ? to put this in perspective , image newton said " oh , looks like my $f = m a$ is pretty much a theory of everything . so now i could know everything about nature if only it were guaranteed that every sufficiently strong consistent formal system is complete . " and then later lagrange : " oh , looks like my $\delta l = 0$ is pretty much a theory of everything . so now i could know everything about nature if only it were guaranteed that every sufficiently strong consistent formal system is complete . " and then later schrödinger : " oh , looks like my $i \hbar \partial_t \psi = h \psi$ is pretty much a theory of everything . so now i could know everything about nature if only it were guaranteed that every sufficiently strong consistent formal system is complete . " and so forth . the point being , that what prevented physicists 300 years ago , 200 years ago , 100 years ago from in principle knowing everything about physics was never any incompleteness theorem , but were always two things : they did not actually have a fundamental theory yet ; they did not even have the mathematics yet to formulate what later was understood to be the more fundamental theory . gödel 's incompleteness theorem is , much like "$e = m c^2$" in the pop culture : people like to allude to it with a vague feeling of deep importance , without really knowing what the impact is . gödel incompletenss is a statement about the relation between metalanguage and " object language " ( it is the metalanguage that allows one to know that a given statement " is true " , after all , even if if cannot be proven in the object language ! ) . to even appreciate this distinction one has to delve a bit deeper into formal logic than i typically see people do who wonder about its relevance to physics . and the above history suggests : it is in any case premature to worry about the fine detail of formal logic as long as the candidate formalization of physics that we actually have is glaringly insufficient , and in particular as long as it seems plausible that in 100 years form now fundamental physics will be phrased in new mathematics compared to which present tools of mathematical physics look as outdated as those from a 100 years back do to us now . just open a theoretical physics textbook from the turn of the 19th to the 20th century to see that with our knowledge about physics it would have been laughable for the people back then to worry about incompleteness . they had to worry about learning linear algebra and differential geometry . and this leads directly to second : has any progress been made on hilbert 's 6th problem for the 20th century ? i had recently been giving some talks which started out with considering this question , see the links on my site at synthetic quantum field theory . one answer is : there has been considerable progress ( see the table right at the beginning of the slides or also in this talk note ) . lots of core aspects of modern physics have a very clean mathematical formulation . for instance gauge theory is firmely captured by differential cohomology and chern-weil theory , local tqft by higher monoidal category theory , and so forth . but two things are remarkable here : first , the maths that formalizes aspects of modern fundamental physics involves the crown jewels of modern mathematics , so something deep might be going on , but , second , these insights remain piecemeal . there is a field of mathematics here , another there . one could get the idea that somehow all this wants to be put together into one coherent formal story , only that maybe the kind of maths used these days is not quite sufficient for doing so . this is a point of view that , more or less implicitly , has driven the life work of william lawvere . he is famous among pure mathematicians as being the founder of categorical logic , of topos theory in formal logic , of structural foundations of mathematics . what is for some weird reason almost unknown , however , is that all this work of his has been inspired by the desire to produce a working formal foundations for physics . ( see on the nlab at william lawvere -- motivation from foundations of physics ) . i think anyone who is genuinely interested in the formal mathematical foundations of phyiscs and questions as to whether a fundamental formalization is possible and , more importantly , whether it can be useful , should try to learn about what lawvere has to say . of course reading lawvere is not easy . ( just like reading a modern lecture on qft would not be easy for a physicist form the 19th century had he been catapulted into our age . . . ) that is how it goes when you dig deeply into foundations , if you are really making progress , then you will not be able to come back and explain it in five minutes on the dicovery channel . ( as in feynman 's : if i could tell you in five minutes what gained me the nobel , then it would not have . ) you might start with the note on the nlab : " higher toposes of laws of motion " for an idea of what lawverian foundations of physics is about . a little later this month i will be giving various talks on this issue of formally founding modern physics ( local lagrangian gauge quantum field theory ) in foundational mathematics in a useful way . the notes for this are titled homotopy-type semantics for quantization .
okay , so i did some poking around and the 66th-75th editions of the crc handbook of chemistry and physics all have the incorrect atomic mass of cu-63 [ 62.939598 ] , and from 76th edition on they seem to have figured it out . those isotope mass tables are put together from a number of sources , so it is hard ( time consuming ) to tell exactly where the error came from . i did notice however that starting in the 76th edition of the crc , where they get it right , they start citing g . audi and a.h. wapstra , " the 1993 atomic mass evaluation " , nuc . phys . a 565 ( 1993 ) 1-65 . in editions 66-75 , they were citing audi and wapstra 's " the 1983 atomic mass table " which appeared in nuc . phys 432 , 1 ( 1985 ) . now , i looked at the 1993 version , and it has the correct 62.929 . . mass , but i have not been able to find wapstra and audi 's 1983 version of the same table , so i do not know if it was one error by the crc which got carried over year after year , or if 62.939 . . is in fact the value given in that paper . i did find at least one piece of evidence which points to an error by the editors of the crc in the 2nd edition of the encyclopedia of physics [ lerner and trigg , 1991 ] . their table of isotopes lists the correct 62.929 . . . value and also cites wapstra and audi 's 1983 paper . i hope that satisfies everyone 's curiosity , because i do not think i can do any better then that . ; - )
the nuclei used are the ones which can be fissioned in a chain reaction ! the lighter ones you think of will not split . that is simply a result of experiments .
i can not really answer your question because calculating a comet 's orbit is not something i can describe in a few lines ( actually it is not something i can describe at all , but as always google is your friend ! ) . as martin says , finding your comet is easy in principle . you photograph the same bit of sky on ( at least ) three occasions a few days apart , then you look for anything that has moved i.e. that is in different positions in the three pictures . this sounds easy , but there is a lot of sky and only a few comets so it is incredibly painstaking work . professionals have automated systems for comparing images and looking for moving objects , but amateurs just have to get on with it . anyhow , once you have found your moving object you can use the position in the sky and the change in position to calculate the orbit . martin casually describes this as " a bit of algebra " but actually it is rather a lot of algebra . i found this book online if you really want the gory details . i imagine most amateur astronomers would just feed their measurements into some software rather than do all the sums themselves . i confess i know little about this area but a quick google found several calculators e.g. this one . incidentally , in a previous question you asked about how to tell a hyperbolic from a parabolic ( and elliptical ? ) orbit . there is no easy way to tell except by calculating the orbit to see what shape it is .
they always reduce the field , and this is the law that magnetic fields induce currents that reduce their strength , a special case of lechatelier 's principle .
the energy differences are not due to the isospin which has little direct impact on the energy . instead the energy difference is due to the spin-spin interaction . however , having a different isospin state impacts that allowed spin states . for the mesons , which are bosons , the states must be symmetric . on the other hand , the baryons , which are fermions , the states must be antisymmetric . consider the $\eta$ and $\pi$ 's . the pions , having a symmetric isospin , must have a symmetric spin . the triplet state is less energetic than the singlet . this makes the pions lighter . a similar arguement can be made for the baryons .
in classical mechanics , you can make up a complicated system with many different natural frequencies . in general , these frequencies are completely independent of each other . due to non-linearities in the coupling forces , it may happen that when two modes are vibrating simultaneously , you get a new frequency appearing in the spectrum as the sum or difference or the two primary modes . but in quantum mechanics , you never see the primary modes at all . . . you only see the sum or difference frequencies . furthermore , if you try to explain them by non-linear forces , you should also expect to see multiples of the fundamental frequencies . these are absent in , for example , the spectra of atoms . it is hard to explain by a classical model involving things like masses and springs . it manifests itself in qm , of course , because the " fundamental " frequencies , the natural modes , evolve in time without any oscillating charges associated with them . the oscillating charges only appear when you have the superposition of two fundamental modes . this is how quantum mechanics is very different from classical .
concerning ( 1 ) , i strongly suggest you b . o'neill 's textbook about semi-riemannian geometry . it is written for mathematicians interested in gr without any particular background in physics . the last chapters focus on the causal structure of spacetime and singularity theorems . there is a large part completely devoted to analyze schwarzschild 's metric and kruskal 's manifold . ( 2 ) i guess that for " lorentzian manifold " you actually means minkowski spacetime . it seems that you are mostly interested in general realtivity , so what you should learn is the general theory of semi-riemannian manifolds . ( 3 ) - ( 4 ) - ( 5 ) schwarzschild metric is the metric of a very important semi-riemannian manifolds describing the spacetime in the presence of spherical symmetry . that metric is indeed a solution of einstein field equations . though the maximal extension of schwarzschild manifold contains a metrical singularity , the singularities considered in hawking-penrse 's theorems are of more general nature . there are several statements of the singularity theorems . however , all these statements say that : under some physically sensible hypotheses on : ( a ) the global spacetime geometry ( e . g . global hyperbolicity ) , ( b ) its energy-momentum content ( e . g . , validity of some energy condition ) , and ( c ) assuming that the gravity is somewhere strong enough ( typically , existence of trapped surfaces ) , then there exists a maximally extended causal geodesic that is incomplete .
after much investigation , simulation and a deep literature search , i have figured out the true answer . you perceive a chirp because you are being hit with the echos of the sharp noise that generated the sound . the times between the arrival of those echos is decreasing inversely with time , so it sounds as if it were a tone with a fundamental frequency increasing linearly in time , hence the chirp . to get a feel for the phenomenon , consider a simulation : above you see a slowed down version of the simulated pressure wave inside a 2d racquetball court . i threw up the generated sound on soundcloud . if you watch the simulation , pick a particular point and watch the reflected sounds go by , you will notice the different instances of the multiple echos arrive faster and faster as time goes on . you can clearly hear the chirps in the generated sound , and if you listen closely you can hear secondary chirps as well . these are also visible in the spectrogram : this phenomenon was studied and published recently by kenji kiyohara , ken'ichi furuya , and yutaka kaneda : " sweeping echoes perceived in a regularly shaped reverberation room , " 　j . acoust . soc . am . vol . 111 , no . 2 , 925-930 ( 2002 ) . more info in particular , they explain not only the main sweep , but the appearance of the secondary sweeps using some number theory . worth reading in full . this suggests that for the best sweep one should both stand and listen in the center of the room , though they should be generic at any location . simple geometric argument following the paper , we can give a simple geometric argument . if you imagine standing in the middle of a standard racquetball court , which is twice as long as it is tall or wide , and clap , your clap will start propagating and reflecting off the walls . a simple way to study the arrival times is with the method of images , so you imagine other claps generated by reflecting your clap across the walls , and then reflections of those claps and so on . this will generate a whole set of " image " claps , located at positions $$ ( m , l , 2k ) l $$ where $m , l , k$ are integers and $l$ is 20 feet for a racquetball court , the time for any particular clap to reach you is $t = d/c$ and so we have $$ t = \sqrt{m^2 + l^2 + 4k^2} \frac{l}{c} $$ for our arrival times . if we look at how these distribute in time : it becomes clear why we perceive a chirp . the various sets of missing bars , which themselves are spaced like a chirp , give rise to our perceived subchirps . details of the 2d simulation for the simulation , i numerically solved the wave equation : $$ \frac{\partial^2 p}{dt^2} = c^2 \nabla^2 p $$ and used impedance boundary conditions on the walls $$ \nabla p \cdot \hat n = -c \eta \frac{\partial p}{\partial t} $$ i used a collocation method spatially , with a chebyshev basis of order 64 in the short axis and 128 on the long axis . and used rk4 for the time integration . i modeled the room as 20 feet by 40 feet and started it of with a gaussian pressure pulse in one corner of the room . i listened near the back wall towards the top corner . i put up an ipython notebook of my code , with the embedded audio and video . i recommend playing with it yourself . on my desktop it takes about minute to do a full simulation of the sound . effect of listening location i have updated the code to generate sound at multiple locations , and generate their sounds . i can not seem to embed audio on stackexchange , but if you click through to the ipython notebook view , you can listen to all of the generated sounds . but what i can do here is show the spectrograms : these are laid out in roughly their locations inside of the room . here the noise was generated in the lower left , but the chirps should be generic for any listening and generation location .
on point i . ) : johhnymo1 's comment touches the essential point , though the result he quoted holds assuming that the manifold is hausdorff . i emphasize this because the definition of paracompactness in the literature is not uniform - sometimes it is assumed that a paracompact topological space is hausdorff , sometimes not ( m . w . hirsch 's book " differential topology " , for instance , does not - neither does he assume that a manifold must be hausdorff , by the way ) . more precisely , the result is a direct consequence of the smirnov metrization theorem : a topological space is metrizable if and only if it is hausdorff , paracompact and locally metrizable ( i.e. . any point has an open neighborhood whose relative topology is metrizable ) . any manifold clearly satisfies the latter condition . ( edit : i have just got acquainted with the smirnov metrization theorem , which allows one to do away with the connectedness hypothesis . moreover , the counterexample i previously wrote is incorrect ) one should also add that paracompactness is equivalent to the existence of partitions of unity , which allow us to glue together locally defined objects in the manifold - for instance , this is how you prove existence of riemannian metrics . on point ii ) : if by " compact " you mean " compact without boundary " ( like $s^n$ ) , compact space-times indeed necessarily have vanishing euler characteristic - conversely , any compact manifold with vanishing euler characteristic admits a time oriented lorentzian metric . however , such space-times are not physically interesting because they necessarily have closed timelike curves . the argument is simple : since any space-time $ ( \mathscr{m} , g ) $ may be covered by the chronological futures of all its points ( which are open sets ) , using compactness one can pass to a finite subcover , say $\mathscr{m}=i^+ ( p_1 ) \cup\cdots\cup i^+ ( p_n ) $ . therefore , $p_1$ must belong to $i^+ ( p_{j_1} ) $ for some $j_1=1 , \ldots , n$ , $p_{j_1}$ must belong to $i^+ ( p_{j_2} ) $ for some $j_2=1 , \ldots , n$ , and so on . since we are dealing with a finite number of points , eventually one must have $p_{j_k}=p_1$ for some $k$ between $1$ and $n$ , thus producing a closed timelike curve . since such space-times are not globally hyperbolic , they are also unsuitable for the analysis of hyperbolic ( i.e. . wave-like ) pde 's . noncompact ( hausdorff , connected and paracompact , as in point ( i ) ) manifolds , on the other hand , always admit a time oriented lorentzian metric . a reference that discusses which topological hypotheses on space-time manifolds are natural is the classic book by s . w . hawking and g . f . r . ellis , " the large scale structure of space-time " ( cambridge university press , 1973 ) .
i take this question to mean : why does the laguerre-gaussian ( lg ) modes have an $e^{i\ell\phi}$ dependance on the azimuthal coordinate $\phi$ ? why is $\ell$ required to be an integer ? question 1 the lg modes are solutions to the paraxial wave equation in cylindrical coordinates . this means that we get solutions that reflect this symmetry . in particular the solutions should only trivially change if you make the change $$\phi\to\phi+\delta\phi . $$ if we define a rotation operator $r_{\delta\phi}$ such that this operator acting on any function $f ( \phi ) $ gives $$r_{\delta\phi}f ( \phi ) \equiv f ( \phi+\delta\phi ) $$ cylindrical symmetric solutions will be the eigenfunctions of $r_{\delta\phi}$ , i.e. $$r_{\delta\phi}f ( \phi ) =\lambda f ( \phi ) , $$ where $\lambda$ is a constant . the solution to this equation is of the form $$f_\ell ( \phi ) \sim e^{i\ell\phi} , $$ i.e. $$r_{\delta\phi}f_\ell ( \phi ) =e^{i\ell\delta\phi}e^{i\ell\phi}=\lambda_\ell e^{i\ell\phi} . $$ therefore cylindrically symmetric solutions such as the lg modes will be of the from $$lg ( r , \phi ) = f ( r ) e^{i\ell\phi} . $$ question 2 the reason $\ell$ has to be an integer ( i.e. . quantized ) is because $\phi$ is periodic . what this means is that $\phi$ and $\phi+2\pi$ are the exact same point , therefore all functions of $\phi$ must meet the requirement $$f ( \phi+2\pi ) =f ( \phi ) . $$ if our function is $e^{i\ell\phi}$ , as we saw in part 1 , then this means $$e^{i\ell ( \phi+2\pi ) }=e^{i\ell\phi}\to e^{i\ell 2\pi} =1 , $$ which is only true if $\ell$ is an integer .
i think that a sketch says more than equations and words the yoyo will , in an infinitesimal sense , have to move around the red dot ( but is kept from doing so by the table ) . the direction in which the yoyo will move , depends on the angle of the string , and thus the direction of torque around this touching point .
ssc : synchrotron self-compton bbc : compton upscattered blackbody radiation ; compton upscattering of stellar blackbody photons xc : upscattering of photons emitted by the accretion flow ; accretion flow photons from : http://arxiv.org/abs/1307.1309 and http://arxiv.org/abs/1403.4768
the uncertainty principle applies to any quantum system , and is way more general than just single particle examples . it is defined for any pair of operators ( physical quantities ) $a$ and $b$ , with the system in a state $|\psi\rangle$ $$ \delta a \ ; \delta b \geq \frac{\hbar}{2} \langle \psi| [ a , b ] |\psi \rangle$$ note : the constant factor ( $\frac{1}{2}$ here ) varies in different derivations , depending on how exactly you define $\delta a$ and $\delta b$ , but the essence is the same . in the case of simple quantum systems , you could take $a$ to be the position operator and $b$ to be the momentum operator . in your case , it seems like you would like to consider the whole nucleus as an effective particle and apply these operators on it is wavefunction/state . sure , you could do that , and you will get an uncertainty relation from that .
friction is always in the opposite direction of the movement of the object . if a ball rolls north on a floor , friction points south . for the ball to experience friction from the wall , it has to move along the wall . if the ball is thrown perpendicular to the wall it will not be moving along the wall , so it can not slip . however , if the ball is spinning , it will slip because the surface is moving along the wall .
first , let me restore the complex conjugation that was omitted without a good reason : $$ \rho_{\rm pure}=|\psi \rangle \langle\psi |= \big ( \begin{matrix} |\alpha|^2 and \alpha^* \beta \\ \alpha \beta^* and |\beta|^2 \end{matrix} \big ) $$ now , let us use a prettier ( inverse ) ordering of the tensor factors for the bra vectors . you meant : $$ \rho=|\psi \rangle \langle\psi | \rightarrow |\psi \rangle |r\rangle \langle r | \langle \psi| $$ $$ =|\alpha|^2 |0 \rangle |r_0\rangle \langle r_0 | \langle 0|+\alpha\beta^* |0 \rangle |r_0\rangle \langle r_1 | \langle 1|+\\ +\alpha^*\beta|1 \rangle |r_1\rangle \langle r_0 | \langle 0| +|\beta|^2 |1 \rangle |r_1\rangle \langle r_1 | \langle 1| $$ i think that the only extra step , key step of decoherence , you want to be shown is the partial trace of $\rho$ over the $r_0/r_1$ degree of freedom . we have $$\mathop{\rm tr}^{r_0 , r_1} \rho = |\alpha|^2 |0\rangle \langle 0| +|\beta|^2 |1\rangle \langle 1 | = \pmatrix{ |\alpha|^2 and 0\\ 0 and |\beta|^2} $$ that is it . note that the mixed terms did not contribute anything to the partial trace because $\langle r_0|r_1\rangle=0$ and similarly for its complex conjugate . the main result is that this reduced density matrix , a partial trace , after decoherence has vanishing off-diagonal entries , unlike the density matrix for the pure state $\rho_{\rm pure}$ that we started with . in more detailed calculations of decoherence , we usually take into account the fact that $|r_0\rangle$ and $|r_1\rangle$ that the environmental degrees of freedom evolve into just a moment later are not exactly orthogonal to one another . that means that the off-diagonal elements get reduced but they do not quite vanish instantly . however , the inner product is decreasing faster than exponentially as the same information is being imprinted and copied into additional degrees of freedom of the environment . by an exponentially growing avalanche , $\exp ( ct ) $ of qubits get modified according to the initial decohering bits . each of these environmental degrees of freedom or qubits contributes a factor of order $i\ll 1$ from the inner product to the off-diagonal entries so the off-diagonal entries go like $$ \rho_{12}\sim i^{\exp ( ct ) }=\exp ( -b\exp ( ct ) ) $$ where $b=\ln ( 1/i ) $ . to make the off-diagonal entries of the density matrix vanish or almost vanish after some time , the entanglement with the environmental degrees of freedom is essential . if the two subsystems were not entangled , in other words , if the total pure state were a tensor product $|a\rangle\otimes |r\rangle$ , the tracing over the $r$ degrees of freedom would give you the pure density matrix $|a\rangle\langle a|$ back : the system $r$ would have no effect on the system $a$ because of the lack of entanglement ( lack of correlation ) . you may read about decoherence e.g. at pages 9-16 here : http://www.karlin.mff.cuni.cz/~motl/entan-interpret.pdf
potentials are curious in that there is not exactly one way to define them . because a potential $\phi$ is related to a field $\mathbf{e}$ by \begin{equation} \mathbf{e}=\nabla \phi \end{equation} the class of potentials that satisfy the field equation is infinite . if a potential $\phi$ satisfies the field equation , then so will any potential $\phi'$ , where \begin{equation} \phi'=\phi+\lambda \end{equation} where $\lambda$ is any function that satisfies $\nabla \lambda=0$ . the simplest example is for $\lambda$ to be some constant value , but in general , an even larger class of functions works . by taking a difference of potentials , there is no ambiguity . if the potential at point $a$ and the potential at point $b$ are defined in terms of $\phi$ , their potential difference is \begin{equation} \delta \phi = \phi ( b ) -\phi ( a ) . \end{equation} doing this same exercise in terms of $\phi'$ gives \begin{equation} \delta \phi ' = \phi' ( b ) -\phi' ( a ) =\left ( \phi ( b ) +\lambda \right ) -\left ( \phi ( a ) +\lambda \right ) = \phi ( b ) -\phi ( a ) =\delta \phi . \end{equation} so while potentials are potentially ambiguous , differences in potentials are not . by convention , physicists frequently assign a potential of "$0$" to a point at infinity . this fixes the value of $\lambda$ and removes the ambiguity in the definition of potential . this is only convention , however , and there are certain problems that this approach cannot resolve . by defining the potential in terms of differences , your author avoided the issue entirely .
i can get you thinking along the right path , but i will leave most of the doing for you . i suspect that for much of this to make sense , you will need to actually do it . the idea is to start at the " top " of the ladder : $$|2 , 2\rangle = |3/2 , 3/2\rangle_1\otimes|1/2 , 1/2\rangle_2 =|3/2 , 3/2\rangle_1|1/2 , 1/2\rangle_2 . $$ with this statement ( and choice of phase ) , you are saying that the combined system can have maximum z-angular momentum only if each of the comprising particles also have maximum z-angular momentum . since we are at the top of the ladder , you can get the other $s=2$ states by applying the lowering operator $j_- = j_{1-} + j_{2-}$ to both the lhs and rhs . here , $j_{1-}$ only acts on the first particle , and $j_{2-}$ on the second . ( a more careful physicist would write $j_-=j_{1-} \otimes 1 + 1\otimes j_{2-}$ , but hey . ) the operator $j_-$ just acts on the lhs . doing this again and again should get you all of the $s=2$ states . the cg coefficients are just the numbers that sit out front of the states . you get these from carefully using the lowering operator . ( there are some shortcuts one can use , but this is the hard but thorough way that i learned . ) try this once to get one set of cg coefficients , and check your work with a table on the internet . those were the $s=2$ states . to get the $s=1$ states , you will have to use some orthogonality and a bit of convention . very briefly , note that $\langle 2 , 1\ , |\ , 1 , 1\rangle=0$ . also note that the vectors $ ( a , b ) ^t$ and $ ( b , -a ) ^t$ are orthogonal .
no , it would not violate the principle of special relativity , which proscribes information traveling faster than the speed of light . let 's call the farthest object we can see x . let 's call x 's neighbor on the far side from us y . the only way for us to get information about y is for information to travel to x ( at or below the speed of light ) , which would then influence x 's behavior . then , information about x 's behavior would have to travel to us ( at or below the speed of light ) . so , nowhere is information traveling faster than light . the only information we can get from x about y is already " out of date"* enough that special relativity is not violated . *the technical term relativists use is " retarded , " but be careful how you use it in everyday conversation . edit : we can make a distinction between the theoretically observable universe according to relativistic considerations , which is what i was talking about above , and the universe observable with present technology and practical limitations . getting sneaky like inferring about y from x actually does increase the latter meaning of observable universe .
there is an excellent talk by lawrence krauss on precisely this subject . i can not recommend watching it highly enough , you should start watching it before even reading the remainder of this post . in summary , we can model the matter just after the big bang at the time we see the cosmic microwave background and determine the characteristic distance scales of the " lumpiness " of the universe at that point . we can view the lumpiness of the universe then by observing the cosmic microwave background radiation at high resolution . now we have something that we can compare the expected visual size of to the apparent visual size , giving us information about the shape of the universe in between .
the double image is almost certainly because you have the axes misaligned with your eyes . the three adjustments you can make are ( usually ) : physical separation of the two lens assemblies main focus supplementary focus for one lens ( sometimes you have totally independent focus , but the above is more common ) the focus options will make things blurry , but having the barrels too wide apart , or too close together can mean your eyes can not easily resolve to a single image . try adjusting the separation first . if that does not help , it is possible you have a badly aligned pair of binoculars . . .
the last part of your question is the easiest to answer , so i will get to that first . the best book on the fundamentals of optical design is " modern optical engineering " by warren j . smith . it is not specific to aspheric optics , but does cover them in addition to the rest of geometrical optics and lens design . it is probably the single most common reference book among optical engineers . now , the rest of your question is a bit complicated , and needs a little bit of background , so bear with me for a moment . as has been mentioned , even an ideal lens will produce a focal spot of some minimum size , determined by the ratio of the lens focal length to its aperture ( this quantity is called the " f-number , or $f/\#$" ) and the wavelength of the light . this is what optical engineers call the diffraction limited spot size . for a circular aperture , the diameter of the diffraction limited spot size will be $$2.44 \times \lambda \times f/\#$$ where $\lambda$ is the wavelength . so as the $f/\#$ decreases ( as the lens gets " faster" ) the diffraction limited spot will become smaller . however , any aberrations in the lens will also become more significant ! this means that a very slow lens ( one with a long focal length , relative to its aperture ) can produce a diffraction limited spot even though it may have some aberration relative to an ideal lens , while a very fast lens will need to have a slightly aspheric shape to achieve diffraction limited performance . this is important to understand because it means that , in some cases , a spherical lens can indeed focus light as close to a point as is physically possible , even though a sphere is not the ideal shape . so what is that ideal shape ? well again , it depends on a few things . for both lenses and mirrors , the ideal shape will change depending on the distance from the object plane to the lens , and from the lens to the image plane . in the case you have asked about , where the incoming light is collimated , optical engineers would say that the object plane is at infinity . in this case , as some other people have pointed out , the ideal shape for a mirror is indeed a parabola . however , for a lens this is not the case . as it turns out , the ideal shape for a lens to focus a collimated beam of light to a point is to have the first surface of the lens ( the one the light hits first ) be elliptical , and the back surface be hyperbolic . lens designers usually specify the shape of a lens surface with the following equation : $$z = \frac{c r^2}{1 + \sqrt{1- ( 1+\kappa ) c^2 r^2}}$$ where $z$ is the " sag " of the lens surface , or its departure from a plane tangent to the lens surface at the center of the lens , $r$ is the radial distance from the center of the lens , $c$ is the curvature of the lens ( the reciprocal of its radius of curvature ) and $\kappa$ is called the " conic constant . " it is the value of $\kappa$ which determines what sort of conic section describes the surface : $\kappa &gt ; 0$ oblate ellipse $\kappa = 0$ sphere $0 &gt ; \kappa &gt ; -1$ prolate ellipse $\kappa = -1$ parabola $-1 &gt ; \kappa$ hyperbola on a related note , it is more than just the conic constant that can be adjusted to control aberrations . even with purely spherical surfaces , the relative curvature of the front and back lens surface can be varied , while keeping the effective focal length constant . adjusting this is more common than adding aspeheric surfaces to a lens , because aspheric surfaces are expensive to manufacture . many optical supply companies even offer off-the-shelf optics with an ideal bending ratio for a given application . these are often sold as " best form " lenses .
short answer : no . longer answer : no , excepting neutrinos none of the products of radioactive decay has the penetrating power to pass through the atmosphere , and neutrino detection is not something we can do from satellites . to elaborate , the immediate products of radioactive decay are ( some set of , depending on the decay in question ) fission fragments , electrons , positrons , alphas , neutrons , photons ( gamma rays ) and neutrinos . plus the remnant nucleus . the only secondary product which might be interesting is cerenkov light . the electrons and positrons will travel a number of cm in air ( at ground level ) . the gamma might go a few meters . the heavy stuff has no penetrating power at all . even if lofted to the top of the troposphere , there is just too much air in the way . cerenkov light will , of course , go through a lot of atmosphere , but you had be looking for a pale blue glow against the general light background . for dispersed radionucleides ( i.e. . contamination ) , the intensity will be awfully low . n.b. i too have seen various tv show and movie where some character from some agency says " we can track the radiation with satellites ! " . i believe this to be misinformed babbling of desperate script writers .
the mass will play the role in the relaxation time to go from a ballistic regime in the langevin equation to an overdamped regime where only diffusion matters . the bigger the mass , the higher the inertia and therefore the longer the time it takes to reach the overdamped regime . once the overdamped regime is reached or , to phrase it differently , if your time window allows you to only see the overdamped regime in both cases then you will see no difference between the two .
what i have calculated is just a fourier transform of the aperture $h ( x , y ) $: $$f ( \omega_x , \omega_y ) =\int dx\ , dy\ , h ( x , y ) e^{-i ( \omega_xx+\omega_yy ) }$$ ( and i was plotting $|f|^2$ as a function of $\omega_{x , y}$ each changing from -100 to 100 . ) already here one can see that $\omega_{x , y}$ both have a dimension of inverse length . so it is not an angle . now , the actual expression for fraunhofer diffraction is something like : $$u ( x ' , y' ) = \int dx\ , dy\ , h ( x , y ) e^{-i\frac{k}{z} ( xx'+yy' ) }$$ where $x'$ and $y'$ are coordinates on the screen , where you observe the diffraction pattern , $k$ is a wave-vector $k=\frac{2\pi}{\lambda}$ , and $z$ is the distance to the screen . as you can see these formulae are very similar . namely you get the fourier transform by renaming : $$\frac{kx'}{z}\leftrightarrow \omega_x\quad\frac{ky'}{z}\leftrightarrow \omega_y$$ so , in practice $\omega_{x , y}$ denote a position on the screen -- given $\omega_x$ , you get an $x'$ by rescaling : $$x'=\frac{\lambda z}{2\pi}\omega_x$$ finally let us put some numbers . let 's say that $\lambda=600nm$ and $z = 2m$ . then a point with $\omega_x = 50cm^{-1}$ will have a coordinate on the disk $x ' = 1.8cm$ .
the reason that these sorts of libraries do not exist is because the particular algorithm that you use to do the calculation will depend upon the exact details of the light field and the input and output planes you are trying to compute . for example , let me outline the the simplest case for this sort of calcualtion : the input light field $g_0$ has a slowly varying phase profile you want to propagate a short distance relative to the size of the beam ( or , in a more pure-mathematical approach , your input field has infinite extent ) the input and output planes are parallel and differ only by a translation along the z-axis the input and output computation grids have the same spacing this grid spacing is sufficiently small to nyquist sample the ( spatially band-limited ) input field . in this case , you can easily use the angular spectrum propagation ( asp ) method with ffts to quickly compute your propagated wavefront . you seem to understand basic scalar diffraction , but to clarify for other readers , this means you compute : $$\begin{eqnarray} g_0 ( \xi , \eta ) = \mathcal{f}\left [ g_0 ( x , y ) \right ] \\ g_z ( \xi , \eta ) = a_z ( \xi , \eta ) g_0 ( \xi , \eta ) \\ g_z ( x , y ) = \mathcal{f}^{-1}\left [ g_z ( \xi , \eta ) \right ] \end{eqnarray}$$ where $\mathcal{f}$ is the fourier transform from spatial coordinates $ ( x , y ) $ to spatial frequency coordinates $ ( \xi , \eta ) $ , and $a_z ( \xi , \eta ) $ is the propagation kernel for distance $z$ . in the future i will simply let capital letters denote fourier transform of corresponding lower case letters . we typically define for asp : $$ a_z ( \xi , \eta ) = \exp \left [ i 2 \pi \frac{z}{\lambda} \sqrt{1- ( \lambda \xi ) ^2 - ( \lambda \eta ) ^2} \right ] $$ where $\lambda$ is the wavelength . however , this quickly falls apart if any of the conditions i outlined above are not satisfied . if the input field varies too rapidly ( or conversely if the computation grid is not fine enough to nyquist sample those variations ) then you will have severe aliasing in the fourier transforms , and your result will be corrupted . if the input and output planes are not parallel , you simply cannot use this method as it is formulated . if you need the output grid to be sampled differently than the input grid , then an fft algorithm will no longer work , because the essential trade-off that you make for the speed of an fft is that you cannot select arbitrary points to compute in fourier space . there are other propagation techniques of course . lets say you have a converging beam , such as that immediately after a focusing lens . barring an extremely fine computation grid spacing , or a very long focal length lens , asp will fail because $g_0$ will be badly aliased . this is the textbook example of the applicability of fresnel diffraction : $$ g_z ( x , y ) = \frac{e^{i k z}}{i \lambda z} e^{i \frac{\pi}{\lambda z} ( x^2 + y^2 ) } \mathcal{f} \left [ g_0 ( u , v ) e^{i \frac{\pi}{\lambda z} ( u^2 + v^2 ) } \right ] $$ where $ ( u , v ) $ are spatial coordinates in the input plane . if we consider the input field $g_0$ to be composed of the product of a quadratic phase front $q ( x , y ) = \exp \left [ -i \frac{\pi}{\lambda \mathcal{f}} ( x^2 + y^2 ) \right ] $ ( representing the phase of the converging lens of focal length $\mathcal{f}$ ) and a residual flat component $s_0 ( x , y ) $ , we can see that over some range of propagation distances , the quadratic phase term inside the fourier transform will act to " flatten " the transformed function such that there is no aliasing . indeed , when $\mathcal{f} = z$ the transformed field will equal to the roughly flat $s_0$ ! in this case the resulting computation is basically identical to yet another propagation technique , fraunhofer diffraction , neglecting the minor detail of the quadratic phase term outside of the fourier transform in fresnel diffraction , and a scaling from angular coordinates to spatial coordinates . even with fresnel/fraunhoffer diffraction , we still run into problems if the propagation distance is too small . note that the term $e^{i \frac{\pi}{\lambda z} ( u^2 + v^2 ) }$ will have very high curvature when $z$ is small , once again producing aliasing problems in the fourier transform . typically , the solution to this is to propagate the field twice ; first forwards by a distance $z_1$ , then backwards by $-z_2$ such that the overall propagation distance is $z=z_1-z_2$ , but realizing that this is necessary , and that this ( or any other ) particular solution is applicable is non-trivial . now what if your input field is significantly tilted ( i.e. . it is not propagating parallel to the z-axis ) ? you will want your computation grid to include the output field , but most of the energy may be located far from the origin of the $ ( x , y ) $ coordinate system . you could make your grid large enough to accommodate this , but ideally you had compute an off-axis chunk of the output plane . once again you need a new algorithm ! consider that in general some people may have a need to propagate complicated optical fields to non-parallel planes or curved surfaces , to change the grid spacing in the output plane , or whatever other complication you can image , and you will begin to see how much decision making and complexity can be involved in selecting a propagation algorithm . doing this in an automated way is not necessarily impossible , but it is very difficult , and there is simply nobody who has undertaken the task of writing an open source library for this . i suspect such code exists in proprietary software packages , like codev or other lens design software , but remember that for a library to be developed there has to be somebody who is simultaneously interested ( or paid sufficiently ) , qualified , and has the time to produce such a thing . also remember that the audience for it would be limited to optical engineers and researchers , so there is little reward .
refs . 1 and 2 define a canonical transformation ( ct ) $$\tag{1} ( q^i , p_i ) ~\longrightarrow~ ( q^i , p_i ) $$ [ together with choices of hamiltonian $h ( q , p , t ) $ and kamiltonian $k ( q , p , t ) $ ] as satisfying $$ \tag{2} ( p_i\mathrm{d}q^i-h\mathrm{d}t ) - ( p_i\mathrm{d}q^i -k\mathrm{d}t ) ~=~\mathrm{d}f$$ for some generating function $f$ . on the other hand , wikipedia ( march 2014 ) calls a transformation ( 1 ) a canonical transformation ( ct ) if it transforms the hamilton 's eqs . into kamilton 's eqs . a ct ( 2 ) according to the definition of refs . 1 and 2 is a ct according to the definition of wikipedia but not necessarily vice-versa , cf . e.g. this phys . se post . considering op 's second question , it seems that what op is really after is not the notion of a ct per se , but rather the notion of a symplectomorphism $^1$ $f:m\to m$ on a symplectic manifold $ ( m , \omega ) $ , i.e. $$\tag{3} f^{\ast}\omega=\omega . $$ here $\omega$ is the symplectic two-form , which in local darboux/canonical coordinates reads $\omega= \mathrm{d}p_i\wedge \mathrm{d}q^i$ . it is straightforward to see that a symplectomorphism $f:m\to m$ preserves the canonical volume form $$\tag{4} \omega~:=~\frac{1}{n ! }\omega^{\wedge n}$$ in phase space $ ( m , \omega ) $ , i.e. $$\tag{5} f^{\ast}\omega=\omega . $$ references : h . goldstein , classical mechanics , chapter 9 . see text under eq . ( 9.11 ) . l.d. landau and e.m. lifshitz , mechanics , $\s45$ . see text between eqs . ( 45.5-6 ) . -- $^1$ a ct ( 2 ) according to the definition of refs . 1 and 2 is locally a symplectomorphism ( by forgetting about $h$ and $k$ ) .
all points in the observable universe are " connected " in the sense that they can be acted upon by forces that have an infinite range ( gravity and electromagnetism ) . however , points that are outside of our cosmological horizon ( due to the expansion of the universe ) are no longer causally connected with points in our local vicinity , since they are receding from us faster than light . the same is true of points that are inside the event horizon of a black hole .
in linear electrodynamics ( i.e. . low intensities ) , the dielectric constant and refractive index remain unchanged . a different thing is nonlinear optics ( applying to ed in general as well ) , for more theory see e.g. here . but this happens only for materials with strong non-linear parameters ( usually $\chi^{ ( 2 ) }$ or $\chi^{ ( 3 ) }$ ) and for rather high field intensities . this usually yields effect such as the second harmonic generation , but there is also the effect of self-focusing among others , which stems from the dependence of the refractive index on the field amplitude .
there is a list on wikipedia . radar guns use an optical doppler effect to measure speed . their acoustic equivalent is used in medicine , where it is called doppler ultrasound and used to measure blood flow or other sorts of motion in the body . animals that use echolocation can use the doppler shift to gain information about the motion of their surroundings . a sonic boom occurs when the doppler shift shifts a frequency to infinity . i guess one can continue to concoct scenarios . i wonder whether , when you drop a cat off a cliff , you can hear the pitch of its screaming drop as it accelerates . ( it is not all that cruel - cats can usually survive a fall at terminal velocity . ) you could use it to determine which way a whale is swimming if you have two boats , both listening to the same whalesong . you could even use the doppler shift to gain information about the position of the whale because both the whale 's position and its velocity contribute to the observed doppler shift at any given place . ( i do not have any information about this actually being done , but it might be an interesting problem to work out the locus of possible whale locations for an observed doppler shift . ) i was also curious about whether the doppler effect gives us information on the motion of the crust that moves during an earthquake . i found this reference which suggests it does .
these terms apply when you are solving the schrodinger equation with a potential that goes to zero at large distances . in this situation , the solutions with $e&lt ; 0$ have the property that $\psi$ dies away to zero for large distance . so the particle is , with high probability , guaranteed to be in a confined region ( not at large distance ) . so those are bound states . the solutions with $e&gt ; 0$ , on the other hand , do not die away to zero at large distances -- instead , they go like $e^{ikr}$ where $k=\sqrt{2me}/\hbar$ . so these solutions represent particles that have high probability to be arbitrarily far away . physically , they are useful when describing particles that start far away , approach the scattering center , and end up far away again . hence the name " scattering states . "
nothing would really happen that may be of any significant consideration at least for normal sized passenger planes from private jets to boeing . lets see two major steps , i have considered landing and take off similar , and flight as separate . first lets consider take off/landing the blue lines are supposed to be magnetic field lines of earth , they are not normally so straight , but this would be a good approximation . lets assume a $30^o$ angle between the field and plane 's body , the earth 's magnetic field being $0.65g$ at maximum that is a mere $6.5 \times 10^{-5} t$ . while the largest wingspan of an aeroplane till now is $97.51m$ we will consider is $100m$ just for our ease . so even if an aeroplane had been a square of such a large side , its area would be $10^4m^2$ , and a decent $30^o$ angle with magnetic field would give you a feeble $0.56weber$ of flux . even if it reaches this much change of flux in just one second which it does not , then we would have a potential difference of about $0.5v$ across the wings , now with the resistance the current would just become too negligible and already these numbers are too large because we considered the plane a square and a constant angle of $30^o$ that the area vector had with the horizontal while its not like that and is much over $45^o$ thus concluding that at least while take-off and landing we do not need to care about any induced currents and voltages now lets consider the flight this is a very typical route for a flight , but still at this point you can rotate the plane in all directions and still get a near $90^o$ angle between the magnetic field and area vector in nearly all places where aeroplanes can land and/or take off ! same calculations with just a difference of angles when applied in this case give even smaller and much less noticeable currents . now you might think of antarctica where the lines are pretty much perpendicular , but even for that if you calculate for $90^o$ you will get negligible results only . now i am going to take an impossible scenario just to show that any real life flux changes are easily negligible . the minimum that a flux can be is $0weber$ and the maximum it can be even for the largest square sheet aeroplane is $0.65weber$ , even if any plane makes this amount of flux change in a mere second the potential difference developed would only be $0.65v$ this would give negligible currents and hence can be neglected over all . now to the case of thunderstorms , the maximum trouble that any aeroplane would receive is well known to arise from air turbulence , during thunderstorms these would create a danger far greater than any lightning bolts etc can produce , lets take a look . during a typical thunderstorm the potential difference between the clouds and earth is more than $1\times 10^8$ , the total resistance of the plane would be negligible as compared to that of the air , the major current that runs between the cloud and earth to establish a channel for the discharge is of the order of $100a$ , now lets calculate the energy density that will arise in the plane due to joules heating , the energy generated would be for the square sheet plane $u = i^2 \frac{\rho l}{a}$ and energy per unit area would be $$u = \frac{i^2 \rho l}{a^3}$$ for our square plane , where sides are equal it would result in $$u = \frac{i^2 \rho}{s^5}$$ lets assume an impossible current of $1000a$ for our square plane of $100m$ side , and check again$$u=\rho \times 10^{-4}$$ clearly negligible energy density for a small plane , like the drones we have nowadays or even any thing with wingspan less than $10m$ the energy density increases rapidly , but i do not think those planes are operational under such harsh weather conditions ! ps : i am not 100% sure about the thunderstorm analysis , but i am sure about the analysis for magnetic flux addendum : the " discharge wicks " you show in your picture are 100% for static discharge with as simple a reason as their other name is " static discharger " . i will not increase the length of the answer explaining their working etc but you can easily check it out here or simply do a google search of " discharge wick " .
the general expression for calculating kinetic energy is $$ke = \frac{m v^2}{2} + \frac{i \omega^2}{2}$$ however , $v$ means the velocity of the center of mass and $\omega$ is rotational velocity around the center of mass . $i$ is moment of inertia about center of mass . you cannot do the above expression just for arbitrary point of the body . as for the second question and pulses of rotational movement : i think during the collisions both pucks roll against each other or against the wall for a very small fraction of time . when rolling you have static friction forces , which have great and temporal effects on speed , rotational speed and their relation .
when considering a big enough chunk of matter , as in your experiment , you are dealing with it is macroscopic observed effects , where granular effects of single electrons are smoothed out . so if you add $n$ electrons to the electron sea of the metal the observed effect will be a uniform increase of $-ne\over s$ in the surface charge density of the metal .
energy and momentum are both conserved . working in the centre of momentum frame the momenta of the incoming photons are equal and opposite so the total momentum is $p=p+ ( -p ) =0$ . the energies of the photons are also equal and equal to $pc$ . the total energy $e=2pc$ . now let the photons scatter into two photons of energy/momentum $e_1 , \ p_1$ and $e_2 , \ p_2$ respectively . since the total momentum is conserved we must have $p_1 + p_2 = 0$ , so $p_2 = - p_1$ . the momenta remain equal in magnitude . further , the energies are equal : $e_1 = |p_1| c$ and $e_2 = |p_2| c = |p_1| c = e_1$ . since the energies are equal and the total energy is still $e=2pc$ , we have that the energy of the final photons must be $pc$ . so nothing can change except for the direction of the outgoing photons . you can get the answer in any other frame by doing a lorentz boost of this result .
if you place a camera you will not see any interference pattern . so , the answer is yes . the camera will cause the wavefunction to " collapse " . but i do not like the term " wavefunction collapse " , because wavefunction is not really any physical object . what the camera will basically do is cause an abrupt change in the state of the particle . here is the defintion of measurement from landau 's book by measurement , in quantum mechanics , we understand any process of interaction between classical and quantum objects , occurring apart from and independently of any observer . the importance of the concept of measurement in quantum mechanics was elucidated by n . bohr . we have defined " apparatus " as a physical object which is governed , with sufficient accuracy , by classical mechanics . such , for instance , is a body of large enough mass . however , it must not be supposed that apparatus is necessarily macroscopic . under certain conditions , the part of apparatus may also be taken by an object which is microscopic , since the idea of " with sufficient accuracy " depends on the actual problem proposed .
first the answer : the motion is not quite stable , but only because of two subtle thing that your brain probably can intuitively feel : the rate at which the outer ring ( the one that is rotating around the vertical axis ) rotates has to speed up and slow down as the inner ring becomes vertical and horizontal respectively . once you slow down and speed up the outer ring , the motion is fine . but if you do not do this , you need to provide twisting torques at the top and bottom periodically to compensate . the rate at which the inner ring spins has to speed up and slow down as the ring becmes horizontal and vertical , due to the centrifugal force in its frame pulling it out and pushing it in . it is these two things that make the motion nonuniform rotation speed , and this makes the motion seem off intuitively . you can not make both motions uniform with the same period--- the inner ring has to speed up and slow down even in the limit of large mass of outer ring . when you do the slowing down and speeding up , as required by the changing moment of inertia , and also the slowing down and speeding up required by the centrifugal pushing and pulling , this motion is natural . it does require stresses and torques on the system , but they are the kind that are naturally provided by the mount , they are just the forces holding the mount in place . solving the lagrangian the analysis kostya did for the lagrangian is correct ( althugh i originally swapped his angle names ) . the outer ring is rotated by a rotation matrix $r_z ( \phi ) $ , while the inner ring is rotated by $r_z ( \phi ) r_x ( \theta ) $ , meaning first rotate around the x axis , then around the z axis . the motion of any point from a change in $\theta$ is always perpendicular to the motion from $\phi$ , so there are no cross-terms . the total moment of inertia for the $\phi$ motion is the sum of the moment of inertia of the outer ring , and the moment of inertia of the tilted inner ring . the inner ring , as it is tilted , goes from having a moment of inertia i when it is horizontal to i/2 when it is vertical . this means that the whole lagrangian is $$ l = {1\over 2} ( a + c \cos^2 ( \theta ) ) \dot{\phi}^2 + {1\over 2} b \dot{\theta}^2 $$ just as kostya says ( for a planar internal motion c=2b ) . this lagrangian has a conserved energy and a conserved $\phi$ momentum , since $\phi$ does not appear in l ( the system is symmetric with respect to rotations around the z-axis ) , and this reduces it to a 1 degree of freedom system . from the conservation of $\phi$ momentum , $$ p_\phi = ( a+c \cos^2 ( \theta ) ) \dot{\phi} = p $$ which gives the rate of change of $\phi$ , and the coefficient b tells you how it speeds up and slows down as you make the inner disk vertical or horizontal . the conservation of energy now tells you $\dot{\theta}$ $$ h = p_\phi \dot{\phi} + p_\theta \dot{\theta} - l = l $$ so that $$ b\dot{\theta}^2 + {p^2\over a+ c \cos^2 ( \theta ) } = 2e $$ the point of this is only that the $\phi$ motion is nonuniform only to the extent that c is nonzero ( the same reason the outer ring is rotating nonuniformly ) , so you get a perfectly fine periodic motion in $\phi$ , and you can adjust the total $\theta$ period to equal the $\phi$ period to make a motion which is qualitatively like the one shown in the film . the film is never exact , even though the outer motion can be much heavier than the inner ring , the inner ring ( if it is flat ) must speed up and slow down due to centrifugal force . i felt i needed intuition about the stresses involved to keep the rings rotating , which you do not see in the lagrangian formulation . some intuition on the stresses to see how the stresses work , imagine the outer ring is infinitely heavy , and rotating with a constant angular velocity ( this is what is shown in the movie ) . now transform to the rotating frame . there are two ficititious forces . the centrifugal force is cancelling on the ring between the two sides , and the effect of this is just to make the inner ring it want to explode outward when it is horizontal ( there is no effect when it is vertical ) . the effect of this is to introduce the centrifugal potential term that pulls the ring horizontal , and this is the source of kostya 's pendulum force ( which is the reason the inner ring can stably oscillate around the horizontal position ) . the coriolis force is $\omega\times v$ , and when the ring is partway between horizontal and vertical , it turns the inner ring like a steering wheel in a definite direction . this needs to be counteracted by the outer ring , and this turning push is provided by the contacts . the turning push is what is responsible for the ice-skater-like slowing down of the outer ring ( but here we are assuming the outer ring is very massive ) . the result is completely intuitive , and you can understand all the effects--- there is a steering wheel pull on the inner ring in opposite directions as it is rotating , which only has the effect of slowing down the outer ring , and which can be provided by the grip of the outer wheel on the inner one .
as far as i know the term " reversal of magnetic poles " does not have a strict definition , so i suppose different commentators might use it in different ways . however i suspect most of us would use it to describe the whole process . you describe the process as " drawn out " but no-one knows how long it takes because the dynamics of the earth 's core are poorly understood . on a geological timescale the process looks instantaneous , but then geological timescales are pretty long . models suggest it could be pretty quick , though how realistic the liquid sodium models are is open to debate .
so i wonder how do we explain the levorotatory and the dextrorotatory in atom level pov ? it is not easy to predict whether solution of given molecule will be levorotatory or dextrorotatory ; this will depend in a complicated way on the structure of the molecule and also on the frequency of light in question . there are models to calculate indices of refraction and absorption for left and right-polarized light for given frequency and molecular structure , but they are quite involved to explain here . the important idea in these is the observation that most often optically active molecules have three-dimensional structure of nuclei that is not congruent with its mirror image ( we cannot take molecule and by rotating and translating it superimpose it on its mirror image ) . optically active molecules have more spatially separated parts ( at least 4 different atoms to allow for this mirror asymmetry ) and an important role in the calculations of indices of refraction has the way these parts interact .
your equations ( 2 ) , ( 3 ) and ( 4 ) are all correct . i do not like the way ( 1 ) is written , because it relies on an unstated understanding that the $e$ 's represent only mass energy , but with that understanding it is also correct . this is just the conservation of energy in it is relativistic form where mass is just another kind of energy ( previously you had to worry about kinetic energy and various kinds of potential and internal degrees of freedom expressed as heat and so on ; with the advent of relativity you have to add mass to that list ) . $q$ is the difference of the masses expressed in terms of energy . your idea for getting the distribution of energy between the product nucleus on the neutron ( use the conservation of momentum ) is correct , but you must work in two dimensions not one ( you can choose to work in the $x$--$y$ plane however ) . choosing the direction of the photon as $\hat{x}$ ( ) which you can do without loss of generality ) it should be $$ h \nu \hat{x} = \vec{p}_n + \vec{p} $$ where i have not written the momenta in the newtonian approximation , because you do not yet know that this is correct . you have a couple of choices here : do in the full relativistic expression do it in the newtonian regime and then check to see if that was reasonable assume that the nucleus will be newtonian but treat the neutron relativistically , and then check to make sure the result was reasonable . the important thing is that if you make the newtonian approximation , you must go back and check to see that it was reasonable after . questions for the student : what condition or conditions make the newtonian approximation " reasonable " ? can you deduce if the approximation will likely be reasonable before you begin calculating ? why did i suggest treating only the recoiling nucleus in the newtonian approximation as an option above ?
let 's assume that we are considering the effective acceleration due to gravity felt by an object at rest relative the the earth 's surface at lattitude $\phi$ . since the object is at rest , $\dot{\vec r} = 0$ , and the coriolis term vanishes . with the $z$-axis along the axis of rotation , we can write $$ \vec\omega = \omega \hat z $$ on the other hand , the position vector is given by $\vec r = r\hat r$ . it follows that the centrifugal force is $$ \vec f_\mathrm{cf} =-\vec\omega\times ( \vec\omega\times \vec r ) = -\omega^2r \ , \hat z\times ( \hat z \times \hat r ) =-\omega^2 r\ , ( ( \hat z \cdot\hat r ) \hat z - \hat r ) $$ since it is the radial component of the centrifugal term contributes to the component of the effective acceleration due to gravity perpendicular to the surface of the earth , we take the dot product of this expression with $\hat r$ to obtain this component ; $$ \vec f_\mathrm{cf}\cdot \hat r = \omega^2 r ( 1-\cos^2\theta ) = \omega^2r\sin^2\theta $$ where $\theta$ is the spherical polar angle measured from the positive $z$ axis . since the polar angle $\theta$ and lattitude $\phi$ are related by $\theta = \pi/2-\phi$ , this result can also be written as $$ \vec f_\mathrm{cf}\cdot \hat r = \omega^2 r\cos^2\phi $$ the perpendicular component of the effective acceleration due to gravity is therefore $$ g_0 = g-\omega^2 r\cos^2\phi $$ i am a bit baffled by the expression you say one is supposed to get . for one thing , the dimensions do not seem to work out since $x$ is dimensionless , but is being equated to something with dimensions of acceleration . however , if one were to take the result i just derived , then notice that it implies $$ \frac{g-g_0}{g} = \frac{\omega^2 r \cos^2\phi}{g_0+\omega^2r\cos^2\phi} = \frac{x \cos^2\phi}{1+x\cos^2\phi} \approx ( x-x^2\cos^2\phi ) \cos^2\phi $$ where in the last equality we have assumed that $x$ is small , namely that the spin of the earth is slow . this is as close as i can seem to get to the desired expression .
angular momentum does not change , but the angular velocity vector does . this is effectively due to a shift in the body 's moment of inertia tensor .
1 . x-ray diffraction takes pictures of fourier space as briefly described on pg . 34 of " introduction to solid state physics " 7th edition by kittel , the scattering amplitude for an arbitrarily-shaped object is $$f ( \mathbf{k}_i , \mathbf{k}_o ) =\widehat{n} ( \mathbf{k}_i-\mathbf{k}_o ) $$ where $\mathbf{k}_i$ is the incident wavevector , $\mathbf{k}_o$ is the outgoing ( or scattered ) wavevector , and $\widehat{n}$ is the fourier transform of the material 's scattering density $n$ . since it is often electrons which scatter light , $n$ is often assumed to be the electron density as a function of location in the material . this is a surprisingly simple equation ; to illustrate it is visual meaning , imagine an object is placed inside a hollow sphere whose walls are lined with photographic paper , and through a small hole the object is bombarded with x-rays of fixed wavevector $\mathbf{k}_i$ , which are scattered by the object and strike the interior of the imaging sphere , forming an image . what is this image ? imagine a bubble of radius $|\mathbf{k}_i|$ centered at $\mathbf{k}_i$ in fourier space . the image formed on the photographic paper is the image of $\widehat{n}$ on the surface of that bubble , a fact which is simple to deduce by examining the set of points of the form $\mathbf{k}_i-\mathbf{k}_o$ for a fixed value of $\mathbf{k}_i$ , and noting that $|\mathbf{k}_i|=|\mathbf{k}_o|$ . that is what x-ray diffraction examines ; it literally takes a bubble-shaped slice of an object in fourier space . this bubble is sometimes call the " ewald bubble " , and it is what diffraction takes a picture of . 2 . fourier space of a crystal now , let 's insert the fact that we are looking at a crystal . a very simple approximation of a crystal is a 3d dirac comb ( ie , a dirac delta placed at each unit cell ) , whereupon $$n ( \mathbf{r} ) \approx \mbox{diraccomb} ( \mathbf{ar} ) $$ where $\mathbf{a}$ is the inverse of the $3\times3$ matrix whose columns are the 3 lattice basis vectors for the crystal . one then fourier transforms to obtain $$\widehat{n} ( \mathbf{k} ) \approx\mbox{det} ( a ) ^{-1}\mbox{diraccomb}\left ( \frac{\mathbf{a}^{-1}\mathbf{k}}{2\pi}\right ) $$ which essentially tells you that the fourier transform of the lattice $n$ is also another lattice . this lattice , residing in fourier space , is often referred to as the " reciprocal lattice " of a crystal . 3 . crystal diffraction now let 's combine the previous two results . if a reciprocal lattice point of $\widehat{n}$ happens to reside on the surface of the ewald bubble , then light will be diffracted in that direction in real space . but how do we image the rest of reciprocal space , not just the points that happen to lie on that one bubble-shaped slice ? suppose we rotate the crystal through euler angles $\alpha , \beta , \gamma$ . then $n ( \mathbf{r} ) $ will become $n ( \mathbf{r} ( \alpha , \beta , \gamma ) \cdot\mathbf{r} ) $ where $\mathbf{r} ( \alpha , \beta , \gamma ) $ is the rotation matrix for the crystal rotation . since $$\mathbf{r} ( \alpha , \beta , \gamma ) ^{-1}=\mathbf{r} ( -\gamma , -\beta , -\alpha ) , $$ we see that $\widehat{n} ( \mathbf{r} ) $ becomes $\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{r} ) $ ( rotations are unitary so the determinant is 1 ) , and hence rotation of a crystal in real space correspond to a reversed rotation of the crystal 's reciprocal space . thus , by rotating the crystal and imaging it after each rotation , we can sweep out a spherical region of radius $2|\mathbf{k}|$ in reciprocal space . visually , we are rotating the ewald bubble , whose surface is attached to the origin , to sweep out a spherical region whose radius is the diameter of the imaging bubble ( which is $2|\mathbf{k}|$ ) . 4 . powder diffraction with those preliminaries out of the way , powder diffraction is quite simple . a powder is a large number of very small crystals oriented in random directions . if there are a large number of crystals all oriented randomly ( isotropic fine powder ) then we can average over all orientations to get $$\widehat{n}_{avg} ( \mathbf{k} ) \propto\int_0^{2\pi}d\alpha\int_0^{\pi}d\beta\mbox{sin} ( \beta ) \int_0^{2\pi}d\gamma\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{k} ) $$ but you do not need to worry about integrating it , because it is visually obvious that each delta function located at a reciprocal lattice point $\mathbf{g}$ will be " rotationally smeared " out to form a series of concentric bubbles of radius $|\mathbf{g}|$ centered at the origin . where these concentric bubbles intersect the ewald bubble , diffraction will occur in that direction . and it is a simple fact of geometry that when one bubble intersects another , the intersection region is a circular ring common to both . as a result , there will be ring-shaped regions on the ewald bubble where constructive interference can occur , and thus , the sample will emit cone-shaped beams of light . and that is how the cone-shaped beams of light in the picture above come to fruition .
" you will agree that if you shoot in an open field , the bullet will stay at the same altitude for several seconds no , we would not . in fact we do a demo ( often called ' shoot the monkey' ) in introductory classes that shows unambiguously that it falls very much as if you had held it at arms length and dropped it . as noted in the comments the mythbusters guys actually ran the experiment with a bullet .
the answer is no , as you do not know the moment of inertia , and changes te situation a lot : suppose the distribution of mass is that such that all mass is concentrated in the extreme points , and another case in which is uniformly distributed along the longitude : both cases generate the same center of mass and distances , but the torque will be different as mass distribution is different .
i am pretty sure that even the brief summarization of all the alternatives will take a book or two . i will try to give a review of basic things from my perspective . let me from the beginning note that the following classification is not accurate -- different classes may and do overlap . more scalar doublets ( multiplets ) first of all one can introduce more scalar multiplets . two higgs doublet model ( 2hdm ) is the most favored , because it is also naturally arises from mssm . nhdms are also considered . doublets are usually considered , because there is a basic constraint on the quantum numbers of the fields , coming from " rho parameter": $\rho = \frac{m_w^2}{m_z^2\cos^2\theta_w}=1$ which can be satisfied only if $ ( 2t+1 ) ^2-3y^2=1$ with the most natural solution $y=1 , t=1/2$ . of course there are other solutions , leading to bigger values , but i have never seen anyone seriously considering those . there is still a lot of freedom to impose some extra discrete symmetries , continuous symmetries , the way these scalars interact with fermions , e.t.c. , which leads to many subclasses of such models . composite higgses the central example is the little higgs model where higgs arrives as a ( pseudo- ) goldstone boson from some higher global symmetries . changing the underlying symmetry one obtains the whole class of such models . extra gauge symmetries are also considered -- they are usually broken dynamically . technicolor was the most popular one -- now it not so favored , while i do not think that it was refuted completely . top condensate is another dynamical model . originated from extra dimensions lots of geometries , compactifications , boundary conditions -- i feel completely lost with those . most popular are higgsless models -- attempts to get rid of higgses completely . they are usually based on some specific boundary conditions . of course the list in incomplete . there are a lot of different " mixtures " between those models , usually with some new funny names . here is a nice recent reference that reviews some of the mentioned models going more deeply into some tehnical details .
space_cadet mentioned already work about deriving spacetime as a smooth lorentzian manifold from more " fundamental " concepts , there are a lot of others -like causal sets , but the motivation for the question was : the reason for my interest in this regards one of the mysteries of quantum mechanics , that of quantum entanglement and action at distance . i wondered whether , if space is imagined as having a topology that arises from a notion of neighbourhood at a fine level , then quantum entanglement might be a result of a ' short circuit ' in the connection lattice . i am not convinced that such an explanation is possible or warranted , the reason for this is the reeh-schlieder theorem from quantum field theory ( i write " not convinced " because there is some subjectivity allowed , because the following paragraph describes an aspect of axiomatic quantum field theory which may become obsolete in the future with the development of a more complete theory ) : it describes " action at a distance " in a mathematically precise way . according to the reeh-schlieder theorem there are correlations in the vacuum state between measurements at an arbitrary distance . the point is : the proof of the reeh-schlieder theorem is independent of any axiom describing causality , showing that quantum entanglement effects do not violate einstein causality , and do not depend on the precise notion of causality . therefore a change in spacetime topology in order to explain quantum entanglement effects will not work . discussions of the notion of quantum entanglement often conflate the notion of entanglement as " an action at a distance " and einstein causality - these are two different things , and the first does not violate the second .
note that $\partial v=s$ , so that $$\tag{1} c~=~\partial s~=~\partial^2v~=~\emptyset$$ is the empty set . ( topologically , the boundary of a boundary is empty , or equivalently , the boundary operator $\partial^2=0$ squares to zero . ) on the other hand , the circulation $$\tag{2} \gamma~=~\oint_{c=\emptyset}\vec{a}\cdot d\vec{r}~=~0$$ along the empty curve $c=\emptyset$ vanishes identically for any vector field $\vec{a}$ . in particular , one can not conclude from ( 2 ) that the magnetic potential $\vec{a}$ should be a gradient field .
the simple answer is that the angle between the front fork and the vertical causes the force from the ground to create a moment about the axis of rotation that turns the wheel in that direction . this has nothing to do with actually riding the bike , and it will happen even if the bike is stationary . basically , if you project the axis of ( steering ) rotation all the way through the wheel , top to bottom , it will not be coincident with the point of contact with the ground . when the bike leans over , the upward ( normal ) force from the ground is not in the same plane as the axis of rotation , which causes a moment about that axis . when the bike begins to turn , the frictional component of the contact force will cause the force to go back into the same plane as the axis of rotation , which causes the wheel to hold its position steady .
whether entropy was zero at the big bang or not is very much an open question of physics , in big part due to the fact that we do not yet have a good enough understanding of physics at high energies and high gravitational fields . but for the zero entropy state this is a bit easier to answer and the answer does depend on laws of physics . zero entropy state basically depends on how many completely distinguishable states the laws of physics allow . the universe is in a zero entropy state precisely when it is in a single state and it can be known which state it is in . in many situations there are infinitely many different zero entropy states . so the zero entropy state at the beginning of the universe is unique if and only if the laws of physics at that time require that there is a single state in which the universe can be found . whether they do require that or not is a very big question in physics which everybody would like to know the answer to .
since $$\cos\left ( x-\frac{\pi}{2}\right ) =\sin x , $$ using $\cos$ or $\sin$ does not matter , it depends on the choice of initial conditions . in addition , in general , there will be a initial phase $\phi$ , so sinusoidal wave is written like $$ y ( x , t ) =a \cos ( kx-\omega t+\phi ) . $$
if you read wikipedia page about corium , they say that critical mass can be achieved locally . but if you are concerned about a critical mass allowing a nuclear explosion , the difficulty in nuclear weapon design , as told here , is to achieve the criticality fast enough . if you do not achieve criticality fast enough , your material heats and its interaction with neutrons decreases , slowing the chain reaction down . and that is with pure ²³⁵u . so basically what happens if criticality happens in a melting nuclear reactor is the release of a lot of heat and radiation , but not in an explosive manner as in an atomic bomb .
mostly because they are heavy . rocks erode putting their constituents into solution , the heavy stuff settles out in river/sea beds , and metals are heavy . for many metals hydrothermal process are more important . super hot water deep in the earth dissolves the rock containing the minerals , it moves along cracks in the rock and cools depositing the salt and metals as lines in the rock . in an asteroid with no geological process the metals are found in their raw state having cooled directly from the original ball of primeval gas
apart from motor and bearing noise , most of the acoustic power comes from the eddy swirls following the trailing edge of the blade after it passes by . there is also an outward pulse of air as the leading edge of each blade pushes forward cutting the air . the trailing eddies produce a broad spectrum of random noise , modulated by the fan blade frequency . the outward pulses , of course , occur at the blade frequency , with harmonics . both are stronger near the tip , because the tip is moving faster . faster splitting of the air means a sharper leading edge of the pressure pulse , so the higher frequency noise is especially concentrated near the tips . a small microphone near the spinning blades would pick up a repeating soft step function from the blades ' leading edges , with faster-rising step functions ( thus composed of higher frequencies ) farther from the hub . that answers one part of your question . by " blade frequency " i mean how often a blade passes any specific point in space , per second . ( i am not a fan engineer , so my jargon may be off . ) ( although , do not misunderstand me - i am a big fan of engineers ! ) actual noise from a fan will show variations at the spin frequency , or full cycles of the blade assembly , not just at what i am calling the blade frequency . this is because the blades are not perfectly identical . who knows , maybe there is the residue of a dead bug on one blade and not on the others . imbalances increase noise . note that fans will put out acoustic noise at frequencies lower than the blade frequency . the eddy swirls are not the same each time a blade passes by - they are random and the variations from one cycle to the next to the next mean subharmonics . fan blades rarely spin bare naked in air . there is probably a grille , a wire cage , something protective to keep kids ' fingers safe and insects out . whatever the structure , there is turbulence as air is pushed by . the noise will be mostly from pushed air leaving the fan , and little ( but some ) from the new air sucked in to replace it . there is a formula for estimating the acoustic power produced by a fan . all i can find at the moment is this from a pdf of unknown origin . the formula is alleged to come from a handbook provided by ashrae ( american society of heating , refrigerating and air-conditioning engineers ) which is , alas , not online for free . $l_w = k_w + 10 log_{10} q + 20 log_{10} p + bfi + c_n$ $l_w$ is the sound power level in db . $k_w$ is a specific constant amount of noise , stated in the manufacturer 's data . it is usually 20-something or 30-something db , unless you have a nasty industrial fan it could be up to 40-something db . q is air flow , cubic feet per minute . ( no offense to the n-1 countries in the world not using us units of measure ! obviously this pdf originated in the us . ) p is air pressure , as inches water ( ! ) . bfi is some sort of correction relating to blade frequency , usually a single digit number of db . finally $c_n$ corrects for inefficient fans . zero for a " perfect " fan , to about 12db for a fan that ' only 40% efficient . efficiency here is " hydraulic efficiency " based on air flow , pressure , and motor power in horsepower ( hp ) : ${q p}\over{6350 ( {hp} ) }$ have fun plugging in numbers . . . i brushed off motor noise at the start of this answer , but of course that is a big factor . unless you pay big $$$ for an exquisitely well-balanced motor , the motor is shaking its mounting , the entire apparatus it is in . a computer motherboard makes a great soundboard to transmit vibrations into the air . a fan in the window or hanging from a ceiling ( vital in florida ! ) or in a stand of any kind , is transmitting vibrations to the structure and , if there is some loose parts involved , will often satisfy the requirements of a chaotic system . that means more subharmonics and plenty of harmonics . good research was done on this back in the 1950s , with renewed interest over the last twenty years or so due to the desire for quiet computers . ( especially in amateur audio recording studios ! ) the best material is in books that i have no easy access to at this time . some references that exist online , at least abstracts of papers : source of noise in fans - howard hardy j . acoust . soc . am . volume 31 , issue 6 , pp . 850-850 ( 1959 ) see http://www.arca53.dsl.pipex.com/index_files/ventnoise1.htm about halfway down the page , " fan noise " " visualization of aerodynamic noise source around a rotating fan blade " , a . nashimoto , n . fujisawa , t . nakano , t . yoda , j . visualization 11 ( 4 ) :365-373 ( 2008 ) ( avail . at acm digital library , springer . com and other purveyors of academic papers , but not for free online . are you near a university with a library ? ) for noises created by aircraft , this article is informative and non-technical : http://www.noisequest.psu.edu/sourcesaviation.overview.html this company making fans , blowers etc . has some lightweight info on fan noise physics : http://www.jmcproducts.com/acoustic-noise/ though i dislike using wikipedia due to its ever-changing nature , at this time this article does have relevance : http://en.wikipedia.org/wiki/quiet_pc
the container will not suddenly accelerate to $v_2$ . when you suddenly change the acceleration to $a_2$ , you are not providing an impulse to the container , so the velocity of the container just after the change will still be $v_1$ and not suddenly accelerate to $v_2$ . the velocity of the container will increase now too but by rate $a_2$ and not $a_1$ ( note that it was increasing before the change too , due to acceleration $a_1$ , in case you missed that ) . the water in the container will also accelerate by $a_2$ , and you you will not experience a sudden splash effect . what would happen to the water is that it will oscillate periodically about an equilibrium position ( infinitely if there is no damping ) . the equilibrium position of water level in an accelerated container is shown by this image:- when you change from $a_1$ to $a_2$ , the equilibrium position changes its inclination $\theta$ , so the fluid level starts oscillating about this level .
i agree with jwenting but in some sense , i feel that he is not answering the question : why there is no " combined $\alpha$ plus $\beta$ decay in which a nucleus emits e.g. a helium atom ? well , let me start with the $\beta$-decay . nuclei randomly - after some typical time , but unpredictably - may emit an electron because a neutron inside the nuclei may decay via $$ n\to p+e^- +\bar\nu$$ which may be reduced to a more microscopic decay of a down-quark , $$ d\to u + e^- +\bar\nu . $$ this interaction , mediated by a virtual w-boson , is why a nucleus - with neutrons - may sometimes randomly emit an electron . so the $\beta$-decay is due to the weak nuclear force . on the other hand , the $\alpha$-decay is due to the strong nuclear force : the nucleus literally breaks into pieces , with a very stable combination of 2 protons and 2 neutrons appearing as one of the pieces ( helium nucleus ) . the two processes above are independent , and each of them can kind of be reduced to a single elementary interaction whose origin is different . this independence and different origin is why the " combined " decay , with an emission of both electron ( or two electrons ) and a helium nucleus , is extremely unlikely . such an emission of a whole atom ( which is electrically neutral but it is surely not " nothing " ! ) could only occur if several of the elementary decay interactions would occur at almost the same time which is extremely unlikely .
i have noticed that none of these answers actually answer the question . the simplest explanation of string theory i can think of : particles we currently consider " point particles " ( electrons , quarks , photons , etc . ) are actually tiny pieces of string with each a characteristic vibration . they interact in a sort of harmony that results in/manifests as the physical laws we observe . if anyone with more knowledge in the field can correct me , i ask for improvements . this is just how i personally explain it to people who ask , and i would hate to give out false information .
the concept of entanglement still applies in qft . what the reeh-schlieder theorem tells you is that there is entanglement in the qft vacuum state . so i take the question to be asking whether we could use the methods employed in proving the theorem , to decide that there is entanglement in biological systems . it seems possible , though first you have to find a biological system that you can describe in terms of quantum field theory . let 's take a much-hyped example , the microtubule . just for argument 's sake , suppose that one of the various possibilities listed in this paper made sense , and that there was an effective field theory describing the dynamics of delocalized electrons in the shell of the microtubule . then maybe you could try to prove the reeh-schlieder property for the ground state of that effective theory at biological temperatures , along these lines .
the mixing matrix tells you exactly the correspondence between the mass states and the flavor states . this is true in the quark sector , too , but unlike the quarks where the mass--flavor identification is pretty strong , it is very weak in the neutrino sector . the elements of the mixing matrix are exactly the flavor content of each mass state $$ \nu_\alpha = \sum_{i=1}^3 v_{\alpha , i} \nu_i \ , , $$ for $v$ the mixing matrix ( in the notation used in 2 ) . this image ( linked rather than imported because i can not find any license information ) shows the flavor makeup of each mass state graphically for both sectors . i believe that there is an assumption of normal hierarchy in that figure as the results do depend a little on the hierarchy . ( image from here . ) the strongest identification in the neutrino sector is between $\nu_1$ and $\nu_e$ , and even that is very rough : flavor changes could occur almost immediately , as evidenced by the recent success of the $\theta_{13}$ experiments ( daya bay , reno and double chooz ) in observing electron-anti-neutrino oscillation to more than five sigma at ranges on order of 1 kilometer .
the fundamental reason why energy is conserved , is invariance of the physical laws by a time translation $t \to t + t_0$ , in a lagrangian formulation . this is a particular case of the noether theorem , which states , that if a lagrangian has a continuous symmetry , there is a corresponding conserved quantity . now , if we look in detail , conservation of a charge $q$ may be expressed as a local conservation law implying a ( electric ) current $j_\mu$ , that is $\partial^\mu j_\mu =0$ this is the relativistic notation , with $\mu =0$ is the time coordinate ( $j_0=\rho$ ) , and $\mu =1,2,3$ are the spatial coordinates ( $j_1 = -j^x , j_2=-j^y , j_3=-j^z$ ) . upper and lower indices are related by the metric matrices $\eta^{\mu\nu}$ and $\eta_{\mu\nu}$ which are simply diagonal matrices ( $1 , -1 , -1 , -1$ ) . so $\partial^\mu j_\mu =0$ is simply $\frac{\partial \rho}{\partial t} + \vec \nabla . \vec j =0$ now , for the conservation of momentum/energy $p_\nu$ , you see that ( comparing to $q$ ) you have a supplementary indice $\nu$ ( for instance the energy is $p_0$ , while the momentum coordinates are $p_1 , p_2 , p_3$ ) , so the corresponding local " current " has now $2$ indices : $t_{\mu \nu}$ , and the local conservation law is simply $\partial^\mu t_{\mu\nu} =0$ the " current " $t_{\mu\nu}$ is more known as the stress-energy tensor . so , the current for the energy $e=p_0$ is $t_{\mu0}= t_{0\mu}$ ( the stress-energy tensor is symmetric in its indices ) , and the local law conservation for energy may be written : $\partial^\mu t_{\mu0} =0$
in high-school level classical mechanics , yes , you are told the maximum magnitude of static friction is simply a coefficient $\mu_\mathrm{s}$ ( depending on the materials ) times the magnitude of the normal force , $f_\mathrm{n}$ . this is independent of area because the smaller the area , the greater the contact pressure and so the more interlocked the uneven surfaces become , meaning more static friction per unit area . thus sliding the inverted pyramid will not be feasible in this model . in reality this nice area-independence would probably break down for such high pressure . coefficients of static friction are recognized as cute toy models suitable for high school but they are not terribly accurate in the real world . in physics language , you have moved beyond the linear regime . more interesting is the idea of tipping the pyramid over . it may be perfectly balanced , yes , but it is in unstable equilibrium . that is , classically , even the smallest perturbation to the system will grow in a positive feedback loop . unless your model specifies that there is a static friction torque at the apex , a slight push will angle the pyramid slighty , and then gravity will take the whole thing down for you . this is true no matter how much inertia the object has ( even though it may not seem like it - you have to mentally separate notions of " arbitrarily small effect " and " zero effect" ) .
the core of perturbative string theory has a mathematically rigorous formulation . in fact much of mathematical physics and mathematical insight into quantum field theory as such has been gained from the study of the low-dimensional qfts that constitute the worldvolume theories of the string and the various branes . for instance the axiomatization of qft in the “fqft” flavor ( roughly dual to the aqft picture ) historically originates in insights gained in the study of ( topological ) string ( namely the moore-seiberg axioms ) . on the other hand , the attempted implementations and applications of core string theory are vast and numerous , and when it finally comes to string phenomenology the usual level of rigor is just that common among practicing quantum field theorists . on the far end , deep aspects of string theory that are felt by many researchers to be of metaphysical relevance , such as the “landscape of string theory vacua” have led and are leading to speculations that are not anymore backed up by any disciplined reasoning . more in detail : the quantization of the string sigma-model may be obtained cleanly via the mathematical sound process of geometric quantization , see the references on the nlab at string – symplectic geometry and geometric quantization . the famous weyl anomaly of the string is formally understood in terms of anomalous action functionals , see for instance ( freed 86 , 2 . ) . various other obstructions to quantization ( quantum anomalies ) in the background fields for the string sigma-model such as notably the freed-witten-kapustin anomaly , have been understood in fine detail in terms of obstructions in differential cohomology , see for instance ( distler-freed-moore 09 ) . particularly well analyzed are the two special sectors of first quantized string theory , that of rational conformal field theory , which contains the example of strings propagating on lie group manifolds – the wess-zumino-witten model ; as well as the example of topological strings . rational conformal field theories indeed stand out as one non-trivial and rich class of qfts which have been subject to complete mathematical classification ( in the same sense in which mathematicians for instance do the classification of finite simple groups ) . for details on this classification see on the nlab at frs formalism . for the topological string much more is true . the topological string has effectively become a subject in pure mathematics , with its rigorous axiomatization via the tcft version of the cobordism hypothesis-theorem , its formulation as mathematical homological mirror symmetry , its relation to geometric langlands duality etc . but the fqft-axiomatics that serves to mathematically formalize the topological string is not restricted to the topological sector , it also applies to the physical string . for instance huang’s theorem shows that the familiar description of physical string via vertex operator algebra is an instance of the fqft-formalization . indeed , in frs formalism these two formalizations , vertex operator algebras ( via their modular tensor categories of representations , and tqft combined via the rigorous ads3-cft2 and cs-wzw correspondence give the classification of rational cft ) . ( in particular this says that in this low dimensionl holography and ads-cft duality is rigorous , of course this is far , far from true in higher dimensions . ) in summary this is a level of rigour with which the worldsheet 2d qft of the string is understood which is well beyond of what one typically encounters for non-trivial interacting ( non-free ) qft . and this is full non-perturbative quantum field theory ( on the worldsheet ! ) , not just the approximation in perturbation theory . from here on , also string field theory ( its action functional , that is ) , has a completely rigorous formulation in terms of operads and l-infinity algebras ( lie n-algebras for n→∞ ) . a snapshot of the state of the art of rigorous foundations of string theory as of 2011 is in ( sati-schreiber 11 ) . the above text with hyperlinks for all technical terms is also on the nlab at string theory faq -- is string theory mathematically rigorous ? .
i would like to try an answer for my own question . the straightforward interpretation of the 3 equations in the original post is that they do present a legitimate conflict . this conflict disappears when the spatial domain is extended to infinity , and the role of the boundary conditions goes away . so , the conflict , the violation of gauge invariance for the finite ring , suggests that either the ring is simply not covered by quantum mechanics , or there is something about the boundary conditions implied by the ring geometry that does not work . so are there other boundary conditions that could be tried ? the obvious ones are to demand periodicity not of the wave function , but of the probability density and probability current density . these are both real ( not complex ) quantities , would let the phase of the wave function have a discontinuity at the boundary , so long as the gradient of the phase was smooth . such a choice for the boundaries would allow the continuous eigenvalue spectrum of the infinite line to apply as well to the ring , which would restore gauge invariance . but this comes at the expense of inhomogeneous and nonlinear boundary conditions . the nonlinear boundary conditions could be accepted if they are consistent with the standard type of hilbert space of hermitean operators , and the principle of superposition . in a paper i recently posted on arxiv . org i go into some detail about how the nonlinear boundaries are indeed consistent with both gauge invariance and the standard structure of hilbert space . the bottom line is that the nonlinearity creates a continuous spectrum of eigenvalues , not all of which are superposable in hilbert space . the nonlinearity in the boundary allows a subset to be superposable . the combination of continuous eigenvalues and superposable discreet eigenvalues makes a band structure for the hilbert space , rather than simple discreet levels . please click here for the larger explanation . so i think the correct solution of this problem is quite significant for quantum systems coupled to their environments , such as josephson junctions used as qubits .
the only way is to figure out some tricky atom structure which will work at higher temperatures . check out existing examples : http://en.wikipedia.org/wiki/bscco http://en.wikipedia.org/wiki/high-temperature_superconductivity#examples you ( probably ) can not make a superconductor by passing current through non-superconductor , but i do not say it is impossible . from the it side , if one would be able to create precise model for superconductivity in any atom structure and then bruteforce different structures , then it might be possible to invent something new . but this is insanely large piece of work .
see this better introduction to the history : http://en.wikipedia.org/wiki/principle_of_least_action#origins.2c_statement.2c_and_the_controversy of course , the notion of " action " only becomes meaningful when one actually knows what the purpose of the action is - to be minimized . around 1744 and 1746 , pierre louis maupertuis figured out that this could be a way to formulate laws of physics when he generalized fermat 's 17th century " principle of least time " ( for the trajectory taken by light in any environment with a variable index of refraction ) by this proverb : nature is thrifty in all its actions . obviously , some word had to be constructed or borrowed to describe the new quantity whose importance was previously unknown to the humans ( and remains to be unknown to most humans even today ) . note that the word " actions " appeared as the only noun of the quote in the context of these minimization problems , so it became known as wirkung ( $w$ ) in german and action ( $s$ ) in english . i am actually not sure why the letter $s$ was chosen . there have been claims that leibniz had found the principle as early as in 1707 . maupertuis also wrote : the laws of movement and of rest deduced from this principle being precisely the same as those observed in nature , we can admire the application of it to all phenomena . the movement of animals , the vegetative growth of plants . . . are only its consequences ; and the spectacle of the universe becomes so much the grander , so much more beautiful , the worthier of its author , when one knows that a small number of laws , most wisely established , suffice for all movements .
the decay of a neutral $\pi^0$ to three photons would indeed violate charge conjugation . the charge conjugation argument goes as follows : the reaction $$\pi^0 \to 3 \gamma$$ is mediated by electromagnetism . qed has a charge conjugation symmetry , so you should be able to apply a charge conjugation to both sides of the equation . under charge conjugation , $\pi^0 \to \pi^0$ while $\gamma \to - \gamma$ ( this follows from the gauge principle ) . therefore , the initial state has an even c transformation while the final state has an uneven c transformation . in other words : applying these transformation properties to both sides of the above equation gives you a minus sign ( this may seem like a bogus calculation , but you can work it out with the actual fields if you like ) . the angular momentum thing is more difficult . you have to take into account that the system of three photons does not only have spin , but can also have an non-zero ' regular ' angular momentum ( e . g . the photons are emitted in a p-wave and not an s-wave ) .
[ quote ] if i understood correctly what i have been taught so far , in qft one must find some way to quantize the fields obeying the field equation in question . [ \quote ] this is correct . in this particular case you start with the lagrangian that has schrodinger 's equation as its eom . the following turns out to be the correct one : $$\mathcal{l} = i\psi^*\partial_{t}\psi - \frac{1}{2m} ( \partial_{j}\psi^* ) ( \partial_j \psi ) . $$ ( the $j$ 's are summed over ) . now to quantize , we can proceed in the way you suggested , i.e. impose ccr on the field $\psi$ and $\psi^*$ and then express them in terms of the creation and annihilation operators . here it turns out that you can define the creation and annihilation operators to be the fourier transform of the fields $\psi$ and $\psi^{\dagger}$ , since you can show that the fourier transforms obey the same commutation relations as the creation and annihilation operators need to , i.e. $$ [ \hat{\psi} ( \mathbf{k} ) , \hat{\psi}^{\dagger} ( \mathbf{k'} ) ] = ( 2\pi ) ^3\delta ( \mathbf{k}-\mathbf{k'} ) . $$ though one thing i am unsure why your professor did was summation instead of integration , since creation and annihilation operators usually depend on a continuous degree of freedom $k$ and thus need to be integrated over . that is , if i was to take that approach , i could postulate that $$\psi ( \mathbf{x} ) = \int \frac{d^3\mathbf{k}}{ ( 2\pi ) ^3} e^{i\mathbf{k . x}}a_{\mathbf{k}} , $$ and from there show that $\psi$ and $\psi^{\dagger}$ obey the correct commutation relations if i impose the commutation relations for the $a$ 's .
how the tap work ? and how we can apply equation of continuity to the water flow when we turn the knob and when we cover the tap with thumb the tap works by changing the minimum cross-sectional area of the flow . for a given pressure difference ( upstream pressure minus downstream pressure ) flow rate is a function of minimum cross-sectional area . using your thumb would do the same thing . you can stop the flow with your thumb if you are strong relative to the force of the flow . http://people.uncw.edu/lugo/mcp/diff_eq/deproj/orifice/orifice.htm where am getting wrong with my understanding of the hydraulic analogy . probably you are misunderstanding the equation of continuity . the equation of continuity only means that the mass flow rate in equals the mass flow rate out . it does not mean that the flow in and the flow out never change . flow rate in and flow rate out can change simultanteously . your statement " on the other hand removing pipe 2 will not change water flow [ rate ] " is incorrect . removing pipe 2 will make a big difference in total flow if it is large in cross section compare to pipe 1 . it will make a small difference in total flow if it is small in cross section compare to pipe 1 . your statement " when we decrease the area of the mouth of tap by our thumb the amount of water flowing out remains same " is also incorrect . instead , the flow rate approaches zero as you make the cross-sectional area of the unblocked portion of the mouth small .
to understand this explanation , you need to understand fourier decomposition of the electromagnetic field . in any homogeneous medium , any electromagnetic field can be thought of as a linear superposition of plane waves , all in different directions . because they run in different directions , the phase delays they undergo in propagating from , say , your aperture to another , parallel plane are all different . therefore the wavefront gets " scrambled " owing to these direction-dependent phase delays . this interference between the different plane wave components of the electromagnetic field is what we commonly call " diffraction " . i further explain this idea , as well as draw some diagrams in this answer here as well as this one here . so with this introduction in mind , let 's look at your paragraph . for simplicity , assume only one transverse direction and one axial ( in the direction of propagation ) direction . let 's also assume scalar optics , i.e. that the electromagnetic field is well represented by the behaviour of one of its cartesian components , so that we can do fourier optics on scalar field . so we have a uniformly lit aperture of width $a$ . its transverse profile is therefore the function ${\rm rect} ( 2 x/a ) $ where ${\rm rect} ( x ) = 1 ; \ , |x| \leq 1$ and ${\rm rect} ( x ) = 0 ; \ , |x| &gt ; 1$ . we take a fourier transform to find the superposition weights of each plane wave component , because each such component has a transverse variation $\exp ( i\ , k_x\ , x ) $ where $k_x$ is the fourier transform variable with units of reciprocal length . the fresnel distance is , as the paragraph says , simply the axial distance needed for this spread to double the beam width . so it is a rough measure of how quickly the light spreads . so this is how the " divergence " arises from diffraction , i.e. the interference between an optical field 's plane wave components as they propagate . also $\sin\theta = k_x/k$ where $k = 2\pi/lambda$ defines the angle that this plane wave component makes with the axial direction . we take the fourier transform , we find that there is a spread of $k_x$ values such that the plane wave components most skewed to the axial direction make an angle without direction of roughly $\lambda/a$ . so , owing to these skewed components , the field 's energy spreads out . the beam width diverges slowly at first and then , after an axial distance of several fresnel distances , the divergence speeds up so that the propagation becomes well modelled by the cone of rays diverging from the centre of the aperture . indeed if you plot contours of constant intensity , they are hyperbolas which begin at right angles to the aperture but bend so that their asymptotes are the cone defined by ray theory . the fresnel distance defines how far the " knee " of the hyperbol is from the aperture . for your question : is it not that the validity holds when all objects are comfortably larger , and not smaller , than the wavelength of light ? this is in general right , but it breaks down near focuses and in situations like this where we are near and aperture and if the aperture is comparable to the light wavelength . in this case you should be able to understand from the fourier analysis the reciprocal relationship between the aperture width and the angular spread .
one way to study this case is through the numerical analysis of diffraction , as described in my other answer to you . you can also do this pretty much as you describe through huygens 's principle or as feynman describes in his popular qed book . if you set up an equation to describe what you have said , you will see that the amplitude at a point with transverse co-ordinate $x$ on a screen at an axial distance $d$ from the plane with the knife edge is : $$\psi ( x ) \approx\int\limits_0^\infty\exp\left ( i\ , k\ , \sqrt{ ( x-x ) ^2+d^2}\right ) \ , {\rm d}\ , x\qquad ( 1 ) $$ where the line of sources runs from $x=0$ to $w$ ( the width of the bright region ) , where we can take $w\to+\infty$ if we like . we have neglected the dependence of the magnitude of the contribution from each source on the distance $\sqrt{ ( x-x ) ^2+d^2}$ . this is because we now invoke an idea from the method of stationary phase , whereby only contributions from the integrand in the neighbourhood of the point $x=x$ where the integrand 's phase is stationary will be important . thus for $x\approx 0$ we can assume $|x-x|\ll d$ and so : $$\psi ( x ) \approx\int\limits_0^w\exp\left ( i\ , k\ , \frac{ ( x-x ) ^2}{2\ , d}\right ) \ , {\rm d}\ , x\qquad ( 2 ) $$ an integral which can be done in closed form : $$\begin{array}{lcl}\psi ( x ) and \approx and \sqrt{\frac{2\ , d}{k}}\int\limits_{\sqrt{\frac{k}{2\ , d}} ( x-w ) }^{\sqrt{\frac{k}{2\ , d}} x} e^{i\ , u^2}\ , {\rm d}\ , u \\ and = and \sqrt{\frac{d}{2\ , k}} e^{i\frac{\pi}{4}} \sqrt{\pi} \left ( {\rm erf}\left ( e^{3\ , i\frac{\pi}{4}}\sqrt{\frac{k}{2\ , d}} ( x-w ) \right ) -{\rm erf}\left ( e^{3\ , i\frac{\pi}{4}}\sqrt{\frac{k}{2\ , d}}\ , x\right ) \right ) \\ and = and \sqrt{\frac{d}{2\ , k}} \left ( c\left ( \sqrt{\frac{k}{2\ , d}} x\right ) + i\ , s\left ( \sqrt{\frac{k}{2\ , d}}x\right ) -\right . \\ and and \qquad\left . \left ( c\left ( \sqrt{\frac{k}{2\ , d}} ( x-w ) \right ) + i\ , s\left ( \sqrt{\frac{k}{2\ , d}} ( x-w ) \right ) \right ) \right ) \end{array}\qquad ( 3 ) $$ where : $$\begin{array}{lcl} c ( s ) and = and \int\limits_0^s\ , \cos ( u^2 ) \ , {\rm d}\ , u\\ s ( s ) and = and \int\limits_0^s\ , \sin ( u^2 ) \ , {\rm d}\ , u\\ \end{array}\qquad ( 4 ) $$ where $c ( s ) $ and $s ( s ) $ are called the fresnel integrals . if i plot the squared magnitude of this function ( related to the fresnel integrals ) in normalised units when $k=d=1$ and $l\to\infty$ ( noting $c ( \infty ) =s ( \infty ) = -1/2$ ) for $x\in [ -10,20 ] $ i get the following plot : which i believe is exactly your plot with a shrunken horizontal axis ( yours is likely mine with the transformation $x_s = 2\ , \pi\ , x_r$ where $x_s$ is satwik 's $x$-co-ordinate and $x_r$ rod 's ) . footnote : one of the loveliest curves from eighteenth and nineteenth century mathematics is the cornu spiral , which is a special case of the euler spiral . $\psi ( x ) $ in ( 3 ) traces a path in the complex plane parameterised by $x$ , which turns out to be the arclength $s$ of the spiral path in $\mathbb{c}$ such that : $$\begin{array}{lcl}x and = and {\rm re} ( \psi ( s ) ) \propto c ( s ) + \frac{1}{2}\\ y and = and {\rm im} ( \psi ( s ) ) \propto s ( s ) + \frac{1}{2}\end{array}\qquad ( 4 ) $$ and i plot the normalised and shifted path $z = c ( s ) + i\ , s ( s ) $ i get the lovely spiral below . the curly bits spiral all the way in to $\pm ( 1+i ) /2$ as $s\to\infty$ . the shifting and then taking magnitude squared explains why the intensity plot above is not symmetric about $x=0$ , oscillating as $x\to\infty$ and dwindling monotonically as $x\to-\infty$ .
if you continued to read on , it goes on to say : actually , all bodies are electrified , but may appear not to be so by the relative similar charge of neighboring objects in the environment . an object further electrified + or – creates an equivalent or opposite charge by default in neighboring objects , until those charges can equalize . therefore , since all bodies are electrified or can be electrified , your statement is correct .
a linear operator $a : d ( a ) \to {\cal h}$ with $d ( a ) \subset {\cal h}$ a subspace and ${\cal h}$ a hilbert space ( a normed space could be enough ) , is said to be bounded if : $$\sup_{\psi \in d ( a ) \: , ||\psi|| \neq 0} \frac{||a\psi||}{||\psi||} &lt ; +\infty\: . $$ in this case the lhs is indicated by $||a||$ and it is called the norm of $a$ . notice that , therefore , boundedness , is not referred to the set of values $a\psi$ , which is always unbounded if $a\neq 0$ , as $||a\lambda\psi|| = |\lambda|\: ||a\psi||$ for $\psi \in d ( a ) $ and $\lambda$ can be chosen arbitrarily large still satisfying $\lambda \psi \in d ( a ) $ since $d ( a ) $ is an subspace . it is possible to prove that $a : d ( a ) \to {\cal h}$ is bounded if and only if , for every $\psi_0 \in d ( a ) $: $$\lim_{\psi \to \psi_0} a\psi = a \psi_0\: . $$ another remarkable result is that a self-adjoint operator is bounded if and only if its domain is the whole hilbert space . regarding $a= \frac{d}{dx}$ , first of all you should define its domain to discuss boundedness . an important domain is the space ${\cal s} ( \mathbb r ) $ of schwartz functions since , if $-id/dx$ is defined thereon , it turns out hermitian and it admits only one self-adjoint extension that it is nothing but the momentum operator . $d/dx$ on ${\cal s} ( \mathbb r ) $ is unbounded . the shortest way to prove it is passing to fourier transform . fourier transform is unitary so that it transforms ( un ) bounded operators into ( un ) bounded operators . ${\cal s} ( \mathbb r ) $ is invariant under fourier transform , and $d/dx$ is transformed to the multiplicative operator $ik$ i henceforth denote by $\hat a$ . so we end up with studying boundedness of the operator : $$ ( \hat a \hat{\psi} ) ( k ) = ik \hat{\psi} ( k ) \: , \quad \hat\psi \in {\cal s} ( \mathbb r ) \: . $$ fix $\hat\psi_0 \in {\cal s} ( \mathbb r ) $ with $||\hat\psi_0||=1$ assuming that $\hat\psi_{0}$ vanishes outside $ [ 0,1 ] $ ( there is always such function as $c_0^\infty ( \mathbb r ) \subset {\cal s} ( \mathbb r ) $ and there is a function of the first space supported in every compact set in $\mathbb r$ ) , and consider the class of functions $$\hat\psi_n ( k ) := \hat \psi_{0} ( k- n ) $$ obviously , $\hat\psi_n \in {\cal s} ( \mathbb r ) $ and translational invariance of the integral implies $||\hat\psi_n||=||\hat\psi_0||=1$ . next , notice that : $$\frac{||\hat a\hat\psi_n||^2}{||\hat\psi_n||^2} = \int_{ [ n , n+1 ] } |x|^2 |\hat\psi_{0} ( k-n ) |^2 dk \geq \int_{ [ n , n+1 ] } n^2 |\hat\psi_{0} ( k-n ) |^2 dk$$ $$ = n^2 \int_{ [ 0,1 ] } |\hat\psi_{0} ( k ) |^2 dk = n^2\: . $$ we conclude that : $$\sup_{\hat{\psi} \in {\cal s} ( \mathbb r ) \: , ||\hat{\psi}||\neq 0} \frac{||\hat a\hat\psi||}{||\hat\psi||} \geq n \quad \forall n\in \mathbb n $$ so $\hat a$ is unbounded and $a$ is consequently .
the issue is that by touching a wire , you are augmenting the circuit with yourself as a resistor . ( at first i wrote " inserting yourself " but as mmc 's comment pointed out , that is a misleading phrase to use . ) and whenever you change the layout of an electrical circuit , all the potentials and currents are subject to change . so the wire that is at ground potential before you touch it will not necessarily still be at the same potential after you touch it . in this specific case , i would guess that the $30\ \mathrm{a}$ value is the current that flows with only the battery and the load . you can calculate that the resistance of the load has to be $7.3\ \omega$ . if you could actually insert yourself into the circuit as shown in the diagram , you would be adding to the resistance of the circuit , which reduces the current . the resistance of the human body varies greatly depending on several factors , but a " typical " value might be on the order of $10\ \mathrm{k\omega}$ which reduces the current to a small fraction of an ampere . even with that small current , though , you are going to experience a large voltage drop because your resistance is so large compared to the rest of the circuit . in the diagram above , there would be a voltage drop of $219.8\ \mathrm{v}$ across your body . by inserting yourself in the circuit , you have prevented the wire coming out of the load from being grounded . note that if you are trying to determine whether this is a dangerous situation to be in , the value you should be looking at is the current of $0.022\ \mathrm{a}$ , not the $219.8\ \mathrm{v}$ voltage drop . that is what it means to say that it is the current that kills you , not the voltage . if you instead grab on to the wire while standing on the ground ( which seems like a more realistic situation ) , you are not actually inserting yourself in the circuit . instead you wind up with a setup more like this , in this case the wire coming out of the load resistor is still at potential zero because it is connected to ground via a zero-resistance path . ( keep in mind that this is an ideal model ; real wires do have some small nonzero resistance so in reality the wire would not quite be at zero volts . ) so the voltage difference across your body is going to be basically zero . besides , any current that flows between the circuit and the ground can do so by one of two paths , either through you or through the open wire . since your bodily resistance is much higher than that of the wire , essentially all the current will go through the wire , not through your body .
yes , a body 's mass is sufficient to cause melting . for example , suppose the moon , instead of orbiting the earth , fell onto the earth . the moon weighs about 7.35e22 kg , and is a distance of about 3.8e8 m from the earth which weighs about 6e24 kg and has a radius of around 6.4e6 m . so the difference in potential energy $u = -gm_1m_2/r$ for the earth-moon system at 3.8e8m and 6.4e6m is . $$ -g ( 7.35\times 10^{22} ) ( 6\times 10^{24} ) ( 1/3.8\times 10^8-1/6.4\times 10^6 ) = 4.5\times 10^{30}\ ; \textrm{joules}$$ where $g= 6.7\times 10^{-11}$ is the gravitational constant . the combined mass is still about 6e24 kg so the collision creates heat of $$ 4.5\times 10^{30}/6.0\times 10^{24} = 750,000\ ; \textrm{joules/kg}$$ to heat a kilogram of iron up by 1 degree kelvin requires about 460 joules . granite needs about 790 while basalt is 840 . other materials making up most of the earth have similar specific heats . so the collision will heat up the material by something around 1000 degrees kelvin . now that was just for the moon whose mass is only a tiny fraction of the earth 's . if a larger mass collided with the earth the temperature would be proportionally larger . if the body were about 4x the mass of the moon , the temperature increase would be around 4000 k and that would certainly thoroughly melt the planet . so yes , it is possible for gravity alone to melt planets .
the comment on this page http://chemistry.about.com/od/photogalleries/ig/nuclear-tests-photo-gallery/operation-teapot-test.htm http://chemistry.about.com/b/2011/04/19/nuclear-explosion-lines-spikes.htm says : sounding rockets or smoke flares may be launched just before a device explodes so that their vapor trails may be used to record the passage of the otherwise invisible shock wave . to learn about every detail of these tests , contact your nearest fbi agent .
there is a number of interesting points to this . the passage from 1 . to 2 . is not trivial . if you do the calculation , you will see that the laplacian $\nabla^2\vec{e}$ from the wave equation gives rise to the term in $\left ( \nabla\chi\right ) ^2$ you mention as well as a term in $\nabla^2\chi$ . this second term only goes away in the small $\lambda$ limit and it is the essence of the eikonal approximation . it is not a calculation you should wave away : work it out in full and implement the approximation , noticing that locally $\chi ( \vec{x} ) =\vec{k}\cdot\vec{x}+\text{slow factors}$ , where $\vec{k}$ is large . ( you will of course need to quantify " slow " . ) ( the calculation that $\vec{s}=\frac{c^2}{n^2\omega}\nabla\chi$ , on the other hand , is trivial . ) as kdn mentioned , the integral curves of $\vec{s}$ and its unit vector $\vec{s}$ are the same . this follows from the definition of integral curves : they are curves such that the vector field is tangent to them throughout . this is independent of the length of the vector . ( in terms of the curve it corresponds to a reparametrization of the " time": it changes the speed but not the direction of the velocity . ) using a unit vector means that light rays will be parametrised by path length . one can simply define light rays to be the integral curves of $\vec{s}$ and be happy about it , though of course that is simply missing the physics . the key fact about the light rays , so defined , is that they are everywhere normal to the surfaces of constant $\chi$ , i.e. the surfaces of constant phase , i.e. the wavefronts . plane waves propagate in straight lines normally to the wavefronts in free space , and so do light rays ( so defined ) . it is the normal to the wavefronts that matters when working out fresnel equations , and therefore the ( so defined ) light rays will obey snell 's law . ultimately , proving 3 . is a matter of definition : what are light rays ? write down any defining property and you will be able to prove the integral curves of $\vec{s}$ obey it . it is important to note that in isotropic media $\vec{s}$ is not only the local unit poynting vector , but it is also the local unit wave vector . ( essentially , this is the same point as above . ) intuitively , light rays ought to follow wave vectors because it is wave vectors that tell light waves where to go . in a birefringent ( not isotropic ) medium the phase propagation direction ( wave vector ) and the energy propagation direction ( poynting vector ) are not necessarily the same ( and the snell law does not apply ) . proving 4 . is an interesting exercise ( i.e. . do it ! ) but it is essentially trivial . it relies on the identity $\frac{d\vec{x}}{d\tau}=\vec{s}$ , which defines light ray curves $\vec{x} ( \tau ) $ , on judicious use of the total derivative $\frac{d}{d\tau}= ( \frac{d\vec{x}}{d\tau}\cdot\nabla ) $ , and some interesting vector calculus manipulations . ( hint : prove $ ( \nabla\chi\cdot\nabla ) \nabla\chi=\frac12\nabla\left ( \nabla\chi\right ) ^2$ . ) presumably you know by now that what you get is called the ray equation , what it means , and how to use it , or you would not have stopped there ; ) . this looks like enough to get you going but if you have more questions , do ask .
if you believe that ( a ) timelike separated events cannot be simultaneous in any reference frame , and ( b ) the set of inertial frames is ( in some appropriate sense ) a continuous set , then l and l 's conclusion follows . after all , if there were two frames in which the order of two timelike separated events differed , then by continuously transforming one frame into another , you could find one in which they were simultaneous . but without some such additional assumption , you are right that the conclusion does not logically follow . there are coordinate systems that preserve the spacetime interval but flip the direction of time , such as the substitution $t\to -t$ that you mention . as bebopbutunsteady observes in a comment to karsus ren 's answer , we often use the term " orthochronous lorentz transformation " to refer to a transformation that preserves the direction of time . the full group of lorentz transformations ( i.e. . , of all transformations that preserves the interval ) includes both orthochronous and non-orthochronous components , which are not connected to each other . physically , we usually only consider the orthochronous ones . you do have to be careful with the terminology : sometimes people use " lorentz transformation " to mean just the orthochronous ones ; sometimes it is the full group . by the way , pretty much the same thing applies to spatial reflections : is $x\to -x$ ( leaving $y , z , t$ unchanged ) a lorentz transformation ? after all , it preserves the spacetime interval . often we refer to non-reflecting lorentz transformations as " proper . " so when people are being careful with their terminology they often refer to " proper orthochronous lorentz transformations . "
there are in fact two field lines that depart each charge headed towards the other . these lines meet at the origin ( the mid-point of the two charges ) , where the field is zero , and vanish there . there are also two other lines , which are born at the origin and depart along the vertical axis . thus , formally , two lines go in and two lines go out , so no lines actually die in empty space . these lines are actually a limiting case of lines that leave the point charges at a small angle $\epsilon$ from the intercharge axis ; these lines make increasingly close approaches to the origin as $\epsilon\rightarrow0$ , and then they shoot off to infinity , increasingly close to the vertical axis . ( if you are sharp , you will notice there is actually an infinity of such lines , since there is also lines that go off perpendicularly to the screen and at any angle in between . thus my " two-for-two " argument is not actually quite right . can you see the limiting behaviour that makes it right ? ) pictures of this were relatively hard to find , but you can see them in this wolfram web app : you also have to consider one key point : at the origin , the field is zero , so actually there should be no field lines through it . or , more formally , the density of field lines should be zero . this comes about in that the angle $\epsilon$ should be really small for the lines to actually approach the origin . you should then plaster the diagram with lines leaving equiangularly at angle $\epsilon$ from each charge , and that will mean a lot of lines on the " outside " of the charges . ultimately , though , the lesson is that individual field lines are not that important , and it is the set of lines , equiangularly leaving the charges ( in 3d ! ) , that makes a physically relevant diagram . and even then , field line diagrams are only of limited utility in understanding electric fields , mostly because they only incorporate with the utmost difficulty the superposition principle , which is at the real heart of classical electromagnetism .
to see if a process will take place you need to calculate it is gibbs free energy $\delta g$ . this is defined as : $$\delta g = \delta h - t\delta s$$ the quantity $\delta h$ is the helmholtz free energy and for liquids and solids is roughly the amount of energy released ( $\delta h$ is negative if energy is released and positive if energy is absorbed ) . $\delta s$ is the entropy change for the process . for precise definitions of these quantities have a look on wikipedia . the equilibrium constant for the process is the exponent of the gibbs free energy . if $\delta g$ is large and negative the process goes to approximately 100% completion while if $\delta g$ is large and positive the process hardly takes place at all . anyhow , for the mixing of two liquids the entropy change is always large and positive so it tends to make $\delta g$ large and negative and this favours mixing . for two liquids to be immiscible the molecules of the two liquids must repel each other so strongly that the enthalpy of mixing overcomes the entropy . as an example take water and mineral oil . water hydrogen bonds very strongly to other water molecules but hardly at all to oil molecules . to mix oil and water you have to put energy in to break all those hydrogen bonds but you get no energy back when the water and oil mix . that makes $\delta h$ large and positive and this disfavours mixing . the surface tension arises from the same physics as the immiscibility . suppose you want to increase the area of the interface . to do this you need to move water molecules from the bulk to the oil/water boundary . but this costs energy because in moving a water molecule to the bounday you break water-water bonds and replace them with water-oil bonds . since it takes energy to increase the area of the boundary this means there is a force , and this is the surface tension . the surface tension is a force per unit length ( because it is an energy per unit area ) and it is independant of the area of the interface . it depends only on the two liquids at the interface . the walls of the container can have an effect because the liquids will generally interact differently with the container , but normally the surface tension is dominant and it would only be in very thin tubes that you had see an effect from the walls . you asked about articles on this subject , and as usual wikipedia has some excellent articles . have a look at http://en.wikipedia.org/wiki/entropy_of_mixing and http://en.wikipedia.org/wiki/surface_tension
actually in electrostatics energy density of e-field is not a physical observable . as you say , only when charges move will there be any work done . while the two ways of calculating total energy end the same , you cannot distinguish whether energy is stored on the charges or in the field . even e-field itself is more of an abstract mathematical entity , without which everything can be calculated in terms of coulomb law . the physical reality of e and b fields ( and the energy density associated ) becomes apparent only in non-static cases . for example , in electromagnetic radiation , fields can propagate in free space without associating with charges and currents , and the radiation may do work on non-charges ( for example , light pressure ) . because from maxwell equations we can derive a general formula of energy density $$\rho = \frac{\epsilon_0}{2} |\vec e|^2 + \frac{1}{2\mu_0} |\vec b|^2$$ which coincides with the electrostatic case , we deduce that even in electrostatics energy is indeed stored in the fields .
there are three main reasons . 1 ) while venus is orbiting the sun at 35.02 km/s , the earth is also orbiting the sun in the same direction at 29.78 km/s . this factor will decrease the relative transit velocity of venus as seen from earth . 2 ) venus is travelling at 35.02 km/s an elliptical orbit . hence the actual distance traveled by venus during the transit will be slightly more than its diameter because it is travelling on a curved path and not a straight line . this factor will increase the actual transit distance covered by venus . however the contribution of this is negligible and can be ignored except of high precision calculations . 3 ) there will be a small but measurable impact because of the surface velocity of earth 's rotation at 0.434 km/s ( at the equator ) about its axis . notice that the tangential velocity of an observer on earth due to the rotation of the earth about its axis will be in opposite direction to the tangential velocity of both the venus and earth around the sun . this factor will increase the relative transit velocity of venus as seen from earth . my calculation , using kepler 's law differ slightly from that of nathaniel but it is essentially same in spirit . we obtain the transit time of 19 mins 56 seconds which is accurate enough . $$ t \approx \frac{d_v}{v_v\{1 - ( t_v/t_e ) ^{2/3}\} + v_e} = 19 \min 56 \sec $$ where $d_v$ = diameter of venus , $v_v$ = orbital velocity of venus , $v_e$ = orbital velocity of earth , $t_v$ = orbital period of venus , $t_e$ = orbital period of earth , $v_e$ = rotation velocity of earth .
if you look at the structure of the circuit on the left , and where voltage must be , you will see that the wire connecting c1 and c2 ensures that the plates connected to point a are at one voltage , let 's call it va , and the other plates connected to voltage vb . so this can be thought of as two batteries , each with voltage differential of va-vb in parallel . for the second circuit , the two capacitors are in series with respect to the battery ( as mentioned by @frankh )
afair it has two massless modes , as there are no quadratic terms around the minimum .
you can not integrate the right hand side because $f=f ( x_i ) $ and you have got a differential on $t$ . as for a ) , if you rearrange terms , you can verify that $$m\frac{d\dot{x_i}}{f ( x_i ) }=g ( t ) \ , dt$$ so that now you can not integrate the left hand side because $f$ depends un $x_i$ and you have got a differential on $\dot{x}_i$ .
the continuous stream of air that you are blowing in , it does not enter the pipe continuously . when the stream of air hits the hard edge in an organ pipe , it flaps in and out of it due to the difference in the density of the air outside and inside the pipe . this oscillation of the air in and out , it will be a periodic energy supply for the standing wave in the pipe . think about it as a skipping rope game . the rope is an standing wave and we move the edge up and down to keep it going . the frequency of the stream of air going in and out of the mouthpiece depends on the geometry of the mouthpiece , mainly the length of the mouthpiece that finishes with the hard edge . the frequency of the standing wave in the instrument depends mainly on the length , but also on other factors like the the diameter , temperature , humidity . . . it is interesting to notice that if you blow harder , the frequency of the flapping in the mouthpiece will increase . this will increase the excitation of the standing wave in the pipe and it will jump to the next resonance frequency . this will be double the frequency if the instrument has open ends in both sides . increasing the speed of the air of your input you can excite higher resonance frequencies of the pipe . this also happens with the skipping rope game where you can move the rope up and down faster and you will see standing waves of higher frequencies with more nodes . if you want a more detailed explanation , here is a link with a chapter of a book by thomas d . rossing : woodwind instruments
all we are doing is using a set of units where certain quantities happen to take convenient numerical values . for example , in the si system we might measure lengths in meters and time intervals in seconds . in those units we have $c = 3 \times 10^8 \text{m}/\text{s}$ . but you could just as well measure all your distances in terms of some new unit , let 's call it a " finglonger " , that is equal to $2.5 \times 10^6 \text{m}$ , and time intervals in a new unit , we will call it the " zoidberg " , that is equal to $8.33 \times 10^{-3} \text{s}$ . then the speed of light in terms of your new units is $$ c = 3 \times 10^{8} \text{m}/\text{s} = 1 \frac{\text{finglonger}}{\text{zoidberg}} ~ . $$ the units are still there -- they have not been " deleted " -- but we usually just make a mental note of the fact and do not bother writing them .
time zones moving on from the calendar to time , we recommend the abolition of all time zones , as well as of daylight savings time , and the adoption of atomic time—in particular , greenwich mean time , or universal time , as it is called today . like the adoption of a modern calendar , the embrace of universal time would be beneficial . for example , the adoption of universal time would give new flexibility to economic management in the vast east-west expanse of russia : everyone would know exactly what time it is everywhere , at every moment . opening and closing times of businesses could be specified for every class of business and activity . if thought desirable , banks and financial institutions throughout the country could be required to open and to close each day at the same hour by the world time . this would mean that bank employees in the far east of russia would start work with the sun well up in the sky , while bank employees in the far west of russia would be at their desks before the sun has risen . but , across the country , they could conduct business with one another , all the working day . ( this would have a second benefit : at least in the far east and far west , the banks would be open either early , or late , convenient for those who are working “sunlight hours , ” such as farmers . ) with universal time , agricultural workers , critically dependent on the position of the sun , could rise with the sun , without producing any impact on other aspects of cultural and economic life . the readings on the clocks , and the date on the calendar , would be the same for all . but , times of work would be attuned with precision to russia’s local and national needs . china already has adopted a single time zone for the same purposes . and all aircraft pilots , worldwide , use universal time exclusively , for exactly the same reason that we are advocating its broad adoption—plus avoiding collisions . moscow could introduce both a simplified calendar , identical each year ( harmonized with the seasons by rare full-week adjustments at year’s end ) , and universal time , which would abolish the international date line , making the date and the time identical everywhere , including alaska and the farthest eastern regions of russia . there , and also in the center of the pacific ocean , the date would change at 00:00:00 , just as the sun passed overhead . source and context . also see this and this for discussion . dst one of the biggest reasons we change our clocks to daylight saving time ( dst ) is that it reportedly saves electricity . newer studies , however , are challenging long-held reason . a report was released in may 2001 by the california energy commission to see if creating an early dst or going to a year-round dst will help with the electricity problems the state faced in 2000-2002 . you can download an acrobat pdf copy of the staff report , effects of daylight saving time on california electricity use , publication # 400-01-13 , ( pdf file , pages , 5.2 megabytes ) . the study concluded that both winter daylight saving time and summer-season double daylight saving time ( ddst ) would probably save marginal amounts of electricity - around 3,400 megawatt-hours ( mwh ) a day in winter ( one-half of one percent of winter electricity use - 0.5% ) and around 1,500 mwh a day during the summer season ( one-fifth of one percent of summer-season use - 0.20% ) . winter dst would cut winter peak electricity use by around 1,100 megawatts on average , or 3.4 percent . summer double dst would cause a smaller ( 220 mw ) and more uncertain drop in the peak , but it could still save hundreds of millions of dollars because it would shift electricity use to low demand ( cheaper ) morning hours and decrease electricity use during higher demand hours . the energy commission has also published a new report titled the effect of early daylight saving time on california electricity consumption : a statistical analysis . publication # cec-200-2007-004 , may 27 , 2007 . ( pdf file , 592 kilobytes ) a more recent study concludes that daylight saving time in indiana actually increases residential electricity demand . that study titled " does daylight saving time save energy ? evidence from a natural experiment in indiana " . ( pdf file ) looked at the electricity use when portions of the state finally started to observe dst . before the new extended dst , portions of indiana did not observe dst . some have wondered whether this study would be true for the entire united states . initial analysis by staff of the california energy commission says a similar study may not yield the same results for california because : the use of residential air conditioning is relatively low in indiana , and the saturations are low . where as california has high usage of air conditioning in the summer . heating use is relatively high in indiana , while it is relatively low in california . the diurnal variation in temperature is low while california is very high . indiana is located in western edge of the same time zone as maine and florida , but the sun actually comes up at an earlier time than those other two states . indiana 's north-south location will affect how long the days are in the summer and might very well lead to different results in different areas . so , while the analysis is of interest to indiana , it is conclusions may not be totally correct for california or the rest of the country . the first national study since the 1970s , was mandated by congress and was done by the u.s. department of energy . the doe study can be downloaded at : http://www1.eere.energy.gov/ba/pba/pdfs/epact_sec_110_edst_report_to_congress_2008.pdf ( pdf file , 285 kb ) [ actually : here ] the key findings in the report to congress are : the total electricity savings of extended daylight saving time were about 1.3 tera watt-hour ( twh ) . this corresponds to 0.5 percent per each day of extended daylight saving time , or 0.03 percent of electricity consumption over the year . in reference , the total 2007 electricity consumption in the united states was 3,900 twh . in terms of national primary energy consumption , the electricity savings translate to a reduction of 17 trillion btu ( tbtu ) over the spring and fall extended daylight saving time periods , or roughly 0.02 percent of total u.s. energy consumption during 2007 of 101,000 tbtu . during extended daylight saving time , electricity savings generally occurred over a three- to five-hour period in the evening with small increases in usage during the early- morning hours . on a daily percentage basis , electricity savings were slightly greater during the march ( spring ) extension of extended daylight saving time than the november ( fall ) extension . on a regional basis , some southern portions of the united states exhibited slightly smaller impacts of extended daylight saving time on energy savings compared to the northern regions , a result possibly due to a small , offsetting increase in household air conditioning usage . changes in national traffic volume and motor gasoline consumption for passenger vehicles in 2007 were determined to be statistically insignificant and therefore , could not be attributed to extended daylight saving time . source and further references and context
here 's part of my answer to the derivvation of the em tensor for the ghost action . it does not match the expression you gave , but i may have made a mistake . can you check my work ? we start with the action \begin{equation} \begin{split} s_{gh} and = - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} \nabla_\mu c^\beta \\ \end{split} \end{equation} let us now vary the action w.r.t. metric . we get \begin{equation} \begin{split} \delta s_{gh} and = - \frac{i}{2\pi} \int d^2 \sigma \left ( \delta \sqrt{g} \right ) g^{\alpha\mu} b_{\alpha\beta} \nabla_\mu c^\beta \\ and ~~~~~~~~~~~~~~~~~- \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} \left ( \delta g^{\alpha\mu} \right ) b_{\alpha\beta} \nabla_\mu c^\beta \\ and ~~~~~~~~~~~~~~~~~- \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} \delta \left ( \nabla_\mu c^\beta \right ) \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \left [ b_{\alpha\mu} \nabla_\beta c^\mu + b_{\beta\mu} \nabla_\alpha c^\mu - g_{\alpha\beta} b_{\rho\sigma} \nabla^\rho c^\sigma \right ] \delta g^{\alpha\beta} \\ and ~~~~~~~~~~~~~~~~~ - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} c^\lambda \delta \gamma^\beta_{\mu\lambda} \\ \end{split} \end{equation} we now use \begin{equation} \begin{split} \delta \gamma^\beta_{\mu\lambda} = \frac{1}{2} g^{\beta\rho} \left [ \nabla_\lambda \delta g_{\rho \mu} + \nabla_\mu \delta g_{\rho \lambda} - \nabla_\rho \delta g_{\mu\lambda}\right ] \end{split} \end{equation} note that in particular , it is a tensor . the last term then becomes \begin{equation} \begin{split} i and = - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} c^\lambda \delta \gamma^\beta_{\mu\lambda} \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} b^{\mu\rho} c^\lambda \left [ \nabla_\lambda \delta g_{\rho \mu} + \nabla_\mu \delta g_{\rho \lambda} - \nabla_\rho \delta g_{\mu\lambda}\right ] \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} b^{\mu\rho} c^\lambda \nabla_\lambda \delta g_{\rho \mu} \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \nabla_\lambda \left ( b_{\alpha\beta} c^\lambda \right ) \delta g^{\alpha\beta} \end{split} \end{equation} we then have \begin{equation} \begin{split} \delta s_{gh} and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \left [ b_{\alpha\mu} \nabla_\beta c^\mu + b_{\beta\mu} \nabla_\alpha c^\mu - g_{\alpha\beta} b_{\rho\sigma} \nabla^\rho c^\sigma + \nabla_\lambda \left ( b_{\alpha\beta} c^\lambda \right ) \right ] \delta g^{\alpha\beta} \end{split} \end{equation}
to elaborate on marek 's ( correct ) answer , since it seems that math is the issue that @casebash is having : start with an integral representing the time elapsed on the moving observer 's clock in terms of the stationary observer 's coordinates ( i am supressing g and c below . feel free to replace all $t$ 's with $c\ , t$ 's , and all $m$ 's with $\frac{gm}{c^{2}}$ 's ) : $$\begin{equation} \delta \tau = \int d\tau \sqrt{{\dot t}^{2} ( 1-\frac{2m}{r} ) - \frac{{\dot r}^{2}}{1-\frac{2m}{r}}- r^{2} {\dot \theta}^{2} - r^{2}\sin^{2}\theta {\dot \phi}^{2}} \end{equation}$$ where $ ( t , r , \theta , \phi ) $ are all considered to be functions of $\tau$ denoting the moving observer 's position at time $\tau$ . our problem amounts to looking for the path beginning at $ ( t_{0} , r , \theta , \phi ) $ and ending at $ ( t_{f} , r , \theta , \phi ) $ that maximizes this integral , subject to the constraint that the quantity under the square root will be equal to $1$ when the calculation is complete . to make our calculations easier , note that the square root is a monotonic function over all its domain , so we might as well maximize $$\begin{equation} \frac{1}{2}\int d\tau \left ( {\dot t}^{2} ( 1-\frac{2m}{r} ) - \frac{{\dot r}^{2}}{1-\frac{2m}{r}}- r^{2} {\dot \theta}^{2} - r^{2}\sin^{2}\theta {\dot \phi}^{2}\right ) \end{equation}$$ which is considerably simpler . it would take a whole page to carefully vary ( meaning that we basically , but not quite , take the derivative of the function with respect to ) this with respect to the four independent functions . so , i am going to just give you a taste of the task by varying with respect to $\theta$ . the path of maximum " moving clock " time ( heretofore called ' proper time' ) will be the one for which these variations are zero . the other four functions follow just as easily . the variation of this integral with respect to $\theta$ gives us $$\begin{align} \delta \delta\tau |{}_{\theta} and =\int d\tau\left ( -r^{2}{\dot \theta}\delta {\dot \theta}-r^{2}\sin\theta \cos \theta {\dot \phi}^{2}\delta \theta\right ) \\ 0 and =\int d \tau \left ( -\frac{d}{d\tau}\left ( {\dot \theta} r^{2}\delta \theta\right ) + \left ( {\ddot \theta}r^{2} + 2 r {\dot r}{\dot \theta}-r^{2}\sin\theta \cos \theta {\dot \phi}^{2}\right ) \delta \theta\right ) \\ and =\left ( \dot \theta ( t_{0} ) r ( t_{0} ) ^{2}\delta \theta ( t_{0} ) -\dot \theta ( t_{f} ) r ( t_{f} ) ^{2}\delta \theta ( t_{f} ) \right ) \\ and \quad + \int d\tau\ ; r^{2}\left ( {\ddot \theta} + \frac{1}{r}2{\dot r} {\dot \theta}- \sin \theta \cos \theta {\dot \phi}^{2}\right ) \delta \theta \end{align}$$ the first term , that we obtained by integrating the total derivative , vanishes since the variation of $\theta$ is zero at the endpoints $t_{0}$ and $t_{f}$ ( our space of states is paths beginning and ending at a fixed value of $\theta$ . if the variation will be zero , then the remaining stuff under the integral must turn out to be zero if it is going to be a minimum for an arbitrary fixed-point variation ( if the integrand is not zero , just imagine making the variation a hundred kajillion only at the point where it is nonzero--clearly this is not an extremum ) . therefore , the $\theta$ variable of the maximum proper time path must satisfy ${\ddot \theta} + \frac{1}{r}2{\dot r} {\dot \theta}- \sin \theta \cos \theta {\dot \phi}^{2}=0$ . it turns out that this equation is exactly the same equation you get for geodesic motion of a particle , which is the path followed by an observer being acted upon only by gravity . a simple calculation of the first integral for any other path ( and the fact that it will come out higher ) will then show you that this , in fact , is the maximum time path to travel .
in the early days of radio , the resonance of the antenna in combination with its associated inductive and capacitive properties was indeed the item which " dialed in " the frequency you wanted to listen to . you did not actually change the length of the antenna , but by changing the inductor ( a coil ) or capacitor connected to the antenna you tuned the resonance . the output signal is an alternating voltage , and by rectifying it with a diode ( called a " crystal " then . . ) you could extract a signal modulated as a varying amplitude of the carrier wave . all this without any battery ! : ) but actually the antenna in a normal modern radio is not the component that " dials in " the selected broadcast frequency . the antenna circuit should indeed have a resonance within the band of frequencies you are interested in but this wide-band signal is then mixed with an internally generated sinusodial signal in the radio in an analog component , this subtracts the frequencies and lets the rest of the radio operate on a much easily handled frequency band ( called the intermediate frequency ) . it is in the mixer you tune the reception in a modern superheterodyne radio receiver . it is much easier to synthesize an exact mixing frequency to tune with than to change the resonance of the antenna circuit . the rest is not really physics , but the difference between an analog and a digital radio comes in the circuits after this and basically an analog radio extracts a modulation from the intermediate frequency which is amplified and sent to the speakers or radio output . in a digital radio , the signal represents a digital version of the audio , just like a wav or mp3-file on a computer is a digital representation which can be turned back into an analog signal you can send to a speaker . the benefit of this is that the digital signal requires ( potentially ) less bandwidth in the air so you can fit more signals in the same " airspace " and that the digital signal can be less susceptible to noise . i write " can " , because unfortunately many commercial digital radio/tv stations do not do this to improve the viewing or listening quality but just to fit in more content . let me reiterate that in a " digital " radio , the component that selects the reception frequency is still analog but the mixing ( tuning ) frequency is digitally controlled and selected . there is also a very interesting thing called software defined radio , sdr , which is the principle where the intermediate frequency ( or in some cases the antenna frequency directly ) is turned into a digital signal and demodulated by a signal processor which is completely software-upgradeable . since it is much easier to program new software than to solder electronic components around , this created large interest in the radio hobby community where you can completely change the properties of a radio receiver just by downloading someone else 's software from the net or write a new one yourself . if you include sdr , and apply it without any intermediate frequency ( take the antenna directly to an analog/digital converter and into a signal processor ) , you do indeed have a purely software-way of tuning your source like you ask for , although this is not how the most common digital radios work currently .
rodrigo has a good explanation for why this intuitive explanation is useful . if you wanted to prove it mathematically , you had have to find the exact field first . here 's an example from griffiths : if you have a line of charge with linear charge density $\lambda$ on the $x$-axis running from $-l$ to $+l$ , the field at some height $z$ above its midpoint is given by $$ e = \frac 1{4\pi\epsilon_0} \frac{2\lambda l}{z\sqrt{z^2+l^2}} . $$ if you have dissimilar numbers $a\gg b$ , then $$\sqrt{a^2+b^2} = a\sqrt{1+\frac{b^2}{a^2}} = a \left ( 1 + \frac 12 \frac {b^2}{a^2} + \mathcal o\left ( \frac{b^4}{a^4}\right ) \right ) . $$ very near the line of charge , $z \ll l$ , we have $$ e = \frac 1{4\pi\epsilon_0} \frac{2\lambda l}{zl} \left ( 1-\frac12 \frac{z^2}{l^2} +\cdots \right ) \approx \frac 1{4\pi\epsilon_0} \frac{2\lambda}{z} $$ which is the field of an infinite cylinder ; very far from the line , $z\gg l$ , we have $$ e = \frac 1{4\pi\epsilon_0} \frac{2\lambda l}{z^2} \left ( 1-\frac12 \frac{l^2}{z^2} +\cdots \right ) \approx \frac 1{4\pi\epsilon_0} \frac{q}{z^2} $$ which is the field due to a point with charge $q=2\lambda l$ .
in a fission reactor , we can talk about the temperature of the fuel , and we can also talk about the temperature of the neutrons . temperature of the neutrons has abundant qualifiers on it . neutrons in a fission reactor are really in a transient state - moving from genesis to absorption again . they are generated at extremely high energies , and absorbed at much lower energies on average . we do not often measure the temperature of neutrons in the same units as the fuel . the fuel is in kelvin $k$ , whereas the neutrons are most often stated in electron-volts , most commonly $mev$ . going between the two is not a problem . there is no theoretical difficulty in applying the same relationship between particle energy and temperature as we do for gases ( $ {\tfrac {1}{2}}m\overline {v^{2}}={\tfrac {3}{2}}k_{{\mathrm {b}}}t $ ) . in fact , for light water reactors ( lwrs ) , the neutrons are in direct momentum exchange with protons that exist in the moderator . they are even of similar masses . in the lower part of the neutron energy spectrum , in fact , the neutrons are relatively well thermalized to the moderator temperature , but it depends on the reactor design . here 's one illustration of that : you can see that there is a " thermal flux " hump . this exists around the temperature of the reactor moderator . but we can not say it is the same . even though the protons ( in water ) and neutrons are about the same mass , the neutron energy distribution is shifted toward the higher energies , because they downscatter from higher energies to begin with , and they are preferentially absorbed by the fuel at lower energies . two mechanisms for temperature dependence in commercial lwrs , the reactivity is related to temperature via two major interplays : moderator temperature affects that above thermal peak of neutrons , and the location of the peak affects reactivity the fuel has absorption peaks between the fast and thermal energies , and the fuel temperature affects the efficiency of these peaks through a more complicated effect i should note that our reactors will come to equilibrium given enough time ( provided we build them stable ) . so if you change reactivity , then ultimately that will result in a change in the fission reaction rate . but practically , the operators control the reaction rate with engineered methods ( control rods ) , and that is set by regulatory concerns . so if you change the moderator temperature , you will change the reaction rate . as i have described , this is due to mechanisms that vaguely fit the intuition you had . changing temperature literally changes the temperature of the neutrons . this matters for the reaction because neutrons are absorbed by the fuel at lower energies much faster than at higher energies . things are a little bit more wonky than this , because a higher reaction rate produces more heat , which heats up the reactor more - creating a feedback loop . but we can still control the temperature of the moderator/coolant that we pump into the reactor . the role of fuel temperature is much more complicated . this is the resonance absorption peak doppler broadening effect . basically , there are lots of very very narrow absorption peaks in the epithermal range ( in the graph ) . as the fuel heats up , these " smear " out into larger energy ranges . with the peaks less sharp , there are fewer " windows " though the neutrons can sneak through the downscattering gauntlet . the doppler effect is actually the dominant effect for the reactor control . it is honestly the main thing that we rely on for keeping the reactors stable , since it is clearly a negative feedback , and it acts very quickly ( since most the the fission heat is deposited directly in the fuel ) . so let me conclude by saying that #1 has a lot of mechanistic qualities that resemble the intuition you had about temperature ( in the conventional sense ) affecting reaction rate . on the other hand , #2 is more important for operating reactors , and it is qualitatively quite differently from the fusion temperature and reaction rate interdependence .
i would like to create a planar ( rectangle shaped ) map of the entire ( both hemispheres ) sky , with stretching anything ( keeping all constellations etc . as seen in the sky ) . this is a physical impossibility . you simply cannot map a spherical entity ( the celestial sphere ) onto a plane without introducing some distortion . cartographers have developed many different projections in their efforts to solve this problem , but none is perfect . all of them are forced to introduce distortion at some point . the mercator projection suggested in another answer is notoriously inaccurate as you get closer to the poles , making greenland as large as south america , and stretching antarctica into an impossible shape . the best solution is to forget about mapping the whole sky , but to concentrate on smaller areas where the distortions are not as severe . this is what most star atlases and planetarium software programs do .
because you were also in orbit around the sun with the earth and still have that velocity . you may be imagining this in terms of stepping off of a slow moving vehicle on the earth : you jump off , you come to a stop relative the ground and watch the trolley car go it is merry way . but that is a feature of friction between you and the ground . there is no such thing as a absolute reference frame in the universe and when you " leave the earth " you do not come to stop relative anything so that you can watch the earth fly away . newton 's laws apply here : " a body in motion ( that is the you or the planet ) will continue in motion unless acted on by an external force " . you just keep going except for changed induced by your drive .
you conjecture is correct . one can relate the 2d ising model with the bond correlated percolation model . the details are in the paper percolation , clusters , and phase transitions in spin models . the basic idea is to consider interacting ( nearest neighbor ) spins as forming a bond with a certain probability . one can then show that the partition function of the ising model is related to the generating function of the bond-correlated percolation model . the above paper demonstrates that the bond-correlated percolation model has the same critical temperature and critical exponents as the 2d ising model . however , the values of $t_c$ and the critical exponents seem to be dependent on exactly how one defines a bond . see section iii . a . 1 in universality classes in nonequilibrium lattice systems ( or arxiv version ) . nonetheless your intuitive picture that there would be spanning clusters below $t_c$ and no such clusters above $t_c$ remains valid . edit 21 may 2012 i found a pedagogical paper that discusses this issue .
it is true that all even-even nuclei ( hundreds of such isotopes have been measured ) have spin-0 in the ground state . this is due to what is often called the pairing effect . protons and neutrons are spin-½ particles , and they have a tendency to respectively pair up in proton-proton and neutron-neutron pairs so that their spin ( and orbital ) angular momentum adds to zero . this is a pillar of the nuclear shell model which says that we can predict many properties of a nucleus by examining the " unpaired " nuclei . the spin and parity of odd-even and even-odd nuclei are generally determined by the " valence nucleon " ( cf . the valence electron in the atomic shell model ) that is left when all pairing has occurred . odd-odd nuclei are not commonly found in nature , which we can describe to its tendency to convert the odd proton into a neutron ( or vice versa ) via $\beta$ decay to gain binding energy through the pairing force . as to " why ? " , that is a larger question . this effect is an empirical observation in nuclear physics , and it is seen to be helpful in predicting how nuclei behave over a large range of isotopes . the " pairing term " is a part of the semi-empirical mass formula , which does a fairly good job of predicting nuclear properties , at least among heavier and stable nuclei . the links to the nuclear shell model and the semi-empirical mass formula are probably good further reads .
to keep with your for simplicity 's sake =p , let 's say the force exerted on the platform while the ball is in contact with the platform is always the average force ; that is , at no point in time while the ball is in contact with the surface is the force greater than any other point in time it is in contact with the surface . let 's also assume the surface to be 100% elastic ( elasticity coefficient of 1 ) , so that the ball would bounce exacty as high as when it began ( 100 meters ) . then when the ball leaves the surface , it must have a veloctiy $v = 63$ m$/$s . that means it is change in momentum $p = \delta v \cdot m$ and since $\delta v = 63 \cdot 2 = 126$ ( since it orginally was going down with 63 m$/$s and now is going up with 63 m$/$s ) then we have $p = 1260$ . now force $f_{avg} = \delta p/\delta t$ ( note only use this for a constant force , otherwise we need calculus ) . expanding , we get $f = m ( v_{f} - v_{i} ) /t$ which i believe is exactly what you have
this a bit of a sketch ; the $s$-matrix acts on shift the state or momentum state of a particle . a state with two particle states $|p , p’\rangle$ is acted upon by the $s$ matrix through the $t$ matrix $$ s~=~1~–~i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) t $$ so that $t|p , p’\rangle~\ne~= 0$ . for zero mass plane waves scatter at almost all energy . the hilbert space is then an infinite product of n-particle subspaces $h~=~\otimes_nh^n$ . as with all hilbert spaces there exists a unitary operator $u$ , often $u~=~exp ( iht ) $ , which transforms the states s acts upon . $u$ transforms n-particle states into n-particle states as tensor products . the unitary operator commutes with the $s$ matrix $$ sus^{-1}~=~ [ 1 – i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) t ] u [ 1~+~i ( 2π ) ^4 \delta^4 ( p~–~p’ ) t^\dagger ] $$$$ = u~+~i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) [ tu~–~ut^\dagger ] ~+~ [ ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) ] ^2 ( tut^\dagger ) . $$ by hermitian properties and unitarity it is not difficult to show the last two terms are zero and that the s-matrix commutes with the unitary matrix . the lorentz group then defines operator $p_\mu$ and $m_{\mu\nu}$ for momentum boosts and rotations . the $s$-matrix defines changes in momentum eigenstates , while the unitary operator is generated by a internal symmetries $a_a$ , where the index a is within some internal space ( the circle in the complex plane for example , and we then have with some $$ [ a_a , ~p_\mu ] ~=~ [ a_a , ~m_{\mu\nu} ] ~=~0 . $$ this is a sketch of the infamous “no-go” theorem of coleman and mundula . this is what prevents one from being able to place internal and external generators or symmetries on the same footing . the way around this problem is supersymmetry . the generators of the supergroup , or a graded lie algebra , have 1/2 commutator group elements $ [ a_a , ~a_b ] ~=~c_{ab}^ca_c$ ( $c_{ab}^c$ = structure constant of some lie algebra ) , plus another set of graded operators which obey $$ \{{\bar q}_a , q_b\}~=~\gamma^\mu_{ab}p_\mu , $$ which if one develops the susy algebra you find this is a loophole which allows for the intertwining of internal symmetries and spacetime generators . one might think of the above anti-commutator as saying the momentum operator , as a boundary operator $p_\mu~= -i\hbar\partial_\mu$ which has a cohomology , where it results from the application of a fermi-dirac operator $q_a$ . fermi-dirac states are such that only one particle can occupy a state , which has the topological content of $d^2~=~0$ . this cohomology is the basis for brst quantization .
if you do not already have an estimate of where you are pointing , the only other option i know of is wcsfixer . there also used to be the pittsbugh wcs correction service , but it seems to be defunct now . these tools only work with fits files , so your first step would be converting whatever format you have into a fits file . the fits website has a fits viewer page that also has a list of tools that can convert formats , although converting from fits to something else is more commonly supported . a note on terminology : wcs stands for " world coordinate system " , the standard used for metadata in fits files that lets software transform between pixel coordinates and sky coordinates . if you have good wcs values , tools such as ds9 will let you find the coordinates of objects in a fits image interactively . if you already have an idea where you are pointing , there are more options . the simplest conceptually is to find the field were you think you are in skyview or google sky and see if you can match up objects . if you can , any of the several professional data reduction toolchains have the tools that can be used to generate wcs metadata . for example , the iraf data reduction environment has a few different options . the learning curve is steep , but not insurmountable for someone computer savvy with some time . daophot and focas ( now incorporated into iraf ? ) are other options . if you already have an approximate wcs solution , there are several other tools that will help refine the solution . the astromatic .nettoolchain is one example . ( i believe astrometry .netuses astromatic .netsoftware " under the sheets , " while wcsfixer uses iraf . ) the algorithms that can determine pointing without an initial guess generally use triangles of stars in the field , because the angles in a triangle are the unchanged by rotation and scale . however , these algorithms can work poorly in highly distorted images . see campusano et al . 1994 and tabur 2007 for more details . another tool of interest is ds9 's catalog overlay feature , but its more useful for confirming that your wcs is correct than it is at doing something about it if you find that it is not .
as has been said , this is probably a very subjective question/answer . not only that , but the composition of galaxies , and even regions within a galaxy , varies a great deal . then there is the question of what constitutes as being part of the galaxy as opposed to perhaps a small orbiting dwarf galaxy . the answer you got from the quora seems to be pretty comprehensive . the volume of an area of interest , divided by the number of stars in that area seems to be the one that most people take as the approach . which may not get a very accurate result , but smoothed out over said volume . although , i will note that the first technique given on the quora site gives an answer that is close to the accepted " average " in the milky way , so at least there does not seem to be a large disagreement there . of course , that assumes that the same initial starting conditions are used in both problems , which is highly unlikely since they are not totally agreed upon anyway . edit to add : for more examples of similar math , here dr . plait calculates the number of habitable planets ( where he shows the calculation for the volume of the galaxy ) . making some assumptions of our own ( like 200,000,000,000 stars which is low in my opinion ) , we come out to an average distance of about 5 light years . doubling the number of stars gives an average of about 4 light years though , so again , we are not off by factors .
well , first off , you can see the andromeda galaxy with your naked eye from a dark place on a clear night . you should try it if you can . it looks like a smudge of appreciable size , not like a point . so half way between the milky way and andromeda , both would look like smudges twice as wide as andromeda does from earth . i think a standard point and shoot camera is slightly better than the naked eye , but not a whole lot . the other way to think about it is to consider the power of the lens used to take the picture . telescopes are usually between 10 and 100 power , although more is possible . a point and shoot is usually 1-3x i think . the naked eye is 1x of course . but long exposures " see " much more than the naked eye . so to get 10x you would need to be 90% of the way to andromeda so the remaing distance is 1/10th of the way to andromeda , to get 33x power equivalent , you would need to be 97% of the way to andromeda , etc . i am guessing you would see some structure at 10x , particularly with a long exposure from space , but to get the equivalent of the truly excellent pictures you see on the web or in astronomy magazines you might have to go to 33x or even 100x . for 100x you would have to go 99% of the way to andromeda . again , your point and shoot with a long exposure is probably three to ten times better than your naked eye .
as guillermo angeris correctly pointed out , this is essentially a numerical roundoff problem , not a physical situation . as a physical example , there are sungrazing comets that get very close to to sun , yet they maintain their original elliptical ( or hyperbolic ) orbit , without the orbit precessing a full third of a circle as you seem to be seeing . computationally , there are a few interesting issues . as kyle pointed out in a comment , many integration schemes are indeed unreliable in that roundoff error ( which is always present in floating-point computations ) can accumulate in a runaway feedback . indeed i often advise using leapfrog methods over euler ( used by box2d ) or even runge-kutta ( see for instance what is the correct way of integrating in astronomy simulations ? over at the computational science stackexchange ) . however , i suspect your problem is even simpler , in the sense that even an unstable numerical scheme should work for one or two orbits . given that everything is going wrong in just one pass , it seems that your timesteps are simply too large . a brief glimpse at the box2d documentation suggests you do not change the timestep mid-simulation , so i presume you are just using a good value to simulate the whole process in reasonable time . the problem is that when gravitating bodies get close in their orbit , they move quickly , sometimes very quickly . the way the code works is it updates each object 's position and velocity at each timestep , where the new velocity is determined by the force . as far as i can tell , this is done in line 206 of b2Island.cpp ( v . 2.2.1 ) : v += h * (b-&gt;m_gravityScale * gravity + b-&gt;m_invMass * b-&gt;m_force);  without looking at your code , i am guessing you simply calculate the gravitational force the body should feel at that moment , and have the simulation chug away . the problem is this moves the orbiting object in a straight line for the next timestep , and that straight line takes it too far away from the gravitating mass for that mass to properly curve its orbit into a closed ellipse . the quick schematic below shows the blue object moving to the tip of the red arrow , rather than staying on the path . physically , your timestep should be smaller than any timescale you encounter in the problem . now for an orbit conserving angular momentum , the product of the orbiting body 's mass , tangential velocity $v$ , and distance from the other object $r$ should be constant : $v \sim 1/r$ . at the same time , the acceleration $a$ it feels is given by newton 's law of gravity : $a \sim 1/r^2$ . so one natural timescale in this problem is $$ t \sim \frac{v}{a} \sim \frac{1/r}{1/r^2} \sim r $$ ( omitting dimensional constants ) , which goes to show that if your timescale is just barely small enough and then you tweak the orbit so as to half the periapsis distance ( distance of closest approach ) , then you would expect to need timesteps at least twice as small in order to preserve the integrity of the simulation .
because a constant vector ( like the translation vector ) is annihilated by the differential operator $\partial^\mu$ .
the state from $1\text{ atm}$ to $2\text{ atm}$ is normally called decompression or contraction . an equation you can use going from one state to the next is : $$\frac{p_1v_1}{t_1}=\frac{p_2v_2}{t_2}$$ where $p$ is pressure , $v$ is volume and $t$ is temperature . now if you want to calculate the force you have to know the surface area of what you are ( de ) compressing . the equation relating force and pressure is : $p=f/a$ , where $a$ is surface area . finally , keep in mind that you probably have to take into account outside pressure as well , because this has a substantial influence .
we are asking how the rod moves through the viscous medium if we apply a force to it . since the reynolds number is so low , inertial forces must be small , and the externally applied force must be balanced by a viscous force . also since we are in the low reynold 's number limit , the viscous force is linear in the velocity of the object . thus there must be a linear relationship between the force being applied to the object and the resulting velocity of the object . now we will determine some properties of the linear relationship using symmetries of the rod . let 's work in coordinates where the direction along the long axis of the rod is $\hat{n}$ . let 's call the other direction $\hat{y}$ . now let 's find what the coordinates of $\mu$ might be in the $n , y$ coordinate system . first consider pulling the rod so the force is directed along the long axis of the rod ( in the $\hat{n}$ direction ) . what will the resulting velocity be ? by symmetry , we know that the velocity cannot move in the $\hat{y}$ direction . thus we conclude the final velocity is in the $\hat{n}$ direction . then the final velocity in this case is given by $\mu_\parallel \hat{n}f$ . now consider pulling the rod perpendicular to the long axis . similar as before symmetry prevents the rod 's velocity to have a component along its length . thus the velocity must be purely in the $\hat{y}$ direction . let 's say the velocity is $\mu_\perp \hat{y}f$ . now if we have components of the force in both directions , we know by linearity that the resulting velocity must be $$\vec{v} = \mu_\parallel \hat{n}f_n+\mu_\perp \hat{y}f_y = ( \mu_\parallel \hat{n}\otimes \hat{n} +\mu_\perp \hat{y} \otimes \hat{y} ) \vec{f} . $$ this tells us that $\mu$ is the expression in paretheses : $$\mu = \mu_\parallel \hat{n}\otimes \hat{n} +\mu_\perp \hat{y} \otimes \hat{y} . $$ it is a diagonal tensor in this coordinate system . now that i have found $\mu$ i should show that it is the same as the expression you give in the question . we agree on the first term of $\mu$ , but the second term is different . it seems i have to show that $1-\hat{n}\otimes \hat{n} = \hat{y} \otimes \hat{y}$ , but this is true because , since $\hat{n}$ and $\hat{y}$ form an orthonormal basis , then by the resolution of identity formula we have $1=\hat{n}\otimes \hat{n}+\hat{y} \otimes \hat{y}$ , so our answers do agree .
although in theory we should all be using si units , for things that are very large or very small these units are an inconvenient size and it is common to invent new units that are more convenient . so , for example particle physicists measure mass in gev ( strictly speaking gev/$c^2$ ) and cosmologists measure distance in light years and/or parsecs . in the case of the hubble constant it has dimensions of $t^{-1}$ so the si unit would be $s^{-1}$ . the hubble time has units of $t$ so the si unit would be the second . however if we take the value for $h$ measured by planck , $67 \text{km} \space \text{s}^{-1}/\text{mpc}$ , and convert to units of per second the value is about $2.2 \times 10^{-18} \text{s}^{-1}$ , which is a lot harder to remember than the number $67$ . that is why cosmologists use those rather strange units . as long as all cosmologists use the same units it does not really matter what the units are . the hubble constant is not actually constant and will change in the future . exactly how it changes depends on the behaviour of dark energy , which is somewhat uncertain at the moment . response to comment : to convert the value of $h$ to $s^{-1}$: the units of $h$ are ( rearranging slightly ) $s^{-1}\text{km}/\text{mpc}$ . kilometers and megaparsecs are both units of length , so their ratio is a dimensionless number . if $n$ is the number of kilometers in a megaparsec then the ratio is just $1/n$ . google helpfully tells us that the number of km in an mpc is $3.09 \times 10^{19}$ , so to convert the hubble constant to units of per second just divide it by $3.09 \times 10^{19}$ .
yes . i feel like there should be more of an explanation , but it is pretty straightforward . a blackbody absorbing energy will increase in mass . the absolute amount of increase is pretty miniscule , but it is not zero . since you ask about an object that does not also radiate energy , a blackhole might be a decent analogy . so , does a blackhole increase in mass when photons fall into it ? sure . if it helps , you can imagine that a photon of sufficiently high energy can produce pairs of electrons ( or other particles ) that could subsequently fall into the blackhole . . . well , except the antiparticle that would be annihilated shortly after creation . either way , it seems easier to imagine the scenario with particles that have a rest mass because it more closely corresponds to our quotidian experience .
let $v^{\mu}=\frac{dx^\mu}{d\lambda}$ . then , $\epsilon=-v_\mu v^\mu$ . to see the conservation of this quantity along the geodesic we have to look at the covariant derivative along the curve , that is $$v^\nu\nabla_\nu\epsilon=-v^\nu\nabla_\nu ( v_\mu v^\mu ) =-v^\nu v^\mu\nabla_\nu v_\mu -v^\nu v_\mu\nabla_\nu v^\mu . $$ but metric compatibility implies that $\nabla_\nu v_\mu=g_{\mu \sigma}\nabla_\nu v^\sigma$ . also using the geodesic equation $v^\nu \nabla_\nu v^\mu=0$ , we find $$v^\nu\nabla_\nu\epsilon=0 , $$ which shows that $\epsilon$ is a conserved quantity .
i think lhs of eqn 2.7 is normalized , meaning $\frac{1}{z}\int\mathcal{d}\phi \mathcal{o} exp\left ( -s_{e}\left [ \phi\right ] \right ) $ evaluated on $\mathcal{m}_{n}$ if you put $\mathcal{o} =1$ , you get 1 . but $z$ itself is proportional to the correlation function of the two primary fields ref : http://arxiv.org/abs/hep-th/0405152 sect iiia hope this is useful
the trace defined as you did in the initial equation in your question is well defined , i.e. independent from the basis when the basis is orthonormal . otherwise that formula gives rise to a number which depends on the basis ( if non-orthonormal ) and does not has much interest in physics . if you want to use non-orthonormal bases , you should adopt a different definition involving the dual basis : if $\{\psi_n\}$ is a generic basis , its dual basis is defined as another basis $\{\phi_n\}$ with $\langle \phi_n|\psi_m\rangle = \delta_{nm}$ . in this case , the trace of $\rho$ , the same obtained with your formula for orthonormal basis ( in that case $\phi_n=\psi_n$ ) is : $$tr \rho = \sum_{n} \langle \phi_n | \rho \psi_n \rangle\: . $$ everything i wrote holds for finite dimension , otherwise some further requirements on operators have to hold true . addendum . ( i am still considering the finite dimensional case with dimension $n&gt ; 1$ . ) using your definition of trace , let us indicate it by $tr_c ( \rho ) $ , you can always find $\rho$ and a non-orthogonal basis $c$ with $tr_c ( \rho ) &gt ; tr ( \rho ) $ . as an example pick out $\rho = |\psi \rangle \langle \psi |$ with $||\psi||=1$ , so that $\rho$ is a pure state . next fix a cone $v$ tightly concentrated around $\psi$ . as every cone always includes a normalized basis , for every $\epsilon \in ( 0,1 ) $ , suitably concentrating $v$ around $\psi$ you can find a normalized basis $c:= \{\phi_n\}_{n=1 , \ldots , n} \subset v$ with $||\psi -\phi_n||^2 &gt ; 1-\epsilon$ . consequently : $$tr_c ( \rho ) = \sum_{n}|\langle \psi|\phi_n\rangle|^2 &gt ; n ( 1-\epsilon ) &gt ; 1 = tr ( \rho ) \: . $$ this is particularly evident for $n=2$ , using $\phi_1:= \psi$ and $\phi_2$ close to $\phi_1$ .
in the west the vast majority of towels are made from cotton , and cotton is basically cellulose . the surface of cellulose is fairly reactive ( the bulk is not unless you are a termite ! ) and will react with water to produce surface hydroxyl groups and negatively charged groups . both of these lower the contact angle of water on the fibres and hence increase capillary forces and wicking . towels from the factory with have been treated with materials not unlike fabric conditioner . this makes the towel feel nice and soft , but it make the surface more hydrophobic and therefore less able to absorb water by wicking . it takes a few uses for this surface treatment to wear off . you can do the experiment for yourself simply by using fabric conditioner when you wash the towel . if you compare two towels , one laundered with fabric conditioner and one not , then it will be immediately obvious that ( a ) the conditioned towel feels softer and nicer but ( b ) the unconditioned towel dries you better .
the idea is as follows : suppose we create ( for example ) a pair of electrons that fly off in opposite directions . because of conservation of momentum we know the momenta must be equal and opposite so if we measure the momentum of one of the particles we know the momentum of the other particle . likewise if we measure the position of one of the particles we can calculate the position of the other . so we wait until the particles are a long way apart like a light-year ( remember this is just a thought experiment ! ) and we measure the momentum of particle a to perfect accuracy ( so it is position is unknown ) and the position of particle b perfectly ( so it is momentum is unknown ) . so we know the momentum of particle a perfectly , but from our measurement of b 's position we also calculate the position of particle a perfectly . the end result is that we know both the momentum of particle a and it is position i.e. $$ \delta x_a \delta p_a = 0 $$ and this contradicts the heisenberg uncertainty principle . note that i said the two particles are a light-year apart . we specified this so any signal from particle a to b , or vice verse , would take at least a year to travel . this means as long as we do our measurements of a and b within a year the two measurements can not affect each other ( because information can not travel faster than light ) . the resolution of the paradox is that anything you do to an entangled system affects the whole system simultaneously i.e. there is no limitation due to the speed of light . you can not just do a measurement of particle a without affecting particle b even when they are many light years apart . any measurement you do is necessarily a measurement on the whole entangled system and you will always find that heisenberg 's uncertainty principle applies i.e. $$ \delta x_{ab} \delta p_{ab} \ge \frac{\hbar}{2} $$ this is unpalatable to lots of people because it suggests that quantum entanglement is necessarily non-local . lots of effort has been expended in trying to get round this e.g. hidden variable theories . however the evidence to date is that entanglement really is non-local .
the forces are never balanced , as there is only ever one force - gravity . the key is to remember newton 's second law : $f = ma$ . force and acceleration are paired , not force and velocity . knowing just an object 's current velocity tells you nothing about what forces are acting on it . there are two ways to see how the velocity goes to zero . either the initial impulse ( instantaneous transfer of momentum ) is depleted by a force applied over time , $$ m \delta v = \int f \ \mathrm{d}t , $$ or the initial kinetic energy is depleted by a force applied over a distance , $$ \frac{1}{2} m \delta ( v^2 ) = \int f \ \mathrm{d}x . $$ in both cases , the force of gravity is acting continuously to slowly cancel the initial velocity , and there is nothing that turns off this force at the apex of the trajectory .
i can not claim any experimental experience in this area ( fortunately :- ) but i thought it was interesting enough to be worth a bit of googling . the results suggest there is a difference between shells and bombs . there is an extensive collection of eye witness accounts of ww2 at http://www.bbc.co.uk/history/ww2peopleswar/categories/, and searching this suggests that falling bombs make little if any sound . i could not find any of the eye witness accounts that mentioned a whistling sound . however if you google for stories from , for example , the current troubles in syria there are lots of reports of the whistling sounds shells make . chapter 5 of the art of noises describes the stereotypical whistling sound falling in tone , and as this dates from the years before hollywood it is presumably relatively uncontaminated . the author attributes this to fact that the shell velocity is highest immediately after firing and falls during flight due to air resistance . it is probably relevant that shells are generally fired at greater than the speed of sound so you would not hear them approaching . you had only hear them after they passed you , and of course the sound of those shells would be red shifted .
this appears to be related to the decomposition of a totally symmetric tensor into traceless parts , which is a fairly involved process . the general equation is $$\mathcal{c} q_{a_1 a_2\cdots a_s} = \sum_{k=0}^{ [ \frac{s}{2} ] } ( -1 ) ^s \frac{\binom{s}{k} \binom{s}{2k}}{ \binom{2s}{2k}} \delta_{ ( a_1 a_2} \cdots \delta_{a_{2k-1} a_{2k}} q_{a_{2k+1}\cdots a_s ) }{}^{c_1} {}_{c_1} {}^{c_2}{}_{c_2} {}^{\cdots c_k}{}_{\cdots c_k} , $$ where $ [ \cdot ] $ denotes the integer part , einstein summation is implied and $q_{ ( a_1 a_2 \cdots a_s ) } \equiv \frac{1}{s ! } \sum_{\sigma\in s_s} q_{a_{\sigma ( 1 ) } a_{\sigma ( 2 ) } a_{\sigma ( 3 ) } \cdots a_{\sigma ( s ) }}$ . for the quadrupole moment it is $\mathcal{c}q_{ab} = q_{ab} - \frac{1}{3} q^c{}_c \delta_{ab}$ , for the octupole $\mathcal{c}q_{abc} = q_{abc} - \frac{1}{5} ( q^d{}_{dc}\delta_{ab} + q^d{}_{da} \delta_{bc}+ q^d{}_{db}\delta_{ac} ) $ ; these yield the factors in your question . an indication ( perhaps proof , although i am not certain about this at the moment ) that the traceless part of a totally symmetric tensor is an irreducible representation is easy to see if one uses the hook formula in dimension 3 . a totally symmetric tensor of rank $s$ has $\frac{1}{2} ( s+1 ) ( s+2 ) $ degrees of freedom and the traceless one has the latter minus the number of ways to obtain the traces , $\binom{s}{2}$ , which yields $2s+1$ . this is the dimension of the irreducible representation of the algebra of so ( 3 ) with spin $s$ . a full proof of this statement is in maggiore gravitational waves - theory and experiments . reference : f.a.e. pirani lectures on general relativity 1965 .
this is probably related to the derivation of de-broglie wavelength . . . since photon has wave-particle duality , we could equate planck 's quantum theory ( wave nature ) which gives the expression for energy of a wave of frequency $\nu$ , ( $e=h\nu$ ) with einstein 's mass-energy equivalence ( particle nature ) which gives relativistic energy for photon ( $e=mc^2$ ) $$mc^2=h\nu$$ the resultant mass gives the relativistic mass for a moving photon ( since photon has zero rest mass )
what a great question ! and because anything that involves food is close to my heart i can answer with authority having done the experiments :- ) there is a simple answer , a more complex answer and even an unexpected answer ! the simple answer is that if you just want to boil off water you should leave the lid off . if you try the experiment of putting a known mass of water in the pan on a steady heat and periodically measuring it , you quickly show water is lost more rapidly with the lid off . the complex answer is that the temperature of the saucepan is roughly constant ( at 100c ) , so the energy going in is mainly used to boil water . with a bit of effort you could even estimate the power input from the rate of water loss and the latent heat of vaporisation . anyhow , with the lid off or on the water is turned to steam at roughly the same rate . however the lid acts as a cooler and the steam condenses on it is underside and drips back into the pan . that is why the water loss is slower when the lid is on . without the lid the steam escapes from the pan . the unexpected answer is that you are not necessarily thickening a bolognese sauce by removing the water . all meat contains some connective tissue , which at 100c is slowly hydrolysed by water into gelatin like molecules , and this thickens the sauce . in particular it gives it that rich texture you associate with meals like osso buco . it takes one to two hours to hydrolyse connective tissue , so if you cook the sauce for an hour or two with the lid on you should still find it thickens up . how much it thickens depends on the meat used . paradoxically the cheaper forms of meat can be better for this as they contain more connective tissue .
the lasing mode ( stimulated emission ) may have nothing to do with the direction of the pump laser . for instance , flashlamp pumped lasers are pumped from the side , e.g. a ruby laser . stimulated emission occurs in the same direction as the stimulating photons -- that refers to another photon in the laser mode , not in the pump . this begs the question of how the lasing gets started . we usually say it is started by ' vacuum ' , or the quantum fluctuations of the field , which get exponentially amplified once the process gets going . so , in general , there is no preference between clockwise and counterclockwise modes in a ring cavity , and so the system can be unstable by switching between the modes . in most lasers , people take great pains to make sure the system has a preferred mode . ring lasers need a way to break the symmetry , which is usually an optical diode ( a faraday isolator ) . as an interesting note , another way to break the symmetry is to have a non-planar ring oscillator and polarization optics ( it is quite a clever solution ! ) .
in your question , i see 3 different context , where considering gravitational forces : a ) 2 point-like objects b ) 1 point-like object and one extended spherical symmetric object ( not too dense ) c ) a auto-gravitating extended spherical symmetric object ( not too dense ) a ) if you take 2 point-like objects , and take the limit $r \rightarrow 0$ , in fact , at some value of $r &gt ; 0$ , you create a black hole , because the ratio $\frac{energy}{radius}$ cannot excess a constant value $\sim \frac{1}{g}$ ( in $c=1$ units ) . note that mass is a kind of energy . so you do not have a problem with $r=0$ , because you create a black hole before . b ) if you consider a problem of a point-like object and a extended spherical symmetric object like earth ( not too dense ) , a theorem states that a object at distance $r$ only feels the gravitational force of masses inside the sphere of radius $r$ . that is , for instance , if the point-like object is inside the earth at radius $r &lt ; r_{earth}$ , it feels only the gravitational force of masses inside the sphere of radius $r$ . if we suppose a constant density $$\rho = \frac{m_{earth}}{4/3 \pi r_{earth}^3}$$ , then the force will be $$ f ( r ) = \frac {g m m ( r ) }{r^2} = \frac {g m ( \rho ~4/3 \pi r^3 ) }{r^2}$$ so , you have a linear force : $$f ( r ) \sim r$$ so , when $r\rightarrow 0$ , nothing bad , about gravitation , appears . ( of course , temperature and pression increase very much . . . ) if the spherical object is very dense , it is an other story , because you have a black hole , and you may have a " singularity": it is thought that something very bad happens to objects reaching the singularity ( tidal forces , roasted , etc . . ) . but you are here in the context of general relativity . c ) the last problem is an auto-gravitating extended spherical symmetric object . i will just give this reference of corse , as usual , if the object is too dense , you need general relativity , black holes , etc . . .
the fact that the radiation will fall off at $\frac{1}{r}$ will break the set of conditions required for the enveloping metric to stay asymptotically flat i am not sure this is right . there are various definitions of asymptotic flatness . older definitions were written in terms of coordinates , newer ones in terms of conformal transformations . the original motivation , as described in ch . 11 of wald , was to accomplish for gr what had already been done for e and m . in e and m in sr , the coordinate-based requirements given by wald are that the fields fall off like $1/r^2$ at $i^0$ , but only like $1/r$ at $\mathscr{i}^+$ . this is clearly designed to allow radiation . the definition of asymptotic flatness in wald is actually framed in a pretty restrictive context . he first gives a definition that is purely for a vacuum spacetime ( not electrovac ) , and then remarks that the definition carries over automatically to a spacetime in which there is a vacuum in some open neighborhood of the boundary . obviously it should be possible to extend this to a case in which the matter fields fall off fast enough , but it looks like he just wants to avoid making the already technical discussion even more technical . but the definition of asymptotic flatness for vacuum spacetimes definitely allows for spacetimes with gravitational radiation , since the adm energy , which is only defined in asymptotically flat spacetimes , includes the energy of gravitational radiation at null infinity . ( this could probably be checked explicitly by power-counting . for an asymptotically flat spacetime , the metric differs from minkowski by $o ( 1/v ) $ , where $v$ is an affine parameter defined in the lightlike direction . ) as further confirmation that these spacetimes with hawking radiation are asymptotically flat , you can find penrose diagrams for them . for example , there is one in figure 2.41 in penrose , cycles of time .
since op 's problem looks like a homework assignment , we will only provide op with a series of hints rather than a full solution . i ) the lagrangian $l~=~\int \ ! dx ~{\cal l}$ is the spatial integral of the lagrangian density ${\cal l}$ . let us call the position field $q ( x , t ) $ and the corresponding velocity field $v ( x , t ) $ . the lagrangian $l [ q ( \cdot , t ) , v ( \cdot , t ) ] $ is a functional of the position and the velocity fields , cf . e.g. this phys . se answer . the momentum field is the functional derivative $$ \tag{1} p ( x , t ) ~:=~\frac{\delta l [ q ( \cdot , t ) , v ( \cdot , t ) ] }{\delta v ( x , t ) } . $$ an infinitesimal variation $\delta l$ of the lagrangian functional $l$ is given by $$\tag{2} \delta l~=~\int \ ! dx~\left ( \frac{\delta l}{\delta q}\delta q+\frac{\delta l}{\delta v}\delta v \right ) . $$ the exercise basically asks op to derive a functional version of noether 's theorem , which he probably knows for ordinary point mechanics . ii ) let $$\tag{3} \delta q~=~\varepsilon y$$ be an infinitesimal variation of the position field , where $\varepsilon$ is an infinitesimal constant , and where $y$ is the generator . correspondingly , the velocity field transforms as $$\tag{4}\delta v~=~\varepsilon \dot{y} . $$ ( the transformation ( 3 ) is a so-called vertical transformation . in general , one could also allow horizontal contributions from variation of $x$ and $t$ . ) iii ) assume that the transformation ( 2 ) is a so-called quasi-symmetry $$\tag{5} \delta\left ( \left . l \right|_{v=\dot{q}}\right ) ~=~ \varepsilon\frac{d}{dt} \left ( \left . f\right|_{v=\dot{q}}\right ) , $$ where $f [ q ( \cdot , t ) , v ( \cdot , t ) ] $ is some functional . ( if $f=0$ is zero , then the quasi-symmetry becomes a symmetry of the lagrangian $l$ . ) iv ) the ( temporal component of the ) bare noether current $j^0$ for a vertical transformation ( 3 ) is simply the momentum field $p$ times the vertical generator $y$ , $$\tag{6} j^0 ~:=~p y . $$ the bare noether charge $q^0$ is the spatial integral $$\tag{7} q^0 ~:=~ \int \ ! dx~j^0 . $$ the full noether charge $q$ is improved with ( minus ) the $f$-functional , $$\tag{8} q ~:=~q^0-f . $$ formula ( 8 ) corresponds to op 's last formula ( v1 ) . noether 's theorem states that $q$ is a conserved quantity on-shell , i.e. if the equation of motion $$\tag{9} \left . \frac{d}{dt} \left ( \frac{\delta l}{\delta v}\right|_{v=\dot{q}}\right ) ~=~\left . \frac{\delta l}{\delta q}\right|_{v=\dot{q}} $$ is satisfied . the proof in the functional setting is very similar to the usual proof given in ordinary point mechanics .
aqwis , it would help in the future if you mentioned something about your background because it helps to know what level to aim at in the answer . i will assume you know e and m at an undergraduate level . if you do not then some of this explanation probably will not make much sense . part one goes back to dirac . in e and m we need to specify a vector potential $a_\mu$ . classically the electric and magnetic fields suffice , but when quantum mechanics is included you need $a_\mu$ . the vector potential is only defined up to gauge transformations $a_\mu \rightarrow g ( x ) ( a_\mu + \frac{i}{e} \partial_\mu ) g^{-1} ( x ) $ where $g ( x ) =\exp ( i \alpha ( x ) ) $ . the group involved in these gauge transformations is the real line ( that is the space of possible values of $\alpha$ ) if electric charge is not quantized , but if charge is quantized , as all evidence points to experimentally , then the group is compact , that is it is topologically a circle , $s^1$ . so to specify a gauge field we specify an element of $s^1$ at every point in spacetime . now suppose we do not know for sure what goes on inside some region ( because we do not know physics at short distances ) . surround this region with a sphere . we can define our gauge transformation at every point outside this region , but now we have to specify it on two-spheres which cannot be contracted to a point . at a fixed radial distance the total space of angles plus the gauge transformation can be a simple product , $s^2 \times s^1$ but it turns out there are other possibilities . in particular you can make what is called a principal fibre bundle where the $s^1$ twists in a certain way as you move around the $s^2$ . these are characterized by an integer $n$ , and a short calculation which you can find various places in the literature shows that the integer $n$ is nothing but the magnetic monopole charge of the configuration you have defined . so charge quantization leads to the ability to define configurations which are magnetic monopoles . so far there is no guarantee that there are finite energy objects which correspond to these fields . to figure out if they are finite energy we need to know what goes on all the way down to the origin inside our region . part two is that in essentially all models that try to unify the standard model you find that there are in fact magnetic monopoles of finite energy . in grand unified theories this goes back to work of ' t hooft and polyakov . it also turns out to be true in kaluza-klein theory and in string theory . so there are three compelling reasons to expect that magnetic monopoles exist . the first is the beauty of a deep symmetry of maxwell 's equations called electric-magnetic duality , the second is that electric charge appears to be quantized experimentally and this allows you to define configurations with quantized magnetic monopole charge , and the third is that when you look into the interior of these objects in essentially all unified theories you find that the monopoles have finite energy .
key is to notice that your steps provide you with a unit length as well as a unit time . so , let 's measure distance in $steps$ and time in $ticks$ , with your speed being $1 \ step/tick$ . the length of the train is $x$ steps , and its speed is $v \ steps/tick$ ( $v&lt ; 1$ ) . it follows that $$x \ + \ 18 \ v \ = \ 18 $$ $$x \ - \ 11 \ v \ = \ 11 $$ adding 11x the first equation to 18x the second yields $29 x = 396$ . the train is $396/29 \ steps$ long . you also need to check if indeed $v &lt ; 1 \ step/tick$ . leave that to you to demonstrate .
actually what you have to transform to define a symmetry for a classical or quantum system are the dynamical variables describing the system and appearing in the action rather than the metric ( moreover time reversal could need a further complex conjugation ) . in any cases here you are thinking of a discrete symmetries . noether theorem instead implies the existence of dynamically conserved quantities provided the symmetries of the action are continuous : there is a dynamically conserved quantity for each continuous ( differentiable actually ) one-parameter group of symmetries of the action . passing to quantum systems ( fields in particular ) , dynamically conserved quantities may arise also for discrete symmetries , provided they are described by simultaneously unitary and self-adjoint operators . parity operator can be taken of this type , but time reversal one cannot ( if the hamiltonian is bounded below as is physically necessary for the stability of the system ) , as it is an anti unitary operator ( there are the only two possibilities permitted by kadison-wigner theorem ) .
i think there is a temperature gradient in the water . the other candidate is that the density of water changes due to its compressibility under pressure . let 's examine the pressure effect first . from an estimate its size , we can see whether it is a significant factor compared to temperature gradients . the bulk modulus of water is about $2*10^9 pa$ ( wikipedia source ) . the pressure in a column of water is $\rho g h$ with $\rho$ the density , $g$ gravitational acceleration , and $h$ the height of the column . for a column of water half a meter tall , this comes to about $5*10^3 pa$ . this means the water at the density of water at the bottom is slightly greater , and the size of the effect is about $2*10^{-6} \rho$ compare this to the effect of temperature . water 's density varies by about $7*10^{-5} \rho$ for a $1 c$ change in temperature at room temperature ( wikipedia source ) . that means that the density gradient due to pressure and compressibility is equivalent to a temperature difference of about $ . 03 c$ . $ . 03 c$ is tiny - only . 01% of the temperature . there must be temperature fluctuations larger than . 01% in a thermometer that is just sitting in your room , so the baubles float mid-way due to a temperature gradient in the water , as you originally suggested . as for whether the baubles will separate - yes , i think they will , but i do not have a galilean thermometer to test this on . if you have got one and some good ideas on temperature control , make the experiment !
i think you are exactly right . in fact the method you used of imagining a second moving car is exactly what you would do if you were trying to find the pressure as a function of position in the original problem with the wall . the technique is called the method of images .
the transformation law for time intervals is given by $$c\delta t'=\gamma ( c\delta t-\beta\delta x ) , $$ with $\gamma=1/\sqrt{1-\beta^2}$ and $\beta=v/c . $ this coincides with your second formula only if $\delta x=0$ .
yes , it is ok , but it is an explanation that has been stripped down to bare bones , and leaves out quite a bit . here 's a little more to help prop up the explanation . first , it is important to realize that in a condensed phase like a solid or liquid the light is not interacting with molecules in isolation . light is interacting with all of the molecules . this makes a big difference . light absorbed and re-emitted by a single molecule can go off in ( almost ) any direction . next , remember that a photon is an excitation of a complete electromagnetic field . it is , unfortunately , often not helpful to think of a photon as a particle that exists at a particular place in space . like all particles in quantum mechanics , there is a chance that it could exist anywhere . individual interactions , on the other hand , can occur with a particular molecule at a particular location . it is best to start thinking about your question in the realm of classical physics , and then modify it later to include quantum mechanics . classically , when light interacts with a molecule , the electron is set into vibrational motion . the energy that the field gave to the molecule will stay with the molecule for a while while this oscillation occurs . the molecule , then , is a radiator and can generate its own light . that is the classical picture of the delay experienced by light during an interaction . after this re-emission , the light will travel at $c$ . how does it know to go straight ? here 's where we need all the other molecules in the liquid . the incident field has a particular spatial pattern , say a sine pattern , and it excites in the molecules an oscillation pattern that exactly matches . the incident light was traveling in a particular direction with a well-defined phase front , and so too does the pattern of oscillation in the liquid . the light re-emitted from the molecules will also share that same pattern , although it will be delayed in time relative to the incident light . the re-emitted light adds to the portion of the incident light that passes unaffected . but the phase fronts , and hence the direction of travel , is parallel to that of the incident wave . it goes off in the same direction . photons : almost the same story . recall that a photon is an excitation of the light field . instead of exciting the molecules into oscillation , the molecule temporarily absorbs a quantum of energy from the field ( the " photon" ) , and the molecule is raised to an excited state . the molecule lives in this state for a short period of time , and then the energy is returned to the field ( "photon emission" ) . but the business about phase fronts and directions still holds . the energy is added to a field propagating in the original direction , slightly delayed . ( another detail we are leaving out . the absorption and re-emission i describe is a very fast process called " virtual transition " . it is fast enough that the uncertainty principle $\delta e \delta t \leq \hbar/2$ allows energy conservation to be temporarily violated . so the frequency of the incident light does not have to match an absorption frequency of the molecule in this process , and the explanation works for transparent media . ) note carefully that i refer to a photon as an excitation of the field , rather than as a particle . when the interaction between light and something else occurs at a particular place , as when it hits a particular pixel in a digital camera , it looks for all the world like a particle hit the pixel . but a more useful way of thinking about it is that the light field exists everywhere , but the interaction occurs at a particular place . hope that helps !
note that : \begin{equation} \begin{aligned} \langle j | a \rangle and = \langle j |\left ( \sum\limits_{i} a_i |i \rangle \right ) \\ and = \langle j| \left ( a_1 |1 \rangle + a_2 |2 \rangle + a_3 |3 \rangle + \cdots + a_j |j \rangle + \cdots \right ) \\ and = a_1 \underbrace{\langle j | 1 \rangle}_{=\delta_{j1} = 0} + a_2 \underbrace{\langle j | 2 \rangle}_{=\delta_{j2} = 0} + a_3 \underbrace{\langle j | 3 \rangle}_{=\delta_{j3} = 0} + \cdots + a_j \underbrace{\langle j | j \rangle}_{=\delta_{jj} = 1} + \cdots \\ and = a_j \end{aligned} \end{equation}
an answer that professor lenny susskind gave to a non-physicist audience at stanford in june 2012 went along these lines ( by memory and some very short notes i took ) : the charge on an electron is$\ \alpha \ \approx \ 1/137\ $which means that 99% of the electron is just the bare electron while about 1% of the time it is an electron plus a virtual photon . whereas the charge on a magnetic monopole would be $1/\alpha \ \approx \ 137$ so the magnetic monopole would have about 100 constituents on average - like lots of photons , current etc . thus the magnetic monopole would be a composite particle and very heavy due to all the strong fields and constituents . do not blame any errors on lenny , it could be my mistaken memory/notes . i think that the dimensionless number $\alpha$ is a reasonable stand-in for electric charge since it is the coupling constant used to calculate connections between electrons and photons . similarly , $1/\alpha$ would be the coupling between magnetic monopoles and photons .
one approach is to start from $t=\frac12 m [ \dot{x}^2+\dot{y}^2+\dot{z}^2 ] $ and use $x=\rho\cos\theta$ , $y=\rho\sin\theta$ so that , e.g. $\dot{x}=\dot{\rho}\cos\theta-\rho\dot{\theta}\sin\theta$ . combine terms and simplify . or , for the plane polar part , using $\frac{d\hat{\boldsymbol{\rho}}}{dt} =\dot{\theta}\hat{\boldsymbol{\theta}} $ ( consider the change in $\hat{\boldsymbol{\rho}}$ with small $dt$ and compare to the change in $\theta$ ) , we have $\dot{\boldsymbol{\rho}}=\dot{\rho}\hat{\boldsymbol{\rho}}+\rho \frac{d\hat{\boldsymbol{\rho}}}{dt} =\dot{\rho}\hat{\boldsymbol{\rho}}+\rho \dot{\theta}\hat{\boldsymbol{\theta}} $ and use the orthogonality of $\hat{\boldsymbol{\rho}}$ and $\hat{\boldsymbol{\theta}}$ after squaring . an alternative is to use $t=\frac12 m \left ( \frac{ds}{dt}\right ) ^2 = \frac12 m g_{ij}\dot{q}^i \dot{q}^j$ where the metric $q_{ij}$ is diagonal with $q_{\rho\rho}=q_{zz}=1$ and $q_{\theta\theta}=\rho^2$ .
i am a student so please point out in gory detail anything i did wrong . for a process to be quasistatic , the time scales of evolving the system should be larger than the relaxation time . relaxation time is the time needed for the system to return to equilibrium . we have an adiabatic process , so equilibrium must be preserved at each point , that is to say ( working within the validity of the kinetic theory for ideal gases and ignoring friction ) $ ( a l ( t ) ) ^\gamma p ( t ) = ( a l_0 ) ^\gamma p ( t_0 ) $ momentum gained by the piston : $\delta p = 2 m v_x$ a molecule would impact the piston every $\delta t = \frac{2 ( l_0+ \delta x ) }{v_x}$ the force exerted on the piston is $f =\frac{\delta p}{\delta t} = \frac{m v_x^2}{l_0+\delta x}$ pressure would be $p = \frac{p}{a}$ and for $n$ such molecules $p = \frac{n m &lt ; v_x&gt ; ^2}{a ( l_0+\delta x ) } = \frac{n m &lt ; v&gt ; ^2}{3a ( l_0+\delta x ) }$ so at the instant $t=t&#39 ; $ where the piston has been displaced by $\delta x$ , we have $ ( a l ( t ) ) ^\gamma p ( t ) = \frac{n m &lt ; v&gt ; ^2}{3a^{1-\gamma}} ( l_0+\delta x ) ^{\gamma -1}$ expanding in series $ = \frac{n m &lt ; v&gt ; ^2 l_0^{\gamma-1}}{3a^{1-\gamma}} ( 1 + \frac{ ( \gamma-1 ) \delta x}{l_0}+ o ( \delta x^2 ) ) $ substituing $\frac{\delta x}{l_0} = \frac{\delta t v_x}{2 l_0} -1$ $ ( a l_0 ) ^\gamma p ( t_0 ) = ( a l_0 ) ^\gamma p ( t_0 ) ( 1 + ( \gamma-1 ) ( \frac{\delta t v_x}{2 l_0} -1 ) ) $ if we want our process to be reversibly adiabitic atleast to first order , we must have from above $\delta t = \frac{2 l_0}{&lt ; v_x&gt ; }$ now , this is time until collision for the starting case . investigating second orders $ ( a l_0 ) ^\gamma p ( t_0 ) = ( a l_0 ) ^\gamma p ( t_0 ) ( 1 + \frac{ ( \gamma-1 ) \delta x}{l_0}+ \frac{1}{2} ( \gamma-1 ) ( \gamma-2 ) ( \frac{\delta x}{l_0} ) ^2 +o ( \delta x^3 ) ) $ looking at just the series terms $ 1 + ( \gamma-1 ) \frac{\delta x}{l_0} ( 1 +\frac{1}{2} ( \gamma -2 ) \frac{\delta x}{l_0} ) \approx 1$ this would be true for $\delta t = \frac{4 l_0}{&lt ; v_x&gt ; } ( \frac{1}{2-\gamma} -1 ) $ now , this is the " time until next collision " for a gas molecule hitting the piston . to maintain reversibility , at least to second order , the piston should be moved from $l_0$ to $l_0 + \delta x$ in time $\tau = \delta t$ so that the system variables follow the adiabatic curve . the $&lt ; v_x&gt ; $ can be calculated from the maxwell distribution
the gas temperature is the same as the temperature of the reservoir this is exactly why there is energy exchange . you should think in small steps : the volume is expanding , but the change occurs slowly enough to allow the system to continually adjust to the temperature of the reservoir through heat exchange . see isothermal process for more details .
what you wrote down is the quotient of the 2-dimensional translation group by a discrete subgroup . but by far not every closed manifold arises as a quotient of groups this way . one should be aware that the term " compactification " in physics is used not so much to refer to what in mathematics is called compactification of non-compact spaces . for instace one-point compactification in the mathematical sense turns the real line line into the circle . ( however it also turns the plane into the 2-sphere , not into the torus . ) instead , what is meant by " compactification " in physics is that you just choose a closed ( and hence compact ) manifold $q$ , then choose spacetime $x$ to be a $q$-fiber bundle over space base space ( often assumed to be just a product $x = q \times y$ ) , and then describe the kaluza-klein mechanism for passing from physics on $x = q \times y$ to effective physics on just $y$ . in particular for calabi-yau " compactifications " you just choose $q$ to be a calabi-yau manifold , and then consider the kaluza-klein mechanism on spacetimes which are $q$-fiber bundles . you do not actually obtain these spacetimes as compactifications of non-compact spacetimes in the sense of mathematics . ( well one could consider that problem , but this is not what is generally meant by " compactification " in physics . )
i know this is an old question , but for the benefit of people visiting here wondering what the answer was , here it goes : a droplet can stay at rest on an inclined plate because of small heterogeneities on the surface . this can either be a small roughness ( of the order of nano/micrometers ) or `dirty ' spots where the surface chemistry is locally different . the existence of these heterogeneities allows the droplet to have a different , bigger , contact angle at the downhill side then at the uphill side ( this means that a perfectly smooth , clean surface will not be able to hold any droplets , this can most easily be shown by numerical simulation , because a perfectly smooth and clean surface is very hard to find/make experimentally ) . this difference in contact angle , thus surface energy , translates into a force pointing upwards , which is therefore able to counteract the gravitational force . because there is a certain maximum to the front and rear contact angles ( which depends on the surface roughness/dirtyness , higher roughness/dirtyness will give a larger difference between the two ) the droplet will at some point start to move , at which point you get into the paradoxical discussion that ron maimon was talking about . the balance that you get looks like this : $$ \rho v g \sin \alpha = k \gamma w ( \cos\theta_u - \cos \theta_d ) $$ where , in order of appearance , you have the density of the liquid , the volume of the droplet , the gravitational acceleration , the sin of the tilt angle , a `fudge ' factor of o ( 1 ) depending on the detailed shape of the droplet , the width of the base of the droplet and the cosines of the uphill contact angle and the downhill angle . the phenomenon of the difference in downhill and uphill contact angle is called contact angle hysteresis . if you want to know more about it , you can google for it : there are plenty of good sources out there . to summarize : local heterogeneities on the surface allow a difference in surface energy at the front and rear of the droplet , thus introducing a force that can counteract gravity .
i am not sure if i know the correct answer ( as i am a student my self ) , but i will try ( and if i am wrong , someone please correct me ) . the first thing that took me some time to figure out is what they mean by adjoint representation . in georgi 's book he defines the adjoint representation of a generator as : \begin{equation} [ t_i ] _{jk} \equiv -if_{ijk} \end{equation} which is equivalent to the adjoint representation of a lie algebra . however , when discussing monopole , they actually mean the adjoint representation of a lie group . this means that $\phi$ takes values in the lie algebra ( the vector space formed by the generators ) and can be expressed in terms of the generators in an arbitrary representation : \begin{equation} \phi = \phi^a t^a \end{equation} where $t^a$ denote the generators in an arbitrary representation ( and there is an implicit sum over repeated indices ) . now , let us look at the simplest example , which is the bosonic part of the $\mathrm{su ( 2 ) }$ gauge invariant georgi-glashow model : \begin{equation} \mathcal{l}=\frac{1}{8} \mathrm{tr} ( f_{\mu \nu} f^{\mu \nu} ) - \frac{1}{4} \mathrm{tr} ( d_\mu \phi d^\mu \phi ) - \frac{\lambda}{4} ( 1-\phi^a \phi^a ) ^2 \end{equation} we can write the kinetic and potential energy , $t$ and $v$ , as : \begin{equation} t=\int \left ( - \frac{1}{4} \mathrm{tr} ( f_{0i}f_{0i} ) - \frac{1}{4} \mathrm{tr} ( d_0 \phi d_0 \phi ) \right ) \mathrm{d^3}x \end{equation} and : \begin{equation} v=\int \left ( - \frac{1}{8} \mathrm{tr} ( f_{ij}f_{ij} ) - \frac{1}{4} \mathrm{tr} ( d_i \phi d_i \phi ) + \frac{\lambda}{4} ( 1-\phi^a \phi^a ) ^2 \right ) \mathrm{d^3}x \end{equation} where we used $l= \int \mathcal{l} \ ; \mathrm{d^3}x = t-v$ . in order to get finite energy solutions we have to impose boundary conditions such that the total energy of the model vanished at spatial infinity . it should be clear that one of the requirements to ensure that the energy vanishes is : \begin{equation} \phi^a \phi^a =1 \end{equation} this implies that the higgs vacuum corresponds to an infinite amount of degenerate vacuum values lying on the surface of a unit two-sphere in field space , which we will denote by $s^2_1$ . furthermore , by imposing the aforementioned finite energy boundary condition , this gives rise to the following map : \begin{equation} \phi : s^2_\infty \mapsto s^2_1 \end{equation} where $s^2_\infty$ denotes the two-sphere associated with spatial infinity ( in 3 dimensions ) . this is in fact the definition of the winding number ( or degree ) between two two-dimensional spheres and is therefore classified by $\pi_2 ( s^2 ) =\mathbb{z}$ ( and it is in theory possible to construct topological solitons ) . now , if $\phi$ was in the fundamental representation , then i do not think it is possible to construct these topological solitons .
but the magnetic field of the electron cannot couple to its own spin ? or can it ? how do i explain the energy spit in this reference frame ? in classical em theory , the common explanation of the ls term from the frame of the electron is not very convincing , because this frame is non-inertial and there are potentially all kinds of non-inertial forces which were not discussed . explaining this in the frame of nucleus seems as an easier task . if we imagine charged rotating and orbiting ball in a central electric field of the nucleus , this central electric field affects the orbital motion of the ball ; the ball accelerates , which changes its own electric and magnetic field . these fields of the ball then could influence the rotation of the ball . i do not know if it gives something close to the ls term , but it seems possible .
your error comes in when you change the sign of $d\vec{x}$ to $-d\vec{x}$ in part 2 . the differential element $d\vec{x}$ always points in the direction from the lower bound of the integral ( $x_2$ in this case ) to the upper bound ( $x_1$ ) . since you switched the bounds of the integral in part 2 , you already switch the direction . by including a negative sign , you end up switching the direction again and thus have a integral equivalent to that of part 1 ) . below is an explicity calculation done in regards to some sign confusion when calculating work . let us first consider the work done by gravity ( $\vec{f}=-mg\hat{x}$ ) on an object moving from the point $\vec{x_1}=x_1\hat{x}$ to $\vec{x_2}=x_2\hat{x}$ where $x_2&gt ; x_1$ . this is $w_1 = \int_{x_1}^{x_2} \vec{f} \cdot d\vec{x} = \int_{x_1}^{x_2} f ( -\hat{x} ) \cdot ( \hat{x} ) dx = -f \int_{x_1}^{x_2} dx = -f ( x_2 - x_1 ) $ where $f=mg$ thus we see $w_1$ is negative , which makes physical sense since the work done by gravity as the object moves up counteracts the motion of the object . now , let us consider the work done as the object moves down . this is $w_2 = \int_{x_1}^{x_2} \vec{f} \cdot d\vec{x} = \int_{x_1}^{x_2} f ( -\hat{x} ) \cdot ( -\hat{x} ) dx = f \int_{x_1}^{x_2} dx = f ( x_2 - x_1 ) $ note that here the differential element $d\vec{x}$ is now equal to $dx ( -\hat{x} ) $ ( i.e. . it is pointing down ) . also note , that i could also flip my integral bounds such that they go from $x_2$ to $x_1$ , but in this case the sign of the differential element would be negative and thus the direction of the differential element would be positive ( i.e. . $d\vec{x} = -dx ( -\hat{x} ) = dx\hat{x}$ ) . from this we see that $w_2$ is positive , again making physical sense since the work done by gravity is now in the direction of the object 's motion . adding together $w_1$ and $w_2$ we see that the total work $w=w_1 + w_2$ done by gravity on the object is zero .
the radius of a non-rotating black hole is $$r_s = \frac{2gm}{c^2} \tag{1}$$ where $m$ is the mass , $g$ is newton 's constant , and $c$ is the speed of light . this is the distance from the center of the black hole to the event horizon . the event horizon is the surface that traps light and objects , it separates inside and outside the black hole . anything that passes inside the event horizon can never escape , even light . this is why it is said that the escape velocity at the surface of a black hole is $c$ . it is also the case that any object with $r &lt ; r_s$ is a black hole . but since the mass should be roughly proportional to the volume , and the volume is proportional to $r^3$ , anything heavy enough will form a black hole . therefore from ( 1 ) the proper answer to your question is : because they are very massive , not because they are small .
i can not say i know any that do it . a small refrigerator modified with clear panels and post who tip acts as the seed-point for nucleation - this is the basic concept of the lab setup a for ice crystal growth . i think your objectives need to be stated better . " large " could be an imaginative thing wherein a real implementation will demand you grow on a substrate . this then creates its own problems regarding purity of the crystal and morphology . i am on mobile , so pardon the lack of references .
you state that the source produces a pulse per second . a hum , on the other hand , is a sound with a frequency of several hundreds of cycles per second . if that source is particularly violent it may be able to get a sound going in a very good resonator . ( example : i have a guitar , and i have noticed that when i sneeze and the guitar happens to lie right next to me sometimes one or two strings are audibly triggered . ) anyway , to have a chance the resonator must be one that is very responsive . that is , the resonator must be shaped ideally for one particular frequency , so that all of the energy that it does aborb goes to that one frequency . the cube that you describe is definitely a very poor resonator . i think you would have a hard time getting the sound from any source to resonate in it at all . for comparison , bottles tend to have a narrow resonance response . you blow over the top of an open bottle , and when you hit the right angle of blowing just over it , and with the right airflow , you get a resonance in that bottle . hum that same note to the bottle and your humming gets amplified . that amplification shows the resonator 's responsiveness . summerising , to have a chance of producing some effect you need a resonator that is very , very responsive .
i am not sure what your level is , but i think you will find that material on accelerator structures generally requires upper division e and m . however in terms of concepts , this overview and these pictures ( from illinois tech ) will go a long way toward answering the questions you have lined out . in a drift-tube linac , the drift tubes sit inside a larger rf cavity and act as shields . thus , the electric field inside a drift-tube can be approximated as zero . the gaps between drift tubes will still have an e-field , however , which varies sinusoidally in the direction of travel . when the particle is in the gap between drift tubes , it will experience an e-field and undergo acceleration . let 's say we are accelerating protons . we want to accelerate them in a particular direction , but half the time , the e-field in the gap is pointing in the wrong direction . hence we can not make a continuous beam with this particular technology ; we have to break the beam up into " bunches " . this is by-and-large a limitation of all rf accelerating structures . a truly comprehensive reference for linacs is the slac " blue book " , which is now freely available from slac . it is somewhat dated , but covers all aspects of building a linear accelerator in great detail . there are also a good number of slac publications that deal with linacs , such as slac-pub-7802 . you may also consult material presented by the us particle accelerator school . finally , there is the rf linac textbook by wangler .
you must first rewrite the old partial derivatives in terms of the new ones . a priori , they are some linear combinations with coefficients that could depend on the spacetime coordinates in general but here they do not depend because the transformation is linear . the rules $$ t'=t , \quad x'=x-vt , \quad y'=y $$ get translated to $$ \frac{\partial}{\partial t} = \frac{\partial}{\partial t'} - v \frac{\partial}{\partial x'}$$ $$ \frac{\partial}{\partial x} = \frac{\partial}{\partial x'}$$ $$ \frac{\partial}{\partial y} = \frac{\partial}{\partial y'}$$ if you write the coefficients in front of the right-hand-side primed derivatives as a matrix , it is the same matrix as the original matrix of derivatives $\partial x'_i/\partial x_j$ . if you do not want to work with matrices , just verify that all the expressions of the type $\partial x/\partial t$ are what they should be if you rewrite these derivatives using the three displayed equations and if you use the obvious partial derivatives $\partial y'/\partial t'$ etc . if you simply rewrite the ( second ) derivatives with respect to the unprimed coordinates in terms of the ( second ) derivatives with respect to the primed coordinates , you will get your second , galilean-transformed form of the equation . i have verified it works – up to the possible error in the sign of $v$ which only affects the sign of the term with the mixed $xt$ second derivative . i guess that if this explanation will not be enough , you should re-ask this question on the math forum .
from the dutch national science quiz 2006 ( my translation ) : question 14: you put a duvet cover together with smaller laundry in the washing machine . why , at the end of the program , all smaller laundry has twisted itself in the duvet cover ? due to the left-and-right cycling of the drum washing machines predominantly cycle one way . to loosen the laundry , the drum sometimes abruptly turns the other way . due to this opposite movement , suddenly a few liters of water bumps very forcefully into the laundry , and therefore the opening of the duvet cover will come to lie completely open . smaller laundry falls in one piece at a time . as soon as the machine goes into centrifuge mode , the smaller laundry pieces are being pushed in further . it is very difficult for laundry that is in the duvet cover to get out . nb : the false answers were ( my translation ) : because small laundry pieces are more sensitive to water vortices than large items due to the area ( size ) difference between the duvet cover and smaller laundry i would like to add , from personal experience , that it is very rare that all smaller laundry ends up in the cover . sometimes half of it will get in there , and sometimes just the odd sock . but perhaps that is just me and my machine .
we should probably distinguish between a particle being " point-like " and a particle being " structure-less " . in classical mechanics we talk of " point-like " particles , objects with no extension . it is the case that in general relativity any " point-like " mass would be inside of its event horizon and so would be a black hole . in quantum-mechanics even a " structure-less " particle - a particle with no consitituent parts - is wave-like and has extension , though not a fixed size , and it can never be come exactly point-like since that would take an infinite amount of energy . i do not believe it to be the case , therefore , that quantum-mechanically all particles are black holes in any sense .
short answer the information is contained in the heat given off by erasing the information . landauer ' principle states that erasing information in a computation , being a thermodynamically irreversible process , must give off heat proportional to the amount of information erased in order to satisfy the second law of thermodynamics . the emitted information is hopelessly scrambled though and recovering the original information is impossible in practice . scrambling of information is what increasing entropy really means in plain english . charles h . bennett and rolf landauer developed the theory of thermodynamics of computation . the main results are presented in the thermodynamics of computation—a review . background erasure of information and the associated irreversibility are macroscopic/thermodynamic phenomena . at the microscopic level everything is reversible and all information is always preserved , at least according to the currently accepted physical theories , though this has been questioned by notable people such as penrose and i think also by prigogine . reversibility of basic physical laws follows from liouville 's_theorem for classical mechanics and unitarity of the time evolution operator for quantum mechanics . reversibility implies the conservation of information since time reversal can then reconstruct any seemingly lost information in a reversible system . the apparent conflict between macroscopic irreversibility and microscopic reversibilty is known as loschmidt 's paradox , though it is not actually a paradox . in my understanding it is sensitivity to initial conditions , the butterfly effect , that reconciles macroscopic irreversibility with microscopic reversibility . suppose time reverses while you are scrambling an egg . the egg should then just unscramble like in a film running backwards . however , the slightest perturbation , say by hitting a single molecule with a photon , will start a chain reaction as that molecule will collide with different molecules than it otherwise would have . those will in turn have different interactions then they otherwise would have and so on . the trajectory of the perturbed system will diverge exponentially from the original time reversed trajectory . at the macroscopic level the unscrambing will initially continue , but a region of rescrambling will start to grow from where the photon struck and swallow the whole system leaving a completely scrambled egg . this shows that time reversed states of non-equilibrium systems are statistically very special , their trajectories are extremely unstable and impossible to prepare in practice . the slightest perturbation of a time reversed non-equilibrium system causes the second law of thermodynamics to kick back in . the above thought experiment also illustrates the boltzmann brain paradox in that it makes it seem that a partially scrambled egg is more likely to arise form the spontaneous unscrambling of a completely scrambled egg than by breaking an intact one , since if trajectories leading to an intact egg in the future are extremely unstable , then by reversibility , so must trajectories originating from one in the past . therefore the vast majority of possible past histories leading to a partially scrambled state must do so via spontaneous unscrambling . this problem is not yet satisfactorily resolved , particularly its cosmological implications , as can be seen by searching arxiv and google scholar . nothing in this depends on any non classical effects .
the centre of mass is the point at which our collection of objects will balance if we put a pivot there . let 's call this point $\bf r$ . the vector joining the point $i$ to $\bf r$ is simply $\bf r_i - \bf r$ . the force acting at this point is $m_i \bf g$ , so the torque at the point $i$ is : $$ \bf t_i = m_i \bf g \times ( \bf r_i - \bf r ) $$ the total torque must sum to zero , because that is how we define the centre of mass , so : $$ \sum m_i \bf g \times ( \bf r_i - \bf r ) = 0$$ and the cross product is distributive over addition so we can take it outside the sum : $$ \bf g \times \sum m_i \bf ( \bf r_i - \bf r ) = 0$$ and this can ony be satisfied if : $$ \sum m_i \bf ( \bf r_i - \bf r ) = 0$$
i would like to know how exactly the equations of motion in the lorenz gauge removes the second degree of freedom . in the lorenz ' gauge ' , we have $$\box a^{\mu} = \mu_0j^{\mu}$$ if $a^{\mu}$ is a solution , then so is $a^{\mu} + n\epsilon^{\mu}e^{-ik\cdot x}$ if $$\box ( n\epsilon^{\mu}e^{-ik\cdot x} ) = 0$$ consistency with the lorenz condition $$\partial_{\mu}a^{\mu}=0$$ requires $$k \cdot \epsilon = 0 $$ and consistency with the equation of motion requires that $$k^2 = k \cdot k = 0$$ but this means that if a polarization four-vector $\epsilon$ satisfies the condition $$k \cdot \epsilon = 0$$ then $\epsilon ' = \epsilon + \alpha k$ also satisfies this condition $$k \cdot \epsilon ' = k \cdot ( \epsilon + \alpha k ) = k \cdot \epsilon + \alpha k^2 = 0$$ this means we can choose an $\epsilon^{\mu}$ such that $\epsilon^0 = 0$ and then the lorenz condition implies that the wave and polarization 3-vectors are orthogonal $$\vec k \cdot \vec\epsilon = 0$$ so there are only two independent polarization vectors ( for freely propagating wave solutions ) . to summarize , the lorenz condition implies that the wave and polarization four-vectors are ( minkowski ) orthogonal leaving three polarization degrees of freedom . the equation of motion implies that the wave four-vector is null . since null-vectors are self-orthogonal ( $k^2 = 0$ ) , we are left with two physical polarization degrees of freedom .
i ) concerning an action principle $s=\int\ ! dt~ l$ , let us assume that the lagrangian is of the form $$\tag{1} l~=~t-u , $$ where $t$ is the kinetic term , and $u ( {\bf r} , \dot{\bf r} , \ddot{\bf r} , \dddot{\bf r} , \ldots ; t ) $ is a generalized potential , which we would like to find . the generalized potential $u$ should satisfy $$\tag{2} {\bf f}~=~-\frac{\partial u}{\partial {\bf r}} +\frac{d}{dt}\frac{\partial u}{\partial \dot{\bf r}} -\frac{d^2}{dt^2}\frac{\partial u}{\partial \ddot{\bf r}} +\frac{d^3}{dt^3}\frac{\partial u}{\partial \dddot{\bf r}} - \ldots , $$ where ${\bf f} ( {\bf r} , \dot{\bf r} , \ddot{\bf r} , \dddot{\bf r} , \ldots ; t ) $ is a given total force on the point particle . ii ) let us for fun consider a force proportional to the $n$'th time-derivative of the position $$\tag{3} {\bf f}~=~-k \frac{d^n{\bf r}}{dt^n} $$ for any non-negative integer $n\in\mathbb{n}_0$ . for an even integer $n$ , we can use the generalized potential $$\tag{4} u~=~ ( -1 ) ^{\frac{n}{2}}\frac{k}{2} \left ( \frac{d^{\frac{n}{2}}{\bf r}}{dt^{\frac{n}{2}}} \right ) ^2 . $$ the case $n=0$ of a force proportional to the position $$\tag{5} {\bf f}~=~-k {\bf r} , \qquad u ~=~\frac{k}{2}{\bf r}^2 , \qquad k~&gt ; ~0 , $$ is the well-known hooke 's law/harmonic oscillator . the case $n=2$ of an applied force proportional to the acceleration $$\tag{6} {\bf f}~=~-k \ddot{\bf r} , \qquad u ~=~-\frac{k}{2}\dot{\bf r}^2 , \qquad $$ behaves like a ( non-relativistic ) kinetic term . the case $n=1$ of a friction force proportional to the velocity $$\tag{7} {\bf f}~=~-k \dot{\bf r} , \qquad k~&gt ; ~0 , $$ is discussed in e.g. this phys . se post and this mathoverflow post . more generally , using very similar methods as in these two posts , one may show that it is impossible to assign a generalized potential $u$ to the force ( 3 ) for any odd positive integer $n$ . so in particular , the case $n=3$ , the abraham-lorentz force $^1$ proportional to the jerk $$\tag{8} {\bf f}~=~-k \dddot{\bf r}\qquad k~&lt ; ~0 , $$ does not have a generalized potential $u$ . -- $^1$ however , see also this related phys . se post .
no . the principle of the aircraft – why it does not fall - is the aerodynamic force , the so-called lift . it depends on the existence and substantial density of the surrounding air . however , 100 km is in the ionosphere . the ionosphere starts about 85 km above the ground and the density over there is about $10^{-10}$ kilograms per cubic meter ( that is because the density and pressure are decreasing pretty much exponentially with the altitude : the boltzmann distribution ) http://www.wolframalpha.com/input/?i=density+of+air+in+ionosphere it is about 10 billion times smaller than the density near the earth 's surface . to compensate this decrease , you may be forced to install 10 billion extra wings . it is simply not possible . all man-made machines at this altitude – above the normal parts of the atmosphere – are powered by thrust , like rockets . the very-low-density air in the ionosphere may still sometimes cause some problems , friction , but it is not high enough to be helpful for flying .
there is no such thing as " abelian anyonic commutation relations " , in the sense that the " abelian anyonic commutation relations " that you write down does not describe abelian anyons . so the starting point of the question is not valid . also anyons do not have a fock space description . the standard many-body text books stress on fock space too much , which lead people to think about many-body systems only in terms of fock space . such foack-space picture can only describe a very small subset of many-body states . most many-body states ( the interesting ones ) require a new picture ( such as tensor network ) to visualize .
the area under the curve in the pv-diagram is the integral $$ \int p \ ; \mathrm dv = \int \frac fa a\ ; \mathrm ds=\int f \ ; \mathrm ds \equiv w $$ by definition of pressure as force per area and ( infinitesimal ) volume as area times distance . this is the mechanical work done by the system on the environment in case of expansion or by the environment on the system in case of compression , which differ by sign . it is called external to emphasize the interaction with the environment .
verified it with fea , it is correct . also , taking into account the " rocking " effect requires the piece-wise description : $$ j\ddot{\theta} + \bigg\lbrace\begin{matrix} -m g r \sin ( \alpha + \theta ) and \theta &lt ; 0 \\ m g r \sin ( \alpha - \theta ) and \theta \geq 0 \end{matrix} = m a r \cos ( \alpha - \theta ) $$
unfortunately , this is not a good idea . the beltrami-laplacian $\delta_{g}$ is in local coordinates given as $$ \delta_{g}\psi ~=~ \frac{1}{\sqrt{g}}\frac{\partial}{\partial x^i}\left ( \sqrt{g}~ g^{ij} \frac{\partial \psi}{\partial x^j} \right ) . $$ a nice feature of $\delta_{g}$ is that it takes a scalar $\psi$ into a scalar $\delta_{g}\psi$ . most importantly , a decomposition $$\delta_{g}\psi~=~g^{ij}\frac{\partial^2 \psi}{\partial x^i\partial x^j} + \frac{1}{2} g^{ij} \left ( \frac{\partial \ln g}{\partial x^i}\right ) \frac{\partial \psi}{\partial x^j} $$ would not contain a zero-order term that could be identified with a potential $v$ , but rather a first-order term . finally , let me mention that for a generic metric $g_{ij}=g_{ij} ( x ) $ on a 3-dimensional riemannian manifold $ ( m , g ) $ ( rather than just curvilinear coordinates for $\mathbb{r}^3$ ) , it might not be possible to find flat coordinates in an open neighborhood , as also discussed here ( in the context of gr ) .
this is a heavy question , that contains many topics in it that are worthy of their own questions , so i am not going to give a complete answer . i am relying mainly on this excellent review paper by nayak , simon , stern , freedman and das sarma . the first part can be skipped by anyone already familiar with anyons . abelian and non-abelian anyons anyons are emergent quasiparticles in two dimensional systems that have exchange statistics which are neither fermionic nor bosonic . a system that contains anyonic quasiparticles has a ground state that is separated by a gap from the rest of the spectrum . we can move the quasiparticles around adiabatically , and as long as the energy we put in the system is lower than the gap we will not excite it and it will remain in the ground state . this is partly why we say the system is topologically protected by the gap . the simpler case is when the system contains abelian anyons , in which case the ground state is non-degenerate ( i.e. . one dimensional ) . when two quasiparticles are adiabatically exchanged we know the system cannot leave the ground state , so the only thing that can happen is that the ground state wavefunction is multiplied by a phase $e^{i \theta}$ . if these were just fermions or bosons than we would have $\theta=\pi$ or $\theta=0$ respectively , but for anyons $\theta$ can have other values . the more interesting case is non-abelian anyons where the ground state is degenerate ( so it is in fact a ground space ) . in this case the exchange of quasiparticles can have a more complicated effect on the ground space than just a phase , most generally such an exchange applies a unitary matrix $u$ on the ground space ( the name ' non-abelian ' comes from the fact that these matrices do not in general commute with each other ) . the quantum dimension so we know that the ground space of a system with non-abelian anyons is degenerate , but what can we say about its dimension ? we expect that the more quasiparticles we have in the system , the larger the dimension will be . indeed it turns out that for $m$ quasiparticles , the dimension of the ground space for large $m$ is roughly $\sim d_a^{m-2}$ where $d_a$ is a number that depends on $a$ - the type of the quasiparticles in the system . this scaling law is reminiscent of the scaling of the dimension of a tensor product of multiple hilbert spaces of dimension $d_a$ , and for this reason $d_a$ is called the quantum dimension of a quasiparticle of type $a$ . you can think of it as the asymptotic degeneracy per particle . for abelian anyons we have a one-dimensional ground space no matter how many quasiparticles are in the system , so for them $d_a=1$ . although we used the analogy to a tensor product of hilbert spaces , note that in that case the dimension of each hilbert space is an integer , while the quantum dimension is in general not an integer . this is an important property of non-abelian anyons that differentiates them from just a set of particles with local hilbert spaces - the ground space of non-abelian anyons is highly nonlocal . more details on anyons and the quantum dimension can be found in the review paper cited above . the quantum dimension can be generalized to other systems with topological properties , maintaining the same intuitive meaning of asymptotic degeneracy per particle . it is in general very hard to calculate the quantum dimension , and there is only a handful of papers that do ( most of them cited in the paper by kitaev and preskill that inspired this question ) . relation to entanglement i can also try and give a handwaving argument for why the quantum dimension would be related to entanglement . first of all , the fact that the entanglement entropy of a bounded region depends only on the length of the boundary $l$ and not on the area of the region is very clearly explained in this paper by srednicki , which is also cited by kitaev and preskill . basically it says that the entanglement entropy can be calculated by tracing out the bounded region , or by tracing out everything outside the bounded region , and the two approaches will yield the same result . this means the entanglement has to depend only on features that both regions have in common , and this rules out the area of the regions and leaves only the boundary between them . now for a system with no topological order the entanglement would go to zero when the size of bounded region goes to zero . however for a topological system there is intrinsic entanglement in the ground space which yields the constant term $-\gamma$ in the entanglement . the maximal entanglement entropy a system with dimension $d$ has with its environment is $\log d$ , so in an analogous manner the topological entanglement is $\gamma=\log d$ where $d$ is the quantum dimension . again this last argument relies heavily on handwaving so if anyone can improve it please do . i hope this answers at least the main concerns in the question , and i welcome any criticism .
jerk_dadt is correct . electric current is the flow of free electrons in the conductor . at any instant , the number of electrons leaving the wire is always equal to the number of electrons flowing from the battery into it . hence , the net charge on the wire is zero . if you say the current carrying conductor is charged , it will violate the kirchoff junction rule , which is based on the fact that in an electric circuit , a point can neither act as a source of charge , nor can the charge accumulate at that point .
as the other answers ( and dmckee 's comments ) note , yes , if you take the square root of a dimensional quantity then you need to take the square root of the units too : $$ \sqrt{4\ ; {\rm kg}} = 2\ ; {\rm kg}^{\frac12} $$ and no , i can not think of any meaningful physical interpretation for the unit ${\rm kg}^{\frac12}$ either . however , in the comments you say that you were " told to plot a graph of distance against square root of mass . " what that means is simply that you should scale the mass axis non-linearly , presumably in order to more clearly show the relationship between the two quantities . for labeling the mass axis , you basically have two choices : label the axis $\sqrt m$ , with equally spaced ticks at , say , $1\ ; {\rm kg}^{\frac12} , 2\ ; {\rm kg}^{\frac12} , 3\ ; {\rm kg}^{\frac12} , 4\ ; {\rm kg}^{\frac12} , \dotsc$ , or label the axis $m$ , with equally spaced ticks at $1\ ; {\rm kg} , 4\ ; {\rm kg} , 9\ ; {\rm kg} , 16\ ; {\rm kg} , \dotsc$ . while , technically , both of these are valid , i would strongly recommend the latter option . just compare these two plots and see which one you find easier to read : $\hspace{60px}$ alas , not all plotting software necessarily supports such axis labeling , or at least does not make it easy , which is why you sometimes see plots with funny units like ${\rm kg}^{\frac12}$ .
plenty ! the cuo$_2$ planes of cuprate superconductors are perhaps the most famous example , but in fact a lot of layered oxides are going to coordinate in this fashion . the lieb lattice is essentially a two-dimensional counterpart of the " perovskite " structure , which is ubiquitous in nature .
it is not very clear to me if you are asking about energy or momentum . you should also ask about a specific interaction process as there are many , this is required especially to answer your last , quantitative , question . however , generally speaking , a $\gamma$ photon cannot give some of its energy to anything else : it is all or nothing . even in the compton scattering , in which you get a less energetic photon , the initial photon is destroyed . the momentum must be conserved as well , so yes : when a photon hits another particle this is accelerated , you can even generate some measurable pressure with a very intense radiation !
as you say , the location of the pole gives the mass . higher-order diagrams shift the location of the pole , causing a ( often infinite ! ) renormalization of the mass . the residue at the pole location simply gives the normalization of the wave function , which , as far as i remember , is just absorbed into scale of the field .
according to wikipedia the magnetic field is indeed the result of feedback . actually the wikipedia article is very good so i am not sure how much there is left to say . the convection currents from the inner core outwards get bent onto spirals by the coriolis effect of earth 's rotation , and this gives a geometry where the magnetic field and electric currents sustain each other . re luboš' comment , i would have a google around the nasa web site as they have loads of data about pretty much everything to do with the earth e.g. http://science.nasa.gov/science-news/science-at-nasa/2003/29dec_magneticfield/ is an article aimed at the general public . there is bound to be raw data on the site somewhere . the wikipedia article mentions how hard it is to numerically model the magnetic field generation in the core . there have been a couple of really quite alarming experiments in the last decade trying to model the core . for example see http://www.nature.com/news/dynamo-maker-ready-to-roll-1.9582 - if 13 tons of liquid sodium is not alarming i do not know what is :- ) see http://physicsworld.com/cws/article/news/2007/mar/09/molten-sodium-mimics-earths-magnetic-field-flipping for an earlier experiment that claims to have modelled the field reversals . for an excellent popular introduction to this see the bbc horizon programme called " the core " . this is on youtube , though i am not sure that is an official upload so how long the programme will stay there i do not know .
dear dbrane , $\lambda_{\rm qcd}$ is the only dimensionful parameter of pure qcd ( pure means without extra matter ) . it is dimensionful and replaces the dimensionless parameter $g_{\rm qcd}$ , the qcd coupling constant . the process in which a dimensionless constant such as $g$ is replaced by a dimensionful one such as $\lambda$ is called the dimensional transmutation : http://en.wikipedia.org/wiki/dimensional_transmutation the constant $g$ is not quite constant but it depends on the characteristic energy scale of the processes - essentially logarithmically . morally speaking , $$ \frac{1}{g^2 ( e ) } = \frac{1}{g^2 ( e_0 ) } + k \cdot \ln ( e/e_0 ) , $$ at least in the leading approximation . because $g$ depends on the scale , it is pretty much true that every value of $g$ is realized for some value of the energy scale $e$ . instead of talking about the values of $g$ for many specific values of $e$ , one may talk about the value of $e$ where $g$ gets as big as one or so , and this value of $e$ is known as $\lambda_{\rm qcd}$ although one must be a bit more careful to define it so that it is 150 mev and not twice as much , for example . yes , it is the characteristic scale of confinement and all other typical processes of pure qcd - those that do not depend on the current quark masses etc . in most sentences about the qcd scale , including your quote , the detailed numerical constant is not too important and the sentences are valid as order-of-magnitude estimates . however , given a proper definition , the exact value of $\lambda_{\rm qcd}$ may be experimentally determined . with this knowledge and given the known lagrangian of qcd - and the methods to calculate its quantum effects - one may reconstruct the full function $g ( e ) $ .
isotones are nuclides having the same number of neutrons . magic proton or neutron numbers give the nucleus greater stability . magic 82-isotone nuclides for instance : isobars are nuclides having the same mass number ( i.e. . sum of protons plus neutrons ) . the number of protons in beta-plus ( beta-minus ) decay decreases ( increases ) by a unit and the number of neutrons increases ( decreases ) by a unit , so that an isobar standing to the left ( right ) of the original nucleus is formed . it may be 1 , 2 or 3 beta-decay stable isobars . beta-decay energy of 154-isobar nuclides for instance : isotones and isobars have great significance for studying of nuclide stability .
the electron charge is called $-e$ ; let me pick the convention where $e$ is positive . the atoms ( e . g . the hydrogen-1 atom ) contain the same number of protons as electrons and they are neutral . they must be neutral because ordinary macroscopic matter is composed of atoms and it has to be neutral because the atoms would otherwise attract the oppositely charged objects and create neutral composites , anyway . it follows that the protons have to have the charge opposite to the electrons , $+e$ . we also know that there are electrically neutral particles inside the nuclei , neutrons , that only impact the nuclear physics , not the atomic physics ( chemical properties ) . they have the charge $0$ . in the late 1960s and early 1970s , people learned that each proton is composed ot three quarks and so is each neutron . one needs two quark types ( "flavors" ) , $u$ and $d$ . proton has $uud$ and neutron has $udd$ . if $u , d$ denote the charges of the quarks for a while , we have the equations for the total charges of these blocks $$ 2u+d = +e , \quad u+2d = 0$$ the second implies $u=-2d$ . substitute it to the first one and you get $$ -4d+d = +e , \quad -3d=+e , \quad d= -\frac e3$$ and $u=+2e/3$ for the charges of the quarks . the electron 's $-e$ is therefore 50% larger than $-2e/3$ , the charge of the up antiquark ( "anti": note that " up " and " electron " have opposite signs of the charge ) . if one wants to choose a " fundamental charge " based on quarks , the " elementary charge " should be the minimum one , $e/3$ , and not $2e/3$ . then the electron has 200% higher charge than the sub-electron elementary charge ( of the down-quark ) . but quarks are confined and there are other reasons , although partly flavored with conventions , why people use the term " elementary charge " for $e$ and not $e/3$ .
well , your lecturer certainly should not have put it like this , however it is true that you have got a lot wrong here . it is stuff you definitely will need to understand better if you are studying power engineering . first , you seem to think that electrons are attracted by magnetic north poles . they are not ; in fact stationary charges and magnetic fields are not concerned with each other in any way at all 1 ! next , you are talking about electrons in circular orbits about the nucleus . that is roughly the bohr model , which kind-of-sort-of-works , but not really . you want to familiarise yourself to the orbital model , which describes very well how bound electrons actually behave . even in an orbital , you might be inclined to talk about " the nucleus is off the center by a distance proportional to the voltage " . that is again kind-of-sort-of-right since the nucleus lies in a locally-harmonic potential which can be read as " pertubation by an electric field ( which in a fixed capacitor is proportional to the voltage ) will cause a proportional displacement of the nucleus " , but the way you phrase it it is still nonsense . voltage " is " not a distance , it is a potential ( i.e. . energy ) . anyway , this is not actually relevant to understanding rotating-magnet phenomena , i.e. inductance in coils . these are concerned only with conduction electrons , which are not bound to any particular atom at all but " move " through the entire conductor , which is why there can be currents . it is these moving electrons that experience a significant force in the presence of a magnetic field . what current actually is is the number and " speed " with which these electrons move through the conductor , while even a strong displacement of the bound ( valence ) electrons would not consolidate a current 2 . now , all of this seems to say there is not any such thing as inductance . sure there is ! only , it is rather more complicated : electrons at rest are not affected by stationary magnetic fields , but in the same way that moving electrons are affected by such fields , moving magnetic fields ( or , more generally , time-varying magnetic fields ) also cause a lorentz force upon resting electrons . so , effectively , what you are saying about electrons being moved around by moving magnetic fields is not all that wrong again , it only works quite a bit differently . a moving magnetic field will in fact " push resting conductance electrons " through a wire a bit , i.e. induce a voltage . but that voltage really can not be read as anything displacement-like , it is a fundamental electrodynamic phenomenon . in fact , the voltage in its pure , exact value can only be measured if you prevent the conductance electrons from moving , as otherwise they would themselves cause a magnetic field cancelling the inductance etc . pp . . as you see , the whole subject is quite a bit more complicated than you thought . i am sure you are capable of understanding it , but probably not in a few minutes , which is why your lecturer can not really be blamed for not trying to explain it right away . 1 actually , electrons are also small magnets themselves ( they have an instrisic quantum-mechanical spin ) and therefore are attracted to inhomogenic magnetic fields , but that is quite another issue . 2 actually , it would . . . but that is mostly relevant in the high-frequency-regime , i.e. bound electrons that jiggle back and forth very quickly .
i will start this with right hand grip rule for solenoids . . . " the coil ( solenoid ) is held in the right hand so that the fingers point the direction of current through the windings . then , the extended thumb points the direction of magnetic field " . ( which would be along the axis of the coil ) the higher the current , the more the magnetic field would be produced . . . for your example , let us assume the aluminium ring as a circular coil . when the uniform magnetic field is produced , there is a change in magnetic flux ( such as this increase in magnetic field ) along the axis of the ring , according to faraday 's law , induced current flows through the ring whose direction is given by lenz 's law . this induced current in the ring flows in a direction such that it opposes the magnetic field in the solenoid ( the one which actually produces it ) . ( but , the magnitude of induced magnetic field is always lesser than the field in the solenoid ) . anyways , there is a repulsion . with the maximum repulsive force produced , the ring is thrown off from the solenoid . this force always depends on the magnitude of $b$ in the solenoid .
for any quantum system with more than one coordinate - which may be a particle on more than one dimension , or several particles - the position kets need to be updated from the single-parameter $|x\rangle$ to accomodate the multiple coordinates $x_1 , \ldots , x_n$ , and are usually written $|x_1 , \ldots , x_n\rangle$ . the inner product you are asking about is simply the wavefunction corresponding to the quantum state $|\psi\rangle$ in the ( two-particle ) position representation : $$\langle x_1 , x_2|\psi\rangle=\psi ( x_1 , x_2 ) . $$ of course , it is a complex-valued function of its two arguments . the pure kets are in fact tensor products : $|x_1 , x_2\rangle=|x_1\rangle\otimes|x_2\rangle$ . this is how multi-dimensional qm is usually built up : each degree of freedom has its hilbert space $\mathcal{h}_j$ , and the total hilbert space is the tensor product $\mathcal{h}=\bigotimes_j \mathcal{h}_j$ . this is perhaps best understood as the space of all ( tensor ) product wavefunctions , of the form $$\psi ( x_1 , x_2 ) =\psi_1 ( x_1 ) \cdot\psi_2 ( x_2 ) , $$ and all linear combinations of such , most of which cannot be written in that form , in which case the different degrees of freedom are called entangled . ( consider , for example , $\psi ( x_1 , x_2 ) =n ( x_1\cdot 1+1\cdot x_2 ) $ . )
you need to know how big one sigma is , for your gaussian distribution . it could be narrow , or it could be wide . you get that by calculating the variance , and taking its square root . then just divide the difference between the two values by sigma . more info here .
in order for the intensity of a light source to stay the same , while each lower frequency photon carries less energy , there must be a greater number ( per time , per area ) of the lower frequency photons in the beam than the original number of higher frequency photons . as for the second part of your question , i admit that it can be confusing that the power transmitted by e and m waves depends on the amplitude of the wave , while the power transmitted by a mode of a vibrating string depends on both the amplitude and the frequency of the wave . ultimately this comes down to fundamental differences in the physics of each wave phenomenon . the energy in a vibrating string is reducible to the kinetic energy of the moving string elements and the potential energy from the tension felt by each element due to the position of its neighbors . so , at fixed amplitude , you can see that you get even more energy if you jiggle the rope faster . the energy in an e and m wave is a different effect entirely : it comes from the average size of the ( squared ) electric field in the wave that can do work to move charged particles . at a fixed amplitude , if you increase the frequency you will not increase the average size of the field .
radiation normally contains subtle correlations . for all practical purposes you can not use it , but it is there . hawking radiation is , according to the theory , perfectly thermal and does not contain any more information than the temperature itself . the problem is that then the process of black hole evaporation is not reversible , in principle . unlike all other processes that we know of ( which might be irreversible in practice , but are reversible in principle ) . that irreversibility ( which implies non-unitarity ) is incompatible with quantum mechanics . that is the problem in a nutshell . it is really a conflict between quantum mechanics and semi-classical general relativity . there are many more things to be said but i get the impression you have not done a lot of reading about this and details would be rather pointless . i suggest you browse around for a bit with that starting point in mind .
it depends also on the shape of the object . if you assume the trampoline is circular , and the object is much smaller ( like a point mass ) then you can start developing the equations . you have to know the initial tension of the trampoline , and also assume the material non-elastic but supsended by perfect strings in a radial direction ( with known stiffness ) . after some math the static deflection ( with pre-tension ) obeys the following : $$ \frac{w}{k\ , r}=\tan\theta+\left ( \frac{f_{0}}{k\ , r}-1\right ) \sin\theta $$ if $r$ is the radius of the trampoline then the dip is $h=r\ , \tan\theta$ and so $\theta$ is the angle from horizontal that the cone makes . for any given angle $\theta$ the trampoline supports weight $w$ ( given above ) given total stiffness of $k$ and pre-tension of $f_{0}$ . so the above will give you the weight $w$ it will support given a dip $h$ . it is the reciprocal of what you want , but it is solvable . if the trampoline has $n$ linear springs each with stiffness rate of $k_i$ then the total stiffness ( springs in parallel ) is $k=k_i\ , n$ . to define the pre-tension $f_{0}$ assume that the free radius of the trampoline surface is $r$ , but the springs are located at $r_0$ then the pre-tension is $f_{0}=k\ , ( r_0-r ) $ . example : a trampoline of 12 feet in diameter needs $f_{0}=100$ lbs total of pulling to string into a 12.5 foot ring . the stiffness is $k=\frac{100}{6}$ in pounds per inch . to dip the trampoline by 5 feet $\tan ( \theta ) =\frac{h}{r}=\frac{5\times12}{12\times12}$ . plug these into the above and you should get $w=115.4$ lbs . i know i am going to confuse some people because i am treating radial quantities such as stiffness and loads as linear , but it works out ( just use cylindrical coordinates ) . approximation small angle approximation ( weight &lt ; 10% pre-tension , cone angle &lt ; 6° ) $$w = f_{0}\ , \frac{h}{r}$$ example : using the same numbers as above a $w=10$ lbs weight will dip $h = ( 12\times12 ) \frac{w}{100} = 14.4$ inches . the full solution above gives $13.1$ inches theory the deformed shape of the trampoline is a perfect cone . the distance from the center to where the springs start in the deformed state is always equal to $r$ . the extension of the springs tension is then $f=f_{0}+k\ , \left ( \frac{r}{\cos\theta}-r\right ) $ which needs to be balanced by the weight as $w = f\ , \sin\theta$ . the pieces come together to make the equation shown above . update solution is corrected for the fact the radial distance is constant , not the surface area of the trampoline . as the trampoline deforms its perimeter crumples and folds on itself like a napkin when lifted from the center .
lubos hit the key point : you need to calculate the polarization of an atom/molecule so that is the starting point . as you may already know , when a dielectric is subjected to the impinging e-field of an em wave , there are dipoles generated that contribute to the total internal field . the resultant field for most materials is given by $ ( \epsilon−\epsilon_0 ) e=p$ . however , classically , the polarization will depend on the relative displacement between the electron cloud and the nucleus and this displacement can be calculated by thinking of the electron as a harmonic oscillator . that is , the electron cloud will oscillate about the nucleus . there are three terms that must come into play to describe the displacement of the electron . the electron cloud bound to the nucleus must have some sort of restoring force : $−mω_0^2x$ where $ω_0$ is the resonate frequency and m is the mass of the electron . the impinging em wave will exert a time varying force , say $cosωt$ , on the electrons : $ee ( t ) = eecosωt$ where $ω$ is the driving frequency . for a gas , atoms are far enough apart that we can “ignore” interactions between them . however , for atoms and molecules in close proximity , one cannot ignore these interactions which behave as “frictional” type forces . that is , the electron oscillators will dissipation some of their energy as heat . therefore , there must be some type of velocity term : $mβ\frac{dx}{dt}$ where $β$ is a damping constant . if we now stuff all of this into newton’s second law , we have an equation for the electron displacement : $$m\frac{d^2 x}{dt^2} = −mω_0^2x - mβ\frac{dx}{dt} + ee_0cosωt$$ physically , we expect that the electron will oscillate at the same frequency as the impinging em wave , so that the equation above has the solution $x ( t ) = acosωt$ . substituting this assumed solution and solving for the amplitude , we get $$x ( t ) = \frac{ee ( t ) }{m ( ω_0^2-ω^2+iβ ω ) }$$ the electric polarization is the density of dipole moments : $p ( t ) = ex ( t ) n $ where $n =$ number of dipoles . solving for the electric permittivity using $ ( ϵ−ϵ_0 ) e=p$ , $$ϵ=ϵ_0 + \frac{p ( t ) }{e ( t ) } = ϵ_0 + \frac{ne^2}{m ( ω_0^2-ω^2+iβω ) }$$ the way the electric permittivity is related to the index of refraction is as follows : most materials are nonmagnetic at optical frequencies ( the relative permeability is very close to one ) . so to a good approximation , the index of refraction depends only on the relative permittivity $ϵ_r : n^2 ( ω ) = ϵ_r = \frac{ϵ}{ϵ_0}$ . therefore , the dispersion relationship from a damped harmonic oscillator view point looks like $$n^2 ( ω ) = ϵ_r = \frac{ϵ}{ϵ_0} = 1 + \frac{ne^2}{m ϵ_0} \frac{1}{ω_0^2-ω^2+iβω}$$ you can see that the index of refraction is frequency dependent . note that this is valid for only a single resonate frequency ; a given substance has several such resonate frequencies and this equation will need to be modified but the quantum mechanical solution looks very similar to the one above . i will not explain this since i think that i have answered your question .
seeing your comment , it seems you are concerned about group of charges with certain mass . then you need to apply gauss law for the cases where it becomes difficult to apply coulombs law or principle of superposition . in case of gravitational force , find the center of masses of either configuration and you can proceed to find force using newtons law of gravitation . vector sum of either force will then give your net force . for sum special cases like force between earth and other planets which also have magnetism , you can add magnetic force for the net force .
i would like to add a little to lubos 's answer : first a historical note : this is what einstein proposed as a way of understanding quantum mechanics in 1919 or thereabouts , in the paper " do gravitational fields play a role in the composition of the elementary particles ? " einstein was of the opinion that a complicated enough classical theory , like general relativity , would lead continuous waves to collapse into standard-size soliton-like particles and these particles he felt might then bang around along the wave in such a way to reproduce quantum mechanics . this idea reappears several times in the literature , but it demonstrably does not work . a field theory , like gr , is a classical theory , and it therefore is local hidden variables ( the variables are not even hidden in this case ) . this is ruled out by bell 's theorem--- the correlations in quantum mechanics do not allow local fields to carry the data that determines the experimental outcome , not without conspiracy ( superdeterminism ) or nonlocal equations ( faster than light changes in the variables ) . neither works in a straightforward field theory like gr . secondly , gr is not as badly understood as all that , although it is not as well understood as one would like , mostly because numerical methods are in their infancy , and one 's intuition must come laboriously from analyzing exact solutions when these are available . the example i gave of particles oscillating into and out of an extremal black hole is not really new ( do not give me too much credit ) , the new thing there is the holographic interpretation , namely that the coming-out is an ordinary coming out event in this universe . the oscillations of particles into and out of a near-extremal black hole were appreciated in the 1960s , but each oscillation takes you to a disconnected branch classically , because crossing a horizon takes an infinite amount of t-time . this is not possible quantum mechanically , since this disconnected maximally extended thing is not compatible with unitarity . the nice thing about the in-out solution for geodesics in the extremal reissner nordstrom is that if you replace the test particle with a little charged black hole , you can make nonrelativistic oscillations if both black holes are near extremal . the external field of the two black holes does not have a full merger , the little black hole , now not considered as a test particle , but as a solution to gr proper , just smears out on the horizon , then bounces back . i did not calculate this in detail yet , but it can be solved completely with an analysis along the lines of atiyah and hitchin in their famous paper on slow soliton scattering ( the atiyah hitchin space ) , except here , unlike the other case , i am not optimistic there will be a simple geometrical solution , rather one has to bite the bullet and trace the bouncing behavior in the solution either by numerical integration or solving for the near-static phase-space geometry of the two extremal black holes . causalities and ctc 's the basic idea you are giving is that perhaps hidden variables plus closed time-like curves can reproduce bell inequality violations . i will give some sentences about why this is extremely unlikely . quantum mechanics has entangled wavefunctions . what this means is that the wavefunction for k particles is in 3k dimensional space , not in 3 dimensional space . the growth in dimensions means that quantum mechanics packs a stronger computational punch than classical mechanics , and you can not simulate quantum mechanics of k-particles with less than exponentially much classical information . this is why quantum computation works in pure quantum mechanics . so the structure of quantum mechanics is exponentially big and has the entanglements that violate bell 's inequality . if you wish to reproduce this from something like gr , you need gross nonlocality and some way of reproducing nonlocality . so if you have a pair of electrons that bind to an atom ( so that their spins anti-align ) , and then you knock out the nucleus , and do bell measurements on the two outgoing electrons , you need to reproduce the nonlocal correlations from ctc 's in gr . this means that the electron needs to have ctc 's " inside " which go back in time and magically alter the attributes of the other electron . this only became required once you put them together in an atom , and let the photons radiate , and during this process the two point electrons did not necessarily come close to each other ( assuming they are classical and described in space ) . how do ctc 's help correlate them ? to make this work , you would have to go all the way back in time to where the two electrons were created from the inflaton field , and correlate them back then . this type of back-and-forth in time description is utterly conspiratorial , and very unconvincing . there is also no shred of a hint that this will reproduce anything like qm , it is just not ruled out , because you are postulating little tiny internal back-in-time paths on all electrons , something we have no evidence for . there are no real ctc 's in physical exterior solutions of gr . the ctc 's in the intepretation i gave of oscillations into and out of extremal black holes are unphysical--- they are only closed in time because of the wrongness of the classical picture of the horizon . the ctc 's in the interior of a kerr solution can only occur when you wind around the ring singularity , and then it should be possible to unwrap the interior so that it has a pure-causal description , simply by including the winding number of your path around the ring . i do not know the interior kerr well enough to see how to do this , and this must work in any number of dimensions , not just 4 , so i hesitate to say it is what happens , but there must be a reconciliation of causality and kerr interior , because you can set up fields at the horizon of kerr , and let them traverse the interior , and the evolution equation should not have additional constraints , as come from ctcs . all in all , the form of the two theories , gr and qm , is completely different , the descriptions are of a different computational complexity , and the causality notion is totally different in the two schemes , so it is implausible in the highest degree that gr can explain qm . what is more , today we have a good quantum version of gr , string theory , which subsumes and extends the classical theory , so that it is a mistake to pretend that this progress does not exist , and to work as if we were living in 1926 . within string theory , you give a full accounting of all gr effects on flat and ads backgrounds in principle , from an ordinary unitary quantum theory . this quantum gr means that we know how gr and qm are reconciled ( in perturbations to flat and ads backgrounds ) , and the classical limit where gr is reproduced is just not quantum , it is an ordinary classical field theory .
one of the planck papers discusses these anomalies in detail : planck 2013 results . xxiii . isotropy and statistics of the cmb . how unexpected is this variance from the standard model and can it be quantified ? how certain is it that the data are accurate ? for the recent discovery of the higgs boson at the lhc , a five sigma result was considered sufficient to make the announcement . what is the sigma for the reported hemispherical asymmetry ? the anomalies found by planck were already seen in the wmap data , so they confirm the existence of these features . i can not find the specific sigma for the hemispherical asymmetry , but most of the anomalies are reported to have a significance of ~$3\sigma$ . that is enough to raise eyebrows in the astronomical world , although particle physicists would be less impressed :- ) the question is if these anomalies are actually meaningful , and we can expect heated debates in the coming years between ' believers ' and ' non-believers ' . there are a few things to consider : the cmb could be distorted by foreground objects , possibly by our local cluster , our own galaxy and even our solar system . the cmb pervades the entire universe , but we can only observe a small part of it : we are seeing a cross-section in the form of a sphere , centred on us and with a certain radius ( it took the photons 13.8 billion years to reach us ) . an observer in a different galaxy and/or at a different cosmic time would see a different part of the cmb . these effects are sometimes called cosmic variance . basically , it means that what we observe may not be truly representative of the entire universe . also , we are dealing with statistical data , and flukes happen . we are biased at seeing patterns , even though they may not mean anything . for example , if you throw a dice 100 times in a row , then all sorts of apparent patterns may occur . for instance , it could contain the sequence 666666 . that specific sequence may seem unlikely and significant , but it is just as likely as any other specific sequence , like e.g. 202020 or 675439 . when can we expect this controversy to be resolved the planck team has yet to release the polarisation data ( probably next year ) , which may shed some light on the situation . but i think these issues will entertain the cosmologists for quite a few years . a promising field of research is the mapping of the large-scale distribution of ( dark ) matter , using gravitational lensing . this would help calculating the integrated sachs–wolfe effect ( which is the gravitational redshift/blueshift of cmb photons as they pass through the potential well of galaxy clusters ) more accurately . the planck probe data are found to be in error the data is reliable ( the planck data agrees with wmap ) , it is all about the interpretation . the standard model must undergo major revision ? it is possible that the explanation lies in a slight deviation from the standard model ( but no major revision ) ; after all , the standard model is an idealisation . our universe may not be exactly homogeneous or isotropic at large scales . the planck team actually tried fitting a non-standard model , a so-called bianchi model , with mixed results ( planck 2013 results . xxvi . background geometry and topology of the universe ) . some others get rather carried away , speculating about the influence of ' other universes ' . as a final note , it is important to stress that , in the overall scheme , the impact of these anomalies is very small . the standard model fits the cmb almost perfectly , and is in agreement with studies of clusters of galaxies and supernovae . ( source : planck 2013 results . i . overview of products and scientific results , fig 19 ) the fact that the standard model works so well and that the cosmological parameters are now known with such accuracy is a very remarkable feat . thanks to the quality of the data and the success of the theories , cosmologists understand the overall picture with confidence , and can now focus on the details . p.s. all the planck papers can be found here : planck 2013 results papers . the most important one is paper xvi , in which the cosmological parameters are discussed .
i think that your description that the points of the configuration manifold are possible states of the system is as close to a precise definition as one will find . so for $n$ particles in three dimensions , the configuration manifold is just $ ( \mathbb{r}^3 ) ^n$ . as for how this relates to constraints , consider the simplest example : two particles attached with a rigid rod with length $l$ , in three dimensions . let the particles have positions $x_i$ and $y_i$ , $i=1,2,3$ . then the constraint of a rigid rod is that $$l = \sum_i ( x_i-y_i ) ^2 \tag{1} . $$ the configuration manifold of this system is that subset of $ ( \mathbb{r}^3 ) ^2$ that verifies ( 1 ) , that is , a level set of the distance function . it is a general principle that the level sets of a smooth function of the coordinates are also smooth manifolds . hence we can say that imposing a constraint is picking out a submanifold of the configuration manifold for an unconstrained system .
after a lot more searching , i have found the answer to my question ! :d below is a summary of the information i found . there is no specific webpage i can link to because i relied on sources who quoted other sources which no longer exist , but maybe this information can be useful to someone else someday . most of what i learned comes from professor lou bloomfield who currently teaches physics at the university of virginia . edit : none of this is quoted material : all information posted below has been completely reworded , and the analogies ( aside from the guitar string ) are mine . when surrounded by normal matter , a light wave 's electric field will cause electrons to jiggle at a rate equal to the frequency of the light wave : the electric component of the light wave will alternately attract and repel charged particles . when electrons in a material transparent to a certain frequency are excited by a light wave of that frequency , this takes energy away from the light wave . but surprisingly , no photons are absorbed : since the material is transparent to the frequency of the wave , there is no higher orbital which matches exactly the energy level an individual photon would impart to an electron . this means the energy transfer can not involve a real particle interaction . so what happens ? instead of absorbing one or more photons , the electrons enter a virtual quantum state : a temporary excitation that does not exactly match one of the states that the electron can occupy . this is very much like vibrating a guitar string by aiming sound at the string . if the sound you aim at the string matches a frequency that the string can vibrate at , it will cause the string to vibrate . if the sound you use is the wrong frequency , the string will wiggle a little bit as though trying to vibrate , then stop when the sound passes . that is what happens to the electrons : they borrow energy from the light wave , wiggle a little , and then return the energy . a virtual quantum state is very limited in duration , and does not count as a particle interaction . the light wave and the electron remain unentangled and continue to act as probability waves . the electron can only play with the light wave 's energy for a brief period before returning it . the characteristics of the light wave remain unchanged because there was no real particle interaction . so the light does not ricochet off of atoms , nor does it get emitted in the usual sense by the electrons which play with it . even though the interactions are all virtual , electrons are matter and they take time to jiggle . as this happens over and over and over again , it slows the progress of the wave . you might think of this like a kind of friction which acts against the progress of the wave . consider a car whose wheels turn at a constant speed , and imagine it encounters a series of large bumps that slow it down slightly . the speedometer is based on the wheel rotation , so it would say the car has not changed speed at all : it is just as fast as it was on flat terrain . the car will , however , cover less ground per time interval because some of the wheel-turning is used to surmount the humps . these humps are akin to the process of electrons temporarily borrowing energy from the light wave . so is the light wave truly slowed , or is the light still moving at c and only its progress is slowed ? this is not actually a well-formed question , and for all practical purposes the answer does not matter . however , i find it easier to think about it as slowing the wave 's progress . this means the characteristic that " light moves at speed c in all reference frames " still holds true , which makes it much easier for me to reason about relativistic effects . additionally , i was incorrect about different frequencies slowing by the same amount : lower frequencies are slowed less than higher frequencies . when the frequency is lower , even though the wave has less energy , the electrons will need to jiggle over a wider area ( they are pulled for a longer period , then pushed for a longer period ) . since the electrons remain bound to their atoms in this interaction , they can not be pulled out of the atom by a virtual excitation . so the slower the frequency is , the " more virtual " the excitation must be , and the less time the electrons have to play with the light . is this information useful ? if so , is there a way i could make it more accessible ? just curious , as i am very new to se .
the crucial word is " beam " , in " beam splitter " . beam means an ensemble , in contrast to " photon " which is an individual particle . a light beam is an ensemble of photons and if it is of a single frequency $\nu$ , all photons have energy $e= h*\nu$ . a light beam can be split in a beam spliter , i.e. the ensemble of photons can be split into two streams of photons : the intensity of the beam goes down , but the individual photons still have frequency $h*\nu$ . now one can think of impinging photons one by one on a beam splitter . a photon is described by a wavefunction which when squared will give the probability of finding the photon in a particular ( x , y , z ) . it will go either where one stream went or the other according to the probabilities , but it will be seen as a whole photon of energy $e=h*\nu$ .
there are various ways to decide which of the assumptions are primary and which of them are their consequences but $e=vq$ may be most naturally interpreted as the definition of the potential . the potential energy is a form of energy and the potential ( and therefore voltage , when differences are taken ) is defined as the potential energy ( or potential energy difference ) per unit charge , $v = e/q$ . that is equivalent to your equation . the potential energy is proportional to the charge essentially because of the linearity of maxwell 's equations ( the superposition principle ) . once we know about the proportionality , we must just give a name to the proportionality factor between $e$ and $q$ and we simply call it potential ( or voltage ) .
try theorem 12 in ( the arxiv version ) of http://arxiv.org/abs/quant-ph/0411098 . what i call " lattice states " there should exactly be the class of states you are interested in .
i have written an answer to mathoverflow in which explicit formulas for the classical and quantum hamiltonians of a spin system ( generators of $su ( 2 ) ) $ were written explicitely . the classical hamiltonians are given by means of functions on the two sphere and the quantum hamiltonians by means of holomorphic differential operators ( which act on the sections of the quantum line bundle ) . for many spin system with a linear hamiltonian in each spin , one just has a distinct one particle hamiltonian per spin . sorry for referring to my own work , but it is by no means original .
have you read about the construction and operation of bubble chambers ? they are large vessels ( with at least one transparent side ) and work by rapidly reducing the pressure in the contained fluid so that it is superheated for a time . doing that requires plumbing ; both for filling the chamber with working fluid , and for arranging the pressure drop . as the working fluid is often cryogenic it was also probably kept in circulation so that it could be periodically re-cooled , calling for more plumbing . as the working fluid is chosen to be transparent except where bubbles nucleate , you can see through to whatever occupies the far side chamber . ( the figure is a negative image so the light background of the image represents a dark background space . ) i would not be surprised if those neat , uniform circles all in a line represent the plumbing in the vessel , and the rest of the structure in the background the panels and welds of the chamber .
there are at least two mechanism of thermal conductivity - free electrons and thermal phonons . the first mechanism can be prevalent in metals , the second one is important in dielectrics . i did not look up thermal conductivity of glass , but such excellent dielectric as diamond has higher thermal conductivity than any metal , as far as i know .
let 's assume that the question is telling us to keep the component of the speed in the plane perpendicular to the central rod constant . then in the case with upward acceleration , newton 's second law in the $x$ ( horizontal ) and $y$ ( vertical ) directions reads \begin{align} t_a\sin\theta_a and = m\frac{v^2}{\ell\sin\theta_a}\\ t_a\cos\theta_a and = m ( g+a ) \end{align} where $t_a$ is the magnitude of the tension for given upward acceleration $a$ and $\theta_a$ is the corresponding angle . this is two equations in two unknowns $t_a$ and $\theta_a$ . solving for $t_a$ ( i used mathematica out of laziness fyi ) $$ t_a = \frac{m v^2}{2\ell}\sqrt{1+\frac{4 ( g+a ) ^2\ell^2}{v^2}} $$ note that for fixed $v , \ell , m$ the tension necessarily increases . note . i had previously only analyzed the vertical component of the tension which op pointed out was not sufficient to answer the question .
this cannot be answered fully today--- this is the question of what is the bit-content of the relevant computation that is going on in the body . if you know the bits to simulate the computation efficiently , excluding the bits that are effectively random , you could say . whenever you have a biological system , some bits are doing important things , like the bits that tell you what domains are bound to an active protein , and some bits are useless , like the bits that tell you the precise orientation of a subpart of the cytoskeleton , or the bit that describes the orientation of some water molecule . when you want to store the data in a person , you are generally speaking about the biologically relevant data . if you store this data , and destroy the organism , and then restore the organism from new molecules , arranged according to the instructions in this irreducible data , the organism will behave in a statistically indistringuishable way from the original . one can say some general things about the size of this data : it is not infinite there are some people who claim that the computation is like an analog computer ( or as close as one can come given quantum limitations ) . this means that there are molecules or large objects in the cell that store analog data in their positions . this is not true at all , and this fallacy is persuasive enough that one must argue against it . when you have a system in a thermal bath , there is always diffusion going on between interactions . if you have a molecule which is storing data in some way relevant to the biology , it must store this data in a way that can be retrieved by the remaining computation effectively . if this data is randomized by diffusion , it is not effective storage , and this bit may be discarded and replaced by a random number generator . if you have a diffusing protein with interactions with other proteins every time $\delta t$ on average , and with diffusion constant d , the protein will randomized into a gaussian of size $\delta x= d\sqrt{\delta t}$ before the next interaction . this means that it is pointless and wasteful in terms of storage to specify the position to more accuracy than this size , and the number of possible positions is on a lattice of size $\delta x$ . the number of points grows as the log of $\delta x$ , so the number of bits a position can encode is bounded by the log of this , and it only grows logarithmically . this means that no matter how absurdly quickly you try to make the protein interact , the number of bits it can store in the position is never very high . it would be better off adding 5 binding domains rather than trying to localize more precisely . the result applies to all other continuous storage mechanisms you can dream up--- the position of untethered molecules diffuses , the angles of proteins randomize , the concentrations of atoms are only relevant to the extent that a localized channel or protein can discriminate between different concentrations . in all biological systems , the spatial resolution cutoff for all motions is coarse enough to make the dominant information storage mechanism molecular binding . it is not that large the binding of molecules at first glance includes a large number of bits , since each protein is a long sequence of amino acids . but this is also a false bit content . in order to be dynamical data , capable of computing , the data has to change in time . even if you have a very complicated protein , if it is only action is to bind to a ligand , then it has exactly two states , and carries one bit of information . if it binds to a polymer , then it can have many different bits , but these bits should be associated to the binding sites of the polymer . the protein has two states , the polymer has many different states of protein binding . to store this protein state efficiently on a computer , i just have to name the proteins with a unique name ( this only takes a few bits ) , and give the state of each one of each type . the proteins which carry 1 bit of information will be fully specified by the number of 1-state proteins and the number of 0-state proteins . to specify this number only requires log growing information , so it has negligible bit content . this is absurd , of course . if you include rough position information , you will always need about 10 bits per proteins ( to name a billion locations in the cell where it could be ) to really specify the state . so there are not going to be 1 bit proteins which are not tethered to one spot . but the point here is that the dynamical data that is doing the computation is far smaller than the data encoded in the protein amino-acid sequence , because this data is rom , it is not ram , and it can be specified ahead of time , you do not need to simulate to know the types of proteins that are running around it is very small in proteins there is a simple formalism ( see here , i authored this : http://arxiv.org/abs/q-bio.mn/0503028 ) which allows you to estimate the bit-capacity of binding proteins . the formalism is also useful for describing how proteins bind . it turns out to be related to d . harel 's higraphs , but it extends the formalism nontrivially to include polymerization ( harel was only interested in finite state automata , and did not consider binding molecules ) . the point of this formalism is to easily estimate the bit-capacity of protein networks . the estimate is simply by multiplying the bit-capacity of a typical protein by the total number of proteins . excluding proteins of known function , metabolic stuff , and so on , you find that there is a range of bit-values in a human cell from as low as 10kb to as high as 1mb , but the higher end is extremely optimistic , and assumes that every different binding state is functionally discriminatable , so you can tell apart every phosphoryllation state of p53 from every other just by looking at the future dynamics . this is clearly false , and i tend to believe the lower estimates . it is rather big in rna on the other hand , rna strands in the cell can do much more . rna is self-binding in complementary pairs , and to predict the future behavior of a strand , you need to know the sequence . this is because the sequence can find another complementary sequence and bind , and this bound sequence can attach to a protein , and so on , in a closed loop computation . in order for this information to contribute to the bit-content , one must assume that there is a vast undiscovered network of interacting rna . i will assume this without any compunctions . this explains many enduring mysteries , which i will not go into . the memory capacity in an rna computer in the cell is easy to estimate , it is twice the number of nucleotides . unlike for proteins , each nucleotide is carrying ram , not rom , if the interactions with other rna is sufficiently complex . this is not true of mrna , this is not true of trna , it is not true of ribosomal rna in isolation , so it requires more roles for rna than were ever imagined . this is a prediction which does not surprise biologists anymore . it all hinges on how much rna computing is going on in the cells the estimate for rna is the memory content of dna , which is on the order of 10^9 bits . the rna can be an order of magnitude greater , two orders of magnitude greater , but no more , since you will run out of space . this is 10^11 bits per cell . the biggest thing is the brain in the brain , there is more genetic material than anywhere else . if you just take the weight of the brain and consider that it is all rna , you get a reasonable estimate of the bit content of a person . it is approximately 10^22 bits per person , or $10^9$ terabytes , a billion terabytes in rna , much less in everything else ( so the wikipedia estimate is probably ok for everything else ) . this is the correct estimate of the memory capacity of a person , since it is riduclous to think that the information in the vast unknown rna in the brain is decoupled from the neuron activity , considering that the neuron activity is otherwise completely computationally pathetic . i described this idea in more detail here : could we build a super computer out of wires and switches instead of a microchip ? .
the reason why we consider the distortion of original field is superposition of nwe charge in system . but it is clear that in rule of superposition we should never consider the effect of any charge on itself . so after once defining the electric field we can use finite point charges to apply f=qe .
i have not made the transition myself , and it certainly depends on what aspect of finance you go into , but the answer is likely " absolutely . " if you were to end up doing computational finance , i assume some of the techniques are similar . in computational astrophysics , we often simulate a physical problem by breaking it up into discrete chunks and evolving some partial differential equations in each of those chunks . this usually done in a multidimensional framework of up to ~7 ( 3 space + 3 momentum + time ) dimensions if you are doing radiation hydrodynamics solving the boltzmann transport equation . i am a bit unfamiliar with computational finance , but i imagine you have a similar problem in that you have some differential equations telling you how each of the important variables changes with respect to your dependent variables . the problem here is likely to be similar to wanting to minimize some " energy " ( read : maximize some profit ) function in some very high-dimensional phase space - certainly greater than 7 . nevertheless , their certainly exists an overlap in techniques . more importantly , the ability to think about and solve problems in the discrete manner used in computational astrophysics will certainly be applicable in areas outside of astronomy , such as finance .
start from the beginning . why constraint relations ? why are they there ? let me emphasize : let 's take origin at top pulley which is at rest . note that length of top rope is constant : $a+b=k\implies a''+b''=0 \implies a''=-b''$ also length of second rope is constant : $ ( c-b ) + ( d-b ) =k\implies c''+d''=2b''$ note that $d$ is a constant as the top pulley and ground is rest : $c''=2b''$ hence , $c''=-2a''$ as stated in comments . also , everything we have done is futile and the block $m_2$ will hit the ground very quickly .
running the rges in reverse should be valid so long as you do not integrate over a scale where degrees of freedom enter/leave the theory . if you integrated out the electrons in qed , you had have irrevocably lost that information in your low energy description of interacting photons . you had see some non-renormalizable theory with interacting corrections to pure em but rg evolving to the uv would not tell you what that would be . just like rg evolving qed to the uv keeps you unaware of the strong or the weak sector physics . on the other hand , so long as you have not crossed any characteristic scale in your theory , the theory at the scales you have integrated out should be the same as the theory at the scale you are currently at . so you should be able to go back to where you came from . to summarize , so long as you do not integrate out some characteristic scale , you can keep going back and forth .
the earth is not a perfect sphere ( or even a perfect oblate spheroid ) so its gravitational field is not axially symmetric . you have probably seen the geoid measured by the goce and grace satellites . as the earth rotates the asymmetries in its gravitational field rotate with it , and any satellite whose orbital period is a ratio of one day can build up a resonance with the daily variations in the earth 's gravity . this is essentially the same physics as the resonances seen in , for example , the moons of jupiter . i had a quick google and found this paper that gives a fairly detailed analysis of the phenomenon . see in particular section 1.4 . the galileo satellites orbit 17 times every 10 days . this is sufficiently far from a simple ratio that resonances do not build up .
no . objects with different masses will often have the same electric charge--note the proton and the electron as examples of this . or even more tellingly , the electron and the muon .
well if you are really interested in gyroscopes what about the equations of motion to get us started ? here i have copied some from http://www.gyroscopes.org/math2.asp which are in euler-like coordinates : q = nutation angle - nutation rate q'= n f = precession angle - precession rate f'= p y = spin angle - spin rate y'= s and primes denote differentiation wrt time . we are assuming that s is constant - this is because there is no friction . $sm_x=i_xn&#39 ; -i_yp^2cosqsinq+i_zpsinq ( pcosq+s ) $ $sm_y=i_y ( pncosq ) -i_zn ( pcosq+s ) +i_xpncosq$ $sm_z=i_z ( -pnsinq ) $ the expression for $sm_z$ simplifies because of the symmetry in the moments of inertia which in our case with the sphere on a rod r are : $i_x=mr^2+2/5mr^2$ $i_y=mr^2+2/5mr^2 = i_x$ $i_z=2/5mr^2$ now this is an external torque gyroscope - as we are on a table on some gravitational environment . the external torque applies in the x direction $f_x=mgrsinq$ note that all of these equations are written in terms of the nutation angle q . in these coordinates q=0 when it is upright and $q=90^0$ when it is fallen over ( although this is an approximation since it falls over at $q=90-arctan ( r/r ) $ ) . the intuition behind your question is that when there is no spin s=0 , there is no precession p=0 . in this case the " fall " corresponds to a nutation from a small angle $q_0$ down to $q=90^0$ - and this nutation is the only motion . as s> 0 these equations will generate a non-zero precession p> 0 , also the nutation will now reach a maximum $q_{max}$ . the requirement is that s is sufficiently large that $q_0-q_{max}&lt ; 90^0$ . if this condition is not met then the top will still hit the surface after some rotation ( precession angle f in these coordinates ) . if one wished to take this further analytically then we could approximate for small initial angle $q_0$ . so $sin q_0=q_0$ and $cos q_0=1$ . then we consider the x axis terms for the external torque . $f_x=m_x$ - which holds at all times $mgrq_0=i_xn&#39 ; -i_yp^2q_0+i_zpq_0 ( p+s ) $ this quadratic in p will determine it at t=0 in terms of other parameters ( but n ' might not be a constant - from graphical solutions i expect it to be sinusoidal ) . likewise one can determine the " falling over " equation when $q=90^0$ . if p=p ( s ) does not satisfy the " falling over " equation then the gyroscope will continue without falling . if it does we can integrate to determine q and hence q ( t ) . edit : additional point on gyroscope intuition comments suggest that the motion described by the equations is not directly in accord with intuition . this is likely to be because a real gyroscope has some elements of friction . this friction has the effect of a damping term in the above equations . the friction will be basically a function of s , but n=n ( s ) through the equations , and the only acceleration term here is n ' . thus the friction , for a realistic gyroscope , will damp the nutation oscillations that otherwise occur . this would make the nutation an approximate constant of motion . treating nutation as a constant , simplifies the equations by setting n'=n=0 and so q , cos q and sin q become constants . however from an intuition perspective it reduces the number of ( euler ) degrees of freedom from 3 to 2 . so in a real gyroscope we might see only the two degrees of freedom : spin and precession . trying to intuit from this the behaviour in the remaining nutation dimension could be misleading as it ordinarily does not display behaviour in that dimension . a related physical system which does display nutation is the " spinning plate on a pole " scenario . as the spin of the plates reduces through friction they start to wobble . if a wobble is too great the plate will fall over , even although it is still spinning a little .
the string that the eggs hang from is allowed to move in the same direction as the eggs when swung perpendicular . this is not the case when you swing in parallel . if you hold the top string still and swing the egg perpendicular you will see that almost none of that energy transfers to the other egg and the egg you pushed will continue to swing . basically the main string only has one degree of freedom and moving parallel is not the direction that the string moves .
i think it is expected that you have a bit of common sense about this . let 's take the operator $o ( t ) $ which is the position operator when $t$ is between 9:00 and 10:00 in the morning , and the momentum operator the rest of the day . now take a system in a stationary state , and ask " what is the expectation value of $o ( t ) $ at each time $t$" ? whoa , the expectation value changes dramatically each day at 9:00 , then changes again at 10:00 ! does that mean the state is not " stationary " at 9:00 or 10:00 ? no , of course it does not mean that ! when an operator has explicit time dependence , as $o ( t ) $ does , it means that you have a different -- possibly totally unrelated -- operator at each time $t$ . wikipedia says " the system remains in the same state as time elapses , in every observable way " . that is correct . i do not think a reasonable person reading that sentence would infer that if you calculate the expected position at 9:30 , and then you calculate the expected momentum at 10:30 , the two calculations should have the same answer for a stationary state .
this might be more of a math question . this is a peculiar thing about three-dimensional space . note that in three dimensions , an area such as a plane is a two dimensional subspace . on a sheet of paper you only need two numbers to unambiguously denote a point . now imagine standing on the sheet of paper , the direction your head points to will always be a way to know how this plane is oriented in space . this is called the " normal " vector to this plane , it is at a right angle to the plane . if you now choos the convention to have the length of this vector ( "the norm" ) equal to the area of this surface , you get a complete description of the two dimensional plane , its orientation in three dimensional space ( the vector part ) and how big this plane is ( the length of this vector ) . mathematically , you can express this by the " cross product " $$\vec c=\vec a\times\vec b$$ whose magnitude is defined as $|c| = |a||b|sin\theta$ which is equal to the area of the parallelogram those to vectors ( which really define a plane ) span . to steal this picture from wikipedia 's article on the cross product : as i said in the beginning this is a very special thing for three dimensions , in higher dimensions , it does not work as neatly for various reasons . if you want to learn more about this topic a keyword would be " exterior algebra " update : as for the physical significance of this concept , prominent examples are vector fields flowing through surfaces . take a circular wire . this circle can be oriented in various ways in 3d . if you have an external magnetic field , you might know that this can induce an electric current , proportional to the rate of change of the amount flowing through the circle ( think of this as how much the arrows perforate the area ) . if the magnetic field vectors are parallel to the circle ( and thus orthogonal to its normal vector ) they do not " perforate " the area at all , so the flow through this area is zero . on the other hand , if the field vectors are orthogonal to the plane ( i.e. . parallel to the normal ) , the maximally " perforate " this area and the flow is maximal . if you change the orientation of between those two states you can get electrical current .
it is also matter of convention . consider a portion $v$ ( of a continuous body $b$ ) bounded by a closed surface $\partial v$ and a point $p$ on $\partial v$ . if $n$ is the outward unit normal to $\partial v$ at $p$: $$f ( p , n ) _i = \sum_{j=1}^3\sigma ( p ) _{ij}n^j$$ is the surface density of force acting on $p$ and due to the part of $b$ outside of $v$ . ( one could adopt the other convention , where $n$ is the inward unit vector . ) with this definition if $f$ is parallel to $-n$ , $f$ is compressive . consequently , always referring to that definition , if $\sigma$ is diagonal , a positive eigenvalue means traction along the corresponding eigenvector $n$ interpreted as the outward unit vector to on closed surface , and a negative eigenvalue means compression along the corresponding eigenvector . the direction of $n$ as an eigevector is irrelevant , it just indicates where the closed surface is placed . indeed the stress tensor of a fluid in equilibrium is $-p \delta_{ij}$ with $p&gt ; 0$ . it implies that every portion of the fluid is compressed .
unlike neutrinos , light will be slowed down by gas in the tunnel , which would have to go through the earth . it is much cheaper and easier to mathematically analyze the opera results to find their error , were they ever to release their detailed protocol , which is unlikely , because they do not seem to want the error discovered .
the short answer is that it can , if $m = 1 = m^{-1}$ . in this way of looking at it , all quantities in planck units are pure numbers . the longer answer is that there are two different ways of thinking about natural unit systems . natural unit systems in terms of standard units one of them , and perhaps the easier one to understand , is that you are still working in a " traditional " unit system in which distinct units for all quantities exist , but the units are chosen such that the numerical values of certain constants are equal to 1 . for example , if you want to set $c = 1$ , you are not literally setting $c = 1$ , you are actually setting $c = 1\ , \frac{\text{length unit}}{\text{time unit}}$ . length and time do not actually have the same units in this interpretation ; they are equivalent up to a multiplication by factors of $c$ . in other words , it is understood that to convert from , say , a time unit to a length unit you multiply by $c$ , and so that is left implicit . in order to do this , of course , you have to choose a length unit and time unit which are compatible with this equation . so you could not use meters as your length unit and seconds as your time unit , but you could use light-seconds and seconds , respectively . if you want to set multiple constants to have numerical values of 1 , that constrains your possible choices of units even further . for example , suppose you are setting $c$ and $g$ to have numerical values of 1 . that means your units have to satisfy both the constraints $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_g}{t_g} and g and = \frac{ ( \text{length unit} ) ^3}{ ( \text{mass unit} ) ( \text{time unit} ) ^2} = \frac{\ell_g^3}{m_gt_g^2} \end{align}$$ where i have introduced $\ell_g$ , $t_g$ , and $m_g$ to stand for the length , time , and mass units in this system , respectively . you can then invert these equations to solve for $\ell_g$ , $t_g$ , and $m_g$ in terms of $c$ and $g$ - but as you can probably tell , the system of equations is underdetermined . it still gives you the freedom to choose one unit to be part of your unit system , such as $$\text{kilogram} = \text{mass unit} = m_g$$ having made that choice , you can now solve for $m_g$ , $\ell_g$ , and $t_g$ in terms of $c$ , $g$ , and $\text{kilogram}$ ( or whatever other choice you might have made ; each choice gives you a different unit system ) . running through the math for this gets you $$\begin{align} m_g and = 1\text{ kg} and \ell_g and = \frac{g ( 1\text{ kg} ) }{c^2} and t_g and = \frac{\ell_g}{c} = \frac{g ( 1\text{ kg} ) }{c^3} \end{align}$$ now you can plug in values of $g$ and $c$ in , say , si units , and get conversions from si ( or whatever ) to this unit system . note that , as i said , length does not literally have the same units as time or mass , but you can convert between the length unit , time unit , and mass unit by multiplying by factors of $g$ and $c$ , constants which have numerical values of 1 . in a sense , you can consider this multiplication by $g^ic^j$ as analogous to a gauge transformation , i.e. a transformation that has no effect on the numerical value of a quantity , and the units of length , time , and mass are mapped on to each other by this transformation just as gauge-equivalent states are mapped on to each other by a gauge transformation in qft . so it is more proper to say $l \sim t \sim m$ ; the dimensions are not equal , just equivalent under some transformation . if you do the same thing but setting $c = \hbar = 1$ instead , remember what you are really doing is specifying that your units must satisfy the constraints $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_q}{t_q} and \hbar and = \frac{ ( \text{length unit} ) ^2 ( \text{mass unit} ) }{ ( \text{time unit} ) } = \frac{\ell_q^2m_q}{t_q} \end{align}$$ ( $q$ is for " quantum " because these are typical qft units ) , and then running through the math , again with $m_q = 1\text{ kg}$ , you get $$\begin{align} m_q and = 1\text{ kg} and \ell_q and = \frac{\hbar}{ ( 1\text{ kg} ) c} and t_q and = \frac{\ell_q}{c} = \frac{\hbar}{ ( 1\text{ kg} ) c^2} \end{align}$$ again , the units are not literally identical , but $\ell_q \sim t_q \sim m_q^{-1}$ under multiplication by factors of $\hbar$ and $c$ . of course , your third constraint does not have to be a choice of one of the fundamental units . you can also choose a third physical constant to have a numerical value of 1 . to obtain planck units , for example , you would specify $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_p}{t_p} \\ \hbar and = \frac{ ( \text{length unit} ) ^2 ( \text{mass unit} ) }{ ( \text{time unit} ) } = \frac{\ell_p^2m_p}{t_p} \\ g and = \frac{ ( \text{length unit} ) ^3}{ ( \text{mass unit} ) ( \text{time unit} ) ^2} = \frac{\ell_p^3}{m_pt_p^2} \end{align}$$ you can tell that this is no longer an underdetermined system of equations . solving it gives you $$\begin{align} m_p and = \sqrt{\frac{\hbar c}{g}} and \ell_p and = \sqrt{\frac{\hbar g}{c^3}} and t_p and = \sqrt{\frac{\hbar g}{c^5}} \end{align}$$ here , since you have set three constants to have numerical values of 1 , your three fundamental planck units will be equivalent up to multiplications by factors of those three constants , $g$ , $\hbar$ , and $c$ . in other words , multiplication by any factor of the form $g^i\hbar^jc^k$ is the equivalent to the gauge transformation i mentioned earlier . you can tell that all these units are equivalent under such a transformation , but more than that , all powers of them are equivalent ! in particular , you can convert between $m$ and $m^{-1}$ by multiplying by constants whose numerical value in this unit system is equal to 1 , and thus it is not a problem that $m \sim m^{-1}$ here . unit systems as vector spaces another way of understanding unit systems , which is kind of a logical extension of the previous section , is to think of them as a vector space . elements of this vector space correspond to dimensions of quantities , and the basis vectors can be chosen to correspond to the fundamental dimensions $l$ , $t$ , and $m$ . ( of course you could just as well choose another basis , but this one suits my purposes . ) you might represent $$\begin{align} l and \leftrightarrow ( 1,0,0 ) and t and \leftrightarrow ( 0,1,0 ) and m and \leftrightarrow ( 0,0,1 ) \end{align}$$ addition of vectors corresponds to multiplication of the corresponding dimensions . derived dimensions correspond to other vectors , like $$\begin{align} [ c ] = lt^{-1} and \leftrightarrow ( 1 , -1,0 ) \\ [ g ] = l^3m^{-1}t^{-2} and \leftrightarrow ( 3 , -2 , -1 ) \\ [ \hbar ] = l^2mt^{-1} and \leftrightarrow ( 2 , -1,1 ) \end{align}$$ in this view , setting a constant to have a numerical value of 1 corresponds to projecting the vector space onto a subspace orthogonal to the vector corresponding to that constant . for example , if you want to set $c = 1$ , you project the 3d vector space on to the 2d space orthogonal to $ ( 1 , -1,0 ) $ . any two vectors in the original space which differ by a multiple of $ ( 1 , -1,0 ) $ correspond to the same point in the subspace - just like how , in the previous section , any two dimensions which could be converted into each other by multiplying by factors of $c$ could be considered equivalent . but in this view , you can actually think of the two dimensions as becoming the same , so that e.g. length and time are actually measured in the same unit . since in planck units you set three constants to have a numerical value of one , in the dimensions-as-vector-space picture , you need to perform three projections to get to planck units . performing three projections on a 3d vector space leaves you with a 0d vector space - the entire space has been reduced to just a point . all the units are mapped to that one point , and are the same . so again , $m$ and $m^{-1}$ are identical , and there is no conflict .
i have found the answer to this question , and it does basically follow from holomorphicity . the holomorphicity property is essentially the statement that the operator is bps : $\phi$ is annihilated by a supercharge $q$ . this is also the reason the property does not hold for vector superfields , which are not bps . the reason that the dimension of the operator is protected from quantum corrections is due to the structure of the superconformal algebra . the superconformal algebra contains , in addition to the supercharges $q_\alpha$ , the conformal supercharges $s_\alpha$ that come from the commutator $ [ q , k ] \approx s$ . the commutation relations of the algebra show that s lowers dimensions by $1/2$ , while $k_\mu$ still lowers dimensions by 1 . the multiplets of the superconformal algebra are larger than those of the conformal algebra . the highest weight states are superconformal primaries that are annihilated by both $s$ and $k$ . starting with such an operator $\phi$ satisfying $ [ s , \phi ( 0 ) ] = [ k , \phi ( 0 ) ] =0$ you may generate the rest of the representation by acting on $\phi$ with both $q$ and $p^\mu$ . the point is that for special operators , called chiral primaries ( also called bps operators ) , $ [ q , \phi ( 0 ) ] =0$ and you get a short multiplet . you can use this fact along with the commutation relations of the superconformal algebra ( specifically $\{ q , s \} \approx r + d$ where r is the r charge and i am assuming a scalar operator for simplicity ) to show that the dimension of a chiral primary is determined solely by it is r-charge . note that in a superconformal theory r-charge is well defined since it cannot be anomalous without breaking superconformal invariance . the point is that these chiral primaries cannot pick up anomalous dimensions . basically , we have the dimension of the operator in the free theory where the coupling is 0 . regardless of the coupling , the chiral primary is still annihilated by the supercharge , it remains a chiral primary in the interacting theory . if this were not the case , continuously varying the coupling would lead to a discontinuity in the number of operators of a given dimension ( this must be an integer , it cannot vary continuously with the coupling ) . so the operator in the interacting theory is still a chiral primary , its dimension is related to its r-charge . in the case i asked about , the chiral superfield is a chiral primary . products of chiral superfields are still chiral , so the composite operator is still bps , and its dimension is related to its r-charge . but the r-charge of the operator is the sum of the r-charges of the individual operators , so we can simply add dimensions and not worry about any anomalous dimension . the answer is a little more complicated than i had expected , but i am almost certain that this is the correct explanation .
the photons and gravitons involved in static fields are not causal , they do not propagate along light cones . they are acausal things in a feynman framework . any gauge charge is visible outside the black hole , this is because gauge fields are determined by a gauss law at infinity . a good classical picture is that a charged black hole has a charge-per-unit-area on the horizon , which is considered as a gr version of an charged plate , the charge per unit area is the electric field density on the horizon , while the horizon always carries mass per unit area away from extremality , and this is by the surface gravity of the black hole . these classical picture do not refer to particles , only to fields . the duality between particle and field description is subtle , and should not be used for casual arguments like this .
first , one inevitably gets the same solutions if he solves the problem in the slab 's rest frame , and then lorentz-transforms the result to the frame where the slab is moving ; or if one solves the problem directly in the frame where the slab is moving . the reason is that maxwell 's equations are covariant under the lorentz transformations . so if they are satisfied in one frame , they will be satisfied in any frame related by boosts , too . however , we must properly transform all the magnetizations and material relations etc . and add the corresponding moving sources which will be the main subtlety in the text below . in your particular problem , one may say some generic statements about the magnetic ( and electric ) fields without much thinking . for example , if $\vec m$ is in the $x$-direction , it means that the electrons may be thought of to spin in the $yz$-plane . take a surface of the slab parallel to the $yz$-plane - i.e. one face that belongs to a $x=x_0$ plane . it is pretty clear that there will inevitably be a component of the magnetic field $\vec b$ in the $x$-direction near the external side of the surface . if one boosts the $b_x$ magnetic field in the $z$-direction , there will inevitably be a nonzero electric field in the $y$-direction , $e_y$ . in the frame where the slab is moving , we seem to have no electric sources $\rho$ of the $\mbox{div}\ , \vec d=\rho$ gauss 's law and no right-hand side of the maxwell-faraday equation , $\nabla\times \vec e = -\partial \vec b / \partial t$ . so because there are no electric sources , you would think that the electric field should vanish . however , this is a flawed argument because the form of maxwell 's equations we are using here are only " maxwell 's equations for materials at rest " . in particular , the gauss 's law is optimized for $\vec d$ which we are imagining to be given by $\epsilon \vec e$ , and is " purely electric " . however , for a moving material , there should be an extra term of the type $\vec v\times \vec m$ included in $\vec d$ . because the latter has a nonzero $y$-component in the moving frame , there will be a nonzero $e_y$ in this frame , too . the precise form of maxwell 's equations in a moving medium may be confusing and unfamiliar so i think it may be a good idea to try to transform the local physics to the rest frame of any material , whenever needed , and perhaps lorentz-transform back . whenever subtleties would occur , one would have to revisit the derivation of the " macroscopic maxwell 's equations " ( for materials ) and redo it with the possibility of moving materials . microscopic maxwell 's equations alternatively , you could always try to use the microscopic maxwell 's equations which include the gauss 's law in the form $\vec \nabla\cdot \vec e = \rho / \epsilon_0$ . but in this form , $\rho$ includes not only free charges but also the " microscopic charges " related to the material . because the slab has nonzero values of $j_y$ and $j_z$ ( currents inside the material ) - recall that the electrons are kind of rotating in the $yz$-plane ( to produce the magnetic $x$-field ) , it is also true that when we boost the system in the $z$ direction , the corresponding multiple of $j_z$ will produce a nonzero value of $\rho$ ( microscopic charge density ) . this will be the source of the $e_y$ field discussed above . in particular , $j_z$ will be proportional to $ [ \delta ( y-y_1 ) -\delta ( y-y_2 ) ] $ in the slab 's frame which means that there will be $\rho \sim [ \delta ( y-y_1 ) -\delta ( y-y_2 ) ] $ in the frame where the slab is moving . it is this $\rho$ that will induce a nonzero value of $e_y$ right outside the material ( in the frame where the slab is moving ) .
if you are thinking about light the ' conservative answer ' would be : for this experment photon behaves as a wave , it doesnt have actual size , and so the only thing that will happen is that the wave will stretch its lenght , but the photon itself will not stretch as it has no diameter .
you have correctly deduced how the circuit works . this particular configuration is better known as a bridge rectifier and is often packaged as a single component containing 4 diodes . there are two uses for this - rectifying alternating current as depicted in your question , and creating circuits that can handle direct current with reversed polarity ( for instance , in the event a battery is inserted backwards ) .
excellent question . there are forces for which a force-carrier particle is not it is own antiparticle ( eg : strong force or the weak force ) so if we accept your explanation , then we might have to abandon newton 's 3rd law for those forces , which is implausible . edit : maybe i am being obtuse . i do not think really matters that the antiparticle is not the same particle , so long as it can be interpreted as some particle with the opposite momentum . we never care about what kind of particles carry the force -- if there are many , we just sum over them anyways . so , maybe the reason that you gave does get to the heart of the matter . also , in my explanation below , the derivation of the potential from the propagator depends on the feynman pole prescription which is the interpretation that every particle has a corresponding antiparticle . so i think you hit the nail on the head :- ) if i were to take a shot , i would say that the law " comes from " the fact that only the relative position matters , when two objects exerting a force on each other . think of the energy in the system and the force as $- \frac{\partial u}{\partial x}$ . the " potential " is obtained by inverting the propagator must depend only on the distance between the charges and nothing else ( by rotational invariance ) . distance $= |x_1 - x_2|$ . physically , you can think of either charge in the potential of the other -- so the energy must be symmetric in their charges . this will give us a force law similar to what we have seen before $$u ( q_1 , x_1 ; q_2 , x_2 ) \sim \frac{q_1 q_2}{|x_1 - x_2|} e^{- m |x_1 - x_2|}$$ where $m$ is the mass of the force-carrier particle . for a photon or graviton $m=0$ so we get the usual coulomb/newtonian potential . once we have this expression for the energy , newton 's 3rd law is purely a result in classical mechanics . by computing $f_1 = -\frac{\partial u}{\partial x_1}$ and $f_2 = -\frac{\partial u}{\partial x_2} = - f_1$ we can show that the forces on the two charges are equal and opposite .
the key thing is that you need to be working with canonically normalized fields in order to use the power counting arguments . let 's expand gr around flat space \begin{equation} g_{\mu\nu} = \eta_{\mu\nu} + \tilde{h}_{\mu\nu} \end{equation} the reason for the tilde will become clear in a second . so long as $\tilde{h}$ is " small " ( or more precisely so long as the curvature $r\sim ( \partial^2 \tilde{h} ) $ is " small" ) , we can view gr as an effective field theory of a massless spin two particle living on flat minkowski space . then the einstein hilbert action takes the schematic form \begin{equation} s_{eh}=\frac{m_{pl}^2}{2}\int d^4x \sqrt{-g} r = \frac{m_{pl}^2}{2} \int d^4x \ ( \partial \tilde{h} ) ^2 + ( \partial \tilde{h} ) ^2\tilde{h}+\cdots \end{equation} where $m_{pl}\sim 1/\sqrt{g}$ in units with $\hbar=c=1$ . $m_{pl}$ has units of mass . in this form you might thing that the interaction $ ( \partial \tilde{h} ) ^2 \tilde{h}$ comes with a scale $m^2_{pl}$ with a positive power . however this is too fast--all the qft arguments you have seen have assumed that the kinetic term had a coefficient of -1/2 , not $m_{pl}^2$ . relatedly , given that $m_{pl}$ has units of mass and the action has units of $ ( mass ) ^4$ , the field $\tilde{h}$ is dimensionless , so it is clearly not normalized the same way as the standard field used in qft textbooks . now classically , the action is only defined up to an overall constant , so we are free to think of $m_{pl}^2$ as being an arbitrary constant . however , in qft , the action appears in the path integral $z=\int d\tilde{h}e^{is [ \tilde{h} ] /\hbar}$ ( note the notational distinction between $\tilde{h}$ and $\hbar$ ) . thus the overall constant of the action is not a free parameter in qft , it is fixed and has physical meaning . alternatively , you have to remember that the einstein hilbert action will ultimately be coupled to matter ; when we do that , the scale $m_{pl}$ sitting in front of $s_{eh}$ will not multiply the matter action , and so $m_{pl}$ sets the relative scale between the gravitational action and the matter action . the punchline is that we can not simply ignore the overall scale $m_{pl}^2$ , it has physical meaning ( ie , we can not absorb $m_{pl}$ into an overall coefficient multiplying the action ) . on the other hand , we want to put the action into a " standard " form where the overall scale is not there , so we can apply the normal intuition about power counting . the solution is to work with a " canononically normalized field " $h$ , related to $\tilde{h}$ by \begin{equation} \tilde{h}_{\mu\nu} = \frac{h_{\mu\nu}}{m_{pl}} \end{equation} then the einstein hilbert action takes the form \begin{equation} s_{eh} = \int d^4 x \ ( \partial h ) ^2 + \frac{1}{m_{pl}} ( \partial h ) ^2 h + \cdots \end{equation} in this form it is clear that the interactions of the form $ ( \partial h ) ^2 h$ have a " coupling constant " $1/m_{pl}$ with dimensions 1/mass , which is non-renormalizable by power counting in the usual way .
i asume $||\vec{p}|| = mg$ ( the weight ) . we also know that $||\vec{r_n}|| = mg*\cos\alpha$ as the block ins't sinking into the rail . now substituting we get $\|\vec{f_f}\|=\mu mg*\cos\alpha$ acting leftwards and $ mg*\sin\alpha$ acting rightwards . as right is the positive direction we get : $mg*\sin\alpha - \mu mg*\cos\alpha $ this is the resultant force , dividing it by mass and rearranging we get : $$a_{x'}=g\sin\alpha ( 1-\frac{\mu}{\tan\alpha} ) $$ which is your answer .
the deal here is that you have to be very careful about what quantity you are interested in and what you have . assumptions you have measurements $f_i$ of the fraction of stuff ( does not really matter what ) during time interval $i$ . you may also have measurements of $o_i$ of the total output of the medium in which stuff makes up a fraction . alternately you may only know $\bar{o}$ the average output or the total output over the entire time range $\mathcal{o}$ . note that for $n$ measurements at uniform spacing $\mathcal{o} = n * \bar{o}$ . case 1: you want to know " how much stuff " totaled over several time periods and you have both fractional values and outputs for every period . this is as good as it gets . you need to add up the daily stuff . the daily amount of stuff is $s_i = f_i o_i$ . that is just the definition of $f_i$ . so assuming you have the daily outputs you get : $$ \mathcal{s} = \sum_i s_i = \sum_i f_i o_i \quad . $$ case 2: you want to know " how much stuff " totaled over several time periods , but you have the total output or average output without periodic output values . the best you can do is $$\mathcal{s} \approx \sum_{i=1}^n f_i \bar{o} = \mathcal{o} \frac{1}{n}\sum_{i=1}^n f_i = \bar{o} \bar{f}$$ where $\bar{f}$ is defined by this relation as the mean fraction . in this case it makes sense to take a mean of the fractional reading , but only because you do not have enough data to get the right answer . this will be approximately correct if ( 1 ) the $o_i$s have small variance or ( 2 ) the $f_i$s have a small variance or ( 3 ) you have a lot of data and the $f_i$s and $o_i$s are uncorrelated . case 3: you want to show that the fraction is uniformly above or below some limit . then you just need the $f_i$s , and computing their mean and variance is fine as long as ( 1 ) the answer is a long way from the limit and ( 2 ) the $f_i$ are uncorrelated with the $o_i$s . case 4: you want to show that the stuff is uniformly above or below some limit . then you need the daily values to do it right . in the absence of the daily output you can fall back on the average output again , but it will only be reliable if all three conditions listed for case 2 apply . side note : you were concerned in the comments that 2 million parts per million does not make sense . and you are partially right . you can not have a fractional concentration of 2 million ppm , but there are cases when that value is meaningful . you probably already understand this . people are happy to talk about 200% in some cases , but " percent " is literally " per one hundred " . two hundred pars per hundred is the number 2 and it makes sense in cases involving changes but not in cases involve the fraction of people who qualify for something .
note that you can write $g$ as $g = d \eta d$ , where : $\eta$ = diag$ ( 1 , -1 , -1 , -1 ) $ , $d$= diag $ ( 1 , ch ( ht ) , ch ( ht ) sin ( \rho ) , ch ( ht ) sin ( \rho ) sin ( \theta ) ) $ so you have : $t ( d \eta d ) t = d \eta d$ , that is $ ( d^{-1}td ) \eta ( dtd^{-1} ) = \eta$ that is : $ ( dtd^{-1} ) ^t\eta ( dtd^{-1} ) = \eta$ , because $d$ and $t$ are symmetric . so $dtd^{-1}$ is an element of $so ( 1,3 ) $ so , the set $s$ of matrix $t$ , is the symmetric part , of the set $\sigma$ of matrix $d^{-1}xd$ , where $x$ is an element of $so ( 1,3 ) $ . an obvious subset of $s$ is the set $s'$ of symmetric matrix of $so ( 1,3 ) $ which commute with $d$ ( in this case $t =x$ ) .
i think the answer is it depends on distance ( relative to the size of your system ) . another well known example of a boson which is comprised of fermionic components is the helium-4 atom , which has integer spin ( both the nucleus and the neutral atom itself ) . fermionic or bosonic behavior of a composite particle ( or system ) is only seen at large ( compared to size of the system ) distance . at proximity , where spatial structure begins to be important , a composite particle ( or system ) behaves according to its constituent makeup . for example , two atoms of helium-4 cannot share the same space if it is comparable in size to that of the inner structure of the helium atom itself ( ~10−10 m ) —despite bosonic properties of the helium-4 atoms . thus , liquid helium has finite density comparable to the density of ordinary liquid matter . ( taken from here . ) i think this provides a concrete example of what you were asking . hopefully this helps .
depends on what you mean by ' central force ' . if your central force is of the form ${\vec f} = f ( r ) {\hat r}$ ( the force points radially inward/outward and its magnitude depends only on the distance from the center ) , then it is easy to show that $\phi = - \int dr f ( r ) $ is a potential field for the force and generates the force . this is usually what i see people mean when they say " central force . " if , however , you just mean that the force points radially inward/outward , but can depend on the other coordinates , then you have ${\vec f} = f ( r , \theta , \phi ) {\hat r}$ , and you are going to run into problems finding the potential , because you need $f = - \frac{\partial v}{\partial r}$ , but you will also need to have $\frac{\partial v}{\partial \theta} = \frac{\partial v}{\partial \phi} = 0$ to kill the non-radial components , and this will lead to contradictions . it is logical that a field of this form is gong to be nonconservative , because if the force is greater at $\theta = 0$ than it is at $\theta = \pi/2$ , then you can do net work around a closed curve by moving outward from $r_{1}$ to $r_{2}$ at $\theta = 0$ ( positive work ) , then staying at $r_{2}$ constant , going from $\theta =0 $ to $\theta = \pi/2$ ( zero work--radial force ) , going back to $r_{1}$ ( less work than the first step ) , and returning to $\theta = 0$ ( zero work ) .
i am not sure if a condensed matter book is going to give you what you want : as pointed out by commenters , you cannot derive a lagrangian , you can only justify it because it represents the correct physics . but here is a simple interpretation of the 3rd order term . for small deformations , hooke 's law holds and the restoring force $f_{a}=−k_{ab}q_b$ . ( for isotropic systems this reduces to the familiar $f_a = -kq_a$ . ) but for larger deformations ( beyond the proportionality limit ) you get non-linear corrections to the spring constant $\delta k_{ab} \sim g_{abc} q_c$ , where $g_{abc}$ is some material-dependent constant . so the constant $g_{abc}$ quantifies how much the stress acting on the springs alters their springiness . this makes perfect sense : the presence of excitations of the field ( mattress ) alters the way the excitations move , i.e. a self-interaction of the field ( or once you quantise , quanta of the field interacting with each other ) . in particular , you can see that if two wavepackets collide , the increased amplitude of the deformation will alter the effective spring constant at the impact point , resulting in scattering effects . in the absence of the non-harmonic terms the wavepackets would just pass right through each other .
" equinox of date " means that in accounting for general precession , the equinox ( or roughly speaking , the origin of the equatorial and ecliptic coordinate systems ) is that for the date for which a computation is performed . alternatively , some standard equinox ( e . g . j2000.0 ) is used . catalogs are always referred to a standard equinox whereas ephemerides are ( usually ) referred to the equinox of date . the difference is significant when pointing a telescope .
since this is homework , we are not supposed to give you the answer . but one mistake you made is in your formula for the magnitude of $r$ - the inner square root needed to be squared . so the length of $r$ is simply the square root of the sum of the squares of the $i$ , $j$ and $k$ lengths . good luck . . .
your equation ( 1 ) describes approximately the centre-of-mass ( com ) coordinates of every atom = ( some nucleons + some electrons ) system . of course there are many other degrees of freedom that are not taken into account in this description . but those degrees of freedom can always be ignored unless they become correlated with the centre-of-mass coordinates . this will happen if , for example , molecules form , or if inelastic collisions result in transitions between internal electronic states . at low densities and ultracold temperatures ( the normal regime for alkali atom bec experiments ) , such processes are strongly suppressed . this is why the internal degrees of freedom are normally not written in the wavefunction explicitly . if you want to write them in , you would have something like $$ \psi^{\prime} ( \mathbf{r} , \mathbf{s} ) = \psi ( r_1 , r_2 , \ldots ) \times \phi ( s_1 , s_2 , \ldots ) , $$ where $\psi ( \mathbf{r} ) $ is given by your equation ( 1 ) and $\phi ( \mathbf{s} ) $ describes all the ( relative ) coordinates $s_1 , s_2 , \ldots$ of the electrons and nucleons . the lack of correlations between com and internal states means that $\phi ( \mathbf{s} ) $ appears as a boring common factor multiplying all expressions , so it can just be thrown away* . the internal degrees of freedom are nevertheless extremely important since they result in interactions between the atoms . this interaction can be modelled fairly well by a lennard-jones type potential . this can be understood intuitively as a sum of two parts : 1 ) a long-range attraction between the nucleons and electrons , 2 ) a short-range coulomb/pauli repulsion once the electron orbitals start to overlap . however , this intuition only makes sense within the born-oppenheimer approximation , in which the quantum correlations between nuclear and electronic degrees of freedom are neglected . in principle , this interaction is a horrendously complicated many-fermion problem . even for just a single pair of $^{87}$rb atoms , the full scattering problem taking all the nuclear and electronic degrees of freedom could not be solved on even the biggest supercomputer in the world . fortunately , there is never enough motional energy ( at low temperatures ) in the centre-of-mass degrees of freedom to cause transitions between the internal states , meaning that there is no chance for the internal structure to become correlated with the external motion . it is therefore perfectly fine in practice to treat the atoms as structureless blobs of matter , with some effective classical potential replacing the complicated quantum motions and interactions of their internal parts . the important role of the interatomic scattering interaction in cold atom experiments is that it correlates the centre-of-mass coordinates $r_1 , r_2 , \ldots$ of the atoms . since your equation ( 1 ) neglects these correlations , it can only be an approximation . what you have written is called the gross-pitaevskii ( gp ) approximation for the ground state . this is the zeroth-order term in a systematic expansion of the ground state in terms of the parameter $ ( na_s^3 ) ^{1/2}$ , where $n$ is the density and $a_s$ is the $s$-wave scattering length . the next order of approximation is called the " bogoliubov vacuum " , and takes the form $$\psi ( r_1 , r_2 , \ldots ) = \prod_{i&lt ; j} \psi ( r_i-r_j ) . $$ the reason that the gp approximation normally gives good results is that the pair wave function $\psi ( r ) $ only has a non-trivial behaviour for very small $r$ , i.e. less than 100 nm . if the spatial resolution of your measuring device is larger than this , which is normally the case for diffraction-limited light scattering , the gp and bogoliubov ground states give equivalent predictions . a good list of references on these issues can be found in the leggett rmp is a good source . leggett also wrote an interesting open-access paper on similar topics . *of course , if you assume that the positions of the electrons or nucleons could be measured , then $\phi ( \mathbf{s} ) $ would need to be taken into account .
no , it is not . your system will go through the same point twice in every oscillation , once moving in each direction , and the friction force will be reversed in each pass , so your approach does not work . what you need to consider is the velocity , not the displacement , so $$ma=-kx - \mathrm{sign} ( v ) f_{\mathrm{fric}} . $$ this is not all that helpful in actually figuring out the motion , and to solve that equation you will have to break it down into several parts . also , if static and dynamic friction are different , your mass will stop at its maximum elongation , and you will then have static friction again . this is what causes stick-slip vibrations .
you have used the right formula for the centripetal force , but the wrong radius . you have used $l$ as radius but it should be $\frac{l}{2}$ . redo the same calculation and you should get the right answer .
to modify the distribution of magnetic field ( irrespectively of the source - pms or coils ) , you need some material with non-unit magnetic permeability , such as steel . these materials ' concentrate ' the field lines , pulling them in away from the surrounding air ( which has a permeability of very nearly 1 ) . so to shield the outside world , you had need to create a ' circuit ' of permeable material ( let 's say steel ) , which channels the magnetic flux from the north pole of your magnet to its south pole . if you leave a little gap in one part of your circuit , the field will be enhanced in the gap .
these are basically ways of describing which leading diagram contributes the largest term to the cross-section ; they are named after which of the mandelstam variables characterizes the 4-momentum of the virtual particle . in t- and u-channel processes the 4-momentum of the exchange particle is space-like ( has a negative norm ) . in the s-channel the 4-momentum of the exchange particle is time-like ( has a positive norm ) . this necessitates the annihilation of the incident particles .
that di water has some specific resistance , not resistance ! to measure that , you need a apropriate probe , inserting the tips of a vanilla ohmmeter into the water , excuse , is ridiculous ! conductivity measurements have to be made using alternating currents . the frequency is about some khz to some dozen khz . for this purpose you can buy special meters , called conductometers , ( in electrochemistry one deals with conductance , not resistance ) at a range of cheap to luxurious .
it is obviously not a sharp cut-off , but as a general guide sound waves cannot propagate if their wavelength is equal to or less than the mean free path of the gas molecules . this means that even for arbitrarily low pressures sound will still propagate provided the wavelength is long enough . possibly this is stretching a point , but even in interstellar gas clouds sound waves ( more precisely shock waves ) will propagate , but their length scale is on the order of light years .
why is 100k rpm an issue ? dental drills works at around 10 khz , with is about 600k rpm . :- if you look at the seiko patent for the design of the kinetic , the drive-train steps up the rotation of the spinning weight by 100 fold for the generator . when i charge my watch i shake it at around 5 - 10 hz , which means 0.5-1 khz for the small pinion , which gives 30 - 60k rpm indeed . so i do not think the wikipedia article is that far off the mark . when things are really small and light and precision made , i think you do not have to worry as much about heat dissipation and wear . ( a typical hard-drive disk is about a couple inches in diameter . the pinion can not be more than a 10th of an inch , if you allow two steps to up the rotation speed . )
resistance can be taken as $$r = \rho \frac{l}{a}$$ where $rho$ is resistivity , $l$ is length of resistor and $a$ is cross sectional area of the resistor . the above is asking you for a ratio of $r_1:r_2$ and you are given all the numbers in the question . shapes within a circuit diagram can typically be ignored , the diagrams are only symbolic to inform you of how the pieces are connected .
i will take a swing at this , but bear in mind that you probably will not get definitive answers because you are asking about two active and difficult areas of research ( pop iii star formation and re-ionization ) . i will answer the particular questions , but i am hoping you get a feel for the fact that we do not have clear-cut answers yet . during what range of years after the big bang did the stars form ? the consensus is somewhere in the range $20&lt ; z&lt ; 50$ , which corresponds to 50 myr $&lt ; t&lt ; $ 200 myr . the spread is quite large because star formation depends broadly on gas density and temperature . because the big bang produces some big density perturbations and some small ones , they will form stars at different times . what is the expected range of masses of these stars . . . this is really , really hard to answer . until about a year ago , consensus was settled on a very heavy mass distribution , with many ( if not most ? ) stars in a range 100-1000 $m_\odot$ ( see e.g. the 2004 review by bromm and larson ) . this is argued on the grounds that the smallest gravitationally unstable mass in a homogeneous isothermal gas , the jeans mass , runs like $t^{3/2}$ and primordial gas cannot cool as much as metal-polluted gas . the difference in temperature is about a factor of 30 , so pop-iii stars would naively be about 100 times heavier than modern stars , whose mass distribution seems to peak around 0.5 $m_\odot$ . however , recently ( as in , articles in science in the last few weeks ) have reported very detailed simulations of early star formation where the stars stop their own growth as they start to radiate . the result is stars that are a few times 10 $m_\odot$ . still very big by modern standards , but not as big as previously thought . so the jury is out , imo . . . . and what is the expected lifetime before they supernova ? well , it depends on how large they are , but broadly stars of about 40 solar masses last about a million years or less . how they evolve is unclear because it is difficult to compute the rate at which they might lose mass from their surfaces . i assume these stars resulted in the re-ionization of ism ( interstellar medium ) , so what is the evidence and estimates of the age of the re-ionization era ? the role of pop iii stars in reionization is not at all clear . this is slightly outside of my own work , but from my own knowledge i think it is quite well known that ionization was complete by about redshift $z\approx6$ , which is about a billion years after the big bang . remember that many pop iii stars could have already been born , evolved , and died , producing enough metals to produce pop ii stars in the next generation . working out just how much radiation had been poured out is theoretically very difficult . however , as far as i know , observations of a $z=7.085$ quasar have given us some idea that the intergalactic medium around it was not re-ionized . so its more likely that stars after pop iii ( and maybe agn/quasars ? ) had more to do with re-ionization because it happened broadly later than they were born . that is just wild speculation on my part , though .
let $q$ denote the set of all possible configurations of the system ( the configuration manifold ) . consider a point $q_0\in q$ . for the sake of conceptual clarity , and to make contact with physics notation , let 's work in some local coordinate patch around $q_0$ . suppose that $q_0$ represents the position of the system under consideration at time $t_0$ . at a given time $t$ later , the system will be at some position say $q ( t ) $ that is determined by the evolution equations ( the euler-lagrange equations if we are doing lagrangian mechanics ) , and the quantity \begin{align} q ( t ) - q ( t_0 ) = q ( t ) - q_0 \end{align} would be the displacement of the system after a time $t$ . suppose , instead we consider some other curve $\gamma ( s ) $ in the configuration space which starts at the point $t_0$ ; \begin{align} \gamma ( s_0 ) = q_0 , \end{align} and suppose that we compute the displacement \begin{align} \gamma ( s ) - \gamma ( s_0 ) = \gamma ( s ) - q_0 \end{align} that would result from moving along this other curve of our choosing . we call this displacement the virtual displacement after a time $t$ corresponding to moving along the curve $\gamma$ . it is called virtual because it is the displacement in the position of the system that would occur if the system were to move along the curve $\gamma$ of our choosing -- a " virtual " curve as opposed to the " real " curve along which the system travels according to the lagrangian evolution of the system . note . i used the parameter $s$ for the curve $\gamma$ instead of $t$ to emphasize that moving along that curve does not correspond to time-evolution . now what about virtual " infinitesimal " displacements ? well , recall that the term " infinitesimal " in physics essentially always refers to " first order " approximations , see , e.g. this se post : rigorous underpinnings of infinitesimals in physics so when we are discussing a virtual infinitesimal displacement , what we have in mind is taking the virtual displacement $\gamma ( s ) - q_0$ , taylor expanding it to first order in $s$ , and extracting only the first order term . let 's do this : \begin{align} \gamma ( s ) - q_0 = \gamma ( s_0 ) + \dot\gamma ( s_0 ) t + o ( s^2 ) - q_0 \end{align} using the fact that $\gamma ( s_0 ) = q_0$ , we see that the taylor expansion of the virtual displacement is \begin{align} \gamma ( s ) - q_0 = \dot\gamma ( s_0 ) t + o ( s^2 ) , \end{align} and now we notice that to first order in $s$ , the size of the virtual displacement is controlled by the coefficient of $s$ , namely $\dot\gamma ( s ) $ . in other words , virtual infinitesimal displacements ( meaning we just keep the first order contribution in $s$ ) , are determined by the velocity vector of the chosen " virtual curve " at $s_0$ . but if you have taken a differential geometry course , then you know that velocities of curves on a manifold are simply tangent vectors to that manifold ! so virtual infinitesimal displacements can be associated with tangent vectors to the configuration manifold . the intuition to keep in mind here as that a virtual displacement just tells us how far we would get away from a certain point on the manifold if we were to travel on a certain curve of our choosing that may not coincide with the actual motion of the system determined by time evolution . the " infinitesimal " part and identifying this part with tangent vectors comes simply from considering what happens only to first order .
the static air pressure seen by the aircraft does not change with the aircraft 's velocity . your confusion is from a common misinterpretation of bernoulli 's principle . it is not true that a fluid 's pressure will decrease simply by virtue of flowing faster . after all , this violates the idea that physics should be the same in all inertial frames . here is a simple counterexample to the typical interpretation of the bernoulli principle . consider a tube of infinite length and uniform diameter with some gas sitting in it . now consider various coordinate systems with a velocity in the direction of the tube . in these different coordinate systems , the velocity of the gas will be different , but we expect the force on the walls of the tube due to the fluid 's pressure to be the same in all cases . ( the tube is not going to rupture simply because of a choice of coordinate system ! ) instead , bernoulli 's principle says that , in a given flow ( say , along a streamline ) , a local increase in velocity is associated with local decrease in pressure . the canonical example is fluid flow through a tube with a constriction ( a venturi ) . quoting from the wikipedia article for bernoulli 's principle : bernoulli 's principle can be derived from the principle of conservation of energy . this states that , in a steady flow , the sum of all forms of mechanical energy in a fluid along a streamline is the same at all points on that streamline . this requires that the sum of kinetic energy and potential energy remain constant . . . . if a fluid is flowing horizontally and along a section of a streamline , where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure ; and if its speed decreases , it can only be because it has moved from a region of lower pressure to a region of higher pressure . consequently , within a fluid flowing horizontally , the highest speed occurs where the pressure is lowest , and the lowest speed occurs where the pressure is highest . note the emphasis on relative changes occurring on a streamline . the specific flaw in your argument is here : now the fluid is accelerated along a streamline , and hence the static pressure should drop according to the relation given above . in the wind tunnel , something has to do work to accelerate the air to the wind tunnel velocity , adding energy to the flow , which violates the conservation of energy assumption in bernoulli 's principle .
boiling point of water changes with altitude because atmospheric pressure changes with altitude . so , how/why boiling point changes with pressure . there is good explanation of this at hyperphysics ( with diagrams ) . now , why does pressure change with altitude ? imagine you are swimming in water . deeper you go more pressure you feel , because there is more water above you . $p = p_0 + dh$ $p$ - pressure , $p_0$- pressure at the surface , $d$ - density of fluid , $h$- height/depth to free surface there is huge column of air above our heads . $\rightarrow h$ at sea level > $h$ at hill station . $\rightarrow p$ at sea level > $p$ at hill station . $\rightarrow$ boiling point at seal level > boiling point at hill station .
if you look closely at the crocodiles ' tails you will see that they wave their tails from side to side to provide propulsion for the jump . compare this to a fish swimming : the side to side motion of the fish 's tail propels it forward , and the crocodiles are using exactly the same sort of side to side motion to propel themselves upwards .
from the lagrangian one can obtain the equations of motion , called the euler-lagrange equations . these equations are , in general and also in this case , differential equations . as far as i understand your level of knowledge , you do not know anything about this subject ? in differential equations you are looking for a whole function as a solution , not only a variable ( in this case the solution is the function you are looking for : plug in the time and get out the position of each mass ) . the functions not only occur in an algebraic equation , there are also derivatives of this function in this equations . so in your case the two functions you are looking for are $\theta_1 ( t ) $ and $\theta_2 ( t ) $ . unfortunately you can not analytically solve these equations stated in the wiki article : " it is not possible to go further and integrate these equations analytically , to get formulae for θ1 and θ2 as functions of time . it is however possible to perform this integration numerically using the runge kutta method or similar techniques . " this means : without numerics you have no chance , at least as far as i know .
the equations hold due to the non-trivial property of the uniqueness of the taylor expansion . consider the left and right hand side of an equation as a function , \begin{equation} f ( \lambda ) =g ( \lambda ) \end{equation} now taylor expand both sides , \begin{equation} f_0 + \lambda f_1 + \lambda^2 f_2^2 + . . . =g _0 + \lambda g _1+ \lambda^2g _2^2 + . . . . \end{equation} since the taylor expansion is unique " both ways " of taylor expanding are equivalent and we must have \begin{equation} f_i = g_i \end{equation} for all $i$ . intuitively you can understand the statement as follows . the equation must hold at all $\lambda$ so it must hold at $\lambda=0 \rightarrow f_0=g_0$ . now suppose that you increase $\lambda$ a tiny bit . in this case all the $\lambda^2 , \lambda^3 , . . . $ terms are going to be very small . at this point we have , \begin{equation} \lambda f_1\approx \lambda g_1 \end{equation} so we must have $f_1=g _1$ ( since we can make $\lambda$ arbitrarily small ) . you can continue the process iteratively to show that you can equate coefficients of a taylor expansion to all orders .
it is not necessarily true . for a zero potential $v_2$ you have $p_2=0$ , whereas if $v_1$ is a rectangular pit , in general , $p_1&gt ; 0$ .
the opposite sign of the shifts is due to the conservation of the location of the center-of-mass or , equivalently , momentum conservation . at least for the simple system of 2 bodies , the animation on the page is being observed from the inertial frame of the center of mass . one may check this fact by seeing that the trajectories are periodic ( ellipses ) even if the masses are comparable and the initial velocities and locations are generic ( but allowing a bound state ) . in the center-of-mass frame , the total momentum of both bodies is zero . so if one of them moves in one direction , the other is moving in the opposite direction . their trajectories are really ellipses that are similar to each other ( one obtains one from another by scaling , multiplication by a negative number ) . it follows that if the apsis of one body is on one side from the center of mass , the apsis of the other body must be on the other side .
given a volume $\delta x$ ( i am assuming linear density here ) , the number of particles that get absorbed is going to be the number of particles in a given volume times the probability that an absorption occurs . this is just the density times the cross section , $n_0 \sigma$ . thus , the change in the number of particles crossing through that volume is the product of the probability that a neutron will got absorbed with the total number of neutrons in the volume \begin{equation} n_f = n_i - n_i ~n_0 \sigma ~\delta x \end{equation} for a one-dimensional problem . in the $\delta x \rightarrow 0$ limit , this becomes the differential equation \begin{equation} \frac{d n}{d x} = - n_0 \sigma ~ n \end{equation} whose solution is just the exponential decay .
the experiment is correct . well , what is really correct is the final interpretation of the experiment . radiation gets redshifted in the gravitational fields . it is one of the well-known consequences of general relativity . it is been known theoretical since 1915 or so and directly and safely experimentally verified in the 1960s . all the " magic " terminology on the page is only meant to express the idea that the energy and the total relativistic mass must scale in the same way because they are related by $e=mc^2$ , the well-known identity in special relativity . thanks to this identity , the conversion of matter into energy and vice versa is not really " magical " in any sense . it may be done . when a particle ( electron ) and an antiparticle ( positron ) annihilate , they are indeed converted to a pair of photons . the gravitational potential may be used to accelerate the electron and positron against each other , for example , and the extra kinetic energy means that the annihilation will result in higher-frequency photons when it is done at a lower altitude , deeper in the gravitational field . because the energy conservation law says that it does not really matter whether we annihilate the electron-positron pair at a higher altitude , or we do so at a lower altitude , and transfer the photons back to the higher altitude , it follows that the photon has to lose some of its energy ( the extra energy from the additional kinetic energy of the electron and positron ) when it is flying up i.e. escaping from the gravitational field . that is the gravitational red shift . the energy , total mass , as well as the frequency get reduced by the factor $\sqrt{g_{00} ( a ) /g_{00} ( b ) }$ between the points a and b .
( 1 ) the first law is written in form of differentials themselves , so i think there may be no escape from using differential equations . ( 2 ) the way most commonly the first law is written is , $du=dq-dw_{\text{work done by the system}}$ . here dw is work done by the system . however , in subjects other than physics , more important quantity is the work done by the experimenter . ( this is quite common in chemistry ) as the process in thermodynamics are most " quasistatic" ( http://en.wikipedia.org/wiki/quasistatic_process ) , the container/piston is always in equilibrium . so , $\vec{f_{ext}}=-\vec{f_{int}}$ ( they are equal and opposite ) , then we have , $dw_{\text{by the system}}=-dw_{\text{on the system}}$ . and so the first law can be written as : $du=dq+dw_{\text{work done on the system}}$ . here dw is work done on the system . as the two $dw$s have different meanings we will not a different answer . ( 3 ) the internal energy of a gas is state variable/state function ( http://en.wikipedia.org/wiki/functions_of_state ) . $u$ depends only on the final and initial states of the system and not on what process was used to get from initial to the final state . so we can use a constant volume process to get $du$ which then can be used in any process without any modification .
try explicitly calcultaing $l_z^{\dagger}$ from the differential representation you correctly gave for $l_z$ . remember that the derivative is by definition an antihermitian operator ( ie $\partial_x^{\dagger}=-\partial_x$ ) which relation you find between $l_z$ and $l_z^{\dagger}$ ? how is this useful to solve your problem ?
there does not need to be an magnetic field in the inductor for there to be " back emf " ( i would prefer " induced emf" ) . the induced emf is the consequence of a changing magnetic field and not of a magnetic field itself and hence there can be a changing magnetic field even at zero magnetic field ( something like a positive acceleration downwards for a ball thrown upwards , momentarily at rest . velocity is zero but the rate of change is not ) . the induced emf is given by $e=-\frac{d\phi}{dt}=-l\frac{di}{dt}$ , where $\phi$ is the magnetic flux through the circuit ( inductor ) . as a matter of fact , in a simple ac generator , which works on the principal of electromagnetic induction , the value of the induced emf is maximum when the magnetic flux through the loop of the generator is zero . now , to derive an equation of the current as a function of time , at any time t:- $$e-ir=l\frac{di}{dt}=-e_i$$ where $e$ is the emf of the ideal battery and $e_i$ is the induced emf . rearranging the equation and integrating:- $$\int_0^t dt=\int_0^{i_s}\frac{l}{e-ir}$$ where $i_s$ is the current at infinite time , i.e. at steady state where there is no longer changing magnetic fields and hence no induced emf . this is given by $i_s=e/r$ since the inductor has no effect at steady state . solving the above equation gives us:- $$i ( t ) =i_s ( 1-e^{-\frac{t}{\tau}} ) $$ where $\tau=l/r$ is called the time constant . at time $t=0$ , the current is zero but the rate of change of magnetic field is non zero and hence the induced emf is equal to the battery emf ( the maximum value ) . as time passes , the induced emf reduces slightly , and the current starts slowly and rises steadily till it reaches the steady state at $t\rightarrow \infty$ . you can get the expression for the induced emf as $$e_i=-l\frac{di}{dt}=i_sre^{-\frac{t}{\tau}}$$ the back emf acts as an opposing emf ( principally like a battery of varying emf fixed in an opposing direction to the original battery ) , and its value is maximum at the beginning ( equal to $e$ ) and hence there is zero current , and its value starts dropping as the rate of change of magnetic field starts dropping exponentially , and becomes zero at steady state ( $t\rightarrow \infty$ ) where the rate of change of magnetic field is zero .
$\hat r , \hat \theta , \hat z$ form an orthogonal , right-handed triad of basis vectors in 3d . taking the cross-product with them is no more exotic or unusual than doing so for cartesian basis vectors . $\hat r \times \hat \theta = \hat z$ , and so on .
the light you see as the image of the sun on the sky is basically undeflected . http://en.wikipedia.org/wiki/diffuse_sky_radiation says it is 75 % when the sun is high and the sky is clear . the frequency dependency is due to rayleigh scattering . for the cloudy sky the fraction is much smaller , up to many orders smaller than unity ( maybe 1 millionth part as a wild guess ) .
i would rephrase your question as : what is the experimental signature of a black hole . if there exists a definite experimental signature of a celestial body that defines a black hole , your question is answered . i found the following paper that clarifies the issue : classical black holes are solutions of the field equations of general relativity . many astronomical observations suggest that black holes really exist in nature . however , an unambiguous proof for their existence is still lacking . neither event horizon nor intrinsic curvature singularity have been observed by means of astronomical techniques . this paper introduces to particular features of black holes . then , we give a synopsis on current astronomical techniques to detect black holes . further methods are outlined that will become important in the near future . for the first time , the zoo of black hole detection techniques is completely presented and classified into kinematical , spectro-relativistic , accretive , eruptive , obscurative , aberrative , temporal , and gravitational-wave induced verification methods . principal and technical obstacles avoid undoubtfully proving black hole existence . we critically discuss alternatives to the black hole . however , classical rotating kerr black holes are still the best theoretical model to explain astronomical observations . from this it is evident that in contrast to most physics subjects where first there is experimental evidence needing explanation and then theory arrives , black holes are an " artefact " of general relativity theory , i.e. the concept carries all the baggage of gr . nevertheless , what is called a " black hole " in gr has some experimental signatures which any other competing gravitational theory would have to account for . it might not be in the enticing format of a " black hole " , but some data are there . these at the moment are consistent with the theoretical black hole expected from gr . so in a sense your question has as answer : " yes the definition of a black hole is within the terminology introduced by general relativity " and may not be defined outside it ; but also " no the experimental signatures do not depend on general relativity in order to exist , just their interpretation and attribution as a black hole " may be in question , if an alternative theory to gr succeeds to describe reality .
( 1 ) the equation $pv=nrt$ is a " proportionality " type of equation , which necessarily has a linear dependence when only one variable is independent . the class of problems your are dealing with now has only one independent ( you control the change ) and one dependent ( the equation tells you the change ) variable , while the others are held constant . and from a pure math perspective , for it to be a non-linear dependence with only one independent variable there would need to be operators other than multiplication/division ( e . g . exponent , logarithm , sin , cos , etc . ) . ( 2 ) when you are equating two gas states you are really solving the equation : $$ \frac{p_1 v_1}{nrt_1}=\frac{p_2v_2}{nrt_2} $$ and for the case you are looking at $n_1=n_2 $ the variable $n$ is on both sides and cancels . so in this case you are not multiplying either side by $n$ . when you are only looking at one gas state , you are not setting equations equal to each other and there is no cancellation , so you still have the variable $n$ .
internal forces are the only contributors to potential energy . potential energy is the energy associated with the configuration ( relative positions ) of a collection objects . the potential energy of a single point particle is not defined . as the configuration of the system changes , its potential energy changes according to the definition $\delta{\mathrm{ ( p . e . ) }} = -w_\mathrm{internal}$ where $w_\mathrm{internal}$ is the work done internally , that is , by the various internal forces against the internal components of the system . in order to define potential energy , you need to have internal components applying forces to one another , and the components need to be able to move relative to one another . you need at least two objects . consider the system consisting of only the dumbbell . you have implicitly modeled the dumbbell as a point particle . that is , the only attributes of interest to your analysis are its position and its mass . the internal structure , which would be of interest in other questions , say , concerning temperature , is ignored . you have applied two external forces , gravity , and the contact force of your hand . not only is there no change in potential energy , there is no potential energy at all . potential energy does not exist in a system consisting of , or modeled by , a single point particle . i am not saying that the p.e. is zero . i am saying that it is not defined . consider the system consisting of the dumbbell and the earth . now i have two objects modeled as point particles , one external force : the force of your hand , and one internal force : the mutual gravitational attraction between the earth and the dumbbell . the internal work done in lifting the dumbbell ( that is , increasing the distance between them ) is $w_\mathrm{internal} = -mgh$ . the minus sign comes from the fact that the displacement is in the direction opposite to the direction of the force . ( gravity down , displacement up . ) so the change in potential energy of the system is $$\delta\mathrm{ ( p . e . ) } = -w_\mathrm{internal} = - ( -mgh ) = mgh$$ the external force of your hand on the dumbbell plays no role at all . one of the roots of your misunderstanding is the unfortunate choice made by almost all introductory textbooks in introducing the subject of potential energy by looking at raising and lowering objects in a static gravitational field ( at the surface of the earth ) . i know of a particular textbook that had a wonderful and correct description of potential energy . until the next edition came out , when that description disappeared , and the conventional description appeared in its place .
as you stated , the degree of green is directly dependent on the thickness of glass you stare at ( beer-lambert law ) . it actually comes from the absorption of the other wavelengths by the glass . due to refraction , even when you look at the glass from a grazing angle in the air , the light rays bend to a higher angle in the glass which makes the light path through the glass shorter ( figure 2 ) . on the contrary , when you stare at the glass from the edge , total internal reflection makes the light rays travel through the whole length of the glass to your eye ( figure 3 ) .
since electrons do not interact through the strong interaction , an electron-quark " atom " is on the face of it the same as an electron-proton atom . ( except maybe weak interaction decays , i am not entirely sure . ) however : a free quark has never been observed in experiments , and it is widely believed - but not proved - that the theory of strong interactions does not allow free quarks at all . instead , quarks bond in pairs or triplets . the most famous such particles are of course neutrons and protons . so in a sense not only can quarks form composite particles , they most likely have to . bound pairs of quarks have energy levels like atoms , but often the difference in energy is very large . it is the strong interaction after all . the energy difference can be so large that particle physicists have several names for particles that have the same constituent parts .
you need to look at the pt phase diagram for water e.g. http://www.standnes.no/chemix/english/phase-diagram-water.htm at temperatures below the critical point the water will boil until the pressure rises enough to bring the water and vapour into equilibrium , so you will have a mixture of water and steam . above the critical point water and steam merge into a single phase i.e. there is no distinction between the liquid and vapour .
if you take the classical analogy of a charge generating field lines then the force at some point can be taken as the density of field lines at that point . in 3d at some distance $r$ the field lines are spread out over a spherical surface of area proportional to $r^2$ so their density and hence force goes as $r^{-2}$ - so far so good . the trouble with the strong force is that the interactions between gluons cause the field lines to attract each other , so instead of spreading out they group together to form a flux tube or qcd string . in effect all the field lines are compressed into a cylindrical region between the two particles so the field line density , and hence the force , is independant of the separation between the quarks . this means it does not matter what the dimensionality of space is , because the field lines will always organise themselves along the 1d line between the quarks . the quarks woould be confined in any dimension space . annoyingly i can not find an authoritative but popular level article on qcd flux tubes , but a google will find you lots of articles to look through .
it comes from the normalization of the polyakov action , \begin{align*} s=\frac{t}{2}\int d^2\sigma\ , ( \dot{x}^\mu\dot{x}_\mu-{x'}^\mu{x'}_\mu ) . \end{align*} the canonical momentum is \begin{align*} \frac{\partial \mathcal{l}}{\partial \dot{x}^\mu}=t\dot{x}_\mu , \end{align*} and this gives the equal time commutator ( or poisson bracket ) that you wrote down , \begin{align*} [ x^\mu ( \tau , \sigma ) , \dot{x}^\nu ( \tau , \sigma' ) ] =\frac{i}{t}\eta^{\mu\nu}\delta ( \sigma-\sigma' ) . \end{align*} the way i think about this is that as the tension of the string goes to zero , the string becomes less and less classical ( alternatively , $t$ plays the role of $1/\hbar$ on the worldsheet ) .
your example has a tricky issue involving angular momentum ( see below ) , but i can address the spirit of the question using a much simpler example . let us imagine we have a chamber containing two gases , $a$ and $b$ , such that $a$-$a$ interaction terms are equal to both the $b$-$b$ interaction terms and the $a$-$b$ interaction terms . ( it does not hurt to imagine the molecules as red and blue spheres of the same size and weight . ) now we imagine swapping some of the $a$ molecules with $b$ molecules in such a way that the $a$ molecules all end up on one side of the chamber and all the $b$ molecules on the other . due to the assumption above , swapping two particles does not change the total energy , but by doing this we have created an ordered system from which work can be extracted . ( to extract work from this system you need a piston that is permeable to $a$ molecules but not $b$ molecules , and another piston that is only permeable to $b$ but not $a$ . see here for details on how , as well as some other relevant stuff about the relationship between work and entropy . ) now , since we can extract work from this system , will its weight change due to the $e=mc^2$ relation ? perhaps surprisingly , the answer is no . this is because the relevant $e$ is the internal energy of the system ( usually written $u$ in thermodynamics ) , and that has not changed . the work that can be extracted from a system at a constant temperature is given by $u-ts$ . by reordering the atoms we have reduced $s$ but kept $u$ constant . when we extract work from this ordered system , its internal energy $u$ also stays constant , but the energy of the environment reduces . effectively , we take heat out of the environment and convert it to work . normally this is not allowed by the second law , because converting heat into work would cause a reduction in entropy - but through a clever use of semi-permeable pistons we can offset that reduction in entropy by an increase in the entropy of the gas mixture . the point is that the $s$ term represents the disorder ( or , more correctly , it represents the opposite of information - see the paper linked above ) and the $u$ term represents the energy . the mass of an object depends only on the $u$ term and not on the $s$ term , so order and mass/energy are actually quite independent things . the tricky thing with your example is that although you keep the energy constant , changing the velocities in the way you describe adds angular momentum to the system . when momentum is involved , $e=mc^2$ is no longer strictly valid , and you have to use the full energy-momentum relation $$ e^2 = ( pc ) ^2 + ( mc^2 ) ^2 , $$ where $p$ is the relativistic momentum . i do not know how to do this for your example . it may or may not be the case that the effective mass would change . however , if it does change it is because the momentum has changed and not because the system has become more ordered .
i assume you are looking at a diode laser ( ld ) data sheet . in practical terms , spectral width is a measure of tunability of the ld as you vary injection current and temperature . this is quite useful in experiments ( say atomic physics with alkali atoms ) . the linewidth is related to the phase noise of laser . it is very complicated to derive the linewidth from first principles . agarwal is a standard reference if you wish to know more . a practical semi-conductor ld has a typical free running linewidth of 40mhz . this means , the uncertainty in the frequency is 40mhz . however , this entire frequency band can shift with changes in temperature and fluctuations in injection current . this rather broad linewidth can be narrowed by optical feedback to obtain the so called external/extended cavity diode laser ( ecdl ) . check out this seminal paper by weimann and hollberg ( pdf ) for more information . i have built many ecdl systems from the ground up with sub-mhz linewidths . it can be quite a tricky endeavor . measuring the linewidth can be quite tricky . the usual direct technique is to beat the unknown laser with a known standard reference and study the beat signal . the equipment necessary is very expensive . however , you can make rough estimates from say a simple saturation spectroscopy based frequency locking setup by studying the amplitude fluctuations of the " error signal " . granted that it is crude and limited to the natural linewidth of your atomic system , but it does not need super expensive equipment or complicated electronics . there are other ways to use atomic physics ( eit , cpt resonances etc ) to make reasonable estimates of laser linewidth and it all depends on what your goal is .
the true eigenstates , when they exist , do not decay . they sit and spin around in phase forever . but atomic eigenstates are not true eigenstates . the reason atomic states decay is because they are coupled to photon states , and the combined photon-atom hamiltonian does not have excited atom eigenstates . when you have an atom in a box of mirrors , there are true eigenstates of the combined photon-atom system inside the box . these are states where a quantum of energy is absorbed by the atom , remitted into the box , in a steady way , so that it is sometimes in the atom , sometimes in the photons of the box . but when you make the box big , the energy will be in the photons nearly all the time , and the atom will be in its ground state , just because there are infinitely many more photon states than atomic states . in the limit of no box , the excited atomic states are never true eigenstates , they always decay into photons irreversibly . this process was described by fermi , and the rate of irreversible decay is given by fermi 's golden rule . for atoms and radiation , the coupling is mostly by a term in the hamiltonian equal to $p\cdot a ( x ) $ , where p is the momentum of the electron and a is the vector potential at the position of the electron , plus a direct two-photon term $a ( x ) ^2$ which you can usually ignore . you evaluate the transition by expanding a in plane waves , the coefficients of which are photon creation operators , and approximating the exponential of the x operator by the first two terms of a taylor expansion . this is called the dipole approximation . the resulting hamiltonian describes transitions between the pure atom stationary states into states of the atom plus a photon , and for long times , the transitions conserve energy , so that the outgoing photon carries the energy difference that is lost by the decay . the dipole approximation is essentially exact for transitions which are dipole-allowed because the atomic motion is nonrelativistic , so that the wavelength of the light is enormous compared to the atom . the result is that there are small matrix elements for transitions between the states , accompanied by creating one photon , and these give the dipole atomic transitions . this is worked out in sakurai 's book , among others .
1 ) notice that by inserting a complete set of position states we can write $$ \hat p \psi ( x ) = \langle x|\hat p|\psi\rangle = \int dx'\langle x|\hat p|x'\rangle\langle x'|\psi\rangle =\int dx'\langle x|\hat p|x'\rangle \psi ( x' ) $$ so if we set $$ \langle x|\hat p|x'\rangle = -i\hbar \frac{\partial}{\partial x}\delta ( x-x' ) =i\hbar \frac{\partial}{\partial x'}\delta ( x-x' ) $$ then we can use integration by parts to obtain $$ \hat p \psi ( x ) =i\hbar \int dx'\frac{\partial}{\partial x'}\delta ( x-x' ) \psi ( x' ) = -i\hbar \int dx'\delta ( x-x' ) \frac{d \psi}{dx'} ( x' ) = -i\hbar \frac{d\psi}{dx} ( x ) $$ so your expression is correct . the derivative of a delta function is essentially defined by the integration by parts manipulation that i just performed ; in fact derivatives of distributions in general are defined in an analogous way . see this lecture for example . hope that helps ; let me know of any typos ! cheers !
there is no unique way to " further decompose " the deformation into the " rigid transformation " and " others " because whatever " rigid part " you choose , you may always calculate " others " as a simple difference ( that is because there is really no global constraint on the " other " part ) . so the " rigid part " may be anything you want .
volume increase in the system is due to work done by the system . therefore w is negative using your notation . think of it this way , work done on the system would push the system inwards , decreasing volume . therefore a volume increase is work done by the system . alternatively you could reason using the formula : $du = q - dw$ ( using your notation conventions , were $u$ is internal energy , $w$ is work and $q$ is heat added to the system ) $dw = pdv . $ therefore $du = q - pdv$ . therefore if $dv$ ( change in volume ) is positive , $du$ ( change in internal energy ) is negative .
thanks to @user12262 for pointing me in the direction of the kww function . after perusing that link and searching scifinder for stretched and compressed exponential functions in relation to nmr , i ran across this paper ( subscription required , sorry ) . to ( briefly ) summarize the paper , the compressed exponential function , $e^{-kt^q}$ , with $1 &lt ; q &lt ; 2$ can be represented as a distribution of gaussian functions with different relaxation rates , $$r_c ( t ) = \frac{1}{\pi} \int^∞_0 p_c ( s ; q ) \ , e^{− ( sr^*t ) ^2} d\textrm{s} , $$ where $r_c ( t ) $ is the observed decay curve , $p_c ( s ; q ) $ is the probability distribution of gaussian decays , $r^*$ represents some average value of the rate , and $s = r/r^*$ . as the value of $q$ approaches 2 , the distribution function approaches a delta spike ( as one would expect ) . in the case of nmr $t_2$ decays , this most likely represents a distribution of relaxation couplings ( e . g . interactions with 1 , 2 , 3 , etc . other nearby spins ) .
the intensity of the light from the sun at the orbit of the earth is around 1.4 kilowatts per square metre . for comparison , a domestic heater is usually around 3 kw , so a satellite with a 2m surface area ( admittedly this is bigger than most satellites ) facing the sun needs to dissipate as much energy as used to heat your living room . this is in addition to the waste heat produced by the electronics on the satellite . for most satellites it is keeping cool that is the problem , and that is why they are wrapped in reflective foil . where you have a satellite that needs to stay really cool , such as the herschel space observatory , the satellite has to carry a supply of liquid helium to cool itself . in fact the herschel space observatory ran out of liquid helium at the end of april last year and can no longer operate . it is certainly true that if you can stay out of sunlight space is a pretty cold place . if you could avoid all reflected light then in principle you could cool to the 2.7k temperature of the microwave background , and this is cold enough to use superconductors . however this is not a practical way to operate most satellites .
interference of sound waves might work for you . one experiment that comes to mind is to have two speakers playing identical periodic sounds . ( i do not think they have to be pure sinusoidals , but " simple " so it sounds like a note . ) depending on where you stand relative to the speakers , you will hear " dull " areas where the sound is not loud , demonstrating deconstructive interference . depending on the patience of your group , you could even have them walk around and map out the destructive areas . ( i actually think mapping it out would be quite hard ; test it before doing it . ) the number of destructive areas and their areas can be adjusted by tuning the frequencies of your source and/or the speaker separation . there is a phet simulation here .
of course lightning " has a frequency component " . if it did not have non-zero frequencies , it could never start or would last indefinitely . lightning is a huge current pulse over a short time , usually several pulses over a few milliseconds . but more importantly , the current is started and stopped very abruptly , which by necessity means it has a broad spectrum containing significant high frequency content . spark gaps in general are infamous for being broad band frequency sources well into the radio range . some early radio transmitters harnesses this by connecting a spark gap to a resonant circuit . since the spark gap produced a wide spectrum , it also produced the frequency of the resonance . however , the other frequencies are not sufficiently attenuated , so spark gap transmitters are specifically banned in the us ( probably most countries ) . i think your question really comes down to a unawareness of fourier analisys . that would be a good search term to look up additional information .
first of all , bernoulli 's law is applicable only to inviscid flow , while poiseuille 's flow is for the viscous fluid . the fact that pressure is constant along the orthogonal cross section of the pipe could be derived from the assumption that the flow is parallel , that is everywhere inside the pipe the velocity field has only z-component ( assuming the cylindrical coordinate system , with pipe oriented along the z-axis ) . then the r-component of navier-stokes equation is then reduced to $0 =- \frac1{\rho}\frac{\partial p}{\partial r}$ ( all terms containing velocity components are equal to zero here ) , which gives the pressure independent of radial coordinate . the assumption that the flow in the pipe would be parallel in derivations of poiseuille 's flow is just an ansatz , compatible with the symmetries of the problem , that is later justified by producing correct solution of navier-stokes equation . however one should remember that such flow would be stable only for relatively large viscosity of the fluid ( that is for reynolds number not exceeding the certain critical value ) .
a well-defined quantum theory is clearly presented by rovelli in the 2011 zakopane lectures : http://arxiv.org/abs/1102.3660 it definitely satisfies your criterion a , easily seen to heuristically give b , and i do not know personally what is the status of c , but i know that a graviton propagator is definable and computable , which might be sufficient . personally , i believe there is a lot of underlying commonality with your own work ( which i follow with a dilettantish interest ) . in particular , rovelli has also introduced fermion and gauge theory coupling by means of lattice field theory living on a quantum graph , which to my mind resembles string-nets : http://arxiv.org/abs/1012.4719 there are also a nice set of recorded lectures at the perimeter ( perhaps you have already seen them in person , however ) , which contains a lot of colloquial talking which helps to fill in between the lines , and which i think expresses rovelli 's personal view of the state of the research much better than his written work : http://pirsa.org/c12012
in $d&gt ; 4$ dimensions , the analogue of the schwarzschild solution is $$ ds^2 = - \left ( 1 - \frac{2 m}{r^{d-3} } \right ) dt^2 + \left ( 1 - \frac{2m}{r^{d-3}} \right ) ^{-1} dr^2 + r^2 d \omega_{d-2}^2 $$
a conducting body can have a potential , and it need not be zero . potential can be arbitrarily set , depending upon your reference potential . the only difference in the tratement of conducting bodies is that they must be equipotential , i.e. , they must have constant potential at all points inside them ( but not necessarily points inside cavities ) . the potential of a metal shell due to its own charge $q_1$ is $\frac{kq_1}{r_{shell}}$ if you add a point charge $q_2$ at the center , then the potential becomes $\frac{kq_1}{r_{shell}} + \frac{kq_2}{r_{shell}}$ . remember , potential at a point is can be defined with respect to the work required to get a test charge there from infinity . if the field at a point is zero , that does not imply that the field is zero all the way to infinity . it just means that you can jiggle a test charge in the neighborhood of that point without doing work .
given the rather large volume of the universe , i suppose it is possible . not as an initial condition as far as i can tell though because of the conservation of angular momentum . however , given the right circumstances of impact events on a rogue planet ( with no other bodies to perturb its non-rotation ) , i suppose it is possible . highly unlikely , but theoretically possible . as to why planets rotate , cornell ( the home of carl sagan ) has a great explanation . what i am saying is that there will be no planets if there was no initial angular momentum in the primordial solar nebula . if a nebula with absolutely no rotation collapses , then there will only be a central non-rotating star and there will not be any planets . planets form out of a protostellar disk , which itself forms only because of the initial angular momentum of the cloud . the dynamics of a rotating body is of course controlled by forces like gravity . kepler 's laws are a direct consequence of gravity .
i find it is not a problem if one simply omits the $o ( \epsilon ) $ terms since we are taking the $\epsilon \rightarrow 0$ limit . the author just did not state it clearly .
you need to know the rate of heat given by the flame to the water . suppose the flame transfers $h$ kj/s to the water . the latent heat of evaporation of water is $2260$ kj/kg for energy balance , the heat given to the water must be equal to the amount of heat required to convert water into steam . $$ h = \dot{m} \times 2260 \\ \therefore \dot{m} = \frac{h}{2260} \text{ kg/sec} $$ if you say the rate of heat transfer does not matter ( ignore the flame and assume the water is at a certain temperature and stays at that temperature throughout the experiment ) , $$ \dot{m} = \frac{\theta a ( x_s - x ) }{3600} \text{ kg/s} $$ where $\theta = ( 25 + 19 v ) $ is the evaporation coefficient ( $kg/m^2\cdot hr$ ) . this is an empirical equation , so you can not derive it from first principles . $v$ is the velocity of air just above the surface of the water ( $m/s$ ) $a$ is the surface area of the water ( $m^2$ ) $x_s$ is the humidity ratio in saturated air at the same temperature as the water surface ( kg h2o in kg dry air ) $x$ is the humidity ratio in the air ( kg h2o in kg dry air ) it is fairly straightforward to find $x$ and $x_s$ from the relative humidity and the mollier chart
the set $g$ gives the representation of the identity and generators of the abstract group of quaternions as elements in $sl ( 2 , \mathbb c ) $ which are also in $su ( 2 ) $ . taking the completion of this yields the representation $q_8$ of the quaternions presented in the question . from the description of the symmetry group as coming from here , consider the composition of two $\pi$ rotations along the $\hat x$ , $\hat y$ , or $\hat z$ axis . this operation is not the identity operation on spins ( that requires a $4\pi$ rotation ) . however , all elements of $d_2$ given above are of order 2 . this indicates that the symmetry group of the system should be isomorphic to the quaternions and $q_8$ is the appropriate representation acting on spin states . the notation arising there for $d_2$ is probably from the dicyclic group of order $4\times 2=8$ which is isomorphic to the quaternions .
just to start things off a very incomplete answer ( and maybe faulty too ) : these are feynman diagrams . the " recursion " idea is as follows . the electron can move from a to b in the simplest possible way ( the mentioned line ) . but , for which you would draw a slightly more complicated diagram , it can also have 2 things happening to it ( emit and absorb a photon ) . and maybe 4 things happening to it ( emit , emit , absorb , absorb or another sequence of events ) . and maybe 6 things . or 8 . and in all those scenarios the photons may have things happening to them before they get reabsorbed . ( there is the recursion , because new electrons can form there , together with positrons , which would be subject to all these things too . ) in fact , and i believe this the be the main point , all of these possibilities ( diagrams ) happen together , but the main contributions to the electron getting to b are from those diagrams where relatively few things happen . by calculating the contributions of the simpler diagrams to " the big picture " you get a good approximation of " the big picture " . i will await better and more complete answers ( or minus points for this one ) , before removing this one .
there are tons of papers on the connection between quantum processes and probability theory ( though i do not understand why you single out coherent states - they do not play a special role in this connection ) . the theory of stochastic processes and the theory of quantum processes are the commutative and noncommutative side of the same coin , with many similarities . see , e.g. , the books by gardiner ( handbook of stochastic processes ) or barndorff-nielsen ( quantum independent increment processes : structure of quantum lévy processes , classical probability , and physics ) online is the following article by barndorff-nielsen http://www.jstor.org/stable/10.2307/3647584
@brandon is correct . you can compute the average kinetic for any free particle using the equipartition theorem , which gives $\langle e \rangle = \frac{1}{2}k_b t$ per quadratic degree of freedom , where $t$ is the temperature and $k_b$ is boltzmann 's constant . for free particles in 3d this gives $\langle e \rangle= \frac{3}{2} k_b t$ ; equating $\frac{1}{2} m_e v^2 = \frac{3}{2} k_b t$ ( in a classical approximation ) shows that the root-mean-square velocity $v$ of free electrons is temperature dependent . $^1$ in the above , the velocities are measured with respect to the environment at temperature $t$ -- we do not even need to consider the fact that the electrons you are talking about are presumably on earth , being accelerated around the sun in centripetal motion . even light will change its speed $c=c_0/n$ in an explosion , because the index of refraction $n$ of the surrounding medium will become inhomogeneous and fluctuate : it is only the speed of light $c_0$ in vacuum that is constant . $^1$note , as also pointed out by brandon , electrons often move at relativistic speeds , so $\frac{1}{2}m_e v^2$ is a poor approximation . $v$ in this formula can exceed the speed of light $c$ , for example . to quantitatively calculate the velocity you need the correction from special relativity . this does not qualitatively change the answer though .
a real object will radiate less energy than a black body at the same temperature . the ratio of the emission to the black body emission is called the emissivity . the engineering toolbox web site has data on emissivities for a range of materials . for example a polished silver surface has an emissivity of 0.02 i.e. it radiates only 2% of the power that a black body of the same temperature radiates . the emissivity is related to the reflectivity by e + r = 1 . a black body reflects none of the light falling on it so r = 0 and e = 1 . anything that has a non-zero reflectivity must have an emissivity of less than 1 otherwise it could be used to build a perpetual motion machine .
in general physics course we assume maxwell 's equations as the result of many experiments . after that , in field theory we build the lagrangian which satisfies the maxwell equations . we also can build non-linear field theories of em interactions , but there is a requirement to getting of the maxwell equations in the limit of weak fields . so these methods are not connected with the derivation . but we can derive the equations by using some postulates , which generalize experimental facts . the number of postulates should be reduced to a minimum . maxwell 's equations can be earned from the coulomb 's law , special relativity theory and superposition principle ( more details you can see in my answer on this question ) . the other question is why do we need derivation of equations instead of postulating them . they satisfy the experiments , so that is enough for using them in practical cases . the postulating them is not much worse then deriving in this situation .
the question seems a bit odd because " time of maximum spring compression " is an odd concept . the spring compression is a function of time and the time of maximum spring compression is zero because it is an instant not a time interval . maybe the question means the time interval from the time the car first touches the spring to the time of greatest compression . assuming this is the case , and bearing in mind that because this is a homework question we are only allowed to give hints , the trick to doing this question is to realise that the spring behaves as a simple harmonic oscillator i.e. the compression of the spring from the moment the car touches it will be : $$d = a sin ( \alpha t ) $$ where $a$ and $\alpha$ are some constants that you need to calculate . the problem simplifies a lot if you think about the relation between the period of a harmonic oscillator and the amplitude of oscillation .
they will equalize pressure at the entrance to the tube between them . that pressure is the density of the fluid times the height from the bottom to the lowest point in the vortex , because the fluid at the lowest point has zero velocity and so is equivalent to standing fluid .
i define $h\sim\psi_{\alpha}^{\dagger}\xi_{\alpha\beta}\psi_{\beta}$ , where i forget the sum/integrals and all these boring staff . i also define $\xi_{\alpha\beta}\equiv\boldsymbol{\xi}\cdot\boldsymbol{\sigma}=\xi_{0}+\xi_{x}\sigma_{x}+\xi_{y}\sigma_{y}+\xi_{z}\sigma_{z}$ to have the most generic one-body hamiltonian written in a compact form . the one body hamiltonian then reads , in matrix notation $$ h\sim\left ( \begin{array}{cc} \psi_{\uparrow}^{\dagger} and \psi{}_{\downarrow}^{\dagger}\end{array}\right ) \left ( \begin{array}{cc} \xi_{0}+\xi_{z} and \xi_{x}-\mathbf{i}\xi_{y}\\ \xi_{x}+\mathbf{i}\xi_{y} and \xi_{0}-\xi_{z} \end{array}\right ) \left ( \begin{array}{c} \psi_{\uparrow}\\ \psi_{\downarrow} \end{array}\right ) $$ as can be easily check . one now wants to add the particle-hole double space ( nambu space ) . one uses that ( the anti-commutation relation ) $$ \psi_{\alpha}^{\dagger}\xi_{\alpha\beta}\psi_{\beta}=-\xi_{\alpha\beta}\psi_{\beta}\psi{}_{\alpha}^{\dagger}+\delta_{\alpha\beta}\xi_{\alpha\beta}=-\psi_{\beta}\left ( \xi_{\alpha\beta}\right ) ^{t}\psi{}_{\alpha}^{\dagger}+\delta_{\alpha\beta}\xi_{\alpha\beta} $$ and you get the unavoidable trace over the one-body energy . this nevertheless renormalizes your energy in a standard way , and one usually drops off this extra term . we thus get $$ h\sim\dfrac{1}{2}\left ( \begin{array}{cccc} \psi{}_{\uparrow}^{\dagger} and \psi{}_{\downarrow}^{\dagger} and \psi_{\uparrow} and \psi_{\downarrow}\end{array}\right ) \left ( \begin{array}{cc} \boldsymbol{\xi}\cdot\boldsymbol{\sigma} and -\mathbf{i}\sigma_{y}\delta\\ \mathbf{i}\sigma_{y}\delta and -\left ( \boldsymbol{\xi}\cdot\boldsymbol{\sigma}\right ) ^{t} \end{array}\right ) \left ( \begin{array}{c} \psi_{\uparrow}\\ \psi_{\downarrow}\\ \psi{}_{\uparrow}^{\dagger}\\ \psi{}_{\downarrow}^{\dagger} \end{array}\right ) -\dfrac{1}{2}\text{tr}\left\{ \boldsymbol{\xi}\cdot\boldsymbol{\sigma}\right\} $$ in a mixed notation ( block matrix in the middle , full vectors on the edge ) . note the only important thing here that $\left ( \boldsymbol{\xi}\cdot\boldsymbol{\sigma}\right ) ^{t}=\left ( \boldsymbol{\xi}\cdot\boldsymbol{\sigma}\right ) ^{\ast}$ and only the $\sigma_{y}$ component changes sign ( look at your $\alpha p\sigma_{y}\tau_{z}$ term in the bdg hamiltonian ) your ordering convention is found by an obvious change of basis from mine . then you choose a representation for the tensor product and you are done . one more time , you can not avoid the final trace term , but most of the people forget to discuss it . it has almmost no role , except when you want to describe some effects related to the phase transition of superconductivity ( for instance , to correctly write the free energy , you need it ) . one more thing : the hamiltonian you gave is a bit famous at the moment for hosting majorana fermions . if you diagonalise the spin-part , you end up with a $p$-wave effective superconductivity at low energy .
first of all i try to restate your question into a more clear form . consider $\mathbb r$ equipped with the equivalence relation : $x \sim y$ if and only if $x-y= 2k\pi$ with $k \in \mathbb z$ . the space ${\mathbb r}/ \sim$ of equivalence classes $ [ x ] $ is $\mathbb s^1$ also as a topological space using the quotient topology . next consider the standard actions of the lie group of translations $\mathbb r$ on the real line $\mathbb r$: $$t ( a ) x:=x+a\quad \forall x , a \in \mathbb r\: , $$ and define the representation of the translation group on $\mathbb s^1$ as $$t′ ( a ) [ x ] := [ t ( a ) x ] \:\forall x , a \in \mathbb r\: . \quad ( 1 ) $$ the map $\mathbb r\ni a \mapsto t′ ( a ) $ is in fact a representation of the translation group on $\mathbb s^1$ in terms of isometries of the circle ( when equipped with the standard metric ) . in particular , one has $t' ( 0 ) = id$ and $t' ( a ) t' ( b ) = t' ( a+b ) $ . however all that has nothing to do with compactness ( false ! ) of the translation group , even if the outlined procedure gives rise to a representation of that ( non-compact ) lie group on a compact manifold , in terms of isometries of that manifold . let us eventually come to the relation with the rotations group of $\mathbb r^2$: $so ( 2 ) \equiv u ( 1 ) $ . as $\mathbb r$ is the universal covering of $u ( 1 ) $ , with covering ( surjective lie group ) homomorphism : $$\pi : \mathbb r^1 \ni a \mapsto e^{ia} \in u ( 1 ) \: , \qquad ( 2 ) $$ every representation of the group of $\mathbb r^2$ rotations $u ( 1 ) $ is also a representation of the group of translations $\mathbb r$ . identifying $\mathbb s^1$ with $u ( 1 ) $ in the standard way , the natural action ( representation ) of $u ( 1 ) $ on the circle is trivially $$r ( e^{ia} ) e^{ix} = e^{i ( a+x ) } \qquad ( 3 ) $$ where the first $e^{ia}$ is viewed as an element of the group $u ( 1 ) \equiv so ( 2 ) $ and the other two are viewed as elements of the circle $u ( 1 ) \equiv \mathbb s^1$ . the interplay of $t ' , r$ and $\pi$ , as one easily proves is : $$r ( \pi ( a ) ) = t' ( a ) \quad \forall a \in \mathbb r\: . \qquad ( 4 ) $$ this is in agreement with the remark above that reps of $so ( 2 ) $ are also reps of $\mathbb r$ . thus , as a matter of fact , it is not possible to distinguish between the action of $\mathbb r$ and that of $so ( 2 ) $ on the circle $\mathbb s^1$ , though they are different groups and only the latter is compact ( and in a certain way related with the component of angular momentum orthogonal to $\mathbb r^2$ . )
summary : the energy change is negligible and if it is not , the energy difference comes from a frequency change of the photon . one must realize that unless the plate was already quickly rotating before the experiment , the energy stored in the rotation of the plate at the end is negligible relatively to the energy of the photon for the same reason why the electron carries most of the kinetic energy in the hydrogen atom even though both electron and proton are orbiting around the shared center-of-mass . in the latter case , the energy of a particle goes like $p^2/2m$ , so for a fixed value of $p^2$ - and indeed , the proton 's and electron 's $p$ only differ by a sign - the lighter particle carries much more kinetic energy . this is totally true for the rotational motion as well . the kinetic energy of rotation is $j^2/2i$ where $j$ is the angular momentum and $i$ is the moment of inertia . for a single photon , the angular momentum relative to the direction of the photon 's motion , $j_1=-\hbar$ , is changed to $j_2=+\hbar$ . the overall change is $j=+2\hbar$ which becomes the angular momentum of the plate . when you square it , you clearly get a negligible number - that is divided by an $o ( 1 ) $ value of the moment of inertia . so the energy obtained in the form of the increased rotation , caused by a single photon 's change of the polarization , is tiny . if you study where this small amount comes from , you will find out that the photon 's frequency is decreased by a tiny fraction so that its reduced energy exactly compensates the increase of the kinetic energy of the plate . but we can only see this change as nonzero if we allow the plate to have an arbitrary angular frequency before the experiment . it is not too difficult to explicitly check that this statement works : if $j$ jumps by $2\hbar$ , the energy $j^2/2i$ jumps by $j\times \delta j/i=2j\hbar/i$ . because $e=\hbar\omega_\gamma$ for photons , the energy conservation law requires that the frequency of the photon decreases by $\delta \omega = -2j/i$ where $j$ is the plate 's angular momentum before it changed the photon 's polarization . but indeed , $-2j/i=-2\omega$ where $\omega$ is the angular frequency of the plate before the experiment . that is exactly the frequency change that the photon experiences . why ? because the periodicity of the photon is conserved if measured relatively to the rotating plate - so its angular frequency changes from $|-\omega_\gamma|$ to $|+\omega_\gamma|$ . but relatively to the inertial system , it is changed from $|-\omega_\gamma-\omega|$ to $|+\omega_\gamma-\omega|$ i.e. it drops by $2\omega$ . it is just like comparing sidereal days and solar days . what would happen with a single photon ? i was working with a single-photon case from the beginning because it is a reasonable approach . a large electromagnetic wave may always be represented as a coherent state of many photons , so one just multiplies the results for a single photon . of course , for a single photon , the " electromagnetic wave " must actually be represented as a wave function determining the probability amplitudes . at any rate , it is true that such wave plates may guarantee that every single photon that enters with a particular polarization leaves with another polarization . if you measure the " exact " polarizations of the final photon , you will get the right one with probability of 100 percent . if you measure a different one , you may get odds between 0 and 100 percent . every photon will be ultimately detected at a single point ; the wave function knows about the odds .
in statics , you can still have a force without acceleration so $f$ is independent of $a$ . $f$ is the cause of the change in the position of an object initially at rest in some frame . to give it physical meaning , you have to define how it is to be measured and one way would be to define 1 unit of f causing one unit of compression in some standard spring . now if $f$ causes a body at rest to change its position , then over a time dt the postion has changed by dx . your job as a physicist is to construct an equation relating f to the change in velocity of the body . so with all this in mind , what would happen if $f=m*d^3x/dt^3$ ? it would mean that even though $f$ is the cause behind the change in velocity of a body , there are some changes in the velocity possible where $f = 0$ such as for $a = const$ . you would end up with particles accelerating in arbitary directions for $f = 0$ .
the math is quite abstruse but if you want you can go to this calculator and punch in numbers to find what you need . http://virtualtrebuchet.com/trebuchet.aspx it is the best simple trebuchet calculator i have seen .
he ignored the radius of the earth as negligible . his estimates for the angle were from the shape of the shadow the sun casts on the moon , and the difference between this and a straight line when the moon is halfway between full and new is too small to percieve precisely . he fooled himself into thinking he measured a different angle , so his estimate was really only giving a lower bound on the distance to the sun . as a lower bound , it was enough to establish that the sun is larger than the earth , and this was important , in that it lent strong support to heliocentric models . but it was not an accurate method .
your boundary condition ( i assume you mean that $\gamma$ is a cylinder centred on $r=0$ in cylindrical co-ordinates and with radius $r_\gamma$ as you imply that the problem has circular symmetry ) is : $\mathbf{u} ( r_\gamma , \theta , z ) \wedge \mathbf{n} = \sum\limits_{\alpha = -\infty}^\infty \tilde{\mathbf{u}}_\alpha ( r_\gamma , z ) \wedge \mathbf{n}\ , e^{i\ , \alpha\ , \theta} = \mathbf{0} ; \ ; \forall \theta , z \in \mathbb{r}$ the next step comes from the $\mathbf{l}^2$-completeness of the set of functions $\left\{f_\alpha : [ 0 , \ , 2\pi ) \rightarrow [ 0 , \ , 2\pi ) , \ , \alpha\in\mathbb{z}: f_\alpha ( \theta ) =e^{i\ , \alpha\ , \theta}\right\}$ on the interval $ [ 0 , 2\pi ) $ and their known linear independence . you simply take this as a proven mathematical fact : i.e. the only way a fourier series can be identically nought is if all of its co-efficients are nought . hence you conclude $\tilde{\mathbf{u}}_\alpha ( r_\gamma , z ) \wedge \mathbf{n} = \mathbf{0}$ for each and every $\alpha \in \mathbb{z}$ .
i assume with energy you mean electricity , and your generators sound like rechargeable batteries to me . in that case you have a series connection , which will simply double the voltage available . no , this would not generate infinite energy . whatever you do , each generator needs fuel that contains chemical energy . in the best ( =impossible ) case it would be entirely converted into electrical energy . whether you temporarily store a part of g1 's energy in g2 or not does not change anything about the fact that the total energy cannot exceed the chemical energy of your fuel ( unless of course there are other influences , e.g. a solar panel and the very energetic sun which will not run out of fuel for some billion years . . . )
there are no derivatives of the delta function in your integral . performing the integration with one of the delta functions you recover energy-momentum convervation in the vertices and overall energy-momentum conservation in the form of a leftover $$ \delta^4 ( p_1 + p_2 - p_3 - p_4 ) $$ for further simplification you need to consider the absolute matrix element squared with sums over the particle spins ( you average over the incoming and sum over the outgoing ) : $$\frac{1}{4} \sum_{s_1 , s_2 s_3 s_4} \vert \mathcal m \vert^2 = \sum_{s_1 , s_2 s_3 s_4}\vert \text{your integral} \vert^2 $$ and use completeness relations like $$ \sum_s u^s ( p ) \bar u^s ( p ) = p_\mu \gamma^\mu - m $$ $$ \sum_s v^s ( p ) \bar v^s ( p ) = p_\mu \gamma^\mu + m $$ good luck !
they represent the scale on which general relativisic effects dominate physics related to bodies of that mass . for instance if you were to create a ( un-rotating , uncharged ) black hole of 1 earth mass it is event horizon would have a radius of about $9\text{ mm} = 2 * m_\text{earth}$ in those units . for scales much , much larger than the " length " of the mass , general relativity may be neglected . for intermediate scale in comes in as corrections on order of $\frac{l}{l}$ where $l$ is the mass in the scaled units and $l$ is the length scale of the problem . this is similar to what particle physicists do by setting $c = \hbar = 1\text{ ( dimensionless ) }$ energy scales and length scales become inter-changeable .
the determinant is fairly easy to calculate . you know already , essentially , the eigenvalues of the stiffness matrix ; more accurately , you know the eigenvalues of the matrix $\mathbf{m}^{-1}\mathbf{k}$ , because the $\omega_i$ are zeros of the equation $$0=\det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) . $$ ( the more aesthetically minded would replace $\mathbf{m}^{-1}\mathbf{k}$ with $\mathbf{m}^{-1/2}\mathbf{k}\ , \mathbf{m}^{-1/2}$ to get a hermitian matrix , but no matter . ) if you express the second determinant in the corresponding eigenbasis , you get $$ \det ( \mathbf{k}-\mathbf{m}\ , \omega^2 ) =\det ( \mathbf{m} ) \det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) =\frac{m^3}2\det\begin{pmatrix} \omega_1^2-\omega^2 and 0 and 0 \\ 0 and \omega_2^2-\omega^2 and 0 \\ 0 and 0 and \omega_3^2-\omega^2 \end{pmatrix} , $$ which gives your textbook 's expression . more generally , this is an expression of the principle that a matrix 's determinant is the product of its eigenvalues . the adjunct , on the other hand , does not ( to my knowledge ) satisfy any such nice relation ; in any case it is a nasty beast to deal with and i think few people judiciously substituting in the definition $k=m\omega_2^2/2$ of $\omega_2$ instead of $k$ .
this is because you are doing a rotation transformation , which is an su ( 2 ) on the state vector ( the vector the matrices act on ) , but when you do the transformation of the pauli operators , it is a regular vector rotation of all three , so it preserves the length . this is a special case for 2 state quantum mechanics , any su ( 2 ) transformation preserves the length of the coefficients of the pauli matrix in the expansion of any 2 by 2 matrix , the pauli matrices make an operator vector .
most of the subducted crust is sea floor , which does not normally contain oil or gas . at this point someone is bound to mention north sea oil/gas , but the north sea floor is actually more like continental rock than basalt sea floor , and it is not being subducted . anyhow , even if gas or oil was subducted it is not going to explode because there is not any oxygen available for it to burn . if it happened i would guess that the oil/gas would get dissolved in molten rock and eventually emerge from volcanos .
let $y ( x ) $ be the curve describing the shape of the cable . let $t ( x ) $ be the tension in the cable . consider a small segment extending from $x$ to $x+dx$ . the horizontal component of the tension at the two ends of this segment must cancel out , so $t_x$ must be constant . if $\theta$ is the angle the cable makes with the vertical , so that $y&#39 ; =\tan\theta$ , then $t\cos\theta$ is constant . in other words , $$ {t\over \sqrt{1+y&#39 ; ^2}}=\alpha $$ where $\alpha$ is some constant . now consider vertical forces . the tension forces on the two ends of our small length element are $$ t_y ( x+dx ) -t_y ( x ) =t_y&#39 ; ( x ) \ , dx= ( t\sin\theta ) &#39 ; \ , dx=\left ( ty&#39 ; \over \sqrt{1+y&#39 ; ^2}\right ) &#39 ; dx= \alpha y&#39 ; &#39 ; \ , dx . $$ this force must balance the weight of the cable and of the hanging bridge below it : $$ \alpha y&#39 ; &#39 ; \ , dx=\beta\sqrt{1+y&#39 ; ^2}\ , dx+\gamma\ , dx . $$ here $\beta , \gamma$ are the linear mass densities of the cable and bridge respectively . so the equation we need to solve is $$ y&#39 ; &#39 ; =b\sqrt{1+y&#39 ; ^2}+c , $$ where $b=\beta/\alpha , c=\gamma/\alpha$ . let $m ( x ) =y&#39 ; ( x ) $ be the slope . then this is a first-order separable equation for $m$ , with solution $$ x=\int{dm\over b\sqrt{1+m^2}+c} . $$ mathematica cheerfully does this integral : $$ x=\frac{\frac{c \tan ^{-1}\left ( \frac{c m}{\sqrt{m^2+1} \sqrt{b^2-c^2}}\right ) }{\sqrt{b^2-c^2}}-\frac{c \tan ^{-1}\left ( \frac{b m}{\sqrt{b^2-c^2}}\right ) }{\sqrt{b^2-c^2}}+\sinh ^{-1} ( m ) }{b} . $$ now " all " you have to do is invert this to get $m ( x ) $ , and integrate $y ( x ) =\int m ( x ) dx$ . there is no nice closed-form solution for this , unfortunately . but mathematica does verify that the solution goes to a catenary as $c\to 0$ and to a parabola as $c\to\infty$ , so i think it is right .
from looking at the pictures of the grids provided on that website , i think i understand what they are doing . it is not actually very complicated . it seems easiest to explain this if we consider only one of the hexagonal " tubes " formed by the grid . normally , the light from a flash spreads out over some ( relatively wide ) angle , but now think of what happens to the light when it is confined to this tube : the light that would normally head off at a steep angle is intercepted by the walls of the tube , while light that was already propagating parallel ( or at least close to parallel ) to the tube axis is passed without obstruction . this results in a more confined range of angles over which the light can spread . of course , the confinement is not perfect , as i will explain below , but you can imagine how the angular spread of the beam would be limited to consist only of those rays which can pass cleanly through the tube . this is simple geometry , and you could adjust this angle by varying the length and diameter of the tube . however , the light that is intercepted by the tube walls is not completely eliminated . instead it scatters into a random direction . it is this scattering that gives the resulting beam a nice soft edge . instead of being sharply confined to the angular range allowed by the tube , the scattering allows it to " bleed " a little bit . i should add that the hexagonal shape of the tubes probably has very little effect on the resulting illumination pattern . more likely a hexagonal shape was chosen for some other reason , like easy manufacturing or more mechanical stability . optically , a square grid or even an array of circular tubes , like a bundle of straws , would work just fine .
there have been experiments with single photons at the time which display interference even after knowing which slit the photon went through . i do not think that the disturbance of the field by a single electron would be detectable , including correlated with which slit it went through . possibly a variant of the experiment in the link , using entangled electrons might show the same effect , interference even if the path is known . this is what one expects from quantum mechanics because the fringe pattern is dependent on the probability , i.e. the square of the wave function and there is no reason to believe that a particle is spread out over all its wave function , but a lot of experimental consistent results that tell us it is just a probability to find the whole particle at a specific point on the screen . in my opinion , the experiments that destroy the interference pattern when attempting to check the slit the particle went through is due to the introduction of a new wavefunction that differed a lot from the undisturbed one . after all we are talking quantum mechanics . any change in the boundary conditions reflects in the mathematical formula of the wavefunction .
i think you may be misreading that the field holds no energy . all of the light can be reflected at steady state . when the light is first incident on the system , energy is stored in the plasmon part of the light-plasmon system . once the steady state has been reached , the energy going in equals the energy coming out ( less a trickle needed to make up for any nonideal loss in the metal ) . this is not only true of surface plasmons : its true of any electromagnetic system that can store energy before steady state is reached . this is very like the establishment of the electromagnetic fields in a parallel lc resonant tank circuit , and indeed this is a good classical picture to keep in your head of the plasmon . the " tank circuit " has an energy input and an energy output . at first , a great deal of energy is " sunken " into the tank circuit whilst its fields are established . once they are , though , the only energy needed per cycle is to make up for nonideal losses , so that the energy input equals the energy output . another idea is to think of a cylindrical water tank with an inlet near the top , and , diametically opposite , an outlet . when the water begins to fill the tank , no water comes out the outlet . however , at last the tank will be filled , and water will begin pouring out of the outlet at the same rate as it comes into the inlet ( less a small volumetric rate is there is some other slow leak in the tank ) .
try to use the water storage analogy . that helps best , even i was taught using that analogy . even the formulas in both the circumstances matches to some extend . the only problem would be when you will be trying to explain the advanced concepts like wave-matter duality they will have to unlearn this analogy because then electrons are no longer like water . but becomes an entirely different beast .
yes , it is possible . a static setup like this will work as long as any small motion of the parts would increase the potential energy . in this case , it looks like there is only one possible motion - rotation of the entire ruler-hanger-hammer piece about the axis where the ruler touches the table . if the ruler were to rotate down a little bit , the entire hammer would go down some , decreasing its potential energy . however , it would also rotate , raising the head , where most of the mass is , and thus increasing the potential energy . when these two effects cancel , the system will be in equilibrium . to figure out when the effects will cancel , we could find the system 's center of mass . if the center of mass is directly under the contact point , the system 's in equilibrium . this is because as the system rotates , the center of mass moves in a circle . to be in equilibrium , it must be at the bottom of that circle . the center of the circle is at the contact point , so the bottom of the circle is directly beneath it . there are other , equivalent ways of doing the same problem . for examples check out these questions : why does the weighing balance restore when tilted and released equilibrium and movement of a cylinder with asymmetric mass centre on an inclined plane
there are numerous excellent reviews out , two that come to mind are : r . hoffmann in angew chem int ed engl , 26 , 846 r . hoffmann in rev mod phys , 60 , 601
this paper details the mathematical model behind what you are doing . by far the most difficult aspect is not the balloon itself but collisions with the environment : http://arxiv.org/pdf/physics/0407003.pdf
sure you can clone a state . if you know how to produce it , you can just produce one more copy . the answer to your question therefore lies in the specifics of the no-cloning theorem . it states that it is not possible to build a machine that clones an arbitrary ( previously unknown ! ) state faithfully . stimulated emission does not fulfill this . given an atom , only a certain range of frequencies , etc . can actually be used to produce stimulated emission , so you can not faithfully clone an arbitrary state . it is just an approximation to cloning , which is not prohibited . see also : http://arxiv.org/abs/quant-ph/0205149 and references therein .
$x_{max}$ is the amplitude of the oscillations , and yes , ${\omega}t - \varphi$ is the phase . we know that the period $t$ , is the reciprocal of the frequency $f$ , or $$t = 1/f$$ we also know that $\omega$ , the angular frequency , is equal to $2\pi$ times the frequency , or $$\omega = 2{\pi}f$$ from here , we can use the initial conditions to find the amplitude . $x ( 0 ) = x_{max}cos ( \varphi ) $ $\dot{x} ( 0 ) = {\omega}x_{max}sin ( \varphi ) $ from here it should be a simple matter to find $\varphi$ .
let me begin with the second question where you do not change the dimensionality , just the volume . the entropy never decreases when you actually compress gas . the compression means that the walls are mostly moving against the colliding molecules which means that they are recoiled backwards at higher velocities . the molecules ' kinetic energy increases so they occupy a larger volume in the momentum space ( in macroscopic language , a gas heats up while being compressed ) which at least compensates the decrease of the volume in the position space . the other answer is incorrect . the second laws says not only that systems exhibit some activity indicating that they do not like a decreasing entropy ; instead , it says that whatever activity physical systems display , they will never achieve a macroscopic decrease of the entropy . it is just impossible . to compress gas by 70% is possible , to decrease the entropy by a macroscopic amount is not . now , the interesting first question . if you could change the effective dimensionality , it would still be true in any consistent theory that the entropy can not decrease . so if your theory were just able to add dimensions like that while keeping a molecule in a sphere of the increasing dimension , the second law of thermodynamics would imply that such an addition of dimensions is not physically possible – it would be another , more sophisticated example of the perpetual motion machine of the second kind . in some sense , it is true that the second law encourages physical systems to lose the dimensions ( a way to increase the entropy , given your formula for the higher-dimensional spherical volumes ) . when the energy dissipates , the energy per degree of freedom effectively goes down which allows us to use a lower-dimensional " effective " description . for example , a gas full of kaluza-klein particles probing ( moving in ) extra dimensions will tend dissipate its energy and decay to many lower-energy quanta which are effectively living just in 3+1 dimensions .
" fully compatible with observations " is a rather vague statement . actually , two aspects of adequacy to reality have to be distinguished when a new theory reaches a degree of explicitation . these are compatibility with older theories , in domains where the new theory is not supposed to bring more than a new formulation . for instance , special relativity is compatible with newtonian mechanics when velocities are small compared with c . since older theories taken in reference have been usually thoroughly tested ( otherwise you do not take them as reference ) , this is a good first check for your new theory . compatibility with new phenomena . indeed what makes a new theory interesting is the change of insight that it might bring on reality . and this means that beyond proposing a new description of reality , it shall predict new observable features which older theories do not account for . as far as lqg is concerned , my understanding is that the first aspect has been addressed in the sense that right from the outset , conpatibility with gr has been used as a guide to develop the theory . for the second aspect , this one of the topics which focuses a good part of the efforts of the lqg community . this means finding new observable features that survive going from the planck scale to the scales that are accessible to us in experiments or astrophysical observations . it is tricky but not impossible . so as far as the statement " fully compatible with observations " , i would advise to replace it with " compatible with previous observation-tested theories , but still expecting genuine experimental predictions for testing " .
it is not correct . the meaning of escape velocity is defined the initial kinetic energy in which a particle can go to infinite without going back . that is the kinetic energy have to have the same magnitude as the gravitational potential on earth given by $mv^2/2=gmm/r$ . since the energy is conserved , it does not matter which direction you are pointing to . for the rocket , it has no initial ke and it gains ke and pe by consuming its fuel . the reason that a rocket move straight up is to reduce air friction at the beginning . then it follows a slanted path later is to increase flight time so a only a lower efficiency engine is required .
i have never seen a paper where the calculation is performed in a manifestly covariant manner . however , i have posted a set of reference notes on my website ( http://jacobi.luc.edu/notes.html ) that contains the variations needed to carry out the calculation . let me summarize the calculation here . the action for gravity on a compact region $m$ with boundary $\partial m$ is $$i_{eh} + i_{ghy} = \frac{1}{2 \kappa^2} \int_{m}d^{d+1}x \sqrt{-g} r + \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h} k ~ . $$ the metric on $m$ is $g_{\mu\nu}$ , and $r = g^{\mu\nu} r_{\mu\nu}$ is the ricci scalar . the induced metric on the boundary $\partial m$ is $h_{\mu\nu} = g_{\mu\nu} - n_{\mu} n_{\nu}$ , where $n^{\mu}$ is the ( spacelike ) unit vector normal to $\partial m \subset m$ . now consider a small variation in the metric : $g_{\mu\nu} \to g_{\mu\nu} + \delta g_{\mu\nu}$ . the quantities appearing in the einstein-hilbert part of the action change in the following manner : $$ \delta \sqrt{-g} = \frac{1}{2} \sqrt{-g} g^{\mu\nu} \delta g_{\mu\nu}$$ $$ \delta r = -r^{\mu\nu} \delta g_{\mu\nu} + \nabla^{\mu}\left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda} \right ) $$ thus , the change in $i_{eh}$ is $$\begin{aligned}\delta i_{eh} = and \frac{1}{2\kappa^{2}}\int_{m} d^{d+1}x \sqrt{-g} \left ( \frac{1}{2} g^{\mu\nu} r - r^{\mu\nu} \right ) \delta g_{\mu\nu}\\ and + \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h} \frac{1}{2} n^{\mu} \left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda}\right ) ~ , \end{aligned}$$ with the boundary term coming from the volume integral of the total derivative in $\delta r$ . the variations of the quantities in the ghy term are a bit more complicated to work out , but they all basically follow from standard definitions and this result for the variation of the normal vector : $$\delta n_{\mu} = \frac{1}{2} n_{\mu} n^{\nu} n^{\lambda} \delta g_{\nu\lambda} = \frac{1}{2} \delta g_{\mu\nu} n^{\nu} + c_{\mu}~ . $$ in the second equality i have introduced a vector $c_{\mu}$ that is orthogonal to $n^{\mu}$ ; it is given by $$c_{\mu} = - \frac{1}{2} h_{\mu}{}^{\lambda} \delta g_{\nu\lambda} n^{\nu} ~ . $$ the reason i have introduced this vector is that the variation in the trace of the extrinsic curvature can be written as $$\delta k= - \frac{1}{2} k^{\mu\nu} \delta g_{\mu\nu} - \frac{1}{2} n^{\mu}\left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda} \right ) + d_{\mu} c^{\mu}$$ where $d_{\mu}$ is the covariant derivative along $\partial m$ that is compatible with the induced metric $h_{\mu\nu}$ . so , the change in the ghy part of the action is $$\delta i_{ghy} = \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h}\left ( \frac{1}{2}h^{\mu\nu} \delta g_{\mu\nu} k + \delta k \right ) ~ . $$ combining this with $\delta i_{eh}$ we see that the several terms cancel , leaving $$\begin{aligned} \delta i = and \frac{1}{2\kappa^2}\int_{m} d^{d+1}x \sqrt{-g}\left ( \frac{1}{2} g^{\mu\nu} r - r^{\mu\nu} \right ) \delta g_{\mu\nu}\\ and + \frac{1}{\kappa^2}\int_{\partial m} d^{d}x \sqrt{-h}\left ( \frac{1}{2} ( h^{\mu\nu} k - k^{\mu\nu} ) \delta g_{\mu\nu} + d_{\mu} c^{\mu} \right ) ~ . \end{aligned}$$ we discard the term $d_{\mu} c^{\mu}$ , which is a total boundary derivative .
i am not an expert at thermography per se , but i have designed and built a commercial microbolometer ir camera and looked at several other cameras ( for thermography ) and as far as i know , they do not have any " funny business " going on . commercial handheld thermography cameras require you to input the surface emissivity , air temperature and transmittance parameters etc to get an accurate reading , so obviously in practice for random measurements you will not get anywhere near +/- 1k accuracy , but the noise ( precision ) can be as low as dozens of mk like quoted . in industrial ( repeatable ) applications you can probably calibrate this very accurately though .
assume that radius and charge density for outer ring and inner ring is $r$ , $\lambda$ and $r$ , $\lambda'$ respectively . because $r&lt ; &lt ; r$ you can assume the magnetic field on the whole surface of inner ring to be : $$b=\frac{\mu_0i}{2r}$$ now , $i$ is equal to $\lambda v$ and $v=r\alpha t$ , so $$b=\frac{\mu_0\lambda r\alpha t}{2r}$$ and $ ( r&lt ; &lt ; r ) $ $$\phi=\pi r^2b$$ now we find induced electric field around the first ring . according to faraday 's law : $$e ( 2\pi r ) =\frac{d\phi}{dt}$$ so $$e=\frac{\mu_0\lambda r\alpha}{4}$$ now we find total torque exerted on inner ring : $$\tau=r\lambda ' e ( 2\pi r ) =\frac{\mu_0\lambda \lambda'\pi r^3\alpha}{2}$$ so you can find the angular acceleration by having it is moment of inertia ( for a ring $i_0=mr^2$ ) : $$\alpha'=\frac{\tau}{i_0}=\frac{\mu_0\lambda \lambda'\pi r^3\alpha}{2mr^2}=\frac{\mu_0\lambda \lambda'\pi r\alpha}{2m}$$
imagine a finite segment of the rope , say on the left side . suppose the tension at the top of the segment is $t_t$ and the tension at the bottom of the segment is $t_b$ . then the segment of rope feels a force $t_t$ up and $t_b$ down , or a net force $t_t - t_b$ up . since the rope is massless there is no gravitational force so that $t_t - t_b$ is the net force on the rope . newtons law says that the acceleration of the rope is $\dfrac{t_t - t_b}{m}$ upward , where $m$ is the mass of the rope segment . however , since $m$ is zero this acceleration must be infinite , which is a problem . what is really going on is that if there were a net force up , the rope segment would immediately accelerate upward . this upward movement would relax the tension in the upper part of the rope ( $t_t$ decreases ) and increase the tension in the lower part of the rope ( $t_b$ increases ) . this will continue until $t_t$ equals $t_b$ and there is no net force on the rope . since the rope is massless , this process happens very fast so it can always be assumed that $t_t$ is equal to $t_b$ . since the segment of rope could have been anywhere , this means that any two points on the rope must have the same tension .
your physical intuition is correct , it is indeed sum over all admissible paths . there is a problem with viewing $dx ( t ) $ as a measure on the function space , because it is not well defined ( e . g . infinite at some ' points ' $x ( t ) $ ) . this is one of the big ( and as far as i know open problems ) in the mathematical formulation of path integrals . often one absorbs the kinetic part of the hamiltonian in the measure to get an exponential dumping factor , e.g. $d x ( t ) exp ( -\frac{i}{\hbar} \int t [ x ( t ) ] $ . salmhofer considers in renormalization : an introduction many of the mathematical questions . ( if you know other good sources , i would be happy to hear about them ! ) path integrals are a nice way to ' visualize ' many calculations ( e . g ' i sum xyz over all possible paths ) , but are hard to compute . indeed , the only calculations i know are based on breaking the path in linear segments ( and even this gets clumsy ) . often one performs some kind of taylor expansion and only considers the first orders . there are then rules how to calculate often recurring terms ( see feymann diagrams ) .
in order to do perturbation the expansion parameter needs to be small . otherwise the the system will be strongly coupled and you are in the non-perturbative regime . it is the same as for instance in qm : for perturbative calculations the pertubation must be small .
re question 1 : a processor is obviously a very complex object , but it is made of from basic structures called logic gates . a logic gate consumes power mainly when it is changing state , and the frequency with which it changes state will probably be , on average , proportional to the clock frequency . to work out the work done whenever the gate changes state you can model it as a capacitor with some effective capacitance , $c_g$ , and you get : $$w = \frac{1}{2}c_gv^2$$ and the power is the work per state change times the number of state changes per second , so : $$p_g \propto c_gv^2f$$ if you add up all the logic gates in the processor you can define an effective total capacitance , $c$ , that will be the sum of all the gate capacitances , $c_g$ , so : $$p \propto cv^2f$$ you had have to establish the constant of proportionality by experiment . re question 2 : presumably the cpu is connected to a heatsink , and the equation is just saying that the heat flow into the heatsink ( i.e. . out of the cpu ) is proportional to the temperature difference between the cpu and the ( presumably roughly constant ) temperature of the heatsink . this seems a reasonable approximation , but it is only an approximation . re question 3 : there are a couple of possible mechanisms at work . modern cpus scale their clock frequencies depending on load , so by only loading at 50% the cpu may be running below it is maximum clock speed . i must admit i do not know how the clock scaling works in modern cpus and the chaps at stack overflow or superuser would probably know more about this . the other possibility depends on what the cpu does in the 50% of the time it is not running your program . at the beginning of this answer i said that the frequency with which the logic gates change state will probably be , on average , proportional to the clock frequency . however the constant of proportionality will probably depend on what the cpu is doing . a cpu that is idling may be flipping fewer logic gates per second that the same cpu when it is crunching numbers , so an idling cpu will use less power . that explains why the power usage and hence temperature falls when you limit the cpu used by your program . ( i am assuming it is not a dual core cpu and 50% means only using one core ! )
here is a tutorial on fission . the concept of " activation energy " is not utilized in the study of fission . there are energy levels that can be modeled with nuclear potential well models . mass is turned into energy in fission . what happens is that there is a high cross section for u235 to capture a thermal neutron and become a u236 in an excited state . there are more than one energy levels where this could happen . the configuration is unstable enough that instead of falling down with gamma rays to a ground state of u236 , which is fairly stable , the system ( u235+neutron ) breaks up into kr92 ba141 and 3 neutrons which can with suitable engineering be thermalized to create a chain reaction in a fission reactor .
well first of all the major points here are potential difference across both capacitors will remain same after insertion of dielectric in $c1$ , and the total charge of the system will remain constant . now in $c1$ after insertion of dielectric , calculate change of capacitance , assume that it has new potential $v1$ and charge $q1$ , assume potential and charge for 2nd capacitor too as $v2$ and $q2$ . equate $v1$ and $v2$ , also equate $q1 + q2$ with initial total charge of system . this way you can calculate $v$ across capacitors and $q1$ and $q2$ . now to calculate polarisation charge , check the electric field $ e1 ( without dielectric ) $ now find electric field $e2 ( with dielectric ) $ this difference has come because of insertion of dielectric so the electric field of dielectric ( calculate for a hypothetical parallel plate capacitor ) is equal to the difference in electric fields , this will give you tue induced charge . lastly , since you know initial and final potential and charges , apply formula for potential energy of capacitors both before and after insertion of dielectric and take the difference , this is the change in potential energy . the system undergoes changes as the following , once the capacitors are charged , when dielectric is being inserted in the capacitor $c1$ charges are induced in it and it is pulled in by electrostatic interactions , this process consumes some energy and this changes the potential . also since the capacitors are in parallel and maintain same potential , the charges travel from one capacitor to another after insertion of dielectric to eliminate the potential difference between capacitors .
i will sketch one of the eternal inflation variants : " false-vacuum driven eternal inflation " . the idea is that you start with a spacetime manifold , on which is everywhere defined some scalar field . the scalar field ensures that whole region undergoes inflation . to ensure some sort of stability , the value of the field is chosen to be at a local minimum of the potential - it is not at the global minimum and hence tends to be called a false vacuum . now , due to some sort of perturbation , or due to quantum mechanical tunneling , the scalar field ends up , at least at some location , having a value at the other side of the potential barrier . one tunneling mechanism is described by the coleman de luccia instanton . this point with the new tunneled-to $\phi$ value is thought of as the origination point of a small bubble containing a different phase inside the inflating spacetime . it is sometimes called a nucleation point in analogy with bubble nucleation , in which bubbles form on particles suspended in a liquid . the potential is now able to roll down the hill and release energy inside the bubble . the bubble wall expands and the bubble contents continue to expand , but at an ever decreasing rate . anything inside this bubble , i.e. in the forward light cone of the nucleation point has the new $\phi$ value , and hence physics in there sees a different vacuum . the spacetime inside this bubble has a " natural " foliation by 3-dimenional hypersurfaces of constant $\phi$ . as $\phi$ decreases , these surfaces become spatially flat , like our observable universe . these bubbles ( "pocket universes" ) originate in regions where inflation actually ends . so in this soft of model , the orgin of our observed universe , is the end of inflation . outside of the bubble in question , other bubbles are continually nucleating . bubbles may even merge . of course this sort of model is only as good as the predictions it makes for our observable universe . there has been some discussion of the possible observability of bubble nucleation in our past . see here for example . so , getting back to the question , if you want to attach the term " multiverse " to this scenario , i would assume it refers to the ensemble of pocket universes , embedded in an inflating false vacuum spacetime which , at least in this toy model , obeys the usual laws of gr and quantum mechanics .
the most fundamental definition of temperature is derived from the zeroth law of thermodynamics . the zeroth law declares thermal equilibrium an equivalence relationship , and thus we can tag each equivalence class with a number that we call temperature . or in less mathematical term , temperature is a physical quantity tagged to each thermodynamic system such that any two systems with the same temperature would stay in thermal equilibrium when they contact . the exact way of assigning temperature to a system is called a temperature scale . there were multiple scales before , most based on thermal properties of a particular substance . then kelvin devised a scale based solely on thermodynamic principles , which we call " absolute scale " .
first remark : the correct expression is $$\langle0|\phi ( 0 ) |p\rangle = ( 2\pi ) ^{-3/2}\left ( 2\sqrt{\vec{p}^{2}+m^{2}}\right ) ^{-1/2}n= ( 2\pi ) ^{-3/2}\left ( 2p^0\right ) ^{-1/2}n , $$ where $\vec{p}$ represents a spatial vector . second remark : under a homogeneous lorentz transformation $\lambda$ , the creation operator of a scalar transform as $$u_0 ( \lambda ) a ( \vec{p} ) u_0 ( \lambda ) ^{-1}=\sqrt{ ( \lambda p ) ^0/p^0} a ( \vec{p_\lambda} ) . $$ now , the solution : the idea is to isolate all the terms depending on $p$ from your expression . using the invariance of the vacuum and the scalar under lorentz transformations , we can write $$\langle0|\phi ( 0 ) |p\rangle=\langle0|\phi ( 0 ) a^\dagger ( \vec{p} ) |0\rangle= \langle0|\phi ( 0 ) u_0^{-1} ( \lambda ) a^\dagger ( \vec{p} ) u_0 ( \lambda ) |0\rangle=\sqrt{ ( \lambda p ) ^0/p^0}\langle 0|\phi ( 0 ) a^\dagger ( \vec{p_\lambda} ) |0\rangle . $$ in this expression we are free to choose a convenient lorentz transformation $\lambda$ . if i take a boost that brings the particle to its rest frame , then $\vec{p_\lambda}=0$ and $ ( \lambda p ) ^0=m$ . therefore , if we define $n$ by $$n= ( 2\pi ) ^{3/2} { ( 2m ) }^{1/2}\langle 0|\phi ( 0 ) a^\dagger ( 0 ) |0\rangle , $$ then we have the desired result ( note that $n$ does not depend on the four moment $p$ ) .
in the context of shm they are probably meant to be interpreted as both scalar equations ( ie $x$ , $v$ and $a$ are all one-dimensional ) , especially since they use $x$ and not $r$ , but they can also be written vector form . the first equation would be ( using $r$ for the position vector ) $$ \vec{a} = \frac{d\vec{r}}{dt} $$ no surprises here . the second would be $$ \vec{a} = \frac{d\vec{v}}{d\vec{r}} \vec{v} $$ this is strange - how can you differentiate a vector with respect to another vector ? well , let 's say we are working in three dimensions . then an expression like $$ \frac{d\vec{v}}{d\vec{r}} $$ would be something like , how does $\vec{v}$ change as i change $\vec{r}$ ? but you can change $\vec{r}$ by changing $x$ , $y$ or $z$ , then in each case there will be a change in $v_x$ , $v_y$ and $v_z$ , so you need 9 numbers to specify $\frac{d\vec{v}}{d\vec{r}}$ . as @pranav says , this is a second-order tensor ; in cartesian coordinates this would be $$ \frac{d\vec{v}}{d\vec{r}} = \begin{bmatrix} \dfrac{\partial v_x}{\partial x} and \dfrac{\partial v_x}{\partial y} and \dfrac{\partial v_x}{\partial y}\\ \dfrac{\partial v_y}{\partial x} and \dfrac{\partial v_y}{\partial y} and \dfrac{\partial v_y}{\partial y}\\ \dfrac{\partial v_z}{\partial x} and \dfrac{\partial v_z}{\partial y} and \dfrac{\partial v_z}{\partial y}\\ \end{bmatrix} $$ so the equation does make sense in the end .
i think the answer to this depends a lot on your definition of " directly . " relativity of simultaneity is built into the lorentz transformation , and lorentz invariance is one of the most precisely tested physical theories in all of history . essentially you are asking for an experiment that verifies one element of the matrix involved in the lorentz transformation , but every element of the matrix is present in all cases . i would consider the sagnac effect to be a fairly direct test , and the sagnac effect was one of the effects observed in the hafele-keating experiment , as well as many other , earlier tests of relativity . every time you fly on a commercial jet , you are benefiting from a ring laser gyro , which works based on the sagnac effect .
that prof müller said it : shock waves in addition of the usual things . the picture in your link is rather different from the thing shown in the video , for the time being , i do not understand really what is the new thing . from thermodynamics it is clear , that they ( hope ? ) to have a higher effective deltat , obviously without having higher temperatures at machiney parts ( which makes them expensive and/or short-lived ) i assume that those shock waves can be transformed into working pressure without the high temperatures of the combustion shock wave touchng machine parts . i hope we will hear more from prof müller in near future . edit : this link is somewhat more detailed and less press-release-silly . in general it says what i surmised ( by application of thermodynamics basics ) http://www.zdnet.com/blog/emergingtech/wave-disk-engines-to-make-hybrid-vehicles-cheaper-more-efficient/1887
yes , kinetic energy is a relative quantity . as you might guess , this means that when you are using energy conservation , you have to stay within a single frame of reference ; all that energy conservation tells you is that the amount of energy as measured in any one frame stays the same over time . you can not meaningfully compare the amount of energy measured in frame a ( e . g . the ground ) to the amount of energy measured in frame b ( e . g . the train ) . however , you can convert an amount of kinetic energy measured in one frame to another frame , if you know their relative velocity . if you are working at low speeds , the easy ( approximate ) way to do this is to just calculate the relative velocity , as you did . so if the train observer measures a kinetic energy $k = \frac{1}{2}mv^2$ , the ground observer will measure a kinetic energy of $\frac{1}{2}m ( v + v ) ^2$ , or $$k + \sqrt{2km}v + \frac{1}{2}mv^2$$ ( in one dimension ) . if you get up to higher speeds , or you want an exact expression , you will have to use the relativistic definition of energy . in special relativity , the kinetic energy is given by the difference between the total energy and the " rest energy , " $$k = e - mc^2$$ one way to figure out the transformation rule is to use the fact that the total energy is part of a four-vector , along with the relativistic momentum , $$\begin{pmatrix}e/c \\ p\end{pmatrix} = \begin{pmatrix}\gamma_v mc \\ \gamma_v mv\end{pmatrix}$$ where $\gamma_v = 1/\sqrt{1 - v^2/c^2}$ . this four-vector transforms under the lorentz transformation as you shift from one reference frame to another , $$\begin{pmatrix}e/c \\ p\end{pmatrix}_\text{ground} = \begin{pmatrix}\gamma and \gamma\beta \\ \gamma\beta and \gamma\end{pmatrix}\begin{pmatrix}e/c \\ p\end{pmatrix}_\text{train}$$ ( where $\beta = v/c$ and $\gamma = 1/\sqrt{1 - \beta^2}$ ) , so the energy as observed from the ground would be given by $$e_\text{ground} = \gamma ( e_\text{train} + \beta c p_\text{train} ) $$ the kinetic energy is obtained by subtracting $mc^2$ from the total energy , so you had get $$k_\text{ground} = \gamma ( e_\text{train} + \beta c p_\text{train} ) - mc^2$$ which works out to $$k_\text{ground} = \gamma k_\text{train} + ( \gamma - 1 ) mc^2 + \gamma\beta c p_\text{train}$$ where $k$ is the relativistic kinetic energy and $p$ is the relativistic momentum . if you wanted it in terms of energy alone : $$k_\text{ground} = \gamma k_\text{train} + ( \gamma - 1 ) mc^2 + \gamma\beta\sqrt{k_\text{train}^2 + 2 mc^2 k_\text{train}}$$ you might start to notice a similarity to the non-relativistic expression above ( $k + \sqrt{2km}v + \frac{1}{2}mv^2$ ) , and indeed , if you plug in some approximations that are valid at low speeds ( $\gamma \approx 1$ , $\gamma - 1 \approx v^2/c^2$ , $k_\text{train} \approx \frac{1}{2}mv^2 \ll mc^2$ ) , you will recover exactly that expression .
starting at ${position}_z$ = $z$ = 0 and $v ( z ) = 0$ and by tracking multiple acceleration values either with a time interval or at fixed intervals , $t$ , then you can get the position . . . . somewhat . it will drift over time . also , your device cannot rotate whatsoever , or else you need a gyroscope to track that and then use trigonometry to properly orient the x y and z values from the accelerometer . assuming it is always oriented such that the $a ( z ) $ is always perfect vertical acceleration ( if you are in a vehicle that is always flat , in which case z does not matter , or you are on a vertical guide rail ) , $$p ( z ) = \int_0^t v ( z ) ~dt = \iint_0^t a ( z ) ~dt $$ also , from here : short answer : forget about it . longer answer : unless you are on a perfectly straight rail , you will not achieve what you want to do without ( a ) a set of gyros ; and ( b ) far more accurate sensors than what you have . accelerometers measure acceleration in the body fixed reference frame , whereas you need some displacement in an earth-fixed frame . therefore , you need not only to integrate the accelerometers , but rotate them into the earth-fixed frame before doing the integration . this is assuming perfect sensors . mems sensors are far from perfect - i have written up a post on some of the errors here . consider two errors : 1 . a bias on the accelerometer . 2 . an initial attitude ( tilt ) error . in addition to whatever acceleration signal there is , integrate a bias and you get a ramp error with time . integrate the ramp and you get a quadratically increasing error with time . this will add up really , really quickly . consider a tilt error . you will now be measuring some of the gravity vector in the forward ( or whatever ) direction . integrate this error twice and you will have the same problem as the bias . so , my advice again is do not ! find another method . also , check this book out for more detailed designs , or use whatever sensors and algorithm these guys are on : http://www.youtube.com/watch?v=6ijarke8vku if you still want to give this a shot , use the trapezoidal method in excel , it is pretty easy . there is an explanation page here with a sample , but here 's a more complete way :
you are slightly misinterpreting some words by prof matt strassler . he says that the force mediated by the exchange of the higgs bosons – the " higgs force " – is attractive , much like gravity between two ordinary positive-mass objects . but that does not mean that " everything " in the presence of a higgs field is attractive . indeed , the higgs potential contributes a term to the stress-energy tensor that is proportional to $g_{\mu\nu}$ , and it therefore includes a negative pressure whose magnitude is equal to the energy density . at this qualitative level , the higgs field behaves just like the inflaton . the negative pressure acts in a " repulsive " way and may cause the exponentially accelerating expansion of the universe . however , the detailed shape of the potential and the energy scale associated with the higgs field and the higgs boson is generally different than what one needs for inflation , so the general expectation is that the ordinary higgs field known from its 125 gev higgs bosons is not capable of producing inflation . this viewpoint is ( or would be ) strengthened by the discovery of the primordial gravitational waves by bicep2 which would indicate that the energy density during inflation was huge , near the gut scale , and therefore much higher than the energy densities expected from the relatively light 125 gev higgs boson 's field . on the other hand , there are semirealistic models that use the ordinary higgs field as an inflaton , too . they typically add some new coupling of the field to the curvature . see e.g. http://arxiv.org/abs/0710.3755 and its references and followups . such models could be very economical when it comes to the field content but this attractive feature is compensated by the awkward interactions one has to add . moreover , there are other reasons to think that new physics does occur at the gut scale so particle physicists generally do not view gut as a liability at all . grand unification helps ( or would help ) to explain and unify many other things in physics .
a planet 's ir appearance depends on several factors including its temperature the amount and type of cloud cover the resolution of the observing telescope if you are observing with broadband filters and simply imaging , unless the planet has a complex cloud structure you should expect the planet to behave more or less like a black body at the mean surface temperature of the object . using narrowband filters or low resolution spectroscopy , you start to move more and more away from the ideal blackbody in appearance . especially if there is a complex cloud system on the planet ( like on the jovian planets ) . in this situation , you have a couple of effects . depending on the cloud composition , you will get absorption lines in the spectra that can be detected depending on where your narrowband filters are located or by the low resolution spectroscopy . the position and strength of these bands will depended on the cloud composition and concentration of materials if you have a mostly uniform cloud cover ( like venus ) the effect above will be the primary thing you observe . however , typically you will have gaps in the cloud system . this means that for certain portions of the planet , instead of seeing the cooler tops of the clouds , you see deeper into the planet 's surface or interior where it is warmer . this means that in those areas you will see emission from a warmer blackbody and it will have more flux than you would expect . in this situation you are effectively measuring bits of two ( or possibly more ) different blackbodies . again , depending on the placement of the filters ( if imaging ) or the resolution of your spectrograph you will be able to see these effects . finally , as your spectroscopy goes to higher and higher resolution , you will be able to resolve these effects more and more . here 's a picuture of the earth 's spectrum ( taken from this page . ) : a wave number of 1000 cm -1 corresponds to a wavelength of 10 microns . as you can see it generally follows the shape of a blackbody but there are several absorption bands due to the presence of various elements in the atmosphere . as the planets get bigger and more complex the spectrum differs even more from a black body to the point where what you have is peaks of emission in various bands that are coming from deep in the planet and huge absorptions features from the clouds higher up .
well , let 's see if i understood correctly the question . you are talking of this : you have some kind of pendulum attached to a point r . in fact , the total kinetic energy is $$t=t_{p}+t_{rot}$$ where $t_{p}$ is the translational energy of the point p and $t_{rot}$ the rotational energy around that point . realize that the expression of the two energies is different , depending of your axis . when you consider your axis system at a fixed point , you only have rotational energy , because the fixed point has not a transalation . the other common option is to take the center of mass , and , in that case , you also have to consider its translational energy . this is useful if you do not have any fixed point . however , $t$ is indepedent of the axis , so when you calculate rotational kinetic energy on p , you obtain the total energy ; in the other hand , the center of mass of the system is translating and rotating ( if the rod has mass ) , so translational energy of the ball will be lower than total energy . if the rod has no mass , then the center of mass is only translating and yes , in that case the rotational energy on p would be the same has the ball 's translational energy . to solve problems with fixed point , the best option is to take that point , because you do not have to deal with translational energy . in this particular case , p is a fixed point , so you can calculate the moment of inertia of the system on p ( using steiner 's theorem ) , write the position and the angular velocity as function of $\beta$ , ( angular velocity would be $\dot{\beta}$ ) , and apply energy conservation . if the rod has no mass , you also can try to calculate center of mass position in terms of $\beta$ , derive it to obtain the velocity and calculate the modulus . if the rod has mass , this second method becomes a difficult problem : you have to calculate center of mass of the two bodies , the inertia tensor at that point , and apply both translational and rotational energies . i hope this will be useful . if you need some aclaration , say it .
ok , it is probably a bad idea to exchange in comments . let me expand what i said in the comments . if my understanding is correct , the op wants to know , as the first step toward solving the whole problem , the ground state energy of the many-body hamiltonian $\mathcal{h}$ defined by $$ \mathcal{h} = \sum_{r , s}h_{rs}c^\dagger_r c_{s} , $$ for a given set of parameters $\{ h_{rs}\}$ . here $c^\dagger_{r}$ and $c_{r}$ are standard fermion creation and annihilation operators . the subscripts $r , s$ run over all lattice sites from 1 to $n$ . the hermiticity requires that $$ h_{rs} = h^\ast_{sr} . $$ in other words , the $n\times n$ matrix $h$ , whose $ ( r , s ) $ entry is defined to be $h_{rs}$ , must be hermitian . in some literature , $h$ is known as the " first-quantized hamiltonian " . note that the above $\mathcal{h}$ takes a slightly more general form than the one described by op . the first step is to diagonalize $\mathcal{h}$ . to this end , we introduce a new set of fermion operators : $$ c_{r} = \sum_{m}v_{rm}f_{m} ; \quad{}c^\dagger_{r}=\sum_{m}v^\ast_{rm}f^\dagger_{m} . $$ we demand that the new fermion operators obey the standard fermion algebra . it can be seen that this is amount to demand $$ \sum_{m}v_{rm}v^\ast_{sm}=\delta_{rs} , $$ or equivalently $vv^\dagger=1_n$ , i.e. $v$ is a unitary $n\times n$ matrix . substituting the above in , we find $\mathcal{h}$ written in terms of new fermion operators , $$ \mathcal{h} = \sum_{r , s , m , n}v^\ast_{rm}v_{sn}h_{rs}f^\dagger_m f_n = \sum_{m , n} ( v^\dagger hv ) _{mn}f^\dagger_m f_n . $$ since $h$ is hermitian , we can always find a unitary $v$ so that $h$ is diagonalized : $$ v^\dagger hv = \lambda . $$ here $\lambda = \textrm{diag} ( \lambda_1 , \lambda_2\cdots , \lambda_n ) $ . $\lambda_i\in\mathbb{r}$ are eigenvalues of $h$ . thus , $$ \mathcal{h} = \sum_{m}\lambda_m f^\dagger_m f_m . $$ this is the desired diagonalized form of $\mathcal{h}$ . the second step is to find the ground state energy of $\mathcal{h}$ . we see that all eigenstates of $\mathcal{h}$ are labeld by the occupation numbers $f^\dagger_mf_m$ . it is easy to see that the ground state of $\mathcal{h}$ is constructed by filling up all modes with negative energy . in other words , in the ground state , $$ f^\dagger_m f_m=\left\{\begin{array}{cc} 1 and \lambda_m&lt ; 0\\ 0 and \lambda_m&gt ; 0 \end{array} \right . . $$ there will be degeneracy if some $\lambda_m = 0$ . then , the ground state energy is $$ e_{g}=\sum_{m , \lambda_m&lt ; 0}\lambda_m . $$
for the photons that make up light to exist they have to be travelling at the speed of light . this means that to store them you have to put them in a container where they can move around at the speed of light until you want to let them out . you could build the container out of mirrors , but no mirror we can build is 100% reflective , or indeed can be 100% reflective . usually when a photon " hits " the mirror it is absorbed by one of the atoms in the mirror and then re-emitted back out into the container . however , occasionally the photon either will not get re-emitted ( leaving the atom in an excited state ) or it does not hit one of the atoms and makes it way through the mirror and out of the container . while the chances of this happening for an individual photon are low , there are lots of photons travelling very fast so it happens many times thus causing the light to " leak " or decay . building a near perfect mirror is hard , so it is easier to convert the light into something that can be stored and then convert that back into light when need it .
it is a good question ! physics is all about linking your intuition to science , so it is good that you are thinking about this . the statement that momentum should be proportional to mass and velocity is intuition . like elfmotat says , you can choose your constant as long as it is consistent with units , i guess . if you want another reason , consider the time-derivative of the equation $p=mv$ . it is newton 's second law ! $f=ma$ . in other words , $f=\frac{dp}{dt}$ , which is a great reason for k to be 1 .
in classic electrodynamics it is well known that an accelerated charge will radiate energy and the radiated power is given by the larmor formula $$p=\frac{\mu_0q^2a^2}{6\pi c} , $$ in si units , where $c$ is the speed of light , $q$ is the charge of the particle and $\mu_0$ the magnetic constant . well , to incorporate the larmor formula into newtonian mechanics we claim that : if the particle is losing energy while accelerating then the effect can be expressed as if the particle feels the effect of a force apposing it is movement . to account for this effect let 's say that the 2nd newton equation can be written as $$m \vec{\dot{v}}=\vec{f}_{ext}+\vec{f}_{rad} , $$ where $\vec{f}_{ext}$ is an exterior force that accelerates the particle and $\vec{f}_{rad}$ is some force that would account for the energy ( photons ) radiated by the accelerated charge . now to move on we impose the following : $\vec{f}_{rad}$ has to be such that the work it does is equal to the energy lost by the particle accordingly to the larmor formula ; we can the use this imposition to write that in an interval $t_2-t_1$ , such that the system is in the same state at those instants , i mean the velocity and the acceleration of the particle is the same in $t_1$ and $t_2$ $^{\bf 1}$ , the work done by $\vec{f}_{rad}$ is equal to energy lost by the particle , then : $$\int_{t1}^{t2}\vec{f}_{rad}\cdot \vec{v}~dt=-\int_{t1}^{t2}\frac{\mu_0q^2}{6\pi c}\dot{\vec{v}}\cdot \dot{\vec{v}}~dt , $$ where i have written $a^2=\dot{\vec{v}}\cdot \dot{\vec{v}}$ . now , integrating by parts we can write : $$\int_{t1}^{t2}\vec{f}_{rad}\cdot \vec{v}~dt=\frac{\mu_0q^2}{6\pi c}\int_{t1}^{t2}\ddot{\vec{v}}\cdot \vec{v}~dt-\frac{\mu_0q^2}{6\pi c} ( \dot{\vec{v}}\cdot \vec{v} ) |_{t_1}^{t_2} . $$ because the particle is in the same state in both the instants $t_1$ and $t_2$ the second term of the sum is zero and then we can write : $$\int_{t1}^{t2}\left [ \vec{f}_{rad}-\frac{\mu_0q^2}{6\pi c}\dot{\vec{a}} \right ] \cdot \vec{v}~dt=0 . $$ since this must be true for all $\vec{v}$ you get the famous abraham-lorentz force . : $$\vec{f}_{rad}=\frac{\mu_0q^2}{6\pi c}\dot{\vec{a}} . $$ this equation has some serious problems when you try to use it . you can see those in this reference . also in that reference eric poisson shows the special relativity generalization of the al force called the abraham-lorentz-dirac force . as for the second part of the question : it is well known that any interaction that propagates at a finite velocity originates a self-force effect . so in classical mechanics only the electromagnetic interaction travels at finite velocity , $c$ , so newtonian gravity does not originate self-force effect since the interaction propagates at infinite velocity . but knowing that , lorentz invariance is correct ( well at least we physicists believe so , and all the tests indicate towards that belief ) nothing can propagate at infinite velocity , and actually there is a maximum velocity allowed , $c$ , so all interactions should originate a self-force effect ( usually called radiation reaction ) . ps : if anybody is still with me after this very long answer and you remember me writing that newtonian gravity would not originate radiation reaction , well in the modern way of looking at gravity ( general relativity ) gravitational waves propagate at the speed of light so there must be a gravitational self-force effect . . . more on that you can check this reference in which eric poisson et al . deduce the first order correction to the movement of a particle in curved space-time . $_{\bf 1}$ more on this you can check the book from griffiths - chapters 10 and 11 .
basically you have a train with an observer a inside who emits a beam of light to the left which is reflected off a wall . . . let 's call that wall w , for reference below . . . at distance $d$ from a . . . . let 's call that distance $d := aw$ ( anticipating that distances between certain other participants will have to be considered , and distinctly named , below ) . the time it takes for the beam to get back to the observer is $t_0 = \frac{2 \ , d}{c}$ which is the proper time . . . . a.k.a. " ping duration " $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ . so far , so good . now consider an observer b outside the train . the train is moving at a velocity $v$ to the right relative to b . ( also : b as well as everyone at rest wrt . b are moving at velocity $v$ relative to a and w ; from a towards w . ) thus the time it takes for the light to hit the wall is $\frac{d}{c + v}$ that is eventually incorrect . let 's try to be more precise : b and a were passing each other ( which is supposed to be visible by everyone else ; a.k.a. " emitting a light signal " ) , there exists some participant ( let 's call it p ) who was and remained at rest wrt . b , who therefore of course was passed by w " sometime " , and specificly : whose indication of being passed by w was simultaneous to b 's indication of being passed by a , and there exists some participant ( let 's call it q ) who was and remained at rest wrt . b ( as well as p ) and who was passed w just as q and w observed ( together , at their meeting ) that b and a had passed each other . therefore $\frac{pq}{bq} = \frac{v}{c}$ , and $\frac{bp}{bq} = \frac{pq}{bq} + 1 = 1 + \frac{v}{c} = \frac{c + v}{c}$ . of interest is then b 's duration from the indication of being passed by a until the indication simultaneous to q 's indication of being passed by w ( and observing that b and a had passed each other ) . this is of course half of the ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_b [ q ] $ , i.e. half of $\frac{2 \ , bq}{c}$ ; thus $\frac{bq}{c}$ which is in turn equal to $\frac{bp}{c + v}$ . and the time it take for it to return to a is $\frac{d}{c - v}$ . arguing similarly to the above , this corrsponding duration of b is more precisely equal to $\frac{bp}{c - v}$ . now , in order to compare a 's ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ with the sum of the corresponding durations of b , i.e. with $\frac{bp}{c + v} + \frac{bp}{c - v} = \frac{2 \ , c \ , bp}{c^2 - v^2} = \frac{2 \ , bp}{c} \frac{1}{1 - \beta^2}$ we still need to establish value of the distance ratio $\frac{aw}{bp}$ . that means we are now left with having to derive " length contraction " ! but that is not difficult , given all of the explicit setup and named participants that were introduced above already : we should consider one more participant , j , who was and remained at rest wrt . a and w , and whose indication of being passed by b was simultaneous to w 's indication of being passed by q ( and observing that b and a had passed each other ) . therefore $\frac{aj}{aw} = \frac{v}{c}$ , and $\frac{jw}{aw} = 1 - \frac{aj}{aw} = 1 - \frac{v}{c} = \frac{c - v}{c}$ . considering the two explicit requirements of simultaneity above , the corresponding ratios of distances should be equal : $\frac{bp}{aw} = \frac{jw}{bq}$ . inserting expressions from above : $\frac{bp}{aw} = \frac{jw}{aw} \frac{aw}{bp} \frac{bp}{bq} = \frac{c - v}{c} \frac{aw}{bp} \frac{c + v}{c} = \frac{aw}{bp} \frac{c^2 - v^2}{c^2} = \frac{aw}{bp} ( 1 - \beta^2 ) = \sqrt{ 1 - \beta^2 }$ . consequently : $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c} = \frac{2 \ , bp}{c} / \sqrt{ 1 - \beta^2 }$ . calling b 's corresponding duration $\frac{2 \ , bp}{c} \frac{1}{1 - \beta^2} = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w}$ therefore $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w} \sqrt{ 1 - \beta^2 }$ , as may have been expected . last week in class we derived the formula for time dilation using light clocks well , should not that have been pretty much the derivation i just sketched ? . edit for completeness , and to emphasize a particular point in the following , here 's also the derivitation involving light clocks " perpendicular to the direction of motion " ( which seems to have been mentioned in passing in the op 's question ) : expanding on the setup described above , with the principal protagonists a and b and suitable auxiliary participants ( w and j at rest wrt . a ; p and q at rest wrt . b ) , and all of them " sitting or moving in one line " , we now also consider participant f ( at rest wrt . a , j , w ) with distance ratios $\left ( \frac{af}{fj} \right ) ^2 + \left ( \frac{aj}{fj} \right ) ^2 = 1$ , and ( without loss of generality , but just to re-use setup relations from above ) with $\frac{aw}{fj} = 1$ , therefore $\frac{af}{fj} = \sqrt{ 1 - \left ( \frac{aj}{fj} \right ) ^2 } = \sqrt{ 1 - \left ( \frac{aj}{aw} \right ) ^2 } = \sqrt{ 1 - \beta^2 }$ ; and participant g ( at rest wrt . b , p , q ) with distance ratios $\left ( \frac{bg}{gp} \right ) ^2 + \left ( \frac{bp}{gp} \right ) ^2 = 1$ , and such that g and f met each other in passing . importantly , the entire region containing the setup is of course supposed to be flat . therefore it can be demonstrated ( what otherwise may be glanced over for seeming " too obvious to even point out" ) , that f 's indication of having been passed by g was simultaneous to a 's indication of having been passed by b ; and vice versa that g 's indication of having been passed by f was simultaneous to b 's indication of having been passed by a . then , by the same argument that was used above for comparison of distance ratios between pairs of participants who were not at rest to each other , we set : $\frac{af}{bg} = \frac{bg}{af}$ , and therefore $\frac{af}{bg} = 1 . $ with $\mathop{\delta , \tau_a}\limits_{\text{ping trip } b_g} = \frac{2 \ , fj}{c} = \frac{2 \ , af}{c} / \sqrt{ 1 - \beta^2 }$ and $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \frac{2 \ , bg}{c}$ follows $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \mathop{\delta \ , \tau_a}\limits_{\text{ping trip } b_g} \sqrt{ 1 - \beta^2 }$ . finally , as can be shown explicitly , it holds symmetrically that $\mathop{\delta}\limits_{\text{ping}} \tau_a [ f ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_f} \sqrt{ 1 - \beta^2 }$ .
i would like to share my thoughts and questions on the issue . the boltzmann h theorem based on classical mechanics is well discussed in various literatures , the irreversibility comes from his assumption of molecular chaos , which cannot be justified from the underlying dynamical equation . here i will try to say something on quantum h theorem , the point i want to make is that , although seemingly h theorem can be derived from unitarity , the true entropy increase in fact comes from the non-unitary part of quantum mechanics . let me first recap the derivation using unitarity $^{1,2}$ . h theorem as a consequence of unitarity denote by $p_k$ the probability of a particle appearing on the state $|k\rangle$ , $a_{kl}$ the transition rate from state $|k\rangle$ to state $|l\rangle$ , then by the master equation $${\frac {dp_{k}}{dt}}=\sum _{l} ( a_{{kl }}p_{l }-a_{{l k}}p_{k} ) =\sum _{{l\neq k}} ( a_{{kl }}p_{l }-a_{{l k}}p_{k} ) \cdots\cdots ( 1 ) . $$ then we take the derivative of entropy $$s=-\sum_k p_k\ln p_k\cdots\cdots ( 2 ) , $$ we obtain $$\frac{ds}{dt}=-\sum_k\frac{dp_k}{dt}\left ( 1+\ln p_k\right ) \cdots\cdots ( 3 ) . $$ together with ( 1 ) we have $$\frac{ds}{dt}=-\sum_{kl}\left\{ ( 1+\ln p_k ) a_{{kl }}p_{l }- ( 1+\ln p_k ) a_{{l k}}p_{k}\right\}\cdots ( 4 ) . $$ for the seond second term let us interchange the dummy indices $k$ and $l$ , we get $$\frac{ds}{dt}=\sum_{kl} ( \ln p_l-\ln p_k ) a_{kl}p_l\cdots\cdots ( 5 ) $$ now use the mathematical identity $ ( \ln p_l-\ln p_k ) p_l\geq p_l- p_k$ , we obtain $$\frac{ds}{dt}\geq \sum_{kl} ( p_l-p_k ) a_{kl}= \sum_{kl}p_l ( a_{kl}-a_{lk} ) \\=\sum_{l}p_l\big\{\sum_{k} ( a_{kl}-a_{lk} ) \big\}\cdots\cdots ( 6 ) $$ now unitarity ensures $\sum_{k}a_{kl}$ and $\sum_{k}a_{lk}$ are both 0 , because as transition rates , $$\sum_{k}a_{kl}=\frac{d}{dt}\sum_{k}|\langle l|s|k\rangle|^2=\frac{d}{dt}\sum_{k}\langle l|s|k\rangle\langle k|s^{\dagger}|l\rangle\\=\frac{d}{dt}\langle l|ss^{\dagger}|l\rangle=\frac{d}{dt}\langle l|l\rangle=0\cdots\cdots ( 7 ) , $$ where $s$ is the unitary time evolution operator describing the system . this is nothing but saying the total transition probability from one state to all states must be 1 . it is clear ( 6 ) and ( 7 ) imply the h theorem : $$\frac{ds}{dt}\geq 0 . $$ where does the irreversibility come from ? now we are in a position to question ourselves with loschmidt 's paradox , analogously to its classical version : there are many unitary and time-reversible quantum mechanical systems , if we have just derived h theorem using unitarity alone , how can it be reconciled with time-reversibility of the underlying dynamics ? what sneaked into the above derivation ? the crucial thing to notice is that , in the quantum regime , the definition of entropy using equation ( 2 ) is inherently an impossible one : the value of the entropy in ( 2 ) depends on the basis we choose to describe the system ! consider a two-level system with two choices of orthogonal basis $\{|1\rangle , |2\rangle\}$ and $\{|a\rangle , |b\rangle\}$ related by $$|1\rangle=\frac{1}{\sqrt2} ( |a\rangle+|b\rangle ) , \\|2\rangle=\frac{1}{\sqrt2} ( |a\rangle-|b\rangle ) . $$ suppose the system is in the state $|1\rangle$ , then the entropy formula gives $s=0$ in the first choice of basis since it has 100% chance to appear in $|1\rangle$ , while in the other basis $s=\ln2$ since it has 50%-50% chance to appear in either $|a\rangle$ or $|b\rangle$ . now we may argue , it is one thing that to say the system is in $\frac{1}{\sqrt2} ( |a\rangle+|b\rangle ) $ and have the potential 50%-50% chance to transit into $|a\rangle$ and $|b\rangle$ after a measurement , but a different thing to say the transition has been realized by some measurement . two situations must be described differently . if we look back to our derivation , it is not hard to see what we really did was , after a basis state evolves to a new state which is a superposition of the basis states , we assumed transitions to original basis states have happened instead of just staying in that superposition state , and in fact the original definition of entropy is not capable of describing such situation , as explained just now . a plausible definition of quantum entropy is the von neumann entropy , which is a basis-independent definition of entropy , and in this description , the entropy of a unitarily evolving system is constant in time , while a ( projective ) measurement can increase the entropy . based on the above comparison , we see the irreversibility really comes as an assumption , the assumption that a measurement/decoherence has happened , and as we know , a ( projective ) measurement is a non-unitary , irreversible process , no paradox anymore . my own question on the issue is , what to make of the fact that von neumann entropy is constant in time ? does it mean it is incapable of describing a closed system evolving from non-equilibrium to equilibrium , or should we just reverse the argument and say any non-equilibrium to equilibrium evolution must be described by some non-unitary process ? 1 . rephrased from section 3.6 of the quantum theory of fields , vol1 , s . weinberg 2 . if i remember correctly ( which i am not quite confident on ) , such derivation was first given by pauli , and he correctly spotted the origin of irreversibility , which he called the " random phase assumption " .
there are 3 actions of the galilean group on the free particle : on the configuration space , on the phase space and on the quantum state space ( wave functions ) . the galilean lie algebra is faithfully realized on the configuration space by means of vector fields , but its lifted action on poisson algebra of functions on the phase apace and on the wave functions ( by means of differential operators ) is the central extension of the galilean algebra , known as the bargmann algebra in which the commutator of boosts and momenta is proportional to the mass . the reasoning is given in the following arguments 1 ) the action on the configuration space : $q = \{x^1 , x^2 , x^3 , t\}$: here the translations and the boost operators act as vector fields and their commutator is zero : translation : $x^i \rightarrow x^i+c^i$ , generating vector $p_i = \frac{\partial}{\partial x^i}$ boost : $x^i \rightarrow x^i+v^i t$ , generating vector $g_i = t \frac{\partial}{\partial x^i}$ this is a faithful action of the galilean group : $ [ p_i , g_j ] = 0$ . 2 ) the lifted galilean action to the phase space $q = \{x^1 , x^2 , x^3 , p_1 , p_2 , p_3\}$ the meaning of lifting the action is to actually write the lagrangian and finding the noether charges of the above symmetry : the charges as functions on the phase space will generate the centrally extended version of the group . an application of the noether theorem , we obtain the following expressions of the noether charges : translation : $p_i = p_i$ boost : $ g_i = p_i t - m x^i$ . the canonical poisson brackets at $t=0$ ( because the phase space is the space of initial data ) : $\{p_i , g_j\} = m \delta_{ij}$ the reason that the lifted action is a central extension lies in the fact that that the poisson algebra of a manifold itself is a central extension of the space of hamiltonian vector fields , $$ 0\rightarrow \mathbb{r}\overset{i}{\rightarrow} c^{\infty} ( m ) \overset{x}{\rightarrow} \mathrm{ham} ( m ) \rightarrow 0$$ where the map $x$ generates a hamiltonian vector field from a given hamiltonian : $$x_h = \omega^{ij}\partial_{j}h$$ ( $\omega$ is the symplectic form . the exact sequence simply indicates that the hamiltonian vector fields of constant functions are all zero ) . thus if the lie algebra admits a nontrivial central extension , this extension may materialize in the poisson brackets ( the result of a poisson bracket may be a constant function ) . 3 ) the reason that the action is also extended is that in quantum mechanics the wave functions are sections of a line bundle over the configuration manifold . a line bundle itself is a $\mathbb{c}$ bundle over the manifold : $$ 0\rightarrow \mathbb{c}\overset{i}{\rightarrow} \mathcal{l}\overset{\pi}{\rightarrow} m\rightarrow 0$$ thus one would expect an extension in the lifted group action . line bundles can acquire a nontrivial phases upon a given transformation . in the case of the boosts , the schrödinger equation is not invariant under boosts unless the wave function transformation is of the form : $$ \psi ( x ) \rightarrow \psi' ( x ) = e^{\frac{im}{\hbar} ( vx+\frac{1}{2}v^2t ) }\psi ( x+vt ) $$ the infinitesimal boost generators : $$\hat{g}_i = im x_i + \hbar t \frac{\partial}{\partial x_i}$$ thus at $t=0$ , we get : $ [ \hat{g}_i , \hat{p}_j ] = -im \hbar\delta_{ij}$ thus in summary , the galilean group action on the free particle 's configuration space is not extended , while the action on the phase space poisson algebra and quantum line bundle is nontrivially central extended . the classification of group actions on line bundles and central extensions may be performed by means of lie group and lie algebra cohomology . a good reference on this subject is the book by azcárraga , and izquierdo . this book contains a detailed treatment of the galilean algebra cohomology . also , there are two readable articles by van holten : ( first , second ) . group actions on line bundles ( i.e. . quantum mechanics ) is classified by the first lie group cohomology group , while central extensions are classified by the second lie algebra cohomology group . the problem of finding central extensions to lie algebras can be reduced to a manageable algebraic construction . one can form a brst operator : $$ q = c^i t_i + f_{ij}^k c^i c^j b_k$$ where $b$ abd $c$ are anticommuting conjugate variables : $\{b_i , c_j \} = \delta_{ij}$ . $t_i$ are the lie algebra generators . it is not hard to verify that $q^2 = 0$ if we can find a constant solution to the equation $q \phi = 0$ with $\phi = \phi_{i j} c^i c^j$ which takes the following form in components , we have $$ f_{ [ ij|}^k \phi_{k|l ] } = 0$$ ( the brackets in the indices mean that the indices $i , j , l$ are anti-symmetrized . then the following central extension closes : $$ [ \hat{t}_i , \hat{t}_j ] = i f_{ij}^{k} \hat{t}_k + \phi_{ij}\mathbf{1}$$ the second lie algebra cohomology group of the poincaré group is vanishing , thus it does not have a nontrivial central extension . a hint for that can be found from the fact that the relativistic free particle action is invariant under poincaré transformations . ( however , this is not a full proof because it is for a specific realization ) . a general theorem in lie algebra cohomology asserts that semisimple lie algebras have a vanishing second cohomology group . semidirect products of vector spaces and semisimple lie algebras have also vanishing second cohomology provided that there are no invariant two forms on the vector space . this is the case of the poincaré group . of course , one can prove the special case of the poincaré group by the brst method described above .
a good way to understand this is to delve a little more deeply into the meaning of timelike coordinates in flat space , then move up to schwarzschild space . in flat space , we have the metric $$ds^2 = -dt^2 + dx^2+dy^2+dz^2 . $$ looking at the metric , we can see that only one of the spacetime coordinate differentials gets a negative sign ( $dt^2$ ) . looking closer , you can see that this term is the only one that will contribute negatively to the metric line element , no matter the values of $dx , dy , dz$ , or $dt$ . in general , you can think of the timelike coordinate as the one that will contribute negatively to the metric line element . what makes a coordinate timelike ? a good way to think about this question is to think about how time differs from the other spacetime coordinates we are used to in flat space . for us , time can only move forward ( as @elfmotat says , you can not avoid getting older ) , while we can move freely in the other spacetime coordinate directions . so , for particles , if a coordinate contributes negatively to the line element , then you can only move forward in the direction of that coordinate . to motivate this , i am just going to appeal to your intuition for lightcones . in minkowski space : particles are forced to follow timelike geodesics $ ( ds/d\tau ) ^2 =-1$ , all of which lie within the future lightcone in the diagram . so the statement that the coordinate that contributes negatively to the metric line element is timelike is kind of a geometrical one . basically , it confines you to a certain region of spacetime and in so doing only allows forward motion . now let 's look at the $r , t$ lightcone around a black hole : once you have crossed the event horizon , all worldlines move backward in $r$ . so we say that $r$ becomes timelike while $t$ becomes spacelike . in the schwarzschild metric , $$ds^2 = -\left ( 1-\frac{2gm}{r}\right ) dt^2 + \left ( 1-\frac{2gm}{r}\right ) ^{-1}dr^2+r^2d\omega ^2 , $$ we can see that for $r &gt ; 2gm$ , the terms in the parenthesis are positive , and therefore the timelike coordinate is $dt$ , since it already has a negative sign outside it . but for $r &lt ; 2gm$ , the terms in the parenthesis are negative , so the time-spacelikeness of $dt , dr$ flip , and $dr$ becomes the only coordinate that contributes negatively to the metric line element . what this means physically is that you can only move forward in $r$ , so in that sense $r$ becomes a timelike coordinate . a bit more pondering should reveal that this mathematical treatment of the problem reproduces the well-known black hole behavior exactly . ( can not go back when you have crossed the event horizon ) .
first thing : even with huge fields , iron will not " float " like that in circular motion . iron will always be attracted to the region where the magnetic field is the most intense : which is in contact with the conductor/superconductor . in addition , there is a limit to the magnetic field you can achieve with any superconductor : it is called the critical field . there are 2 types of critical fields , hc_1 and hc_2 , which correspond to 2 things : hc_1 corresponds to the maximum field that can be " ejected " ( screened ) automatically by the superconductor ( meissner effect : http://en.wikipedia.org/wiki/meissner_effect ) . it corresponds to a magnetic energy ( b^2/ ( 2*µ0 ) locally ) that equals the volumic energy difference at zero field , between normal ( resistive ) and superconducting macroscopic states . hc_2 corresponds to another limit , only existing in so-called type ii superconductors : when hc_1 is reached , either the whole bulk superconductor goes to resistive state ( type i superconductors ) , or an interface forms , between an enclosed resistive region ( in a tube like shape ) , and the surrounding superconducting region . the resistive region lets flow a quantum of magnetic field ( h/e_s , where e_s is the charge of the superconducting pair , usually 2*e , twice the electron charge ) , while the current to confine it flows in the nearby superconducting surrounding . these " quantum vortices " multiply to let flow the excess magnetic field , until their density is again too high for the whole superconductor to keep beeing superconducting : again a question of lower energy . too many interfaces , too few bulk superconductor to lower the overall energy , and the whole superconductor transits to resistive state . this field , linked with the too high density of vortices , is hc_2 . it is of the order of 10-20 tesla in best cases ( nb3sn , niobium-tin : 15 tesla ) , usually much less . the " floating " of magnets above superconductors , are due to these vortices , which cannot move in space as freely as usual magnetic flux lines in vaccuum can . this phenomenon " traps " the field exactly as it is , and the magnet with it , and only a force above a certain threshold ( usually higher than the weight of the magnet used for the demonstration ) can move the magnetic flux lines inside the superconductor . this " floating " is the only difference in principle between superconductors and usual conductors . one last thing : you can have objects , even living creatures like frogs , floating inside a copper coil , but this happens only with very high fields ( more than 10 teslas for water , see http://en.wikipedia.org/wiki/magnetic_levitation#diamagnetism ) hope i answered your question :- )
in general it is not possible to tell , because some colours simply cannot be assigned a definite wavelength . some colours , like green or blue , have essentially unique combinations of wavelengths that produce the ( subjective ) impression of green or blue . some colours , like yellow , can be produced in multiple ways : light of 580 nm will look yellow but so will certain combinations of blue and green . certain colours , like pink or most earth-toned colours , can only be produced by combining multiple different wavelengths , so they cannot be assigned a unique wavelength of their own . in general , the most complete way to characterize the ' colour ' of an object is not its colour but its reflectance spectrum : how much of each wavelength it reflects from a white light . for the turquoise mineral , this looks like this spectrum , taken from ultraviolet-visible , near infrared and mid infrared reflectance spectroscopy of turquoise . b . reddy , r . frost , m . weier and w . martens . j . near infrared spec . 14 no . 1 , p . 241 ( 2006 ) . the spectrum depends on the specific sample . reddy et al . also present the spectrum of a sample from senegal , which as you can see shares some features but not others . ( in particular , it has a lot more content in the region from 350 nm to 550 nm , which is precisely the blues and greens , compared to the hump at the red end of the spectrum . )
yes , equating the acceleration due to gravity with centripetal acceleration , i get the following required speed ( which differs slightly from your rough figure ) : $$ v=\sqrt{gr} = 28,455\ , \mathrm{kph} $$ but yes , assuming you were to get the pod moving at that speed before it enters the evacuated tube , then once it enters it will be in orbit just above the surface of the earth for the entire duration of its flight in the tube , in this case that duration would be roughly 8 minutes and 45 seconds .
i am not an expert in algebraic topology by the stretch of anyones imagination , but hopefully i can shed some light on this . the starting point is , as you mention , the maxwell equations themselves . cast into a geometric language the curvature 2-form $\bf{f}$ , which you can think of as the faraday tensor for $u ( 1 ) $-maxwell theory ( there are generalisations for yang-mills theory ) , can be written $\mathbf{f}$ $= e \wedge d \sigma + b$ and we have the equations : $d \bf{f} = 0$ $\ , \ , \ , \ , \ , \ , ; \ , \ , \ , \ , \ , \ , $ $d^{\star} \bf{f} = \bf{j}$ the ' self-dual ' here is referring to hodge duality apearing in the above , since in vacuum we have an obvious symmetry here . a good physical description can be found here ( hodge star operator on curvature ? ) . so , we have a space-time manifold $\mathcal{m}$ that has a curvature 2-form satisfying some properties ( maxwell equations ) . what can this tell us about the topological structure on $\mathcal{m}$ ? the theory of de-rham cohomology is essentially the study of differential forms on manifolds . the idea is that by analysing the way in which $p$-forms behave one can deduce some global structure properties . this makes physical sense to me , since if certain classes of functions are behaving in very specific ways it must say something regarding the curvature of the manifold , right ? herein the link lies and why things can be said regarding the $\it{second}$ homology group , since $\bf{f}$ is a 2-form . a little more mathematical : if a $k$-form $\omega$ satisfies $d \omega = 0$ it is called closed . if one can write $\omega = d \lambda$ for some $ ( k-1 ) $-form $\lambda$ , $\omega$ is called exact . cohomology is the study of whether or not these two notions are interchangeable . the idea is analogous to gauge potentials for maxwell 's equations in $\mathbb{r}^{4}$ wherein we have the identity $\nabla \times ( \nabla a ) = 0$ for any function $a$ , which we of course know as the vector potential associated with $\bf{f}$ . consider the space of closed $k$-forms : $z^{k} ( \mathcal{m} ) = \{ \omega \in c^{k} ( \mathcal{m} ) : d \omega = 0 \}$ so if $\omega$ is closed then so is $\omega + d \tau$ . so , we have a very natural equivalence relation on $z^{k}: \omega \sim \omega'$ iff their difference is exact . the $k$-th de-rham cohomology $h^{k} ( \mathcal{m} ) $ is defined as a quotient of $z^{k}$ by the space of exact forms : $b^{k} ( \mathcal{m} ) = \{d \lambda : \lambda \in c^{k-1} ( \mathcal{m} ) \}$ as $h^{k} ( \mathcal{m} ) = z^{k} ( \mathcal{m} ) /b^{k} ( \mathcal{m} ) $ the dimensions of $h^{k}$ are called the betti numbers $b_{k} = \dim h^{k} ( \mathcal{m} ) $ . which are a topological invariant of the space . the euler characteristic is defined in terms of them also : $\chi = \sum ( -1 ) ^{k} b_{k}$ , which is an important curvature invariant . on manifolds it tells you whether your space is compact if it vanishes , for example , and relates to the hodge dual contracted riemann tensor . edit ( some more ) : in essence , this means we have a specific way to test the homological structures since , by definition of the maxwell equations , we have $\bf{f}$ $\in z^{2} ( \mathcal{m} ) $ . as a last note , the hodge dual gives a canonical way to associate ( using poincaré duality ) $h^{k} ( \mathcal{m} ) $ with its dual space . so the maxwell equations really give some deep insight into both ( co ) -homological groups in vacuum . a good reference is this paper by dotti and kozameh ( http://www.famaf.unc.edu.ar/~gdotti/1.pdf ) .
$$a=\pi r^2$$ $$\frac{da}{dr}=\pi\cdot2r$$ $$da=2\pi rdr$$ alternatively , you can write : $\lim_{\delta r\to 0}\frac{\delta a}{\delta r}=\lim_{\delta r\to 0}\frac{\pi\{ ( r+\delta r ) ^2-r^2\}}{\delta r}=\lim_{\delta r\to 0}\frac{2\pi r\delta r+\delta r^2}{\delta r}=2\pi r+0$ you have to ignore $ ( dr ) ^2$ as it is very small . why ? because you took the limit while taking infinitesimal rings .
but that is exactly the deeper meaning ! setting things up so that all coordinates are in the same units ( besides being a reasonable requirement for $x^\mu$ to be considered a four-vector ) is a constant reminder that time is really not that different from space . in fact , if it were not for that sign in the metric , spacetime would be completely symmetric in its coordinates . this is made even more apparent when we switch to a system of units in which $c = 1$ . now we do not even have to make a decision to set $x^0 = ct$ , because by measuring both time and space in meters ( or seconds if you prefer ) , we have completely abandoned the notion of time being a distinct entity which is somehow independent from space .
at the galactic center , there is an object called sagittarius a* which seems to be a black hole with 4 million solar masses . in 1998 , a wise instructor at rutgers made me make a presentation of this paper http://arxiv.org/abs/astro-ph/9706112 by narayan et al . that presented a successful 2-temperature plasma model for the region surrounding the object . the paper has over 300 citations today . the convincing agreement of the model with the x-ray observations is a strong piece of evidence that sgr a* is a black hole with an event horizon . in particular , even if you neglect the predictions for the x-rays , the object has an enormously low luminosity for its tremendously high accretion rate . the advecting energy is pretty " visibly " disappearing from sight . if the object had a surface , the surface would heat up and emit a thermal radiation - at a radiative efficiency of 10 percent or so which is pretty canonical . of course , you may be dissatisfied by their observation of the event horizon as a " deficit of something " . you may prefer an " excess " . however , the very point of the black hole is that it eats a lot but gives up very little , so it is sensible to expect that the observations of black holes will be via deficits . ; - )
a good question . the string tension actually is a tension , so you may measure it in newtons ( si units ) . recall that 1 newton is 1 joule per meter , and indeed , the string tension is the energy per unit length of the string . because the string tension is not far from the planck tension - one planck energy per one planck length or $10^{52}$ newtons or so - it is enough to shrink the string almost immediately to the shortest possible distance whenever it is possible . unlike the piano strings , strings in string theory have a variable proper length . this minimum distance , as allowed by the uncertainty principle , is comparable to the planck length or 100 times the planck length which is still tiny ( although models where it is much longer exist ) . for such huge energies and velocities comparable to the speed of light , one needs to appreciate special relativity , including the $e=mc^2$ famous equation . this equation says that the string tension is also equal to the mass of a unit length of the string ( times $c^2$ ) . the string is amazingly heavy - something like $10^{35}$ kg per meter : i divided the previous figure $10^{52}$ by $10^{17}$ which is the squared speed of light . basic equations of perturbative string theory more abstractly , the string tension is the coefficient in the nambu-goto action for the string . what is it ? well , classical physics may be defined as nature 's effort to minimize the action $s$ . for a particle in special relativity , $$ s = -m\int d\tau_{proper} $$ i.e. the action is equal to ( minus ) the proper length of the world line in spacetime multiplied by the mass . note that because nature tries to minimize it , massive particles will move along geodesics ( straightest lines ) in general relativity . if you expand the action in the non-relativistic limit , you get $-m\delta t+\int dt\ , mv^2/2$ , where the second term is the usual kinetic part of the action in mechanics . that is because the curved lines in the minkowski space are shorter than the straight ones . string theory is analogously about the motion of 1-dimensional objects in the spacetime . they leave a history which looks like a 2-dimensional surface , the world sheet , which is analogous to the world line with an extra spatial dimension . the action is $$ s_{ng} = -t\int d\tau d\sigma_{proper} $$ where the integral is supposed to represent the proper area of the world sheet in spacetime . the coefficient $t$ is the string tension . note that it is like the previous mass ( from the point-like particle case ) per unit distance . it may also be interpreted as the action per unit area of the world sheet - it is the same as energy per unit length because energy is action per unit time . at this moment , when you understand the nambu-goto action above , you may start to study textbooks of string theory . piano strings are made out of metallic atoms , unlike fundamental strings in string theory . but i would say that the most important difference is that the strings in string theory are allowed - and love - to change their proper length . however , in all the other features , piano strings and strings in string theory are much more analogous than the string theory beginners usually want to admit . in particular , the internal motion is described by equations that may be called the wave function , at least in some proper coordinates . also , the strings in string theory are relativistic and on a large enough piece of world sheet , the internal so ( 1,1 ) lorentz symmetry is preserved . that is why a string carries not only an energy density $\rho$ but also a negative pressure $p=-\rho$ in the direction along the string .
you get a rise in a capillary tube because it reduces the energy stored in the surface tension at the air-water and air-glass interface . the water rises until the reduction in the surface tension energy is balanced by the increase in the gravitational potential energy of the water . but it is not at all obvious how you could extract energy from this . if you evaporate water from the top of the tube then you will certainly pull up more water to replace the water lost by evaporation . i suppose this is analogous to a tree pulling up water , though my limited memory of biology i think the sap is driven up the tree by osmotic pressure in the roots as well as by capillary action . i suppose you could put a microturbine at the bottom of the capillary tube then heat the top and extract energy as the water rises up the tube to replace the water that is evaporated . however i doubt this would be as efficient as just using the same amount of heat in steam engine . were you wondering if there was a way to make the water rise up the tube , then fall back , then rise up again , generating energy with each cycle ? the only way you could do this was if there was some way to change the air-water or air-glass surface tension in some reversible way . you can easily reduce the air-water surface tension by adding surfactant , and this will make the water drop , but you had need to get the surfactant back out to make the water rise again .
can any solid material with a low heat capacity exist that feels closer to human body temperature than another solid material with a higher heat capacity ; where both materials were previously kept in either a mundane oven or freezer for a sustained period ? let me rephrase to : is there any solid which disobeys the inverse proportionality of thermal conductivity and specific heat capacity ? consider $1000kg$ of wood and $1000kg$ of aluminium , both at $320k$ ( very warm ) . at the instant you place a finger on such large thermal masses , your perception of temperature comparison is dependent on heat conductivity of the materials , not their heat capacity ( their masses are so large compared to your finger , their temperature is almost constant depsite losing heat to your finger ) . using such large masses and ( equal masses for that matter ) is necessary since otherwise i can instantly answer yes to your question by giving you 100g of wood and 1g of gold ( beaten to the same surface area of the wood ) just taken from the freezer and you would perceive gold being closer to body temperature than the wood after a second . so lets define the question by specific heat capacity , and instantaneous perception of heat transfer . to answer it though , there is in fact no metal which disobeys this relation due to the electron sea being the majority carrier of kinetic energy in the bulk metal . their having large mean free paths and low masses allow them to attain very high velocities ( which is a property of high temperature ) and therefore are able to transfer energy quickly in the bulk material . in other words , if metals used anything heavier to transmit heat , like their nuclei , it would not only take much more heat to accelerate them to the same velocities the electrons could attain ( resulting in higher heat capacity ) , but the rate at which that kinetic energy is transmitted across the material is accordingly slower ( lower thermal conductivity ) . in fact the lattice of metal nuclei do in fact contribute to both properties via phonons not translational kinetic energy like in gases , but phonons are still greatly superseded by the effect from electrons . therefore the inverse relation between thermal conductivity and heat capacity is valid for metals . what you are looking for is a non conductor with both higher heat capacity and thermal conductivity than a conductor . for that i give you diamond ( figuratively . . . i can not afford one ) , which has a specific heat capacity of $0.5 j/gk$ , higher than that of any metal denser than vanadium ( which is almost all of them ) , but has a thermal conductivity of $&gt ; 900w/mk$ , trumping silver 's $421w/mk$ which is tops for all pure metals . indeed , $1kg$ of silver would feel much closer to body temperature than $1kg$ of diamond ( that is alot of diamond ! ) despite diamond having a higher heat capacity .
long distance quantum communication in earth 's atmosphere is possible . there are successful realizations of quantum key distribution and quantum teleportation , showing that it is possible to detect single photons and entangled pairs having traveled distances up to 300 km . of course , the losses are huge and unavoidable , so the total attenuation for a link between two canary islands in the mentioned experiments was around 30 db for single photons and 70 db for pairs . it means that the source of entangled photons has to be quite bright : the one used in canary experiments produced $~10^6$ pairs/s . and only about 0.07 pairs/s were detected at the receiving side . nevertheless , the noise , mostly coming from stray light and detector dark counts , can be made low enough ( ~200 hz ) to reach coincidence signal-to-noise ratio of 15:1 . this is achieved by frequency filtering and timing - the coincidence window was limited to 3 ns , and that required corrections for time drift of gps-based clock synchronization system . active synchronization was done utilizing temporal correlations of entangled photons : the local time was adjusted to maximize the coincident events rate . authors call it " entanglement-assisted clock synchronization " . there are several sources of losses in free space atmospheric links : scattering and absorption of ~0.07 db/km , diffraction losses due to limited aperture on the receiving side which is unavoidable , as well as beam wandering and distortion due to atmospheric turbulence . beam wandering may be to some extent fixed by active tracking systems . as for specific " shaping " of photons , there are some speculations on using the beams with orbital angular momentum to reduce the effects of turbulence , but these do not seem to be very realistic , and some authors tend to claim that to the contrary , higher-order laguerre-gaussian beams are more sensitive to turbulent distortions . satellite quantum communication is a feasible task , we will probably see it in the nearest future . in some aspects it is even simpler , since the path travelled in the dense layer of atmosphere is much shorter . inter-planet optical communication at the single-photon level would have to deal with enormous losses just due to diffraction spreading of the beam , it does not seem to be possible with current technology .
the reason you can hear sound around the door is only in part due to diffraction . as you said , the walls are not completely rigid . in fact , the sound is passing through the door as well . consider a speaker on the other side of a wooden door that is producing a sound . the door is completely shut . the reason you can hear it is because the sound is passing through the door , as well as " reflecting " off of it . it is analogous to a light wave interacting with water . the glare you see off the water is light reflecting off the surface , but the water is translucent , which means light is also entering the water . in the same sense , the sound will bounce off the door ( this can result in an echo ) as well as pass through the door , which is why you can hear it on the other side . which is the most important when hearing indirectly ? well it depends on the situation . if the door is open , then diffraction will be the significant contributor . however , if the door is shut , then assuming the door is sealed well , the wave passing through the door will obviously be the biggest contributor . the material of the door is important since sound waves pass through some materials better than others . yes , the contributions can be calculated , however , it would be very difficult to calculate in some situations .
as witten explains in his notices of the ams article ( please see also his more recent lecture ) , the fully quantum string theory is characterized by two coupling constants ( or in the language of deformation quantization : two deformation parameters ) . the string coupling $g_s$ and the string tension $\alpha^{'}$ . in perturbation theory , one gets dependence of the string amplitudes on powers of $g_s$ ( or equivalently in $\hbar$ ) through the genus expansion . the dependence of the amplitudes $\alpha^{'}$ is obtained once one takes into account that in the presence of background fields , the string lagrangian is not free , it is described by a sigma model . if we compute the quantum correction to this sigma model we get terms with more and more derivatives multiplied by more powers of the of the string tension ( as in chiral perturbation theory ) . when the quantum corrections to the trace of the energy momentum tensors are calculated then here also terms depending on powers of $\alpha^{'}$ will appear and the condition of vanishing of the beta function will results einstein 's equations with correction terms proportional to $\alpha^{'}$ . please see equations 3.7.14 . in polchinsky 's first volume , where the beta functions are given to the first power of $\alpha^{'}$ . witten explains that for a while , the work on string theory concentrated on finding candidates of $\alpha^{'}$ deformed theories ( as conformal field theories ) , then $\hbar$-quantize them as in ordinary quantum theory . but , as witten explains , after the discovery of the full set of string dualities and the role of membranes , it was realized that in order to fully quantize string theory , the two quantizations or two deformations ( $\hbar$ , $\alpha^{'}$ ) must be perfomed together . this route has profound consequences , for example , it leads to the conclusion that the string full quantum theory should be in the realm of noncommutative geometry , because in the presence of a $b$-field and brane boundary conditions , the position-position commutation relations will obtain $\alpha^{'}$ deformation and become noncommutative . as a consequence , the ordinary uncertainty relation will get $\alpha^{'}$ deformation and turns into a generalized ( minimal scale ) uncertainty , in which the position uncertainty has a nonvanishing minimum : $\delta x \geqslant \frac{\hbar}{\delta p}+ \alpha^{'}\frac{\delta p}{\hbar}$
i thought i might just start with an introduction first . : ) the basic principal behind free electron laser is that of synchrotron radiation . when electrons or charged particles are made to change momentum ( like being bent in a in an arc where the force is radially inwards ) they emit electromagnetic radiation . if the particles are relativistic then the electromagnetic radiation the lab observer relative to the electron will observe the electromagnetic radiation being emitted in a cone in the direction of motion . ( i will post figures if people really want ! ) in the case of the free electron laser you need to have magnetic arrays which are simply dipole magnets aligned such that the electrons will see as its travelling in a straight line , alternating vertical magnetic fields . this causes the electrons to " undulate " horizontally . with relativistic electrons ( which is not hard to do ) you will emit synchrotron radiation like a pencil beam . the wavelength from an undulator is given by lambda = undulator_period/ ( 2gamma^2 ) * ( 1+a^2/2 ) , where gamma is the relativistic gamma factor , a = e*b*undulator_period/ ( 2*pi*electronmass*speedlight ) where b is the strength of the magnetic field and e the electric charge . but the radiation from all the electrons are not very temporally coherent ! now as to why its called a laser ! the fundamental feature of why lasers are powerful is because the waves emitted by all sources are emitted in a temporally coherent fashion . i.e. all the electric fields add constructively so the total power scales like pn^2 where p is the power emitted by a single source and n is the number of sources . in a fel the undulation is small enough that the electrons are irradiated by its own synchrotron radiation and is therefore travelling in an optical field as well as the magnetic field from the dipole magnets . the optical field causes the electrons to micro-bunch and cause further amplification of that particular wavelength ( micro-bunching effect ) and is referred to as self-amplified spontaneous emission ( sase ) . this positive feedback generates powers that give an additional gain of n . typical bunches of electrons are 10^11 electrons , so you can get orders of magnitude more power . i presume you can reduce the size of the magnetic arrays to just a stretch of material with carefully oriented domains however it will be difficult to reach the fields required . to measure thats the field of spectroscopy which i am less familiar . for uv you will have to have some material with dispersion properties like a prism of sorts and a detector that will respond to uv radiation . for x-rays you can use monochromators that use the bragg principal and determine the energy ( calibrated to know emission lines from elements ) . the energy loss along the undulator is a small factor . the emitted photons are very small fractions of its total energy . for example for 10 to 100 nm wavelengths you would need something like 300 mev electrons ( rest mass of an electron is 0.5 mev ) so beta = 0.9999986 , and 10 nm photons are only 10s of ev . i thought the military were focusing on uv fels like you said , to reduce water absorption . xrays are much harder to generate with enough power to cause damage . that was more than i initially intended to say . hope its useful .
as the name suggests , when something has potential energy , it just means that it has the potential to do some work . the amount of potential energy that any body possesses when associated with a force depends on the position of that body . for example , the higher you go , more is your potential to drop and do some ' work ' . the work that the body does only depends on its starting and end position . it has nothing to do with the trajectory . it does not matter if the body is in equilibrium or not . a book kept on your table is also in equilibrium , its weight and normal force cancel out each other . but , it still possesses potential energy because it is at a height from the floor , which is the reference frame that we have chose to have zero potential . you can apple the same logic to any type of potential energy , gravitational , electric , spring etc . force fields do not have to be homogeneous . if the field does some work by moving a body from one point to another in such a way that it gains potential energy , the potential energy can be evaluated as a function between the two end points .
the actual equivalent to a newtonian " slice of time " in gr is a " space-like hypersurface " . it is a 3 dimensional hypersurface in 4d space-time ( hence the hypersurface ) . it is " space-like " because any two points are connected by a " space-like " path in the metric . this means that they are independent from a causality perspective , and can form the basis of forward predictions . if this is a global hypersurface ( ie doesnt just stop somewhere , or miss out regions ) then it is called a " cauchy surface " . as remarked elsewhere there is more choice in gr involved . the simplest way to see this is to note that newtonian theory is $e^4$ with ( x , y , z , t ) and once t= const is chosen at the space origin , say , the entire hypersurface is now determined . in say minkowski space ( the nearest gr equivalent to newtonian space ) there is a choice of timelike vector to be made first . there are 4 degrees of freedom here , but there is the timelikeness restriction and the restriction on time direction . ( physically this is a choice of inertial frame . ) this allows a foliation to form . to obtain an actual hypersurface ( hyperplane in this case ) we can choose one point p on that hypersurface ( ie a specific time in the frame ) . now the timelike vector is a normal to that hyperplane and we can determine whether an arbitrary point ( x , y , z , t ) is on that hyperplane . there will be 3 degrees of freedom as expected . gr generalises the minkowski example in that the hypersurface might be curved and not flat , requiring an infinite number of independent points to identify it . one further generalisation is that in relativity we have a metric which allows null distances between different points . thus we can have a " null hyperplane " and " null hypersurface " too , which are quasi-generalisations of the timelike slice newtonian idea .
the analysis of the availability or not of the which-way information , and the consequences on the interference pattern , is purely a quantum analysis . now , there are different experimental devices to make appearing or disappearing the which-way information . you do not necessarily need to use polarizers for this ( you could have used mirrors , beam splitters and detectors ) . even , if , in this particular experiment , polarizers have been used , the interesting analysis is not a possible classical analysis ( orthogonal waves do not interfere ) , but it is the quantum analysis . more precisely , in your experiment , the two analysis ( classical and quantum ) are correct , because of the very particular experimental devices , but the fundamental analysis is quantum , and the quantum analysis will still be true , event if you do not use polarizers in your experiment , while , in this case , the classical analysis will fail .
the essence of a glide is that the aircraft is descending . just like a car rolling down a moderate grade , it is trading potential energy to replenish the kinetic energy lost to drag . whether the nose points up or down only relates to the angle of attack , which only relates to speed . an aircraft traveling at slow speed has a higher angle of attack , so its nose will point up , compared to when it is traveling at high speed . one of the things you learn in flight training is how to handle a loss of power . there is a mnemonic for that : abc a : trim for the airspeed ( 65 kts in a c172 ) that gives you the best glide range . this is fairly slow and nose-high . ( there is even a somewhat slower speed that gives you less range but more time aloft . ) b : look for the best landing site , be it a field , road , or if you are lucky , an airport . c : look in the cockpit for what you can do , like trying to restart the engine , and calling on the radio . so , under a , you can see that a slow glide is relatively nose-up , even while the aircraft is descending .
let me use a bit more standard notation . if you pull the string from equilibrium then to the first order there will be a force $f$ pulling it back that is proportional to tension $t$ and the distance $u$ that the string is moved from the equilibrium $$f = -a t u$$ where $a$ is some constant not interesting to the further discussion . for a piece of string with mass $m$ we get to the first order the equation for harmonic oscillator $$ 0 = m \ddot{u} - f = m \ddot{u} + a t u $$ which can be solved by $u \sim \exp ( i\omega t ) $ giving us the relation $$\omega \sim \sqrt{t}$$ and differentiating both sides gives you the relation you wanted . now , this was actually just a quick and dirty derivation . for a full treatment one needs to account for the infinite number of degrees of freedom of the string . however , this derivation only rests on the fact that the pull-back force is proportional to the tension and that resulting equations are in the form of harmonic oscillator . first point translates to the full string directly while the second is correct because the string is described by a wave equation which can indeed be decomposed into simple harmonic oscillators .
yes , quantum tunnelling in the double well potential can be solved in a wick-rotated euclidean formulation $$ s_e [ x ] ~=~\int \ ! dt_e \left [ \frac{1}{2}\left ( \frac{dx}{dt_e}\right ) ^2 - ( -v ) \right ] , $$ see e.g. ref 1 . here $t_e=it_m$ denotes euclidean time . the euclidean action is in turn interpreted as the usual kinetic minus potential term with a potential $-v$ . thus the double well turns into a double hill with no classically forbidden region in between ! the non-trivial solutions are called instantons . another approach uses semi-classical wkb methods on the tise . there is of course no imaginary time in the tise , but there are imaginary wavenumbers in the classically forbidden region , cf . e.g. refs . 2 and 3 . references : s . coleman , aspects of symmetry , section 7.2 . d . griffiths , intro to qm , chapter 8 . a . galindo and p . pascual , qm2 , chapter 9 .
you are absolutely right : the probabilities predicted by quantum mechanics are conceptually fully analogous to probabilities predicted by classical statistical mechanics , or statistical mechanics with a somewhat undetermined initial state - just like your metaphor with dice indicates . in particular , the predicted probability is a " state of our knowledge " about the system and no object has to " collapse " in any physical way in order to explain the measurements . there are two main differences between the classical and quantum probabilities which are related to one another : in classical physics - i.e. in the case of dice assuming that it follows classical mechanics - one may imagine that the dice already has a particular value before we look . this assumption is not useful to predict anything but we may assume that the " sharp reality " exists prior to and independently of any observations . in quantum mechanics , one is not allowed to assume this " realism " . assuming it inevitably leads to wrong predictions . the quantum probabilities are calculated as $|c|^2$ where $c$ are complex numbers , the so-called probability amplitudes , which may interfere with other contributions to these amplitudes . so the probabilities of outcomes , whenever some histories may overlap , are not given as the sum over probabilities but the squared absolute value of the sum of the complex probability amplitudes : in quantum mechanics , we first sum the complex numbers , and then we square the result to get the total probability . on the other hand , there is no interference in classical physics ; in classical physics , we would surely calculate the probabilities of individual histories , by any tools , and then we would sum the probabilities . of course , there is a whole tower of differences related to the fact that the observable ( quantities ) in quantum mechanics are given by operators that do not commute with each other : this leads to new logical relationships between statements and their probabilities that would be impossible in classical physics . a closely related question to yours : why quantum entanglement is considered to be active link between particles ? the reason why people often misunderstand the analogy between the odds for dice and the quantum wave function is that they imagine that the wave function is a classical wave that may be measured in a single repetition of the situation . in reality , the quantum wave function is not a classical wave and it cannot be measured in a single case of the situation , not even in principle : we may only measure the values of the quantities that the wave function describes , and the result is inevitably random and dictated by a probability distribution extracted from the wave function .
the $\delta$ function is not continuous , so it is a priori not differentiable . in fact , it is not even well-defined as an ordinary real-valued function , but can be made so in terms of distributions - linear maps on a space of test functions given by $f\mapsto\int\delta f=f ( a ) $ . it is possible to sensibly define derivatives of distributions by looking at representations as limits of functions : if $\delta_i$ is a family of functions so that $\lim_{i\rightarrow\infty}\int\delta_i ( x ) f ( x ) \mathrm dx=f ( a ) $ for any test function $f$ , then it can be considered a representation of the dirac delta . now , if we take the family of derivatives $\frac{\mathrm d}{\mathrm dx}\delta_i$ we arrive at $$ \int\left [ \frac{\mathrm d}{\mathrm dx}\delta_i ( x ) \right ] f ( x ) \mathrm dx=-\int\delta_i ( x ) \left [ \frac{\mathrm d}{\mathrm dx}f ( x ) \right ] \mathrm dx $$ through integration by parts and using the fact that $f$ has by definition compact support ( which makes the boundary term vanish ) . as the derivative is linear as well , this defines another linear map $f\mapsto-\int\delta f'$ on the space of test functions , which we call the derivative of our distribution . symbolically , $$ \left [ \frac{\mathrm d}{\mathrm dx}\delta ( x-a ) \right ] f ( x ) =-\delta ( x-a ) f' ( x ) $$ which you can just plug in into your formula above without any need for actual computation as it holds true by definition .
the energy stored in the spring is equal to the work done in compressing it . the force needed to compress the spring to the maximum is only a small part of this calculation . suppose the spring is completely relaxed , and you apply a small force compressing the spring slightly . the energy stored is the product of the force and the distance , measured in newton-metres , or joules . now you increase the force slightly , and the spring compresses a short distance more . the additional energy stored in this new force multiplied by this next compression distance . this continues , until the spring is totally compressed . so to calculate the energy stored , you need to know how far the spring compresses , and how the compression force changes as the spring compresses . edit to rely to comment : suppose the spring is quite long in its relaxed state , with a spring constant of 40 lbf/in . the spring will need to compress by 50 in , to reach the winch maximum of 2000 lbf . . so the energy stored in the compressed spring would be ( with suitable conversion to si ) $$e_s=\frac12kx^2=\frac127005\times1.27^2=5650 \text{ joules}$$ so this would be the energy available to loft a projectile . the problem would be to transfer as much as possible of this energy to a light projectile . there are inherent , internal losses in the rapid extension of a spring that make it difficult to efficiently launch a missile . usually , the spring pushes slowly , with huge force on the short end of a lever , while a light mass accelerates quickly at the long end of the lever . google " trebuchet " , or even " punkin chuckin " . spelling is correct ! the equation in the comment would give the maximum height possible , with no losses in the spring .
goldstein 's " closed " means the orbiting body will eventually return to some point with the same velocity it had there previously ; i.e. that the path will repeat itself exactly . note that this can occur even in the case of precession : if the ratio between radial period and angular period is rational , the orbit will be closed . there is no precession only if this ratio is unity . what goldstein is saying is that in this point in the text , all he has shown is that the objects ' separation $r$ will satisfy $r_1 &lt ; r &lt ; r_2$ for all time , and that this does not necessarily imply that the orbit will ever repeat itself . for example , we have not ruled out the possibility that $r$ could be periodic with period $\pi$ , while $\theta$ could repeat every 3 units of time . other authors use " closed " or " elliptical " to mean goldstein 's " bounded , " and " open " or " hyperbolic " to mean " unbounded . " part of the degeneracy in terms probably comes from the fact that for two ideal point masses in newtonian mechanics , bounded orbits will be closed in goldstein 's sense . see the application of bertrand 's theorem to the inverse square law .
the way i understand your description , the motion of the light and the spring tension are perpendicular to the direction of acceleration . consider the problem in the ( instantaneous ) rest frame of the mirrors and spring . let 's use coordinates where x is the direction of the photon 's motion ( in the unmoving rest frame ) , and y is in the direction of acceleration ( and motion ) of the mirrors . the relevant equations are spring force $f = kx$ , photon momentum in the mirror ( x ) direction = $p_x = \hbar k_x$ , photon rate $1/t$ where $t$ is the time between photons , and the photon force = change in momentum per unit time = $2\hbar k_x / t$ . the requirement for no change to the mirror is that $kx = 2\hbar k_x/t$ . clearly the spring is just a spring in the moving rest frame and so there is no change to the spring force $kx$ . as for the photons , their momentum perpendicular to the direction of velocity ( or acceleration ) is unchanged , so there is no change to the momentum any single photon imparts to the mirror $2\hbar k_x$ . uh , note that since the photon reverses its direction , it imparts twice its momentum to the mirror . another way of saying the same thing is to note that stationary and moving observers agree on the number of photon wavelengths $\lambda_x$ between the two mirrors . they also agree on the distance between the two mirrors . therefore they agree on the wave number $k_x = 2\pi/\lambda_x$ for the photon 's momentum in the x direction . so what is left is the rate at which the photons impact the mirror $1/t$ . this rate does not depend on the acceleration ( relativity problems rarely do ) . instead it depends on the velocity of the mirror . as the mirror velocity increases , the distance traveled by the photons in the moving frame increases . thus the time between photons increases and the photon force decreases . of course in the unmoving rest frame the photon rate is unchanged , it is only in the moving frame that the photon rate changes . this effect is called time dilation . consequently , the non moving observer , on noting the force on the mirror , must conclude that the spring constant $k$ has changed due to the motion of the spring . for a discussion of this interesting effect , see : http://www.mathpages.com/home/kmath068/kmath068.htm
i will try to answer your questions while explaining as much basic quantum mechanics as possible , so that you will be able to fill in the blanks with only linear algebra . hilbert space- as a novice to qm i am very sad that in none of the books i have read i found the reasons for using hilbert space h at first place followed by a full geometrical explaination of this space and how we build this space out of r . it goes same for for its dual space hd . where can i get this ? every single author starts this topic by just bombard novices with bunch of rules for h , hd which i can not just trust and this is only pointless learning by heart . in qm , the state of a system is given by a set of complex amplitudes . if i have a coin that says 0 on one side and 1 on the other , and i flip it , classically i will have a coin in either the state 0 or the state 1 , with a probability of 1/2 for each outcome . in qm , instead of probabilities over different states , you have amplitudes . to get the probability you take the square of the amplitude . so , if i flip a quantum mechanical coin , i may end up in the state $$ \frac{1}{\sqrt{2}} |0\rangle + \frac{1}{\sqrt{2}}|1\rangle . $$ all this means is that the probability that i will observe state 0 is $$ \left ( \frac{1}{\sqrt{2}} \right ) ^{2}= \frac{1}{2} . $$ so , to describe the state of a quantum system , you need a set of observation outcomes , and an amplitude for each outcome . above , our outcomes were 0 and 1 , and our amplitudes were $1/\sqrt{2}$ . if we were dealing with regular probabilities , the space of states of a system with $n$ possible outcomes would be $\mathbb{r}^{n}$ . this is because each outcome has a probability , so you can write the entire state as a set of $n$ probabilities , which are just real numbers . in quantum mechanics it is the same , except instead of probabilities we have $n$ amplitudes . so the space of states is $\mathbb{c}^{n}$ . the reason physicists talk about hilbert spaces instead of just complex euclidean spaces is that in general your set of measurement outcomes could be infinitely big , instead of just $n$ . the hilbert space is just a generalization of the complex euclidean space , and you can often just think of complex euclidean spaces instead . dirac 's notation . . . because i do not understand hilbert space i do not understand what i am allowed to do with kets | ⟩ and bras ⟨ | and for example : why do we have to write an operator on the left side of kets |a^ψ⟩=a^|ψ⟩ but for bras it is vice versa ( we write it on the right ) ⟨a^ψ|=⟨ψ|a^ . why can we factor out a constant from kets |aψ⟩=a|ψ⟩ and we can only factor out complex conjugate from bras ⟨aψ|=a∗⟨ψ| . now that we understand that states are given by complex vectors , we can understand bra-ket notation . in my coin flipping example , imagine that i use the amplitudes $1/\sqrt{3}$ and $\sqrt{2/3}$ . i can represent the state of my coin as $$\left|\mbox{coin}\right&gt ; = \frac{1}{\sqrt{3}} |0\rangle + \sqrt{\frac{2}{3}}|1\rangle = \frac{1}{\sqrt{3}}\left ( \begin{matrix} 1 \\ 0 \end{matrix}\right ) + \sqrt{\frac{2}{3}}\left ( \begin{matrix} 0 \\ 1 \end{matrix}\right ) = \left ( \begin{matrix} 1/\sqrt{3} \\ \sqrt{2/3} \end{matrix}\right ) $$ there is a lot of notation being used in the expression above . as is often done , i wrote the name of the system inside a ket . this is to be read a " the state of the coin " and means nothing else mathematically , other than to say it is a vector in a hilbert space . this is actually what i have been doing when i wrote $|0\rangle$ and $|1\rangle$ . i have also decided that the outcome 0 should correspond to the vector $\left ( \begin{matrix} 1 \\ 0 \end{matrix}\right ) $ , and 1 should correspond to $\left ( \begin{matrix} 0 \\ 1 \end{matrix}\right ) $ . this was an arbitrary decision . i did it only because i know that my system 's state is given by a set of complex numbers , and i want to be able to write the entire state down as a simple vector in a hilbert space . so , kets are vectors in a hilbert space . i have decided to write them as column vectors , but this was also arbitrary . if kets are column vectors in a hilbert space , then bras are row vectors with the rule that for any $\left|\psi\right&gt ; $ , $$\left&lt ; \psi\right| = \left|\psi\right&gt ; ^{\dagger} . $$ here i used $\dagger$ to mean conjugate transpose . it is unfortunate that there are so many different notations for this operation , but you seem to understand what it means . okay , so states are vectors in a hilbert space . operators are matrices acting on the vectors . a matrix times a vector gives a vector , so this is a good way to model transformations . now the confusion about what is allowed and what is not allowed with bras and kets just comes down to linear algebra . hopefully you can now answer questions like why does $\alpha^{*}\left&lt ; \psi\right| = \left ( \alpha|\psi\rangle \right ) ^{\dagger}$ . this has turned into a long answer so i will leave it here for now .
dear dan , first of all , you should not use the term " uncertainty principle " if you are talking about " light sources " and light may be explained by ordinary - classical ( non-quantum ) - electrodynamics where no uncertainty principle applies . this is just an exercise in the propagation of waves . second , when you flip the switch , there may be temporary variations of the intensity , but they are not necessary , either . for example , you may find a minimum such that the number of wave peaks on the two trajectories ( coming from the two slits ) differs by 13.5 - one arm is 13.5 wavelengths longer than the other one . it will mean that the destructive interference only occurs when the beams from both slits are synchronized , and there will always be a period lasting about 13 periods after each flip of the switch when only one beam is coming to the detector . that will indeed eliminate the destructive interference , and give you the " apostrophes " in your ascii art . the precise shape of the graph depends on the character of the switches , geometry of the experiment , and other things .
you are talking about the inductive effects of the coil of wire . essentially a wrapped up coil of metal with electrons running through it creates a linear magnetic field since moving electrons through a wire creates a redial field and if you approximate the coil to have infinite loops the field becomes liner . but , this effect would be very , very small for the wires you are talking about since ( a ) the coils are not very densely packed and ( b ) not very much current is flowing through them here is the wiki on inductors : http://en.wikipedia.org/wiki/inductor the simple relationship between voltage ( $v$ ) , inductance ( $l$ ) , and current ( $i$ ) is : $$v ( t ) = l \frac{di ( t ) }{dt}$$ one last thing to consider , magnetic field drops off with distance fast ( so as you move away from the source it gets really weak ) . the plastic protective covering around your wires are a relatively similar thickness to the wires themselves and would buffer anything nearby to most the magnetic effects ( which would be weak to begin with )
thermal conductivity has dimensions of $\mathrm{power / ( length * temperature ) }$ . power is the rate of heat flow , ( i.e. . ) energy flow in a given time . length represents the thickness of the material the heat is flowing through , and temperature is the difference in temperature through which the heat is flowing . in si units , it is commonly expressed as $\mathrm{watts / ( meter * kelvin ) }$ , and in us units , it is commonly given in $\mathrm{btu/hr/ ( feet\ *\ ^of ) }$ . it expresses the rate at which heat is conducted through a unit thickness of a particular medium . that rate will vary linearly based on the temperature difference across the material , so it is expressed as a value per degree of temperature difference , thus heat rate per unit thickness per degree of temperature difference .
your equations ( 1 ) ( 2 ) , saying $\delta \psi =-\frac{1}{4}\lambda ^{\mu \nu}\gamma _{\mu \nu}\psi$ with or without $^c$ , just says that both $\psi$ and $\psi^c$ are in the same representation , namely $ ( 1/2,0 ) + ( 0,1/2 ) $ . the third equation ( 3 ) , saying $ ( p_l\psi ) ^c=p_r \psi^c$ , just says that the charge conjugation swaps the two irreducible components of the reducible representation that is the dirac spinor .
at the critical point , the bulk gap is closed , and there is nothing to prevent the edge state from penetrating into the bulk . so the gapless mode simply merge into the bulk . the modes ( of opposite chirality ) from both edges will mix . both the chirality and majorana property will be " canceled out " by the mixing .
there is a difference between temperature and energy . plasma is , as you said , very hot - but there is not very much of it . the density of plasma in the tube is very low . so when it does hit the walls of the tube it transfers very little energy . so the mass of the glass tube increases in temperature only very slightly . it is like a firework sparkler , the sparks are at 2000degc but they are very small , have very little mass and contain very little energy - so when one lands on you it transfer much less energy than a hot cup of coffee at 80deg c .
the answer is heavily dependent on the material . for most materials the electric constant $\epsilon$ is very far from being a simple scaling factor , it has a complicated frequency response so that we write $\vec{d} ( \omega ) = \epsilon ( \omega ) \vec{e} ( \omega ) $ ( assuming an isotropic and local medium , so that $\epsilon$ is scalar and local ) . for example , the sellmeier equation ( with different coefficients for different materials ) describes many materials at optical frequencies very well and it is simply a set of resonances , which you can think of as arising from electrons on springs ( i.e. bound to molecules ) so they undergo forced harmonic motion in response to the electric field . the " speed " of their reaction is set , as with any forced harmonic oscillator , by the strength of their binding ( the " spring constant" ) and their mass ( inertia ) . recall that each electron only has to shift a very short distance to beget the dielectric effect , so its not like electrons have to rush all over the place to beget the charge shifts : the polarization essentially arises from the appearance and alignment of dipoles . you should keep the system of electrons on springs rather than electrons running all over the place in your head . as the field 's frequency rises , charge separation and polarization stay pretty much in phase with the electric field until you get nearish the first resonance . the sellmeier model when losses are added is called the ketteler-helmholtz model and models essentially a system of damped harmonic oscillators .
ok , i have found this : http://www.cv.nrao.edu/course/astr534/brightness.html i proves that it is not possible to build such optical system . the conservation of brightness also applies to any lossless optical system , a system of lenses and mirrors for example , that can change the direction of a ray . no passive optical system can increase the specific intensity or total intensity of radiation . if you look at the moon through a large telescope , the moon will appear bigger ( in angular size ) but not brighter . many people are disappointed when they see a large , nearby galaxy ( e . g . , andromeda ) through a telescope because it looks so dim ; they expected to see a brilliantly glowing disk of stars , as in the photograph below . the difference is not in the telescope ; it is in the detector—the photograph appears brighter only because the photograph has summed the light over a long exposure time . though the andromedia example is not necessarily correct . . . because it consists of many stars which have huge surface brightness . i think if we could have large enough aperture can could resolve sirius as a disk it would be more eye damaging sight than the sun . . .
analyzing the acceleration of the center of mass of the system might be the easiest way to go since we could avoid worrying about internal interactions . let 's use newton 's second law : $\sum f=n-mg=ma_\text{cm}$ , where $m$ is the total mass of the hourglass enclosure and sand , $n$ is what you read on the scale ( normal force ) , and $a_\text{cm}$ is the center of mass acceleration . i have written the forces such that upward is positive the center of mass of the enclosure+sand moves downward during process , but what matters is the acceleration . if the acceleration is upward , $n&gt ; mg$ . if it is downward , $n&lt ; mg$ . zero acceleration means $n=mg$ . thus , if we figure out the direction of the acceleration , we know how the scale reading compares to the gravitational force $mg$ . the sand that is still in the top and already in the bottom , as well as the enclosure , undergoes no acceleration . thus , the direction of $a_\text{cm}$ is the same as the direction of $a_\text{falling sand}$ . let 's just focus on a bit of sand as it begins to fall ( initial ) and then comes to rest at the bottom ( final ) . $v_\text{i , falling}=v_\text{f , falling}=0$ , so $a_\text{avg , falling}=0$ . thus , the ( average ) acceleration of the entire system is zero . the scale reads the weight of the system . the paragraph above assumed the steady state condition that the op sought . during this process , the center of mass apparently moves downward at constant velocity . but during the initial " flip " of the hour glass , as well as the final bit where the last grains are dropping , the acceleration must be non-zero to " start " and " stop " this center of mass movement .
first of all , this is a really amazing piece of technology , in particular because it has achieved something that optical engineers have dreamed of for a long time 1 , and done so with underlying techniques that we have had for quite a while . just to give you some impression of how cool this is , i am an optical engineer and the first time i heard about this i was certain that it was either a hoax , a massive exaggerated description of something less impressive ( like an unsharp filter in photoshop or something ) , or simply a piece of godawful technology journalism . however , once i found the doctoral dissertation of the guy who developed this technology dr . ren ng ) , i realized how it works , and really how clever it is . ( note : i will probably add on to this answer a couple times , because i do not have time to give a good and thorough summary right now . if you want a really thorough description , check out the thesis i linked above . it may be a little over your head if you do not have background in optics though , which is what my summary here should help with . ) hardware this technique depends on some very clever data processing techniques , and on the unusual type of camera ( ren ng calls it " plenoptic " which is not a term i have heard before ) that is used to capture the data . this camera has an array of very small lenses ( a " microlenslet array" ) at its image plane , where a normal camera would just have the image sensor . the image sensor is positioned slightly behind this . the microlens array alters the incoming light before it hits the sensor . if you were to look at this raw data as it is captured by the camera , it would look similar to the image you would expect from a normal camera , but it would be composed of thousands of little blobs rather than being a nice continuous image ( i am not talking about pixels , these dots are many pixels across ) . if you zoom in on this image , you would see that each dot is actually a very small , possibly blurry image of a portion of the scene being photographed . on its own , this image is ugly and not really good for anything , but because we know exactly how it was altered by the lenslet array , we actually have more information about the light that entered the camera 2 . with some clever data processing algorithms , we can retrieve this information and use it to determine the focus condition of each part of the scene being imaged . algorithms ( this section will be expand when i have more time ) in the most basic sense , the job of an imaging system is to produce an image where each point records the color and brightness of a corresponding point in the scene being photographed . a plenoptic camera also aims to determine , for each point in the image , how the light from the corresponding point in the scene was focused . this amounts to figuring out not only where that light hits your sensor ( which a normal camera measures ) but what path it took to get there . the data processing done after the image is captured is able to reconstruct this information because instead of having only one piece of information about each ray of light -- which pixel on our sensor it hit -- we now have a second piece of information -- which lenslet in the array did that ray pass through . once we have figured out the path of each bundle of light that we captured from the scene , we can calculate the image we would have gotten from a normal camera for any focus setting over a large range . 1: there are actually other way to achieve this , but the methods developed by ren ng are impressive and novel in that they are reliable and do not require insane amounts of computing power/time . this is what makes his technology marketable to consumers . 2: actually , we did not really get extra information , we just traded a little of one type of information for a little of another type . a normal camera would be able to produce one pixel in the output image for each pixel in its sensor , but this camera will produce an output image with about one pixel for each microlens in the array . the details of the algorithm may raise or lower that ratio a little , but the basic idea about trading one type of information for the other will always hold . this is , by the way , on reason that this technology did not happen sooner -- it was not until recently that we could produce high quality lenslet arrays with enough lenses to produce a good picture .
if your argument were fine , no energy would have been required for anything in this universe and there would not have been an energy crisis ! ! ! your intuition is fine when there is no resisting force acting on your body . say , you want to move a block of mass $m$ applying a horizontal force with constant velocity $v$ on a floor with friction coefficient $\mu$ . then we know that frictional force $f = \mu mg$ . hence you need to keep applying a force $f$ equal in magnitude but opposite in direction to the frictional force to keep the body moving at $v$ . and hence the work = $f . v$ which is definitely non zero .
the much larger mass of the rotating cylinder than that of the bullet , together with the friction between the cylinder and chamber position mechanism , would make the downward bias negligible .
this comes from the microscopic origin of the model . for example , in the case of the hydrogen atom , the dipole operator is given by ( up to some signs ) $\hat d=e \hat z e$ where i have assumed that the electric field is in the direction $z$ , and $\hat z$ is the position operator of the electron ( of charge $-e$ ) . let 's now have a look at the effects of the parity operator $\hat \pi$ . we have $ [ \hat h , \hat \pi ] =0$ , meaning that the eigenstates such that $\hat\pi\ , |g/e\rangle=\pm|g/e\rangle$ and we also have $\hat \pi\ , \hat z\ , \hat \pi=-\hat z$ . it is thus easy to show that $\langle g/e|\ , \hat z\ , |e/g\rangle=0$ by symmetry , which answers the question . microscopically , one can show that the selection rule of the of the matrix elements of $\hat z$ between the eigenstates $|nlm\rangle$ of the hydrogen atom are such that $\langle nlm|\ , \hat z \ , |n\ , ' l\ , ' m\ , '\rangle\propto \delta_{m , m'}\delta_{l , l\ , '\pm1}$ .
the usual way linear polarisation is measured is by shining polarised light onto a polarising filter , rotating that filter and then using malus ' law to fit the data to a $i_0 cos^2 ( \theta_{beam} - \theta_{polariser} ) $ shape . by finding the angular position of the intensity peak we can infer the angle of polarisation of the incoming beam . now , assume we shine a beam of a certain spread in the transverse plane , having different polarisations everywhere . if we shine this beam onto a polariser , we will get a pattern of intensities $i ( x , y ) = i_0 cos^2 ( \theta_{beam} ( x , y ) - \theta_{polariser} ) $ . we can find the values of $\theta_{beam} ( x , y ) $ as follows : calibration -- rotate $\theta_{polariser}$ and calculate $\max_{x , y} ( \theta_{beam} ( x , y ) ) $ , the peak intensity . the maximum of the peak intensity over all values of $\theta_{polariser}$ will give you $i_0$ . fix $\theta_{polariser}$ to the value that gives the maximum peak intensity . now you know that you have aligned your polariser with one of the polarisations present in the beam . then , the ratio $i ( x , y ) / i_0$ will give you $cos^2 ( \theta_{beam} ( x , y ) - \theta_{polariser} ) $ , from which $\theta_{beam} ( x , y ) $ can be inferred . this method works for polarisation that is constant in time . for other types of polarisation , you can always take a snapshot over a short period of time that has roughly constant polarisation . or , if the polarisation is circular , you can use a half waveplate to convert it to linear .
for geometrical optics we can introduce eikonal $\psi$ by relation $$ f = ae^{-ik_{\mu}r^{\mu} + i\varphi} = ae^{i\psi} . \qquad ( 1 ) $$ for small time interval and space lengths eikonal can be expanded in a form $$ \psi = \psi_{0} + \mathbf r \frac{\partial \psi}{\partial \mathbf r} + t \frac{\partial \psi}{\partial t} , $$ so , by compairing with left side $ ( 1 ) $ it can be directly show that $$ \mathbf k = \frac{\partial \psi}{\partial \mathbf r} , \quad \omega = -\frac{\partial \psi}{\partial t} . \qquad ( 2 ) $$ so by comparing $ ( 2 ) $ with hamilton-jacobi equations we have clear analogy : wave vector acts rule of classical momentum and frequency acts rule of hamiltonian in geometrical optics . so it is possible to introduce the analogy of principle of the least action for rays . for light it can be done by maupertuis principle : $$ \delta s = \delta \int \mathbf p d \mathbf l = 0 \to \delta \psi = \delta \int \mathbf k d \mathbf l = 0 . $$ for example , for optically isotropic and homogeneous space $\mathbf k = const * \mathbf n $ , and $$ \delta \int dl = 0 , $$ which leads to fermat 's least time principle .
the sun is not the hottest known thing . we can make hotter temperatures than even the center of the sun in the lab . it is just a matter of putting a lot of energy into a small space . you might be getting confused with the physical law that you cannot concentrate heat from a black body to a higher temperature than the source . so you cannot focus sunlight onto a spot and get a higher temperature than the surface of the sun - since otherwise the heat would flow from your hotspot back to the sun and heat it ! edit : the lab record is something like a few million-million degrees , a million times hotter than even the center of the sun ( 15 million deg ) . although at this point the definition of temperature gets a bit tricky .
bosonic fields are " more significant " than fermionic fields because they may get large vacuum expectation values – from a condensate of many bosons in the same state . consequently , there may exist a meaningful classical field theory limit . massless fields are " more significant " than massive fields because the massive fields in string/m-theory because the massive ones have masses of order the string scale or the planck scale which is huge and at these short distances or high energies , the classical reasoning breaks down , anyway . so we apply the classical effective equations of motion only at distances much longer than the string or planck scale and at these low energies , only the massless fields are visible ( the massive fields can not be excited so they are " integrated out " and do not appear in the action ) . becker-becker-schwarz try to jump to the truly consistent full theory , which is the supersymmetric one , as quickly as they can so the general bosonic string theory 's effective action may be absent in the book . but the corresponding action for the superstring theory is on page 311 etc . – type ii supergravity . borrow another textbook such as polchinski if your primary interest is bosonic string theory . there are kinetic and potential terms for the tachyon , some kinetic terms for the dilaton , the einstein-hilbert action for the metric tensor , and some natural " squared field strength " from the $b$-field . strictly speaking , the tachyon terms should be removed if we talk about " massless fields " because the tachyon field is not massless . but because of its negative squared mass , it is even " less massive " than the ordinary massive states as well as the massless states – it is " below " the massless level – so we usually do include it , too .
i believe that the answer to this question involves multiple parts . i will try and hit all of them . since you mentioned the ray model , i will assume you are relatively familiar with geometric optics . first , we do see different images of the same object at times ! or rather , we see a blurry image rather than a sharp one . if you bring a pencil so close to your eyes that you cannot focus on it using your iris ( more on that later ) , then you will see a blurry image . this happens for exactly the reason you mention . rays from the same point of the object take different paths to your retina . if the path taken to your lens makes a large angle compared to the path that passes straight through the lens , then in general the rays will not recombine at a single point , but will have some spread . as the object moves closer to your eye , the angles increase , so the spread becomes larger and you see a blurry image . your iris is very important for creating sharp images . generally , the distance between your lens and your retina is fixed . the distance between the object and your lens is not fixed , but we would like to be able to resolve detail for some range of object distances , rather than just a single one . so , the only parameter your body can control is the focal length . by constricting and relaxing , the iris changes the curvature of your lens . the change in curvature leads to a change in focal length . so , whatever object you are attempting to focus on , your iris constricts so that the object is beyond the focal length of your lens . this ensures that the rays will converge toward the retina and produce an image . however , even with the object beyond the focal length you still get a blurry image if the rays make a large angle with the axis that is perpendicular to your eye/lens/retina ( as discussed before ) and this is one reason why you have a pupil . the pupil only allows rays that are approximately all parallel to each other to fall on your lens . it effectively acts as a collimator . so now , you have an object at or beyond the focal length and a set of approximately parallel rays that fall on your lens and are focused . this leads to a relatively sharp image on your retina ( assuming that the object is far enough away that the pupil can do its job ) . the final piece of the puzzle is your brain . even though your iris , lens , and pupil do what they can to create a sharp image for your retina , it is still imperfect . there are still a number of aberrations in the image that falls on your retina . your visual cortex and related support areas in the brain do all of the processing and reconstructing that leads to you perceiving a sharp image .
to better sum up my question : could a gravitational wave be described as a wavefunction ? at the moment the only candidates for describing a quantized gravitational field and at the same time embed the standard model of particle physics , are string theories . there is no quantization of gravity alone , as following the recipe for quantizing other fields leads to infinities due to the spin 2 of the proposed graviton . quantisation of gravity is a field of active theoretical physics research . we have experimental evidence that general relativity holds . we do not have experimental evidence that a graviton exists . we can assume it does and then theorize about interactions of the graviton as wave/particle with other fields and wave functions , but it is just an imaginary exercise at this level . and yes , you would need as prerequisite quantum field theory to start understanding string theory . p.s. the collapse of the wavefunction concept is misleading , as the wave itself is not a wave in the field . it is a probability wave for finding a particle in an ( x , y , z , t ) location .
i agree that the person on the embankment will say that the person on the train should not see them as simultaneous well , then the person on the train should not see them as simultaneous . some things change between reference frames , but conclusions of the form " in frame $s$ , an observer will see . . . " do not change , since the statement itself specifies which frame you have to be in to understand what it is saying . the observer on the embankment could easily see the train observer intercept the forward flash before the rear flash . ( of course , the embankment observer could not do this in real time ; one has to wait until after one 's hypothetical grid of rulers and clocks reports back what happened when and where . ) one nice thing about sr is that time-ordering is invariant . that is , two events $a$ and $b$ can have one of three relations to one another : $a$ is in $b$ 's past light cone ( and $b$ is in $a$ 's future ) , the reverse of that statement , or $a$ and $b$ are spacelike separated . whichever one of these holds will hold for all observers . so we know , just from the embankment analysis , that " in the frame of the train , the forward flash reaches the observer 's eyes first , " and this statement is always true for anyone who speaks it in its entirety . what about the train observer ? indeed , as you say , the flashes occurred at the same distance from each of them , the speed of light is constant in both frames , and either one can claim to be at rest suppose two people , $c$ and $d$ , stand equal distances from you and are known to pitch balls at exactly the same speed . with everyone standing at rest , $c$ and $d$ each toss you a ball . you get the ball from $c$ before the one from $d$ . this is not a logical inconsistency . it simply means $c$ threw a ball before $d$ in your reference frame . that is , the person on the train , operating under the sr assumption of " the speed of light is constant , " and using the data ( retroactively obtained from a ruler-clock grid , or maybe obtained in real time based on brightnesses ) that the flashes were equidistant , must conclude that the forward flash went off first .
the lamp is not a point source . the smaller the angular size of the source , the narrower is the penumbral shadow region .
( there is a couple of these questions kicking around , but i did not see anyone give the " two boosted copies " answer . generically , i would say that is the right answer , since it gives an actual causality violation . ) in your scenario , the two planets remain a hundred thousand light years apart . the fact is , you will not get any actual causality violations with ftl that way . the trouble comes if the two planets are moving away from each other . so , let 's say that your warp drive travels at ten times the speed of light . except if the two endpoints of the trip are moving , then what does that mean ? ten times the speed of light relative to which end ? let 's say tralfamadore is moving at a steady 20% of $c$ ( the speed of light ) , away from earth . ( so , earth is moving at a steady 20% of $c$ away from tralfamadore . ) if i leave tralfamadore ( in the direction of earth ) and i am travelling at anything less than 20% of $c$ relative to tralfamadore , then i am still moving away from earth . i will never get home . let 's say instead i am travelling at 60% of $c$ relative to tralfamadore . i will catch up to earth . relative to earth , how fast am i approaching ? you might guess the answer is 40% of $c$ , but it is 45.45% . generally , the velocity subtraction formula of relativity is : $$w = ( u-v ) / ( 1-uv/c^2 ) $$ let 's say instead i am travelling at 100% of $c$ relative to tralfamadore . plug $u=c , v=0.2c$ into the formula and get $w=c$ . relative to earth , i am approaching at 100% of $c$ ! the speed of light is the same for everyone . so finally , let 's say instead i am using your warp drive to travel at 1000% of $c$ relative to tralfamadore . relative to earth , i am approaching at -980% of $c$ . in earth 's reference frame , i will arrive on earth before i leave tralfamadore . now you may say this in itself is not a causality violation , because we have applied earth 's calendar to tralfamadore . and that is true , but i will make a round trip : in the futuristic earth year of 3000 , tralfamadore is 98,000 light years away , and receding at 20% of $c$ . i leave earth at 1000% of $c$ , relative to earth . in earth year 13000 tralfamadore is 100,000 light years away , and i catch up to it . i turn around and leave tralfamadore at 1000% of $c$ , relative to tralfamadore . in earth year 2796 , i arrive home . earth 's calendar certainly applies to earth , and i arrived home two centuries before i left . no two ways about it , i am a time traveller ! there is nothing special about ten times the speed of light . given a warp drive that moves a certain amount faster than light , you can make the above time machine using two endpoints that are moving apart a certain amount slower than light , provided that the warp drive can move faster than light relative to either end . this time machine works for any form of ftl : tachyons , warp drives , wormholes , what have you .
basically , in atomic physics , you would have two electrons , each with an angular momentum $l_1$ and $l_2$ and spin $s_1$ and $s_2$ , and you want to couple all those to get the best approximation for the resulting spectrum . so you have two options : 1- you couple $l_1$ and $l_2$ to $l$ , and $s_1$ and $s_2$ to $s$ , and then you couple $l$ with $s$ to get $ls$ . 2- you couple $l_1$ and $s_1$ to $j_1$ , and $l_2$ and $s_2$ to $j_2$ , and then couple $j_1$ and $j_2$ to to get $jj$ . so you have two ways to couple those , and the choice depends on how far the electrons are from each other where the specific angular momentum coupling is more pronounced . so if the electrons are close to each other , then you use ls coupling . while if you have them far apart , you use jj coupling . i hope this helps .
1 ) spline curves are designed for just this sort of thing . a spline is basically a set of cubic ( typically ) polynomials at each section of your data that go exactly through your points . but since they are just cubic polynomials , they are pretty smooth . assuming the path is a reasonably small portion of the earth , and the points are close together on a global scale , you can just apply a spline to the latitude as a function of time , then to longitude as a function of time , then to elevation as a function of time . as an added bonus , most spline implementations will also let you take the derivative at any point along the spline . this gives you the speed -- up to appropriate factors because you are dealing with lat/long/elevation . i do not know what sort of computer packages you have access to , but python 's implementation can definitely do the job . ( for specifics about that , it is probably better to ask on stackoverflow . ) 2 ) i am guessing you mean to use this smoother path as the source for those intermediate points ? then , you can just ask the spline to evaluate at a time that is halfway between two times that you have in your original data . how you do this depends on your computer package . ( and again , is an issue better suited to stackoverflow . ) 3 ) in terms of prettiness , this can certainly make things look much better , and it is generally a reasonable thing to do . but you have to remember that the points you do not have in your data will just be made up , regardless of what you do . a spline is good at giving you something that looks reasonable . but whether or not the gps actually traveled that path is anyone 's guess .
the very claim that there are " four fources " is an approximation . we know that the electromagnetic and the weak force have to be unified to an electroweak theory . so counting the electroweak theory as one force , there are just three known elementary forces . the electroweak theory is based on the $su ( 2 ) \times u ( 1 ) $ group which has two factors , but these two factors are not in one-to-one correspondence with the electromagnetism and the weak force , respectively . the strong force with its $su ( 3 ) $ group is another seemingly independent factors , except that there is evidence that all three non-gravitational forces get unified into a grand unified force of a gut theory at high energies . string theory unifies the non-gravitational forces with gravity , too . every vacuum of string theory predicts gravity described by gr plus extra non-gravitational forces . the number of factors and their higgs-like breaking patterns are essentially random properties of the string vacua . according to the anthropic picture of the world , the number of low-energy forces is an accidental property of our world that could be different in different parts of the multiverse . according to non-anthropic reasoning , the precise selection of our vacuum - including the fact that it has 4 low-energy forces - could be derivable from some more unique theoretical principles . however , this research program remains a wishful thinking as of 2011 .
if you go to this link you will see that the lifetime of the pi0 is orders of magnitude shorter than of the charged pions . 8.4 ± 0.6 × 10^−17 seconds , a time characteristic of electromagnetic reactions . it decays to two photons , which can be measured in the laboratory . if it is produced with some energy in the laboratory system , its speed can be estimated by measuring the four momenta of the photons and equating the sum to the four momentum of the pi0 . its speed then can be found for that individual measurement . there is no general " speed " of the pi0 , as there is no general speed of any elementary particle , their four momenta being dependent of the interaction that produced them and very variable . to have a speed a fraction of the speed of light any pion or other elementary particle should have an energy given by the relativistic formulae . have a look here where they calculate the energy necessary for a velocity 1% of the velocity of light for various particles .
here we will only consider the first half of the question ( v2 ) . the birkhoff 's theorem is e.g. proven ( at a physics level of rigor ) in ref . 1 and ref . 2 . imagine that we have managed to argue that the metric is of the form of eq . ( 5.38 ) in ref . 1 or eq . ( 7.13 ) in ref . 2: $$ds^2~=~-e^{2\alpha ( r , t ) }dt^2 + e^{2\beta ( r , t ) }dr^2 +r^2 d\omega^2 . \qquad\qquad ( a ) $$ it is a straightforward exercise to calculate the corresponding ricci tensor $r_{\mu\nu}$ , see eq . ( 5.41 ) in ref . 1 or eq . ( 7.16 ) in ref . 2 . the notation is here $x^0\equiv t$ , $x^1\equiv r$ , $x^2\equiv\theta$ , and $x^3\equiv\phi$ . assuming a vanishing cosmological constant $\lambda=0$ , the einstein 's equations in vacuum read $$r_{\mu\nu}~=~0~ . $$ the argument is now as follows . from $r_{tr}=0$ follows that $\beta$ is independent of $t$ . from $e^{2 ( \beta-\alpha ) } r_{tt}+r_{rr}=0$ follows that $\partial_r ( \alpha+\beta ) =0$ . in other words , the function $f ( t ) :=\alpha+\beta $ is independent of $r$ . define a new coordinate variable $t:=\int^t dt&#39 ; ~e^{f ( t&#39 ; ) }$ . then the metric $ ( a ) $ becomes $$ds^2~=~-e^{-2\beta}dt^2 + e^{2\beta}dr^2 +r^2 d\omega^2 . \qquad\qquad ( b ) $$ rename the new coordinate variable $t\to t$ . then eq . $ ( b ) $ corresponds to setting $\alpha=-\beta$ in eq . $ ( a ) $ . from $r_{\theta\theta}=0$ follows that $$1=e^{-2\beta} ( 1-2r\partial_r\beta ) \equiv\partial_r ( re^{-2\beta} ) , $$ so that $re^{-2\beta}=r-r$ for some real integration constant $r$ . in other words , we have derived the schwarzschild solution , $$e^{2\alpha}~=~e^{-2\beta}~=~1-\frac{r}{r} . $$ finally , if we switch back to the original $t$ coordinate variable , the metric $ ( a ) $ becomes $$ds^2~=~-\left ( 1-\frac{r}{r}\right ) e^{2f ( t ) }dt^2 + \left ( 1-\frac{r}{r}\right ) ^{-1}dr^2 +r^2 d\omega^2 . \qquad\qquad ( c ) $$ it is interesting that the metric $ ( c ) $ is the most general metric of the form $ ( a ) $ that satisfies einstein 's vacuum equations with $\lambda=0$ . the only freedom is the function $f=f ( t ) $ , which reflects the freedom to reparametrize the $t$ coordinate variable . references : sean carroll , spacetime and geometry : an introduction to general relativity , 2003 . sean carroll , lecture notes on general relativity , chapter 7 . the pdf file is available here .
the pressure in the straw is ( almost ) the same throughout the straw . i say almost because air does have some weight so the pressure would be slightly higher at the bottom of the straw than the top , but this is going to be a very small effect . the pressure at the air/water interface has to be the same as the pressure of the water , otherwise the water would flow into or out of the straw until the pressures equalised . so if you can calculate the pressure of the water you can calculate the pressure of the air in the straw . the flex of the walls of the straw does not affect this concusion . however it does affect the volume of the straw so if you are using the distance the water enters the straw you will have to correct for a volume increase . if the straw is stretchy the water will be able to enter the straw further for the same pressure . temperature will change the density of the water and therefore the pressure . you need a table of water density:temp , or some suitable approximate equation to estimate it . i do not think humidity will have much effect , though i am not sure i would swear to this in court .
note that $\vec{\mathbf v}_\mathrm{av}$ is defined as the average value of $\vec{\mathbf v}$: $$\vec{\mathbf v}_\mathrm{av}:=\frac{1}{t_1-t_0}\int_{t_0}^{t_1}\vec{\mathbf v} ( t ) \ , \mathrm dt . $$ since $\vec{\mathbf x}$ is the antiderivative of $\vec{\mathbf v}$ , this equals $$\frac{\vec{\mathbf x} ( t_1 ) -\vec{\mathbf x} ( t_0 ) }{t_1-t_0} . $$ however , when acceleration is constant , and thus $\vec{\mathbf v}$ is a line ( that is , $\vec{\mathbf v} ( t ) =\vec{\mathbf a}t+\vec{\mathbf v}_0$ ) , then by plugging into the average value integral , you obtain the equality $$\vec{\mathbf v}_\mathrm{av}=\frac{\vec{\mathbf v} ( t_1 ) +\vec{\mathbf v} ( t_0 ) }{2} . $$
i recommend you chapter 5 ( page 150+ ) of the ads bible , http://arxiv.org/abs/hep-th/9905111 concerning your individual questions , which are mostly answered at the beginning of that chapter , the additional virasoro generators correspond to bulk coordinate reparametrizations that preserve the metric at infinity , but they do map the ground state to excited states yes , the cfts in ads/cft typically have a nonzero central charge which is directly related to the $ads_3$ curvature radius in the planck units ; there is no reason for $c=0$ here because the boundary cft is not really coupled to gravity ( which is what the world sheet cft is doing ) for the same reason , you can not directly interpret the cft as string theory ; the full string theory needs $c=0$ in total , so extra ghosts must be added ; also , the interpretation of " winding/twisted " sectors is different in boundary cfts and string cfts . of course , this does not eliminate the fact that similar " building blocks of cfts " are used in both kinds of cfts . . .
if the universe were non-homogenous , you had have a point . but the key point of robertson-walker cosmology is that you fit the data very well by having a universe that is very nearly spatially homogenous . this means that , if each observer clicks of $t$ amount of time since the big bang , then they will all agree that there is no special point in the universe ( like a center or an edge ) , and will all agree on the general geometry and shape of the whole universe . since you have this global time parameter , you do not get the effects you imply in your question . everyone can agree on what $7 billion years after the big bang , at this point ) means .
what you are talking about is called a combined cycle engine . they are commonplace in stationary power generation , i.e. utility-scale electricity generation . there has even been some talk of combined cycle engines in cars . as pointed out in the answer by dmckee , the reason this has not been widely applied in cars is that no one has demonstrated an economically competitive combined-cycle car . i promise you , if such a thing can pay for itself in gas savings then it will eventually be built and sold , unless some better technology makes it irrelevant . in general there are many reasonable ideas that are physically permissible but economically or technically difficult or nonviable . you are effectively suggesting to add a steam engine to a car , which is quite a difficult proposal . i would suggest that a hybrid gas-electric car is more economical than what you suggest , and even they have had a hard time catching on . in electric power generation it matters much less that the combined cycle engine has a larger sunk cost than a normal engine , is heavier , etc . , so the economic balance works out . bringing the question back to physics , no matter what you use for heat scavenging , your engine including all of its " subengines " cannot exceed the carnot efficiency corresponding to the largest temperature difference in the engine . adding additional heat engines will help to approach the carnot limit . in order to beat carnot , you can not use heat as an intermediate step between chemical energy ( fuel ) and mechanical work .
the moment of inertia is merely a generalisation/application of the ‘usual’ inertia to rotations . since translations and rotations are different kinds of motion , it appears sensible ( to me ) to have different kinds of inertia associated with them . regarding your second question : imagine a particle at position $ ( x , 0,0 ) $ which you would like to rotate with angular velocity $\omega$ about the $ ( 0,0 , z ) $ axis . to do so , you have to initially accelerate the particle along the $ ( 0 , y , 0 ) $ axis to velocity $v_y = \omega x , v_{x , z} = 0$ , as this is the velocity the particle would have at this point if it were already rotating . as you can clearly see , the momentum $p$ associated with this velocity is proportional to $r$ ( $p_y = m v_y = m \omega x$ ) , hence it takes more energy to accelerate a particle to angular velocity $\omega$ if it is further away from the centre of rotation . as this is exactly the quantity described by the ‘moment of inertia’ , the moment of inertia depends on the radial distance of the mass .
you should read the wikipedia article on nuclear reactions for a start . while the number of possible nuclear reactions is immense , there are several types which are more common , or otherwise notable . some examples include : fusion reactions — two light nuclei join to form a heavier one , with additional particles ( usually protons or neutrons ) thrown off to conserve momentum . spallation — a nucleus is hit by a particle with sufficient energy and momentum to knock out several small fragments or , smash it into many fragments . induced gamma emission belongs to a class in which only photons were involved in creating and destroying states of nuclear excitation . alpha decay - though driven by the same underlying forces as spontaneous fission , α decay is usually considered to be separate from the latter . the often-quoted idea that " nuclear reactions " are confined to induced processes is incorrect . " radioactive decays " are a subgroup of " nuclear reactions " that are spontaneous rather than induced . for example , so-called " hot alpha particles " with unusually high energies may actually be produced in induced ternary fission , which is an induced nuclear reaction ( contrasting with spontaneous fission ) . such alphas occur from spontaneous ternary fission as well . neutron-induced nuclear fission reactions – a very heavy nucleus , spontaneously or after absorbing additional light particles ( usually neutrons ) , splits into two or sometimes three pieces . this is an induced nuclear reaction . spontaneous fission , which occurs without assistance of the neutron , is usually not considered a nuclear reaction . at most , it is not an induced nuclear reaction . direct reactions : an intermediate energy projectile transfers energy or picks up or loses nucleons to the nucleus in a single quick ( 10−21 second ) event . energy and momentum transfer are relatively small . these are particularly useful in experimental nuclear physics , because the reaction mechanisms are often simple enough to calculate with sufficient accuracy to probe the structure of the target nucleus . from this list one can see that " direct reactions " are specific scattering reactions with the purpose of studying a particular nucleus . thus the former list of nuclear reactions cannot be described as a combination or a series of " direct reactions " .
the electron-phonon scattering is a process that takes the electron across its fermi surface ( from an occupied state to an empty state , or vice versa ) by absorbing or emitting a phonon . compared to the electron energy in most solid state materials , the phonon energy is neglectable , such that the electron will ( almost ) not change its energy when scattering with a phonon . therefore the phonon can only scatter off the electrons on the fermi surface . however the greatest possible momentum transfer on the fermi surface is $2k_f$ ( assuming spherical fermi surface with radius $k_f$ ) . so all scattered phonon must have a momentum $q\leq 2k_f$ .
i would guess you mean self diffusion : see http://en.wikipedia.org/wiki/self-diffusion for details . suppose you take an aqueous solution of ( for example ) salt that is uniform so there are no concentration gradients . there is no net diffusion , but the sodium and chloride ions wander around due to random thermal motion , so if you watch a particular sodium atom it will " diffuse " around in a random walk motion .
here is a mathematical derivation . we use the sign convention $ ( + , - , - , - ) $ for the minkowski metric $\eta_{\mu\nu}$ . i ) first recall the fact that $sl ( 2 , \mathbb{c} ) $ is ( the double cover of ) the restricted lorentz group $so^+ ( 1,3 ; \mathbb{r} ) $ . this follows partly because : there is a bijective isometry from the minkowski space $ ( m ( 1,3 ; \mathbb{r} ) , ||\cdot||^2 ) $ to the space of $2\times2 $ hermitian matrices $ ( u ( 2 ) , \det ( \cdot ) ) $ , $$\mathbb{r}^4~=~m ( 1,3 ; \mathbb{r} ) ~\cong ~ u ( 2 ) ~:=~\{\sigma\in {\rm mat}_{2\times 2} ( \mathbb{c} ) \mid \sigma^{\dagger}=\sigma \} ~=~ {\rm span}_{\mathbb{r}} \{\sigma_{\mu} \mid \mu=0,1,2,3\} , $$ $$ m ( 1,3 ; \mathbb{r} ) ~\ni~\tilde{x}~=~ ( x^0 , x^1 , x^2 , x^3 ) \quad\mapsto \quad\sigma~=~x^{\mu}\sigma_{\mu}~\in~ u ( 2 ) , $$ $$ ||\tilde{x}||^2 ~=~x^{\mu} \eta_{\mu\nu}x^{\nu} ~=~\det ( \sigma ) , \qquad \sigma_{0}~:=~{\bf 1}_{2 \times 2} . $$ there is a group action $\rho : sl ( 2 , \mathbb{c} ) \times u ( 2 ) \to u ( 2 ) $ given by $$g\quad \mapsto\quad\rho ( g ) \sigma~:= ~g\sigma g^{\dagger} , \qquad g\in sl ( 2 , \mathbb{c} ) , \qquad\sigma\in u ( 2 ) , $$ which is length preserving , i.e. $g$ is a pseudo-orthogonal ( or lorentz ) transformation . in other words , there is a lie group homomorphism $$\rho : sl ( 2 , \mathbb{c} ) \quad\to\quad o ( u ( 2 ) , \mathbb{r} ) ~\cong~ o ( 1,3 ; \mathbb{r} ) , \qquad \rho ( \pm {\bf 1}_{2 \times 2} ) ~=~{\bf 1}_{u ( 2 ) } . $$ the lie group $sl ( 2 , \mathbb{c} ) =\pm e^{sl ( 2 , \mathbb{c} ) }$ has lie algebra $$ sl ( 2 , \mathbb{c} ) ~=~ \{\tau\in{\rm mat}_{2\times 2} ( \mathbb{c} ) \mid {\rm tr} ( \tau ) ~=~0 \} ~=~{\rm span}_{\mathbb{c}} \{\sigma_{i} \mid i=1,2,3\} . $$ the lie group homomorphism $\rho : sl ( 2 , \mathbb{c} ) \to o ( u ( 2 ) , \mathbb{r} ) $ induces a lie algebra homomorphism $$\rho : sl ( 2 , \mathbb{c} ) \to o ( u ( 2 ) , \mathbb{r} ) $$ given by $$ \rho ( \tau ) \sigma ~=~ \tau \sigma +\sigma \tau^{\dagger} , \qquad \tau\in sl ( 2 , \mathbb{c} ) , \qquad\sigma\in u ( 2 ) , $$ $$ \rho ( \tau ) ~=~ l_{\tau} +r_{\tau^{\dagger}} , $$ where we have defined left and right multiplication of $2\times 2$ matrices $$l_{\sigma} ( \tau ) ~:=~\sigma \tau~=:~ r_{\tau} ( \sigma ) , \qquad \sigma , \tau ~\in~ {\rm mat}_{2\times 2} ( \mathbb{c} ) . $$ ii ) note that the lorentz lie algebra $o ( 1,3 ; \mathbb{r} ) \cong sl ( 2 , \mathbb{c} ) $ does not$^{\ddagger}$ contain two perpendicular copies of , say , the real lie algebra $su ( 2 ) $ or $sl ( 2 , \mathbb{r} ) $ . for comparison and completeness , let us mention that for other signatures in $4$ dimensions , one has $$o ( 4 ; \mathbb{r} ) ~\cong~su ( 2 ) \oplus su ( 2 ) , \qquad\text{ ( compact form ) }$$ $$o ( 2,2 ; \mathbb{r} ) ~\cong~sl ( 2 , \mathbb{r} ) \oplus sl ( 2 , \mathbb{r} ) . \qquad\text{ ( split form ) }$$ ( the compact form has a nice proof using quaternions , see also this math . se question . ) to decompose minkowski space into left- and right-handed weyl spinor representations , one must go to the complexification , i.e. one must use the fact that $sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) $ is ( the double cover of ) the complexified lorentz group $so ( 1,3 ; \mathbb{c} ) $ . note that ref . 1 does not discuss complexification$^{\ddagger}$ . one can more or less repeat the construction from section i with the real numbers $\mathbb{r}$ replaced by complex numbers $\mathbb{c}$ , however with some important caveats . there is a bijective isometry from the complexified minkowski space $ ( m ( 1,3 ; \mathbb{c} ) , ||\cdot||^2 ) $ to the space of $2\times2 $ matrices $ ( {\rm mat}_{2\times 2} ( \mathbb{c} ) , \det ( \cdot ) ) $ , $$\mathbb{c}^4~=~m ( 1,3 ; \mathbb{c} ) ~\cong ~ {\rm mat}_{2\times 2} ( \mathbb{c} ) ~=~ {\rm span}_{\mathbb{c}} \{\sigma_{\mu} \mid \mu=0,1,2,3\} , $$ $$ m ( 1,3 ; \mathbb{c} ) ~\ni~\tilde{x}~=~ ( x^0 , x^1 , x^2 , x^3 ) \quad\mapsto \quad\sigma~=~x^{\mu}\sigma_{\mu}~\in~ {\rm mat}_{2\times 2} ( \mathbb{c} ) , $$ $$ ||\tilde{x}||^2 ~=~x^{\mu} \eta_{\mu\nu}x^{\nu} ~=~\det ( \sigma ) . $$ note that forms are taken to be bilinear rather than sesquilinear . there is a lie group homomorphism $$\rho : sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) \quad\to\quad o ( {\rm mat}_{2\times 2} ( \mathbb{c} ) , \mathbb{c} ) ~\cong~ o ( 1,3 ; \mathbb{c} ) $$ given by $$ ( g_l , g_r ) \quad \mapsto\quad\rho ( g_l , g_r ) \sigma~:= ~g_l\sigma g^{\dagger}_r , \qquad g_l , g_r\in sl ( 2 , \mathbb{c} ) , \qquad\sigma~\in~ {\rm mat}_{2\times 2} ( \mathbb{c} ) . $$ the lie group $sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) $ has lie algebra $sl ( 2 , \mathbb{c} ) \oplus sl ( 2 , \mathbb{c} ) $ . the lie group homomorphism $$\rho : sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) \quad\to\quad o ( {\rm mat}_{2\times 2} ( \mathbb{c} ) , \mathbb{c} ) $$ induces a lie algebra homomorphism $$\rho : sl ( 2 , \mathbb{c} ) \oplus sl ( 2 , \mathbb{c} ) \quad\to\quad o ( {\rm mat}_{2\times 2} ( \mathbb{c} ) , \mathbb{c} ) $$ given by $$ \rho ( \tau_l\oplus\tau_r ) \sigma ~=~ \tau_l \sigma +\sigma \tau^{\dagger}_r , \qquad \tau_l , \tau_r\in sl ( 2 , \mathbb{c} ) , \qquad \sigma\in {\rm mat}_{2\times 2} ( \mathbb{c} ) , $$ $$ \rho ( \tau_l\oplus\tau_r ) ~=~ l_{\tau_l} +r_{\tau^{\dagger}_r} . $$ the left action ( acting from left on a two-dimensional complex column vector ) yields by definition the ( left-handed weyl ) spinor representation $ ( \frac{1}{2} , 0 ) $ , while the right action ( acting from right on a two-dimensional complex row vector ) yields by definition the right-handed weyl/complex conjugate spinor representation $ ( 0 , \frac{1}{2} ) $ . the above shows that the complexified minkowski space $m ( 1,3 ; \mathbb{c} ) $ is a $ ( \frac{1}{2} , \frac{1}{2} ) $ representation of the lie group $sl ( 2 , \mathbb{c} ) \times sl ( 2 , \mathbb{c} ) $ , whose action respects the minkowski metric . references : anthony zee , quantum field theory in a nutshell , 2003 . $^{\ddagger}$ for a laugh , check out the wrong second sentence on p . 113 in ref . 1 . nevertheless , let me rush to add that ref . 1 is overall a very nice book .
if the container full of air is spinning around you , the drag will eventually set you spinning as well , regardless of the rotational speed or the air density . low air density just means that it will take much longer . eventually the air and you will share the same rotation , so that as you speed up , the air and the container will slow down . only in ( complete ) vacuum will you never start spinning . but there is no such thing as a complete vacuum , there are always at least some atoms or molecules around . when the density gets too low , quantum effect will start to take over , as individual particles push you one way or the other .
from this google book hit , i think it is called a ketenyl radical . searching ketenyl radical seems to bring up hits with the same compound formula . incidentally , it was not that hard to find by searching hcco compound . ninth hit , i think . . .
i think your questions concerns somehow another question : what is the relation between the macroscopic observables ( like electrical current , temperature ) of system consisting of many particles and parameters of single particles forming this system . the answer on this question is given by statistical physics . first , it will be useful to think about an electron gas in a metal wire without any current . there is no current , but i dare you all particles are moving if the temperature is not equal zero . the electrical current results from averaging of velocities of all charge carriers and , therefore , equals zero . if we apply voltage all electrons gain an additional component to their chaotic velocities in the direction of the electric field . we have got electron drift , however it is not equal to microscopic current of each electron . is the velocity of the particle in the direction of the current ? it depends on the charge sign of charge carriers . if we have got electrons then their averaged speed is opposite to the direction of the electrical current .
the fact that $p = \large \frac{\partial l}{\partial \dot{q}} = 0$ introduces a problem in the equivalence between lagrangian and hamiltonian representations . the idea is that the hamiltonian representation plus the constraint $p = 0$ is equivalent to the lagrangian representation the lagrangian $l$ is a function of $q$ and $\dot q$ , that is $l ( q , \dot q ) $ if we work with the lagrangian , we will apply the euler-lagrange equations which are : $$\frac{\partial l}{\partial q} = \frac{d}{dt} ( \frac{\partial l}{\partial \dot{q}} ) $$ because $\large \frac{\partial l}{\partial \dot{q}} = 0$ , the equation is simply $\large \frac{\partial l}{\partial q} = 0$ , that is $ \frac{1}{q} - 2\lambda = 0$ , so $q = \frac{1}{2 \lambda}$ now try to work with the hamiltonian . the hamiltonian $h$ is a function of $q$ and $p$ , that is $h ( q , p ) $ the link between the two is the legendre transformation : $$h=\dot{q}\frac{\partial l}{\partial \dot{q}}-l$$ because your lagrangian does not depends of $\dot q$ , then $p = \frac{\partial l}{\partial \dot{q}} = 0$ , and so : $$h ( q , p ) = - l ( q , \dot q ) = - \ln ( q ) + ( 2q-10 ) \lambda$$ from this hamiltonian , you get the equations of movement : $$\dot q = \frac{\partial h}{\partial p} ~ , ~\dot p = - \frac{\partial h}{\partial q}$$ so we have : $$\dot q = 0~ , ~\dot p = \frac{1}{q} - 2\lambda \tag{1}$$ from this , we cannot recover the equation obtained from euler-lagrange equations , we have to add the constraint $p = 0$ . if $p = 0$ , it means that $\dot p = 0$ , and so : $$q = \frac{1}{2 \lambda}\tag{2}$$ this is coherent with the fact that $\dot q = 0$
only with this , as the waves have different wavelengths , i guess there can not be any interference , we will only see the difraction pattern , the two functions of the form sin2 ( x ) /x2 , with the principal maximums separated a distance d . am i right here ? sort of . the diffraction pattern is visible " at infinity " , which is in fact your case #3 . i will explain there . 1 . - in the first one , the system is configured such that the slits are far away from the lens . here , we can approximate the wave that arrives as a planar wave , and therefore the lens will perform the fourier transform in the focal plane of the screen . the diffraction of the slits also performs the fourier transform , so this configuration should lead to having only two bars of light in the screen , centered in the focus . am i right ? sort of . your lens has a limited diameter , so if you place it far from the slits , it will capture only the central portion of the diffraction pattern , i.e. the top of the sinx/x function . in other words , you will loose the fringe pattern and reconstruct slits without fringes . 2 . - the slits are in the focal plane on the lens , such that the lens is in the middle of slits-screen . here , the same thing should happen , right ? as the light comes from the focal plane , the lens must do the fourier transform with no extra things , and we should get the two bars , again both of them in the same line ( center of the screen ) . am i right here ? this will be somewhat different because you do the image of the slits at infinity , i.e. a blurry image at close distances . depending on how close is the lens , you may only be taking the " no-fringe " portion of the diffraction pattern . 3 . - the last one , i can not see . . . the lens is just behind the slits so the distance between slits and lens is 0 . that is exactly the typical school case . the diffraction pattern is , before the lens , located at infinity , or in other words , the fringes are defined as angles and not position ( $\sin\theta/\theta$ ) . the role of the lens is to bring these fringes at a finite distance ( the focal length ) . you probably learned that a lens makes an image , initially located at infinity , located at the focal length . that is the same with the diffraction fringes . now , as the two slits are both close to the lens , you will not do an image of them . that means that you should not see separated slit images , but instead , you should see two superimposed diffraction patterns , centered at the same point .
i think user689 's answer is basically correct , so you should regard this as just an extension/clarification of their answer . if you place a small volume of hot tea in contact with your tongue then the large thermal capacity of your tongue will cool the tea a lot and your tongue will heat up a little . this is essentially what happens when you sip tea . you pull in a certain volume of tea , this tea is spread out over the tongue and stays in contact with it for a second or so . in this time the tea cools and your tongue heats . however the heating of the tongue is relatively small because the heat from the tea is spread over a large area . if you suck the same volume of tea through a straw , then because the cross sectional area of the straw is small the velocity of the tea in the straw is high ( as user689 points out ) so the whole volume of the tea rapidly hits , and heats , a small area on the tongue . this causes much greater heating of that small area and scalds the tongue . exactly what the sensory effects of the scald are i will leave to our medical/biological colleagues . this also suggests a reason why drinking cool fluids through a straw is pleasant . the same effect means that when drunk through a straw a cold drink causes more intense local cooling than if it was drunk without a straw .
it seems you are correct the $p$ is the momentum of the center of mass ( com ) of your $qq$ ( sorry do not know how to add bars ) system in the lab frame . due to conversation of momentum the $qq$ system may only have momentum in the x direction since that is your initial conditions . with regards to the 2nd part of your question in the cmf ( center of mass frame ) of the $qq$ system the $qq$ pair will have equal and opposite momentums ; however , in the case where the e of the initial condition is just sufficient to produce the $qq$ pair they can have no additional kinetic energy , thus they will be at rest in their cmf . again due to conservation of momentum the $qq$ com must still be moving in the x direction in the lab frame .
the forces on the screw are not symmetric . once the screw is no longer turning loosely in the hole tightening the screw compresses the two materials held together ( i.e. . increases the stress on the material , i.e. stores energy in the material ) , while loosening reduced the compression ( i.e. . releases the stress ) . so a random dislocation will be more likely to occur in the " loose " direction than the " tight " direction .
an electromagnetic wave with a well-defined frequency and direction , i.e. $\vec k$ , only has two possible truly physical i.e. transverse polarizations , i.e. the linearly polarized waves in the $x$ and $y$ direction ( or the two circularly polarized ones ) . that implies that a truly physical counting of polarizations gives you 2 , more generally $d-2$ in $d$ spacetime dimensions . starting from the $a_\mu$ potential fields , one component is unphysical because it is pure gauge , $a_\mu\sim\partial_\mu\lambda$ , and one of them is forbidden due to gauss ' constraint $\rm div\ , \vec d=0$ etc . that already constrains the allowed initial state of the electromagnetic field . both of these killed polarizations are ultimately linked to the $u ( 1 ) $ gauge symmetry . if one is allowed to count off-shell and unphysical fields , there may be many more components than two . but it is always possible to deduce that there are two physical polarizations at the end . for example , when we view $\vec b , \vec e$ as basic fields , there are six components , a lot . but these fields only enter maxwell 's equations through first derivatives , and not second as expected for " normal " bosonic fields , so these fields are simultaneously the canonical momenta for themselves . this brings us to three polarizations but one of them is killed by the constraints , the maxwell 's equations that do not contain time derivatives . the hertz vector is just the most famous " non-standard " example how to write the electromagnetic field as a combination of derivatives of some other fields . one must understand that the room for mathematical redefinitions etc . is unlimited and it is a matter of pure maths . all these descriptions may describe the same physics . at the end , the only " truly invariant because measurable " number of " fields " that all these approaches must agree about is the number of linearly independent physical polarizations of a wave/photon with a given $\vec k$ . if you can analyze any mathematical formulation of electromagnetism or another field theory and derive that there are $d-2$ physical polarizations ( this usually boils down to the difference of the number of a priori fields minus the number of independent constraints and the number of parameters defining identifications i.e. gauge symmetries – but the independence is sometimes hard to see and requires you to make many steps of the counting ) , then you have proved everything that is " really forced to be true " . various formalisms may offer you other ways to count the number of off-shell fields ( with different answers ) and they may be useful ( because they satisfy certain conditions or enter some laws ) but to discuss them , one has to know what the laws where they enter actually are . a truly physical approach is only one that counts the physical polarizations . the gauge symmetry is just a redundancy , a mathematical trick to get the right theory with 2 physical polarizations out of a greater number of fields with certain extra constraints or identifications . the precise number of constraints or identifications may depend on the chosen mathematical formalism and it is not a physically meaningful question – it is a question of a subjectively preferred mathematical formalism because the physics is equivalent for all of them .
at the moment of the lunar eclipse both the earth and moon are moving tangentially to the line joining them to the sun , and their velocities are parallel . the diagram above shows the moon just before , during and just after the lunar eclipse . i am guessing the question is asking whether the velocity of the moon , $v_m$ , is greater or less than the velocity of the earth , $v_e$ . if so , then you just have to see what direction the earth 's shadow moves across the moon . if $v_m &gt ; v_e$ then the moon starts at the lower position and moves up past the earth , so the shadow starts at the top edge of the moon and moves down . if $v_m &lt ; v_e$ then the moon starts at the top position and moves down , so the shadow starts at the bottom edge and moves up . note that my diagram shows the top view of the solar system i.e. looking down on the north pole , so the top edge is the east edge and the bottom edge is the west edge .
i think it could be simply that the dirac operator is invariant under isometries , so if $\phi$ is an isometry and $\psi$ a solution to $$d\psi = 0 , $$ then $\phi^* \psi$ is also a solution , where $\phi^*$is pullback . then it would be similar to how harmonic functions $f$ on the sphere -- $\nabla^2 f = 0$ -- come in representations of the rotation group , the $y^l_m$ . in more detail if $\phi$ is a diffemorphism , that in coordinates takes the form $y^\mu = y^\mu ( x^\nu ) $ ( not a tensor expression ) , and $v^\mu$ is a vector field , then we can define a vector field $$ ( \phi_* v^\mu ) ( \phi ( p ) ) = \frac{\partial y^\mu}{\partial x^\nu} v^\nu ( p ) $$ called the pushforward of $v^\mu$ . naturally we can pushforward any tensor , in particular the metric . by definition $\phi$ is an isometry if $$ ( \phi_* g_{\mu\nu} ) ( \phi ( p ) ) = g_{\mu\nu} ( \phi ( p ) ) . $$ this means that if we have any tetrad ( also known as a vierbein or a frame ) , that is a set of vector fields $e_a^\mu$ such that $e_{a\mu} e^\mu_b = \eta_{ab}$ for some symmetric matrix $\eta_{ab}$ with signature $+---$ , it is pushed forward to another tetrad . i let $\eta_{ab}$ be general because in spinor problems it is more natural to use a null tetrad $$\eta_{ab} = \begin{pmatrix} 0 and 1 and 0 and 0 \\ 1 and 0 and 0 and 0 \\ 0 and 0 and 0 and -1 \\ 0 and 0 and -1 and 0\end{pmatrix} . $$ since $\eta_{ab}$ has zeros on the diagonal all the tetrad vectors are null . it is well known ( see for example spinors and space-time or the newman-penrose paper ) that to every null tetrad corresponds exactly two bases for two-spinors , called dyads , say $ ( o^a , \iota^a ) $ and $ ( -o^a , \iota^a ) $ . thus at least for isometries connected to the identity , the pushforward of tetrads lifts to a pushforward of dyads . ( however when the isometry group is not simply connected this might not be continuous globally , but i think it does not matter here , since we can consider isometries close to the identity , which will take us to lie algebra representations , and then we integrate them , and discard the representations that require passing to the simply connected cover . ) since we can pushforward dyads we can pushforward two-spinors ( by linearity ) , since we can pushforward two-spinors we can pushforward dirac spinors . $\newcommand{\dslash}{\ ! \not d}$ in particular for a dirac spinor $\psi$ , $\dslash\psi$ is of course also a dirac spinor , so $$\beta = \phi_* ( \dslash\psi ) = \phi_* ( \dslash \phi^* \phi_* \psi ) $$ makes sense , where $\phi^*$ as the inverse of $\phi_*$ so the second equality is just inserting the identity . now $\phi_* \dslash \phi^*$ defines a differential operator , it is the transformed dirac operator under the isometry $\phi$ . but since the dirac operator is defined by the metric and $\phi$ preserves the metric , this must be just the dirac operator again . ( you can probably make this argument more convincing . ) thus we have established that $$\beta = \dslash ( \phi_*\psi ) . $$ in particular if $\beta = 0$ , so that $\psi$ is a zero mode for the dirac operator , then $\tilde{\psi} = \phi_* \psi$ is also a solution . thus the isometry group ( or at least its lie algebra ) acts on zero modes .
however , there is no reason why $\phi_i$ should be a state of the system . how is that possible ? if you perform the measurement and find result $\lambda_i$ , with zero uncertainty in the measurement then indeed the state is now $\phi_i$ . you ask how this is possible . any state is possible , not just energy eigenstates . a few ways out occur to me : the state instantanously " evolves " to a new , valid state almost . the system does indeed evolve to a new state , namely $\phi_i$ , but that evolution is not instantaneous . it is very fast in some situations , but in others it can be quite slow . concretely , consider a particle in a 1-d box , and let ω be the momentum operator . then the state can be written as an " linear combination " ( in this case an integral combination ) but no state of definite momentum is a solution . if we imagine the walls of the box , the infinite potential barriers , to be finitely wide , then if 1 ) is the case , it looks like the measurement of the momentum could cause the particle to tunnel through an infinite barrier ( it would collapse to a state with uniform position probability , and evolve to a state that is nonzero on both sides of the barrier ) . the thing is , you can not build a system which measures the momentum to arbitrarily high precision in a particle-in-a-box . the boundary conditions of the system prevent that . the infinite wall potential is mathematically pathological ( the wave functions have discontinuous derivatives at the edge of the box ) . consider a less pathological system , such as a particle in a finite height box . in this system , it is possible in principle to measure the momentum to arbitrarily high accuracy . if you measure the momentum to be $p_0$ with an accuracy of $\sigma_p$ , then the resulting wave function in the $p$ basis will be a reasonably sharp wave packet : $$\propto \exp \left [ - ( p-p_0 ) ^2 / 2 \sigma_p^2 \right ] $$ in the position basis this is $$\propto \exp \left [ -x^2 / 2\sigma_x^2 \right ] $$ where $\sigma_x = 1/\sigma_p$ . as $\sigma_x$ is very large , the wave function in the $x$ basis is a very wide function . the crucial thing here is that you never ever measure anything to infinite accuracy , so the wave functions resulting from your measurement are not exact eigen-states of what you think is the measurement operator . this is not just " experimental dirtiness " . this is a fundamentally important aspect of qm which you should keep near your mental centre as you learn more .
i can´t fully come up with an explanation from more basic principles , but in the case you describe you will have kinetic friction . or at least that is what all engineering books say . . . there are a number of situations where this effect is clearly demonstrated : pulling a cork out of a bottle , using a basic corkscrew such as this : if you simply pull on the corkscrew , it is harder to pull it out than if you first get it rotating and then pull . while not entirely the same , a similar situation arises when a car rolling down a road with a strong side wind brakes and locks the wheels . while the wheels were rolling , there is no relative motion between the road and the tire , so there is static friction in effect , and unless the wind is really strong , as in a hurricane , the force will not overcome friction and the car will not skid sideways . once the brakes are locked , the car starts skidding forward , because the force due to the inertia of the car overcomes friction . once this happens , the car will also start skidding sideways , due to two factors : the lesser important is that the coefficient of friction in effect once there is relative motion is the kinetic , not the static one . the main effect is due to the fact that the direction of movement does not really matter at all : at the contact point you have a force due to inertia and a force due to the side wind , and once their combined magnitude exceeds friction , you will start having movement in the direction of that combined force , so forward but also to the side .
what i did wrong here was switching order of integration . let 's integrate from $0$ to $\beta'$ , preserving the variable $\beta=\frac s w$ i introduced in the op , and priming it to avoid confusion with integration variable . the rhs integral will then look as $$\int_0^{\beta'}\text{d}\beta\int_0^{\beta^2}\frac{\chi' ( x ) \text{d}x}{\sqrt{\beta^2-x}}=\mathcal i . $$ its domain of integration looks like : here we integrate over $x$ from $0$ to the curve , then over $\beta$ from $0$ to the top horizontal line . so , to switch order of integration , we should integrate from the curve to the top over $\beta$ , then over $x$ from left to right . so , we get : $$\mathcal i=\int_0^{x ( \beta' ) = ( \beta' ) ^2}\text{d}x\int_{\beta ( x ) =\sqrt x}^{\beta'}\frac{\chi' ( x ) \text{d}x}{\sqrt{\beta^2-x}}=\\ =\int_0^{ ( \beta' ) ^2}\text{d}x\chi' ( x ) \left . \left ( \cosh^{-1}\frac \beta {\sqrt x}\right ) \right|_\sqrt x^{\beta'}=\\ =\int_0^{ ( \beta' ) ^2}\text{d}x\chi' ( x ) \cosh^{-1}\frac{\beta'}{\sqrt x} . $$ after substitution of the previously introduced variables , the result in landau and lifshitz is easy to get .
feynman in multiple writings suggested thinking about " exchanging particles " in terms of exchanging them as they move through time . that is , they can either move in two parallel paths as they move forward , or they can cross paths ( exchange roles ) . the antisymmetric cancellation applies to the latter , but not to the former . now if you think that through , it means that the parallel path remains strong even as the crossover paths cancel out , resulting in the two particles avoiding each other and maintaining unique paths ( wave functions ) . the net result is not full cancellation , but cancellation at the edges , where the particles would cross . ( feynman goes into a lot more detail about rotations , but frankly that part can get you sidetracked a bit ; it is the " anti-crossover " part that counts in terms of actual outcomes . ) another consequence of identical fermions cancelling each other out is that packing more fermions into a tight space forces their space-filling wavelengths to become shorter also . since in quantum mechanics the spatial wavelength of a particle defines its momentum , particles that are squeezed in this fashion also get very , very hot . a neutron star is a good example . pauli exclusion -- the " constriction of space because crossover cancels but parallel does not " -- allows neutrons to pack together very densely indeed . there are limits , however . when gravity gets too monumental , even pauli exclusion is unable to keep up with the pace , and the entire star collapses , very quickly . thus is born a stellar-sized black hole , or at least this is one example of how one can form .
the acceleration of uniform circular motion is a very basic computation that we do for first year students . $$ a = \frac{v^2}{r} $$ which for someone standing on the earth 's equator comes to $$ a_\text{equator} \approx \frac{\left ( 465\text{ m/s}\right ) ^2}{6400\text{ km}} = 0.03\text{ m/s}^2$$ or less than 1% of g . that is a measurable quantity , but not very significant . indeed fluxuations of local $g$ at that level can ( and do ) occur simple due to local deposits of heavy ore . mining and oil companies use precise gravitation maps in surveys for exactly this kind of reason .
you will need to include a vertical and horizontal force at point c due to ladder ac ( since we should not assume a direction for the total force -- even though our intuition may prove to be correct ) . the vertical forces on ladder bc are then related by $f_v + n - w = 0$ and you have already established that $n = \frac{3w}{4}$ . there are now five forces acting on ladder bc , but if we look at the force moments about b , we can neglect the torque due to n and the static friction acting on the foot of the ladder . we can then sum the torques due to w , $f_v$ , and $f_h$ to zero . you should indeed find that the total force f at c ( using the pythagorean theorem ) comes to $\frac{w}{2}$ and the direction is 30º above the horizontal ( i find it pointing to the right ) .
first of all , thanks for this question because it made me think about relativity which was always fun ! it is true that $e'=\frac{1}{\gamma} e$ . you say that relativity states that the energy should increase by a factor of $\gamma$ . this is certainly true for a massive particle whose energy is $\gamma mc^2$ , but why would you expect this to hold for the energy in the fields in this situation ? i think the answer simply is that there is no contradiction ; the energy in the fields transforms by a factor of $\frac{1}{\gamma}$ and that is that ! actually , not quite ! ( as mark argued in the comments ) after the discussion in the comments below , i realized that perhaps " that is that " was both premature and does not get at the heart of mark 's question . so i dug deeper ( namely i scoured jackson 's em ) and i found an answer that is significantly more complete . the definition of the energy and momentum densities in the fields given by the $\theta^{00}$ and $\theta^{0i}$ components of the ( symmetric-traceless version of the ) stress tensor ( see jackson 12.114 ) $$ \theta^{00} = \frac{1}{8\pi} ( \mathbf e^2+\mathbf b^2 ) , \qquad \theta^{0i} = \frac{1}{4\pi} ( \mathbf e\times\mathbf b ) ^i $$ leads to the following candidate for the electromagnetic four-momentum : $$ p_\mathrm{cand}^\mu=\left ( \int d^3 x\ , \theta^{00} , \int d^3x\ , \theta^{0i}\right ) $$ unfortunately , this quantity does not transform as a four-vector should in the presence of sources . the basic reason this is that $$ \partial_\alpha\theta^{\alpha\beta} = -f^{\beta\lambda}j_\lambda/c \neq 0 $$ and the spatial integrals of $\theta^{0\alpha}$ yield a four-vector only if the four-divergence of the tensor vanishes identically . to remedy this one needs to add a term $p^{\mu\nu}$ to the stress tensor that takes into account the so-called poincare stresses of the sources ; $$ s^{\mu\nu} = \theta^{\mu\nu} + p^{\mu\nu} $$ this new tensor does have vanishing four-divergence provided the poincare stresses are chosen appropriately for the system at hand , and therefore the spatial integrals of the $s^{0\mu}$ are the components of a four-vector . jackson indicates that the poincare stresses should be thought of as the contributions to the energy of the system that come from the non-electromagnetic forces necessary to ensure the stability of electric charges . from this vantage point , the answer to the question is that the extra energy that seems to go missing is the energy present in the sources . perhaps this is begging the question in the sense that i have nowhere attempted to write down the poincare stresses present in the parallel plate capacitor system , but for the time being , i am more satisfied , and hopefully , mark , you are too . btw see ch . 16 in jackson for many more details including the explicit calculation of poincare stresses for a charged shell of uniform density . cheers !
i know that this does not directly answer your question about purcell 's reasoning ( see addendum i wrote after reading purcell 's argument ) , but here 's how a uniqueness proof would go . suppose that two fields $\mathbf b_1$ and $\mathbf b_2$ both satisfy the magnetostatics equations $$ \nabla\times\mathbf b_i = \mu_0\mathbf j , \qquad \nabla\cdot\mathbf b_i = 0 , \qquad i = 1,2 $$ let $$ \mathbf d= \mathbf b_2 - \mathbf b_1 $$ then the curl and divergence conditions give $$ \nabla\times \mathbf d= 0 , \qquad \nabla\cdot\mathbf d = 0 $$ now your question reduces to what we can say about the vector field $\mathbf d$ . take the curl of both sides of the first equation and use the following vector calculus identity : $$ ( \nabla\times ( \nabla\times \mathbf d ) ) _i = \partial_i ( \nabla\cdot \mathbf d ) - \nabla^2d_i $$ along with the zero divergence condition , to obtain $$ \nabla^2 d_i = 0 $$ in other words , each component $d_i$ satisfies laplace 's equation . now , if we assume that the magnetic field components vanish at infinity ( as would be the case for a bounded current distribution ) then we recall that the only solution to laplace 's equation that vanishes at infinity is the zero solution . this gives $\mathbf d = 0$ and thus $\mathbf b_1 = \mathbf b_2$ . addendum . after having read purcell 's argument , i am fairly confident that he also is making the physical assumption that bounded current distributions produce fields that vanish at infinity . such a boundary condition cannot be derived from the equations themselves . purcell is rather vague when he specifies this boundary condition ; what he says before the argument is " we do not consider sources that are infinitely remote and infinitely strong . "
here are ray diagrams that show what is going on . in the top case , a weak ( thin ) lens does not have the power to pull the rays together tight enough . an object farther away than the tree would make rays converge on the retina . this is farsightedness . remember the fundamental formula for thin lenses ( using some appropriate sign convention ) : $$ {1\over f}={1\over d_1}+{1\over d_2} $$ if $d_1$ increases , then $d_2$ must decrease . thus rays of farther away objects converge in a shorter distance after the lens , hopefully on the retina . in the second diagram , the lens is of proper thickness , and each part of the tree and nearby paraphernalia focus on the retina . just for comparison , the bottom diagram shows a lens too strong ( thick ) causing rays to converge too quick , before reaching the retina . to meet at the retina , a slightly longer distance , the outside world object must decrease its distance - this is nearsightedness . note that in all cases , the tree image on the retina is upside down . the only difference is if it is sharp or blurry .
the only force which works is gravity$^1$ . so , change in gravitational potential energy equals final kinetic energy ( assume initial is zero ) . $$mgh=mv^2/2$$ $$v=\sqrt{2gh}$$ here $h$ is vertical height traversed . see the velocity does not depend on angle of string , mass of body too . . let 's see the kinematics of body . the length of string is $h cosec\theta$ ( $\theta $ being angle with horizontal assumed $\pi/6$ ) acceleration of body along the string=$g\sin\theta$ now $\text{using} : v^2=u^2+2as$ $$v^2=0+2\times h cosec\theta\times g \sin\theta$$ $$v=\sqrt{2gh}$$ working in differentials for $v$ along the rope . $$dv/dt=v\dfrac{dv}{dx}=a$$ $$\int_0^{v_f} v . dv=\int_0^{hcosec\theta} a . dx=ax\bigg|_0^{hsosec\theta}$$ $$\dfrac{v_f^2}2=gsin\theta . hcosec\theta \ \ ; \ \ a=gsin\theta$$ $1 ) $assuming the pulley being used to slide to be friction less . though not possible . also the rope is assumed to be in-extensible and straight .
the up quark has a charge of $+2/3$ , the down has a charge of $-1/3$ . if you have a bound state of charged particles , the total charge is just the charge of the elementary constituents . the neutron consists of one up quark and two down quarks , so the total charge $q$ is : $$q = 2/3 + 2 \times ( -1/3 ) = 0$$
yes it can be seen , given that you do not get much noise and that your sensor is sensitive enough for it to be able to detect the signal . as you have not specified anything about the light not much can be concluded , more than that it depends on the sensor , the transmitter and the noise from everything else .
both " perfectly open " ( zero acoustic impedance ) and " perfectly closed " ( infinite acoustic impedance ) boundary conditions are only idealizations that never occur in practice . for the case of the human vocal tract , they are not even very good approximations . the " bottom end " of the resonating cavity is not , in fact , the lungs , but the vocal folds ( as georg pointed out ) . this end has some acoustic impedance that is neither extremely low nor extremely high . i am sure the impedance also changes somewhat with the pitch and volume of the phonation . the " top end " of the cavity is of course the mouth , and its impedance changes with the vowel sound you are pronouncing . for aptly-named " open " vowels such as " ah " , " oh " , and so on , the impedance is actually low enough that it might be a good approximation to say it is perfectly open . but for closed vowels ( "ee " , " oo " , . . . ) and especially for humming with closed lips , the acoustic impedance is much higher ( but still far from infinite ) .
the decay rate is $$\left|\frac{\mathrm{d}n}{\mathrm{d}t}\right| = \lambda n . $$ half-life is $$\tau_{1/2}=\frac{\ln 2}{\lambda} . $$ i think you can figure it now .
this question is very difficult to answer , and in the end , the answer is going to be more semantic than it is going to be physcial . the reason for this is that it is very difficult to pull apart what is done by " gravity " and what is done by the matter content of the universe . the safest answer to this is to say that during inflation , the matter content of the universe has a predominantly negative pressure , which causes objects to expand . during a truly inflationary period , the density of this substance does not decrease as the universe expands , so the rate of expansion is approximately exponential . there are various ways to create a scenario like this if you assume different properties for the quantum field theoretic vacuum or various types of matter coupled to gravity .
when no current is flowing , the system is in thermal equilibrium . the electrons do transfer kinetic energy to the atoms through collisions , but the atoms also transfer kinetic energy to the electrons , and these two processes happen at the same rate , so there is no net energy transfer and the system neither heats up nor cools down . this is just the same as any other case of thermal equilibrium : effectively , the electrons and the atoms are at the same temperature , and that is why there is no heat flow . however , when you switch the voltage on there is an electric current accelerating the electrons , which increases their kinetic energy . now they have , on average , more kinetic energy to give to the atoms than the atoms have to give to them . this means that there is a net transfer of energy from the electrons to the atoms . moving an electron in an electric field changes its potential energy , and this is where the energy for the heating ultimately comes from .
as mark wrote , your reasoning is ( almost ) correct and your friend 's is not . here 's the sort of explanation i use when i am teaching this topic : in order to apply newton 's second law , you need to choose one object to apply it to . ignore everything else except the object you choose and the forces that act on it . in your case , the problem asks about a force exerted on the bottom mass , so you should write out newton 's second law for the bottom mass . there are exactly three quantities that go into the equation : mass of the object this is given in the problem . ( typically the mass is given . ) acceleration of the object this is also given in the problem . ( in most cases , it is either given or it is what you will be solving for . ) when you are determining acceleration , only consider how the object is changing its velocity . gravity is irrelevant , the value and direction of velocity are irrelevant , whether the object is slowing down or speeding up or curving is irrelevant . the problem tells you that the elevator is accelerating at $5\ \mathrm{m/s^2}$ upward , so the only thing you need to assume is that the mass is moving along with the elevator , and that means the acceleration of the mass is $a = 5\ \mathrm{m/s^2}$ ( assuming positive values are upward ) .netforce on the object this is usually the part of newton 's law that takes a bit of thought . you have to enumerate each of the forces acting on the object and include the correct term for each one . in this case , there are two forces acting on the object ( which is the lowest mass ) : the force exerted by the spring above , and gravity . the spring force goes up and gravity goes down , so you would write the net force as $f_s - f_g$ , or $f_s - mg$ once you plug in the value of the gravitational force . note that each of these terms represents a force on the object . $f_s$ is the force that the spring exerts on the mass ; $f_g$ is the force that the earth exerts on the mass . the problem asks for the force that the spring exerts on the mass , so you do not need to invoke newton 's third law in this case . once you have all these quantities , you can put them into the equation $$\sum f = ma$$ and solve for whatever you need to .
it depends on what do you mean when you say effective . you are absolutely right when you say that the only way for your oven to cool is by diffusing and radiating its heat to your house , regardless of whether its door is open or not . so the total heat energy transferred from your oven to your room is fixed . but when the door is open , the power , or energy per unit time , is larger . the word " efficiency " is usually used to denote the amount of energy produced divided by the amount invested , and is not appropriate in this context .
hints : 1 ) the key sentence is the volume charge density $\rho$ [ . . . ] does increase with distance from the sphere center . 2 ) from gauss ' law in integral form $\phi_e=\frac{q}{\epsilon}$ , one gets $$\tag{1} 4\pi r^2 \cdot e ( r ) ~=~ \frac{1}{\epsilon}\int_0^r\ ! 4\pi r^{\prime 2} dr^{\prime} ~ \rho ( r^{\prime} ) . $$ 3 ) to get the idea , say for simplicity that the increase is linear $$\tag{2} \rho ( r ) ~\propto~r\qquad \text{for}\qquad r~\leq~ r . $$ 4 ) use eqs . ( 1 ) and ( 2 ) to prove that then the electric field increases quadratically $$\tag{3} e ( r ) ~\propto~r^2\qquad \text{for}\qquad r~\leq~ r . $$ 5 ) what happens if $\rho ( r ) =ar^{\alpha}$ is a power law of $r$ ?
if you use tight-binding hamiltonian , it is reasonable to start not from semiclassical , but one-particle approximation . in that case , you have an amplitude ( complex number ) at each site , the state is complex vector of length $n$ , hamiltonian is $n\times n$ ( sparse ) matrix and the problem of time evolution and/or eigenstates ( for one particle state ) is solvable for relatively large lattices . if you are interested in many particle physics , you may build a model on top of these oneparticle states . the details are dependent on what exactly you wish to compute . unfortunately , i do not know a reference with rigorous transfer from one formulation to another .
according to quantum mechanics , the particles - whether it is the first particle or the second particle or any other particle in the universe - refuse to have any well-defined state or property prior to the measurement . the only " kind of " exception is the case - relevant for a maximally entangled scenario - in which we are interested in a property of the second particle - or any other particle - that is predicted to take a particular value with probability equal to 100 percent . of course , if the probability is 100 percent , then you can be sure what the measured property will be , and you may assume that this value of the property exists even before the measurement . however , for the very same particle that has some value of the quantity equal to something at 100 percent , there still inevitably exist other observables that are not known . ( just design a hermitian observable with random off-diagonal matrix elements . ) in the basis of eigenstates of those other properties , the probability amplitudes are generic , and some of the options have probabilities that differ from 0 percent as well as 100 percent . for those quantities - and it is a majority of observables - the usual prescriptions of quantum mechanics hold : the value is not determined prior to the measurement . it is not just unknown to the physicists : it is unknown to nature . a picture in which those observables are determined leads to wrong predictions and contradictions .
there is indeed a scalar field model of gravity , in fact einstein originally tried that before settling on a spin 2 description . scalar gravity is called einstein-nordstrom gravity , here is a link to wikipedia : http://en.wikipedia.org/wiki/nordstr%c3%b6m%27s_theory_of_gravitation. at the nonlinear level it amounts to using $r$ in einsteins equations instead of $g_{\mu\nu}$ . what you wrote down was indeed guessed . the problem is that $\rho$ actually is not relativistically invariant--energy and momentum get mixed under boosts--so you really need to use $t$ , the trace of the stress energy tensor . you also need to have the gravitaional sector to have nonlinear interactions , because gravity carries energy and so it couples to itself . so you can generalize what you wrote , that is the einstein-nordstrom theory . while scalar gravity does reproduce the newtonian limit , the newtonian limit is easy to get . the problems all amount to the fact that the graviton is a spin 2 particle , not a spin 0 particle . for example , scalar gravity cannot couple to light . this is because a scalar ( spin-0 ) can only couple to the trace of the stress energy tensor $t$ , but maxwell 's equations are famously conformally invariant at the classical level and so $t=0$ . this violates einstein 's equivalence principle ( which one reason why einstein would not have liked it ) . it also is empirically ruled out ( which is a great reason for us to rule it out , though einstein did not have those experiments when he was developing gr ) . scalar gravity also has completely different properties for gravitational waves : it has one helicity 0 polarization instead of 2 helicity 2 ones . this would change the output of radiation from a binary pulsar system , for example . another consequence is that birkhoff 's theorem is no longer true . a scalar mode can be sensitive to overall changes of scale in an object--a spherically symmetric object with time varying radius $r ( t ) $ will radiate in scalar gravity , but will definitely not radiate in gr .
when we say that the spin of a silver atom ( in a magnetic field ) is $+1/2$ or $-1/2$ we mean its component in the direction of the magnetic field ( referred to as $s_z$ ) is $+1/2$ or $-1/2$ . the magnitude of the spin is the same in both cases , it is just the direction that is different . hund 's rule just tells you what the magnitude of the total spin is , and does not say anything about the direction that spin is pointing . for example with two unpaired electrons hund 's rule tells us the total spin will be $s = 1$ . however put that atom in a magnetic field and you can have $s_z = 1$ , $0$ or $-1$ . the two blobs in the stern-gerlach experiment correspond to the electrons with $s_z = +1/2$ and $-1/2$ , but all the electrons have the same total spin $s = 1/2$ . the two unpaired electron system with $s = 1$ would give us three blobs corresponding to $s_z = 1$ , $0$ and $-1$ .
contrary to what queueoverflow says , you do not actually need to perform any integration here ; a pretty cool symmetry argument will give you the answer . let the cube we are considering in the problem have side length $\ell$ . the trick is to consider putting the charge at the center of an imaginary cube of side length $2\ell$ . the flux through the surface of this cube is just $q/\epsilon_0$ by gauss 's law since it is a closed surface containing the charge . now imagine dividing each face of this larger cube into four squares of side length $\ell$ . by symmetry , the flux through each of these squares is the same , but there are a total of 24 such squares since there are six faces , so the flux through each of these squares is $q/ ( 24\epsilon_0$ ) . now , simply notice that if we were to cut the larger cube into eight cubes of side length $\ell$ , then each of these squares mentioned in the last paragraph would be a face of one of these cubes at which the charge is at a corner . qed . regarding your confusion . notice that a bunch of the flux coming from the charge does not even pass through the faces of the cube when the charge is at its corner . only one eighth of its total flux goes through the faces of the cube as illustrated by the argument above . then , each of the three faces opposite the charge get one third of this flux because the other three faces next to the charge are parallel to the electric field , so there is no flux through them .
the answer to a slightly different question—what is the $\langle\sigma_{\mu} \sigma_{\mu}\rangle$—is 4 since the pauli 's independently square to the identity . edit : this is incorrect . the answer should be 2 since the $\mu = 0$ term has a minus sign attached to it , as @jeffdror pointed out in the comments . onto your question . let 's start off by asking what is $\langle\sigma_{\mu}\rangle$ in the state $\left|n\right\rangle$ ( my notation means that the spin is aligned along the unit vector n ) . well , let 's say $\mu = 3$—then if the spin makes an angle $\theta$ with the z-axis the expectation of $\sigma_3$ will be $\cos ( \theta ) $ , just the projection of the spin onto the z direction . similarly , $\sigma_1$ will have expectation $\sin ( \theta ) \cos ( \phi ) $ , where $\phi$ is the azimuthal angle , and $\sigma_2$ will have expectation $\sin ( \theta ) \sin ( \phi ) $ ( this can all be proved explicitly using the fact that $\left|n\right\rangle = ( \cos ( \theta/2 ) , e^{i\phi}\sin ( \theta/2 ) ) $ ) . in any event , it is clear that $\langle\sigma_{\mu}\rangle\langle\sigma_{\mu}\rangle = 1$ . edit : as @jeffdror pointed out , i neglected the $\mu = 0$ term . this trivially has expectation $\langle\sigma_0\rangle = \langle\sigma_0\rangle\langle\sigma_0\rangle = 1$ . however , there is a minus sign built into the minkowski metric with which the sum over $\mu$ is being performed , so this term cancels the positive one contribution from the $\mu = 1,2,3$ components , and the total sum is indeed zero .
as mentioned in the comments , to find all possible terms we normally only consider local , gauge invariant , lorentz invariant interactions . there are in fact an infinite number of these . this is easiest understood using the lagrangian . the gauge invariant field stength tensor is given by \begin{equation} f _{ \mu \nu } = \partial _\mu a _\nu - \partial _\nu a _\mu \end{equation} the only other tensors with lorentz indices are \begin{equation} \epsilon _{ \alpha \beta \gamma . . . } \quad , \quad g _{ \mu \nu } \end{equation} to lowest order in $ f $ the only non-zero invariants are : \begin{equation} f _{ \mu \nu } f ^{ \mu \nu} \quad , \quad \epsilon _{ \alpha \beta \gamma \delta } f ^{ \gamma \delta } f ^{ \alpha \beta } \end{equation} if we restrict ourselves to terms with mass dimension of $4$ or lower these are the only options ( these terms are called renormalizable terms ) . however , one can also write down other invariants which have higher mass dimensions . one such example is the mass dimension six term , \begin{equation} \partial ^\mu f _{ \mu \nu } \partial ^\alpha f _\alpha ^{ \ , \ , \nu } \end{equation} such terms are small at low energies and are often ignored . in general there are an infinite number of allowed ( non-renormalizable ) terms in the lagrangian . though it may not be trivial , such terms could be written in terms of the electric and magnetic fields to find the different combinations of $\bf e$ and $\bf b$ that form lorentz invariants .
$pt- , t- , p-$ transformations refer to subgroup of discrete transformations of the lorentz group . they transform connected components of the lorentz group between each other ( $pt$ transformation transforms $l^{\uparrow}_{+}$ representation to $l^{\downarrow}_{+}$ ) . in general , they can not be represented as the special case of rotation , which refer to subgroup of continuous transformations . you can not get some other connected component from orthochronous group by acting of any continuous transtormation 's matrix on your representation . so , by nature , it can not be rotation .
the first thing you need to get to grips with is that particles are waves . this can be shown with a simple experiment called the double slit experiment , which i will attempt to explain . imagine a water wave travelling across a tank . then imagine you place a wall in the middle of the tank , and place two thin slits in it . if you create a wave ( by dropping a stone etc ) on one side of the wall , it will travel through the two slits and interfere like this . the double slit experiment does the same thing , but for light . if you have a wall with two slits in it and shine a beam of light through the slits onto a flat screen behind , you can see a similar interference pattern on the screen . this shows that light acts as a wave . now imagine that rather than a beam of light you can create a steady stream of electrons . electrons are a small " fundamental " particle ( "fundamental " means they cannot be broken down into smaller components ) . if you point your electron stream at your two slits you will see a very similar interference pattern as before ! until this experiment was done it was believed that electrons were solid particles ( like billiard balls ) , but this showed that they also act as a wave ! since we have shown that particles can also show wave-like properties , can we show that waves can have particle-like properties ? it was shown by einstein and arthur compton that light can in fact be shown to be made up of particles , due to the fact that light must have momentum . this is known as wave-particle duality . as i said at the beginning , waves and particles are the same thing . there are some " waves " like electromagnetic waves which make particles move . these are only called " waves " because it is easier to model and calculate that way . it is possible to describe the interaction as 2 ( or more ) particles ( but it is considerably more difficult ) . i hope this answers your question .
i ) first of all , one should never use the dirac bra-ket notation ( in its ultimate version where an operator acts to the right on kets and to the left on bras ) to consider the definition of adjointness , since the notation was designed to make the adjointness property look like a mathematical triviality , which it is not . see also this phys . se post . ii ) op 's question ( v1 ) about the existence of the adjoint of an antilinear operator is an interesting mathematical question , which is rarely treated in textbooks , because they usually start by assuming that operators are $\mathbb{c}$-linear . iii ) let us next recall the mathematical definition of the adjoint of a linear operator . let there be a hilbert space $h$ over a field $\mathbb{f}$ , which in principle could be either real or complex numbers , $\mathbb{f}=\mathbb{r}$ or $\mathbb{f}=\mathbb{c}$ . of course in quantum mechanics , $\mathbb{f}=\mathbb{c}$ . in the complex case , we will use the standard physicist 's convention that the inner product/sequilinear form $\langle \cdot | \cdot \rangle$ is conjugated $\mathbb{c}$-linear in the first entry , and $\mathbb{c}$-linear in the second entry . recall riesz ' representation theorem : for each continuous $\mathbb{f}$-linear functional $f : h \to \mathbb{f}$ there exists a unique vector $u\in h$ such that $$\tag{1} f ( \cdot ) ~=~\langle u | \cdot \rangle . $$ let $a:h\to h$ be a continuous$^1$ $\mathbb{f}$-linear operator . let $v\in h$ be a vector . consider the continuous $\mathbb{f}$-linear functional $$\tag{2} f ( \cdot ) ~=~\langle v | a ( \cdot ) \rangle . $$ the value $a^{\dagger}v\in h$ of the adjoint operator $a^{\dagger}$ at the vector $v\in h$ is by definition the unique vector $u\in h$ , guaranteed by riesz ' representation theorem , such that $$\tag{3} f ( \cdot ) ~=~\langle u | \cdot \rangle . $$ in other words , $$\tag{4} \langle a^{\dagger}v | w \rangle~=~\langle u | w \rangle~=~f ( w ) =\langle v | aw \rangle . $$ it is straightforward to check that the adjoint operator $a^{\dagger}:h\to h$ defined this way becomes an $\mathbb{f}$-linear operator as well . iv ) finally , let us return to op 's question and consider the definition of the adjoint of an antilinear operator . the definition will rely on the complex version of riesz ' representation theorem . let $h$ be given a complex hilbert space , and let $a:h\to h$ be an antilinear continuous operator . in this case , the above equations ( 2 ) and ( 4 ) should be replaced with $$\tag{2'} f ( \cdot ) ~=~\overline{\langle v | a ( \cdot ) \rangle} , $$ and $$\tag{4'} \langle a^{\dagger}v | w \rangle~=~\langle u | w \rangle~=~f ( w ) =\overline{\langle v | aw \rangle} , $$ respectively . note that $f$ is a $\mathbb{c}$-linear functional . it is straightforward to check that the adjoint operator $a^{\dagger}:h\to h$ defined this way becomes an antilinear operator as well . -- $^{1}$we will ignore subtleties with discontinuous/unbounded operators , domains , selfadjoint extensions , etc . , in this answer .
you may just not bother to use a test function , here . this problem is so easy you can work it all just using the properties of the commutator . $$ [ xp_y , x ] =x [ p_y , x ] + [ x , x ] p_y$$ now $ [ p_y , x ] $ vanishes because of the fundamental commutation relation between $p_i$ and $x_i$ which is $$ [ p_i , x_j ] = -i\hbar \delta_{ij}$$ on the other hand $ [ x , x ] =0$ because anything commmutes with itself .
no . you can add an arbitrary constant shift ( or an arbitrary operator commuting with $x$ ) without affecting the ccr . for 1-dimensional qm , the general solution of the ccr with $\hat x$ represented as multiplication by $x$ on wave functions with argument $x$ is $\hat p=\hat p_0-a ( \hat x ) ~~$ , where $\hat p_0$ is the canonical momentum operator , and $a ( x ) $ is an arbitrary function of $x$ . proof . the difference $\hat a:=\hat p_0-\hat p~$ commutes with $\hat x$ , hence is a function of $\hat x$ .
assuming 1 , you live somewhere that is colder outside than in 2 , the curtain has finite thermal resistance ( ie some insulating value ) 3 , the curtain is close enough to the window to reduce convection then yes . try measuring the air temperature on the window side of the curtain , it should be lower than the room .
analytically solving an integral is sometimes a strange art , and " guessing " the correct change of variables can help a lot . here , the idea is to introduce $\theta$ such that $y=a ( 1-\cos\theta ) $ or , equivalently , defining $\theta=\arccos ( 1-y/a ) $ . how $\theta$ was guessed ? it is difficult to tell , but it happens to simplify the last integral . under this change of variable , we have $$dy=d ( a ( 1-\cos\theta ) ) =a\sin\theta d\theta=a\sqrt{1-\cos^2\theta}d\theta , $$ so the integral becomes $$x = \int \sqrt{\frac{y}{2a-y}} \ , dy =\int a\sqrt{\frac{ ( 1-\cos\theta ) ( 1-\cos^2\theta ) }{1+\cos\theta}}d\theta$$ which is simpler to integrate .
we start by mentioning a couple of standard formulas $$\tag{1} \psi ( x ) ~=~\langle x | \psi \rangle , $$ and $$\tag{2} \langle x | y \rangle ~=~\delta ( x-y ) . $$ the canonical commutation relation ( ccr ) is $$\tag{3} [ \hat{x} , \hat{p} ] ~=~i\hbar{\bf 1} . $$ the standard schrödinger position representation reads $$\tag{4}\hat{x}~=~x , \qquad \hat{p}~=~-i\hbar\frac{\partial}{\partial x} . $$ we may conjugate the standard schrödinger position representation ( 4 ) by an unitary operator $\hat{u}=e^{-if ( \hat{x} ) }$ , where $f:\mathbb{r}\to\mathbb{r}$ is a given differentiable function . in this way we obtain an unitary equivalent position representation $$\tag{5}\hat{x}~=~x , \qquad \hat{p} ~=~-i\hbar e^{-if ( x ) }\frac{\partial}{\partial x}e^{if ( x ) } ~=~-i\hbar\frac{\partial}{\partial x}+ \hbar f^{\prime} ( x ) , $$ of the ccr ( 3 ) . the standard schrödinger position representation ( 4 ) corresponds to $f\equiv {\rm const}$ . for a general irreducible representation of the ccr ( 3 ) , see the stone-von neumann theorem . the representation ( 5 ) implies $$\tag{6} \langle x | \hat{p} |\psi \rangle~=~ ( \hat {p} \psi ) ( x ) ~=~-i\hbar e^{-if ( x ) } ( e^{if}\psi ) ^{\prime} ( x ) ~=~-i\hbar\psi^{\prime} ( x ) + \hbar f^{\prime} ( x ) \psi ( x ) . $$ from ( 6 ) we conclude that the momentum matrix elements reads $$\tag{7} \langle x | \hat{p} |y \rangle~=~-i\hbar\delta^{\prime} ( x-y ) + \hbar f^{\prime} ( x ) \delta ( x-y ) $$ in the representation ( 5 ) . finally , here and here are two other phys . se posts that also discuss ambiguities in $x\leftrightarrow p$ overlaps .
there is a mistake in your diagram , in that you drew the x-axis for body one in the rest frame of body 3 as perpendicular in the euclidean sense to the t-axis of body one . the two lines are not euclidean perpendicular , but minkowski perpendicular . the t axis is correct for body 1 in the second diagram , but the x-axis for body 1 slopes up , not down . it slopes up by the same amount as the t-axis for body 1 slopes to the right . this looks awkward in a euclidean geometry diagram , but it is correct minkowski geometry . the direction of the slope is fixed by einstein 's argument about simultaneity at a distance , reproduced in this answer : einstein&#39 ; s postulates &lt ; ==&gt ; minkowski space . ( in layman&#39 ; s terms ) . this argument also fixes the relative non-simultaneity of e1 and e2 . when you use the correct notion of perpendicularity , you will find that t ( e2 ) is bigger than t ( e1 ) , not smaller . but they are indeed non-simultaneous , as you note .
let 's look at the relationship between momentum and energy . as you know , for a mass $m$ kinetic energy is $\frac12mv^2$ and momentum is $mv$ - in other words energy is $\frac{p^2}{2m}$ now to counter the force of gravity we need to transfer momentum to the air : $f\delta t = \delta ( mv ) $ the same momentum can be achieved with a large mass , low velocity as with small mass , high velocity . but while the momentum of these two is the same , the energy is not . and therein lies the rub . a large wing can " move a lot of air a little bit " - meaning less kinetic energy is imparted to the air . this means it is a more efficient way to stay in the air . this is also the reason that long thin wings are more efficient : they " lightly touch a lot of air " , moving none of it very much . trying to replicate this efficiency with an engine is very hard : you need compressors for it to work at all ( so you can mix air with fuel and have the thrust come out the back ) and this means you will have a small volume of high velocity gas to develop thrust . that means a lot of energy is carried away by the gas . think about the noise of an engine - that is mostly that high velocity gas . now think of a glider : why is it so silent ? because a lot of air moves very gently . i tried to stay away from the math but hope the principle is clear from this .
yes . provided you are only interested in the direction of the acceleration , and not it is magnitude . and further assuming your time samples are equally spaced , you can take the second derivative of the path and this will be proportional to the acceleration . a decent method in practice would be to use a second order central finite difference scheme wherein you say that : $$ a_x ( t ) = x ( t-1 ) - 2x ( t ) + x ( t+1 ) $$ and $$ a_y ( t ) = y ( t-1 ) - 2y ( t ) + y ( t+1 ) $$ this will give you decent estimates for the cartesian components of acceleration at every time , caveat to an overall scaling in magnitude that you will not know without knowing the actual timing , but the direction should be alright .
one of the main ways black holes are noticed is by looking at a solar system where the star appears to move as though it were a binary star system ( e . i . two stars ) when only one is seen . in these situations , depending on the distances , the black hole " feeds " off the original star , and a stream of the stellar plasma is slowly pealed off the star into the black hole . this matter can sometimes form a very vivid accretion disk , that can be observed using telescopes ( see herbig–haro object ) . this process can take a very long time , on the order of millions of years . however , of course , a rough black hole could enter a star system head on and collide right with the sun and " suck it up , " which would happen rather quickly ( to an outside observer ) .
photon frequency and wavelength are the same as the corresponding classical mode . when you quantize the electromagnetic field , you first treat the spatial dependence by decomposing the field into normal modes , which is a generalization of treating fields of the form $$e ( t ) =e_0 ( t ) \sin ( kx ) , $$ like you had find in a conducting box of length $l$ such that $k=\pi n/l$ for some integer $n$ . this is really no more than kinematics , or a restatetement of the problem if you will . the true dynamics of the field are then encoded in the temporal dependence . this dependence is through $e_0$ , which obeys maxwell 's equations in the form $$\frac{d^2e_0}{dt^2}+\omega^2 e_0=0$$ for $\omega=ck$ . this equation describes a harmonic oscillator , and quantum mechanics says that harmonic-oscillator systems can only have a discrete set of possible energies , with an even spacing of $\hbar\omega$ between them . if the state of the field is such that there is , whenever you look , only one excitation present , then we say the field is in a single-mode , single photon state . this photon then has a well-defined frequency ( $\nu=\omega/2\pi$ ) and wavelength ( $\lambda=2\pi/k$ ) . there can then be another additional complication . in any given box , and more so in free space , there will always be more than one mode present . there is then the possibility that there be exactly one photon in the system , but that we do not know in which mode it is in - the photon is in a schrödinger 's-cat state . usually the photon will be concentrated in modes within some bandwidth $\delta\omega$ around some central frequency $\omega_0$ , which means that the photon frequency is uncertain to some extent . the remarkable thing is that the electric field will then be nonzero in some spatially and temporally localized region : it will pass any given point in time of order $1/\delta\omega$ and thus have a width of order $c/\delta\omega$ . it is also important to realize that you can get this same effect with a classical field : a localized wavepacket will have a corresponding spread in its frequency and wavelength - exactly the same as it is hard to tell the pitch of short notes . the difference between a one-photon state and a weak classical field is a statistical one : for the same mean intensity , a one-photon wavepacket will always give one count on a photo-multiplier tube or avalanche photodiode , whereas a classical field will sometimes give none , sometimes several . this seems like a small difference but it makes things like single-photon interferometry possible .
m-theory compactified on a 2-torus is the same as m-theory compactified on a circle and then compactified on another circle because $t^2=s^1\times s^1$ . m-theory compactified on a circle is type iia string theory with $g_s$ being an increasing power of the radius of the compactified dimension . and if type iia is compactified on a circle of a small radius , we get type iib string theory via t-duality . when we connect the m-theory/iia duality and the iia/iib t-duality , we get the $f=ma$ relationship between m-theory and type iib you mentioned . one may avoid the type iia intermediate step , too . m-theory on a two-torus is naively a 9-dimensional theory ( the supergravity approximation would lead us to this belief ) . however , m-theory contains m2-branes , two-dimensional objects , and both of their spatial dimensions may be wrapped on the 2-torus . this produces point-like objects in the remaining 9 large dimensions . these objects are light when $a\to 0$ and they also have bound states of $n$ objects . so one obtains a continuum of new states in the $a\to 0$ limit and they may be reinterpreted as the momentum modes with respect to a new , " emergent " , 10th spacetime dimension of the resulting type iib string theory .
fluid dynamics problems such as this are generally best approached by control volume analysis . consideration of conservation of mass , momentum , energy , and sometimes angular momentum for an isolated control volume system generally provide an engineering answer . to figure out the force exerted on the pipe by the fluid it would seem appealing to isolate the fluid with a control volume . however , since the pipe is stationary , it is entirely equivalent to find the force acting on the pipe externally to keep it in place . assuming steady flow within the pipe , conservation of mass for an isolated pipe-fluid control volume gives $$\dot{m}_{in}a_{in}=\dot{m}_{out}a_{out}$$ with $\dot{m}_{in}=\overline{\rho_{in} \vec{u}_{in}\cdot\vec{n}_{in}}$ and $\dot{m}_{out} = \overline{\rho_{out} \vec{u}_{out}\cdot\vec{n}_{out}}$ where $\vec{n}$ are surface unit normal vectors and $\vec{u}$ is the surface velocity vector and $a$ is the pipe cross section area . conservation of momentum in vector form yields $$\sigma f = \vec{f}_{pipe}+\vec{f}_{gravity}+\vec{f}_{other}+\overline{p}_{in}a_{in}\vec{n}_{in}+\overline{p}_{out}a_{out}\vec{n}_{out} = \dot{m}\overline{\vec{u}_{in}}a_{in}-\dot{m}\overline{\vec{u}_{out}}a_{out}$$ if you further assume that the only force acting on the pipe is to resist the change in momentum of the fluid and that the pressure drop from entrance to exit is negligible , then you will arrive at the approximation $$\vec{f}_{pipe}= \dot{m}\overline{\vec{u}_{in}}a_{in}-\dot{m}\overline{\vec{u}_{out}}a_{out}$$
the problem is that the magnetic field around a bar magnet are not uniform , so different parts of your coil will experience different values of $b$ . if the magnetic is small enough , you can approximate it is field by a dipole , $\mathbf{b} ( {\mathbf{r}} ) =\frac{\mu_{0}}{4\pi}\left ( \frac{3\mathbf{r} ( \mathbf{m}\cdot\mathbf{r} ) }{r^{5}}-\frac{{\mathbf{m}}}{r^{3}}\right ) $ ( here $\mathbf{m}$ is a constant vector equal to the dipole moment ) . then , supposing your coil is very flat ( not really a solenoid ) , you can calculate what $\frac{\partial\mathbf{b}}{\partial t}$ is when $\mathbf{m} \cdot \mathbf{r} = 0$ , i.e. , at the moment the magnet passes through the coil like a basketball falling through a hoop . of course , $\mathbf{m}\cdot \dot{ \mathbf{r}} \neq 0$ . the peak emf should then be somewhat close to what you would predict from experiment . a lot of early experiments had to be really clever to isolate exactly what was going on . it really depends on what apparatuses are available to you .
this is a pretty imprecise answer , because i have not heard the term in a long time . i think ( someone please correct me if this is wrong ) a " myriotic " field is one in which there are an infinite number of quanta , so you do not have a well-defined notion of number operator or vacuum state .
developments as of summer 2014 the hottest kid on the organic-pv block is perovskites : in february 2012 , hardin , snaith and mcgehee published an article in nature photonics announcing " the renaissance of dye-sensitized solar cells " . the inventors of one implementation , oxford photovoltaics ltd ( a spinoff of the university of oxford ) described their new technology in science , in november 2012 , and have a patent application pending for their dye-sensitized solar cell . for more , see recent papers by henry j . snaith 's team . folk have got excited because of the rapid increases in efficiency since 2009 , combined with the very low cost and abundance of the raw material : as of summer 2014 , the record was $17.9\pm0.8\%$ , held by the korea research institute of chemical technology ( image source ) confirmed organic pv records under standard test conditions progress in photovoltaics : research and applications publishes a biannual review of pv efficiency records , under the name solar cell efficiency tables . the 2014 june review gave a record efficiency under standard test conditions for an organic pv cell of $10.7\pm0.3\%$ . for an organic pv mini-module , the record is $9.1\pm0.3\%$ . ( nb the higher perovskite efficiencies discussed above had not passed audited standard tests in time for the latest biannual review ) cells have higher efficiencies than sub-modules , which tend to have higher efficiencies than modules . a ( sub- or mini- ) module is made up of several cells , and power is aggregated from all of the cells . the most efficient cell in a ( sub-/mini- ) module will , by definition , have an efficiency equal to or higher than all other cells in that ( sub-/mini- ) module ; and not all the module surface area is covered in cells ; therefore record cell efficiency will always be higher than ( sub-mini/ ) module efficiency . here 's the recent history of organic pv cell record efficiencies ( % , standard test conditions ) : 2012-10 $10.7\pm0.3$ 2011-10 $10.0\pm0.3$ 2010-11 $8.3\pm0.3$ 2006-12 $~5.15\pm0.3$ 2006-03 $~3.0\pm0.1$ overall pv record the record efficiency for any pv cell is $44.4\pm2.6\%$ , for a ingap/gaas/ingaas inverted metamorphic cell ( 2013-04 ) , measured at 302 suns , am 1.5 , cell temp $25^{\circ}c$ .
actually , the rotation group $so ( 3 ) $ does act " on physics " , even in the presence of spin . the thing is that the wave function $\psi ( \vec x , \sigma ) $ is a redudant description of a physical state . a wave function with a different overall phase $c\psi ( \vec x , \sigma ) $ describes exactly the same physical state . after all , the only quantities of interest are only the expectation values of observables $$\langle x\rangle_\psi:=\frac{\langle\psi|x|\psi\rangle}{\langle \psi|\psi\rangle} . $$ and these are invariant under a rotation $r$ $$\langle x\rangle_{r\psi} = \langle x\rangle_{\psi} . $$ mathematically , we can say that the action of the rotation group on physical states is a projective representation , i.e. it acts on lines $\lbrace\lambda\psi ( \vec x , \sigma ) , \lambda\in\mathbb c\rbrace$ ( one-dimensional subspaces ) in a hilbert space , but not on the individual vectors . however , as you can read in the wikipedia page above , every projective representation of a lie group like $so ( 3 ) $ can usually be obtained from a linear representation of its universal covering group like $su ( 2 ) $ . ( linear representation just means that the group acts on individual vectors . ) to summarize , the rotation group $so ( 3 ) $ acts on ordinary quantum mechanics , too , but for practical calculations , it is useful to generalize it to $su ( 2 ) $ instead . there is even a bit hair splitting as to whether you consider wave functions as physically relevant quantities and add an additional symmetry ( "up to phase" ) , or whether you take the quotient " wave function up to phase " as physically relevant quantities and work in the quotient space . as for the second question , i think it is possible to classify all lie groups $g$ with a homomorphism $g \to so ( 3 ) $ via group cohomology , but i am not familiar enough with this topic to give an answer .
note that in the sum $$=\sum_{s_1=\pm 1} . . . \sum_{s_n=\pm 1}\langle s_2| t_{_{nn}}^{\dagger}|s_1\rangle\langle s_1| t_{_{nnn}}|s_3\rangle\langle s_3| t_{_{nn}}^{\dagger}| s_2\rangle\langle s_2| t_{_{nnn}}|s_4\rangle . . . \langle s_1| t_{_{nn}}^{\dagger}|s_n\rangle\langle s_n| t_{_{nnn}}|s_2\rangle$$ every pair $|s_i\rangle\langle s_i|$ occurs twice with some operator/matrix between them . so you cannot simply execute the sum over $s_i = \pm1$ for both occurrences separately . as pointed out in the paper you linked , one solution via transfer matrix is to group two neighbouring spins to one 4-state spin . then the problem can be reduced to nn interactions only .
it is happening because of the acceleration of the earth orbital speed around the sun ( earth is near the perihelion ) . between december 13 and december 31 the earth is speeding up and also it is normally rotating around its axis . these 2 movements ( constant rotation and increasing orbital speed ) add up to create the observed apparent movement of the sun on the earth sky . the sun rises later and also sets later every day . it is a bit tricky to visualize , so try it with a globe .
i think it is a question how hard you can suck the water in . the force $f$ you need to accelerate the water column depends on the mass of water : $$f=mg . $$ and the mass depends on the density $\rho$ and the volume $v=ha$ with the length $h$ and the surface area inside the straw $a$ . so , the force you need to accelerate the water column is proportional to the height of the water column : $$f=\rho a h g . $$ if you create a complete vaccum the maximum force $f_{max}$ acting on the surface is $p a$ . then following applies : $$pa=\rho a h g \\ \leftrightarrow h = \frac{p}{\rho g} $$
newton 's cradle fights air resistance and restitution ( efficiency of rebound ) . you then want the highest density , hardest balls with the highest restitution . cobalt-sintered tungsten carbide is magnetic . polished hardened tool steel ball bearings are a good start . http://www.wired.com/wiredscience/2011/10/what-went-wrong-with-the-mythbusters-newton-cradle/ platinum plus gallium and indium alloys heat treat to a very hard and springy state , density around 19 g/cm^3 versus less than 8 g/cm^3 for tool steel ( steve kretchmer , niessing co . , eastern smelting ; platinum sk ( tm ) alloys ) . for more shallow wallets , tungsten steel , thoriated tungsten . 95.5% pt , 3.0% ga , 1.5% in ; 95.2% platinum , 4.8% ga , in , cu ; 1550-1650 c melt . 700 c for 30 minutes and slow cool to harden ( not reducing atmospheres ) . vickers hardness 318/rockwell a 76/rockwell c 32 . 125,000 psi tensile , 104,000 psi yield .
if you say that earth 's velocity around the sun is 67,000 mi/h , your reference point is the sun itself , which makes the aeroplane 's velocity 68,000 mi/h , not 1000 . using special relativity only , and ( a ) observing from the sun , a clock on the plane would seem to run slower than a clock on earth . a person ( b ) on earth would measure also measure an aeroplane 's clock to be slower , but by a different factor . time dilation , $ \delta t ' = \frac{\delta t}{\sqrt{1-\frac{v^2}{c^2}}} $ ( a ) observation from the sun : time dilation of aeroplane clock vs earth clock , ( i ) $\frac{\delta t}{\sqrt{1-\frac{68000^2}{c^2}}}$ and ( ii ) $\frac{\delta t}{\sqrt{1-\frac{67000^2}{c^2}}}$ ( b ) observation from the earth : time dilation of aeroplane clock vs earth clock , ( iii ) $\frac{\delta t}{\sqrt{1-\frac{1000^2}{c^2}}}$ these are all different values due to different velocities as measured from different observation locations . this is what relativity is all about . ( side note , this equations are correct iff $c$ is in mi/h . ) up to this point we have ignored general relativity , which takes into account time dilation due to gravitational acceleration ( clocks are measured to be faster in lower g ) . this is an opposite effect from the time dilation due to sr . as it turns out , aeroplanes are travelling way too slow to have their clocks observed to be slowed at all --in fact they measure to be faster than earth clocks , because the difference in gravity outcompetes the difference in velocity in this case . if you are wondering if there is a sweet spot where time dilation due to sr and gr cancel out , there is . you can find out more by searching for time dilation due to gravitation and motion . an important thing to note is that the effect of time dilation are observational effects , and are due different conditions at the observation point and the point being observed . when two objects have relative velocity , they both measure the other 's clock to be slower than their own but a third object with the same velocity as one of the original two will clearly not measure those two as equally slow .
if you did not make the measurement , the he has a much lower chance of being in a higher-than-base energy level . i think this is the incorrect step in your reasoning . the $he$ atom would be in a 50-50 superposition of the ground and an excited state if the electrons on earth were not measured , and should collapse to one state or the other with equal probablility if they are . in either case , a mesurement of the $he$ atom to determine if it were in the ground or an excited state has exactly the same distribution of results .
notice that the vector $$ \mathbf n = ( 0 , \sin\theta , \cos\theta ) $$ defined in the question is a unit vector . the operator that corresponds to measuring angular momentum in the direction of this unit vector is $$ \mathbf n\cdot \mathbf l $$ when the question says that the angular momentum measured in the direction of $\mathbf n$ is $+1$ , what it is referring to is the state $|u\rangle$ that is an eigenvector of $\mathbf n\cdot\mathbf l$ with eigenvalue $+1$ ; $$ \mathbf n \cdot\mathbf l |u\rangle = |u\rangle $$ the matrix representation $\vec u = ( u^1 , u^2 , u^3 ) ^t$ ( here superscript $t$ means transpose indicating that the matrix representation is a column vector ) of this state refers to the matrix representation of $|u\rangle$ in the basis $|1,1\rangle , |1,0\rangle , |1 , -1\rangle$ ; $$ \vec u = \big ( \langle 1,1|u\rangle , \langle 1,0|u\rangle , \langle 1 , -1|u\rangle\big ) ^t $$
no , you cannot say that $\hat{f}= ( x+a ) + ( x-a ) $ ( which would simplify to $2x$ ) , you specified its definition in the original problem : $$ \hat{f}\psi ( x ) =\psi ( x+a ) +\psi ( x-a ) $$ with words , you could say that $\hat{f}$ is a ( bi-directional ? ) translational operator . but symbolically , there is no other way to state what $\hat{f}$ means , except through how it operates on $\psi ( x ) $ ( or any general function of $x$ ) .
i think you are misunderstanding the article . i also think that the name ' flying saucer ' is a bit misleading . i think referring to it with the actual project name : ' low density supersonic decelerator ' ( ldsd ) fits better . the concept of this project is that a spacecraft uses a inflatable saucer-shaped balloon to increase its reference area ( the surface area used in the drag equation ) to increase its atmospheric drag during atmospheric entry . during nasa 's test flight they want to test whether this system would suffice to land big spacecrafts safely on to mars . to simulate the atmosphere of mars they lift the test craft with a helium balloon high up into the atmosphere of earth such that the density is similar to that of mars at its surface . they than use a rocket to get up to speeds comparable with a atmospheric entry at mars . this video also gives a good explanation of the ldsd , and this video give a little longer and more detailed explanation .
first , although it is common in some textbooks , i do not think it is a good thing to necessarily relate the equiprobability postulate to ergodicity . second , what this postulate enables is to estimate the probability distribution for the macrovariable you want to look at . you can of course look at the most probable value for this macrostate and this will correspond to a " thermodynamic interpretation " of what you can expect to observe . however , in statistical mechanics , what you can expect is not the most probable value but rather the average value and they need not be the same . moreover , you may want more than simply the average value ; you can also want to predict what is the free energy difference between one value of the macrostate and another in some kind of transformation in your sytem and for this you are in trouble if you do not try to get the probabilities right .
the amount of thermal energy in the earth 's core and mantle is determined primarily by the temperature . the three-dimensional temperature field inside the earth is imprecisely known . one dimensional " onion skin " models of the earth 's interior are based upon empirical evidence from seismology , geodesy , and mineral physics , but there are almost certainly lateral variations in temperature which are important to understand . the article mentioned in the comments can provide you with estimated temperatures at important discontinuities inside the earth . connect these with constant gradients , or find an alternative model temperature profile . you should be able to find some specific heat capacity measurements for periodite ( mantle ) , perovskite , and liquid iron . to an order of magnitude these should be like $10^2$ - $10^3$ $\text{j} \text{ kg}^{-1} \text{k}$ . you will also need the density profile , and can get that from a one dimensional model of the density of the earth . you might try preliminary reference earth model ( prem ) or the reference earth model website . the latter reference models include three dimensional models .
unlike the case in psychology and sociology , in physics we are capable of repeating the same experiment/measurement in the same conditions over and over . any valuable , unexpected results that reflect reality should repeat also , so there is no such problem . this is true unless the experiment is not repeated enough to filter unexpected results out of noise . this is why the lhc collides particles over and over , to catch any such repeated unexpected events over the background noise with big certainty .
this is much to do with the possible eigenvalues of the operators . normal operators on a hilbert space are closely analogous to complex numbers , with the adjoint taking the role of the conjugate ; these relations are typically inherited directly to the operator 's eigenvalues . thus , if a linear operator $l$ has an eigenfunction $f$ with eigenvalue $\lambda$ , $$lf=\lambda f , $$ then saying "$l$ is self-adjoint " means that $l^\dagger=l$ which translates to $\lambda^\ast=\lambda$ , i.e. that $\lambda$ be real . similarly , $l$ being nonpositive implies that $\lambda\leq0$ . in your case , the behaviour can be reduced to an equation of the form $$ \frac{\partial^2 p}{\partial t^2} ( x , t ) =\hat lp ( x , t ) , $$ where $\hat l$ is some differential operator . in general the solution will not be of this form , but you can take a first stab of the problem by inserting in an eigenfunction of the differential operator for the spatial dependence . that is , you use the trial solution $p ( x , t ) =p_0 ( x ) t ( t ) $ , where $\hat lp_0=\lambda p_0$ . this hugely simplifies the time-propagation equation , which reduces to the solvable form $$ \frac{\partial^2 }{\partial t^2}t=\lambda t . $$ while the solutions of this equation are formally all the same ( i.e. . $t ( t ) =t_+e^{\sqrt{\lambda}t}+t_-e^{-\sqrt{\lambda}t}$ ) regardless of what $\lambda$ is , the behaviour will be very different and depend , sometimes sensitively , on $\lambda$: if $\lambda&gt ; 0$ , then at least one of the exponentials $e^{\pm\sqrt{\lambda}t}$ will have a blow-up . if $\lambda$ has an imaginary part , however small , then one of the two square roots $\pm\sqrt{\lambda}$ will have a positive real part , and the corresponding contribution to $t ( t ) $ will oscillate at a blowing-up amplitude . if $\lambda$ is negative or zero , then both roots $\pm\sqrt{\lambda}$ will be imaginary or zero , and both exponentials will be completely oscillatory and have bounded amplitude for all time . it is clear that only the third case is consistent with conservation of energy . in terms of the differential operator , it corresponds to a condition of self-adjointness ( i.e. . $\lambda\in\mathbb r$ ) and non-negativity of the operator .
there are many quantum field theory models which are exactly solvable in the large $n$ limit , such that the $\mathbb{c}p^n$ model , the thirring model , the $o ( n ) $ vector model etc . please see the following review by moshe moshe and jean zinn-justin covering many of these models . the main idea is that feynman diagrams ( for example the vacuum diagrams in the case of the partition function ) are proportional to certain powers of $n$ depending on the number of vertices , lines and loops , and the leading order diagrams can be summed . there are other methods which lead to the same results such as variational computations . when the fields in the model belong to the fundamental vector representations of $o ( n ) $ or $u ( n ) $ , the computation of the large $n$ limit ( summation of the leading diagrams ) is quite easy , however , when the fields belong to the adjoint representation ( for example , the gluons in qcd ) , the analysis becomes more complicated . the case of large $n$ qcd was solved by t'hooft , please see the following review by aneesh manohar .
ye he will burn out and his weight will increase to very high extent and he will die that is why it still in theory . and you cannot travel twice the speed of light . according to some scientist when something reaches to 99.99% of light and if it tries to accelerate further to attain the velocity of light it starts traveling in time and time slows down for it so as it could not travel faster than the light as it is the speed limit and you have to follow traffic rules . in other words rest all the things start travelling faster than it in time and it feels as if for it time is going at normal speed . according to stephen hawking traveling at this speed for 2yrs according to it frame of reference will forward you in time to about 100 yrs . where as rest of the peoples will see it as if it was travelling for about 100 yrs
consider making the substitution $k = p/\hbar$ in the first expression , while simultaneously defining $$ \sqrt{\hbar} \ , a ( p ) = \phi ( k ) $$ then the first integral will become $$ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \sqrt{\hbar}\ , a ( p ) e^{i\left ( \frac{p}{\hbar} x - \frac{\hbar}{2m}\frac{p^2}{\hbar^2}t\right ) } \frac{dp}{\hbar} $$ which is exactly the second integral . it is just a change of notation !
yes , helium can leave the earth , and yes , we will run out of helium , but because of different reasons . when you buy a helium balloon and its contents get released , this helium goes into the atmosphere . it is not gone , and it could in principle be purified out of normal air . however , the total amount of helium in the atmosphere is so small it is technologically not feasible to do that . at some point the technology might be developed , but it is unlikely to be economical . on top of that , helium does also escape from the atmosphere . since it is so light , it drifts naturally to the upper layers , and there it is easily torn away by the solar wind . however , this process will occur on geological timescales , unless we were to waste so much helium that the total atmospheric content changed appreciably . keep in mind , though , that even if the helium does not leave earth it is lost to us once it is diluted in the atmosphere . so : yes , we will run out , and yes , it will make everything awful . and yes , you should cringe when you see helium balloons at a childrens ' party .
i take it you mean the determinant with the straight bars . . . ? if so , the only way to compute it for general background gauge field is , as nervxxx mentioned in the comments , to expand the determinant in $a_\mu$ . if you are considering a specific background gauge field ( like a constant magnetic field ) you should look whether you can find the eigenvalues of your operator , i.e. solve \begin{equation} ( i d\ ! \ ! \ ! / - m ) \psi = \lambda \psi \ ; . \end{equation} the determinant will then be given as the product of all the eigenvalues . ( in order to regularize the expression , it may be useful to rewrite it as the exponential of the sum of the logarithm of all eigenvalues , but that depends on the specifics of the problem . )
in most of the cases , the charge in dielectric material is created by imbalance in the charges existing in the dielectric . the dielectric is initially neutral . the balance is broken by certain “charging effects” such as friction , heat and pressure . the answer to which charging effect is responsible for the charge imbalance depends on the material and the surrounding conditions . also in most cases the generation of a charge on a dielectric requires another body to be involved . with respect to the rest of your questions : 1 . the charge does not go anywhere ; it stays where it was generated . charges can’t move in a dielectric as you know . 2 . the moving charges are mostly electrons , for electrons to move they need the necessary energy to overcome what is called the band gap . for dielectrics the band gap is large such that the electrons require a lot of energy to overcome the gap to arrive to conduction band where they can move freely . see the picture 3 . yes they are bounded to certain atoms , what makes those atoms specific is that they are the ones who experienced the imbalance of charges by the charging effects i mentioned above . the same things can be said for positive or negative charges . some dielectric materials have a tendency toward having negative charge when they are charged ; some have a tendency toward having a positive charge . that tendency is determined by materials properties . you can find a lot of information on this topic here
notice that the factors of $r^2$ cancel , and $\hat r\cdot\hat r = 1$ so the integral expression you wrote down reduces to $$ \frac{q}{4\pi\epsilon_0}\int \sin\theta \ , d\theta \ , d\phi $$ the bounds of integration are $0&lt ; \theta&lt ; \pi$ and $0&lt ; \phi&lt ; 2\pi$ so we really need to compute $$ \int_0^\pi \sin\theta\ , d\theta\int_0^{2\pi}d\phi = 2 ( 2\pi ) = 4\pi $$ so the factors of $4\pi$ cancel and we are left with $q/\epsilon_0$ as desired .
yes , it is a normal field theory , so you may derive the equations of motion . they will be the ordinary maxwell 's equations for the electromagnetic field $$ \partial_\nu f^{\mu\nu} = j^\nu $$ with $j^\mu$ calculated as the sum of the conserved currents for the dirac field and for the higgs fields , combined with the dirac equation coupled to the electromagnetic field ( with some yukawa interaction $y\cdot \phi\psi$ terms ) , and the klein-gordon equation for a charged scalar field with some $v' ( \phi ) $ and $\psi \psi$ terms added in the right hand side etc .
if you are looking at radio waves then the mirror will have to be made of thicker metal , because as you increase the wavelength you also have to increase the thickness of the metal to get the same reflectivity . that is actually how satellite dishes work . they are basically a big curved mirror that concentrates all of the microwaves coming down from the satellite . they are often full of holes to keep the weight down , and this does not matter because the wavelength of the waves is larger than the holes . this is the same principle as seeing a light on in your microwave . you can see the light escaping through the door but the microwaves are not escaping because they are too long . once you get beyond visible light into the shorter wavelengths ; ultraviolet light is easy to make mirrors for , but x-rays are very difficult . and so making x-ray telescopes is very difficult . sometimes they do it by using a bag of gas to act like a lens rather than mirrors or by using a metal mirror but at a very grazing angle which makes the mirror very large . mobile phone waves are at the microwave end of radio waves , and a sheet of aluminium would work nicely as a mirror for those . source : the naked scientists one can see that for making lenses , materials of different refractive indices can be used based on required convergence , divervence and dimensions can be compared to those of the above described mirrors , although there may a problem that making enormous lenses for radio waves require materials/machinery that we may not have at the moment , mirror though as you see are the big satellite dishes that we have seen many times .
there is a rigorous formal analysis which lets you do this . the true problem , of course allows both the proton and the electron to move . the corresponding schrödinger equation thus has the coordinates of both as variables . to simplify things , one usually transforms those variables to the relative separation and the centre-of-mass position . it turns out that the problem then separates ( for a central force ) into a " stationary proton " equation and a free particle equation for the com . there is a small price to pay for this : the mass for the centre of mass motion is the total mass - as you had expect - but the radial equation has a mass given by the reduced mass $$\mu=\frac {mm}{m+m}=\frac{m}{1+m/m} , $$ which is close to the electron mass $m$ since the proton mass $m$ is much greater . it is important to note that an exactly analogous separation holds for the classical treatment of the kepler problem . regarding self-interactions , these are very hard to deal with without invoking the full machinery of quantum electrodynamics . fortunately , in the low-energy limits where hydrogen atoms can form , it turns out you can completely neglect them .
the trouble is because you assumed that the final velocity of the small block is $\sqrt{2gh}$ . this is true only if the wedge was stationary ( in a frame of reference that is inertial ) , then what happens is that that the normal force from the wedge on the mass completely balances $mg\cos\alpha$ , leaving the component $mg\sin\alpha$ down the wedge as you said . but the situation is a little more complicated now , because the wedge is moving simultaneously as the small block slides down . so the forces do not balance out as described in the previous paragraph . one can look at it in terms of energy to gain a better idea . the earth-wedge-mass system is isolated , so its total energy is conserved . the wedge does not gain or lose any potential energy , so the only change in potential energy comes from the mass . the change is $- mgh$ . this must be distributed to the kinetic energies of both the wedge and the mass . that is , \begin{align} and \delta k + \delta u = \delta e = 0 \nonumber \\ \implies and \delta k_{wedge} + \delta k_{block} - mgh = 0 . \end{align} if the wedge was not moving at all , we would then have $\delta k_{wedge} = 0$ , so \begin{align} \frac{1}{2}mv^2 = mgh \implies v = \sqrt{2gh} \end{align} like you said . but we see that if the wedge was moving , it ' eats ' up some of the potential energy that would otherwise have gone to the mass . in other words , the small mass ' speed will not be $v = \sqrt{2gh}$ at the bottom . having identified the flaw in your argument , how do we solve the question ? there are a few ways . you can draw your force diagrams , carefully balancing out the forces and finding the geometric relation how the position of the mass relates to the position of the wedge . this analysis is perhaps easier in the wedge 's frame of reference , but then you would have to add a fictitious force as it is not an inertial frame . but the easiest analysis would be in terms of energy conservation , like the equation i gave you . we have \begin{align} \frac{1}{2}mv^2 + \frac{1}{2}mv^2 - mgh = 0 . \end{align} now all you have to do is find how $v$ is related to $v$ . this is simple from conservation of momentum and some trigonometry , try it . ( edit ) i noticed after posting that you specifically highlighted the fact that $v = \sqrt{2gh}$ is with respect to the wedge . lest you start pointing that out , this is not true , because the force the mass feels down the wedge is not $mg\sin\alpha$ , because in this frame ( wedge 's frame , which is not inertial ) , there is the fictitious force .
you are on the right track , but there is more that can be said . for an introduction to this topic , i highly recommend sean carroll 's spacetime and geometry , which i will follow below for the purpose of illustrating where that $\gamma$ comes from . the book grew out of lecture notes , the relevant chapter of which can be found online here . algebraic background note : this is long , but only because each step has been broken down into very simple parts . you want a directional derivative - something that tells you how a tensor changes as you move along some path in your manifold . just like in standard multivariable calculus , you define this as the sum of the derivatives of your object in each direction , weighted by how much your coordinate is changing in that direction : $$ \frac{\mathrm{d}}{\mathrm{d}\lambda} := \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \nabla_\mu . $$ here $\lambda$ parameterizes your path , and the appropriate derivative to use is the covariant derivative . the scalar function $x^\mu$ is being differentiated with the standard directional derivative : $$ \frac{\mathrm{d}}{\mathrm{d}\lambda} := \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \partial_\mu . $$ note that this is consistent with the fully covariant directional derivative in the case of scalar functions , and , despite appearances , this is not a circular definition ( you should be able to differentiate known functions of $\lambda$ with respect to $\lambda$ ) . the affine connection is simply defined as the set of coefficients needed to augment the partial derivative in order to make a covariant derivative : $$ \nabla_\mu v^\nu = \partial_\mu v^\nu + \gamma^\nu_{\mu\lambda} v^\lambda $$ for any vector $\vec{v}$ . you do not even need to know how to generalize this to covariant derivatives of arbitrary tensors . the geodesic equation arises from imposing the requirement that your curve parallel-transport its own tangent vector . 1 that is , the directional derivative of the tangent vector along the direction it is pointing vanishes . a path will be a set of smooth functions \begin{align} x^\mu : \mathbb{r} and \to m \\ \lambda and \to x^\mu ( \lambda ) , \end{align} one for each coordinate $\mu$ . if you denote the tangent vector at $\vec{x} ( \lambda ) $ by $\vec{t} ( \lambda ) $ , then $$ t^\mu = \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} . $$ putting all this together , we have \begin{align} 0 and = \frac{\mathrm{d}}{\mathrm{d}\lambda} \left ( \frac{\mathrm{d}x^\sigma}{\mathrm{d}\lambda}\right ) \\ and = \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \nabla_\mu \left ( \frac{\mathrm{d}x^\sigma}{\mathrm{d}\lambda}\right ) \\ and = \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \left ( \partial_\mu \left ( \frac{\mathrm{d}x^\sigma}{\mathrm{d}\lambda}\right ) + \gamma^\sigma_{\mu\nu} \frac{\mathrm{d}x^\nu}{\mathrm{d}\lambda}\right ) \\ and = \frac{\mathrm{d}^2x^\sigma}{\mathrm{d}\lambda^2} + \gamma^\sigma_{\mu\nu} \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \frac{\mathrm{d}x^\nu}{\mathrm{d}\lambda} \end{align} summary after all this ( possibly too much ) algebra , the take-home message is that the connection arose from the difference between partial and covariant differentiation along a curve . it is not that we want our curve 's tangent to never point in a different coordinate-dependent direction ( $\ddot{x}^\lambda = 0$ , which would be sufficient for " straight line " in euclidean space ) . rather we want the change in the coordinate-dependent direction we are pointing to be compensated by the fact that our tangent space is rotating with respect to our coordinates as we move along the curve . 1 some people define the geodesic in a more global sense , as the path that extremizes the arc length between two points . these definitions agree if and only if the affine connection you are using is indeed the metric compatible , torsion free christoffel connection .
there are two definitions for the wavenumber , one is $k=\frac{2\pi}{\lambda}$ , the number of wavelengths per $2\pi$ units of distance ( i.e. . on the circle ) , the other one is $k=\frac{1}{\lambda}$ , which is the number of wavelengths per unit distance . the formula for the raman shift is refering to the latter definition .
i think there are 2 main sources of confusion : first , because of gravity , extending your arms feels like work . we are only interested in the radial movement , though , and in this direction , the skater 's arms are pulled by the centrifugal force ( in the long tradition of spherical cows in vacuum , we could replace the figure skater with two beads on a spinning rod ) . second , the idea of rotational energy as kinetic energy . the relevant work variable is ( as already mentioned ) the radial extension of the skater 's arms , and as far as that is concerned , rotational energy plays the part of potential energy . think of the skater pulling in her arms as compressing a spring , and extending the arms as its release . going by either the bead or spring model , the rotational energy gets converted into kinetic energy of the arms , accelerated by the centrifugal force in direction of the radial work variable and ultimately dissipating via vibrations when the arms abruptly reach maximal extension . of course , if the skater does not let her arms be accelerated and slowly extends them instead , the energy dissipates right away , which might be the more realistic approach .
this is a two-part answer based on my own ideas and on the specifically referenced book . i am quite confident about the first part , but less so about the second . 1 ) the radiated power is indeed refractive index dependent . the question arises because of the refractive index dependence of the radiated power . this dependence is hardly mentioned anywhere online or in books , and it is not obvious that it is true ( indeed john implied that it is not true in his answer ) . so first we establish the claim : the power radiated per unit surface are of a black body is proportional to $n^2$ , where $n$ is the index of refraction of the medium it is radiating into . this follows from stefan 's law , when the speed of light $c$ in stefan 's constant is replaced by $c_0/n$ , where $c_0$ is the speed of light in vacuum . the $n^2$ dependence of the radiated power can also be derived by a modification of the normal derivation of planck 's law . the bose-einstein factor is constant for constant $e$ , but the density of states $g ( e ) de$ is directly wavelength-dependent via the mode-number . the wavelength dependence of $g ( e ) de$ gives an $n^2$ term for fixed $e$ , which integrates to give an $n^2$ term in the total radiated power . the $n^2$ dependence of the radiated power is also discussed explicitly in the book by r . siegel and john reid howell : " thermal radiation heat transfer " , 4th ed . : " the . . . and total emissive power of a blackbody into a medium with constant index of refraction $n$ are given by the stefan-boltzmann law : $\pi i_b = e_b = n^2 \sigma t^4$ . " the above reference could be wrong , although i have only heard good things about that book , but if it is wrong then we will need a good reference or derivation . 2 ) the resolution of the apparent paradox in the question . i just found it and have not gone through it myself , but siegel and howell give a possible resolution in their book : " section 18-5 shows how a portion of blackbody radiation from within a medium with $n&gt ; 1$ cannot pass through an interface into an adjacent medium with smaller $n$ . when entering into vacuum , there is a $1/n^2$ interface-reflection factor that removes the $n^2$ provided by [ stefan 's law ] , so the maximum energy passing into the vacuum is $\sigma t^4$ . "
that function is not defined there because you want to use a different one , depending on what kind of motion you are modelling . using the obvious newton law $$ a \propto f , $$ you can think of it as calculating the force that acts on your object , e.g. the spring force . in the case of gravitation , you simply put in the constant gravity acceleration downwards . ( i would like to note that in this case , 4th order runge-kutta is something of an overkill as 2nd order is already exact ! but it is nevertheless a good idea to use it here , to be consistent : 4th-order runge-kutta is used in a very wide range of applications . )
in physics , it is often possible/meaningful to analytically continue various real quantities into a corresponding complex variable . in case of time , such analytical continuation allows us to wick-rotate a real time integration contour into an imaginary integration contour if we avoid singularities in the complex plane . wick-rotation is a mathematical procedure that does not count/qualify as an extra physical space-time dimension . that said , there exists a branch of string theory , known as f-theory , that formally operates with two time-dimensions , cf . this phys . se post .
you ask : is it true that any magnetic field induces an emf in a loop of wire ? no , not in any setup . only if there is a change in the magnetic lines passing through a loop . it is how electric generators work . here is a simple example of a loop in a changing magnetic field .
obvouusly the highest number of splashes will be when the impact happens at right angle . this is because at such impact all the kinetic energy of the drop at one moment goes into the forces directed into different directions and tears the drop apart . conversely the least splashes will be when the drop impacts at narrow angle . in this case the drop continues sliding over the surface and keeps its integrity while kinetic energy slowly transforms into heat due to friction . the surface tension keeps the drop intact because all parts of the drop keep the same movement direction . as such i would recommend a form which meets the drop in the most probable impact place with a sloppy angle . evidently , such form should not be symmetric ( because all even functions have zero derivative at zero ) . the only variant of yours that has such property is the fourth . additionally it changes the drop 's velocity vector to the right direction so that any splaches that can occur will go to the right where it has a border that covers the greatest angle of possible splashes path out than all other variants . the slope 's curvature should be such that the amount of energy converted into heat was uniform along the drop 's path and the pressure never exceed the surface tension . so when the drop has the highest speed , the angle between the forces acting on the drop ( inertia and gravity combined ) and the surface should be the smallest , but as long as the drop slows , the angle slows the angle should rise so to keep the component which is perpendicular to the surface constant . the fourth picture roughly satisfies this criterion . thus the fourth variant in the best .
i have always found the cornell to be a good source of info ( and also the former home of carl sagan ) : blue stragglers are stars which stay on the main sequence ( the normal , hydrogen-burning phase of a star 's lifetime ) longer than they are expected to . the color of a star is a measure of its temperature and its mass - blue stars are hotter and more massive than red ones . the more massive a star is , the faster it burns up its hydrogen , so blue stars are expected to spend less time on the main sequence than red stars . therefore , when you look at a color-magnitude diagram of a globular cluster ( whose member stars all formed around the same time ) you expect to see an orderly transition ; stars which are bluer than a certain value ( known as the " turnoff " point ) will have already left the main sequence , while those which are redder will still be on it . the location of the turnoff point can be used to estimate the age of the cluster . but , it is usually the case that several stars in a cluster are observed along the main sequence past the turnoff point , and these are referred to as blue stragglers . the most likely explanation for blue stragglers seems to be that they are the result of stellar collisions or mass transfer from another star . that way , a star which is red , cool and already somewhat old can get extra mass and turn bluer . it spent most of its life as a red star and therefore burnt its hydrogen at a slow enough rate to still be on the main sequence , but then at a certain point it gets extra mass and effectively " disguises " itself as a blue star , which makes us think it is younger than it really is . i think the key takeaway is that we really are not 100% sure . however , that in no way means that anyone can simply assert that their pet theory ( guess , lie , whatever ) is the answer either . especially if they do not bring any evidence , testable hypotheses , or data to examine to the table . solstation also has quite a bit of good info on blue stragglers and what we know about them . of particular interest is this graphic : this paper from cornell also has some additional info . abstract : in open star clusters , where all members formed at about the same time , blue straggler stars are typically observed to be brighter and bluer than hydrogen-burning main-sequence stars , and therefore should already have evolved into giant stars and stellar remnants . correlations between blue straggler frequency and cluster binary star fraction , core mass and radial position suggest that mass transfer or mergers in binary stars dominates the production of blue stragglers in open clusters . analytic models , detailed observations and sophisticated n-body simulations , however , argue in favour of stellar collisions . here we report that the blue stragglers in long-period binaries in the old ( 7 × 109-year ) open cluster ngc 188 have companions with masses of about half a solar mass , with a surprisingly narrow mass distribution . this conclusively rules out a collisional origin , as the collision hypothesis predicts a companion mass distribution with significantly higher masses . mergers in hierarchical triple stars are marginally permitted by the data , but the observations do not favour this hypothesis . the data are highly consistent with a mass transfer origin for the long-period blue straggler binaries in ngc 188 , in which the companions would be white dwarfs of about half a solar mass .
