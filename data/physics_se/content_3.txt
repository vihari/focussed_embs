sem no . with tem you can see the atomic structure but only very local . it does not give you the idea of the whole sample . besides , tem is a much complex technique than xrd and involves a specimen preparation which may be very ( very ! ) difficult and alter you sample . powder xrd is a simple technique , most of the times non destructive . you do not need to do a fourier transform . . . powder xrd gives you a diffraction pattern with diffraction peaks . in a general way , sharper peaks means a more " crystalline " sample and well organized ( closer to a single crystal ) . however this is not always straightforward ( it depends on type of sample , material , size of particles . . . ) . it would be helpful if you could say what type of sample are you trying to analyse .
any optical assembly , whether a telescope or a camera or whatever , will generally have a minimum distance at which it can focus . that said , the scale for distances is set by the focal lengths of the optics . as you point out , they are $360~\mathrm{mm}$ and either $6~\mathrm{mm}$ or $20~\mathrm{mm}$ . ( the $360~\mathrm{mm}$ is the really important one . ) distances long compared to these are all much the same to your telescope , so the building half a kilometer away and the stars thousands of light years away are all " at infinity " as far as it is concerned . the $360~\mathrm{mm}$ actually means that parallel rays " from infinity " need to travel $360~\mathrm{mm}$ beyond the lens to converge to a point . diverging rays from " closer than infinity " objects need to travel further . thus the minimum distance at which you can focus is set by how much you can extend the optical path , and the maximum distance you can focus is set by how short the path can be made . any telescope , whether for terrestrial or astronomical use , should be able to focus at infinity - that is after all where its targets of interest are . note though that i mentioned changing the optical path . in order to focus , assuming you do not have some multimillion dollar deformable optics on hand , you need to turn some knob somewhere to move something . while there are systems built that are fixed to focus at a single distance , i am guessing that is not what you have , since it seems designed for changing eyepieces ( doing so requires refocusing ) and personal use ( while contacts are fine , people who wear glasses are best served by removing them and changing the focus to correct for nearsightedness , since you generally need to get your eye close to the eyepiece ) .
just look at how people actually did it . ; ) measuring shadow length at the same time at different places , comparing horizons ( as also suggested in comments ) , . . . also have a look at this list , some of which are down to earth methods . ; ) by the way , there are easy ways to prove that the earth is rotating , like the foucault pendulum , as well .
you definitely do not want to do the integral from $-\tau/2$ to $\tau/2$ ; you need to go all the way to $\pm \infty$ for the integral to be meaningful . in this particular case , as a comment notes , the integral will diverge if you take the lower limit to $-\infty$ ; physically , you want to start the integral at $t=0$ because that is when the field comes into existence . here is a hint towards the easiest way to do this integral ( and many fourier integrals , for that matter ) : $\cos ( x ) = \frac{1}{2} ( e^{ix} + e^{-ix} ) $ . the integral should be easy from there . you can also use integration by parts , but that is a little more tedious and error-prone .
in mathematics , there is a complete symmetry between $+i$ and $-i$ . both the imaginary unit and the minus imaginary unit obey $$ i^2 = ( -i ) ^2 = -1 $$ the exchange of $i$ and $-i$ is known as the ${\mathbb z}_2$ automorphism group of the complex numbers ${\mathbb c}$ . when you introduce the complex numbers for the first time , it is a complete convention whether you call a square root of $ ( -1 ) $ as $+i$ or $-i$ . however , in physics , we have to break the symmetry between $+i$ and $-i$ because we must know whether a wave in a particular situation is $\exp ( i\omega t ) $ or $\exp ( -i\omega t ) $ , for example . in particular , $xp-px=i\hbar$ and not $-i\hbar$ . also , and the following choice of the sign is actually not independent from the previous one in the commutator , schrödinger 's equation was chosen to be $$ h |\psi \rangle = i\hbar\frac{d}{dt}|\psi\rangle $$ where $h$ is the hamiltonian that may be replaced by $h=e$ when acting on an energy eigenstate $|\psi\rangle$ . this equation is totally universal everywhere in quantum mechanics where a hamiltonian is well-defined ( it may be even quantum field theory or some descriptions of string theory ) . the equation above , with $h=e$ , is solved by $$|\psi ( t ) \rangle = \exp ( et/i\hbar ) |\psi ( 0 ) \rangle = \exp ( -iet/ \hbar ) |\psi ( 0 ) \rangle = \exp ( -i\omega t ) |\psi ( 0 ) \rangle $$ all the forms are equivalent because $1/i = -i$ – this equation is equivalent to $i^2=-1$ – and because $e=\hbar\omega$ without a minus sign . so your sign is wrong ; the sign you denounced is the right one and the sign you wanted is the incorrect one . just to be sure , in quantum field theory , we work with various objects – quantum fields – that are expanded into terms that depend on time as $\exp ( -i\omega t ) $ while there must also be terms that depend on time via $\exp ( +i\omega t ) $ . but these are terms in operators , not the time dependence of the wave function . one must be careful about the precise statements and objects . i have not made any statement of the sort that only the expression $\exp ( -i\omega t ) $ and not $\exp ( +i\omega t ) $ appears in quantum theory papers and books . of course , both of them may appear somewhere – in quantum field theory , both of them have to appear because there are both creation and annihilation operators , both particles and antiparticles . but when we are asking how an energy $e$ wave function ( and i mean the ket vector ) depends on time , it is always via $\exp ( -iet/\hbar ) $ . the bra vector has the opposite sign ( plus ) in the exponent .
great question ; i remember being so confused by this when i first took analytic mechanics . the components of the angular velocity " in the body frame " are not zero because when one writes these components , one is not referring to measurements of the motions of the particles in the body frame ( because , of course , the particles are stationary in this frame ) . instead , one is referring to angular velocity as measured in an inertial frame but whose components have simply been written with respect to a time-varying basis that is rotating with the body . in practice , we make measurements of the positions $\mathbf x_i ( t ) = ( x_i ( t ) , y_i ( t ) , z_i ( t ) ) $ of the particles in an inertial frame . then , we note that for a rigid body ( let 's consider pure rotation for simplicity ) , the position of each particle $i$ satisfies \begin{align} \mathbf x_i ( t ) = r ( t ) \mathbf x_i ( 0 ) \end{align} for some time-dependent rotation $r ( t ) $ . then we compute $\boldsymbol\omega ( t ) = ( \omega^x ( t ) , \omega^y ( t ) , \omega^z ( t ) ) $ in the standard way in terms of $r ( t ) $ . to see how this is done in detail , see , for example stackexchange-url once we have $\boldsymbol\omega$ , we can write its components with respect to any basis we like . if we write it in the standard ordered basis $\{\mathbf e_i\}$ , then we will just get $\omega_x ( t ) $ as its components . if we write it in some basis $\{\mathbf e_{i , b} ( t ) \}$ that is rotating with the body ( like one that points along the principal axes of the body ) then we get different components $\omega^i_b ( t ) $ , and these are the body components . main point reiterated . angular velocity is being measured with respect to an inertial frame , but its components can be taken with respect to any basis we wish such as one rotating with the body .
as understood by einstein 's general theory of relativity completed in 1915-16 , gravity is indeed a manifestation of ( nothing else than ) the curvature of space and i have some doubts about your implicit claim that you have made this discovery " independently " of einstein . according to the precise equations of general relativity , the so-called einstein 's equations $$ g_{\mu\nu} = \frac{8\pi g}{c^2} t_{\mu\nu} , $$ what influences the curvature of spacetime is the stress-energy tensor that knows about the density of energy and momentum and the flux of energy and momentum . terms like " flux of momentum " may sound obscure but they are described by well-defined mathematical formulae . in particular , " flux of momentum " is nothing else than the component of pressure . so pressure also influences the curvature of spacetime – and therefore the gravitational field and the behavior of objects in this field – according to general relativity . on the other hand , it is irrelevant for the curvature and gravity whether the same stress energy tensor – the density of mass , energy , momentum , and components of pressure and stress – are achieved by the electromagnetic field , one material , or another material . however , it is still impossible to " create " curvature of space without any material ( or energetic ) carrier . the equations explicitly show that the ricci tensor is zero if there is no energy/momentum density in the space . so one can not create a " black hole out of nothing " . nevertheless , black holes may suck all the material and make the spacetime around ricci-flat ; the ricci ( or einstein ) tensor is equal to zero almost everywhere in the space . this ricci-flatness is still importantly violated at the black hole singularity which is the reason why the black holes still carry a nonzero mass/energy . the question is getting increasingly impenetrable as one continues to read it so what you exactly wanted to do with the frame-dragging effect remained unknown to me ( and i guess that not only me ) . frame-dragging is a particular new gravitational effect that occurs in the gravitational field induced by rotating bodies .
scale bars may be helpful . also , spr 's decay exponentially . if you google " surface plasmon image , " you might notice that there is a lack of images and mostly cartoons . imaging a surface plasmon does not really make sense , as the light is traped along the interface between a metal and a dielectric . and while you may be able to image light near a spr , it would be really hard , your objective lens would have to be ridiculously close to the metal , most likely less than a micron if your objective is in air . imaging the light that is coupled out of a spr makes more sense . to get outcoupling of light you need to use a type of grating , or create some kind of rough patch for the light to escape via . in order to get this kind of out coupling , you need to do angled imaging , which it seems you perform . so maybe what you have is just a bump in your metal film ? the wavelengths of light that are emitted from film of course are affected by the permittivity/refractive index of your surounding materials , but they are also very much modified by the size/shape/geometry of your nanostructure . this is very important if your looking to gain insight about what you are seeing . so in order to gain some kind of quantitative insight from this blimp in your image , it sounds like you are going to have to make that same identical rough patch . or actually create a meaningful outcoupling grating . as for really knowing if it is a spr , i am not sure what to tell you . you had probably need a high intensity laser to create that kind of resonance , since you are in the uv ( blue color ) . you would need something higher than the color that is being emitted , as a great deal of loss is typically seen with spr 's . i am not sure what you imaged with ( you sounded like the tem imaging and the other imaging were separate ? ) , and i can not speak for tem ( not familiar ) . i would suggest googling to see if you can find tem 's producing spr 's . from a quick google search , i think they can , but it looks like advanced filtering might be needed . . . good luck
stopping short of actually doing the calculation , checking sign conventions , etc . , my impression is as follows : at the classical level , the definition of bosonic canonical momentum $p_i:=\frac{\partial l}{\partial\dot{\phi}^i}$ is standard . the fermionic canonical momentum is more subtle , because the lagrangian for the fermions is already on first order form . a construction is feasible via the standard dirac-bergmann procedure leading to second-class constraints . a shortcut is offered by the faddeev-jackiw procedure . for a simple illustrative ( bosonic ) example of these two methods , see e.g. this phys . se answer . in the bosonic case , one should distinguish between the mechanical/kinetic momentum ( which is roughly the velocity ) , and the canonical/conjugate momentum . in the schrödinger/position representation , the former corresponds to covariant derivatives , while the latter corresponds to partial derivatives . ( a similar situation takes place for a charged particle in an em field with the role of christoffel symbols replaced by the em gauge potential , cf . this phys . se answer . ) note in particular that the commutator between two kinetic momenta will in general not be zero . in ref . 1 eq . ( 92 ) witten correctly places the covariant derivative in the super-charge formula , but then proceeds below eq . ( 92 ) to wrongly/confusingly refer to the covariant derivative as the momentum conjugate to $\phi^i$ . ref . 2 forgets to inform its readers that that it in the middle has changed the meaning of $p_i$ to now denote the covariant derivative , cf . eq . ( 10.217 ) . moreover , ref . 2 wrongly/confusingly refers to the new $p_i$ as the conjugate momentum just above eq . ( 10.210 ) . these mistakes are undoubtedly spurred by the confusing terminology of ref . 1 . finally at the quantum mechanical level , operator ordering prescriptions should be chosen such that hermiticity and susy are preserved . references : e . witten , constraints on supersymmetry breaking , nucl . phys b202 ( 1982 ) 253 . k . hori , s . katz , a . klemm , r . pandharipande , r . thomas , c . vafa , r . vakil , and e . zaslow , mirror symmetry , 2003 . the pdf file is available here or here .
no , op 's calculation is correct . in more detail , the paper states on page 323 ( apparently assuming that $\mu$ and $\nu$ are real numbers ) , that the result is $$ \mu^2 + 2 \nu^2 ~=~ 2\mu^2 -1~ . $$ the first expression is correct , and corresponds to op 's $3\mu^2-2$ . the second expression is wrong . in other words , the paper makes a mistake in the very last step while reducing with $\nu^2=\mu^2-1$ .
in short , the answer goes like this : all fermion masses are assumed to unify at some high scale ( e . g . ~$10^{16} gev$ ) in msugra . so , the mass differences between them at low energies are due to the running of the masses from that high scale down to the observed scale ( e . g . ~1tev at the lhc ) . the $\beta$ function for the stop mass has a positive contribution due to the top yukawa coupling , $y_t$ , which is large due to the fact that the top mass is not small compared to the higgs vacuum expectation value , $v$ , ( $m_t = y_tv$ ) . this implies that the stop mass drops more rapidly as the renormalization scale is lowered than the 1st and 2nd generation squarks , which have negligible yukawa couplings . this leads to a mass spectrum where the stop is light and the first two generation squarks are nearly degenerate ( ditto for the stau ) . the supersymmetry primer by martin has a good discussion of this ( http://arxiv.org/abs/hep-ph/9709356 ) , particularly on pg . 46 ( where you can find the $\beta$ functions ) and 75 ( which discusses the squark and slepton spectrum ) .
you are absolutely right . in fact , the uncertainty relations are a direct consequence of this wave nature and the " conjugated " variables being $p \leftrightarrow x$ and $e \leftrightarrow t$ . i will attempt a simple explanation . remember that the wavefunction gives you a probability amplitude for finding the particle at some location $x$ at some time $t$ ? the probability to find the particle at some point $x$ at time $t$ is given by $|\psi ( x , t ) |^2$ . now here 's the catch : if you put your plane wave into that equation , you will find that $|\psi ( x , t ) |^2 = a$ , independent of $x$ and $t$ . so for your plane wave , the particle has equal probability to be anywhere . in terms of uncertainty , then , we could say that the uncertainty in position is at a maximum , and so is the uncertainty in time . what about momentum and energy ? well , your plane wave has a definite momentum of $p = \hbar k$ and a definite energy of $e = \hbar \omega$ , so their uncertainty is zero . this is an extreme case of the uncertainty relation : certainty in one variable means maximum uncertainty in the other variable . any other possible form of the wave function can always be written as a sum of plane waves ( that is the fourier theorem ) : $$\psi ( x , t ) = \int de \int dk a ( k , e ) \exp ( i ( kx-e/\hbar t ) $$ you can think of $\psi ( x , t ) $ as the wavefunction in position and time domain and $a ( k , e ) $ as the wavefunction in momentum and frequency domain , and the conversion between those two is achieved via the fourier transform . you can now mathematically show how if $\psi ( x , t ) $ is " narrow " ( i.e. . position has low uncertainty ) then $a ( k , e ) $ must be broad and vice versa . the intuition behind that : you need to add up a lot of plain waves with lots of different momenta to go from the broad plain wave distribution to a narrow wave packet , but this then implies that you have high uncertainty in momentum .
velocity is a vector , in your case 1-dimensional , and so its sign indicates its direction . it is a matter of convention which direction is taken to be positive , but you can establish the convention in use by examining your formula \begin{equation} h = - 16 \cdot t^2 + v \cdot t + h_0 \end{equation} the first element on the right comes from acceleration , the second from initial velocity and the last is the initial height . now , consider how h changes for very small t , i.e. before the effect of acceleration becomes visible . as you can see positive v leads to h increasing with time , i.e. velocity oriented upward . similarly , negative v leads to h decreasing with time , i.e. velocity oriented downward . this means that your code is probably correct assuming it does not violate some externally defined velocity sign convention . the way you read your graph is this : negative velocity means the body falling down , positive velocity means the body climbing . if you start with initial velocity zero , you would expect the body to start falling immediately , which is indeed what happens . if you set the initial velocity to a positive number , it will start positive , indicating that the body initially climbs , then it will linearly decrease to zero , indicating that it slows down until it stops at the highest point and finally the velocity will become negative indicating that the body starts to fall down .
general remarks . let $\delta w$ denote the differential work done by a system , so $\delta w$ is postive when the system does work on something else and negative when work is done by something else on the system . for a given process taking place over a path $\gamma$ in thermodynamic state space , the systematic way of determining whether work was done by or on the system is to determine the sign of $w$ , the total work done by the system , which is given by $$ w = \int_\gamma\delta w $$ this can be computed in various ways depending on the system at hand , and the process it undergoes . the trick is to attempt to find an expression for $\delta w$ that allows for the efficient calculation of the integral for $w$ . example - adiabatic compression . suppose , for example , that we want to determine the work done by the gas during process $1$ of your diagram . recall that the first law of thermodynamics in differential form can be written as follows : $$ de = \delta q - \delta w $$ the sign convention here is that $\delta q$ denotes the heat transferred to the system , and $\delta w$ , again , denotes the work done by the system . since process $1$ is adiabatic , we have $\delta q = 0$ by definition . it follows that $$ w = -\int_\gamma de = -\delta_\gamma e $$ where $\delta_\gamma e$ denotes the total change in energy along the path $\gamma$ . let process $1$ start at point $a$ and end at point $b$ , then we can write this result as $$ w = - ( e_b - e_a ) = e_a-e_b $$ so to determine the sign of the work done , we simply need to know whether or not the internal energy of the gas increased ( in which case $w&lt ; 0$ so that work was done on the gas ) or decreased ( in which case $w&gt ; 0$ so that work was done by the gas ) . how to we figure this out for this adiabatic process ? well take , for example , a monatomic ideal gas , and recall that for such a process , we have $$ t_av_a^{\gamma-1} = t_bv_b^{\gamma-1} , \qquad \gamma = \frac{5}{3} $$ then we see that since $v_b&lt ; v_a$ , we have $t_b&gt ; t_a$ ; the temperature of the gas increased . but for a monatomic ideal gas , the internal energy can be written purely as a function of temperature and number of particles ; $$ e = \frac{3}{2}nkt $$ so assuming the number of particles is fixed , the internal energy also increase , and therefore , $w&lt ; 0$ , so work was done on the gas .
when two black holes collide , they coalesce to form a larger black hole . the event will produce gravitational waves that will have a particular signature that depends on the details of the collision . black hole merger events are one of the main gravitational wave signals that are being sought at ligo . there are a few misconceptions in your question : 1 ) black holes do not have infinite mass . they in fact have a finite mass . the orbit of the earth around a black hole with the same mass as the sun 's would in fact be identical to the earth 's current orbit . so , the idea of black holes sucking up the space around them is inaccurate . 2 ) the gravity at the event horizon of a black hole is finite and ( generally ) inversely proprortional to the mass of the black hole--a sufficiently large black hole would not exert any detectable local strains on objects at all ! 3 ) the geometry of spacetime will be well-defined everywhere except at the central singularity of the black hole . every observer will report only one direction of time . this direction , in an absolute sense , will be observer dependent , just as it is in special relativity ( the rate of aging will depend on speed and position , but will always be well-defined and knowable ) .
the moon orbits around the sun , but so does the earth . they orbit together with the moon 's orbit perturbed by the nearby earth . if fact , despite their different masses they experience the same acceleration , so it should not be surprising that they are bound to the same orbit since they are bound to each other ( i.e. . at basically the same distance from the sun ) . the moon experiences motion relative to the earth and is bound to it by the earth 's gravity , and once bound , unless the tidal forces due to the sun pull them apart , they will stay bound together - accelerating towards the sun at the same rate , as essentially one object . this is the key : outside of the hill sphere the difference in gravitational force ( the so-called " tidal force" ) is great enough to break the gravitational binding . you may be interested to know that the earth-moon roche limit vis-à-vis the sun is about 33.6 million kilometers and that the earth is roughly 150 million kilometers from the sun . so we are quite safe from the danger of having our moon stolen .
it turns out my approach was correct , but i had different errors preventing acceptance of my solution . to summarize : calculate the maximum constant deceleration $a$ that can be applied for time $t$ that results in non-negative final velocity : $a_{max}=\frac{v_0}{t}$ . this is needed because the next equation does not take into account decleration that stops at velocity 0 ( deceleration due to friction ) calculate the slowing ( might be stopping ) decleration that places the vehicle at distance $d$ at time $t$: $a_{slow} = 2\frac{v_0t-d}{t^2}$ if $a_{slow} &gt ; a_{max}$ , then applying $a_{slow}$ causes the vehicle to exceed distance $d$ before time $t$ ( and to go " in reverse " back to $d$ ) -- use $a_{stop} = \frac{v_0^2}{2d}$ , else use $a_{slow}$ thanks to all who commented/answered
it is a side effect of the unreasonable effectiveness of mathematics . you are in good company thinking it is a little strange . many quantities in physics can be related to each other by a few lines of algebra . these tend to be the models that we think of as " pretty . " terms manipulated by pure algebra tend to pick up integer factors , or factors that are integers raised to integer powers ; if only a few algebraic manipulations are involved , the integers and their powers tend to be small ones . other quantities may be related by a few lines of calculus . from calculus you get the transcendental numbers , which can not be related to the integers by solving an algebraic equation . but there are lots of algebraic transformations you can do to relate one integral to another , and so many of these transcendental numbers can be related to each other by factors of small integers raised to small integer powers . this is why we spend a lot of time talking about $\pi$ , $e$ , and sometimes bernoulli 's $\gamma$ , but do not really have a whole library of irrational constants for people to memorize . most of constants with many significant digits come from unit conversions , and are essentially historical accidents . carl witthoft gives the example of $e=mc^2$ having a numerical factor if you want the energy in btus . the btu is the heat that is needed to raise the temperature of a pound of water by one degree fahrenheit , so in addition to the entirely historical difference between kilograms and pounds and rankine and kelvin it is tied up with the heat capacity of water . it is a great unit if you are designing a boiler ! but it does not have any place in the einstein equation , because $e\propto mc^2$ is a fact of nature that is much simpler and more fundamental than the rotational and vibrational spectrum of the water molecule . there are several places where there are real , dimensionless constants of nature that , so far as anyone knows , are not small integers and familiar transcendental numbers raised to small integer powers . the most famous is probably the electromagnetic fine structure constant $\alpha \approx 1/137.06$ , defined by the relationship $\alpha \hbar c = e^2/4\pi\epsilon_0$ , where this $e$ is the electric charge on a proton . the fine structure constant is the " strength " of electromagnetism , and the fact that $\alpha\ll1$ is a big part of why we can claim to " understand " quantum electrodynamics . " simple " interactions between two charges , like exchanging one photon , contribute to the energy with a factor of $\alpha$ out front , perhaps multiplied by some ratio of small integers raised to small powers . the interaction of exchanging two photons " at once , " which makes a " loop " in the feynman diagram , contributes to the energy with a factor of $\alpha^2$ , as do all the other " one-loop " interactions . interactions with two " loops " ( three photons at once , two photons and a particle-antiparticle fluctuation , etc . ) contribute at the scale of $\alpha^3$ . since $\alpha\approx0.01$ , each " order " of interactions contributes roughly two more significant digits to whatever quantity you are calculating . it is not until sixth- or seventh-order that there begin to be thousands of topologically-allowed feynman diagrams , contributing so many hundreds of contributions at level of $\alpha^{n}$ that it starts to clobber the calculation at $\alpha^{n-1}$ . an entry point to the literature . the microscopic theory of the strong force , quantum chromodynamics , is essentially identical to the microscopic theory of electromagnetism , except with eight charged gluons instead of one neutral photon and a different coupling constant $\alpha_s$ . unfortunately for us , $\alpha_s \approx 1$ , so for systems with only light quarks , computing a few " simple " quark-gluon interactions and stopping gives results that are completely unrelated to the strong force that we see . if there is a heavy quark involved , qcd is again perturbative , but not nearly so successfully as electromagnetism . there is no theory which explains why $\alpha$ is small ( though there have been efforts ) , and no theory that explains why $\alpha_s$ is large . it is a mystery . and it will continue to feel like a mystery until some model is developed where $\alpha$ or $\alpha_s$ can be computed in terms of other constants multiplied by transcendental numbers and small integers raised to small powers , at which point it will again be a mystery why mathematics is so effective . a commenter asks is not α already expressible in terms of physical constants or did you mean to say mathematical constants like π or e ? it is certainly true that $$ \alpha \equiv \frac{e^2}{4\pi\epsilon_0} \frac1{\hbar c} $$ defines $\alpha$ in terms of other experimentally measured quantities . however , one of those quantities is not like the other . to my mind , the dimensionless $\alpha$ is the fundamental constant of electromagnetism ; the size of the unit of charge and the polarization of the vacuum are related derived quantities . consider the coulomb force between two unit charges : $$ f = \frac{e^2}{4\pi\epsilon_0}\frac1{r^2} = \alpha\frac{\hbar c}{r^2} $$ this is exactly the sort of formulation that badroit was asking about : the force depends on the minimum lump of angular momentum $\hbar$ , the characteristic constant of relativity $c$ , the distance $r$ , and a dimensionless constant for which we have no good explanation .
you are not " replacing a mathematical proof " . what the statements you are referring to mean is that in tensor notation , the proof is immediate , so that nothing needs to be written down . this is because if you have a tensor equation as above , in order to prove lorentz invariance , do a lorentz transformation and go to another set of coordinates $x^{\mu'}$ . then using the usual transformation laws we get that ${\partial_{\mu}} = \lambda^{\mu'}_{\mu}\partial_{\mu'}$ and $f_{\mu\nu} = \lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}f_{\mu'\nu'} $ , we can write the maxwell equation in terms of the new coordinates to become $\lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}\lambda^{\sigma'}_{\sigma} ( f_{\mu'\nu ' , \sigma'}+f_{\nu'\sigma ' , \mu'}+f_{\sigma'\mu ' , \nu'} ) =0 . $ however , this can only hold if the thing inside the brackets is zero itself . namely maxwell 's equation in the primed coordinate system also holds . more succintly , what a " tensor equation " means is that there was nothing special about the coordinate system in which the equations were derived . you could have equally well chosen another system and derived the same equations . thus invariance under coordinate change is immediate .
the answer is yes . what you describe is the quantum mechanical picture behind the elastic rayleigh scattering . for the actual calculation , see r . loudon , the quantum theory of light , chapter 8.8 .
this is a very complex problem to solve so you will probably want to start with some simplifying assumptions such as n=2 to make it more tractable . you will be looking for a minimum energy solution where the energy is a combination of the gravitational potentials and the energy in the surface tension . depending on parameters there may be some meta stable solutions where energy is locally minimum but not globally . for example a state in which the fluids all form layers with horizontal separation boundaries ordered by density with the densest at the top would be at least metastable because any perturbation such as a distortion of the boundary would increase both types of energy . however , if one of the layers is sufficiently small in volume there may be a preference for the liquid in that layer to form a bubble between the layers above and below . this would depend on the surface tensions between the three layers . even with just two fluids there may be bubbles formed for either the top or bottom layer . working out the shape of the bubble to provide the minimum energy could be non-trivial . with more liquids the number of odd arrangements you need to look at is going to grow . you have to consider that a heavy fluid may prefer to form a bubble above a lighter one if the surface tensions make that a lower energy configuration . in all finding the optimal solution will be a mixture of working through large numbers discrete cases and then optimising the shape of the surface areas . you should perhaps be thinking in terms of how this can be done numerically on a computer rather than an analytical solution . you will want to think about whether the problem is np hard or not . some simplifying assumptions may help but i cant see any physical reason why anything like your possible relationship between surface tensions should hold .
$su ( 2 ) $ has irreducible unitary representation of every spin $0,1/2,1,3/2 , \dots$ . indeed , the spin $j$ is just the historical way of recording the dimension $1+2j$ of the representation space of an irreducible unitary representation . on the other hand , the quantum numbers of $su ( n ) $ ( characterizing its irreducible unitary representations ) are significantly more complicated than a single spin . for example , $su ( 3 ) $ is physcally associated with flavor or color , not with spin .
no . there are numerous well-known mass distributions that start out non-singular , and which collapse to form a black-hole after a finite amount of time . black hole solutions , however , have a region of infinite density and infinite spacetime curvature , so they are not at all well-behaved in any ordinary sense .
a matter distribution with a sinsuoidally varying monopole or dipole moment will only produce variations in the gravitational field within the matter distribution . if the quadrupole ( or a higher multipole moment ) varies sinsuoidally , you will produce gravitational waves in a way very analogous to how electromagnetic waves are produced , with the amplitude differing by just a few numeric constants . i would not expect the back-reaction of the wave to amplify the variation that created the wave in the first place , though . is this what you mean by ' a resonance ' ?
the curvature radius is increasing at a rate of the speed of light . to see how fast a comoving observer is receding from you ( assuming you are a comoving observer as well ) , you need the hubble parameter and the observer 's distance . $$ v=h\cdot d $$ $$ h = \dot r/r $$ you can see that everything past the hubble radius $d_h = c/h$ recedes faster than the speed of light , independent of the content of the universe . why do not you read up on it , there is an excellent paper by davis and lineweaver about common misconceptions .
not exactly . fusion of atoms in supernova nucleosynthesis is thought to be responsible for the various atoms that make up the periodic table . while there has not been one in our part of the galaxy for quite some time , plenty of supernova are occurring through out the universe right now . so , while you are made of old stuff , in terms of atoms , most of it probably is not as old as the big bang itself . “the nitrogen in our dna , the calcium in our teeth , the iron in our blood , the carbon in our apple pies were made in the interiors of collapsing stars . we are made of star stuff . ” -carl sagan
there are several magazines and websites for the amateur astronomer that include calendars of astronomical events and viewing charts , etc . you might want to start at sea and sky 's astronomy calendar of celestial events for calendar year 2012 or sky and telescope 's this week 's sky at a glance .
most of the astronomy images we find online have some color modification how close to the false color images would they be this is a common misconception , that the pictures you see of galaxies and nebulae are necessarily " false color " , " modified color " , or " photoshopped " . some of them , yes . but a lot of them are quite simply true color , but taken with a sensor ( cmos , ccd ) that does not suffer from the limitations of the human eye . e.g. look at this image of the horsehead nebula : all that color is real . it is there , in the photons reaching you . but your eye cannot see it . a cmos , however , can . this is not " false color " , although saturation was likely increased in post-processing , in addition to what the sensor can do . but the hues are probably real ( e . g . , the red you see in the image , or the blue , was present in the photons hitting the sensor - albeit at a lower saturation level ) . ( an astute observer may object that the eye and the cmos do not see the exact same hues , but let 's not go down that rathole now . ) " false color " means when the image shows green where the cmos ( or the human eye , if luminosity was higher ) would see red , or something like that . this is not always the case with images of nebulae and galaxies ; in fact , if the image was taken with visible light , chances are the hues are preserved . proper false color images are those taken in uv or ir , and then artificially converted to visible light . this is an example of it , the sun in ultraviolet : now , to answer your question : unfortunately , even from a near distance , most of these objects will not look much better . they are , after all , faint , rarefied clouds of dust and gas . they are just not bright enough for the human eye to see color . there are few exceptions . a notable one would be close binary systems where the components are stars of very different temperatures . kind of like albireo , but much closer . from a starship , looking at the two stars orbiting each other , you had see very clearly a striking color difference - perhaps a large , somewhat dim , deep red star , and a blinding , crisp dot of bluish white light , the smaller and more active companion . the views from the center of a globular cluster undergoing a compression phase should be pretty spectacular , too . night would never be dark on a planet in the middle of the cluster .
how can i prove thevenin 's and norton 's theorem ? here 's an outline - you can fill in the dots . measure the voltage ( with an ideal voltmeter ) between any two nodes of an arbitrary linear circuit . call this voltage the open circuit voltage $v_{oc}$ since there is zero current through an ideal voltmeter . then , place an ideal ammeter across the same two nodes and measure the current . call this current the short circuit current $i_{sc}$ since there is zero voltage across an ideal ammeter . now , you have the voltage when there is zero current and the current when there is zero voltage . so , you have two points on the current-voltage ( iv ) plot for these two nodes . as you know , it takes two points to uniquely identify a line in this plane and the equation for this line is $$v ( i ) = v_{oc} - \frac{v_{oc}}{i_{sc}}i$$ can you take it from here ? update 1 to respond to a comment . @alfredcentauri , how do we prove that the characteristic is a straight line ? if the circuit is linear , the superposition theorem holds and , thus , any circuit voltage or current can be expressed as a sum of terms , each term involving one source and equal to the circuit voltage or current due to that source alone , i.e. , the result obtained by zeroing all other sources . in the previous section , we measured the voltage across two terminals of a circuit and labelled that voltage $v_{oc}$ . by superposition , and assuming a dc circuit , if we connect a current test source across these terminals such that $i = i_s$ , the voltage across the terminals is given by $$v = v_{oc} - r_{eq}i_s$$ where $r_{eq}$ is the equivalent resistance between the terminals when all the circuit sources are zeroed ( with all circuit sources zeroed , there is just a resistor network between the terminals ) . thus , by superposition , we know that for a linear dc circuit , there is , between any two terminals , an equivalent circuit with identical terminal characteristics : a voltage source with voltage $v_{oc}$ in series with a resistor with resistance $r_{eq}$ and , since there is just one line between the two measured points in the i-v plane found in the previous section , it follows that $$r_{eq} = \frac{v_{oc}}{i_{sc}} $$ update 2: to verify the validity of my arguments above , i found a formal proof of thevenin 's theorem in one of my undergrad textbooks : " fundamentals of circuits , electronics , and signal analysis " by kendall l . su . i will excerpt and paraphrase this proof found in the appendix a . 1 on page 568 to simplify the proof , we shall assume that the network in question is excited by an independent current source [ $i_{sn}$ ] at its terminal pair ( terminals a and b in figure a . 1 ) . furthermore we shall assume that the network contains $n-1$ independent current sources and $m$ independent voltage sources , and a number of lti elements , including lti controlled sources . since the network is lti , the superposition property prevails . that is to say , $v_{ab}$ is a linear combination of all the strengths of the independent sources . this fact can be expressed analytically as $$v_{ab} = \sum_{j=1}^{n-1}z_{nj}i_{sj} + \sum_{k=1}^{m}h_{nk}e_{sk} + z_{nn}i_{sn} = v_{oc} + z_{nn}i_{sn}$$ $$v_{oc} = \sum_{j=1}^{n-1}z_{nj}i_{sj} + \sum_{k=1}^{m}h_{nk}e{sk}$$ $$z_{nn} = \frac{v_{ab}}{i_{sn}} , e_{sk}=0 , i_{sj}=0$$ . . . the circuit of figure a . 2 [ a voltage source with voltage $v_{oc}$ in series with $z_{nn}$ connected between terminals a and b where the source $i_{sn}$ is connected ] has exactly the relationship described by $$v_{ab} = v_{oc} + i_{sn}z_{nn}$$ the current source $i_{sn}$ cannot tell the difference between [ the actual circuit and the equivalent circuit ] . hence the two circuits are equivalent electrically .
assuming the train does not accelerate during the ball 's fall , it will land in the spot you aimed at . think about it this way . before you drop the ball , it is moving along with the train ( i.e. . it has some horizontal speed ) . when you drop it , the ball still has this speed , and since an object in motion tends to stay in motion unless you exert a force on it ( newton 's first law ) , the ball will continue to move at this horizontal speed as it falls . thus it will land exactly where it would if the train were at rest and so the observer will not be able to figure out he is in a moving reference frame . this actually speaks to something much deeper : namely that physics behaves the same in any inertial reference frame ( a reference frame moving with constant velocity ) . there is thus no concept of " absolute motion . " the train is moving with respect to the earth , but that is no different from the train being at rest and the earth moving underneath it . of course this all assumes that the train does not accelerate during the ball 's fall . if the train accelerates , then the ball will still move as it would have if the train had not accelerated . thus in this case the ball may land elsewhere than where you aimed it , and an observer can figure out in this case that the train is accelerating .
note , that , on the third line , the $\beta$ indice of $\sigma^{\mu\nu}$ and the $\dot \beta$ indice of $\tilde \sigma^{\mu\nu}$ must be raised for indice coherence . same error for the following lines . the formula you want to demonstrate is certainly false . take $\beta = \nu = 0$ , and noting that $\sigma^{\alpha0}= -\frac{1}{2}\sigma^\alpha , \tilde \sigma^{\mu0}= \frac{1}{2}\sigma^\mu $ , if the formula was exact , it would imply ( with $g_{11}=g_{22}=g_{33}$ ) , and in short tensorial notation : $-\frac{1}{4} \vec \sigma \otimes \vec \sigma = 0\tag{1}$ which is obviously false .
potential energy is a property of the system , not any one object . thus there should only be one copy of the typical $1/r$ potential energy between two charges ( plus an analogous gravitational term if that can not be neglected ) . the easiest way to see this is to start from " infinite " separation . instead of pushing the two charges together , hold one fixed and move the other toward it . the moving charge must fight the standard coulomb force ( with a little help from gravity ) to get closer to the stationary one , so the potential energy obtained here is just the integral of this force over the distance traversed ( $d$ to $\infty$ ) . but what about the stationary object ? well , sure , we need to exert a force on it to keep it from being repelled by the approaching charge . but it is not moving , so the change in $\vec{f} \cdot \vec{x}$ energy vanishes . the fact that at some point in the future we will let both objects move does not change the potential energy , so you should get the same potential energy as if the problem were stated : a point mass $m_1$ with charge $q_1$ is fixed at the origin . another point mass $m_2$ with charge $q_2$ is brought in from infinity . what is the potential energy of the system ? it may also help to remember that "$2\infty = \infty$ . " moving objects from $x = -\infty$ and $x = \infty$ to the origin covers the same distance as moving one object from $x = \infty$ to the origin .
i am not completely sure , but i think they are introduced by gibbs , and that book ( available for download ) is of historic importance . the word ensemble really just means " set " in french , you consider the space of canonical coordinates of the detailed mechanics = mircostates and you impose statistics by the fundamental postulate .
for a static situation ( i.e. . no charges moving or current flowing ) the net electric field is always zero inside of a conductor . where by ' inside ' i mean actually inside the material itself , not a region of free space that is simply enclosed by the material . the reason for this is , say there is an electric field $\vec{e}_0$ , applied to the conductor- by definition there is an essentially infinite well of free charge that can move . this charge will move in response to the applied electric field and set up its own induced field , $\vec{e}_{ind}$ . this field will oppose the orignal field and charge will keeping moving until it reaches equilibrium such that $\vec{e}_0+\vec{e}_{ind}= \vec{e}_{net} = 0 $ . in practice this happens almost instantaneously . above i have not mentioned anything about shapes or charges outside the conductor so this holds for all static electric field configurations , including the one you describe above .
a translation by $x^\nu \to x^\nu - \epsilon^\nu$ corresponds to an infinitesimal transformation of the fields , by $$\phi \to \phi + \epsilon^\nu \partial_\nu \phi$$ as we are performing an active rather than passive transformation . the lagrangian transforms as , $$\mathcal{l}\to \mathcal{l}+\epsilon^\nu \partial_\nu \mathcal{l}$$ by substituting $\phi$ into the lagrangian . notice the change is up to a total derivative , and hence noether 's theorem is applicable to the symmetry . the conserved current density is given by , $$j^\mu = \frac{\partial\mathcal{l}}{\partial ( \partial_\mu\phi ) }x ( \phi ) -f^\mu ( \phi ) $$ where $x=\delta\phi$ and $f^\mu$ is such that $\partial_\mu f^\mu=\delta \mathcal{l}$ infinitesimally . for our case , we obtain the symmetric stress-energy tensor ( analogous to that of general relativity ) , $$t^\mu_\nu=\frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi ) } \partial_\nu \phi - \delta^\mu_\nu \mathcal{l}$$ where the kronecker delta is raised with the minkowski metric . the current satisfies , $\partial_\mu t^{\mu}_\nu = 0$ , and the corresponding noether charge , $$e=\int \mathrm{d}^3 x \ , t^{00}$$ is the total energy of the system , whereas , $$p^i = \int \mathrm{d}^3 x \ , t^{0i}$$ is the $i$th component of the total momentum of the field , where $i= ( x , y , z ) $ only . a caveat : the stress-energy tensor derived by noether 's theorem is not always symmetric , and may require the addition of a term which satisfies the continuity equation , and ensures symmetry in the indices . alternate method recall to obtain the einstein field equations in general relativity , we may vary the einstein-hilbert action , $$s\sim \int \mathrm{d}^4 x \ , \sqrt{-g} \ , \left ( r + \mathcal{l}\right ) $$ similarly , in quantum field theory , we may promote our minkowski metric to a generic metric tensor , thereby replacing the kinetic term of the lagrangian with covariant derivatives . up to some constants , the stress-energy tensor is given by $$t^{\mu\nu} \sim \frac{1}{\sqrt{-g}} \frac{\partial ( \sqrt{-g}\mathcal{l} ) }{\partial g^{\mu\nu}}$$ evaluated at $g_{\mu\nu}=\eta_{\mu\nu}$ , which is precisely the definition we implement when obtaining the einstein field equations for general relativity .
no , it does not have to be numbered unless , like @dmckee comments , you have received specific instructions to do the numbering . i think your question can best be split up in 2 parts for the answer : writing a report and writing a research paper . writing a report for a report the main goal is typically to show exactly what you have done and how you have done it . quite often the report will be used by other students to continue your work . for this purpose it is convenient to have a numbered list for the methods , because it immediately attracts attention and makes it easy to follow the ' recipe ' . writing a research paper when you write a research paper your goal is in general to resolve a particular issue that exists in the scientific community . your focus will be on the research question and the conclusions you can draw from the experiments/simulations that you did . in this case a numbered list for the method will draw way too much attention to it , much more than it deserves . it is , after all , easy to spot because it disrupts the flow of the paper . in some journals it is not even allowed to use numbered lists if they are not inline ( i.e. . 1 ) . . . 2 ) . . . ) . so in conclusion , if you do not have specific instructions from someone ' higher up': use the numbered list if you want to focus on the method , use the paragraph if you want to focus on a different part of the report/paper
this is exactly the point of the symmetry factor . let 's call the term in $z$ that we are considering $t$ . without considering the symmetric exchanges that produce the symmetry factor , the contribution of each diagram to $t$ is simply its associated term without any numerical factor in front ( a factor of 1 ) . this is because when we count every possible exchange of vertices , propagators , derivatives , etc . that leaves the feynman diagram invariant , this number neatly cancels out the factorials in the taylor expansion and our choice of 1/6 and 1/2 in the field lagrangian . if the symmetry factor for a diagram is 1 , each of these exchanges gives rise to an identical term in the $t$ . when a diagram has a symmetry factor that is not 1 , some of these exchanges mentioned above no give rise to additional terms . hence the contribution of that particular diagram must be divide by the symmetry factor $s$ . this is a confusing topic , i wrote a note specifically on this kind of counting here
the possibility of spontaneous lorentz symmetry violation due to the infrared problem of the dirac-maxwell equation was conjectured a long time ago by frohlich , morchio and strocchi , in references [ 1,2 ] mentioned in the given balachandran and vaidya article . in perturbative qed , we usually assume that the scattering states are free eigenstates of the number operator . but , due to the masslessness of the photon , this assumption is not true due to the infinite range of the electromagnetic force . this fact gives rise to the existence of nonfree asymptotic states for example an electron surrounded by an indefinite number of soft photons . this state can be represented by a photon coherent state ( eigenstate of the annihilation operators of the physical polarizations ) . in fact the inclusion of these ‘dressed” states solves the infrared divergence problem in perturbation theory . now for large gauge transformations which do not vanish at infinity , one can prove that the charge density of such a charged state is conserved at every direction . this is a direct consequence of the gauss law . ( please , see an explanation in the following wikipedia page ) . this implies that a charged vacuum can assume any charge distribution at infinity , which is the sign of a spontaneous lorentz symmetry breakdown since a lorentz boost modifies the charge distribution at infinity ) . the only way that this violation does not take place is when the total charge ( of the universe ) is zero .
in a $d$-dimensional euclidean space ( with positive definite norm ) , one has $$ \vec{\nabla} \cdot \frac{\vec{r}}{r^d} ~=~{\rm vol} ( s^{d-1} ) ~\delta^d ( \vec{r} ) , $$ cf . the divergence theorem and arguments involving either test functions and integration by part , or $\epsilon$-regularization , similar to methods applied in this phys . se answer . here ${\rm vol} ( s^{d-1} ) $ is the surface area of the $ ( d-1 ) $-dimensional units sphere $s^{d-1}$ . by similar arguments one may show that the identity $$ \vec{\nabla} ( r^{2-d} ) ~=~ ( 2-d ) \frac{\vec{r}}{r^d} , \qquad d\neq 2 , $$ contains no distributional contributions in $d$-dimensional euclidean space . for the related questions in minkowski space , one suggestion is to introduce an $\epsilon$-regularization in the euclidean formulation , and then perform a wick rotation , and at the end of the calculation , let $\epsilon\to 0^+$ .
your problem lies in assuming that $$ \nabla^2 = \frac{\partial^2}{\partial r^2} + \cdots $$ this is not the case , you need to use $$ \nabla^2 = \frac{1}{r^2}\frac{\partial}{\partial r}\left ( r^2\frac{\partial}{\partial r}\right ) +\cdots $$ then will you obtain the correct answer of $-1/2$ .
yes . yes . yes . see below . the falkenhagen relation ( nb : paywall , but ( a ) it is on the first page of the " look inside " option and ( b ) your university 's library might have a copy ) suggests that $$\frac{\eta_s}{\eta_0}=1+a\sqrt{c}$$ where $\eta_s$ is the solution viscosity , $\eta_0$ the solvent viscosity , $a$ a constant that depends on the electrostatic forces on the ions , and $c$ the concentration of the solute . there are other approximations , e.g. ones that go to higher order $c$ , that account for larger concentrations , so the above may not be exactly what you need for whatever purposes you have .
if you can measure the input energy ( sun 's spectrum ) and compare it to the reflected energy ~30% ( earth 's reflected spectrum ) you can compute the absorbed energy ~70% ( planet 's absorption ) . isolating that absorbed energy to just the surface requires some approximations and a lot of instrument calibration . the sun ( in space ) has a spectrum of a black body at ~5700 k . when this energy hits the atmosphere , some is reflected ( called albedo ) and some is absorbed and later radiated out again at a different frequency as heat ( measured by ir spectroradiometers ) . the atmosphere is quite cool compared to the surface of the earth and it does not have a lot of mass for heating directly , but it is good at catching the longer wave heat infrared ( ir ) . so when the surface of the earth is measured from space about 50% of the ir ( heat ) energy passes through the clouds and 50% is reflected back ( green house effect ) . by comparing the sun 's input , to the measured albedo they can correct for some of that effect . it can also be calibrated against sensors on the ground which show what amount of energy makes it through the atmosphere . notice that $h_2o$ , $o_3$ ( ozone ) , $co_2$ and $o_2$ absorb noticeable amounts of the radiated energy ( absorption bands ) . in this image the yellow represents the spectrum outside the atmosphere like in the image above , with the red showing what frequencies and power strength reach the surface at sea-level . note how $o_3$ absorbs the higher frequency ( shorter wave-length ) ultra-violet and keeps us from getting too sun burnt ? so the ir radiation from space can be measured and compared to the model and ground based sensors . this image is a broad-spectrum interpretation of what the " brightness " or power would look like if we could see in this frequency band . this is not a spectrum view but rather a view of the total amount of energy seen reflected using very wide ir vision . the wide vision makes seeing details difficult as you can see there are mostly big blobs of different strengths of reflected ir power . this is where nasa 's camera is much more powerful because of a combination of filters and frequency resolution they can increase the dynamic range of the image dramatically and see much more than blobs ( see the detail in the lut desert image below ) . where the current model which is always being refined by new information looks something like this . the specific instrument that nasa uses is called moderate resolution imaging spectroradiometer ( modis ) and there is an entire team to support calibration . the modis instrument has 36 different spectral bands ( groups of wavelengths ) , including some that detect thermal radiance , or the amount of infrared energy emitted by the land surface . since the two modis instruments scan the entire surface each day , they provide a complete picture of earthly temperatures and fill in the gaps between the world’s weather stations—particularly in extreme environments where temperatures are simply not measured . the spectral band separation for the 36 modis bands is based on a complex system of dichroic beamsplitters , focal plane masks , and individual bandpass filters . there are four co-registered focal planes separated by three dichroic beamsplitters to cover the vis ( 400 nm to 550 nm ) , nir ( 650 nm to 950 nm ) , swir ( 1200 nm to 2250 nm ) , mwir ( 3600 nm to 4600 nm ) and lwir ( 6500 nm to 14500 nm ) . linear detector arrays , one for each band , are covered by bandpass filters of dimensions as small as 1 mm by 7 mm . the modis requirements include relative bandwidths as small as 1% , with tight tolerances on band center , band width , edge slopes , and out-of-band specifications . -- ieee vis -- visual spectrum , nir - near infrared , swir - short wave infrared , mwir - medium wave infrared , lwir - long wave infrared . the advantage of the modis sensor is most of the atmospheric temperatures are much lower than the surface of the earth . so using narrower ir filters and comparing different bands the near direct temperature of the earth 's skin can be measured ( assuming proper calibration ) . using this information directly they can make daily direct measurements . for example the lut desert was found to often be the hottest spot on the planet : the lut desert has an areal extent of about 80,000 km and contains several odd geomorphic distinctions making it a unique place . large areas of the lut desert regularly exceed 65.0°c , and the hottest spot on earth was detected in the lut five out of seven years . ir imaging in general is often used in industrial applications to find irregular sources of heat . it can be used to see objects in great detail where each pixel acts as a tiny thermometer allowing you to measure the temperature of every pixel on an image . this is how the modis works , but it is much more complicated . here is an example from a flir handheld device illustrating the concept with the left image in black and white with the reference spot highlighted , compared to the right image in ir with the same spot highlighted .
for simplicity of notation say $p = \frac{x - n}{x + n}$ given $\delta x$ is the uncertainty in x and $\delta n$ is the uncertainty in n then $\delta ( x - n ) $ = $\delta ( x + n ) = \sqrt {\delta ^2x + \delta ^2n}$ and therefore : $\delta p = p \sqrt{ ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x - n} ) ^2 + ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x + n} ) ^2}$ this is based upon equations 1b and 2b of the following reference : http://www.rit.edu/~w-uphysi/uncertainties/uncertaintiespart2.html
yes , the experiment is oversimplified , because the uncertainty principle is not about " disturbance through measurement " . although that is what heisenberg said ( one of the things he said ) , it turned out you can not interpret it that way in a very rigorous sense . whether there is something like " disturbance through measurement " that gives rise to an uncertainty relation is currently under heated debate in the quantum foundations community ( see the work of ozawa and recently some collaborators on the one hand and the work of busch , lahti and werner on the other , if you want , i can look up some references ) . that said , your opinion is correct in the sense that this is exactly how you derive the uncertainty relation . with position and momentum , one could ask the question " but why do not position and momentum commute " and then one can turn to the fourier transformation and remark that the uncertainty relation is something that holds for any wave ( water , electromagnetic , etc . ) , because the fourier transform tells us that a small wave packet must consist of a lot of frequencies and a wave with only one frequency is infinite in space , etc . now , since we have wave functions , we have this phenomenon in quantum mechanics as well . this means that indeed the uncertainty principle in our formalism does not require any measurement , it is an intrinsic property of the wave function in phase space . edit : even with your extended question , assuming everything else would hold , the uncertainty principle should still be there . it just tells you that the product of the variances of momentum and position are lower bounded , which comes from the wave-function itself . there is no reference to any measurement in the uncertainty principle other than that you need to measure to actually compare anything . being more concrete , i would say the following : given a state of an electron ( i.e. . a preparation scheme that prepares the exact same physical electron over and over again ) , you can measure momentum and you will obtain a probability distribution according to the wave function ( repeating the experiment multiple times ) . then , assuming you have no disturbance in measurement , you measure the position of the exact same state . in that case , this will also have some probability distribution according to the wave function . these two measured probability distributions have variances whose product is lower bounded . that is what the uncertainty principle tells you . the question of whether or not you could have information without disturbance ( at least asymptotically ) is still a matter of debate . . .
no , nothing in physics depends on the validity of the axiom of choice because physics deals with the explanation of observable phenomena . infinite collections of sets – and they are the issue of the axiom of choice – are obviously not observable ( we only observe a finite number of objects ) , so experimental physics may say nothing about the validity of the axiom of choice . if it could say something , it would be very paradoxical because axiom of choice is about pure maths and moreover , maths may prove that both systems with ac or non-ac are equally consistent . theoretical physics is no different because it deals with various well-defined , " constructible " objects such as spaces of real or complex functions or functionals . for a physicist , just like for an open-minded evidence-based mathematician , the axiom of choice is a matter of personal preferences and " beliefs " . a physicist could say that any non-contractible object , like a particular selected " set of elements " postulated to exist by the axiom of choice , is " unphysical " . in mathematics , the axiom of choice may simplify some proofs but if i were deciding , i would choose a stronger framework in which the axiom of choice is invalid . a particular advantage of this choice is that one can not prove the existence of unmeasurable sets in the lebesgue theory of measure . consequently , one may add a very convenient and elegant extra axiom that all subsets of real numbers are measurable – an advantage that physicists are more likely to appreciate because they use measures often , even if they do not speak about them .
put simply , yes waves interfere even if they are not directly aligned . in fact all waves interfere of a given type . interference is really just a re-statement of the superposition principle ; that is , given 2 waves , the resulting pattern is simply the sum of the two waves at all positions in the space . the first figure you provide shows how , in 1d , 2 waves that are in the same phase ( which you are calling ' parallel' ) will interfere constructively ; i.e. the sum of the two waves has a larger magnitude than either wave individually . the second shows how two waves in opposite phase will interfere destructively , i.e. the sum of the waves has a smaller magnitude than either . the phase difference for the first is $0^o$ , and the second is $180^o$ . consider all the other values , for example , $30^o$ . we can see the general behaviour from the sum+difference trig identities . for $\sin$ , we have : $$\sin ( u ) + \sin ( v ) = 2 \ , \sin\left ( \frac{u+v}{2}\right ) \ ; \cos\left ( \frac{u-v}{2}\right ) $$ but we have a constant phase difference , so $$v = u+2\gamma \\ u-v = -2\gamma$$ which is independent of $u$ , so for convenience create some constant $\eta$: $$\eta = 2\cos\left ( \frac{u-v}{2}\right ) = 2\cos ( \gamma ) $$ switching to $\theta$ , this leaves us with : $$\sin ( \theta ) + \sin ( \theta+2\gamma ) = \eta \sin\left ( \frac{\theta+\theta+2\gamma}{2}\right ) \\ = \eta \sin ( \theta+\gamma ) $$ what does that mean ? it means that the sum of two waves with the same period but a phase difference of $2\gamma$ will be exactly the same as one wave shifted by $1\gamma$ , scaled in amplitude by $\eta = 2\cos ( \gamma ) $ . when $\cos ( \gamma ) = 0$ , i.e. $\gamma = 90^o$ , and the phase difference is $2\gamma = 180^o$ , the scaling is zero and you get no wave out , perfect destructive interference . in 2d , you draw a triangle formed by each source and a chosen point on the 2d plane . we now have a 1d wave along each line from a source to the point . the phase difference between the two waves depends on the difference between the lengths of the two wave paths . this gives you , yes , a phase difference , and the waves will constructively or destructively interfere . looking at your diagram , the bold lines are say the peaks , so where they cross will be peaks . where the thin lines cross will be troughs . where thin and thick lines meet will be zero , so a straight line connecting those can be all zero , the destructive interference of the 1d example . to the illustrations ! the path difference , i.e. the greater or lesser distance travelled along a line from one source compared to the other looks like this : the contours are at $0$ and $\pm1$ . as you can see , the path difference is zero ( i.e. . the locus ) in a straight line along the centre . our scaling value $\eta$ depended on $\cos ( \gamma ) $ , and the phase difference there was $2\gamma$ . here , the phase difference is determined by the path difference ; if the path lengths are the same , and the path difference is zero , the phase difference is zero too . since $\gamma$ is a phase difference , we need the path difference as a proportion of the period of the wave . so if the period is 1/3 , the phase difference $p$ for a given path difference $d$ is : $$p = \frac{d}{1/3} \times 360^o = 3d \times 360^o$$ $\gamma$ is only half this , so $\gamma = 1.5d \times 360^o$ . we can go back and plot , instead of the path difference $d$ , the scaling factor $2\cos ( 540d^o ) $ ( i am really plotting in radians , i.e. $2\cos ( 4.5\pi d ) $ ) : so we can see that the centre line , where the phase difference is zero , has constructive interferece - the scaling is 2x here . the black bands are where the path differences give destructive interference , as the sources cancel out . hopefully you can see , then , that interference , both constructive and destructive , happens at all angles - not just along the ' parallel ' line between the two sources .
i agree that this might be better for chemistry stackexchange . however i will give you an short answer after doing a little searching . you should look at henry 's law which states that that the concentration , $c$ , of a gas in a liquid at a specific temperature is proportional to the partial pressure , $p$ , of that gas in the atmosphere above the liquid , $$ p=k_hc . $$ so the concentration will be the higher if $k_h$ is smaller . wikipedia also gives a table for a few common gasses of these $k_h$ constants at room temperature $ ( 298.15 k ) $ . for $co_2$ it is equal to $29.41\frac{atm}{mol/l}$ . however the constant of $o_2$ is equal to $769.23\frac{atm}{mol/l}$ , so you would need roughly 26 times higher partial pressure of $o_2$ to get the same concentration . and for nitrogen it is even roughly 56 times higher $ ( k_{h , n_2}=1639.34\frac{atm}{mol/l} ) $ . i do not know how many other gasses also have a relatively low henry 's law constant , $k_h$ . but i believe , if i did the conversion correctly , that ammonia has an even lower value of roughly ( the literature values deviate quite a bit ) $0.02\frac{atm}{mol/l}$ . edit : apparently ozonated water is being used medically . ozone has a henry 's law constant of about $100.30\frac{atm}{mol/l}$ , so it will escape the water eventually and therefore it has to be made before consumption . but i was not able to find a source that ozonated water is fizzy , so i do not know why it does for $co_2$ , it probably has something to do with the fact that it also forms carbonic acid .
here is a cute little trick i have often found pretty handy : just keep squaring your matrices until they are diagonal ! in this case you are going to have to make use of the standard identities of pauli matrices $$\left\{ \sigma_{i} , \sigma_{j}\right\} =2\delta_{ij}$$ you also need to make use of the fact that the different “species” of pauli matrices , $\sigma$ and $\tau$ , won’t “see” each other . in other words , when you’re working through the algebra , pauli matrices of different species can pass through each other as if they were scalars . anyways , the given hamiltonian is $$h = \eta_{k}\tau_{z}+b\sigma_{x}+\alpha k\sigma_{y}\tau_{z}+\delta\tau_{x}$$ as i mentioned above , we first square it : $$h^2 = \eta_{k}^{2}\tau_{z}^{2}+b^{2}\sigma_{x}^{2}+\left ( \alpha k\right ) ^{2}\sigma_{y}^{2}\tau_{z}^{2}+\delta^{2}\tau_{x}^{2}+2b\eta_{k}\tau_{z}\sigma_{x}+2\alpha k\eta_{k}\sigma_{y}\tau_{z}^{2}+2\delta b\sigma_{x}\tau_{x}+\delta\eta_{k}\left\{ \tau_{z} , \tau_{x}\right\} +\alpha kb\left\{ \sigma_{x} , \sigma_{y}\right\} \tau_{z}+\alpha k\delta\sigma_{y}\left\{ \tau_{z} , \tau_{x}\right\}$$ now , using the anticommutator identity for either species of pauli matrices , the above expression simplifies to $$h^2 = \eta_{k}^{2}+b^{2}+\left ( \alpha k\right ) ^{2}+\delta^{2}+2b\eta_{k}\tau_{z}\sigma_{x}+2\alpha k\eta_{k}\sigma_{y}+2\delta b\sigma_{x}\tau_{x}$$ for reasons that will become obvious shortly , we rearrange the above expression in the following way and square it $$\left ( h^{2}-\eta_{k}^{2}-b^{2}-\left ( \alpha k\right ) ^{2}-\delta^{2}\right ) ^{2}=\left ( 2b\eta_{k}\tau_{z}\sigma_{x}+2\alpha k\eta_{k}\sigma_{y}+2\delta b\sigma_{x}\tau_{x}\right ) ^{2}$$ expanding out that further we get $$\left ( h^{2}-\eta_{k}^{2}-b^{2}-\left ( \alpha k\right ) ^{2}-\delta^{2}\right ) ^{2} = 4b^{2}\eta_{k}^{2}\tau_{z}^{2}\sigma_{x}^{2}+4\left ( \alpha k\right ) ^{2}\eta_{k}^{2}\sigma_{y}^{2}+4\delta^{2}b^{2}\sigma_{x}^{2}\tau_{x}^{2}+4\alpha kb\eta_{k}^{2}\tau_{z}\left\{ \sigma_{x} , \sigma_{y}\right\} +4\delta b^{2}\eta_{k}\sigma_{x}^{2}\left\{ \tau_{x} , \tau_{z}\right\} +4\alpha k\eta_{k}\delta b\left\{ \sigma_{x} , \sigma_{y}\right\} \tau_{x}$$ once again , using the anticommutators identities we get $$\left ( h^{2}-\eta_{k}^{2}-b^{2}-\left ( \alpha k\right ) ^{2}-\delta^{2}\right ) ^{2}=4b^{2}\eta_{k}^{2}+4\left ( \alpha k\right ) ^{2}\eta_{k}^{2}+4\delta^{2}b^{2}$$ note that the above expression contains only diagonal matrices ; we have effectively diagonalized the hamiltonian . this can be made more explicit by writing $$h^{2}=\left [ \eta_{k}^{2}+b^{2}+\left ( \alpha k\right ) ^{2}+\delta^{2}\pm2\sqrt{b^{2}\eta_{k}^{2}+\left ( \alpha k\right ) ^{2}\eta_{k}^{2}+\delta^{2}b^{2}}\right ] \mathbb{i}_{4\times4}$$ now , from the above expression , it is not hard to figure out that $$e_{k}^{2}=\eta_{k}^{2}+b^{2}+\left ( \alpha k\right ) ^{2}+\delta^{2}\pm2\sqrt{b^{2}\eta_{k}^{2}+\left ( \alpha k\right ) ^{2}\eta_{k}^{2}+\delta^{2}b^{2}}$$ forgive me if you’re wondering : “with all this algebra , how is that a trick ? ” well , this was a tough example . but this trick is pretty general whenever your hamiltonian consists of matrices ( or their tensor products ) which satisfy the clifford algebra . i’m sure you can pick a much simpler example where this trick will really be a trick . for example , you can check the hamiltonian in equation ( 51 ) of : martin leijnse and karsten flensberg . “ [ introduction to topological superconductivity and majorana fermions . ] [ 1 ] ” semiconductor science and technology 27 , no . 12 ( 2012 ) : 124003 . ( [ arxiv ] [ 2 ] ) where you can simply compute the eigenvalues ( equation ( 52 ) ) in your head using this trick .
molten solder has a low contact angle on ( clean ) copper . so if you looked at a cross section of the pipe joint as the solder was flowing in you had see something like : the solder is drawn into the joint in exactly the same way as water rises in a capillary tube . both are correctly described as capillary action .
the symbol $k_\textrm{b}$ pretty much invariably denotes boltzmann 's constant . apart from that , your question is asking about the statistical mechanics fact that for a canonical ensemble ( i.e. . a physical system in contact with a heat bath at some temperature $t$ ) the probability for the system to have energy $e$ is equal to $$p = \frac{1}{z}e^{-e/k_\textrm{b}t}$$ where $z$ is a normalization factor known as a partition function .
a voltage or current given as a complex constant is a phasor . a voltage given as the complex constant $v_z$ represents the real voltage $$v ( t ) = \operatorname{re} \left ( v_z e^{i\omega t} \right ) \ \ , $$ where $\omega$ is the voltage 's angular frequency and $t$ is time . currents represented as phasors work the same way .
i was also wondering this for a while and found an not entirely complete derivation of the formula ( starting from page 14 ) . in which the following equation is used , $$ \ddot{\vec{r}}+\underbrace{\frac{\mu_i}{\|\vec{r}\|^3}\vec{r}}_{-a_i}=\underbrace{-\mu_j\left ( \frac{\vec{d}}{\|\vec{d}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) }_{p_j} , $$ where $\vec{r}$ is the vector between the centers of gravity of a spacecraft and the celestial body with gravitational parameter $\mu_i$ , $\vec{d}$ is the vector between the centers of gravity of a spacecraft and the celestial body with gravitational parameter $\mu_j$ and $\vec{\rho}$ is the vector between the centers of gravity of celestial body $\mu_i$ and $\mu_j$ . and looking at the spacecraft from an accelerated reference frame of a celestial body than $a$ is defined as the primary gravitational acceleration and $p$ as the perturbation acceleration due to the other celestial body . and the soi is defined due to laplace as the surface along which the following equation satisfies , $$ \frac{p_j}{a_i}=\frac{p_i}{a_j} , $$ so $$ \frac{\mu_j\left ( \frac{\vec{d}}{\|\vec{d}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) }{\mu_i\frac{\vec{r}}{\|\vec{r}\|^3}}=\frac{\mu_i\left ( \frac{\vec{r}}{\|\vec{r}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) }{\mu_j\frac{\vec{d}}{\|\vec{d}\|^3}} . $$ this will not return a spherical surface , but it can be approximated by one when $\mu_i &lt ; &lt ; \mu_j$ , who is radius is equal to $$ \|\vec{r}\|\approx r_{soi}=\|\vec{\rho}\|\left ( \frac{\mu_i}{\mu_j}\right ) ^{\frac{2}{5}} . $$ this is where the slides of the lecture stop and i will try to to fill in the rest . when $\mu_i &lt ; &lt ; \mu_j$ than the soi will be relatively close to $\mu_i$ so $$ \|\vec{\rho}\|\approx\|\vec{d}\| , $$ and if you look at the figure on page 14 of the lecture sheets you can see that $\vec{d}$ and $\vec{\rho}$ almost point in opposite direction and form a triangle with $\vec{r}$ such that $$ \vec{\rho}+\vec{d}=\vec{r} . $$ by rewriting the definition of the surface using the approximation you get $$ \mu_j^2\frac{\vec{d}}{\|\vec{\rho}\|^6}=\mu_i^2\frac{1}{\|\vec{r}\|^3}\left ( \frac{\vec{r}}{\|\vec{r}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) $$ the other approximation which has to be made is that $\|\vec{r}\|&lt ; &lt ; \|\vec{\rho}\|$ so that $$ \frac{\vec{r}}{\|\vec{r}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\approx\frac{\vec{r}}{\|\vec{r}\|^3} . $$ now the equation can be reduced to $$ \mu_j^2\frac{\vec{d}}{\|\vec{\rho}\|^6}=\mu_i^2\frac{\vec{r}}{\|\vec{r}\|^6} . $$ by generalizing $\vec{r}$ as a constant radius can make this problem one dimensional , so $\vec{r}=\|\vec{r}\|$ , which returns to final equation $$ \mu_j^2\|\vec{r}\|^5=\mu_i^2\|\vec{\rho}\|^5\longrightarrow\|\vec{r}\|=\|\vec{\rho}\|\left ( \frac{\mu_i}{\mu_j}\right ) ^{\frac{2}{5}} . $$
it is a standard terminology – and set of insights – not only in string theory but in quantum field theories or anything that can be approximated by ( other ) quantum field theories at . . . low energies . such a low-energy action becomes very accurate for the calculation of interaction of particles ( quanta of the fields ) of low energies , in this case $e\ll m_{\rm string}$ . equivalently , the frequencies of the quanta must be much smaller than the characteristic frequency of string theory . the previous sentence may also be applied in the classical theory : the low-energy effective action becomes accurate for calculations of interactions of waves whose frequency is much lower than the stringy frequency or , equivalently , whose wavelength is much longer than the string scale , $\lambda\gg l_{\rm string}$ . low-energy effective actions may completely neglect particles whose mass is ( equal to or ) higher than the characteristic energy scale , in this case $m_{\rm string}$ , because such heavy particles can not be produced by the scattering of low-energy particles at all – so they may be consistently removed from the spectrum in this approximation . the scattering of the light and massless particles that are kept may be approximately calculated from the low-energy effective action and this approximation only creates errors that are proportional to positive powers of $ ( e/m_{\rm string} ) $ so these errors may be ignored for $e\ll m_{\rm string}$ . you may imagine that there are corrections in the action proportional to $\alpha'$ or its higher powers that would make the effective action more accurate at higher energies but become negligible for low-energy processes . there are lots of insights – conceptual ones as well as calculations – surrounding similar approximations and they are a part of the " renormalization group " pioneered mainly by ken wilson in the 1970s . in particular , by " low-energy effective actions " , we usually mean the wilsonian effective actions . but they are pretty much interchangeable concepts to the 1pi ( one-particle-irreducible ) effective actions , up to a different treatment of massless particles . it is impossible to teach everything about the renormalization group and effective theories in a single stack exchange answer . this is a topic for numerous chapters of quantum field theory textbooks – and for whole graduate courses . so i just conclude with a sentence relevant for your stringy example : string theory may be approximated by quantum field theories for all processes in which only particles much lighter than the string mass are participating and in which they have energies much smaller than the string scale , too . if that is the case , predictions of string theory for the amplitudes are equal to the predictions of a quantum field theory , the low-energy effective field theory , up to corrections proportional to powers $ ( e/e_{\rm string} ) $ .
netwon 's law of cooling i think is exactly what you want to look at . roughly , heat exchange is porportional to the difference in temperature between the beer and the cooling apparatus ( be it water or something else ) and also to the contact area between the two . a secondary effect is the mixing of the beer . if the cold beer sits at the outer walls , the temperature difference is reduced and the middle will never cool . the reverse goes for the cooling liquid .
studies have been done at a few popular frequencies , but in general this is hard to do with rf . you can get a feel for how it was done at 2ghz and 5ghz from this article http://www.ko4bb.com/manuals/05%29_gps_timing/e10589_propagation_losses_2_and_5ghz.pdf they also publish tables of their results which you might be able to scale to other frequencies as a starting point .
to calculate fuel consumption , you can typically use the tsiolkovsky rocket equation shown here ( without taking relativity into account ) : $$ \delta v = v_e * ln ( \frac{m_0}{m_1} ) $$ $m_0$ is the initial total mass , including propellant , $m_1$ is the final total mass , $v_e$ is the effective exhaust velocity , and $\delta v$ is the maximum change in the speed of the vehicle ( with no external forces ) the $\delta v$ from earth 's surface to leo from kennedy space center is $9.3 - 10\ ; km/s$ , and leo ( kss ) to geo is $4.24\ ; km/s$ . source : delta-v budgets , earth-moon space , high thrust assuming an exhaust velocity of 4.5 km/s , single stage rocket , we get $$ \frac{m_0}{m_1} = e^{\delta v/v_e} = e^{9.3/4.5} = 7.90 $$ $$ m_{propellant} = ( 1 - \frac{m_1}{m_0} ) m_0 = ( 1 - \frac{1}{7.90} ) m_0 = ( 1 - 12.66\% ) m_0 = 87.3\%\ ; m_0 $$ just getting the rocket to leo takes 87.3% of your initial mass . now let 's try this again for leo -> geo : $$ \frac{m_1}{m_2} = e^{\delta v/v_e} = e^{4.24/4.5} = 2.57 $$ $$ m_{propellant} = ( 1 - \frac{m_2}{m_1} ) m_1 = ( 1 - \frac{1}{2.57} ) m_1 = ( 1 - 38.98\% ) m_1 = 61.02\%\ ; m_1 $$ however , this is a percentage of the initial mass at leo , which is 12.66% of the original . let $m_1$ be the new initial mass for the rocket equation achieved after reaching leo ( from above ) and $m_2$ be the final mass after reaching geo . the fraction of launch mass $m_0$ would be $$ \frac{m_2}{m_0} = \frac{m_2}{m_1}*\frac{m_1}{m_0} = \frac{1}{2.57}*\frac{1}{7.90} = . 3898 * . 1266 = 4.93\% $$ $$ m_{propellant} = ( \frac{m_1}{m_0} - \frac{m_2}{m_0} ) m_0 = ( 12.66\% - 4.93\% ) m_0 = 7.73\%\ ; m_0 $$ thus , getting from earth 's surface -> leo takes about 87.3% of the mass of whatever you launch ejected at 4.5 km/s , where leo -> geo takes only 7.73% of of that mass at 4.5 km/s , leaving you in geo with 4.93% of what you started with . note : because of the difficulty from earth-> leo , we typically use multi-staged rockets which can ease the fuel required . because you said ' a rocket ' , i took that literally and did the calculation with a single-stage-to-orbit configuration .
it fits remarkably well . one of the defining features of a cosmological constant is its equation of state . the equation of state , $w$ , is given by $p \over \rho$ , where $p$ is the pressure it contributes , and $\rho$ is the energy density . a cosmological constant has $w=-1$ . the wmap seven year report recorded the value as $w=-1.1 ± 0.14$ . within the error margins , the cosmological constant fits very well .
spinor algebra is very helpful in sorting out spinorial equations . in chiral representation the ( four-component ) wave function of the fermion field $\psi$ is considered as a formal sum of first rank spinor and first rank co-spinor fields : $\psi{} ( x ) =\left\{\xi{} , \ \dot{\eta{}}\right\}=\ \left\{\xi{} , \ 0\right\}+\left\{0 , \ \dot{\eta{}}\right\}=\ \left ( {\begin{array}{ cc} {\xi{}}^1 \\ {\xi{}}^2\\ {\eta{}}_{\dot{1}} \\ {\eta{}}_{\dot{2}} \end{array}}\right ) $ any quantities transforming like the products ${\xi{}}^{\mu{}}{\xi{}}^{\nu{}}$ , ${\eta{}}_{\dot{\mu{}}}{\eta{}}_{\dot{\nu{}}}$ , ${\xi{}}^{\mu{}}{\eta{}}_{\dot{\nu{}}}$ are called second rank spinors and denoted by $a^{\mu{}\nu{}}$ , $b_{\dot{\mu{}}\dot{\nu{}}}$ , $c_{\dot{\nu{}}}^{\mu{}}$ correspondingly . analogously one can define the spinors of higher ranks . there are 3 lorentz-invariant constant second rank spinors ${\epsilon{}}_{\mu{}\nu{}}$ , ${\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}$ and $c_{\mu{}\dot{\nu{}}}$ that play important role in spinor algebra . 1 . transition from subscript to superscript spinor indices spinors ${\epsilon{}}_{\mu{}\nu{}}$ and ${\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}$ are written as $ {\epsilon{}}_{\mu{}\nu{}}=\left [ \begin{array}{ cc} 0 and +1 \\ -1 and 0 \end{array}\right ] \hspace{10mm} {\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}=\left [ \begin{array}{ cc} 0 and -1 \\ +1 and 0 \end{array}\right ] $ transition from subscript to superscript spinor indices is established by means of spinors ${\epsilon{}}_{\mu{}\nu{}}$ and ${\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}$: $ {\xi{}}_{\mu{}}=\ {\epsilon{}}_{\mu{}\nu{}}{\xi{}}^{\nu{}} , \ \hspace{10mm} {\eta{}}^{\dot{\mu{}}}=\ {\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}{\eta{}}_{\dot{\nu{}}} $ 2 . complex conjugated spinors complex conjugates of spinors transform as co-spinors , and vice versa , so that we can denote \begin{array}{ cc} {\xi{}}_{\dot{\mu{}}}=\bar{{\xi{}}}_{\mu{}} \\ \\ {\eta{}}_{\nu{}}=\bar{{\eta}}_{\dot\nu} \end{array} 3 . charge conjugation second rank spinor $c_{\mu{}\dot{\nu{}}}$ ( often denoted as $i\sigma_2$ ) has the form $ c_{\mu{}\dot{\nu{}}}=\left [ \begin{array}{ cc} 0 and +1 \\ -1 and 0 \end{array}\right ] $ and can be used to transform first rank spinors to co-spinors and vice versa ( charge conjugation ) : $ {\chi{}}_{\dot{\mu{}}}=\ c_{\nu{}\dot{\mu{}}}{\xi{}}^{\nu{}} $ 4 . majorana condition majorana ( or neutrality ) condition is a lorentz-invariant property of electrically neutral spinors . it is expressed in the following form : $ {\eta{}}_{\dot{\mu{}}}=\ c_{\nu{}\dot{\mu{}}}{\xi{}}^{\nu{}} $ i.e. co-spinor $\dot{\eta{}}$ is charge conjugated to spinor $\xi{}$ . in spinor components this condition is written as follows : \begin{array}{c} {\xi{}}^1=- \ \bar{{\eta}}_{\dot 2} \\ \\ {\xi{}}^2=+ \ \bar{{\eta}}_{\dot 1} \end{array} 5 . mass term " mass term " in spinorial equations appears when there is " mixing " of spinor and co-spinor components . for instance , the free dirac equation in spinorial form can be written as \begin{array}{columns} {\partial{}}^{\mu{}\dot{\nu{}}} {\eta{}}_{\dot{\nu{}}}\ =\ -im \ {\xi{}}^{\mu{}} \\ \\ {\partial{}}_{\mu{}\dot{\nu{}}} {\xi{}}^{\mu{}}=\ -im \ {\eta{}}_{\dot{\nu{}}} \end{array} the most general form of manifestly lorentz-invariant spinorial equation with " mixing " is as follows : \begin{array}{cols} {\partial{}}^{\mu{}\dot{\nu{}}}{\eta{}}_{\dot{\nu{}}}=\ \ f_{\nu{}}^{\mu{}} \ {\xi{}}^{\nu{}} \\ \\ {\partial{}}_{\mu{}\dot{\nu{}}}{\xi{}}^{\mu{}}=\ \ {\dot{f}}_{\dot{\nu{}}}^{\dot{\mu{}}} \ {\eta{}}_{\dot{\mu{}}} \end{array} where $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ are second rank spinor and second rank co-spinor correspondingly . let us demonstrate that free dirac equation is just a special case of the more general equation presented above . the most general form of spinorial equation can be made similar to free dirac equation , if we require that spinor $\xi{}$ and co-spinor $\dot{\eta{}}$ are eigenvectors of second rank spinor matrices $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$: \begin{array}{ ccc} f_{\nu{}}^{\mu{}} \ {\xi{}}^{\nu{}}=\ \lambda{}\ {\xi{}}^{\mu{}} \\ \\ \\ {\dot{f}}_{\dot{\nu{}}}^{\dot{\mu{}}} \ {\eta{}}_{\dot{\mu{}}}= {\lambda{}}\ {\eta{}}_{\dot{\nu{}}} \end{array} here $\lambda{}$ is an eigenvalue . it is important to note that this " eigenvector " condition is lorentz-invariant . now our spinorial equation has the form : \begin{array}{cols} {\partial{}}^{\mu{}\dot{\nu{}}}{\eta{}}_{\dot{\nu{}}}= \lambda{}\ {\xi{}}^{\mu{}}\\ \\ {\partial{}}_{\mu{}\dot{\nu{}}}{\xi{}}^{\mu{}}=\ {\lambda{}}\ {\eta{}}_{\dot{\nu{}}} \end{array} which is very similar to free dirac equation . now the " type " of equation ( i.e. . dirac , majorana or weyl ) will only depend on the special choice of second rank spinor matrices $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ . in particular , we can choose $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ as $f_{\nu{}}^{\mu{}}=\left [ \begin{array}{cc} 0 and m \\ -m and 0 \end{array}\right ] $ $\dot{f}^{\dot \mu}_{\dot \nu}=\left [ \begin{array}{cc} 0 and m \\ -m and 0 \end{array}\right ] $ the eigenvectors corresponding to the eigenvalue ( $\lambda = - im$ ) will be : $ \xi_d = \left [ \begin{array}{c} 1 \\ \\ -i \end{array}\right ] \phi ( x ) $ $ \dot\eta_d = \left [ \begin{array}{c} 1 \\ \\ -i \end{array}\right ] \phi ( x ) $ this case corresponds to dirac equation . alternatively , we can choose $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ as $ f_{\nu{}}^{\mu{}}=\left [ \begin{array}{cc} im and 0 \\ 0 and -im \end{array}\right ] $ $ \dot{f}^{\dot \mu}_{\dot \nu}=\left [ \begin{array}{cc} -im and 0 \\ 0 and im \end{array}\right ] $ and the eigenvectors corresponding to the eigenvalue ( $\lambda = - im$ ) will be : $ \xi_m = \left [ \begin{array}{c} 0 \\ \\ 1 \end{array}\right ] \phi ( x ) $ $ \dot\eta_m = \left [ \begin{array}{c} 1 \\ \\ 0 \end{array}\right ] \phi ( x ) $ it is easy to check that spinors $\xi_m$ and $\dot\eta_m$ automatically satisfy majorana condition . hence , majorana equation is also the special case of the most general spinorial equation . both dirac and majorana spinors belong to $\left ( \frac{1}{2} , 0\right ) + \left ( 0 , \frac{1}{2}\right ) $ representations of $sl ( 2 , c ) $ , but they are only subspaces in the entire space of $\left ( \frac{1}{2} , 0\right ) + \left ( 0 , \frac{1}{2}\right ) $ representation . here you can read more about the most general form of spinorial equation . you will see that it can be used to develop the concept of electromagnetic mass and charge . to read more about spinors and spinorial algebra , you can read : laporte , o . and g . e . uhlenbeck , phys . rev . 37 , 1380 ( 1931 ) rumer , yu . b . and a.i. fet . group theory and quantum fields . moscow , ussr : nauka publisher , 1977
the " lift " produced by a sail is ( primarily ) directed horizontally . the term lift is used since the mechanism is the same as the one that produces lift on an aircraft wing . the key concept is that both wings and sails are airfoils ; the only difference is that wings are ( typically ) oriented horizontally , and sails are ( typically ) oriented vertically . due to the way it modifies the airflow around it , a horizontally oriented wing experiences a vertical lift force ; the sail modifies the airflow around it in exactly the same way ; however , because it is oriented vertially , the corresponding " lift " force is oriented horizontally . consider the usual type of picture of how a wing provides lift : imagine taking an airplane 's wing , and rotating it ninety degrees so that it is sticking straight up out of the ground . also imagine that there is wind blowing onto the wing . now , consider the above picture as though you were suspended above this vertical wing . due to the motion of the air across the wing , this upward pointing wing experiences a horizontal " lift " force . the only difference between the wing and the sail is that the base ( flatter ) side of the sail is not filled in ; what is going on is that there is a ( relatively ) stationary pocket of air sitting inside of the curve of sail , so that as the wind blows by it , it passes by the sail just like in the diagram above ( i.e. . imagine that the grey area is a stationary pocket of air ) . " the physics of sailing " page provides some useful diagrams spelling out the vectoral forces on sailboat . this link correctly describes the lift as being related to the change in momentum of the air in some places , but also refers to bernouli 's princple in others . its easy to conflate bernouli 's principle with the equal times fallacy , c.f. this question for more details on the mechanisms of lift .
the vast majority of research cyclotrons do not use the classical lawrence design anymore . lawrence 's cyclotron was in many ways the simplest circular accelerator you can build , and later designs are much more complex . that said , we can still consider a classical cyclotron with four accelerating gaps . to review a bit , the cyclotron uses a perpendicular magnetic field to keep accelerated charges within a circular region . as they circulate , the charges cross the gap between the " dees " and experience the electric fields there , which are timed to provide acceleration . for a given particle momentum , the orbit radius is given by the larmor radius : $$ r = \frac{p}{|q|b} . $$ most often the figure of merit that we care about is the energy . in the real world , we are limited by radius ( how big we make the cyclotron ) and the achievable magnetic field . clearly , for a given r and b , adding more accelerating gaps will not increase the maximum achievable energy . it will , as you point out , bring a particle up to speed more quickly . consequently , it will also increase the separation between orbits as the particles spiral outward . for most applications , the amount of time it takes to accelerate particles is not of concern . orbit separation was also not so important in the early days , since there was often no need to extract beam ( just put your target in the path of the spiral ) . so there was no compelling reason to have anything more complex than two dees . these days , though , the separation between orbits is very important because we often need to extract a beam of particles from a machine . you want good separation to ensure clean capture and low losses , which reduces radioactive activation of machine components . there are other issues ( relativistic effects , phase/orbit stability ) that have led to cyclotron designs that are a far cry from the classical cyclotron . for example , the 590 mev cyclotron at psi ( which is a " separated-sector " cyclotron ) uses four main rf cavities ( instead of dees ) that fit in between its eight magnets . the k1200 superconducting cyclotron at msu still has " dees " , but their shape is considerably different from a ' d ' .
first thing , for a rotating ball , $i=\frac{2}{5}mr^2$ . you also need to be clear on what $\omega$ you are talking about . the kinetic energy of a rotating ball is $\frac12 i_{cm}\omega_{cm}^2 + \frac12 mv_{cm}^2$ . here , $v_{cm}=v$ . but , $\omega_{cm}=v_{cm}\times \frac{r}{r}$ . since $r&lt ; &lt ; r$ , we can take the net kinetic energy to be just $\frac12 mv^2$ ; the $\frac12 i_{cm}\omega_{cm}^2$ term becomes too small to matter . the main thing is is that you need to remember that the formula " kinetic energy=rotational energy + translational energy " works only when you consider all rotations about center of mass . you cannot just keep tacking on terms for each motion you see . even though the ball is revolving around the center of the loop , we still classify this as translational motion . if you do not do this , you can easily get confused while building the expression for ke . basically , for a ball of center of mass moment of inertia $i$ , mass $m$ , radius $r$ , rotating about itself with $\omega_cm$ , revolving in a circle of radius $r$ with $\omega'$ , the energy is not $\frac12 i\omega^2+ \frac12 ( i+mr^2 ) \omega'^2+\frac12 mv^2$ , it is $\frac12 i\omega^2+ \frac12 mv^2=\frac12 i\omega^2+ \frac m ( \omega'r ) ^2$ .
in practice , the apparatus measuring the spin should be localized somewhere in space ( it cannot fill the whole universe ! ) and this fact implies that you always make a measurement of position ( actually very rough in general ) , even if you are measuring the spin . suppose that $\omega \subset r^3$ is the bounded region in $r^3$ where the apparatus is localized . the simplest ( naive ) mathematical model of the apparatus i could imagine is the following . the yes-no observable associated with the apparatus measuring , say , if the spin is directed along z+ , has the form of the orthogonal projector : $$p_{\omega} \otimes p_{z+}$$ here $p_{z^+} = |z+\rangle \langle z+|$ is the obvious projector in $c^2$ along the states with spin $z+$-directed , whereas $p_\omega$ is the operator ( orthogonal projector in $l^2 ( r^3 ) $ ) $$ ( p_\omega \psi ) ( x ) = \chi_\omega ( x ) \psi ( x ) \: . $$ this observable admits two values ( its eigenvalues ) $0=$ no and $1=$yes . yes means that the particle is found in $\omega$ and the spin is found to be directed along $z+$ . no means that the the particle is not found in $\omega$ or the spin is not along $z+$ . there is another elementary yes-no observable associated with the spin detected along the direction $-z$ , with analogous meaning . it is the orthogonal projector : $$p_{\omega} \otimes p_{z-}\: . $$ the observable associated with the spin along $z$ -- referring to this experiment -- is not the standard operator $s_z = \sigma_z/2$ ( i am assuming $\hbar =1$ ) . it is instead constructed , via spectral decomposition , taking the above elementary observables ( projectors ) into account and combining them with the corresponding values of the spin ( which turn out to be the eigenvalues of the overall observable ) . $$\gamma_{z , \omega} = \frac{1}{2}p_{\omega} \otimes p_{z+} - \frac{1}{2} p_{\omega} \otimes p_{z-} = p_\omega \otimes s_z\: . $$ you see that it includes a rough measurement of the position : it just checks if the position of the particle is in $\omega$ . to measure the spin of the particle the supports of the components $\phi_i$ must have a non-negligible intersection with $\omega$ . in general the measurement procedure of the spin , for instance along $z+$ , even affects the surviving component $\phi_{+1/2}$ . you see that , only if the support of $\phi_{+1/2}$ is completely included in $\omega$ , the wavefunction is not affected by the measurement of the spin , otherwise , after the procedure ( supposing to have found spin $+1/2$ ) , the state , up to a normalization constant , is described by : $$p_\omega \phi_{+1/2} \otimes |z+\rangle \: . $$ it is questionable if we have defined an observable $\gamma_{z , \omega}$ in that way . the point is that $\gamma_{z , \omega}$ admits a third eigenvalue , $0$ , associated with the projector $p_{r^3-\omega} \otimes i$ . actually , there is no real measurement in the region $r^3-\omega$ , since we are not assuming that there are detectors therein . we are simply using the argument : " if the particle is not found in $\omega$ it must be found outside it " . for several reasons i am always a bit suspicious to this sort of formal arguments . a more physically safe interpretation could be that we are performing a conditioned measurement of the spin . $p_\omega$ is nothing but a filter : only the particles which pass through it are measured .
http://www.antenna-theory.com/antennas/shortdipole.php is a website with useful info . , including formulas . to oversimplify , it seems to say that once the antenna is a tenth or less of the wavelength , the exact ratios do not matter so much . the antenna is inefficient , but it works for both sending and receiving . if you can detect the signal , of course you can amplify it as much as you want .
firstly , note that they postulate those commutation relations in the beginning of section 3.5 in order to show that they are wrong , which they demonstrate in the ensuing pages . the ultimate point is to show that one needs to impose anti-commutation relations on fermionic fields . in fact , the correct relations are postulated in equation 3.96 ; \begin{align} \{\psi_a ( \mathbf x ) , \psi_b^\dagger ( \mathbf y ) \} and = \delta^{ ( 3 ) } ( \mathbf x - \mathbf y ) \delta_{ab} \end{align} you could then ask , are these equivalent to the anti-commutation relations of the mode operators that they write in ( 3.97 ) ? namely , \begin{align} \{a^r_\mathbf p , {a^s_\mathbf q}^\dagger\} = \{b^r_\mathbf p , {b^s_\mathbf q}^\dagger\} = ( 2\pi ) ^3\delta^{ ( 3 ) } ( \mathbf p - \mathbf q ) \delta^{rs} \end{align} and the answer is yes . to show that the second set implies the first , write the fields in their integral mode expansions , compute the anti-commutator of these integral expressions , and apply the anti-commutators between modes . to show that the first set implies the second , invert the integral expressions for the fields in terms of the modes to obtain integral expressions for the modes in terms of the fields , and do the analogous thing . main point . the commutators/anti-commutators between fields are equivalent to the commutators/anti-commutators between modes .
you can analyze this by imagining a tiny tiny gap between the two masses . physically we have exactly that , as the electrons at the surface of the first block are certainly not in contact with the electrons at the surface of the second block . then we have a series of two collisions : the first is the initial impulse , the second is the collision between the two objects . the answer will differ , of course , depending on the degree of elasticity of the collision .
due to the law of conservation of angular momentum they will only orbit if there is angular momentum in the initial conditions . if they start at rest , there is none and so they will collide . since we are supposed to provide links and whatnot , here 's the wikipedia entry . http://en.wikipedia.org/wiki/orbit part of the difficulty in answering this question , however , is that there are many different kinds of orbits and the details of the trajectories depends on the relevant masses and linear and angular momentums .
large inflatable balls such as soccerballs , footballs and basketballs have an internal rubber bladder which needs to be inserted by hand into the carcass of the ball and inflated to the desired pressure to suit the user ( eg : basketballs can be inflated to the produce the desired bounce height for the individual user ) . also , transporting deflated balls from the factory to distributors who package and pass onto retailers saves shipping volume and hence cost . small balls such as tennis balls can be pressurized in the factory , but will lose pressure within a few onto or so after being opened ( they come in a pressurized can ) . also , the higher curvature of small balls allows them to ' spring back ' more easily when they bounce , even when they are not pressurized , although unpressurized balls do not bounce as high as inflated balls .
the acceleration of planet number $n$ except for the planet $0$ will go like $-1/n^3$ because the shift of planet $0$ from zero to $\epsilon$ is equivalent to adding a " dipole " ( a pair of positive and negative mass , relatively shifted ) at the location $0$ relatively to the balanced ( but unstable ) uniform chain and this dipole acts with inverse cube , instead of the inverse law . we see that indeed the planets $+1$ and $-1$ are most affected and fastest to get some acceleration . however , planet $-1$ will move to the left , away from a potential collision . nevertheless , planet $-2$ is trying to escape from planet $-1$ , although by a smaller speed , but that will be enough to guarantee that the $0-1$ collision will be the first one . other collisions will follow . you may numerically simulate it – the problem is not integrable even for small $\epsilon$ , i think , simply because you are interested in the moments when the distance $\epsilon$ grew to a large number $o ( 1 ) $ , anyway .
to answer this question , we will first compute the values of $\lambda$ for which $\rho ( \lambda ) $ is ppt and separately compute the values for which it is entangled . let $t$ be the transpose map , such that the partial transpose map may be written as $ ( \mathbb{i}\otimes t ) $ , where $\mathbb{i}$ is the identity on $\mathbb{c}^d$ . one can show that the partial transpose maps the standard maximally entangled state into the swap operator $$ ( \mathbb{i}\otimes t ) |\psi\rangle\langle\psi|=\frac{1}{d}w , $$ where $w=\sum_{i , j}|i\rangle\langle j|\otimes|j\rangle\langle i|$ . for reference , you can take a look at john watrous ' excellent lecture notes . the swap operator has states with eigenvalue $-1$ , let 's call one of them $|w\rangle$ . we then have \begin{align} \langle w| ( \mathbb{i}\otimes t ) \rho ( \lambda ) |w\rangle and =\lambda\langle w|\frac{\mathbb{i}}{d^2}|w\rangle+ ( 1-\lambda ) \langle w|w|w\rangle\\ and =\frac{\lambda}{d^2}-\frac{ ( 1-\lambda ) }{d} . \end{align} we want this expression to be positive , which gives us the condition $$\lambda\geq\frac{1}{1+d} . $$ on the other hand , we can calculate the maximum overlap $\langle\psi|\rho_s|\psi\rangle$ that a separable state $\rho_s$ can have with $|\psi\rangle$ , such that if the overlap of $\rho ( \lambda ) $ is greater than this maximum , we know that $\rho ( \lambda ) $ is entangled . it can be shown ( see for example this review ) that in our case this maximum is precisely $\frac{1}{d}$ . therefore , $\rho ( \lambda ) $ is entangled whenever \begin{align} \langle\psi|\rho ( \lambda ) |\psi\rangle and \geq\frac{1}{d}\\ \rightarrow\frac{\lambda}{d^2}+ ( 1-\lambda ) and \geq\frac{1}{d} , \end{align} which gives the condition $$\lambda\geq\frac{d^2-d}{d^2-1} . $$ however , you can quickly check that both conditions cannot be met simultaneously , so there is no value of $\lambda$ for which $\rho ( \lambda ) $ is entangled and ppt .
$g$ denotes the local acceleration due to gravity near earth 's surface . $g = 9.8 \ , \mathrm{m}/\mathrm{s}^2$ . whatever acceleration you find , you should express it as a multiple of this value . example : $$54 \ , \mathrm{m}/\mathrm{s}^2 = 54 \ , \mathrm{m}/\mathrm{s}^2 \times \frac{1g}{9.8 \ , \mathrm{m}/\mathrm{s}^2} = 5.5g$$
two photon absorption is the nonlinear mechanism most sensitive to band gap , and it depends on the band gap being twice $\hbar \omega$ or smaller . it can be estimated using standard second order perturbation theory . see for example http://aristotle.sri.com/srini/73-jap.pdf
actually the pressure at those two points does not only depend on the height from the surface but also the distance of those points from the right wall . this is because there is an acceleration towards the right , and if you move from right to left in the same horizontal level in the container , you would not get a pressure difference corresponding to gravity , but you would get a pressure difference corresponding to the rightward acceleration . a better way to look at this model is to view from the frame of the container and apply a pseudo-force on the liquid corresponding to the acceleration . it can be said that a net force ( vector sum on gravity and pseudo-force ) will act on the liquid , and the surface of the liquid will align itself perpendicular to the direction of the force . now the condition of equal pressure will be found when the perpendicular distance of those two points is same from the surface ( can be regarded as the apparent depth of the point ) . applying that condition you should get your answer . p.s. i have not calculated the answer but i do not think $a=2g$ is right . . .
infinitely divided first thing , em waves cannot be infinitely divided ( not too sure about sound waves , there is some concept of phonons ) . em waves come in finite packets known as photons . but this is irrelevant to the main problem . yes , in generay , for a source of sound/light that has no directional preference , the waves will be uniformly distributed , for a speaker , not so much . but the waves will be distributed in all directions , just not uniformly . we will see . now , the waves need not be radiated everywhere at once . normal antennae radiate in all directions , but there are directional antennae as well , which radiate mainly forwards . diffraction now , comparing your exeriences with light against sound is a bad idea , because light has a small wavelength ( in micrometers ) , and sound has wavelengths in meters . what does this change ? it changes the extent of diffraction . diffraction is the bending of waves around obstacles , and is more effective for larger wavelengths . see the diagrams here ( these are for a source emitting in all directions , though ) . all waves can bend around corners , but we only notice this for sound , as it has a large wavelength . explanation with diffraction now , how does this explain hearing someone from behind them ? there are two ways of looking at this . one way ( which i do not like ) , is by just saying that the sound diffracts around your head . after all , the head is a type of obstacle ( one can also consider air particles ) . remember that sound intensity is drastically decreased when you stand behind a speaker , so this seems to explain the phenomenon . explanation without diffraction i like to explain this in the following manner . ( if you do not know the mechanism of sound wave propagation , first read this and understand what comressions/rarefactions are ) . note that i am not sure if this is an accepted explanation , its just much easier to " wrap your head around " . and it seems to work . i shall refer to this diagram ( apologies for the sloppyness ) for this part of my answer : in ( a ) , there is a person speaking . he generates pressure oscillations ( i.e. . , sound ) . in the diagram , i have denoted compressions by black lines and rarefactions by yellow . there should not be that large a size difference , but i am rather lazy . i have also not made the sound waves propagate outwards as the diagrams progress , due to the aforementioned laziness . now , the air behind the person is at normal pressure . so , it will be sucked into the rarefactions , and similarly , the compressions will be sucked into this region . see the arrows in ( b ) . now , due to the exchange of air , the pressure of an area next to a rarefaction will decrease , and that of one near a compression will increase ( c ) . finally , we get a ( slightly weaker ) set of compressions and rarefactions behind the speaker ( it happens on the top , too , but i have not drawn that ) . one can easily see that this process will continue , and the sound will finally go in all directions , with varying intensity . ? ? ? i do not believe they have " height " , to reach more than one person at once , but if they did they would probably collide at one point . could you please elaborate this ? i do not understand what exactly you are asking . if you are talking about sound from a speaker , they will reach everyone nearby . i do not know what you mean by " height " . most probably , this explanation will boil down to being diffraction in the end , but i personally feel that it is easier to understand than diffraction ( which is more of a ' it just is ' phenomena till you learn waves in proper ) conclusion sound is able to diffract extensively . thus it can do all sorts of things that we do not observe with light , like bending around obstacles . sound can be heard from behind a source , but the intensity will be less . this can be explained both by diffraction and the fundamental pressure difference way .
well , you end up with integrals but those are very , very easy to solve for the harmonic oscillator ! since your problem is already formulated in terms of the raising and lowering operators $a_+$ and $a_-$ . recall that $$a_+ | n \rangle = \sqrt{n+1} | n+1 \rangle$$ $$a_- | n \rangle = \sqrt{n} | n - 1 \rangle$$ $$ a_+ a_- | n \rangle = n | n \rangle$$ where $|n\rangle$ is short-hand for $|\psi_n^0 \rangle$ . these relations make it almost trivial to compute matrix elements involving eigenstates of the harmonic oscillator and those operators . just as an example , let 's prove that all eigenstates have zero expectation value for $x$: we know $x$ is proportional to $a_+ + a_-$ . inserting that into the matrix element gives us $$\langle n | x | n \rangle \propto \langle n | a_+ | n \rangle + \langle n | a_- | n \rangle = \sqrt{n+1} \langle n | n+1\rangle + \sqrt{n} \langle n | n- 1 \rangle = 0$$ because eigenstates are orthogonal . edit : continuing with the derivation where you left off , you see that you get a non-zero contribution only if $m = n+1$ or $m = n-1$ . so in the infinite sum over all states $m \not= n$ , only two terms will contribute , making it possible to easily carry out that sum : just add those two non-zero terms .
neutron star properties indeed are determined by quantum effects , without a distinct location for individual neutrons . this special form of matter is referred as neutron-degenerate matter . and in a certain sense neutron star can be treated like a giant nucleus . neutrons in it could be seen as filling consecutive states according to pauli principle , one neutron for each state up to a certain maximal energy level . but the number of neutrons is so large that speaking of individual levels , shells etc . ( which is appropriate for the systems of few particles ) is pointless . however there are some differences : neutron star is not in a ground state like a stable nucleus ( and not in a distinct excited state like say nuclear isomer ) but is rather a thermal system , with temperature and thermodynamical properties with fermi-dirac statistics . additionally neutron star has outer regions ( actually usually having more volume than volume of mostly neutron matter ) which have a lot of ' life ' other than just neutrons : electrons , various atomic nuclei , ions , magnetic fields . . . the force holding a neutron star is gravity . . . indeed the neutron star is the result of balance between the gravity which tries to pull the matter together and the pressure of degeneracy opposing it . and because the gravity force acting on a single nucleon is so much weaker we end up with such a large object .
add any instant in time , light of different wavelengths can be said to interfere . however , because of the extreme frequencies of optical light , any cross interference will get time-averaged away very quickly unless the two waves are very close in frequency .
start with your $\hat{h} = \hbar \omega \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) $ . i will omit hat notation from this point . the commutator then reads as \begin{equation} \left [ h , a \right ] = \hbar \omega \left [ \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) a - a \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) \right ] = \hbar \omega \left ( a^\dagger a a - a a^\dagger a \right ) , \end{equation} which is nothing but \begin{equation} \left [ h , a \right ] = \hbar \omega ( a^\dagger a - a a^\dagger ) a = \hbar \omega \left [ a^\dagger , a \right ] a , \end{equation} but we know that \begin{equation} \left [ a^\dagger , a \right ] = -1 , \end{equation} therefore \begin{equation} \left [ h , a \right ] = -\hbar \omega a , \end{equation} qed . proof of the second relation is done in the same way .
you are on the right track with integration by parts , but you need to get the right differential equation to integrate and then apply integration by parts to . to do this , first take the partial of the original differential equation with respect to $\lambda$ . then combine the resulting equation with the original differential equation to obtain a differential equation where $\lambda$ is not explicitly present as a multiplicative factor anywhere . at this point , you should be able to manipulate the equation that results to obtain $$ \psi ( \partial_r^2\partial_\lambda\psi ) + \frac{2m}{\hbar^2} \psi u \psi - \partial_r^2\psi\partial_\lambda\psi =0 $$ i will let you do the rest . by the way , are you sure there is no multiplicative factor of $\frac{\hbar^2}{2m}$ on the right hand side of the result you want to prove ? hope that helps ! cheers !
antenna gain is often expressed in the following form , $g = \frac{4\pi a_{e}}{\lambda^{2}}$ , where $a_{e}$ is the effective area of the antenna and $\lambda$ is the operating wavelength . however , using the antenna equation , the effective area can be expressed in terms of the main beam width ( 3db width ) $\omega$ , $a_{e} = \frac{\lambda^{2}}{\omega}$ . assuming both antennas operate at the same wavelength , the following is true , $g_{b} = g_{a}\frac{\omega_{b}}{\omega_{a}}$ . i hope this is helpful . p.s. this essentially follows from the definition of gain .
show that if $|\nu\rangle$ is an eigenvector of $n$ with eigenvalue $\nu$ , and if $a|\nu\rangle\neq 0$ , then $a|\nu\rangle$ is an eigenvector of $n$ with eigenvalue $\nu-1$ . convince yourself that the spectrum of the number operator is non-negative . assume , by way of contradiction , that there is some non-integer eigenvalue $\nu^*&gt ; 0$ and let $m$ denote the smallest integer larger than $\nu^*$ . use property 1 repeatedly ( $m$-times ) to show that $a^m|\nu^*\rangle$ is an eigenvector of $n$ with eigenvalue $\nu^*-m &lt ; 0$ . this is a contradiction qed .
the origin of your problem was already explained in the previous answers , let me just do so in a bit more detail . it is better to think of some normalizable wave function rather than the $\delta$-function itself . as you probably know , you can get arbitrarily close to a $\delta$-function by making a wave packet narrow and taking a suitable limit ( see below for a concrete example ) . now you are right that you can localize your particle to an arbitrarily narrow region around $x=0$ . you can even make it " static " in the sense that the average ( quantum expectation value ) of its momentum will be , and stay , zero . however , the uncertainty principle tells you that once the spread ( dispersion ) of coordinate $\delta x$ is very small , the spread of momentum $\delta p$ will be very large . therefore , even if you initially localize the particle to a very narrow wave packet , it will broaden quickly with time . in fact , this broadening will be the faster , the sharper was the initial localization of the particle . if the wave function is initially gaussian , then for a free particle you can solve the schroedinger equation exactly and see that the wave function remains gaussian , just its width grows ( asymptotically as $\sqrt t$ ) . so after some time , the particle is no longer localized , as a consequence of the very same uncertainty principle . even if the particle is not free but moves in some potential , making the initial wave packet at $t=0$ very narrow will result in the same spreading since the average kinetic energy ( which is proportional to $\delta p^2$ if the average momentum is zero ) is much higher than the variation of the potential energy within the size of the wave packet . there is one caveat in my above argument though . if you , simultaneously with narrowing down the wave packet , confine the particle by an increasingly strong potential , then you can keep it localized . consider as a model the harmonic oscillator , defined by the hamiltonian $$ h=\frac{p^2}{2m}+\frac12m\omega^2x^2 . $$ the coherent states are gaussian wave packets which thanks to the special form of the potential remain localized : both $\delta x$ and $\delta p$ are time-independent and given explicitly by $$ \delta x=\sqrt{\frac{\hbar}{2m\omega}} , \quad \delta p=\sqrt{\frac{m\hbar\omega}2} . $$ if you now take the limit $\omega\to\infty$ , the gaussian packet goes asymptotically to the $\delta$-function . this corresponds to your particle localized infinitely close to $x=0$ . the prize for this is that you have to make the potential infinitely strong , which simultaneously makes the particle oscillate with infinite frequency around the origin . so you cannot quite say that its momentum is zero .
to name an simple example , a 1d simple gravity pendulum with lagrangian $$l ( \theta , \dot{\theta} ) = \frac{m}{2}\ell^2 \dot{\theta}^2 + mg\ell\cos ( \theta ) $$ has one degree of freedom ( d . o.f. ) , $\theta$ , although its solution $\theta=\theta ( t ) $ has two integration constants . here , $\theta$ is the angle of the pendulum ; $\dot{\theta}$ is the ( angular ) velocity ; and $p_{\theta}:=\frac{\partial l}{\partial\dot{\theta}}=m\ell^2\dot{\theta}$ is the ( angular ) momentum . furthermore , the configuration space with coordinates $ ( \theta , \dot{\theta} ) $ and the phase space with coordinates $ ( \theta , p_{\theta} ) $ are both two dimensional spaces . in other words , it takes two coordinates to fully describe the instantaneous state of the pendulum at a given instant $t$ . so , to answer the main question : no , the corresponding velocity ( or momentum in the hamiltonian formulation ) is not counted as a separate d . o.f. . references : landau and lifshitz , mechanics : see e.g. first page of chapter 1 or first page of chapter 2 ; h . goldstein , classical mechanics : see e.g. page 13 or first page of chapter 8 in both 2nd and 3rd edition ; j.v. jose and e.j. saletan , classical dynamics : a contemporary approach : see p . 18 ; or wikipedia , either here or here .
the drag of a fluid acting on an object inside is the flow of momentum through the boundary of the object . the momentum conservation law is the entire content of the navier stokes equation , which can be written in integral form : $$ {\partial\over \partial t} \int_r \rho v^i = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p \hat{n} + \nu ( \rho ) \nabla v^i ) \cdot \hat{n} $$ where $\hat{n}$ is the normal to the boundary of $r$ , $p$ is the pressure , $\nu$ is the viscosity ( as a function of the density $\rho$ ) , and v is the velocity . the left hand side says that you are looking at the flow of total i-component of momentum out of region r . the first term on the right is the physical amount of momentum flowing out of the boundary of r by the flow of the fluid . the last term is the flow of momentum through the boundary of r due to forces at the edge . using the divergence theorem , you learn that $$ \int_r {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p - \nabla\cdot ( \nu \nabla v^i ) d^dx = 0$$ and you conclude that the ns equations are satisfied . $$ {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p -\nabla \cdot ( \nu \nabla v^i ) $$ if you expand this out , and use the continuity equation , you will recover the more standard forms , but this is the form in which it is most transparently a continuity equation for the momentum flow . so you see that the flow of the i-component of momentum into any region r due to the fluid , which is the i-th component of the force exerted by the fluid on whatever is inside r , is given by the boundary integral $$ f^i_r = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p\hat{n} + \nu \nabla v^i ) \cdot \hat{n}$$ for the case where you have a solid object that the fluid cannot penetrate , the velocity is perpendicular to the object 's surface , and the first term is zero ( obviously-- the first term describes the momentum carried along with the fluid , and this is not entering r ) . so the drag is the integral of two terms across the surface , the pressure across the object , which tells you how much the object is pushing to get the water to go around , and the gradient of the velocity , which describes how the viscosity pulls the object . for a moving object , this works at one instant to tell you how much momentum is entering or leaving the object , which is the instantaneous drag force .
your argument is correct that there if there is no current in the resistor which is in parallel with the $2c$ condenser , then the charge on the $2c$ condenser must be $0$ . this , as you probably already know , is because the two elements are in parallel and so they must have the same potential across them . however , you should give a clear argument as to why the current through the resistor is zero in the first place . having done this you ought to be able to find the potential drop across the series resistor . after this you should be able to find the potential drop , and charge on , the remaining condenser . i will let you fill in the details yourself . answer to additional question the short answer to how the capacitor discharges is that it discharges through the resistor it is in parallel with . it will probably be useful to view the problem this way : you can consider the power supply and the resistor together to be one compound object : an power supply with internal resistance . any real power supply will have such an internal resistance . you can consider the capacitor and resistor in parallel as a leaky capacitor that will discharge if left for long enough . any real battery behaves like this . the other capacitor , then , is like an ideal battery . phrasing the problem this way , the situation is this : you are charging a perfect battery and a leaky battery in series with a non-ideal power supply . at first , both batteries start charging , but as the leaky battery leaks ( through its internal resistor ) , the voltage drop concentrates more and more on the ideal battery , and the voltage drop on the leaky battery goes to zero .
this is at best only a partial answer since i do not know for sure , but i would guess the tube contains a mixture of some gas ( possibly argon ) with a small amount of mercury vapour . effectively what you have is a gas discharge lamp , though i would guess it is at a lower pressure than the usual gas discharge lamps because you do not want excessive electron scattering . these do not give you a single line , but rather you get a whole series of lines and the overall colour depends on which lines are strongest and contribute most to the light . most gases tend to give blue/violet discharges - i would guess your picture is of an argon discharge , and you can see that it is violet not green . i believe that adding a small amount of mercury vapour gives a blue/green light , which is why i suspect it is an argon/mercury mixture . to be honest i am not sure about this , and a quick google did not help much . maybe it will give you somewhere to start from if you want to try following this up yourself .
after switching to the frame mentioned above , we are left with only a static electric field , perpendicular to the initial velocity of the particle . now we consider $$\frac{d u^\alpha}{d\tau}=\frac{e}{mc}f^{\alpha\beta}u_\beta$$ this decomposes as $$\frac{du^0}{d\tau}=\frac{e}{mc}f^{0\beta}u_\beta=\frac{e}{mc}f^{0i}u_i=\frac{e\gamma}{mc}\vec{e}\cdot\vec{v}$$ and $$\frac{du^i}{d\tau}=\frac{e}{mc}f^{i\beta}u_\beta=\frac{e}{mc} ( f^{i0}u_0 -f^{ij}u_j ) =\frac{e}{mc} ( \gamma c ) \vec{e}$$ since $\vec{b}$ is zero in this frame . now i write all the velocities as parallel and perpendicular to the electric field and define $\omega_e=\frac{ee}{mc}$ $$\frac{d ( \gamma c ) }{d\tau}=\omega_e ( \gamma v_{||} ) $$ $$\frac{d ( \gamma v_{||} ) }{d\tau}=\omega_e ( \gamma c ) $$ $$\frac{d ( \gamma v_{\perp} ) }{d\tau}=0$$ then differentiating the second equation by $d/d\tau$ we get $$\frac{d^2 ( \gamma v_{||} ) }{d\tau^2}=\omega_e \frac{d ( \gamma c ) }{d\tau}=\omega_{e}^2 ( \gamma v_{||} ) $$ solutions to this are $ ( \gamma v_{||} ) =a\sinh ( \omega_e \tau ) +b\cosh ( \omega_e \tau ) $ which implies $ ( \gamma c ) =a\cosh ( \omega_e \tau ) +b\sinh ( \omega_e \tau ) $ and $\gamma v_{\perp}=\text{const . }$ . now at $\tau=0$ we know that $v_{\perp}=v_0$ and $v_{||}=0$ . let $$\gamma_0=\frac{1}{\sqrt{1-\frac{v_{0}^{2}}{c^2}}}$$ then the initial conditions demand that $b=0$ , $a=\gamma_0 c$ , and $\text{const . }=\gamma_0 v_0$ . then we have $$ ( \gamma c ) =\gamma_0 c \cosh ( \omega_e \tau ) \implies \gamma=\gamma_0 \cosh ( \omega_e \tau ) $$ $$ ( \gamma v_{||} ) =\gamma_0 \cosh ( \omega_e \tau ) v_{||}= ( \gamma_0 c ) \sinh ( \omega_e \tau ) \implies v_{||}=c\tanh ( \omega_e \tau ) $$ $$\gamma v_{\perp}=\gamma_0 \cosh ( \omega_e \tau ) v_{\perp}=\gamma_0 v_0\implies v_{\perp}=\frac{v_0}{\cosh ( \omega_e \tau ) }$$ now $dt/d\tau=\gamma$ so that $$t=\int_{0}^{\tau}\gamma d\tau&#39 ; =\int_{0}^{\tau}\gamma_0 \cosh ( \omega_e \tau&#39 ; ) d\tau&#39 ; $$ then $$\frac{t \omega_e}{\gamma_0}=\sinh ( \omega_e \tau ) $$ the important one though is $$\cosh ( x ) =\sqrt{1+\sinh^2 ( x ) }\implies \cosh ( \omega_e \tau ) =\sqrt{1+\frac{t^2 \omega_{e}^{2}}{\gamma_{0}^{2}}}$$ after plugging these into the above equations for $v_{\perp}$ and $v_{||}$ we can solve for $x_{||}$ and $x_{\perp}$ from $$dx=v\ , dt\implies x_{||}=\int_{0}^{t}v_{||} ( t&#39 ; ) dt&#39 ; \quad \text{and}\quad x_{\perp}=\int_{0}^{t}v_{\perp} ( t&#39 ; ) dt&#39 ; $$ these are what i was looking for .
if you think about it , " vertical " is generally defined as the direction in which things fall - that is , parallel to the local gravitational field . so by that definition of " vertical , " no , it is not possible . but if you were to use a different definition of " vertical " and " horizontal , " sure , it might be possible .
a passive machine on a surface ( of gravitationaly attracting sphere = ideal earth ) cannot be in a stable or meta stable state with only two supports . a di-pod cannot span an area , in a way that it is center of gravity stays within if tilted . unlike the doll below , which has always a projected area around it is point of support under it is center of gravity - a two-legged system spans only a line , which is easy to cross . self-balancing system should revert to it is state after a small ( and to be defined , in the example below it would be $x_a - x_b$ ) dicsplacement . in terms of a potential this is the case for stable and meta-stable states . in the picture the state of your system is related to potential ( y-axis ) , and some abstract position is at the x-axis : e.g. your system is staying " upright " at $x_a$ , it will go back to $x_a$ if pushed with less effort than $\delta u$ , and will be " falling down " untill it reaches next stable position at $x_c$ " lying down " addendum : a bag hook could be considered stable virtually having only one support , but it is not an option , for a ) moving machine b ) on a surface upon further reflection - the hook is actually hanging on a table , which has at least three supports itself .
a screw acts exactly as an inclined plane . the perpendicular force decomposes and a longitudinal component is generated . this is balanced by an equal reaction of the material : for instance if you try to fasten a screw on a spaceship , as the screw moves forward , the spaceship will move a little backward .
when the electrostatic force was originally being studied , force , mass , distance and time were all fairly well understood , but the electrostatic force and electric charge were new and exotic . in the cgs system , the charge was defined in relation to the resulting electrostatic force ( it is called a franklin ( fr ) an " electrostatic unit " ( esu or ) sometimes a statcoulomb ( statc ) ) . in that system , we express the force on one charged particle by another as $f_e=\frac{q_1 q_2}{r^2}$ where the unit of charge is the esu , the unit of force is the dyne and the unit of distance is the centimeter . in the mks system ( now called si ) , we would write $f_e = k_e\frac{q_1 q_2}{r^2}$ where the unit of charge is the coulomb , the unit of force is the newton , and the unit of distance the meter . it would seem that if things are equivalent , then $k_e$ is indeed just a conversion factor , but things are definitely not equivalent . a little history is probably useful at this point . before 1873 , when the cgs system was first standardized , it finally made a clear distinction between mass and force . before that , it was common to express both in terms of the same unit , such as the pound . so if you think of it , people still say things like " i weigh 72 kg " rather than " i weigh 705 n here on the surface of earth " and they also say $1 \mathrm { kg} = 2.2\mathrm{ lb}$ confusing mass and weight ( the cgs imperial unit of mass is actually the slug ) . this is important , because there is a direct analogy to the issue of units of charge and to your question about the units of $k_e$ . the franklin is defined as " that charge which exerts on an equal charge at a distance of one centimeter in vacuo a force of one dyne . " the value of $k_e$ is assumed to be 1 and is dimensionless in the cgs system . in cgs , the unit of charge , therefore , already implictly has this value of $k_e$ built in . however in the si units , they started with amperes and derived coulombs from that and time ( $c=it$ ) . the resulting units of $k_e$ are a result of that choice . so although the physical phenomenon is the same , it is the choice of units that either gives $k_e$ dimension or not . see this paper for perhaps a little more detail on how this works in practice .
you define the density auto-correlation function as $$s_{\rho\rho} = \langle \delta \rho ( \mathbf{x}_1 ) \delta \rho ( \mathbf{x}_2 ) \rangle$$ where $\delta \rho ( \mathbf{x} ) = \rho ( \mathbf{x} ) - \langle \rho ( \mathbf{x} ) \rangle$ is deviation from the local mean value . the fourier transform of $s_{\rho\rho}$ is related to the structure-factor $$s ( \mathbf{q} ) = \langle \rho \rangle^2 ( 2\pi ) ^d \delta ( \mathbf{q} ) + \frac{1}{v}\int d^d x_1 d^d x_2 e^{-i \mathbf{q} \cdot ( \mathbf{x}_1-\mathbf{x}_2 ) } s_{\rho\rho}$$ where $\langle \rho \rangle$ is the average density of the whole system , i.e. , $v \langle \rho \rangle = \int d^d \mathbf{x} \ , \rho ( \mathbf{x} ) $ . the structure factor $s ( \mathbf{q} ) $ is related to the pair-correlation function $g ( \mathbf{x} ) $ via $$s ( \mathbf{q} ) = \langle \rho \rangle \big [ 1 + \langle \rho \rangle \int d^d \mathbf{x} \ , g ( \mathbf{x} ) e^{-i \mathbf{q} \cdot \mathbf{x}} \big ] $$ if the system is isotropic , then $g ( \mathbf{x} ) = g ( |\mathbf{x}| ) $ is called the radial-distribution function . most of these relations are already in the wikipedia page linked in the question .
you seem to be describing some interesting work , although the presentation above makes it tough to understand exactly what you are doing and what you would like to do with it . perhaps you could link to the paper you allude to above . anyways let me first point you in the direction of some work i think is related to what you are trying to do , for instance : http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4543 , and references within . as to your main question , i believe that it will be strongly dependent on precisely which geometry you put on your manifold . the geodesic equation ( in some coordinate chart ) is just a differential equation , and so you will have to use the analysis of singular points from the theory of differential equations ( is the singularity for instance regular or irregular : http://en.wikipedia.org/wiki/regular_singular_point ) . so i think that any concrete answer to your main question is impossible unless you can provide a concrete example you would like to study . finally let me just say that for applications to general relativity and black holes , presumably you would want the geometries you are studying to result from solutions to the einstein field equations . of course , black hole solutions have different types of singularities : coordinate ones ( which are non-essential and can easily be removed by changing coordinates ) and essential ones where things become undefined and physical problems result , for this however there is of course the cosmic censorship hypothesis : http://en.wikipedia.org/wiki/cosmic_censorship_hypothesis .
when the electrons pair up this opens an energy gap between the energy of the cooper pairs and the energy of the lowest quasiparticle excitation . there is a nice article discussing this effect here ( nb it is a pdf ) . the gap means that you cannot scatter a cooper pair by an arbitrarily small energy . if the energy is less than the gap energy it will not scatter because there are no available energy states for it to scatter into . that is why the electrons in a superconductor do not scatter off impurities , defects , etc . if you apply enough energy , e.g. a very high voltage , the collisions are energetic enough to scatter the cooper pairs and the superconductivity breaks down .
let 's start with the metric $$\mathrm{d}s^2 = -\left ( 1- \frac{2 g m}{r} \right ) \mathrm{d}t^2 + \left ( 1- \frac{2 g m}{r} \right ) ^{-1} \mathrm{d}r^2 + r^2 \mathrm{d}\omega^2 . $$ " absorbing " a metric coefficient really means defining a new coordinate so that in terms of the new coordinates that coefficient disappears ( or becomes one ) . then you probably give the new coordinate the same name as the old one , but that is a seperate step . so let 's try to define a new time coordinate $\tau ( t , r ) $ such that $$ \mathrm{d}\tau = \frac{\partial \tau}{\partial t} \mathrm{d}t + \frac{\partial \tau}{\partial r} \mathrm{d}r= \sqrt{1- \frac{2 g m}{r}} \mathrm{d}t . $$ if this works then the metric will be $\mathrm{d}s^2 = -\mathrm{d}\tau^2 + \cdots$ , which is what i think you are after . from the previous equation we have $\partial \tau/\partial r = 0$ , so $\tau ( t , r ) =\tau ( t ) $ , but then we need $\partial \tau/\partial t$ to be a function of $r$ , which is clearly impossible if $\tau$ itself is not a function of $r$ . so there is no $\tau$ coordinate we could introduce with the desired property ( this is actually a consequence of the nontrivial curvature ) . mathematically $\sqrt{1- \frac{2 g m}{r}} \mathrm{d}t$ is not an exact differential . you can get around this if you are willing to introduce cross terms $\mathrm{d}\tau \mathrm{d}r$ etc . it is different for the $r$ coordinate . introduce $\rho ( t , r ) $ such that : $$ \mathrm{d}\rho = \frac{\partial \rho}{\partial t} \mathrm{d}t + \frac{\partial \rho}{\partial r} \mathrm{d}r= \left ( 1- \frac{2 g m}{r}\right ) ^{-1/2} \mathrm{d}r . $$ the first term gives $\rho ( t , r ) = \rho ( r ) $ and the second gives $$\frac{\mathrm{d}\rho}{\mathrm{d} r} = \left ( 1- \frac{2 g m}{r}\right ) ^{-1/2} , $$ which integrates to ( for $r &gt ; 2 g m$ ) $$ \rho = \text{const}+r \sqrt{\frac{r-2 g m}{r}}+g m \left ( \log \left ( r \sqrt{\frac{r-2 g m}{r}}-g m+r\right ) \right ) , $$ which i invite you to try and invert for $r ( \rho ) $ . : ) in principle it can be inverted and you get the metric $$\mathrm{d}s^2 = -\left ( 1- \frac{2 g m}{r ( \rho ) } \right ) \mathrm{d}t^2 + \mathrm{d}\rho^2 + r ( \rho ) ^2 \mathrm{d}\omega^2 . $$ this has absorbed the coefficient in front of the $\mathrm{d}r$ at the expense of making the rest of the metric more complicated .
i think that wolfram is arguing that the study of cellular automata and perhaps similar computational systems could serve as an organizational principle , providing a coherent framework to look at different problem ( just like the more familiar frameworks provided by physics and chemistry ) . this explains the title of his new book , a new kind of science ( i.e. . the study of the above-mentioned structures ) . on the other hand , tegmark argues that our universe is one big mathematical structure . this may be difficult to wrap your head around , but it would mean that we are just mathematical structures that are complex enough to be self-aware and do everything we do . i assume this would not have any observational consequences ( as we cannot proof that something cannot be described by mathematics , exactly because we need mathematics to prove anything ) and is therefore purely speculative . as you can see , wolfram is calling for a new framework to conceptualize and study problems , while tegmark is positing a theory of the universe . in my opinion , these are two completely different things . disclaimer : i have not read the book by wolfram , nor was i previously familiar with tegmark 's proposal .
when you say : " however , i am pretty sure that this is a dirac mass , and not a majorana mass . " that is where you are confused . ( why did you think you were sure of this ? ) a majorana mass has that form , and so does a dirac mass . they have the exact same feynman rule arrow structure when you use 2-component notation . it is just that for a majorana mass , the 2-component fields being connected are the same , and for a dirac mass they are different ( typically with opposite charge under some gauge or global symmetry ) . the answers about the majorana-weyl condition are not relevant . in 4 dimensions , a majorana fermion is simply a 2-component weyl fermion with a mass term by itself . a dirac fermion is a pair of 2-component weyl fermions with a mass term connecting them .
the properties of spin in higher dimensions mean that you can measure d/2 spin directions simultaneously , corresponding to a set of maximally commuting rotation planes . for example , in 4d , you can measure the spin in the x-y rotation plane and the z-w rotation plane , simultaneously , but not the spin in the x-y and x-w planes . the angular momentum of objects in string theory obeys the same commutation relations as in any other quantum theory , so in string theory , the spins are restricted as above--- you can measure as many of these simultaneously as there are independent rotation planes .
no , the heavier object does not fall faster . instead , they heavy and light object fall at the same acceleration ( and hence the same speed if they are both simply dropped ) . this is an example of the equivalence principle . the more massive object has more gravitational force on it , but it also has more inertia . specifically , because the object is twice as massive , it has twice the inertial mass . the force on it is doubled , so the acceleration stays the same . if we look at $$f = ma$$ we see that when $f$ and $m$ are both multiplied by 2 , $a$ stays the same . check these questions for more : free falling of object with no air resistance why is heavier object more reluctant to get falling down ? projectile motion without air resistance
instead of using the chain rule ( although it of course gives the same answer ) expand the square of the $i^\mathrm{th}$ term in the sum parentheses to obtain $$ \alpha_i^2 - 2\alpha_i\alpha_{i+1}+\alpha_{i+1}^2 $$ differentiating this with respect to $\alpha_i$ gives $$ 2\alpha_1 - 2\alpha_{i+1} $$ now , from the $ ( i-1 ) ^\mathrm{th}$ term $$ \alpha_{i-1}^2 - 2\alpha_{i-1}\alpha_i + \alpha_i^2 $$ you get an additional $$ -2\alpha_{i-1} + 2\alpha_i $$ when you take the $\alpha_i$ derivative . putting these results together gives the answer in the book .
geant is a framework---which means that you use it to build applications that simulate the detector and physics you are interested in . the simulation can include all of physics and the complete detector including electronics and trigger ( i.e. . you can write your simulation so that it output a data file that looks just like the one you are going to get from the experiment 1 ) . 2 the various parts of geant are validated by being able to correctly predict the outcomes of experiments . particular models are tuned on well known physics early in the analysis of the data . this allows you to get simulated optical properties , detector gains and so on correctly matched to the actual instrument . geant is also heavily documented . read the introduction and the first two chapters of the user 's guide for application developers , which will give you the basics . after that you can delve into the hairy details in the physics and software references . there is much , much too much to cover in a stack exchange answer . ( i mean literally . . . . if i tried i would end up overrunning the 32k characters per post limit . ) it helps to know that geant4 derives from geant3 and earlier efforts . this thing has a history that goes back for decades and has been tested in thousands of experiments large and small . the use in the higgs search goes something like this we have a theory--the standard model--which tells us what coupling to expect for the particle we hope to detect we write ( and test ) a geant physics module implementing those physics . maybe more than one . we may need to write a new event generator or tweak an existing one in parallel to this effort . you construct a geant simulation of your detector . you include a simulation of the electronics , trigger and so on . 3 you simulate a lot of data from the desired channel and from possible interfering channels ( including detector noise and backgrounds ) . you are going to use a cluster or a grid for this , because it is a big problem you combine this simulated data . you run your analysis on the simulated data . 4 you extract from these results an " expected " signal . actually , you did all of the above at lower precision several times during the design and funding phase and used those result to determine how much data you would have to collect , what kinds of instrumentation densities you needed , what data rate you had to be able to support and so on ad nauseum . once you have got the data , you start by showing that : you can detect lots of well known physics in your detector ( to validate the detector and find unexpected problems ) 5 that your model correctly represents the detector response to that well known physics ( to let you debug and tune your model ) then you may need to re-run some of the " expected " processing . only then can you try to compare data to expectation . 6 1 indeed the data format is often thrashed out and debugged from the mc before the experiment is even built . 2 for big , complicated experiments like those at the lhc geant is usually paired with one or more external event generators . in the neutrino experiments i am currently working on that means genie and cry . not sure what the collider guys are using right now . 3 for speed reasons we often simulate the electronics and trigger outside of geant proper , but this decision is made on a case by case basis . 4 indeed the analyzer is often programmed and debugged from the mc output before there is real data . 5 this is also where most of the actual repetition of results in the particle physics world comes from . you will not get funding to repeat bigexper 's measurement of the wingding sum rule , but if your proposed nextgen spectrometer can do that as well as your spiffy new physics ( tm ) it helps your case with the funding agencies . 6 many of these steps will be done by more than one person/group in the collaboration to provide copious cross-checks and protection against embarrassing mistakes . ( see also , opera 's little issue last year . . . )
my incomplete understanding is that the width of pulsars generally seems to depend on their periods and the angle between their magnetic axes and rotational axes . the general trend is for shorter period pulsars to have pulse widths that are a larger fraction of their periods . see on the pulse-width statistics in radio pulsars for some more detail . it looks like there are pulse half widths as narrow as a few degrees ( ~1/100 of a period ) , and some that are as wide as ninety degrees ( 1/4 of a period , meaning that if we were able to see both pulsing sides , the pulsar is on ~1/2 the time ) . given that level of variation , i am not sure if there is a very general sort of plot of intensity versus phase . some are nicely gaussian pulses , with a nice interpulse at 180 degree phase separation , whereas others have much more structure . take a look at some of the figure in multi-frequency integrated profiles of pulsars ( there are a total of 34 pulsar profiles plotted , and it should be available to all as it was posted to arxiv ) .
gravitational potential is a scalar quantity so can be added algebraically directly for both ( or more ) bodies . also gpe is just gravitational potential times mass . $$e=\underbrace{\big ( \sum p\big ) }_{\text{due to all bodies in vicinity}}\times m$$ now , rest of your aproach is allright ! continue using this .
great question lucas . the velocity of an object in orbit around a massive body can be expressed roughly as $v ( r ) \sim \sqrt{ \frac{gm}{r}}$ the closer you are to the mass ( e . g . the black-hole ) , the bigger v ( r ) becomes . it turns out , for a black-hole like the one at the galactic center , with stars about 100 au away . . . . they travel at about 500 km/s---fast ! now , the effects of general relativity are only significant when you are near the event horizon . in this case , even though the stars are relatively ' close ' ( about $10^{15}$ cm ) , they are still almost about 1000 times further away than the event-horizon ! and so the effects of general relativity ( e . g . time dilation ) are very very small ( in this case , currently unobservable at all ) .
malicious counter example the desired object is a sphere of radius $r$ and mass $m$ with uniform density $\rho = \frac{m}{v} = \frac{3}{4} \frac{m}{\pi r^3}$ and moment of inertia $i = \frac{2}{5} m r^2 = \frac{8}{15} \rho \pi r^5$ . now , we design a false object , also spherically symmetric but consisting of three regions of differing density $$ \rho_f ( r ) = \left\{ \begin{array}{l l} 2\rho\ , and r \in [ 0 , r_1 ) \\ \frac{1}{2}\rho\ , and r \in [ r_1 , r_2 ) \\ 2\rho\ , and r \in [ r_2 , r ) \\ \end{array} \right . $$ we have two constraints ( total mass and total moment of inertia ) and two unknowns ( $r_1$ and $r_2$ ) , so we can find a solution which perfectly mimics our desired object .
the linear terms it seems you can handle . as piece of general advice , the meaning of these terms are always clearly if integrate over the momentum coordinates of each of the fields , using delta functions to preserve the value . so the non-linear term would be $$\sum_{q_1 , q_2 , q_3} g ( q_1 , q_2 , q_3 ) \psi ( q_1 ) ^*\psi ( q_2 ) \psi ( q_3 ) \delta ( -q_1+q_2+q_3 -k ) $$ maybe you can also see this way that the structure is determined by momentum conservation/translation invariance . now when i integrate this by $\int\ ! dk\ , e^{ikr}$ , the $k$ integral is resolved trivially and i am left with fourier transforms over the $q$s . since fourier transforms take multiplication to convolution , you can calculate that we get $$\int dr_{123}\ , \tilde{g} ( r-r_1 , r-r_2 , r-r_3 ) \tilde{\psi}^*\ ! ( r_1 ) \tilde{\psi} ( r_2 ) \tilde{\psi} ( r_3 ) $$ which is more or less the most general third order nonlinear term you can write . in your case you can also reduce this further by using the real space transforms of $x$ , either by plugging in directly to the first equation i wrote , or by calculating $\tilde{g}$ and plugging into the second .
in most cases , it does not really make sense to talk about a lowered effective mass caused by sitting in a gravitational potential well , since the equivalence principle says that locally the spacetime looks flat , and hence it looks like the gravitational field vanishes . however , in certain special cases , there is a sensible notion of energy that is different from your rest mass . this is when you have a timelike killing vector , which means there is a preferred time coordinate under whose flow the metric is invariant . the existence of time translation symmetry leads to a conserved energy . if $\xi^a$ is the killing vector representing the time flow , and $p^a$ is the 4-momentum of an object , the conserved energy is $$e = -g_{ab}\xi^a p^b$$ while the rest mass is $$m = ( g_{ab}p^a p^b ) ^{1/2} . $$ the schwarzschild spacetime is a classic example of this . the metric is $$ds^2 = -\left ( 1-\frac{2gm}{r}\right ) dt^2+\frac{dr^2}{\left ( 1-\frac{2gm}{r}\right ) }+r^2d\omega^2$$ since the metric is independent of time , it has a killing vector $\xi^\alpha = ( 1,0,0,0 ) $ . let 's first consider the 4-momentum of an observer initially at rest at some radius $r_0$ . initially at rest here means they start of with $p^a \propto \xi^a$ , and the normalization of $p^a$ tells us that it is $$p^\alpha = m\left ( \left ( 1-\frac{2gm}{r_0}\right ) ^{-1/2} , 0,0,0\right ) , $$ then the energy for this particle is $$e = -g_{0\alpha}p^\alpha=m\left ( 1-\frac{2gm}{r_0}\right ) ^{1/2}$$ as long as we are outside the event horizon $r=2gm$ ( which is the only place where the energy really makes sense ) , this shows that the killing energy $e$ is less than the rest mass . and if you are far away from $r=2gm$ , you can expand to first order in $gm/r$ to get $$e\approx m - \frac{gmm}{r_0}$$ which is the rest mass minus the newtonian gravitational potential energy . so in this sense the potential energy of the particle is negative , since it is killing energy is less than its rest mass energy . finally , for a particle falling in from at rest at infinity , we use the fact that killing energy is conserved along all points along the geodesic . at infinity , the metric is asymptotically minkowski , and being initially at rest means $p^a\propto\xi^a$ , hence \begin{align} p^\alpha and = m ( 1,0,0,0 ) \\ e and = m \end{align} since $e$ is conserved , we see that in this case it is always equal to the rest mass energy . you can sort of see this as a cancellation between kinetic energy and potential energy : to get the kinetic energy you need to specify who your observer is , so lets say it is the observers at constant radius . their 4-velocity is $$u^\alpha=\left ( \left ( 1-\frac{2gm}{r}\right ) ^{-1/2} , 0,0,0\right ) $$ and they would define the total energy ( rest mass plus kinetic , but not including potential energy ) as $$t = -g_{ab}u^a p^b = \frac{m}{1-\frac{2gm}{r}} \approx m + \frac{gmm}{r} . $$ to derive this , we used the fact that $e=m$ is conserved , which means that $p^t = \dfrac{m}{\left ( 1-\frac{2gm}{r}\right ) ^{1/2}}$ . if we continue to assign the potential energy $v = \frac{gmm}{r}$ , then we get $$e=t-v = m + k-v = m \implies k=v $$ so in some sense the relations you postulated hold when there is a well-defined " effective mass " i.e. killing energy , but in a general spacetime with no timelike killing vector , you will not be able to make a sensible definition of such a thing .
yes easily . fiber-fed 5-10kw nd-yag lasers are commonly used for cutting metal in machine shops . fibers are so transparent , especially when designed for a single wavelength laser , that the power loss and so heating in the fiber is very small . it is generally less than an optically fed laser where dirt accumulates on the lenses . many systems have a thin fiber wrapped around the main power delivery fiber with a small low power laser diode shining through it . if there is a fault in the main fiber , the high power laser beam will leak out and cut the thinner fiber . the loss of light in the wrapping fiber signals the controller to kill the power to the main laser .
the reason incoherent light is passed through a slit is to make it coherent . the light from a point source is forced to be coherent , the incoherent part is scattered off , or absorbed , at the opaque surface where the slits are formed . laser light is by construction coherent so no first slit is needed . scattering also often results in coherent waves and that is why we [ may see interferences ] with street lights ; even see them in light patterns in sunlight . 1 this video at about 4 minutes shows interference patterns from street lights .
you have a couple of mistakes . first , if you say that $g = 6\times 10^{-11} k$ , then $k$ should be $\frac{\text{m}^3}{\text{kg}\cdot \text{s}^2}$ . if we instead define $k$ as you did , then it is a dimensionless number : the conversion factor between the two sets of units . you should rather have said that $6\times 10^{-11} / k$ is the value of $g$ in the system of units you want to use . note that we are dividing by $k$ instead of multiplying ; this is because $\frac{\text{m}^3}{\text{kg}\cdot \text{s}^2} = \frac1{k} \frac{\text{au}^3}{\text{kg}\cdot \text{d}^2}$ . your other problem is that $\text{au}/\text{m}$ should have $10^{11}$ instead of $10^{-11}$ . taking that into account , we get that $k = 4.485\times 10^{23}$ , and $g = 1.488\times 10^{-34} \frac{\text{au}^3}{\text{kg}\cdot \text{d}^2}$ . wolframalpha then says that $t = 365.13 \text{d}$ , which seems pretty close to correct .
this is a tricky question and in my opinion some elder physicists are deliberately try to confuse students by using euphemisms when characterizing phase transitions . roughly speaking ( there might be counter examples , please comment . i am interested of finding all of them ) : first order phase transition : finite correlation length scales as e.g. $k^\alpha , \alpha = 2$ ( short or finite range interaction ) in fourier space ( 1d ) second order phase transition : infinite correlation length scales as e.g. $k^\alpha , \alpha = 1$ ( long or infite range interaction ) in fourier space ( 1d ) scale invariance ( i think soc goes here . . . ) note that there is a continuous transition with exponent $\alpha$ that escapes my mind . also $\alpha$ is dependent of dimension and probably something else ( see e.g. anne tanguy et al . from individual to collective pinning : effect of long range interactions , pre 1998 ) . also daniel fisher has a nice paper , collective transport in random media : from superconductors to earthquakes . also i just stumbled upon : http://www.tcm.phy.cam.ac.uk/~bds10/phase/introduction.pdf which has a nice overview . in general the purpose of these correlation lengths , roughness exponents and orders of phase transitions is just to find universality classes . the goal is to group the phenomena together and say " look , all these systems have properties of x , y and z . this simple model has the same properties . so by explaining the simple mode , i explain all these systems . " simple characterization of phase transitions can be found at : http://link.aps.org/doi/10.1103/revmodphys.76.663
the basic setup is correct , conservation of energy might be the quickest way to go . $$ ( m_1 -m_2 ) g h = \frac 1 2 i \omega^2 + \frac 1 2 ( m_1+m_2 ) v^2 , i=\frac{mr^2}{2} , \omega = v/r$$ gives me one of your options as the result . the two $m$ in your formula seem to refer to different quantities .
the equipartition theorem states that each degree of freedom has an average energy of 1/2kt . this is valid at large enough temperatures where quantum mechanics does not play a role . a = 3/2 kt ( 3 degrees of freesom ) b = 3/2 kt c = 1/2 kt ( 1 vibrational degree of freedom in a di-atomic molecule ) d = kt ( 2 axis of rotation , the third has very low moment of inertia , and will not be excited )
the beta function beyond 1-loop is scheme dependent , but the physical quantities you can extract from it are scheme independent ( at least if you can compute the beta function at all order ) . for instance , even though the fixed point position is scheme dependent , the critical exponent are not . on the other hand , if you stop at a given order in the loop expansion , it is possible that physical quantities depends on the scheme . in the case of the functional rg ( such as wilson-polchinski or wetterich 's version ) , the physical quantities should be regulator independent , but as you make approximations , a spurious dependence on the regulator appears ( for instance , when you change a parameter b of the regulator , the critical exponent eta depends on b ) . a way to cope with this , it to use the pms ( principle of minimum sensitivity ) , which tells you that when the real value of eta is given by the extremum of eta ( b ) . one last point : when one does a loop expansion ( say a 4-epsilon expansion ) , one needs to resum the series , which involve some ( non-physical ) parameters . the final result should be independent of the resummation techniques , but because one knows only a finite number of terms , one gets again a spurious dependence on the resummation parameters .
i suspect you are thinking that pulling a string wrapped round a cylinder would compress it in the same way that pulling on a string wrapped round a coke can would crush it . if so , this will not happen because the tension in the string can not produce any force out of the 2+1d manifold that it lives in . however you are quite correct that the tension in the string will contribute to the stress-energy tensor , and therefore act as a source of gravity . i have to confess i do not know what the effect would be in 2+1d , however in 3+1d the effect would be similar to a cosmic string . assuming the mass of the string is negligable , so the tension is the only contribution to the stress-energy tensor , the spacetime around the string is flat but has an angular deficit . that means you do not feel any gravitational field from the string , but if you travel in a circle around the string you had find you had rotated by less than 360º . i suppose the equivalent to your 1d object in the 2+1d universe would be a 2d object in our 3+1d universe i.e. instead of a stretched string you had have a stretched membrane . an example of this would be a domain wall . this does produce a gravitational field , and in fact the field is repulsive i.e. you would be accelerated away from the wall in a direction normal to it . there is a description of the field produced in this book .
i do not think the particle-anti-particle picture is a very good one to grasp what is going on . essentially , it is a consequence of zero-point energy . in classical physics , the lowest energy state of a system , it is ground state , is zero . in quantum mechanics , its a non-zero ( but very small ) value . the easiest way to see how this zero point energy arises is through an elementary problem is quantum mechanics , the quantum harmonic oscillator . the classical harmonic oscillator is a system in which there is a restorative force proportional to the displacement . for example , a spring - the further you pull the end of a spring , the more force the spring resists your pull . modeling this system in classical physics is very easy . things are a bit different in quantum mechanics - the state of a particle is specified by its wavefunction , which encodes the probabilities of finding the particle in certain positions . another property of quantum systems is that their energies come in discrete energy levels . if you are interested in how it is worked out , you can see here . you can derive the following result for the energy levels of the particle $$e=\hbar \omega ( n+\frac {1}{2} ) $$ since n specifies the energy level , setting n to zero will give us the ground state . however , we can see this is not zero - so the lowest possible state of a quantum system still contains some energy . in a practical example , liquid helium does not freeze under atmospheric pressure at any temperature because of its zero-point energy . one very important thing to note is the following - zero-point energy does not violate the conservation of energy . a common explanation is that the uncertainty principle allows particles to violate it ' if they are quick enough ! ' . this simply is not true . from the wiki page on conservation of energy : in quantum mechanics , energy of a quantum system is described by a self-adjoint ( hermite ) operator called hamiltonian , which acts on the hilbert space ( or a space of wave functions ) of the system . if the hamiltonian is a time independent operator , emergence probability of the measurement result does not change in time over the evolution of the system . thus the expectation value of energy is also time independent . the local energy conservation in quantum field theory is ensured by the quantum noether 's theorem for energy-momentum tensor operator . note that due to the lack of the ( universal ) time operator in quantum theory , the uncertainty relations for time and energy are not fundamental in contrast to the position momentum uncertainty principle , and merely holds in specific cases ( see uncertainty principle ) . energy at each fixed time can be precisely measured in principle without any problem caused by the time energy uncertainty relations . thus the conservation of energy in time is a well defined concept even in quantum mechanics . now , on to your question - in quantum field theory , all particle are modeled as excitations of fields . that is , every particle has an associated field . for the particles that carry forces , these are the familiar force fields - such as the electromagnetic field . fields take a value everywhere in space . now , in classical mechanics , this value would be zero in most places . however , as we saw above , the ground state of a quantum field is non-zero . so , even in empty space ( or ' free space' ) these fields have a a very small value . so , empty space has vacuum energy .
if the neutron decayed to a two body state ( any two body state ) the energy spectrum of the products in the neutrons rest frame would be single valued ( this is required by the conservation of energy and momentum ) . it is not . instead the electron energy spectrum is a continuum that runs from that roughly the two-body limit down to as near zero as our instruments can measure . to grab an image from the wikipedia : so , a third particle is required . that third particle is known to be uncharged ( because our detectors are sensitive to charged particles and do not see it ) . it is also known to be of very low mass because the end-point of the electron energy spectrum is almost exactly what you would expect from the two body decay . the lifetime of the neutron suggests that the interaction that is responsible for it is decay is very weak ( and going on a little further in history it obeys the principle of weak universality suggesting that it is the same interaction responsible for the decay of strange hadrons ) . the sum of these requirements constrain the properties of the third particle quite a lot , and much observation since then has shown quite conclusively that neutrinos exist .
the first assumption is that whatever vev the higgs picks up is constant in space , because this has less energy than one that increases the kinetic term in the lagrangian . so we can do one global transformation to make the vev be in the second component only . you can imagine doing this prior to symmetry breaking , if you know what it is going to be ahead of time , and since the other fields are invariant , bob 's yer uncle . stated differently , the pre-symmetry-breaking electrons and neutrinos are not the ones we observe , so we just label whatever remains as electrons and neutrinos . " without loss of generality " , we work in an electron-neutrino ( global ) basis in which the higgs starts out with only the second component of the vev being nonzero and real . if you buy that part , then it is just a matter of showing that you can perform a gauge transformation that gauges away all the other components of the higgs except the real part of the second component . this gauge transformation will of course mix $\nu$ and $e$ spatially , but you can say that when we perform the path integral we have a gauge redundancy , and so we only integrate along a slice that obeys some gauge fixing condition . the components of $l$ might as well be labeled $l_1$ and $l_2$ . it is only after we have chosen a gauge that we decide , hey , let 's name them $\nu$ and $e$ .
no . momentum is conserved . since momentum is mass times the velocity of the center of mass , if the momentum is zero , the center of mass can not move . alternately , if the center of mass is already moving , it will keep moving indefinitely in a straight line when there are no external forces . however , in curved spacetime the above may not hold . see http://dspace.mit.edu/handle/1721.1/6706
white dwarves used to be the interior of a star , which was the hottest part of the star . they shine white because they are still very hot from this past part of their history . as they age , they will cool , and as they cool , they will lose temperature , and their blackbody profile will shift to redder and redder colors , and eventually into the infared and radio ranges where they will not seem to shine at all to the naked eye . and yes , a white dwarf state is a stable final state of a star , so long as it does not interact further with any matter . if that happens , it is possible to have a white dwarf supernova
mobile phones transmit at microwave frequencies so they can induce currents in metals and other conductors . the energy dissipates as heat . the principle is the same used by microwave ovens . the phone would not itself get hot because ( presumably ) the microwave radiation would be directed away from metal components in the phone . your key must have been close to the antenna and was perhaps aligned with it in a way that maximized the effect , or perhaps its length was just right for a resonance . here is a paper that seems relevant http://www.springer.com/about+springer/media/springer+select?sgwid=0-11001-6-1296921-0 edit : let me add a small calculation . suppose a phone were outputting 1 watt and 50% was absorbed by the key for 3 minutes . this puts 1w*180s*0.5 = 90 j of heat into the key . if the key weighs 10 g and is made of iron with a heat capacity of 450 j/kg/k then the temperature goes up by 90j/0.01kg/450j/kg/k = 20k so this could raise the temperature of the key from 25c to 45c which would make it feel quite hot to the touch . actual result depends on how fast the heat is conducted away and how much of the power is absorbed but it is not beyond the bounds of possibility that enough heating is possible to account for the observation . i would not blame anyone for being skeptical though . it would perhaps be unusual for the phone to transmit on full power for that long .
calling it a built-in voltage is something of a misnomer . people usually think of " voltage " as " what you measure with a voltmeter " . so " voltage " is normally synonymous with " electrochemical potential of electrons " ( in stat mech terminology ) and with " difference in fermi level " ( in semiconductor terminology ) . under this definition , the built-in " voltage " is not actually a voltage . then what is it ? it is what chemists call " galvani potential " , and some physicists call " electrostatic potential " . it is the line-integral of electric field . ( maybe you should call it " built-in potential " , not " built-in voltage " . ) voltage / fermi level measures the total " happiness " of electrons , the sum of all influences on the electron . the electric field ( galvani potential ) is just one of those many influences . other influences include diffusion ( entropy ) , the kinetic energy of the electron 's wave function , etc . etc . but it is the sum of all influences that determines how the electron moves . that is why it is the voltage , not the galvani potential , that determines the most important things like current flow and energy dissipation . so to summarize : the " voltage " across a p-n junction is zero , when the word " voltage " is defined in the most common and sensible and intuitive way . after all , the junction is in equilibrium ; an electron is equally happy to be on either side . for more details see my other answer : fermi level alignment and electrochemical potential between two metals going around a loop , both the voltage differences and the galvani potential differences sum to zero . but only the former is really important . for the galvani potential differences , most of them are unobservable , like the volta potential at the junction when you solder an aluminum wire to a copper wire . it is possible to figure out the galvani potential differences everywhere in a p-n junction circuit , including at the wire contacts , at the voltmeter , and so on . if you do figure them out , and add them all up , you will get zero ! but since none of those parameters matter for the circuit behavior , people rarely think about them or try to figure them out .
at tree level , the conditions for the electroweak-symmetry breaking vacuum of the mssm can be found in any of the standard review articles and are : $$ \sin ( 2\beta ) = \frac{2b_\mu}{2|\mu|^2 + m_{h_u}^2 + m_{h_d}^2} $$ $$ \frac{1}{2} m_z^2 = -|\mu|^2 - \frac{m_{h_u}^2 \tan^2\beta - m_{h_d}^2}{\tan^2\beta - 1} $$ now , the right logical way to think about this is that a top-down theory determines the values of $\mu$ and the soft-breaking parameters $m_{h_u}^2$ , $m_{h_d}^2$ , and $b_\mu$ . these are really the inputs . on the other hand , we only want to consider the slice of parameter space on which ewsb is realized as in our world , with a particular value of $m_z$ . one can choose other useful coordinates on this slice , like $\tan \beta$ , as software inputs to choose only those points on this slice of parameter space , rather than specifying the high-scale input , which in general will fail to realize correct ewsb .
energy of a fission nuclear bomb comes from the gravitational energy of the stars . protons and neutrons can coalesce into different kinds of bound states . we call these states atomic nuclei . the ones with the same number of protons are called isotopes , the ones with different number are nuclei of atoms of different kinds . there are many possible different stable states ( that is , stable nuclei ) , with different number of nucleons and different binding energies . however there are also some general tendencies for the specific binding energy per one nucleon ( proton or neutron ) in the nuclei . states of simple nuclei ( like hidrogen or helium ) have the lowest specific nucleon binding energy amongst all elements , but the higher is the atomic number , the higher the specific energy gets . however , for the very heavy nuclei the specific binding energy starts to drop again . here is a graph that sums it up : http://en.wikipedia.org/wiki/file:binding_energy_curve_-_common_isotopes.svg it means that when nucleons are in the medium-atomic number nuclei , they have the highest possible binding energy . when they sit in very light elements ( hidrogen ) or very heavy ones ( uranium ) , they have weaker binding . thus , one can say that for the low " every-day " temperatures , the very heavy elements ( like the very light ones ) are quasistable in a sense . fission bomb effectively " lets " the very heavy atomic nuclei ( plutonium , or uranium ) to resettle to the atoms with lower number of nucleons , that is , with higher bound energies . the released binding energy difference makes the notorious effect . in terms of the graph cited above , it corresponds to nucleons moving from the right end closer to the peak . yet this is not the only way to let nucleons switch to the higher binding energy state than the initial one . we can " resettle " very light elements ( like hydrogen ) and let nucleons move to the peak from the left . that would be fusion . heavy nucleons emerge in the stars . here the gravitational energy is high enough to let the nucleons " unite " into whatever nuclei they like . stars usually are formed from the very light elements and the nucleons inside , again , tend to get to the states with lower energies , and form more " medium-number " nuclei . the energy difference powers stars and we see the light emission , high temperatures and all other fun effects . however , sometimes the temperatures in the stars are so high , that nucleons form the very heavy nuclei from the medium-number nuclei . even though there is no immediate " energy " benefit . these heavy elements then disseminate everywhere with the death of the star . this stored star energy can then be released in the fission bomb .
strictly speaking vacuum is the state of lowest energy . that means no matter or radiation ( photons or any other particles ) . note that space is not a perfect vacuum . also note that , technically , a gas of planets and comets etc . has a pressure ( there is usually little reason to care about it though ) . there is also radiation pressure due to the photons . people often use the term vacuum loosely to refer to anything less than atmospheric pressure . this is the sense people use when they say space is a vacuum . edit ( re the comments ) : yes , there is a minimum energy . imagine that you start with vacuum . there is nothing there by definition . now create some particle . this necessarily takes some energy ( at least $mc^2$ where $m$ is the mass of the particle ) , so the state with a particle in it has more energy . now the value of the vacuum energy is a subtle thing . without gravity only energy differences matter , so you can always set the vacuum energy to zero . but with gravity it is tricky , because all energy gravitates . indeed , physicists now believe that empty space has an energy density , now known as dark energy . now people will tell you a big song and dance about quantum fluctuations and zero point energy , but this is only one side of the coin , and only comes in when you try to actually calculate the vacuum energy from a more basic theory ( quantum field theory ) . the basic picture is really simple though : vacuum energy is just a number - some physical constant that we could go out and measure . now if you check very carefully all the laws we know then you will find that gravity is the only place the vacuum energy comes in , so for most purposes you can forget about vacuum energy . ( people also mention the casimir effect around this point , but that is another thing entirely . ) on the other question : whether true vacuum is achievable theoretically . well , it depends what you mean " theoretically . " if you mean " in the mind of a theoretical physicist " then sure , it is possible . ; ) but if you mean there is some way to build a box and make a perfect vacuum inside of it then no , you can not , because the box will always have some finite temperature and hence blackbody radiation will fill the cavity . you can make it arbitrarily close to true vacuum by cooling the box , but you could never actually reach it .
first consider there is no friction . the point of contact between the ball and the table moves with the direction of the global motion . now introduce friction : you have kinematic friction slowing down this point thus make the ball roll due to the induced torque . you will have a motion in between the cases of pure sliding and pure rolling . in this case the direction of the friction force is obvious ( by definition of the friction ) . now if you do the things at the limit case , you will have a pure rolling . in that case the point of contact has zero instantaneous velocity and if the motion is horizontal , with constant and angular and linear motion , you do not need any friction , if you had friction , this would induce a torque and the angular momentum will change . if you introduce acceleration or a non horizontal surface : in that case you have static friction : the point cannot move forward , friction is directed opposite to the " accelerated " direction , you introduce a torque .
calculate the size of the earth e.g. through vertical sticks ( what erathostenes did ) ( or just assume it , this requires travelling ) measure the acceleration of gravity at the earth 's surface and calculate the earth 's mass ( this is cheating by assuming newton 's law of gravity , which was developed astronomically ) assume the moon 's mass is small and calculate its distance from its orbital period . confirm the orbit is roughly circular by measuring the moons position at a specific time of day over 1 month ( or so ) . in principle the sun 's distance can then be determined by measuring how much of the moon is lit on the night exactly between new and full moon , thus measuring the angles of the sun-moon-earth triangle ( greeks did it , but this is probably pretty hard to get right ) sun 's and moon 's diameter can be determined through their viewing angle sun 's mass can be calculated through the earth 's orbital period
for a hamiltonian of the form $$\hat h = -\frac{\hbar^2}{2m}\frac{d^2}{dx^2}+v ( x ) $$ in one single spatial dimension , all the energy eigenvalues are non-degenerate , under suitable regularity conditions for $v ( x ) $ . this means that if two eigenfunctions share the same eigenvalue , they must be equal or , at most , differ by a phase . for a proof of this fact , and what sort of horribleness you must introduce into the potential to break this behaviour , try " can degenerate bound states occur in one dimensional quantum mechanics ? " sayan kar and rajesh r . parwani . europhys . lett . 80 no . 3 ( 2007 ) , p . 30004 ; arxiv:0706.1135 . it is important to note that this is strictly a one-dimensional result , and fails to hold as soon as a second degree of freedom - be it spin or a second spatial dimension - is present ; examples for that are trivial to construct .
alright , i will try to answer why we need dirac eigenstates in this procedure , but i am not sure if it is anything more that the tautology that the fujikawa method is precisely defined by using the dirac eigenstates . let me briefly recap what the idea is : ( all of this is for a euclidean theory . ) we consider the infinitesimal local transformaton $$\psi ( x ) \mapsto ( 1 + \mathrm{i}\alpha ( x ) \gamma_5 ) \psi ( x ) \equiv \psi' ( x ) $$ which , as it is a gauge transformation , should not change the value of the path integral : $$ z = \int \mathcal{d}\psi\mathcal{d}\bar\psi \mathrm{e}^{\int\bar\psi \mathrm{i}\not{d}\psi\mathrm{d}^4x} \overset{ ! }{=} \int\mathcal{d}\psi'\mathcal{d}\bar\psi'\mathrm{e}^{\int \bar\psi\not{d}\psi\mathrm{d}^4x + \int\alpha\partial_\mu j_5^\mu\mathrm{d}^4x}$$ to derive the anomaly term , we must examine what the change of the measure $\mathcal{d}\psi \mapsto \mathcal{d}\psi'$ is . in general , we can say that it is , by the usual transformation formulae for grassmannian integrations , $\det ( m ) ^{-1}$ for the operator acting as $\psi ' = m\psi$ . now , we must recall that the functional measure $\mathcal{d}\psi$ is only defined in the limit of some ( uv ) regularized theory , it does not exist on its own . . so to actually calculate the correct $\det ( m ) $ , we must obtain it as the limit $\lim_{\lambda \rightarrow \infty}\det ( m_\lambda ) $ of some $m_\lambda$ and some uv cutoff scale ( not necessarily hard ) $\lambda$ . uv regulators suppress high momenta . and what are the eigenstates of the dirac operator ? . . . right , they are the momentum modes ! so , the most natural regularisation of our theory is to exponentially suppress states with high dirac eigenvalues as per $$ \widetilde{\psi}_n = \mathrm{e}^{-\frac{\not{d}^2}{2\lambda}}\psi_n$$ where the $\psi_n$ are the unregularised dirac eigenstates . and by choosing the regulator that way , the only consistent way to define the measure is to see it as the integral over the coefficients of these modes , i.e. $$\mathcal{d}\psi = \lim_{\lambda \rightarrow \infty}\prod_n \mathrm{d}\widetilde{a}_n$$ where $\psi = \sum_n \widetilde{a}_n\widetilde{\psi}_n$ . ( the sum over the $n$ is actually an integral if we use no ir cutoff , but it does not matter here , you could as well use one , but it clutters notation a bit and contributes nothing insightful . ) now , the rest of the fujikawa anomaly method follows through as hopefully described in your book . i hope this is at least an approximate answer to your question .
the answer in the book is almost correct albeit oversimplified . if you want to jot them down , then there are several reasons for requiring that $d=9+1$ in superstring theory . none of them however are in any way " simple " ( compared to the kind of explanations that the book seems to give ) . let me jot down some of the reasons that come to mind . ( there may be more . ) note : here i take $d$ to be the number of space-time dimensions . in representation theory , massless particles must form representations of $so ( d-2 ) $ . in superstring theory , this is only possible in $d=10$ . in other dimensions , the quantization of the superstring is at odds with this requirement . a further requirement is the cancellation of the gravitational anomaly . let me explain this first . superstring theory relies heavily on conformal field theory . when put in an arbitrary gravitational background , the conformal symmetry is broken . ( this breaking is called " anomaly " . ) this creates a huge problem in the formulation of strings . fortunately , this anomaly is proportional to $d-10$ and so the anomaly vanishes ( i.e. . , the symmetry is restored ) in $d=10$ . the hilbert space of strings is plagued with negative norm states . such negative norm states should not be present as it leads to negative probability and amplitudes which do not make sense in the quantum theory . it turns out , in $d=10$ , these negative norm states are then elevated to states with zero norm . while these states are also difficult to handle , things settle down quite well when one identifies such zero norm states with pure gauge states . thus existence of zero norm states points to gauge symmetries . starting from a classical theory , there are often several ways to quantize a theory . if the theory is to be consistent , then one requires that all these different methods of quantization lead to the same physical hilbert space of states . one also requires that all amplitudes be the same . this is called the no-ghost theorem . it turns out that this is only possible when $d=10$ . so there are several reasons to believe that $d=10$ within the framework of superstring theory . the above explanations are not independent of each other of course . you can simply pick whichever is your favourite and use that as your explanation for $d=10$ .
the square modulus of the transmission coefficient ( $|t|^2$ ) is the transmission probability , and you have the data to calculate that for your wave-packets ( i.e. . it is just $|\psi|^2$ as you surmised , if you normalized your wavefunction ) . you can compare that to the expected transmission coefficient for plane waves with the same average energy as your wave-packet . i do not know how well the comparison will be since you have uncertain energy , but you can use wider wave-packets and at least see if it seems to be converging to the plane wave solution . incidentally it is a good idea to check the norm of your wave function , it can help determine if anything is going wrong with your numerical simulation . ps : this http://arxiv.org/abs/quant-ph/0301114 might be of some value . it is an explicit solution for a wave-packet on a square barrier ( i.e. . like your problem , but only one ' edge ' . )
ok , eventually i figured out the answer . $$\mathcal{u}=e^{-i\phi a^{\dagger}a}$$
the simplest way to think of avogadro 's number is as a unit conversion factor . just as there are 2.54 centimeters in an inch , there are avogadro 's number atomic mass units in a gram . accordingly , if you know the atomic mass of an element you can count the number of atoms in one gram of it . let 's take carbon as an example . the most common isotope of carbon has an atomic mass of 12 . there are rare forms of carbon that are heavier because they have extra neutrons , but we will ignore them here . thus 1 gram of carbon contains $\frac{\text{avogadro&#39 ; s number}}{12}$ carbon atoms . likewise if we had $6.02\times10^{23}$ carbon atoms , we had have 12 grams of carbon .
as you said , the equation you give works only in " wavefunctionspace " . as long as you deal with wave functions , you can always decompose the object your derivative acts on as $$\int \frac{\mathrm d\omega }{\sqrt{2\pi}} \tilde f ( \omega ) e^{i \omega t}$$ so your derivative will draw a factor of $i \omega$ from the exponential . the general shape still is a superposition of plane waves and therefore still in $\mathbf f$ , and the operator $i\hbar\frac{\mathrm d}{\mathrm dt}$ is self-adjoint .
forces do not always induce motion . instead , they can be counteracted by other forces . in this case , we can clamp the wires into a form . any force created by the current is counteracted by the form , so the wires do not move . steady current , static wires , constant force . since there is no motion in this case , there is no work done . you are correct that if we allow the charges to move arbitrarily ( rather than forcing them to remain on a wire that is fixed in place ) , then the situation would be much more complex .
there are different eigenvalues $h_n$ with different probabilities ( lets call them $\phi_n$ ) . your expectation value is then $\sum_n h_n\cdot \phi_n$ if you have only one eigenvalue it is obviously identical to the expectation value
dear michael , there is an article dedicated to this criterion : http://en.wikipedia.org/wiki/clearing_the_neighbourhood the relevant quantity is $$\lambda / \lambda_e = \frac{m^2 / p}{m_e^2 / p_e} $$ where $m$ is the mass of the would-be planet and $p$ is the orbital period . the subscript $e$ means that the value is for the earth . the definition above guarantees the ratio is $1$ for the earth and $1.08$ for its evil sister , venus . uranus and neptune are close with 2.51 and 1.79 , respectively . it is 8,500 and 300 for jupiter and saturn , 0.01 and 0.006 for mercury and mars . pluto , much like ceres , eris , makemake , and haumea have figures between $10^{-9}$ and $10^{-7}$ - too small . the " threshold " was chosen to be $1/153,000$ ( by stern and levison in 2000 , i do not understand where the threshold came from - probably some model of how the planets are clearing the neighborhood as a function of $\lambda$ ) so the non-planets are beneath it . all the explanations should be in this paper : http://www.boulder.swri.edu/~hal/pdf/planet_def.pdf
the expression you quote is for a ideal monatomic gas , and we get $c_v = 3/2$ for the three degrees of freedom . for ideal diatomic gases we do indeed have to count rotational degrees of freedom and we get $c_v = 5/2$ . see the wikipedia article on ideal gases for more info .
i do not know why they should violate thermodynamics either , but they do not exist because they are static . they cannot be created at any finite time - they must have existed since the beginning of time and will exist forever . the physically realistic schwarzschild solution is created from collaps and does not have the second asymptotic region .
it is a bad question . for one thing , answer ( c ) is utter nonsense . ( maybe that is a bit harsh . it might be just regular nonsense . ) in order for something to convert gravitational potential energy into kinetic energy , it has to drop to a lower height under the influence of gravity . this does not happen during a collision . collisions in physics are effectively instantaneous events ; they occur at one point in space and time and then they are over and done with . there is no change in height by which gpe could be converted into ke during the collision . whatever ( kinetic ) energy the balls run away with , they had to obtain it from the kinetic energy that the cart had coming into the collision . now , the kinetic energy of the cart at the point of the collision was converted from the gravitational potential energy that the cart had higher up the ramp . but that conversion was done by the cart alone ; the balls had nothing to do with it . the other reason i do not like this problem is that they do not tell you at which point on the ramp the cart has the speed of $5\text{ m/s}$ . it is possible that the cart maintains a constant velocity as it goes down the incline , but that would require some mechanism to keep the cart from accelerating , and if some such mechanism is involved , it should be mentioned in the problem . if that is the case , the gravitational potential energy that the cart started out with would have been converted into some other form of energy , not kinetic . it might be heat , electricity , spring energy , etc . but there is no way to know unless they tell you what mechanism is keeping the cart from accelerating . in a pinch , if you encountered this problem on the test and did not have any opportunity to ask for clarification , i would just assume that $5\text{ m/s}$ is the speed at the end of the ramp , immediately prior to when the cart hits the balls . why ? the alternative is that the problem is unsolvable . if the speed of the cart coming into the collision is not $5\text{ m/s}$ , you have no other information that would allow you to calculate what it is . ( self-check : do you understand why this is the case ? ) once you assume that the speed of the cart coming into the collision is $5\text{ m/s}$ , you have a collision of 3 objects , each of which has a mass and initial and final velocities . all 3 masses , all 3 initial velocities , and two of the final velocities are known , so you should have enough information to solve for the third . if you do not find any solution , then the situation is impossible and the answer is ( d ) ; on the other hand , if you do find a solution for the final velocity of the cart , then that velocity will distinguish between choices ( a ) ( $v_f = 0$ ) , ( b ) ( $v_f &lt ; 5\text{ m/s}$ ) , and ( c ) ( $v_f = 5\text{ m/s}$ , if you ignore the stuff about energy being converted ) .
the boiled egg wins hands down when there is friction . the internal degrees of freedom of the liquid in the raw egg will affect its rotational motion and increase its friction and absorb part of the energy into internal motion . two sliding eggs will retain the same velocity if there is no friction and initial rotation , they will not roll so it is a tie . if an initial rotation is given ( lets think space ) the boiled will go faster because the raw will be turning energy into heat due to the internal degrees of freedom . actually when growing up , we separated boiled from raw eggs by giving them a spin . the boiled ones spin nicely . the raw wobble and stop .
the capacitance is the ratio of charge on the plates over the voltage applied . $$c = \frac{q}{v} \leftrightarrow q = c \cdot v$$ the calculation you show determines the capacitance from measured voltage and charge on the plates . you basically know the result you want and determine the size of the capacitor you need . a larger capacitor , with a larger capacity , will hold a bigger charge at the same voltage . doubling the area will double the capacitance ( in case of a plate capacitor ) , so for 4 farads of capacity you get $$q = c \cdot v = 4 f \cdot 5 v = 20 c$$ the pysics works as follows : the voltage is a driving force , pushing electrons through the wires an onto the plates of the capacitor ( or sucking them off on the positive pole ) , until the mutual repulsion of the electrons leads to a balance of foces . if you have a larger plate , the charge can distribute over a larger area , there is less " pileup " and therefore a smaller " pushback force " . this is why , with larger plates , you get a bigger charge into your capacitor with the same voltage .
verifying this in its entirety is tedious but good practice , so here 's the skeleton of what you need to do without giving it all away : recall the fundamental structure relation \begin{align} \{\gamma^\mu , \gamma^\nu\} = 2g^{\mu\nu} \end{align} where , as usual , there is an identity matrix implicit on the right hand side . the expressions you really want to compare are the expression on the first line which reads \begin{align} -ig_{\nu\rho} ( -ie\gamma^\nu ) i ( k_\alpha\gamma^\alpha + m ) \gamma^u i ( k_\beta \gamma^\beta+m ) ( -ie\gamma^\rho ) \end{align} and the expression on the second line which reads \begin{align} 2ie^2 ( k_\alpha\gamma^\alpha\gamma^\mu\gamma^\beta k'_\beta -2m ( k+k' ) ^\mu+ m^2\gamma^\mu ) \end{align} it is useful to match the stuff in each line that does not depend on $k$ and $k'$ first , and then match the stuff that does depend on $k$ and $k'$ . for example , the term on the first line that does not have $k$ and $k'$ in it is \begin{align} -ie^2g_{\nu\rho}\gamma^\nu\gamma^\mu\gamma^\rho m^2 \end{align} while the stuff on the second line that does not have $k$ and $k'$ in it is \begin{align} 2ie^2m^2\gamma^\mu \end{align} these things are the same since \begin{align} g_{\nu\rho}\gamma^\nu\gamma^\mu\gamma^\rho and = g_{\nu\rho}\gamma^\nu ( \{\gamma^\mu , \gamma^\rho\} - \gamma^\rho\gamma^\mu ) \\ and = g_{\nu\rho}\gamma^\nu ( 2g^{\mu\rho}-\gamma^\rho\gamma^\mu ) \\ and = 2\gamma^\mu-g_{\nu\rho}\gamma^\nu\gamma^\rho\gamma^\mu \\ and = 2\gamma^\mu-\frac{1}{2} ( g_{\nu\rho}\gamma^\nu\gamma^\rho + g_{\rho\nu}\gamma^\rho\gamma^\nu ) \gamma^\mu \\ and = 2\gamma^\mu - \frac{1}{2}g_{\nu\rho}\{\gamma^\nu , \gamma^\rho\}\gamma^\mu \\ and = 2\gamma^\mu - \frac{1}{2}g_{\nu\rho} ( 2g^{\nu\rho} ) \gamma^\mu \\ and = 2\gamma^\mu-4\gamma^\mu \\ and = -2\gamma^\mu \end{align} do a similar ( but more tedious ) thing for the stuff that depends on $k$ and $k'$ .
hawking radiation is a very slow process of the black hole losing energy and shrinking . if you counter this by supplying a little bit of matter or energy falling into the black hole you can easily overcome it and sustain the black hole . other than hawking radiation i do not think there is any known process for black holes to shrink . the area theorem in classical general relativity states that the area of black hole horizon always increases in any physical process . so at least classically there is no way for black holes to die , or even shrink a little . hawking radiation evades that because it is a quantum process ( which is also why it is a slow process ) . as for the final stage of the evaporation , i think the honest answer is nobody knows . the logical possibilities are either the black hole shrinks to nothing and disappears , or it leaves behind some long-lived " remnant " . either one of this possibilities has its strong and weak points , but ultimately you had need to know more about a quantum theory of gravity to know for sure .
they do not make any claim in the paper about interpretations of quantum theory , either for the copenhagen interpretation or against many-worlds interpretations . nor does the phys . rev . lett . 101 , 20403 ( 2008 ) that they cite as their principal theoretical source . the vienna group 's stated intention here , as , i think , in a number of papers over the last few years , has been to try to rule out contextual classical particle models for experiments . this is to me something of a straw man , but they have been hacking away at it . i was going to ask that you expand your question to say why you think this experiment supports the copenhagen interpretation over other interpretations , because i do not see that to be the case , but i finally saw the off the cuff remark to that effect from zeilinger at the very end of the new scientist article [ you should have cited this ] . that is definitely not enough to rule out many-worlds interpretations without a much more substantial argument . there is not , so far , enough of an argument to have loopholes in it .
the human body produces a wide range of bioelectromagnetic signals from various electrical impulses in the brain . the origin of the magnetic field is the charge exchange in the muscular and neural tissues ( i.e. no magnetic material is usually present in the body with very rare exceptions ) . the brain 's magnetic field varies from 10s of ft to 100s of ft [ 1 ] . the frequency varies from 0.1 hz to &lt ; 100 hz . measurement of brain magnetism is limited by the complexity of signal due to the overlap of the signals from various parts of the brain . in magentoencephelograpahy this is done using squids [ 2 ] . the second part of your question is about magnetic excitement of brain functions . this is in fact possible and some testing/use is reported with limited clinical success . the strength of the applied magnetic fields are 1-5t for trans-cranial excitation [ 3 ] . [ 1 ] http://www.bem.fi/book/12/12.htm#02 [ 2 ] http://www.scholarpedia.org/article/magnetoencephalogram [ 3 ] http://en.wikipedia.org/wiki/transcranial_magnetic_stimulation [ 4 ] http://www.bem.fi/book/22/22.htm an excellent source for this topic is [ 1 ]
when first coming into contact with the water , it is conduction . the skin feels the water colder than air because water is a better conductor of heat than air . so the skin cools faster in water than in air . for longer intervals convection will enhance the effect bringing cooler water next to the skin and removing the water already heated by the skin . the difference will persist plotting water temperatures ( equal with air temperature ) up to the temperature the skin raises the water when in contact with it . after that , the water is felt as warm .
what is your hilbert space ? in $l^2 ( \mathbb r ) $ your eigenfunction would have infinite norm . if you dealt instead with a bounded set $l^2 ( [ a , b ] ) $ , your operator would not be hermitian unless you impose suitable boundary conditions to discard boundary terms . these boundary conditions , however , would rule out your candidate eigenvector !
having spent rather a lot of years designing and building adaptive optic systems , i have seen this sort of problem in a number of guises . certainly you can apply some fitting function to the image of each of your spots separately and determine the centroid of each spot alone . if your detector has sufficient resolution , these two measurements will yield the different locations of the two airy discs . the whole deal with the " rayleigh limit " is simply that the two-spot pattern has no local minimum between the two peaks . in the absence of knowledge of the source , you can not tell whether it is two point sources or a single extended source . if you know in advance that two sources ( such as neighbor stars ) are producing the image , and you have a good idea of their relative brightness , you can fit the image to a different function , i.e. the sum of two airy discs , to determine the peaks . however , remember that all light entering the lens system at a given angle will be focussed to the same location . so it is not moving the laser beam to different positions on the lens but rather aiming them at different entrance angles . ( i am ignoring coma and other off-axis effects of a real optic system here )
a lot of what the lhc hopes to achieve in this area is described in a paper by carlos salgado , so i would suggest you take a look at that . in summary : collisions of nuclei yield much better data about parton saturation and the color glass condensate than collisions of individual nucleons . lhc p-pb runs will allows us to study the high-energy behavior of the cgc . the kinematic range of high-energy nucleon-nucleus collisions extends down to values of björken $x$ as small as $10^{-6}$ ( compared to $10^{-3}$ for rhic ) , which expands our knowledge of the nuclear parton distributions ( e . g . whether and how well they factorize into individual nucleon pdfs ) asymmetric collisions in particular allow measurements of certain characteristics of the nuclear pdfs independent of uncertainties in the proton/neutron pdfs jet quenching and " quarkonia suppression " ( essentially jet quenching where the jet progenitors are bound states of heavy quarks ) can be used as a probe of the qgp at higher energies and densities than were accessible at rhic the lhc presents the first opportunity for collisions of multi-hadron systems ( i.e. . nuclei ) with energies exceeding 1 tev . so in general , if there is anything new and cool to be discovered at those energies , this is where we will find it .
nacl melts at around 800°c . molten nacl has a density of about $1.556 \frac{g}{cm^3}$ [ 1 ] , at room temperature ( solid ) it has one of $2.71\frac{g}{cm^3}$ [ 2 ] . sadly i could not find a value for the density at barely underneath melting point but i strongly assume that the density is a strictly monotonously falling function of temperature . therefore solid nacl will probably sink in liquid salt . the case that ice floats on liquid water is special and known as the anomaly of water .
patents are for inventions , which are feats of engineering , not advances in physics . edison was a great businessman as well as a great inventor : he took understood principles of physics and turned them into useful machines . these machines are codified in the patents . he did not , however , contribute to the understanding of the laws of physics . now , admittedly , tesla did not advance our understanding much either . he , too , was predominantly an engineer/inventor . however , by experimenting with high-voltage current , x-rays and radio waves , he indirectly helped our understanding of electromagnetic radiation and the electromagnetic force .
i will undertake a hand waving answer and maybe since the question will come to the top somebody knowledgeable will give a full answer . virtual particles are the province of quantum mechanics and led to the development of quantum field theory with creation and annihilation operators . this led to the realization that the vacuum , in the qft description is not " empty " but is like a sea where virtual particles continuously create and annihilate with no loss of energy . the vacuum is a " ground state " . interestingly enough this field theoretic description with creation and annihilation operators does not correspond one to one with particle physics . back in 1963 i sat through a field theoretical course for nuclear physics where creation and annihilation operators acted on nuclear levels . but i digress . now the curvature of space is a unique proposal of general relativity . general relativity has not been quantized in an irrefutable manner . string theorists believe that they have managed to do that , but i leave it to them to describe what a sea of virtual particles in a string universe is like . trying to naively say : suppose gravity is quantized in the classic qft manner and gravitons exist in the vacuum sea too , the answer would be : the higher the curvature the more the distribution of the particle antiparticle sea would be weighted statistically towards heavier pairs , due to energy considerations with respect to flat space .
time does run more slowly inside a massive spherical shell than outside it , however you could not stop time this way because if you made the shell massive enough it would collapse into a black hole . you need to be very careful talking about gravitational time dilation as it is easy to misunderstand what is happening . no observer will ever see their own clock running at a different speed , that is every observer still experiences time passing at the usual one second per second . however if two observers in different places compare their clocks they may find that their clocks are running at different rates . the best know example of this is the static black hole , which is described by the schwarzschild metric . if an observer at infinity and an observer at a distance $r$ from the black hole compare their clocks they will find the clock near the black hole is running more slowly . the ratio of the speeds of the clocks is : $$ \frac{\delta t_r}{\delta t_\infty} = \sqrt{1 - \frac{2gm}{c^2r}} \tag{1} $$ if we graph this ratio as a function of $r/r_s$ , where $r_s = 2gm/c^2$ , we get : and you can see that the ratio goes to zero , i.e. time freezes when $r/r_s = 1$ . this value of $r$ is actually the position of the black hole event horizon , and what we have discovered is the well known phenomenon that time stops at the event horizon of a black hole . but note that when i say time stops i mean it stops relative to the observer at infinity . if you were falling into a black hole you would not notice anything odd happening to your clock as you crossed the event horizon . you specifically asked about a spherical shell . the easy way to calculate the time dilation is to use the weak field expression : $$ \frac{\delta t_r}{\delta t_\infty} = \sqrt{1 - \frac{2\delta\phi}{c^2}} $$ where $\delta\phi$ is the difference in gravitational potential relative to infinity . if we have a spherical shell of mass $m$ and radius $r$ then the time dilation at the outer surface is just : $$ \frac{\delta t_{outer}}{\delta t_\infty} = \sqrt{1 - \frac{2gm}{c^2r_{shell}}} \tag{2} $$ so it is the same as the expression for the black hole given in ( 1 ) . if we can assume the thickness of the shell is negligable the potential remains constant as we cross the shell and go inside it , so everywhere inside the shell the time dilation remains constant and is given by equation ( 2 ) . now you can see why you can not stop time inside a spherical shell . to make the ratio of times zero you need to decrease the shell radius to $r_{shell} = 2gm/c^2$ . but this is the black hole radius so our shell has now turned into a black hole .
the einstein field equations , describing the gravitational field are given by $$r_{\mu\nu}-\frac12 g_{\mu\nu}r = -\frac{8\pi g}{c^4}t_{\mu\nu}\ . $$ they relate in a complicated manner the gravitational field that can be seen as the metric $g_{\mu\nu}$ itself to the stress-energy tensor $t_{\mu\nu}$ that might also depend on $g$ . newtonian gravitation if now velocities are small compared to the speed of light $c$ , the stress-energy tensor approximately only consists of its time-component , $$t_{tt} \approx \rho c^2$$ and the metric is flat with the exception of $$g_{tt} \approx 1 + \frac{2 u}{c^2}$$ where we can find , directly from the einstein equations , that the newtonian $$\delta u = 4\pi g \rho$$ holds . this is a static approach and we see that there is no dependence on any flux term $j_i \propto t_{ti}$ . velocity matters : rotating disc of dust considering the whole theory , we find that of course the metric will depend on contributions of all components of $t$ but only have a meaningful effect if associated characteristic velocities are approaching the speed of light . a prominent analytic solution is that of a rigidly rotating disc of dust . taking this solution , you can get an idea of how relativistic effects are important for the theory calculating the multipole moments $q_n$ with respect to some relativistic parameter $\mu$ ( corresponding also to the angular frequency $\omega$ of the disc ) . in the following picture you can see that the kerr-spacetime is approached ( from above ! ) for all moments $q_n ( \mu ) $ for $\mu \rightarrow \infty$ . this means that there is some $\mu$ where the effects of rotation dominate those of the mass itself . ( picture taken from here . ) so , to conclude , there will only be some measurable time change for a person living on a rotating planet if it is extremely fastly rotating . it is hence needless to say that this person would have some other difficulties than to measure this deviation from an almost flat metric . sincerely
yes you can say that they energies are the same . and with energy we usually mean that the potential plus kinetic energy always remains constant . so kinetic energy can change and so does potential but the sum of those 2 is always constant unless you add friction .
i suppose my first major question is simply , has this problem been solved yet ? after a bit of research i came across the abraham-lorentz force which appears to refer exactly to this " problem of self-force " . as the article states the formula is entirely in the domain of classical physics and a quick google search indicates it was derived by abraham and lorentz in 1903-4 , why is it that feynman state the problem was still unsolved in 1963 ? has it been solved in the classical case but not in qed ? this is still only a theoretical problem , as a measurement of the expected self-force needs to be very sensitive and was never accomplished . theoretically , self-force can be said to be described satisfactorily ( and even there , only approximately ) only for rigid charged spheres . for point particles , the common notion of self-force ( lorentz-abraham-dirac ) is basically inconsistent ( with basic laws of mechanics ) and can be regarded as unnecessary - for point particles there exist consistent theories like frenkel 's theory or feynman-wheeler theory ( with or without the absorber condition ) and their variations without self-force ( there are other works free of self-force too ) . j . frenkel , zur elektrodynamik punktfoermiger elektronen , zeits . f . phys . , 32 , ( 1925 ) , p . 518-534 . http://dx.doi.org/10.1007/bf01331692 j . a . wheeler , r . p . feynman , classical electrodynamics in terms of direct interparticle interaction , rev . mod . phys . , 21 , 3 , ( 1949 ) , p . 425-433 . http://dx.doi.org/10.1103/revmodphys.21.425
when it comes to fundamental charges , the ( left-handed ) up-type quarks actually have either the same values of the charge as the down-type quarks , or exactly the opposite ones . it just happens that the electric charge is not a fundamental charge in this sense . let me be more specific . all the quarks carry a color – red , green , or blue – the charge of the strong nuclear force associated with the $su ( 3 ) $ gauge group . there is a perfect uniformity among all quarks of all types . the quarks also carry the hypercharge , the charge under the $u ( 1 ) $ gauge group of the electroweak force . both the up-quarks and the down-quarks ( well , their left-handed components ) carry $y=+1/6$ charge under this group . the right-handed components carry different values of $y$ but i will not discuss those because that would make the story less pretty . ; - ) finally , there is the $su ( 2 ) $ group of the electroweak force . the left-handed parts of the up-quarks and down-quarks carry $t_3=+1/2$ and $t_3=-1/2$ , exactly opposite values , and there is a perfect symmetry between them . ( the right-handed components carry $t_3=0$ . ) it just happens that neither $y$ nor $t_3$ but only their sum , $$ q = y+ t_3$$ known as the electric charge , is conserved . the individual symmetries generated by $y$ and $t_3$ are " spontaneously broken " due to the higgs mechanism for which the latest physics nobel prize was given . the symmetry is broken because a field , the higgs field $h$ , prefers – in order to lower its energy – a nonzero value of the field . more precisely , one component is nonzero , and this component has $y\neq 0$ , $t_3\neq 0$ but $q=0$ . so the first two symmetries are broken but the last one , the $u ( 1 ) $ of electromagnetism generated by the electric charge , is preserved . it is often the case that every symmetry that is " imaginable " and " pretty " is indeed a symmetry of the fundamental laws but at low energies , due to various dynamical mechanisms , some of these symmetries are broken . the laws of physics may still be seen to respect the symmetry at some level but the vacuum state is not invariant under it , and the effective low-energy laws therefore break the symmetry , too .
i think the last line does not follow from the previous steps . it is used to show how $\gamma$ comes in place , so i extrapolated a bit and show the next few steps : since $$ \frac{c_v}{nk_b} = \frac{c_v}{c_p-c_v} = \frac{\frac{c_v}{c_v}}{\frac{c_p}{c_v}-\frac{c_v}{c_v}}=\frac{1}{\gamma-1} $$ therefore , $$ \frac{c_v}{nk_b} ( pdv+vdp ) = \frac{1}{\gamma-1} ( pdv+vdp ) = -pdv $$ dividing both sides with $pdv$: $$ \frac{1}{\gamma-1} ( 1+\frac{v}{p}\frac{dp}{dv} ) =-1 $$ continue to simplify the expressions and you will reach your result of $pv^\gamma$is constant .
first , the critical dimension . there are many ways ( seemingly inequivalent ways but ultimately bound to give the same result ) to calculate $d=10$ for the superstring that mirror the methods to calculate $d=26$ for the bosonic string . for the bosonic string , one may use a conformally invariant world sheet theory . because of the residual conformal symmetry , it has to have $bc$ ghosts . the central charge of the $bc$ system is $c=1-3k^2$ where $k=2j-1$ where $j$ is the dimension of the $b$ antighost , in this case $j=2$ . you see that my formulae imply $k=3$ and $c=1-27=-26$ so one has to add 26 bosons , i.e. 26 dimensions of spacetime , to get $c=0$ in total . now , for the superstring , the local symmetries on the world sheet are enhanced from the ordinary conformal group to the $n=1$ superconformal group . one needs to add the $\beta\gamma$ ( bosonic ) ghosts for the new ( fermionic ) generators . their dimension is $j=3/2$ , different from $j=2$ of $bc$ by $1/2$ , as usual for the spin difference of things related by supersymmetry . you see that $k=2j-1=2$ and $3k^2-1=12-1=11$ . now , the central charge of $\beta\gamma$ is $3k^2-1$ and not $1-3k^2$ , the sign is the opposite one , because they are bosons . so the $bc$ and $\beta\gamma$ have $c=-26+11=-15$ . this minus fifteen must be compensated by 10 bosonic fields and 10 fermionic fields ( whose $c=1/2$ per dimension : note that a fermion is half a boson ) and $10+10/2=15$ so that the total $c=0$ . if some of the steps are not understandable above , it is almost certainly because the reader is not familiar with basics of conformal field theory and it is not possible to explain conformal field theory without conformal field theory . it is a whole subject , not something that should be written as one answer on this server . in this formalism with the new world sheet fermions $\psi^\mu$ transforming as spacetime vectors , one has to protect the spin-statistics relationship . vector-like fermions violate it so they are only allowed in pairs . this is achieved by the gso projection – well , there are actually two gso projections , one separate for left-movers and one for right-movers . only 1/4 of the states are kept in the spectrum . the projection is a flip side of having four sectors – the left-moving and right-moving fermions may independently be periodic or antiperiodic . i wrote about the gso projection a month ago : http://motls.blogspot.com/2012/11/david-ian-olive-1937-2012.html?m=1 again , if anything is incomprehensible and incomplete , it is because it is not really one isolated insight that a layman may understand from one sentence . it is one of many technical results that follows from a large subject – string theory – that has to be systematically studied if one wants to understand it .
your question is interesting , and gets specifically to the kinds of questions that quantum mechanics was intended to answer in the first place . it helps to understand the motivation behind the original bohr model of the atom , and how that led to qm in the first place . the problem bohr was trying to address can be paraphrased as , " if an electron orbits a nucleus like a planet , why does not it gradually lose energy and spiral into the nucleus ? " the answer came when bohr realized that the orbital momentum was quantized , effectively meaning that since the electron had mass then by the relationship $p=mv$ , the velocity was also quantized ( note : this simple expression is more complicated when relativity is included , but the discussion can continue without including it ) . these quantized values of momentum/velocity are what one would call eigenvalues , or observables in quantum mechanics . since the electron can only change orbits by given off specific quantities of energy instead of giving off energy continuously , it remained in a stable orbit relative to the nucleus , thus preventing it from spiraling in . what is important to understand is the idea of the potential well . in the bohr model , the electrons closest to the nucleus have higher velocity than the electrons further away . in other words , they have greater kinetic energy ( $k . e . = \frac{1}{2} mass \times velocity^2$ ) , but it had less potential energy since it was closer to the nucleus ( obeying the relationship $p . e . = mass \times distance \times gravity$ ) . however , the total energy ( $k . e . + p.e. $ ) associated with orbits closer to the nucleus is less than those further away . so in order for the electron to move closer to the nucleus , energy must be given up . this is accomplished by the emission of a photon . alternatively , if one wants to cause an electron to move into a more distant orbit , then one must add energy through use of a photon . it is in contemplating how to determine the orbit of the electron that the uncertainty principle first became apparent . the only probe that we have available to determine the position of an electron in its orbit is a photon , and the photon must be of sufficient energy in order for it to be small enough to give a meaningful result , however if we use a photon small enough ( in terms of wavelength ) , it will have enough energy to shift the electron into a different orbit , and then we would have to start the process all over again . a free electron has sufficient energy to escape the nucleus . in other words , it has acquired sufficient energy to fill its potential energy deficit . if there were only one nucleus in the universe , the potential deficit would only be eliminated when the electron was at infinite distance from the nucleus . in that situation , the implication is that the electron would also have zero velocity . this situation is obviously unrealistic , first there are more than one nuclei in the universe , and second , to verify that a particle has zero velocity at infinite distance is clearly an impossible task . if we move past the bohr model and into more modern quantum mechanics , the question then is whether there are eigenstates that have eigenvalues for momentum of a particle that are equal to zero ? it is important to review some basic facts about matrix operations and linear algebra if 0 is an eigenvalue of a matrix a , then the equation a x = λ x = 0 x = 0 must have nonzero solutions , which are the eigenvectors associated with λ = 0 . but if a is square and a x = 0 has nonzero solutions , then a must be singular , that is , det a must be 0 . this observation establishes the following fact : zero is an eigenvalue of a matrix if and only if the matrix is singular . this means that the matrix in question is not of full rank . in qft this has a very specific interpretation . the annihilation operator has the power to destroy the vacuum state and map it to zero . this situation is understood to be associated with the free field vacuum state with no particles . this state is necessary because it allows us to find vacuum state solutions for the associated quantum mechanical system . by means of analogy , we can see that the solution to ground state problem is the solution to the homogeneous part of a differential equation . the schrodinger equation is a linear , homogeneous equation which governs evolution of the wave function of a particle . the solutions of the schrodinger equation can be used to understand particle motion . the exact position and momentum of a particle can only be known if h ( planck 's constant ) approaches zero , however , in quantum mechanics , planck 's constant is fundamental to the theory , so this cannot occur in the single particle case . because of this , momentum and position uncertainty establish an inverse relation to each other , and if the uncertainty of momentum is zero , then the uncertainty in position is infinite . for these reason 's it is not possible to talk about a particle " stopping " or being " stopped " in any meaningful or non- contrived sense .
yes they do , and this is a great way to introduce perturbation theory . exact solutions to n-body problems where n is greater than 2 are hard to find . it has been accomplished in the case of 3 and 4 bodies , and a general solution where bodies do not collide has also been found , but these problems are extremely complex . one way to get around this complexity is to start with a two body problem ( like mercury orbiting the sun ) and then perturb the solution to reflect the effects of other celestial bodies . this is possible especially since the mass of the sun is so much greater than the mass of all the other planets in orbit . update : to modify to reflect ron 's comments , a good way to understand how other planets effect each others orbits is to look at the anomalous precession of mercury which can be visualized here . each planet causes a perturbation on the orbit of the planet , which can be understood as a slight lateral acceleration from the intended path if there were only two bodies . the anomalous precession of mercury was found after the effects of all the other planets were subtracted . one of the successes of general relativity is that it can explain the anomalous precession , which appears to be non-conservative until one accounts for the full stress-energy-momentum pseudotensor . as far as the rotation of the earth , the moon , sun , jupiter and other planets do have small calculable effects on the tides on earth . tidal forces arise because objects like earth have spatial dimension , and the gravitational force from another body has greater influence on the side of the planet closer to the force . differentiating the gravitational force allows one to determine the tidal force . the ocean tides caused by tidal forces do effect the rotation of the planet , so although the effect of other planets beyond the moon , sun and jupiter is negligible , it is calculable , and does have some non-zero effect on the rotation of the planet .
firtree is correct - i will just try to flesh out his answer a bit . ( 1 ) your last question first - charge ( or current ) at a point is like mass at a point . for finite masses , if you want to see how much is contained in an infinitely small volume ( i.e. . , at a point ) , the answer is zero . so instead , people consider the mass density which can have non-zero values at a point . you probably understand the relationship between mass and ( mass ) density quite well . similarly for a finite current , the amount of current at a point ( i.e. . , in an infinitely small volume ) is zero . the current density is the limit of the amount of current in a small volume around a point as the volume goes to zero - just like mass density , but with current instead . so , just as one speaks of mass density at a point and not mass at a point ( for extended bodies ) , one speaks of charge density at a point and not charge at a point or current density at a point and not current at a point ( we are ignoring point particles for now - they do fit into this formalism , but you need dirac delta functions ) . ( 2 ) now , in analogy with mass flow , your picture of flow of charge is correct . mass density times velocity gives a mass current density . $\vec{j}_{m} ( t , \vec{x} ) :=\rho_{m} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a mass current density $\vec{j}_{m}$ and want to know the mass flow $\dot{m}$ through some area a , then you take \begin{equation} \dot{m} = \int \vec{j}_{m} \cdot d\vec{a} \end{equation} similarly , charge density times velocity gives a charge current density . $\vec{j}_{q} ( t , \vec{x} ) :=\rho_{q} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a charge current density $\vec{j}_{q}$ and you want to know the flow of charge $\dot{q}$ through some area a , then you take \begin{equation} \dot{q} = \int \vec{j}_{q} \cdot d\vec{a} \end{equation} so , the picture in your head is quite close - just picture charge or a fluid of charged particles flowing .
the article that describes this research does discuss their favored model for explaining these features . the article starts off by talking about earth analogues , which are always helpful . in this case , it talks about brines rising in antarctic ice shelves ( brines being just what they are in cooking -- salty water , and salt lowers the freezing point of water by up to and over 10°c ) . the model the authors suggest starts with a thermal plume that heads from the liquid interior up through the ice and melts some of the overlying ice . the melting causes the surface to collapse a bit and fracture ( volume of water is &lt ; volume of ice ) , and this confines the water to that region . the thermal plume was transient/temporary and sinks back down , and the water at the base of the " lake " re-freezes . the article then talks a lot about the morphology ( what it looks like ) in images and theory , showing that the two intersect . as the lens of water lies below the surface , the surface fractures and calves , like glaciers calving on earth . material fills in with an impurity-rich matrix and freezes . when the lake underneath refreezes , since the volume of the ice is greater than water , it creates a positive convex topographic relief ( jumbled because of the previously calved blocks ) as opposed to the concave one when there was liquid below . from what i can tell , the article is not actually suggesting that all these areas presently contain liquid water lakes underneath them . they do suggest one particular area , thera macula , to be presently active , however . their argument for this is based on its morphology : " the large concentric fracture system encircling thera macula resembles those of collapsing ice cauldrons , and , given the absence of a continuous moat , suggests that subsurface melt and ice disaggregation is forming thera macula , rather than the collapse of a dome . " they suggest , " today , a melt lens of 20,000–60,000 km 3 of liquid water probably lies below thera macula ; this equates to at least the estimated combined volume of the great lakes . " the timescale for this amount of liquid to re-freeze is 100,000-1,000,000 years . the type of terrain that this creates ( "chaos terrain" ) is spread throughout the moon , so if their model is correct , then it has had many intra-ice lakes throughout its history , and likely throughout its recent history . the authors conclude with , " our analyses suggest that ice–water dynamics are active today on europa , sustaining large liquid lakes perched in the shallow subsurface . "
i think the easiest way to do this is to avoid solving differential equations to the greatest extent possible . there is , in fact , a way to use ladder operators and only requires you to solve one , fairly easy differential equation ; first , we note that the ladder operator technique can be used to derive the entire spectrum of one-dimensional harmonic oscillator . $$ e_n = ( n+\tfrac{1}{2} ) \hbar\omega $$ the technique can also be used to show that the corresponding , properly normalized eigenvectors satisfy the following properties $$ a^\dagger|n\rangle = \sqrt{n+1}|n+1\rangle , \qquad a|0\rangle = 0 $$ the left-hand property shows that , once one has one of the eigenstates , every other eigenstate corresponding to a higher eigenvalue can be obtained by applying the raising operator . in particular , if one knows the position basis representation of the ground state , then one can obtain the position basis representation of every other eigenstate by applying the position basis representation of the raising operator . the right-hand property shows that the ground state is annihilated by the lowering operator . writing this condition in the position basis , one obtains a simple differential equation for the ground state wavefunction , and then , per the left-hand property , one generates all other wavefunctions .
assuming we have already proved the uncertainty principle ( which can be found here ) , we know : $$\sigma_a \sigma_b \geq \sqrt{\big ( \frac{1}{2}\langle\{\hat{o}_a , \hat{o}_b\}\rangle - \langle \hat{o}_a \rangle\langle \hat{o}_b\rangle\big ) ^{2}+ \big ( \frac{1}{2i}\langle [ \hat{o}_a , \hat{o}_b ] \rangle\big ) ^{2}}=c$$ where c is a constant . since the state we are looking at is an eigenstate of $\hat{o}_a$ , we know $\sigma_a=0$ ; also since $\hat{o}_a$ and $\hat{o}_b$ do not commute , the right hand side ( $c$ ) is greater than zero . ergo : $$\sigma_b &gt ; \frac{c}{\sigma_a}\rightarrow \infty$$
in the einstein convention , pairs of equal indices to be summed over may appear at the same tensor . for example , the formula ${a_k}^k=tr~a$ is perfectly legitimate . but your formula looks strange , as one usually sums over a lower index and an upper index , whereas you sum over lower indices only , which does not make sense in differential geometry unless your metric is flat and euclidean ( and then higher order tensors are very unlikely to occur ) .
there is a much better description here of fizeau 's nineteenth century experiment . some of the key features that enabled fizeau to succeed : a lens to collect the light from the source a collimating lens to prevent the light diverging during its journey a large diameter beam to minimise broadening of the beam by diffraction more lenses to focus the light on the detector the light went directly into a very sensitive detector : the human eye . almost certainly he did this experiment at night .
it would be an extremely cumbersome and inefficient way to do it . already one uses the acceleration of ions in ion propulsion systems in space : an ion thruster is a form of electric propulsion used for spacecraft propulsion that creates thrust by accelerating ions . ion thrusters are categorized by how they accelerate the ions , using either electrostatic or electromagnetic force . electrostatic ion thrusters use the coulomb force and accelerate the ions in the direction of the electric field . electromagnetic ion thrusters use the lorentz force to accelerate the ions . the term " ion thruster " by itself usually denotes the electrostatic or gridded ion thrusters . [ citation needed ] reply to the edit : of course , the lhc cant accelerate 1 kg of protons in an hour , but maybe a derivative of it could and would be the basis of space propulsion system . contemplate the lhc system , http://en.wikipedia.org/wiki/lhc . the large circle is 27kimometers diameter . thousands of magnets and cryogenic support give us a few horsepower in energy . how can this be amplified to the science fiction numbers you propose ? scientists are not crazy to use kilometers of land and enormous power were they able to get the same energy result in miniature . the technology yes , as the link shows , is usable , but multiplying the energies by enormous factors does not belong to the present technology or available energies . in addition how would one produce megawatts in space ? if one can , one does not need an intermediate wasteful step of such magnitude .
i walk through a physical argument similar to what trimok has shown in my blog article ramanujan and the casimir effect . you basically assume that each available classical e-m mode is filled to exactly the level of one-half of a " quantum " , and put a small box inside one twice as big . the gist of the argument goes like this ( quoting from my article ) : we simplify things a little by putting everything in a one-dimensional box . so the energy modes are 1,2,3 , etc . we can choose our units so that the pressure is numerically equal to the energy . now make the box twice small . the energy modes are 2,4,6 . . . etc . but the pressure is energy per unit volume ( length , since it is one-dimensional ) . . . so the pressures are ( get this : ) 4,8,12 . . . what is the difference in pressure between the big box and the small box ? 1 + 2 + 3 . . . . - ( 4 + 8 + 12 . . . ) which is . . . 1 - 2 + 3 - 4 + 5 . . . . = 0.25 ! to get the actual casimir effect , you then put the big box in yet a bigger box . you get a pressure difference equal to one-quarter of what you got in the first set of boxes . continuing this process gives you a chain of boxes from which you can calculate the pressure relative to infinity .
first of all , the first equation for $ds^2$ is only valid if $f$ is nothing else than the azimuthal angle $\phi$ . second , if you are evaluating $x_i x^i$ , the squared distance from the origin without any infinitesimals , then it is exactly equal to $-t^2+r^2$ and nothing else . the polar coordinate $r$ is chosen as $\sqrt{x^2+y^2}$ so its square already includes two of the three terms one encounters in cartesian coordinates . there is nothing wrong if the formula for a squared distance from a particular point ( the origin ) just has two terms .
some key reading if you want to understand this stuff is chapters two to five of the iers technical note 36 , the iers conventions ( 2010 ) . it is not just the j2000/fk5 frame ( aka the eme2000 frame ) that is associated with some epoch date . every earth-centered inertial frame has some epoch date . there are two fundamental reasons why this must be the case : the earth 's rotation axis is not constant . solar system astronomers are regularly improving their best estimate of what constitutes an inertial frame . note that the j2000/fk5 frame is now twice passé . the current best estimate of what constitutes an inertial frame is the international celestial reference frame 2 ( icrf2 ) . it is predecessor , the icrf , represented a vast improvement over the j2000/fk5 frame . the icrf2 is even better than the icrf . the icrf was supposed to be co-aligned with the j2000/fk5 frame on j2000.0 ( 12 noon terrestrial time on 2000 january 1 ) . it turned out that this was not the case ; there is a slight bias between the frames at the epoch . the j2000/fk5 frame also turns out to be rotating a tiny bit , about 3 milliarcseconds/year . unless you are doing milliarcsecond astronomy , you can ignore that bias and rotation . for most applications , j2000/fk5=icrf=icrf2 . the first item , that the earth 's rotation axis is not constant , is important . the earth 's rotation axis precesses with a period of about 26,000 years . accounting for the change in the precession between the epoch time and the time of interest ( e . g . , today ) yields a transformation from the epoch frame ( e . g . j2000 ) to the mean of date frame . in addition to this long-term precession , the earth 's axis also displays some shorter term variations in where it points . these short-term variations ( from ~5.5 days to 18.6 years ) are collectively called nutation . accounting for the earth 's nutation on top of the precession yields the transformation to the true of date frame . finally , the earth rotates at about one revolution per sidereal day about this precessed and nutated axis . applying this rotation of the true of date frame yields the earth-centered , earth-fixed frame . a somewhat widely-used name for the result of this process is the earth rnp ( rotation , nutation , and precession ) matrix . well , almost . precession and nutation are semi-analytical models , as is the concept of one revolution per sidereal day . there are some things those models just can not capture . that one revolution per sidereal day is incorrect for two reasons . one is that the earth 's rotation rate is very gradually slowing down . another is that when you look at the rotation rate very closely , the earth sometimes rotates faster than nominal , other times slower . there are two key parameters that describe this , dut1=ut1−utc and δt=tt-ut1 . if you care about this detail i suggest you use the latter as it is continuous . dut1 has discontinuities at the leap seconds . this is a correction that you add when you compute the rotation part of the rnp matrix . there are some things that the semi-analytical precession and nutation model just do not cover ( yet ) . the chandler wobble , for example . these are collectively called " polar motion " , and can only be observed ( and predicted to some extent ) . polar motion needs to be applied after computing the rnp matrix . the full result is sometimes called the prnp ( polar motion , rotation , nutation , and precession ) matrix . these fine scale variations in the earth 's orientation , along with dut1 and δt , are called the " earth orientation parameters " . these are published on a regular basis as " iers bulletins a and b " . i will say more about this below . you do not want to code this on your own . you can get code to do these calculations from a number of places . the best sites are : international astronomical union ( iau ) standards of fundamental astronomy ( sofa ) the sofa code is the " official " version of all of the concepts described above . you can get both fortran and " ctran " ( fortran converted to ugly c ) versions of the sofa code from the sofa website . also be sure to check out the cookbooks , particular the cookbook on " sofa tools for earth attitude " . naval observatory vector astrometry software ( novas ) the us naval observatory is responsible for iers bulletins a and b . they have their own software , distinct from the sofa code . the novas software is available in fortran , c , and python . the difference in terms of results is negligible , in the microarcseconds . that is the kind of error one would expect from using double precision and performing the same computations a bit differently . there are a number of others ( e . g . , jpl spice , gsfc gmat , orekit ) , but i suggest you go to the source , and that would be either the iau or the us naval observatory . i mentioned iers bulletins a and b couple of times above . the international earth orientation and reference systems service ( iers ) is the worldwide organization responsible for defining things such as the icrf and for determining how the earth is oriented . ( yes , the acronym does not match . it used to before they changed the name but not the acronym . ) as far as those technical bulletins are concerned , they just contain numbers ( and a tiny bit of text ) . these numbers are time-tabulated values for the earth orientation parameters . these bulletins are updated monthly . a couple of final points : i put the link to iers technical note 36 at the top of this answer . read it . be very careful of time . there are a number of time scales involved in this modeling . a few of them that you will run into are : tai - international atomic time . time according to an earthbound physicist who uses an atomic clock at sea level . tt - terrestrial time . time according to an earthbound astronomer . physicists and astronomers disagree by 32.184 seconds . ut1 - universal time . conceptually , what a sundial says the time is , but smoothed to eliminate things such as the equation of time . utc - coordinated universal time . that is what the clock on your computer shows if you are using network time protocol . next time we have a leap second ( probably the end of next year ) , you will be able to see a minute with 61 seconds in it . utc ticks at the same rate as tai and tt but occasionally has leap seconds so as to stay within a second of ut1 . tcb - barycentric coordinate time . a general relativistic timescale that on average ticks faster than clocks on the surface of the earth . tdb - barycentric dynamical time . a general relativistic timescale that on average ticks at the same rate as clocks on the surface of the earth . gast - greenwich apparent sidereal time . if you run into this time scale you are looking at an out-of-date concept for calculating earth rotation . use the newer earth rotation angle concept , which relies on ut1 . update i did not answer the title of the question , how to determine satellite position in j2000 from latitude , longitude and distance from earth ? this is the easy part . the only tricky aspect is that latitude is almost always geodetic latitude rather than geocentric latitude . i will assume that latitude $\phi$ , longitude $\lambda$ , and altitude $h$ are in the wgs84 reference system . see [ department of defense ( 2000 ) , " world geodetic system 1984: its definition and relationships with local geodetic systems " nima tr8350.2 ] ( http://earth-info.nga.mil/gandg/publications/tr8350.2/wgs84fin.pdf ) equations 4-14 and 4-15 in the above reference describe the transformation from latitude $\phi$ , longitude $\lambda$ , and altitude $h$ are in the wgs84 reference system to cartesian earth-centered , earth-fixed ( ecef ) coordinates . the equations below use two key parameters that describe the shape of the earth ( see tables 3.1 and 3.3 of the above reference ) : $$ \begin{aligned} a and = 6378137\ \text{m} and and \text{earth equatorial radius} \\ e^2 and = 6.69437999014\times10^{-3} and and \text{square of earth eccentricity} \end{aligned} $$ first you need to compute the " radius of curvature in the prime vertical " ( equation 4-15 in the reference ) : $$n = \frac a {\sqrt{1-e^2\sin^2\phi}}$$ then simply compute the ecef coordinates via equations 4-14: $$ \begin{aligned} x and = ( n+h ) \cos\phi \cos\lambda \\ y and = ( n+h ) \cos\phi \sin\lambda \\ z and = ( ( 1-e^2 ) n+h ) \sin\phi \end{aligned} $$
the dynamical degrees of freedom are in the schroedinger state , not in the quantum field operators ( which would require a heisenberg picture with a fixed state ) . the squeezed states at time $t$ are created from the vacuum by multiplication with a unitary matrix $e^{ix ( t ) }$ where $x ( t ) $ is an appropriate hermitian expression quadratic in the creation and annihilation operators in momentum space , with coefficients depending on the time $t$ .
i am going to offer a completely different way of doing this . sometimes it is nice to work in symbols before getting into the very specific numbers . i take my inspiration from the bohr atomic model here . the total energy of the electron in orbit around the nucleus at any given radius is calculated as follows ( wikipedia 's equation here , not mine ) . that is , the sum of the kinetic and potential energy is just half the potential energy ! neat , is not it ? so , how would we apply this to the earth ? well , $z k_e e^2$ is going to have to be replaced with $g m m$ . but let 's not forget , the entire point was to introduce a radius $r&#39 ; =1.05 r$ , and i am seeking a value of $\delta e = e&#39 ; -e$ . also , the kinetic energy is 1/2 the magnitude of this total energy metric , i will use $e_k$ for that . $$\delta e = gmm/2 \left ( -1/r&#39 ; + 1/r \right ) = e \left ( -1/1.05+1\right ) =e\frac{0.05}{1.05} = 2 e_k \frac{0.05}{1.05}$$ so the energy would change by about 9.5% times the original kinetic energy . given your original energy , i believe this would be $8.8 \times 10^9 j$ . this all said , your question says : if the initial magnitude of the satellite’s mechanical energy was $e_{m , i} = 9.26 \cdot 10^{10}$ j and it continues at the same speed , how much work was done by the rockets in moving the satellite to the higher orbit ? our work assumed that it would attain a new speed . maybe the question is written wrong . i do not know .
dark matter is only the " caboose " on a whole train of answer . when one object totally dominates the gravitational field of some interacting bodies , like the sun does in the solar system since it has 99% of the mass , all the objects orbiting the big one can be reasonably approximated as interacting with the central body . this is called the two-body problem . in the two-body problem , both the angular and linear velocities decrease for objects in circular orbits as the orbital radius increases . in other words , venus ' year is longer than mercury 's , earth 's is longer than venus ' , etc . also , [ correction : ] mercury literally flies through space at greater miles per hour than venus , etc . now , a galaxy 's central black hole is the most massive single object in the galaxy , but it is typically only a tiny fraction of the total mass . the mass of a galaxy is dominated by the overall soup of matter reaching out from the center . since this is not the two-body problem , the solar system results need not apply . to solve for how objects should behave in a soup , you basically consider just the amount of soup between your given object and the center . therefore , objects farther out are basically behaving as if they are orbiting a more massive body than objects closer in . the net result of that is that objects orbiting in a soup should have their angular and linear velocities decrease less than the traditional two-body problem . in fact , we observe that their linear velocities seem to be almost perfectly the same . that means there must be a lot of matter in the soup . . . however , we can not find enough light-emitting , ordinary matter to account for how much matter we just inferred was there . this is the first and only part of the story where we need to invoke dark matter . mond is a theory that attempts to resolve this final discrepancy in a different way . instead of inferring lots of mysterious matter of unknown properties , it modifies the math surrounding very slow gravitational interactions ( technically , those involving very low accelerations ) . this math works extremely well , but it has not achieved widespread acceptance because there is no " story " or explanation backing up the math at all , and mond is difficult or impossible to mesh with other , established theories like special and general relativity .
there is no integration of the radial part because , as you said yourself , we want the probability of finding the electron somewhere in the spherical shell between $r$ and $r+dr$ from the nucleus . ( in a differential shell between $r$ and $r+dr$ , and no need to integrate over $r$ . )
general relativity complicates things a bit , but in special relativity an inertial frame is a time independent co-ordinate system that is homogenous and isotropic . the time independence means that everything in that frame is at rest wrt everything else , and the homogenous and isotropic bit means the co-ordinate system has no curvature . take your question 1: you could define a rotating frame that rotates along with your rod , but this frame would not be isotropic ( except at the pivot point ) because there is an acceleration acting towards the pivot point and this picks out a particular direction . it is also not homogeneous because the acceleration varies with position . incidentally , your example of the rod is basically the same as the rotating disk i mentioned in my answer to your other question . you can treat the rod as a radial strip in the disk . re your other questions , can you ask these separately . as it is , the answer is heading towards essay size !
this is an extremely comprehensive review of electronic properties in two-dimensional electron systems ( 2dess ) : http://rmp.aps.org/abstract/rmp/v54/i2/p437_1 but , as you can imagine , it covers almost everything there is to cover in 2dess . for areas ( in transport ) you are focusing on you will find only sections iv c and d useful ; it involves computation of relaxation times in certain regimes . the research/review articles that i have come across so far do not talk comprehensively about transport ; most of them are limited to certain cases . general transport theory is covered in many excellent textbooks . one such example is this book by lundstrom : http://www.amazon.com/fundamentals-carrier-transport-mark-lundstrom/dp/0521631343 chapter 2 of the about book discusses computing relaxation times due to various scattering mechanisms : electron-impurity , electron-phonon ( optical and acoustic ) , electron-electron , as well as various types of scattering : inter- and intra-valley scattering etc . the rest of the characteristic scales can easily be determined from the relaxation time ( except the phase coherence length ) . the core of the book , however , makes use of the boltzmann transport equation ( bte ) , discussed in chapter 4 , which is inherently classical . however , since we often use band theory ( quantum ) alongside the bte , this is called semiclassical transport . this is why i said that you can compute all characteristic scales except for the phase coherence length . you need a fully quantum treatment to determine the phase coherence length . such a treatment is possible using non-equilibrium green 's function ( negf ) formalism . this book by datta : http://www.amazon.com/quantum-transport-transistor-supriyo-datta/dp/0521631459 gives you a very good intuition for negf since it approaches negf from the landauer formalism . if you are already familiar with the landauer formalism then all you need to do is take a look at the comparison between negf and landauer on page 27 , and then skip to chapters 8-10 . even towards the end of lundstrom 's book , i.e. chapter 8 and 9 , you will start seeing crossovers from semiclassical to quantum , diffusive to ballistic etc . the two books by datta and lundstrom are mutually complementary , and serve as an extremely valuable resource on carrier transport . if you are overwhelmed by the content in these books ( not surprising if true ) and then you can always visit the website http://nanohub.org/ to watch video lectures given by both these authors . this is an excellent class taught by datta : http://nanohub.org/resources/6172 and follows the same textbook i listed above .
if you have a string moving in a fluid in which the damping force $f$ is proportional to the transverse velocity of the wave $v$ , then you can compute the damping per wavelength that will occur - and from this you can derive the factor $\gamma$ that you are asking about . a very deep treatment can be found at http://www.people.fas.harvard.edu/~djmorin/waves/transverse.pdf
the temperature of the gas that is sprayed goes down because it adiabatically expands . this is simply because there is no heat transferred to or from the gas as it is sprayed , for the process is too fast . ( see this wikipedia article for more details on adiabatic processes . ) the mathematical explanation goes as follows : let the volume of the gas in the container be $v_i$ , and its temperature $t_i$ . after the gas is sprayed it occupies volume $v_f$ and has temperature $t_f$ . in an adiabatic process $tv^{\ , \gamma-1}=\text{constant}$ ( $\gamma$ is a number bigger than one ) , and so $$ t_iv_i^{\ , \gamma-1}=t_fv_f^{\ , \gamma-1} , $$ or $$ t_f=t_i\left ( \frac{v_i}{v_f}\right ) ^{\gamma-1} . $$ since $\gamma&gt ; 1$ and , clearly , $v_f&gt ; v_i$ ( the volume available to the gas after it is sprayed is much bigger than the one in the container ) , we get that $t_f&lt ; t_i$ , i.e. the gas cools down when it is sprayed . by the way , adiabatic expansion is the reason why you are able to blow both hot and cold air from your mouth . when you want to blow hot air you open your mouth wide , but when you want to blow cold air you tighten your lips and force the air through a small hole . that way the air goes from a small volume to the big volume around you , and cools down according to the equations above .
consider non-relativistic quantum mechanics with a finite range potential . the spectrum of the hamiltonian has a discrete sector ( possibly empty ) , corresponding to bound states , and a continuous sector , corresponding to scattering states . the asymptotic wave function of the scattering states can be characterized by phase shifts . the small momentum limit of the phase shifts determines the scattering lengths ( and scattering volumes etc . ) . by the magic of analyticity there are some relations between binding energies and phase shifts . one example is levinson 's theorem ( http://ajp.aapt.org/resource/1/ajpias/v32/i10/p787_s1 ) . another important example has to do with shallow bound states . if there is a shallow bound state with energy $e=-e_b$ , then the s-wave scattering length a satisfies $e_b=1/ ( ma^2 ) $ . this is explained in most text books on qm ( i was perusing weinberg 's book earlier this year , and he has a whole section devoted to shallow bound states . ) with regard to your questions : 1 ) for shallow bound states , $e_b=1/ ( ma^2 ) $ . in general , no direct relation except for ``global'' statements like levinson 's theorem . 2 ) the scatttering length is defined through the low-momentum limit of the scattering phase shift ; phase shifts determine asymptotic behavior of scattering states . 3 ) we call negative $a$ attractive and positive $a$ repulsive because the asymptotic wave functions are pulled in or pushed out , respectively . also , the simplest mechanism for small negative/positive $a$ is a weak repulsive/attractive potential . if there is a shallow bound state then $a$ is positive . this means that the scattering wave is pushed out ( ``repulsive'' ) even though the underlying potential is obviously attractive . this means that at low energy one cannot distinguish scattering from weakly repulsive potentials and strongly attractive ones with a shallow bound state .
actually , that first statement is not correct . the universe is not expanding due to dark energy . it is accelerating due to dark energy . the normal expansion , called metric expansion , is an effect of general relativity . when you get a homogeneous distribution of matter or radiation ( a perfect fluid , a uniform gas , radiation , a homogeneous distribution of galaxies in the case of the universe today ) , you can solve the einstein field equations for general relativity for an expanding universe , called the frw metric . in this metric , the distance in between bound objects ( i.e. . galaxies today ) increases over time . this does not require dark energy , just a universe filled with matter or radiation . a simple article about some misconceptions about the expanding universe is here , i recommend reading it : http://www.mso.anu.edu.au/~charley/papers/lineweaverdavissciam.pdf so , prior to the discovery of dark energy , the expanding universe was understood perfectly well . however , we assumed that this expansion was slowing down . however , a discovery in the later nineties that was awarded the 2011 nobel prize in physics showed that not only is the universe expanding , it is accelerating . so , this is the role of dark energy . many different candidates for dark energy have been proposed , but one is heavily favored , the cosmological constant . in einstein 's equations for general relativity , you can throw in an extra term , $\lambda$ , the would play the role of a negative pressure vacuum energy . this has the effect of accelerating the expansion of the universe . why are we confident dark energy is just a cosmological constant ? one of the defining features of a cosmological constant is its equation of state . the equation of state , $w$ , is given by $p \over \rho$ , where $p$ is the pressure it contributes , and $\rho$ is the energy density . a cosmological constant has $w=-1$ . the wmap seven year report recorded the value as $w=-1.1 ± 0.14$ . within the error margins , the cosmological constant fits very well . quantum field theory also predicts the existence of a vacuum energy , so it was hoped that this would match the value of the cosmological constant . however , the value calculated by qft was enormously higher . using the upper limit of the cosmological constant , the vacuum energy in a cubic meter of free space has been estimated to be 10^-9 joules . however , the qft prediction is a whopping 10^113 joules per cubic meter . this is the ' vacuum catastrophe ' . for a simple page from the usenet faq about the cosmological constant , see here : http://www.astro.ucla.edu/~wright/cosmo_constant.html for a very thorough description , see here : http://philsci-archive.pitt.edu/398/1/cosconstant.pdf so , because the cosmological constant works so well as a description of dark energy , and is supported by the evidence , we prefer that over a description such as quintessence , or something similar to the explanation you proposed . addition - regarding the quantum fluctuations : the very early universe was filled with an obscenely hot and dense plasma and a bath of radiation . the metric expansion of space cooled and redshifted the radiation , and broke up the plasma into a much less dense gas of hydrogen . this is the essential nature of the big bang model , which you should note has nothing to do with a ' bang ' . the model was been confirmed by observations , which you can read about here . however , there are a few problems - first is the flatness problem . we observe that the universe is very , very close to being spatially flat . since expansion would cause the universe to deviate away from flatness , it must have been even flatter at the time of the big bang . ridiculously flat . how did it get this way ? second is the horizon problem . we observe that the universe is homogeneous on large scales , that is , it is pretty much the same everywhere . this means that primordial plasma must also have been perfectly homogeneous , which is confirmed by observations of the cosmic microwave background . however , if the expansion of the universe was extremely rapid from time zero onward , how did this plasma come to equilibrium ? it certainly would not have the time to do this . and third is the monopole problem . grand unified theories , or guts , are theories that unify the electroweak interaction with the strong nuclear force . they have the unfortunate feature of predicting that hot temperatures of the early universe should have produced an abundance of heavy magnetic monopoles , which we certainly do not observe . fourth is the homogeneity problem - why are there no inhomogeneities besides galaxies ? what made the early plasma so ' smooth ' ? a model called inflation fixes all of these problems . inflation proposes that the very early universe underwent an enormous expansion , growing the universe by at least 60 $e$-folds . this expansion would be driven by the inflaton field . this field would reach an undesirable energy value , called a false vacuum . when it is in this false vacuum , it has the property that it exerts an enormous negative pressure ( somewhat similar to dark energy ) . this drives inflation . after a very short period of time , the inflaton field reaches it is true vacuum ( through normal quantum effects such as tunneling ) . when this happens , it decays into a bath of radiation , heating the universe so that the big bang model can go from there . so , how does this solve the problems of the big bang model ? well , the enormous expansion would eliminate any curvature , making the universe extremely flat . this solves the flatness problem . second , it would allow the universe to expand very slowly before inflation , allowing it to come to equilibrium . this solves the horizon problem . any monopoles produced in the early universe would have been spread out so that we would only see about one in the entire observable universe , so the monopole problem is solved . and finally , inflation would ' iron out ' any large scale inhomogeneities with the rapid expansion . so , this is where those quantum fluctuations come in - prior to inflation some regions of the primordial plasma would have become very slightly denser due to quantum fluctuations - when the universe inflates , the random changes in density that come from quantum mechanics will get magnified , and you end up with what is called a " scale free power spectrum . " it is like drawing a small line on a flat balloon . blow the balloon up , and the line will become very large . similarly , small density perturbations become primordial ' seeds ' . since these are due to random fluctuations , we would expect this to produce a universe that has an even distribution of galaxies , such as ours . from there , dark matter clumps around these seeds , which then draws in the rest of the matter to form proto-galaxies . from there , full galaxies develop .
they are two forms of the same equation , but clausius-clapeyron uses vapor pressure ( $p^*$ ) where van ' t hoff uses the reaction equilibrium constant ( $k$ ) . why does this work out ? well , think of vaporization as a chemical reaction : $$ \text{x} ( l ) \longrightarrow \text{x} ( \text{g} ) $$ the equilibrium constant is defined in terms of activity ( a ) : $$ k=\left . \frac{a_\text{x ( g ) }}{a_{\text{x} ( l ) }}\right|_\text{equil} $$ for an ideal liquid solution at modest pressure , a is just the mole fraction x . and for an ideal gas , a is just its partial pressure in bar : $$ k=\left . \frac px\ , \right|_\text{equil}$$ one equilibrium condition is $x=1\ $ and $p=p^* , \ $ so . . . $$ k=p^* $$ or $p^*$ is the equilibrium coefficient for vaporization . now since k is characteristic of a reaction at a given temperature , this would imply that if we change x , then the new partial pressure at equilibrium would change according to $$ k=p^*=\frac{p}{x} \qquad\rightarrow\qquad p=xp^*$$ and that is exactly what happens . neat , huh ?
x-rays in order to make metal radioactive one have to turn it into another element or isotope . this can be performed only with high-energy particles ( including photons ) . x-rays can be produces if an electron enters metal with very high speed in two ways : deceleration radiation ( bremsstrahlung ) an atom absorbs part of the electron 's kinetic energy , moves to one of the excited states and moves back to ground state emitting a high-energy photon in any case the energy of the incident radiation ( or particles ) must be comparable to the energy of x-ray or gamma photons . this is far from cellular phone frequency range for sure . shields and cages solid metallic shield reflects electromagnetic waves back . the material of the shield is important since it should have good conductivity . most of metals works well . as far as i know , copper and gold are the best especially for high frequencies ( microwaves ) . if the frequency is quite low there is no need for solid shield . the effective area that reflects the wave is proportional to $\frac{\lambda^2}{4\pi}$ , where $\lambda$ is the wavelength . it can be much larger than the antenna size . so metallic lattice works well if the distance between the wires is lower than $\lambda$ . gsm phones uses frequencies about 1 ghz which corresponds to $\lambda\approx$ 30 cm . for low frequencies the diffraction effects are important . if the size of the shield is comparable to $\lambda$ the radiation can just bypass the obstacle as it happens with sound . in this case metallic box is the best solution . it can be a cage with appropriate cell size . there should be no big holes like doors and windows . grounding grounding removes charges from the outside surface of faraday cage , but if you are inside there is no way to determine whether it is grounded or not . this is more concerned with safety . shape of the antenna this is important if you need something more interesting than just screening . using the effect of bragg diffraction , it is possible to build a shield that reflects one frequency and does not affect others ( this will work only for some directions ) . the shape of the shield also allows to control polarization of the radiation . edit 1 . answering the questions in the comments is the ' energy ' of incident radiation proportional to the frequency of transmitted emf waves , or what people commonly also call ' radiated power ' measured in watts/meter-sq ( as well ) ? there are two energy characteristics for emws : intensity - the amount of energy incident on unit area per unit time ( measured in w/m$^2$ ) . it describes total power of the radiation . energy of quantum - the energy of single photon ( measured in joules ) . it is equal to product of planck 's constant and frequency : $h\nu$ or $\hbar\omega$ . this value is very important in quantum mechanics since quantum system can absorb only integer number of photons . if one photon is not enough to change system state then even 1000 photons will just pass through with no effect . if emf radiation that is several hundred/thousand times in excess of international norms , could cause the x-ray generation this is possible if you have a system that can collect energy of low-frequency radiation and turns it to something else . for example , if emw induces plasma discharge between some metallic details and electrons collect enough energy before collision there can be x-rays ( i am not sure such situation is possible ) . edit 2 . answering the questions in the comments would the lattice structure/size computation be good enough if i base it on the highest frequency ( thus get the smallest lattice size needed ) ? also , does it matter if the material ( s . a . common steel mesh ) has the cross-over joints fused or insulated from each other ? if lattice period is smaller than $\lambda$ then it should work as a good screen . since you need computations and optimization it is better to ask someone who specializes in electrodynamics and antenna theory . may be it is better to ask this as a separate question . edit 3 . answering the questions in the comments is it possible to make practical application of bragg diffraction to cause destructive interference of the emf wave , when the waves are for large no . of different carrier waves , and clustered around 4-5 group of central frequencies ? this can be done with multiple bragg mirrors one for each frequency . afaik it is done for infrared radiation . apart from bragg diffraction are there other ways in which the emf can be reduced / nullified in a small region ( say within a radius of 5-6 meters ) , where the emf energy is captured using an antenna , converted to electrical energy , and converted to heat/light single antenna affects emf only within $\lambda$ distance . 5 meters is too much for 1 ghz which corresponds to $\lambda\approx$ 30 cm .
personally , i naively took it to mean a complete description of the physical universe , which does remain very vague , and may not be meaningful . i can describe for you the mathematical form of a theory of everything :  #=0  where # is a symbol which includes convoluted differential forms . this is the form all partial mathematical models of electromagnetism , mechanics , thermodynamics . . . every type of mathematical model for physical systems has taken in the past and will appear in the future . they can be reduced to differential forms on the left and a zero on the right . it will have its mathematical axioms that will make the solutions rigorous but it will have the physics postulates that have to be satisfied as the input in choosing a particular toe from multitudes of similar ones . the word " theory " in physics does not simply have the burden of mathematical rigorous existence . it could be very rigorous mathematically and irrelevant for the physics to be modeled . mathematical theories become models for physical states . the fact that it will be mathematically rigorous means that the solutions will describe correctly all known physical data and predict , given the boundary conditions , any new ones we could think about , in precise numbers . there is nothing vague about it . we expect that the standard model , which describes almost completely all known up to now particle data , will naturally nest in the model of the toe . there is nothing vague about this either . we expect gravity to be modeled naturally within toe . at the moment it seems that string theories offer all these options , but as there are thousands of possibilities , the final model has not been found yet , not even the class of models within string theories , which can be candidate for the embeddings necessary of the sm to assure consistency with existing data . if/when decided upon the predictions of the model will be tested for consistency with new data .
for the reasons given in the comment above , i think the argument from the $m\rightarrow 0$ limit is valid . but if one does not like that , then here is an alternative . suppose that a massless particle had $v&lt ; c$ in the frame of some observer a . then some other observer b could be at rest relative to the particle . in that observer 's frame of reference , the particle 's three-momentum $\mathbf{p}$ is zero by symmetry , since there is no preferred direction for it to point . then $e^2=p^2+m^2$ is zero as well , so the particle 's entire energy-momentum four-vector is zero . but a four-vector that vanishes in one frame also vanishes in every other frame . that means we are talking about a particle that can not undergo scattering , emission , or absorption , and is therefore undetectable by any experiment .
$$ \frac{1}{\sqrt{2}} \begin{vmatrix} \chi_{\mathbf{p}_1} ( \mathbf{x}_1 ) and \chi_{\mathbf{p}_2} ( \mathbf{x}_1 ) \\ \chi_{\mathbf{p}_1} ( \mathbf{x}_2 ) and \chi_{\mathbf{p}_2} ( \mathbf{x}_2 ) \end{vmatrix} $$ corresponds to $a_{\mathbf{p}_1 s_1}^{\dagger} a_{\mathbf{p}_2 s_2}^{\dagger} |0\rangle$ . here $s$ is the spin for the fermions . in the slater determinant $\mathbf{x}$ includes both spatial and spin parts .
main point : you should allow the possibility of sign factors appearing into the definition of the hilbert space representation of fermionic operators , cf . fermionic fock space . in more detail , consider the car algebra $$\tag{1} \{c_{\sigma} , c_{\tau}\}~=~0 , \qquad \{c_{\sigma} , c^{\dagger}_{\tau}\}~=~\hbar {\bf 1} , \qquad\{c^{\dagger}_{\sigma} , c^{\dagger}_{\tau}\}~=~0 , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ next define $$\tag{2} c_{\sigma}\left|0\right&gt ; ~:=~0 , \qquad \left|\sigma\right&gt ; ~:=~ c^{\dagger}_{\sigma}\left|0\right&gt ; , \qquad \left|\sigma\tau\right&gt ; ~:=~ c^{\dagger}_{\sigma}\left|\tau\right&gt ; , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ note that these definitions imply that $$\tag{3} \left|\sigma\tau\right&gt ; ~=~ -\left|\tau\sigma\right&gt ; , \qquad \sigma , \tau\in \{\uparrow , \downarrow\} . $$ in particular $$\tag{4} \left|\sigma\sigma\right&gt ; ~=~ 0 , \qquad \sigma\in \{\uparrow , \downarrow\} . $$
to close the loop , andrew , the answer to your newest question is : the best and most famous reference about the electrodynamics of moving bodies is einstein , albert ( 1905-06-30 ) . " zur elektrodynamik bewegter körper " . annalen der physik 17: 891–921 . see also a digitized version at wikilivres:zur elektrodynamik bewegter körper . the english translation , " on the electrodynamics of moving bodies " , is here : http://www.fourmilab.ch/etexts/einstein/specrel/www/ the content of this paper became known as the special theory of relativity . i am just partly joking because for uniformly moving media , the lorentz boost to the rest frame is still the most natural way to proceed .
you are not doing this wrong . as you know energy of each photon is $e = hf = 2.27ev$ so they can not produce any photoelectrons on a metal with work function greater than that .
first of all , according to earnshaw 's theorem one can not create a potential well in the ball . second , the function should fit laplace 's equation : $$ \delta \varphi = 0 . $$ in fact the second point is enough to prove the first one . so the answer is : no , this can not be made for every continuous function . yes , this follows from gauss 's law .
there is a difference between finding a solution and recognizing a solution . oracle can recognize the solution or solve a particular instance of the problem but cannot give you the solution for complete problem . or in other words , oracles gives you a part of solution and you may need to consult oracle a number of times to get the complete solution . oracle also may be thought as a library function ( as in programming languages ) which will give you solution for one instance of a problem , and the real cost of computation is measured by how many times you call the function and not the inherent complexity of the function itself which is taken as black box . for example , lets say we have a oracle for a function $f ( x ) = x^2$ , on presenting this oracle with a pair $ ( a , b ) $ it will tell whether $b^2 = a$ or not . in this case time complexity is taken as how many time you need to consult the oracle to get the desired result . more concrete example can be taken from oracle for verifying if the number if prime . lets say we want to find the first prime number $p : a &lt ; p &lt ; b , a &lt ; b \in z^+$ . the problem has different complexity when you are given access to the oracle and when you are not given . physical example of an oracle : lets say our problem is to determine the angle between the floor and the wall which may not be necessarily $90^\circ$ to it . all you can do is throw a ball which will go elastic collision on the wall and return back . you have control of the angle you throw and you can note the angle it came back . each throwing of a ball can be compared with the calling of oracle function and the constraint on the angle of the returning ball ( reflection , which gives you a hint on the orientation of the wall ) can be considered an oracle . the number of times you need to repeat the throwing of ball to get the orientation with desired accuracy may be considered as the complexity of the problem relative to the oracle .
when we say that object a is moving at speed v relative to an object b it means that there is a reference from where b is at rest and a is moving at speed v in that reference frame . if a is a photon then it moves at the speed of light c in all reference frames so if b is is us then in our reference frame it is moving at c , so it makes sense to say that the photon moves at speed c relative to us , but is it ok to say it the other way round ? if a and b are both objects that have mass so that they move at less than the speed of light , and if a is moving at speed v relative to an object b then in the frame where a is at rest b will be moving at speed v relative to a in the opposite direction . so for speeds less than the speed of light , the speed of a relative to b equals the speed pf b relative to a . it is tempting to extrapolate this to the case where a is a photon and conclude that therefore b ( us ) is also moving at speed c relative to the photon . however this would mean that we were moving at speed c in a reference frame where the photon is at rest . this is not possible because we cannot move at speed c in any reference frame and a photon cannot be at rest in any reference frame . so the answer is " no " , it is not correct to say that we move at speed c relative to the photon .
dear rodrigo , it is an interesting stronger version of the uncertainty principle for general operators $a , b$ that i have never seen before but i just verified it holds . just to be sure , the anticommutator is simply $$\{a , b\}\equiv ab+ba . $$ i like when the braces are only used for pairs of grassmannian objects but people use it as a bookkeeping device to simplify $ab+ba$ in all situations . nothing difficult about the notation . note that the commutator and anticommutator appear totally symmetrically in the inequality , a fact we will derive . to see why the stronger inequality holds , open wikipedia here http://en.wikipedia.org/wiki/uncertainty_principle#mathematical_derivations where only the simpler version of the inequality ( without the squared anticommutator ) is proved by combining two inequalities . the first one , $$ ||a\psi||^2 ||b\psi||^2 \geq |\langle a\psi|b\psi\rangle|^2 $$ remains unchanged . however , the second inequality from the wikipedia article may be strengthened to a full-fledged equality $$ |\langle a\psi|b\psi\rangle|^2 = \left| \frac{1}{2i} \langle \psi | ab-ba | \psi \rangle \right|^2 + \left| \frac{1}{2} \langle \psi | ab+ba | \psi \rangle \right|^2 $$ this identity simply says that the squared absolute value of a complex number is the sum of the squared real part and the squared imaginary part ( which was omitted on wikipedia ) . combining the previous two inequalities , one gets your " stronger " uncertainty principle . ( of course , the equation derived above is uselessly weak unless the expectation values of $a , b$ vanish themselves . it can be strengthened into yours by repeating the same prodedure for $\delta a = a-\langle a\rangle$ and similarly $\delta b = b-\langle b\rangle$ instead of $a , b$ . ) i wrote what the anticommutator means mathematically and why the inequality is true . now , what does the anticommutator term mean physically ? i do not know what this question mean . it is a term in an equation that i can read and explain for you again . the precise answers in physics are given by mathematics . so i guess that the answer you want to hear is that it means nothing physically , it is just pure mathematics . this fact does not mean that it can not be useful . well , in normal cases , the stronger version is not " terribly " useful because the anticommutator term is only nonzero if there is a " correlation " in the distributions of $a , b$ - i.e. if the distribution is " tilted " in the $a , b$ plane rather than similar to a vertical-horizontal ellipse which is usually the case in simple wave packets etc . maybe this is what you wanted to hear as the physical explanation of the anticommutator term - because $ab+ba$ is just twice the hermitean part of $ab$ , it measures the correlation of $a , b$ in the distribution given by the wave function - although the precise meaning of these words has to be determined by the formula .
in general , both iqhe and fqhe are rigid quantum states , whose rigidness is protected by the finite energy gap ( $h\omega$ for iqhe ) between the ground state ( s ) and the exited states . finite temperature can support excitations to overcome the gap , which destroys the rigidness of the state . under finite temperature , the quantization of the hall conductivity is no longer exact . if the energy scale of temperature $k_bt$ is greater than the gap , the rigidness will be completely destroyed , and the quantization effect can not be observed anymore . in particular for the iqhe you considered , the landau level is still quantized under any temperature , but the fermion occupation is not . the fermion occupation follows the fermi-dirac distribution . under finite temperature , the fermion surface is no longer sharp , so the fermions can not integer fill the landau level , as a result , the quantized transport will be smeared out by the temperature .
if you look at this problem in 2d you have the following parameters at some instant which describe your trajectory ( position and velocity ) around a celestial body with gravitational parameter $\mu$: radius $r$ , radial velocity $\dot{r}$ and angular velocity $\omega$ . there are also a few others , but these do not really matter in this problem , due to symmetry . you can calculate the radius of your periapsis by using conservation of specific angular momentum and orbital energy . $$ \epsilon=\frac{\omega^2r^2+\dot{r}^2}{2}-\frac{\mu}{r}=\frac{\omega_p^2r_p^2}{2}-\frac{\mu}{r_p} $$ $$ h=\omega r^2=\omega_pr_p^2 $$ from here you can derive an expression for the radius of the periapsis $r_p$: $$ \omega_p=\frac{h}{r_p^2} $$ $$ \epsilon=\frac{\left ( \frac{h}{r_p^2}\right ) ^2r_p^2}{2}-\frac{\mu}{r_p}=\frac{h^2}{2r_p^2}-\frac{\mu}{r_p} $$ $$ \epsilon r_p^2=\frac{h^2}{2}-\mu r_p\rightarrow \epsilon r_p^2+\mu r_p-\frac{h^2}{2}=0 $$ $$ r_p=\frac{-\mu\pm\sqrt{\mu^2+2\epsilon h^2}}{2\epsilon} $$ these two solutions correspond with apoapsis and periapsis ( the to point in the orbit at which the radial velocity is zero ) . and you might suspect that the ' plus ' solution might correspond with apoapsis and ' minus ' with periapsis . however the opposite is true , since for an elliptical orbit $\epsilon$ is negative ( it will also hold for trajectories with higher eccentricity ) , so : $$ r_p=\frac{\sqrt{\mu^2+2\epsilon h^2}-\mu}{2\epsilon}=\frac{\sqrt{\mu^2+\left ( \omega^2r^2+\dot{r}^2-\frac{2\mu}{r}\right ) \omega^2r^4}-\mu}{\omega^2r^2+\dot{r}^2-\frac{2\mu}{r}} $$ to find what would be the best angle to burn to lower your periapsis the most you could use a fixed amount of $\delta v$ and see which angle $\phi$ would lower the periapsis the most ( $\frac{\delta}{\delta\phi}\delta r_p=0$ ) : $$ \delta r_p=\frac{\sqrt{\mu^2+\left ( \omega^2r^2+\dot{r}^2-\frac{2\mu}{r}\right ) \omega^2r^4}-\mu}{\omega^2r^2+\dot{r}^2-\frac{2\mu}{r}} - \frac{\sqrt{\mu^2+\left ( ( \omega r+\delta v\sin{\phi} ) ^2+ ( \dot{r}+\delta v\cos{\phi} ) ^2-\frac{2\mu}{r}\right ) ( \omega r+\delta v\sin{\phi} ) ^2r^2}-\mu}{ ( \omega r+\delta v\sin{\phi} ) ^2+ ( \dot{r}+\delta v\cos{\phi} ) ^2-\frac{2\mu}{r}} $$ i will leave deriving this equation to you . but i suspect that lowering the specific angular momentum will have the greatest impact , so burning in the radial direction , especially when are far away . edit : to answer your second question in your ( first ) comment . adding the radial part to $\omega^2r^2$ can be done using the fact that the radial velocity component ca be expressed as $v_\theta=\omega r$ , so : $$ v_\theta+\delta{v_\theta}=\omega r+\delta{v}\sin{\phi}\rightarrow \left ( v_\theta+\delta{v_\theta}\right ) ^2=\left ( \omega r+\delta{v}\sin{\phi}\right ) ^2 $$ however your equation can be simplified to this as well : $$ \left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) ^2 r^2=\left ( \left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) r\right ) ^2=\left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) ^2 r^2 $$
the general two-particle state will look like $\displaystyle \int dp_1 dp_2 \psi ( p_1 , p_2 ) a^\dagger_{p_1} a^\dagger_{p_2}| 0\rangle $ here $\psi ( p_1 , p_2 ) $ is the momentum-space wavefunction . since the creation operators commute , only the symmetric part matters , so we may as well take $\psi ( p_1 , p_2 ) =\psi ( p_2 , p_1 ) $ ( there would be a minus sign if they were fermions ) . if you would like the state to be normalizable , it should be square integrable . the position-space wavefunction $\psi ( x_1 , x_2 ) $ , is the fourier transform . you can then choose this to be supported when $x_1$ and $x_2$ are close to the positions at which you would like to localize the particles ( or vice-versa , because of the symmetry : the particles are indistinguishable ) , for example by gaussians . the phase can then carry information on the momenta of the particles ( as is hopefully familiar from 1-particle gaussians ) , as well as on how the two are entangled . this really does not depend much on the details of the sort of particle you are talking about , except the wavefunction will be symmetric or antisymmetric depending on whether you have bosons and fermions , and particles other than scalars will carry spin degrees of freedom so the wavefunction becomes a matrix .
i have turned my comment into an answer so the question can be closed . a very similar question was asked and answered here .
jem-euso is designed to look for air showers caused by extremely high energy cosmic rays ( > $10^{19}$ ev ) . according to the website , it will be attached to the international space station in 2016 .
as far as i know we do not yet have definitive verification of non-abelian statistics which would indicate the existence of non-abelian anyons . the latest results i know about are the ones mentioned by akhmeteli and an , et . al . " braiding of abeliana and non-abelian anyons in the fractional quantum hall effect . " arxiv : 1112.3400 . however , i do not believe either are accepted as definitive proof . you are correct in that there are other candidate systems proposed , such as $p+ip$ superfluids , superconductors , and other systems . some good references and also a review seem to be given in these review articles . as i understand it , the big push to observe evidence of majorana fermions . that we do have some pretty good evidence for . pairs of majorana fermions are supposed to realize non-abelian statistics , but this has not yet been implemented and observed ( to my knowledge ) . as a final note , we do have evidence ( see here and here ) of abelian anyonic statistics , however i am under the impression that there may be some controversy about this .
1 ) some of the assumptions of the gross-pitaevskii equation ( gpe ) are : all atoms are in the same condensate wave function , the condensate is at $t=0$ , collisions between atoms are sufficiently low energy that the interactions can be well described by the $s$-wave scattering length , so that the interaction can be written $g\delta ( \mathbf{x}_i-\mathbf{x}_j ) $ . generalized gpes can also be solved , allowing for thermal and quantum depletion ( some atoms not in the condensate ) and allowing for other forms of interaction , such as dipolar . 2 ) the interaction term , $g|\psi ( \mathbf{x} ) |^2$ , is in addition to the external potential $v_\mathrm{ext} ( \mathbf{x} ) $ , the effective potential is the sum of both : $v_\mathrm{ext} ( \mathbf{x} ) +g|\psi ( \mathbf{x} ) |^2$ . the condensate density is $n_0 ( \mathbf{x} ) =|\psi ( \mathbf{x} ) |^2$ , so the interaction term is $gn_0 ( \mathbf{x} ) $ which is the potential due to interaction with the condensate itself . more detail in response to the op 's comment : the interaction potential between two atoms can usually be written as $v ( \mathbf{r}_{ij} ) $ where $\mathbf{r}_{ij} =\mathbf{x}_i-\mathbf{x}_j$ . for neutral atoms without a significant magnetic dipole moment , the dominant interaction is van der waals so $v ( \mathbf{r}_{ij} ) \propto r_{ij}^{-6}$ . when considering the scattering between two atoms , we can do a partial wave expansion ( matching incoming and outgoing wave functions and expanding in terms of legendre polynomials , e.g. " quantum mechanics " , ch . 17 , landau and lifshitz ) . for slow particles with van der waals interaction , the $s$-wave term is dominant and the interaction can be simplified to $v ( \mathbf{r}_{ij} ) = g \delta ( \mathbf{r}_{ij} ) $ where $g=4\pi\hbar^2 a_s/m$ and $a_s$ is the $s$-wave scattering length . to get a feel for the scattering length , in the the $s$-wave approximation , the cross section is $\sigma=4\pi a_s^2$ , so $a_s$ is a length scale for the interaction . the interaction potential in the gpe can be written $$\int d\mathbf{x'} v ( \mathbf{x}'-\mathbf{x} ) |\psi ( \mathbf{x'} ) |^2$$ when $v ( \mathbf{x}'-\mathbf{x} ) =g\delta ( \mathbf{x}'-\mathbf{x} ) $ , this simplifies to $$\int d\mathbf{x'} g\delta ( \mathbf{x}'-\mathbf{x} ) |\psi ( \mathbf{x'} ) |^2 = g|\psi ( \mathbf{x} ) |^2$$ 3 ) the external potential $v_\mathrm{ext} ( \mathbf{x} ) $ is generally due to applied optical or magnetic fields , and is often approximately a harmonic oscillator . the oscillator strength may be very strong in some directions creating quasi one or two dimensional confinement . a particle in a box is not possible yet ( the atoms would interact with the " walls" ) , but the external potential may be locally approximately uniform near the center of the trap . lattice potentials are also common , where ( in addition to harmonic confinement ) the atoms are trapped in a standing wave created by counterpropogating lasers resulting in a periodic potential . many other shapes are possible , such as toroids . a good reference is the book " bose-einstein condensation in dilute gases " by pethick and smith . this slightly dated review is also good ( free arxiv version here ) : section iii is relevant to your question 2 .
the use of the chemical potential $\mu$ as state variable is useful in situations where composition $n$ is variable and/or cannot be easily controlled . from an experimental point of view the chemical potential is fixed when the system is in contact with energy and particle reservoirs . at equilibrium , the chemical potential of the system equals that of the reservoir $\mu = \mu_\mathrm{res}$ . thus by modifying the parameters of the reservoir you can control the chemical potential of the system . a typical example is when the system is a layer of molecules adsorbed in a surface . this is an open system and composition is variable --and generally unknown-- . by using a gas of the same molecules as reservoir you can fix the value of the chemical potential of the open system . the chemical potential $\mu_\mathrm{res}$ of the gas can be obtained from evaluating the fundamental equation of the gas or , if this is not available , from integrating the gibbs duhem relation if the equation of state of the gas is known .
the answer is in the comment by dmckee : the reduced mass appears when you consider a two body problem , and transform it into two independent one-body problems ( motion of the center of mass , and motion of body 2 with respect to the position of body 1 ) , by way of combining the equations provided by the fundamental principle of dynamics ( f=ma ) , which hold whatever the interaction is . in other terms , the reduced mass is an inertial mass , whereas in the realm of the equations of dynamics , charge plays a role analogous to gravitational mass , when determining some force magnitude . gravitational mass and inertial mass , even though equal numerical quantities , as far as anyone can tell thus far , are conceptually two different concepts . there is therefore no reason why any equivalent reduced quantity should be useful with charges .
but in the particle 's rest frame , the process is absorption rather than emission , and it can not have some fixed rate . welcome to the joys of non-locality , where the picture of emission and absorption of a tachyon particle travelling from a to b does not really work : if we go to the critical frame , a tachyonic interaction looks like an instantaneous transfer of momentum without energy transfer - a spooky action at a distance ( cough non-local realism cough ) . non-critical observers ( including the interacting partners ) may claim to see tachyon emission and absorption , but as you already mentioned , in general will not agree on who is the emitter and who is the absorber ; as a tachyon 's worldline is space-like , it does not come with a rest frame and we can not have a clock riding along with it and tell us what ' really ' happened . the rate has to be determined by how many tachyons are available in the environment to be absorbed . the interaction is non-local , and at any given moment all matter at space-like distance is potentially available for tachyon ' exchange ' . we do not have to wait for tachyons to arrive in the neighbourhood . the real question - what makes a tachyonic interaction take place - of course still remains . also , one needs to think about how the situation changes in a general relativistic setting , where we do not have a single critical frame that covers the whole interaction .
that really depends on how you are going to deflect the motion . this is the best case : you apply a radial force . for instance , you could attach a fixed string to your mass , effectively making it a pendulum ( if that helps your imagination ) . what is important is that the mass will be moved in a circular path , because the taut string provides a fixed radius . the radial force will never do any work ( since force and movement are always perpendicular to each other ) . if you release the string after the 90° turn has been completed , you will not have put in any work into the system . it could be much worse though ( and will in reality be somewhat worse ) . for instance , you could also slow the mass to rest and then accelerate it to the terminal velocity in the new direction . this would require you to put in an amount of work equal two twice $e_{kin}=mv^2/2$ . given your quantities , that would be $w=1\ , \text{j}$ . but in theory , your initial energy is the same as the final energy , because kinetic energy is a scalar quantity and does not depend on the direction of travel . therefore , if you do it cleverly , you do not need any energy ( or work ) at all .
how should i notate my bounds of integration ? i guess you mean for this integral , passing to gauge pressure \begin{align} \int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_s ( x , t ) +b ) \ dp_s ( x , t ) . \end{align} since $p_s=p_g+p_a$ , all you need to do is that change of variables . besides , $dp_s=dp_g$ because $p_a$ is a constant . also , there is no need to say $dp_s ( x , t ) $ , since these are dummy variables . \begin{align} \int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_s+b ) \ dp_s and =\int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_g+p_a+b ) dp_s\\ and =\int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_g+p_a ) dp_s+ ( p_{s , l}-p_{s , o} ) b\\ and =\int_{p_s ( o , t ) }^{p_s ( l , t ) }p_g dp_s+ ( p_{s , l}-p_{s , o} ) ( b+p_a ) \\ and =\int_{p_g ( o , t ) }^{0}p_g dp_g-p_g ( b+p_a ) \\ and =-\frac{p_g^2}{2}-p_g ( b+p_a ) =-\frac{p_g}{2} ( p_g+2p_a+2b ) , \\ \end{align} where in the last line $p_g$ is $p_{g , o} ( t ) $ . note that $p_{s , l}-p_{s , o}=-p_g$ . how does dividing atmospheres by absolute pressure give you gauge pressure ? here i think you are confused . psi , psig , atm , and pa are all units of pressure . that is why you can add gauge pressure to atmospheric pressure in the first place . the problem is that pressures expressed in these units are not proportional since they have different zero , which is the same problem when working between the different temperature scales ( farenheit , celsius , and kelvin ) . my advice is to make every algebraic and mathematical manipulation without units , or using a single system like si , and only translate to more practical units at the end .
yes , poynting 's theorem holds even if charges or currents are crossing the surface . it is useful to study the proof of the theorem . it really boils down to maxwell 's equations only . write down some multiples and/or ( additional ) derivatives of maxwell 's equations , add them up , and you get poynting 's theorem . whenever maxwell 's equations hold , the theorem must hold , too . it would be bad if charges were not allowed to cross the surface . indeed , charges crossing the surface are nothing else than ( the elementary description of ) currents and the work done by/on currents is indeed one of the important contributions to the energy conservation law that the theorem describes very well . it is even clearer that you get a nonzero contribution to the $\vec e\times \vec b$ if you focus on the plane in between the charges . two repelling charges imagine they are separated in the $x$ direction . the electric fields on the $x$-axis go in the $x$-direction , too . the direction is appropriately tilted towards the $y$ or $z$-axes if we move away from the $x$-axis . as the charges start to accelerate , one effectively gets a current in the $x$-direction and there will be magnetic fields encircling the $x$-axis . the cross product of $\vec e$ mentioned in the previous paragraph and this " around " $\vec b$ vector will include the component in the $x$-direction again , with the opposite signs on the two sides of the two-charge system ( we are assuming the same signs of the charges ) . so this $\vec e\times b$ integrated over the transverse areas near the $x$-axis are exactly what will contribute to the equation of the poynting 's theorem . one point charge poynting 's theorem is a law in classical electrodynamics . classical electrodynamics does not contain " photons " . it only contains the electromagnetic waves – they are interpreted as a coherent state of many photons from the quantum theory . it is not just a matter of interpretations . classically , incoming electromagnetic waves are never " fully absorbed " by an object . it would violate the second law of thermodynamics , among other things . in the real , quantum world , absorption of electromagnetic waves occurs because the photons are being absorbed one by one and kick the atoms to higher discrete energy levels . but if the energy is continuous , and it always is in classical electrodynamics , there are always " soft photons " emitted and the electromagnetic field never disappears completely in the final state . at any rate , even if you could prepare such a fine-tuned situation in which the electromagnetic wave would be entirely absorbed , the theorem would work because it always works whenever maxwell 's equations do .
no , it is not correct because of relativity . because you use $e=mc^2$ , clearly , you are using relativistic effects so you need to take the full theory of relativity into account . while you might think that you have only used the special theory of relativity , you are also considering gravitational effects – the gravitational binding energy – and the general theory of relativity is the only ( and right ) viable theory of gravity among those that are compatible with the special theory of relativity . in general relativity , it is still true that the ( absolute value of the ) gravitational binding energy never exceeds the original total latent energy $e=mc^2$ of the system . up to the purely numerical factors that are corrected by general relativity , your relationship between $r$ and $\rho$ is exactly the relationship we find for black holes . ( in 3+1 dimensions , $r=2gm/c^2$ for the schwarzschild . when combined with $m=c\rho r^3$ , we get $r=2cgr^3\rho/c^2$ , exactly your relationship , for a dimensionless numerical value of $c$ . ) the only special value of $r$ for a given $\rho$ is the black hole radius . the total mass/energy of a black hole is a subtle thing . in general relativity , the total mass/energy cannot really be written as a volume integral of a nicely behaving quantity . but it may be defined asymptotically , using a probe of the gravitational field at infinity ( adm mass ) . and the adm mass of the black hole may be seen to be positive . negative-mass black holes are prohibited because they have naked singularities etc . they also violate various energy conditions and should not arise in a consistent theory because the theory would be in conflict with causality ( causes preceded their effects ) . the marginal case is the $m=0$ black hole , and one may check that this massless black hole inevitably has $r=0$ , too . classically , using your argument , you might indeed think that the gravitational binding energy converted to mass may ultimately " beat " the mass you started with . but general relativity slows down how much mass you may " subtract " in this way when the gravitational binding gets really strong , and at most , you may converge closer to a black hole which guarantees that the original mass you started with is never cancelled ( and surely never exceeded ) .
when you mix colors using watercolors , then they mix as " subtractive colors " . however , light itself mixes as " additive colors " . even though it might seem strange why the inherently same thing works so differently , it makes sense if you think about watercolors , etc . as absorbing everything but that specific color .
the classical equations of motion are not affected by changing the lagrangian $$l \qquad \longrightarrow \qquad l&#39 ; = l+ \frac{df}{dt}$$ by a total time derivative . put $f= -q_1 q_2$ . then $$l&#39 ; = -\frac{1}{2} ( q_1^2 + q_2^2 ) . $$ this lagrangian $l&#39 ; $ does not contain time derivatives , and thus there are no dynamics . the classical equations of motion are $$ q_1=0 \qquad \mathrm{and}\qquad q_2=0 , $$ in conflict with what is said in the original question formulation ( v1 ) .
jerry schirmer 's answer applies in an infinite space . if you put the system in a box there is no problem : the normalized wavefunction is $\mathrm{e}^{ikx}/\sqrt{v}$ . this is the usual theoretical device to make everything nice and well behaved . then we take the limit $v\rightarrow\infty$ at the end of the day and , if we have done our job correctly , all of the $v$ dependence should drop out of the final answer . it is not too unreasonable either , since we do not actually know that the universe is infinite . infinite for all practical purposes might as well just be a big box . edit in response to comments : assume we operate in a box of volume $v$ ( in any number of dimensions - interpret as length , area or volume as appropriate ) . we want to find the normalisation factor $n$ for the wavefunction $\psi = n \mathrm{e}^{i\vec{k}\cdot\vec{x}}$ . so we calculate the norm of the wavefunction , which must be equal to one : $$ \begin{array}{lcl} 1 and = and \int\mathrm{d}x\ \psi^\star \psi \\ and = and \int\mathrm{d}x\ n^\star n \\ and = and \left| n \right|^2 v \end{array}$$ so $n$ has a magnitude of $1/\sqrt{v}$ and an arbitrary phase which can be chosen to make it real and positive .
consider an expression of the form $$\int_x^c f ( x , y ) \mathrm{d}y$$ where $c$ is a constant . you can think of this as a mapping from real numbers to real numbers : you pick any real number $x$ , plug it in , and calculate the value of the integral . that is exactly what a single-variable function is . so you can label this function $i_c$ , define it as $$i_c ( x ) = \int_x^c f ( x , y ) \mathrm{d}y$$ and then it should make sense that you can integrate it like any other function : $$\int_a^b i_c ( x ) \mathrm{d}x = \int_a^b\int_x^c f ( x , y ) \mathrm{d}y\ , \mathrm{d}x$$ so that tells you that integration over a variable that appears in the limit of an inner integral is a perfectly reasonable thing to do , and conceptually , there is nothing complicated about it . actually coming up with a symbolic expression for the double integral is another matter , of course . in this case , the major step in going from equations ( 4 ) and ( 5 ) is integrating over $\varphi$ , so let 's do that : for the term on the left side , $$\int_{\color{red}\varphi}^{\varphi_w}\epsilon_o\frac{\mathrm{d}}{\mathrm{d}\varphi}\biggl ( \frac{e^2}{2}\biggr ) \mathrm{d}\varphi = \epsilon_o\biggl ( \frac{e_w^2}{2} - \frac{\color{red}{e}^2}{2}\biggr ) \tag{a1}$$ where $e_w = e ( \varphi_w ) $ and $\color{red}{e} = e ( \color{red}{\varphi} ) $ and for the first term on the right , $$\int_{\color{red}\varphi}^{\varphi_w}\sqrt{\frac{m_e}{2e}}\frac{j_{eo}}{\sqrt{\varphi}}\mathrm{d}\varphi = \sqrt{\frac{m_e}{2e}}j_{eo}\bigl ( 2\sqrt{\varphi_w} - 2\sqrt{\color{red}{\varphi}}\bigr ) \tag{a2}$$ the reason i am using some red variables , by the way , is that when a variable of integration appears as one of the limits , you should consider it to be a separate variable inside and outside the integral . so think of $\color{red}{\varphi}$ and $\varphi$ as different variables . if you prefer , you could give the variable a different label instead of using a different color ( so you could call it , say , $\varphi_b$ , instead of $\color{red}{\varphi}$ ) . anyway , that was easy . the tricky term is $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\int_\varphi^{\varphi_w}\frac{\varphi'/\varphi_i - 1}{\sqrt{\varphi ' - \varphi}}\frac{\mathrm{d}\varphi'}{e'}\mathrm{d}\varphi\tag{a3}$$ in this one you can not do the integral over $\varphi'$ because you do not know $e ' = e ( \varphi' ) $ and you do not have enough information about it to express it as something you can integrate . what they have done in the paper is exchange the order of integration . this is a procedure you can use on any multiple integral where the limit of the inner integral depends on an outer variable of integration . for example , in a double integral of the form $$\int_a^b \int_x^b f ( x , y ) \mathrm{d}y\ , \mathrm{d}x$$ the region of integration is $$\begin{align} a and \leq x \leq b and x and \leq y \leq b \end{align}$$ which is the blue shaded region in this picture : but you can express the same region as $$\begin{align} a and \leq y \leq b and a and \leq x \leq y \end{align}$$ as shown by this picture : which gives you this identity $$\int_a^b \int_x^b f ( x , y ) \mathrm{d}y\ , \mathrm{d}x = \int_a^b \int_a^y f ( x , y ) \mathrm{d}x\ , \mathrm{d}y$$ using this procedure on equation ( a3 ) from above turns it into $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\int_{\color{red}{\varphi}}^{\varphi'}\frac{\varphi'/\varphi_i - 1}{\sqrt{\varphi ' - \varphi}}\frac{1}{e'}\mathrm{d}\varphi\ , \mathrm{d}\varphi'$$ which is a fairly simple function of $\varphi$ , integrable as $\int 1/\sqrt{\varphi'-\varphi}\mathrm{d}\varphi = -2\sqrt{\varphi'-\varphi}$ . the full result after performing the inner integral over $\varphi$ is $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\biggl ( \frac{\varphi'}{\varphi_i} - 1\biggr ) \bigl ( -2\underbrace{\sqrt{\varphi ' - \varphi'}}_{0} + 2\sqrt{\varphi ' - \color{red}{\varphi}}\bigr ) \frac{\mathrm{d}\varphi'}{e'}\tag{a4}$$ putting together all the terms , ( a1 ) , ( a2 ) , and ( a4 ) , we get $$\begin{multline}\epsilon_o\biggl ( \frac{e_w^2}{2} - \frac{\color{red}{e}^2}{2}\biggr ) = 2\sqrt{\frac{m_e}{2e}}j_{eo}\bigl ( \sqrt{\varphi_w} - \sqrt{\color{red}{\varphi}}\bigr ) \\ - 2\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\biggl ( \frac{\varphi'}{\varphi_i} - 1\biggr ) \sqrt{\varphi ' - \color{red}{\varphi}}\frac{\mathrm{d}\varphi'}{e'}\end{multline}$$ then divide both sides by $2\sqrt{\frac{m_e}{2e}}j_{eo}$ and you get equation ( 5 ) , except for one factor of 2 on the left side which i can not seem to account for ( maybe it is lost somewhere in my calculations ) .
it does not sound exactly like a chord , but this type of technique was widely used in the 8-bit and 16-bit computer game eras , when the number of sound channels available was limited . it has a very distinctive retro video game sound , but the ear is able to identify the chord that it is supposed to be . here is a youtube video explaining how to achieve the effect using synthesis software - your situation is different , but you can probably adapt some of the advice . you can experiment with the speed of modulation , but the guy in the video sets it to around 30 hz , which sounds good . actually , i think you should be able to achieve the effect of a chord through a different method . to play a note at a frequency of $f_1$ at the same time as a note with frequency $f_2$ , you just need to time the sparks so that there are sparks at times $0 , \frac{1}{f_1} , \frac{2}{f_1} , \frac{3}{f_1} , \dots$ and also at times $\frac{1}{f_2} , \frac{2}{f_2} , \frac{3}{f_2} , \dots$ . this will sound to the ear exactly like the two notes being played at once , because that is essentially what it is .
if m1 has a positive velocity and m2 has a negative velocity , you get (m1.velocity - m2.velocity) = (positive - negative) = positive . on the other hand if the signs are switched , you will get a negative result because the particles are moving away from each other .
a slight variant on your fine answer . . . a reference is ramo et al , fields and waves in communication electronics , chapter 12 . first , reciprocity : $z_{21}=z_{12}$ tells you that ( assuming a conjugate-matched load ) : $$ g_{dt} a_{er} = g_{dr} a_{et}$$ for both transmitting ( subscript t ) and receiving ( r ) antennas , $g_d$ is the antenna directional gain . $a_{er}$ is the effective area of the receiving antenna , defined as the ratio of useful power removed from the receiving antenna $w_r$ to average power density $p_{av}$ in the incoming radiation . thus the ratio $g_d/a_e$ is the same for both transmitting and receiving antennas . for large aperture antennas , it can be shown that the maximum possible gain satisfies : $$ \frac{ ( g_d ) _{max}}{a_e} = \frac{4 \pi}{\lambda^2} $$ for other geometries , $a_e$ is defined to give the same result . for example , for a hertzian dipole , with a maximum directivity of 1.5: $$ ( a_e ) _{max} = \frac{\lambda^2}{4 \pi} ( g_d ) _{max} = \frac{3}{8 \pi} \lambda^2 $$ anyway , for the problem at hand , as you deduced , the useful power removed from the receiving antenna is : $$ w_r = p_{av} a_{er} \text{ , with the power density } p_{av} = \frac{e_b^2}{2 z_o} , z_o=377 \text{ ohms} $$ ( here , electric field and voltage are sinusoids measured as peak values . ) with a conjugate-matched load with real part $r_l$ , equating load power dissipated with power delivered gives for the receiving antenna 's thevenin equivalent source voltage $v_a$: $$\frac{ ( v_a/2 ) ^2}{2 r_l} = \frac{e_b^2}{2 z_o} a_{er} $$ $$ v_a = 2 \sqrt{a_{er}} \sqrt{\frac{r_l}{z_o}} \ , e_b $$ substituting for $a_{er}$ from the reciprocity relation , the maximum voltage $v_{a , max}$ is : $$ v_{a , max} = \sqrt{\frac{ ( g_{dr} ) _{max}}{\pi }} \sqrt{\frac{r_l}{z_o}} \ , \ , \lambda e_b $$ i am cautious about the $\cos \psi$ factor because beam patterns differ for different antennas .
you ask how can you formally go from the summation over $s$ to the double sum over $s_1$ and $s_2$ ? as we will see in a moment , passing to the double sum relies on the mathematical fact ( about tensor products of hilbert spaces ) that if $s_1$ labels a basis of states for system $1$ , and if $s_2$ labels a basis of states for system $2$ , then the set of all pairs $ ( s_1 , s_2 ) $ labels a basis of states for the composite system . let 's assume that the system at hand is a quantum system described by a state space ( hilbert space ) $\mathcal h$ . let the ( state ) labels $s$ index an orthonormal basis of $\mathcal h$ consisting of eigenvectors $|s\rangle$ of the total system 's hamiltonian $h$ ; \begin{align} h|s\rangle = e ( s ) |s\rangle , \end{align} then the partition function is obtained by summing over these states ; \begin{align} z = \sum_s e^{-\beta e ( s ) } . \end{align} notice that the state labels $s$ do not have to be numbers . they could , for example , be pairs of numbers or triples of numbers , whatever is most convenient to label the possible states for the system at hand . now , suppose that the system consists of a pair of subsystems $1$ and $2$ , then the hilbert space of the combined system can be written as a tensor product of the hilbert spaces for the individual subsystems ; \begin{align} \mathcal h = \mathcal h_1\otimes \mathcal h_2 . \end{align} let $h_1$ denote the hamiltonian for subsystem $1$ , and let $h_2$ denote the hamiltonian for subsystem $2$ . let $\{|1 , s_1\rangle\}$ be an orthonormal basis for $\mathcal h_1$ consisting of eigenvectors of $h_1$ , and let $\{|2 , s_2\rangle\}$ be an orthonormal basis for $\mathcal h_2$ consisting of eigenvectors of $h_2$ , then we have the following basic fact ; the tensor product states \begin{align} |s_1 , s_2\rangle := |1 , s_1\rangle\otimes|2 , s_2\rangle \end{align} yield an orthonormal basis for the full state space $\mathcal h = \mathcal h_1\otimes \mathcal h_2$ . in particular , the set of states one needs to sum over in the partition function can be enumerated by pairs $s= ( s_1 , s_2 ) $ . moreover , if the systems are non-interacting , then the hamiltonian of the full system is essentially the sum of the hamiltonians of the individual subsystems ( with appropriate identity operator factors ) ; \begin{align} h = h_1\otimes i_2 + i_1\otimes h_2 , \end{align} so that if $e_1 ( s_1 ) $ and $e_2 ( s_2 ) $ denote the energy eigenvalues of $h_1$ and $h_2$ corresponding to the states $|1 , s_1\rangle$ and $|2 , s_2\rangle$ , then the energy $e ( s_1 , s_2 ) $ of a tensor product basis state is their sum ; \begin{align} h|s_1 , s_2\rangle = e ( s_1 , s_2 ) |s_1 , s_2\rangle = \big ( e_1 ( s_1 ) + e_2 ( s_2 ) \big ) |s_1 , s_2\rangle , \end{align} and the partition function can be written as a sum over the tensor product basis states ; \begin{align} z = \sum_{ ( s_1 , s_2 ) } e^{-\beta e ( s_1 , s_2 ) } = \sum_{s_1}\sum_{s_2} e^{-\beta\big ( e_1 ( s_2 ) + e_2 ( s_2 ) \big ) } = \sum_{s_1} e^{-\beta e_1 ( s_1 ) } \sum_{s_2} e^{-\beta e ( s_2 ) } = z_1z_2 \end{align} where in the second equality we have used the fact that a single sum over all possible pairs $ ( s_1 , s_2 ) $ is equivalent to iterated sums over the possible values of $s_1$ and $s_2$ .
the thing to understand is how we tag neutrinos for flavor in the first place . neutrinos are created and destroyed in reactions that also involve a charged lepton ( electron , muon or tau ) . at vertex level these are $$ w^\pm \to l^\pm + \nu_l $$ and various rotations . the flavor of a neutrino is defined as coincident with that of the charged lepton produced . ( i have neglected $z \to \nu + \bar{\nu}$ reactions here , but these are far off-shell at the energies available in the core of the sun , so they do not contribute . ) the reactions in the sun are fusion reaction that do not have enough excess energy to create a heavy lepton , so the neutrinos must ( by definition ) be electron-type . this rule for neutrino flavors can be tested at accelerator where we can generate beams of known neutrino content ( because we can count the number and type of hadrons decaying to charged leptons ) , and when the beam is directed onto a detector very near-by the number of charged-current neutrino interactions of each flavor that we detect agrees with the flavor content of the beam . but there is more , we can predict the energy spectrum of the solar neutrinos assuming that they are electron-type , and that is the spectrum that we detect . by conservation of energy neutrinos created in concert with heavy leptons ( that is , other flavors ) would have a different energy spectrum . now , the evidence that the theoretical oscillation framework we use is correct is pretty diverse , but some of the very best " one plot " evidence comes from kamland , where we plot the flux of electron-type anti-neutrinos from japanese power reactors as a function of observed energy and compare to the expected $\sin \left ( \frac{l}{e} \right ) $ behavior ( appropriately convoluted to account for the many different distances to reactors ) . ( image from http://kamland.lbl.gov/ ) . full disclosure : i was a member of the kamland collaboration for about 4 years and am named as an author on the paper from which that figure is drawn .
i give the answer with the general method even though a much straightforward way would be to guess the result because it is simple . the unit of $h$ is the inverse of time denoted by $ [ \mathrm t^{-1} ] $ . the dimension of a temperature is denoted by $ [ \theta ] $ . to find the numerical value of $t$ in kelvin , one should find a combination of $c : [ \mathrm{lt^{-1}} ] $ , $\hbar= [ \mathrm{ml^2t^{-1}} ] $ , $\mathcal g : [ \mathrm{m^{-1}l^3t^{-2}} ] $ and $k_{\mathrm b}: [ \mathrm{ml^2t^{-2}\theta^{-1}} ] $ ( $ [ \mathrm l ] $ and $ [ \mathrm m ] $ represent length and mass respectively ) such that $$ c^x \ ; \hbar^y \ ; \mathcal g ^z\ ; k_{\mathrm b}^u\times h$$ has the dimension of a temperature ( $x$ , $y$ , $z$ and $u$ are unknown ) . this gives the system $$\left\{\begin{array}{rcc} y-z+u and =0 and \quad [ \mathrm m ] \\ x+2y+3z+2u and =0 and \quad [ \mathrm l ] \\ -x-y-2z-2u and =1 and \quad [ \mathrm t ] \\ -u and =1 and \quad [ \theta ] \end{array}\right . $$ the solution is $$\left\{\begin{array}{cl} x and =0\\ y and =1\\ z and =0\\ u and =-1 \end{array}\right . $$ we obtain thus $$t=\frac{\hbar}{2\pi k_{\mathrm b}}h . $$ the value of $h$ is $h=67.8\ , \mathrm{km . s^{-1} . mpc^{-1}}=2.194\times 10^{-18}\ , \mathrm{m . s^{-1}}$ . we find a temperature of $t=2.67\times10^{-30}\ , \mathrm k$ .
after stating the solution , i will try to give some physical insights to the best of my knowledge and some more references . the dimension of the required state space is given by the verlinde formula , having the following form for a general compact semisimple lie group $g$ on a riemann surface with genus $g$ corresponding to the level $k$: $$ \mathrm{dim} v_{g , k} = ( c ( k+h ) ^r ) ^{g-1} \sum_{\lambda \in \lambda_k}\prod_{\alpha \in \delta} ( 1-e^{i\frac{\alpha . ( \lambda+\rho ) }{k+h}} ) ^{ ( 1-g ) }$$ ( please see blau and thompson equation 1.2 . ) . here , $c$ is the order of the center , $h$ is dual coxeter invariant , $\rho$ is half the sum of the positive roots , and $r$ is the rank of $g$ . $g$ is the genus , $\delta$ is the set of roots and $\lambda_k$ is the set of integrable highest weights of the kac-moody algebra $g_k$ . for the torus ( $g=1$ ) , this formula simplifies to : $$ \mathrm{dim} v_{\mathrm{torus} , k} = \# \lambda_k $$ i.e. , the dimension is equal to the number of integrable highest weights of the kac-moody algebra $g_k$ . the integrable highest weights of a level-$k$ kac-moody algebra are given by the following constraints : $$ \lambda - \mathrm{dominant} , 0 \leq \sum_{i=1}^r \frac{2 \lambda . \alpha^{ ( i ) }}{ \alpha^{ ( i ) } . \alpha^{ ( i ) }}\leq k$$ where $ \alpha^{ ( i ) }$ are the simple roots , please see , for example , the following review by fuchs on kac-moody algebras . ( my favorite reference for the representation theory of kac-moody algebras is the goddard and olive review which seems not available on line ) for example for $su ( 3 ) _k$ whose dominant weights are $2$-tuples of nonnegative numbers $ ( n_1 , n_2 ) $ , the above condition reduces to : $$\mathrm{dim} v^{su ( 3 ) }_{\mathrm{torus} , k} = \# ( n_1\geq 0 , n_2\geq 0 , 0\leq n_1 + n_2 \leq k ) = \frac{ ( k+1 ) ( k+2 ) }{2}$$ to perform the computations for the more general cases , one can use the seminal review by slansky . the verlinde formula was discovered before the chern-simons theory came into the world . originally it is the dimension of the space of conformal blocks for the wzw model . this formula has been derived in a large variety of ways , please , see footnote 26 in the fuchs review . it is still an active research topic , please see for example a new derivation in this recent article by gukov . the chern-simons theory may be the most sophisticated example in which the dirac quantization postulates can be carried out in spirit . ( more precisely their generalization in geometric quantization ) . i mean starting from a phase space and utilizing a specified set of rules to associate a hilbert space to it . in the case of the chern-simons theory , the phase space is the set of solutions of the classical equations of motion . the classical equations of motion require the field strength to vanish , in other words the connection to be flat . this phase space ( the moduli space of flat connections ) is finite dimensional , it has a kähler structure and it can geometrically quantized as a kähler manifold , just like the case of the harmonic oscillator . thus the problem can be reduced in principle to a problem in quantum mechanics . the case of the torus is the easiest because everything can be carried out explicitly in the abelian and the non-abelian case , please see the following explicit construction by bos and nair , ( a more concise treatment appears in dunne 's review ) . in the case of the torus , the moduli space of flat connections in the abelian case is also a torus and in the non-abelian case it is : $$\mathcal{m} = \frac{t \times t}{w}$$ where $t$ is the maximal torus of $g$ . basically , a fock quantization can be carried away , but there is a further restriction on the admissible wave functions coming from the invariance requirement under the large gauge transformations ( please see for example , the dunne 's review ) . the invariant wave functions are called non-abelian theta functions and they are just in a one to one correspondence with the kac-moody algebra integrable highest weights . ( in the abelian case , the wave functions are the jacobi theta functions ) . in the higher genus case , although the quantization program leading to the verlinde formula can be carried out in principle , few explicit results are known , please see the following article by lisa jeffrey ( and also the following lecture notes ) . the dimension of these moduli spaces is known . in addition . witten in an ingenious work computed their symplectic volumes and their cohomology ring in some cases . witten 's idea is that as in the case of a simple spin , the dimension of the hilbert space in the semiclassical limit ( $k \rightarrow \infty$ ) becomes proportional to the volume and the leading exponent of $k$ is the complex dimension of the moduli space ( please observe for example , that in the case of $su ( 3 ) $ on the torus , the leading exponent is $2$ which is the rank of $su ( 3 ) $ which is the dimension of the maximal torus $t$ ) .
denoting by $\gamma^a$ the minkowski space gamma matrices with respect to the lorentz tetrad $\{e^a\}$ , and covariant derivative $d_a$ , then the gammas are covariantly constant . start with the massless dirac equation $$ \gamma^{b}d_{b}\psi = 0$$ act again with the dirac operator $$\gamma^{a}d_{a}\gamma^{b}d_{b}\psi=0 $$ so , since $d$ annihilates $\gamma$ $$\gamma^{a}\gamma^{b}d_{a}d_{b}\psi = 0 $$ so $$\frac{1}{2}\{\gamma^{a} , \gamma^{b}\}d_{a}d_{b}\psi + \frac{1}{2}\gamma^{a}\gamma^{b} [ d_a , d_b ] \psi = 0 \ \ ( 1 ) $$ but $$\{\gamma^{a} , \gamma^{b}\}=2\eta^{ab} $$ and $$ [ d_a , d_b ] \psi = {\mathcal{r}_{ab}}\psi $$ where ${\mathcal{r}}_{ab}$ is the spin-curvature ( antisymmetric in a and b ) . ${\mathcal{r}}_{ab}$ satisfies the identity $$ -\gamma^b{\mathcal{r}}_{ab} = {\mathcal{r}}_{ab}\gamma^b = \frac{1}{2}\gamma^b r_{ab}$$ where $r_{ab}$ is the ricci tensor ( in the lorentz tetrad ) . so ( 1 ) becomes $$ [ d^ad_a+\frac{1}{4}\gamma^a\gamma^br_{ab} ] \psi = 0 $$ i.e. $$ [ d^ad_a-\frac{1}{4}r ] \psi = 0 $$
i would like to expand a bit on the answer to the second question , but for completeness i will do both . as said in brightblades ' answer , the expansion of space is not limit by the speed of light . so objects can be moving at moderate speeds , but because the space between them and us expands faster than light , it is emissions will never reach us . for this reason , there are already regions of space that are beyond our horizon . whether or not this will always be depends on what " big thing " your cosmology ends with . if it is a " crunch " , then everything comes back together in a reverse " bang " at the end of time . if it is just a " freeze " , then the acceleration expands and possibly even accelerates forever . abraham loeb wrote a curious paper ( available on the arxiv ) about how to reach cosmological conclusions in the absence of nearby galaxies . about 100 billion years from now , all the galaxies in our local group will be beyond the horizon of the milky way ( or rather milkomeda , after the milky way collides with andromeda ) , and the cmb will be at a wavelength longer than the observable universe . but you will still be able to reach conclusions about cosmology by using hypervelocity stars being ejected from the galaxy . the point is that you can still get accurate results about the global structure of the universe using local results . as for our current model , we have a great deal of evidence that the cosmos is structured according to the concordance model . you can always say " it might turn out to be wrong " , but it can not turn out to be that wrong because of said evidence . it is like gr as a generalization of newtonian gravity : yes , newton was " incorrect " but his theory was also quite accurate , to the extent that we still use it for , say , n-body simulations of star clusters .
the simplest extension of the ideal gas law is the van der waals equation of state : $$ ( p + \frac{a&#39 ; }{v^2} ) ( v - b&#39 ; ) = kt $$ where $a&#39 ; $ and $b&#39 ; $ account for the interatomic attraction and the finite particle size respectively . these two parameters are thus candidates for " number ( s ) that describe deviations from the ideal gas law " . the proper measure of such deviations are referred to as virial coefficients which are parameters in the virial expansion - a power series expansion of the pressure in terms of the temperature . you can find details at the referenced links or in a stat mech textbook such as pathria or huang .
the system has constant pressure by definition . even in a system with changing pressure , though , for some small time , dt , there would be constant pressure . in that moment , and with those conditions , the relationship holds . adjust the pressure slightly , and a new , similar relationship is set up . the second question is related to this being a thought experiment ( an ideal situation ) . if gravity is not a factor , the pressure remains constant throughout the container . the international space station may be the best environment for testing this out , and i suspect someone may get around to trying it , if they have not already .
i think you are right , a macroscopic anisotropy is the result of a microscopic ordering of some type of the constituent . even a free electron gas ( or a plasma ) can show anisotropic optical properties when subject to an external magnetic field . when you talk about composite arrangement of isotropic elements though , you are are talking about macroscopic subparts of your system , you can not say that a molecule is isotropic . in this sense , the most simple composition of macroscopic isotropic objects which results in an anisotropic one is the following : take two isotropic pieces of solids with different optical properties and create a junction between them . the direction normal to the junction is a preferential direction , therefore the junction constitutes a local anisotropy in the system of the combined solids .
this can be resolved by being clear about what surface you are integrating over . in the first equation , $$ \oint {\overrightarrow{b} . \overrightarrow{da}} = 0 , $$ you are integrating over any closed surface , i.e. a surface without a hole in it , such as a sphere . the equation says that the magnetic flux coming in must equal the magnetic flux going out . but in the second equation , $$ e = \frac{d}{dt} \int_\sigma {\overrightarrow{b} . \overrightarrow{da}} , $$ you are integrating over a surface with a hole in it , where the hole is a loop of wire , as shown in this diagram from wikipedia : this equation says that the rate of change of flux passing through the surface must equal the emf in the wire loop . you can imagine it as coming in through the hole , and out through the surface . ( or the other way round or a bit of both . ) you can see a connection between the two equations if you imagine making the wire loop smaller and smaller until the hole closes completely . then you are back at a closed surface again , where the first equation applies - and this infinitely small loop of wire can never experience an emf .
kind of . the negative sign indicates the direction of the force exerted by the spring on the mass . if you pull the mass to the right , the force from the spring is to the left . since they go opposite directions , there is a minus sign . the problem states an external force exerted on the mass displaces it , presumably to a new equilibrium . the spring must exert an equal and opposite force on the mass to keep the mass at rest . the force exerted by the spring is actually -1 times the external force on the mass . however , the problem did not really give the direction of the forces to begin with . so , in a more carefully-worded problem ( a force of 160n to the right displaces the equilibrium point of a mass 0.05m to the right . . . ) , the minus sign becomes important . in this problem as it is written , it is fine to ignore the minus sign .
first , why is the the static killing vector $\frac{\partial}{\partial t}$ equal to $-f^{\frac{1}{2}} dt$ ? the vectors $\partial/\partial t$ and $-f^{\frac{1}{2}} dt$ are not equal to each other . they are parallel to each other , and the factor of $-f^{1/2}$ is just so that the four-velocity is properly normalized . if you plug $u$ into the metric , you have to get 1 . second , why is the velocity , along the killing time vector ? what would happen if there is a component perpendicular to it ? does this mean , the fluid does not move through space ? the killing vector supplies a preferred frame of reference , one in which the observables ( e . g . , curvature scalars ) stay constant . a perfect fluid is one for which there exists a frame such that the stress-energy tensor is diagonal ; in this frame , there is no spatial flux of energy-momentum . so basically this means that in this frame , the fluid does not move through space ( in the sense that there is no flux ) .
let 's imagine a swimmer in a swimming pool and approximate the earth as an inertial frame . the swimmer can certainly accelerate relative to the earth frame ( in the direction parallel to the earth 's surface ) ; we see this happen all the time in real life . it follows from newton 's second law , as you point out , that the net external force on the swimmer is nonzero . the only object that can possibly exert a horizontal force on the swimmer is the water in the pool ( the swimmer is making physical contact with nothing else , and the force due to gravity cannot affect his/her horizontal acceleration ) . it follows that the net horizontal force of the water on the swimmer is nonzero . if you had a blob of water floating around in outer space , and a swimmer inside , and if the blob of water and swimmer began stationary relative to an inertial frame , then the swimmer attempting to swim would cause some of the water to move backward , and this would propel the swimmer forward relative to the inertial frame . however , in this case the total external force on the blob+swimmer system would be zero , so the center of mass of the water+blob would remain stationary , but even in this case , the swimmer could accelerate relative to the inertial frame . essentially , the swimmer is doing the same thing that a torpedo would do ; he/she expels water backward , and this propels him/her forward .
heat pump should transfer heat from outside into the house . it should not generate heat ( ideally ) it should only force the heat to move . one joule of work executed by the heat pump can transfer several joules of heat - for example 4 joules ( it is the reason why the heat pump is efficient " source " of thermal energy ) . this ratio is called coefficient of performance or cop . the bigger is the difference between input and output temperature -> the more work must the heat pump do to transfer the heat . therefore the cop is decreasing . if the cop did not decrease with increasing temperature difference , it would be possible to construct a perpetual motion machine of the second kind . if work required to compress the gas was included into the computation the cop would be visible . and in order to achieve good cop the much lower output temperature would be needed . edit : computation of $cop$ with example . we will start from ideal heat engine modeled by carnot cycle . carnot cycle has efficiency $$\eta = \frac{t_h - t_c}{t_h}$$ let 's assume $t_h=310k$ , $t_c=270k$ and assume $100j$ of heat $q$ will be delivered from hot reservoir to the heat engine . as a result $$\eta \times 100j = \frac{310 - 270}{310} \times 100j = 12.9j$$ of work $w$ will be done by heat engine and $87.1j$ of heat will end up in cold reservoir . what happens when we reverse the process ? ( carnot cycle is reversible ) in the reversed process $87.1j$ of heat will be taken from cold reservoir , $12.9j$ of work will be delivered to the engine ( now the heat pump ) and $100j$ of heat will end up in the hot reservoir . the cop is $$cop = \frac{q}{w} = \frac{100j}{12.9j} = 7.75$$ and in general $$cop_{ideal} = \frac{q}{w} = \frac{q}{\eta \times q} = \frac{1}{\eta} = \frac{t_h}{t_h - t_c}$$
you should show your work , but my guess is that you have to notice the change of variables : $$\frac{d\chi}{dx}=\frac{d\chi}{d\xi}\frac{d\xi}{dx}$$ you need to do this a second time ( using the derivate of a product . see if you can continue from there .
if , indeed , $\langle\psi_1|\psi_2\rangle=d\in\mathbb r$ , you are right . in general , however , $\langle\psi_1|\psi_2\rangle=d\in\mathbb c$ and then we have $\langle\psi_2|\psi_1\rangle = \langle\psi_1|\psi_2\rangle^*=d^*$ where the asterisks denotes complex conjugation . this can be seen from the definition of the inner product in terms of wavefunctions : $\langle\psi_2|\psi_1\rangle = \int dx\langle\psi_2|x\rangle\langle x|\psi_1\rangle = \int dx \psi_2^* ( x ) \psi_1 ( x ) = \left ( \int dx \psi_1^* ( x ) \psi_2 ( x ) \right ) ^* = \left ( \int dx \langle\psi_1|x\rangle\langle x|\psi_2\rangle\right ) ^* = \langle\psi_1|\psi_2\rangle^*$ one more comment on the distinction of states and wavefunctions . $\langle\psi_1|\psi_2\rangle$ is the inner product of two states , $|\psi_1\rangle$ and $|\psi_2\rangle$ . those states can equivalently be represented by wavefunctions , $\psi_1 ( x ) =\langle x|\psi_1\rangle$ which are essentially nothing but the inner product with the eigenstates of the position basis .
note that you begin with a superpotential $w$ which gives you a generalization of the creation/anihilation operator as $a^{+} = -\frac{d}{dx} + w ( x ) $ and $a^{-}= +\frac{d}{dx} + w ( x ) $ . you will get 2 hamiltonians $h_- = a^+ a^-$ and $h_+ = a^- a^+$ . so you obtain 2 different potentials $v_{+}$ and $v_{-}$ for superpartners $v_{-} ( x ) = w ( x ) ^2 - \frac{dw}{dx}$ and $v_{+} ( x ) = w ( x ) ^2 + \frac{dw}{dx}$ . then , suppose that $\psi^-$ is a solution of $a^-\psi^- ( x ) = 0$ with $\psi^-$ normalizable , you will find that $h_- \psi_- = a^+a^-\psi^- = 0$ , so $\psi^-$ is an eigenstate of $h_- $ , with eigenvalue 0 . you have $\psi^- ( x ) \sim e^{-\int^x_{x_0} w ( x ) dx}$ ( from the definition of $a^-$ ) with the same reasoning , you will find that a $\psi^+$ solution of $a^+\psi^+ ( x ) = 0$ will give you $\psi^+ ( x ) \sim e^{+\int^x_{x_0} w ( x ) dx}$ which would be an eigenstate of $h_+ $ , with eigenvalue 0 . but you have a problem , you can see that you cannot have , at the same time $\psi^+$ and $\psi^-$ normalizable , because $\psi^+ ( x ) \sim \frac{1}{\large \psi^- ( x ) }$ . so , one of the functions $\psi^+$ or $\psi^-$ has to vanish . so , you have , at most , one ground state with zero energy . [ edit ] the practical link with supersymmetry is as follows . we define a 2 -dimensional space , with $\psi = ( \psi_- , \psi_+ ) $ . one of the $\psi$ states is a bosonic state , the other is a fermionic state . we define the supersymmetric generators : $q^- =\left [ \begin{array}{cccc} 0 and 0 \\ a^- and 0 \end{array} \right ] $ $q^+ =\left [ \begin{array}{cccc} 0 and a^+ \\ 0 and 0 \end{array} \right ] $ the hamiltonian is $h = q^-q^+ + q^+q^-$ $h =\left [ \begin{array}{cccc} a^+a^- and 0 \\ 0 and a^-a^+ \end{array} \right ] = \left [ \begin{array}{cccc} h_- and 0 \\ 0 and h_+ \end{array} \right ] $ it can be easily seen that $ ( q^- ) ^2= ( q^+ ) ^2 = 0$ and $ [ h , q^- ] = [ h , q^+ ] = 0$ unbroken supersymmetry corresponds to a ground state $|0&gt ; $ such as $&lt ; 0|h|0&gt ; = 0$ , and this imply $q^+|0&gt ; = q^-|0&gt ; = 0$ in this case , one of the functions $\psi^+$ or $\psi^-$ is normalizable , and the other vanishes . for instance , suppose $\psi^-$ is normalizable , then it means that $a^- \psi^- =0$ the case of ( spontaneously ) broken supersymmetry , is when $&lt ; 0|h|0&gt ; \neq 0$ , which implies $q^+|0&gt ; \neq 0$ or $q^-|0&gt ; \neq 0$ . in this case , neither of the ground states $\psi^+$ nor $\psi^-$ are normalizable .
i think ( although i cannot find any source for this that is even remotely reliable ) : the cracking is due to a sudden temperature gradient being exerted on the ice , because compared to the ice , the wine is usually pretty warm . this sudden rise in temperature just on the outside causes the ice to fracture . this is accompanied by a cracking sound which is due to the sudden displacement ( release of energy ) of different layers of the cube during this fracture . i am actually pretty sure this is at least part of the reason , because often when you throw in the ice , you can see fractures appearing in the ice at the same moment you hear the sound . the increase in bubbling is due to there being more seed locations for bubble formation . basically , this is true for most things you would throw in ; bubbles tend to form best in places where there is an in-homogeneity of some sort ; that is why they often form on particular points on the surface of the glass , and not just " somewhere in the middle " of the fluid . throwing an ice cube in will drastically increase the possible number of formation sites , because ice cubes are usually pretty rough , microscopically speaking .
in terms of your ordinary matrix multiplication , you have , for the case of a 4x4 matrix $m = g_{ab}$: $m\cdot m^{-1} = i$ , which is the same thing as $g_{ab} g^{bc} = \delta_{a}{}^{c}$ and $tr\left ( m\cdot m^{-1}\right ) = 4$ , which is the same thing as $g_{ab}g^{ab} = \delta_{a}{}^{a} = 4$
irreducible representations of the lorentz group are uniquely described by $ ( j_l , j_r ) $ where both numbers belong to the set $\{0,1/2,1,3/2,2 , \dots\}$ . the dimension of the representation is simply $$ d = ( 2j_l+1 ) ( 2j_r+1 ) $$ it is not hard to see that $d=5$ only occurs for $ ( j_l , j_r ) = ( 2,0 ) $ or $ ( 0,2 ) $ . i have not encountered such particles or fields in practice but it is possible to construct them .
in the literal a ) , the problem say that the force is proportional to the square of velocity , i.e. . $f\propto v^2$ for that force is equal to the square of the velocity , you must put the constant of proportionality this is $\beta$ , like the particle is acted by an opposing force then why the negative sign appears , $f=-\beta v^2$ by newton 's second law $f=ma$ and $a=\dfrac{dv}{dt}$ , then $m\dfrac{dv}{dt}=-\beta v^2$ and only remains to integrate , as in the picture . hope that helps a little , if anything asks again : )
simple answer : nothing is guaranteed 100% . ( in life or physics ) now to the physics part of the question . soft-answer : physics uses positivism and observational proof through the scientific process . no observation is 100% accurate there is uncertainty in all measurement but repetition gives less chance for arbitrary results . every theory and for that matter laws in physics are observational representations that best allow prediction of future experiments . positivism can overcome theological and philosophical discrepancies such as what is the human perception of reality . is real actually real type questions . the scientific process is an ever evolving representation of acquired knowledge based on rigorous experimental data . no theory is set in stone so to speak as new results allow for modification and fine tuning of scientific theory .
there is no " vectorlike " gauge theory in the standard model , and this is a consequence of naturalness . this means that all particles in the standard model are naturally massless , and the mass only comes from higgs mechanism . this is one of the great features of the standard model that is easy to break in any modification or extension . the teminology " vectorlike " comes from the 1950s , when people did not like 2-component spinors and thought that the world is fundamentally parity invariant . a " vectorlike " gauge field couples to a 4-spinor according to $\gamma^\mu a_\mu$ , while a " pseudovectorlike gauge field " couples to a 4-spinor according to $\gamma^5\gamma^\mu a_\mu$ . both are parity invariant , but in the first case , a is a vector ( meaning it changes sign under reflection ) , and in the second case it is a pseudovector . but the gauge fields in nature are neither vectors or pseudovectors , they are parity-violating . they couple as " v-a " meaning $ ( 1-\gamma_5 ) \gamma^\mu a_\mu$ , which is a projection operator to one two component part of the 4 component dirac spinor . this means that 4-component language is a little obfuscatory for this ( although 4 component spinor notation is still useful , becuase feynman trace identities are easier than fierz identities , and the 4-component notation most easily generalizes to higher dimensions ) . the point is that there is no parity , and the gauge fields are neither " vectors " or " pseudovectors " , they are parity violating vector fields which do not have a definite transformation under parity , because nature is chiral . so i would drop the " vectorlike " terminology , and use the term " naturally mass allowing " . a vectorlike gauge theory is " naturally mass allowing " because you can make the fermion massive . this means that the left and right partners have the same charges , and this can be considered an accident . the correct question is " why are all gauge theories in nature mass forbidding ? " this is true of all the fields on the standard model--- none of the right handed and left handed fields in the standard model can pair up to form a mass , because they are different su ( 2 ) multiplets and have different u ( 1 ) charge . why are they all unpartnered and charged ? there is a simple reason for this--- any field which can get partnered will have an arbitrary mass term in the lagrangian , and this term , without fine tuning , will end up generically being of order the planck mass . so the only fermions we see at low energies are those which are forbidden to have a mass , and therefore are chiral fermions without a partner to make a mass term with . further , all the fermions we see at low energies need to have a gauge charge , because without a charge of some sort , the fermion can get a majorana mass even without a partner , just by mixing with it is antiparticle . this is only forbidden if the particle is gauge charged in some way , so that the antiparticle has the opposite charge and the majorana mixing is forbidden . so all the fermions are chiral fermions with no partner to make a mass , so none of the low energy theories are vector-like . the simplest right way to formulate gauge theories in a parity violating universe is in terms of 2-component spinors , each with an independent coupling to a collection of gauge fields . this procedure can lead to an inconsistency , if there is an anomaly in one of the gauged symmetries , so there are global constraints on the type of chiral fermions and the representations they can be in . if none of the fermions have a partner , then the theory is natural , meaning " naturally massless " and the fermions can only get a mass from a higgs mechanism . the naturalness arguments say that the higgs mechnism must be the source of mass of all fermions in nature . but if the higgs is a fundamental scalar , the higgs itself can have a mass , and the naturalness argument fails for the higgs itself . so there is the question of why the higgs has an unnaturally light mass . this is the hierarchy problem .
the continuity equation without sink and sources reads $$\frac{\partial \rho }{\partial t} + \nabla \cdot ( \rho\vec v ) ~=~ 0 . $$ hint on how to derive it : establish first the integral form of the continuity equation for an arbitrary ( sufficiently regular ) 3d spatial integration region . next use the definition of the 3d divergence to argue the differential form of the continuity equation . the continuity equation can be rewritten with the help of a material derivative $\frac{d \rho }{d t}$ as $$\frac{d \rho }{d t} + \rho \nabla \cdot \vec v ~=~ 0 . $$ thus for an incompressible fluid $\nabla \cdot \vec v = 0$ , the density $\rho$ of a certain fluid parcel does not change as a function of time .
a brief history of the misapplication of magnetohydrodynamics to the analysis of the solar wind : 1959: soviet satellite luna 1 directly observed the solar wind for the first time and measured its strength . http://en.wikipedia.org/wiki/luna_1 so as of 1959 , by direct experimental observation , it was known that the heliopause was at least the radius of the earth or r⊙ . pneuman and kopp 1971 model : according to a more complex but still simplified mhd [ magnetohydrodynamics ] model of the coronal structure ( isp p . 114-117 etc . , the model of pneuman and kopp 1971 ) , the dipolar magnetic field lines form closed loops if they originate at solar latitudes of less than about 45° ( above or below the solar equator ) . however , those arising greater than about 45° are open field lines that may curve around the closed region to some extent but eventually extend far into space in all directions , at least beyond a heliocentric distance of about 2 r⊙ . http://www.mcgoodwin.net/pages/spacephysics_ess471.pdf ( page 36 ) this is the only paper i have been able to find that approximately reads on the magnetopause question . this is a highly cited paper and it is early enough to influence the expectations at the voyager launches ( 1977 ) . so i believe that the pneuman and kopp paper gave the expectation that the heliopaue would be at around 2 r⊙ based on mhd calculations . since this was a huge error , i have not been able to find any better detail . the man who developed mhd was hannes alfvén . he got the 1970 nobel prize in physics for this . his nobel prize lecture was partially dedicated to the task of claiming that his theory was being abused . in particular , he noted that the space physics situation was out of control . from his lecture , i have italicized the parts having to do with space physics predictions : plasma physics , space research and the origin of the solar system [ nobel prize lecture , 1970 , by hannes alfvén ] . . . the cosmical plasma physics of today is far less advanced than the thermonuclear research physics . it is to some extent the playground of theoreticians who have never seen a plasma in a laboratory . many of them still believe in formulae which we know from laboratory experiments to be wrong . the astrophysical correspondence to the thermonuclear crisis has not yet come . the reason for this is that several of the basic concepts on which the theories are founded , are not applicable to the condition prevailing in cosmos . they are " generally accepted " by most theoreticians , they are developed with the most sophisticated mathematical methods and it is only the plasma itself which does not " understand " , how beautiful the theories are and absolutely refuses to obey them . it is now obvious that we have to start a second approach from widely different starting points . if you ask where the border goes between the first approach and the second approach today , an approximate answer is that it is given by the reach of spacecrafts . this means that in every region where it is possible to explore the state of the plasma by magnetometers , electric field probes and particle analyzers , we find that in spite of all their elegance , the first approach theories have very little to do with reality . it seems that the change from the first approach to the second approach is the astrophysical correspondence to the thermonuclear crisis . . . . http://nobelprize.org/nobel_prizes/physics/laureates/1970/alfven-lecture.pdf the above lecture includes a table with a detailed comparison between the " first approach " and " second approach " . conclusion : the problem in estimating the heliopause was mostly due to theoreticians overestimating their understanding of the limitations of mhd . in particular , the mhd equations fail when electric currents are strong enough to overcome the magnetic field . this breaks the mhd assumption that ions and electrons remain pinned to magnetic field lines .
the electric field $\vec{e}$ builds up because of the misbalance of charge on the left and right side of your model . to a test charge $q$ , which we place in the middle of the charge clusters , the left side cluster appears more negatively charged than the cluster on the right side . thus you can already distinguish between the cathode ( left , negative ) and the anode ( right , positive ) of the system . what you constructed is just a very simple form of a capacitor . so your question is the same to : towards which direction do electrons flow in a capacitor ? the force on a test charge is $\vec{f}=q\cdot\vec{e}$ . it is parallel to the electric field . but your question was concerned about the direction . one could try to answer it simply with the coulomb force , which tells us that , equally charged particles are repelling . the anode will exert a less repelling force on the test charge than the cathode . thus the direction is towards the anode .
in short , i think the answers are : 1 ) yes , the approximation $ \langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle $ gives you the correct behavior for a spin system with homogenous spin values , but 2 ) there is more to mean field theory than this level of calculation the approximation $$ \langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle $$ provides an approximation for determining what the mean spin value is within the ising model , but is insufficient to actually calculate $ \langle s_i s_j \rangle$ , as you have noted . the result of this approximation is a free energy in terms of the mean field $\langle s \rangle = m$ . to get the two-point correlation function , we have to determine the energetic cost for having non-uniform $m ( {\bf r} ) $ . one natural way of doing this is to use the landau expansion , i.e. we write ( see , e.g. chaikin and lubensky chapter 4 ) $$ f = \int d^d x f + \int d^d x \frac{c}{2} |\nabla m |^2 $$ where $f ( x ) = \frac{1}{2} r m^2 + u m^4 + \cdots $ . the free energy from the first term is something that you can get from making the approximation $\langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle$ . however , this does not get you the value $c$ , which is essentially a phenomenological term ( you can relate it to the effective line tension between domains in the ising model ) . whenever you see a description of the correlation function in mft , some term like this has been included . there is also an equivalent mft scheme in field theory where the mft can be derived by a saddle-point approximation ( see kardar 's statistical physics of fields , for instance ) . however , i do not remember offhand how to get from an ising model to the appropriate field theory . . . i think this is done with a hubbard-stratonovich transformation , but i do not remember the details .
these methods do not merely simplify known feynman techniques . they uncover previously unknown structures in the final amplitudes by using entirely new ( motivic ) techniques . the renormalization procedures of the feynman method are quite hidden in the new formalism , because it does not begin by imposing locality on the underlying physics . it makes all internal lines ( in the twistor diagrams ) on shell . the hopf algebraic structure of renormalization would appear before the traditional description . so yes , the twistor description should clarify renormalization , but probably not in the way you are expecting . for a start , the complex renormalization procedure is not even required in many computations . no , these things are not being done now , because they do not really make sense with the current state of knowledge . to date , people have focused on solving n=4 sym and related theories , usually supersymmetric , or else working on concrete gluon amplitudes for the lhc . only now is it time to begin attacking qcd itself , and beyond .
in classical electrostatics , gauss ' law can be used to derive the relationship between the electric potential , $\varphi$ for a homogeneous medium ( constant permittivity , $\epsilon$ ) and volume charge density , $\rho_{v}$ in the form of the poisson equation , which , in cartestian co-ordinates , is given by : $$\nabla^{2}\varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}=-\frac{\rho_{v}}{{\epsilon}_{r}{\epsilon}_{0}} , $$ where ${\epsilon}_{r}$ is the relative permittivity ( dielectric constant ) for the homogenous medium . now , for an ionic solution at thermal equilibrium and at temperature $t$ , the charges are uniformly distributed . under the effect of an electrostatic field , the positive ions are attracted towards the negative electrode and negative ions attracted towards the positive electrode . furthermore , positive ions become repelled by other positive ions and similarly negative ions become repelled by other negative ions , until a new equilibrium is reached . at equilibrium , the ions are distributed with various energies , $e$ given by the maxwell-boltzmann distribution law , in which the probability of a particle having energy $e$ is proportional to $\exp ( -\frac{e}{kt} ) $ where $k$ is boltzmann 's constant and $t$ is the ( absolute ) temperature [ in kelvins ] . if $z_{i}$ is the number of charges of the $i^{th}$ ionic species , then its electric potential energy is $z_{i}e\phi$ where $e$ is the elementary electric charge ( $e = 1.602\times10^{-10}$ coulombs ) . the concentration ( number density ) of the $i^{th}$ ionic species at position $\textbf{r}$ is then given by : $$n_{i} ( \textbf{r} ) =n_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) , $$ where $n_{i}^{\infty}$ is the number concentration of the $i^{th}$ ionic species in the bulk solution . the volume charge density is therefore : $$\rho_{v} ( \textbf{r} ) =\sum\limits_{i=1}^nz_{i}en_{i} ( \textbf{r} ) =\sum\limits_{i=1}^nz_{i}en_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) . $$ which , when substituted into the poisson equation , gives the poisson-boltzmann equation for potential of an ionic solution : $$\nabla^{2}\varphi=-\frac{1}{{\epsilon}_{r}{\epsilon}_{0}}\sum\limits_{i=1}^nz_{i}en_{i}^{\infty}\exp\left ( -\frac{z_{i}e\phi ( \textbf{r} ) }{kt}\right ) . $$ this is a non-linear partial differential equation , the solution of which is dependent upon the specific geometry and properties of the electrolyte . for the case of an infinite sheet ( plate ) electrode located in the y-z plane at the origin , with the potential of the plate $\phi=\phi_{0}$ at $x=0$ and the potential in the solution $\phi\rightarrow0$ , $\frac{d \phi}{dx}\rightarrow0$ as $x\rightarrow\infty$ , the solution , for low potential , $\phi$ , that is , $\lvert\frac{z_{i}e\phi}{kt}\rvert\ll 1$ , the poisson-boltzmann equation becomes linearized to $\nabla^{2}\varphi=\kappa^{2}\phi$ ( the debye-hueckel equation ) which has the solution : $$\varphi ( x ) =\varphi_{0}\exp ( -\kappa x ) $$ where $\kappa=\sqrt{\frac{2z^{2}e^{2}n}{\epsilon_{r}\epsilon_{0}kt}}$ . for the non-electrostatic case ( ie : current flow and/or ion flow ) , the ions in the solution will undergo transport under the effect of the applied electric field . these ions will initially accelerate up to a speed , $s$ , limited by the hydrodynamic properties of the solute ( stokes law ) . we can calculate the hydrodynamic force , $f_{h}$ exerted on an ion of radius $r$ , travelling at speed , $s$ through the solute of density , $\rho$ and viscosity , $\eta$ to get $f_{h}=6\pi\eta rs$ from which the stokes-einstein equation for the diffusion coefficient is derived : $d=\frac{kt}{6\pi \eta r}$ from the nernst equation we can calculate the electrochemical potential of an ionic species from ionic concentration ( activity ) gradients present in solution . when coupled with the conservation of mass , we get the nernst-planck equation : $$\frac{\partial n_{i}}{\partial t}= \nabla \cdotp \lgroup d_{i} ( \nabla n_{i}+\frac{q_{i}n_{i}}{kt}\nabla \phi ) \rgroup . $$ of course , these models include assumptions which may not always be valid , such as the use of stokes law or the assumption of non-interaction of ions . more accurate numerical models may require the use of empirical data to account for these and other effects , including chemical reaction kinetics .
no . there are two different types of an angular momentum . first is connected with the coordinate representation , so it can be interpreted from classical mechanics point of view . second is not connected with coordinate representation , but it exist in every particle of the free field ( i.e. . , is an own angular momentum ) which you have tested . they are the principal different types of an angular momentum . from the qm position , they both are the eigenvalues of the representations of 3-rotation generator , but first refers to reducible , and second - to irreducible representations . maybe it is more convenient for you to compare the spin and electrical charge . mainly we do not ask about origin of charge and do not interpret it as the result of other quantity . it is independ quantity , and it is existence leads to electromagnetical interaction . also , the existence of spin ( it is value ) leads to some spin interaction ( simplistically can imagine as result of fermi-dirac or bose-einstein statistics . also , we can make an analogy between the quantum spin of particle and an own angular momentum ( classical spin ) of the system of particles . first and second are not connected with motion the particle ( or system ) as whole .
it is certainly possible for a particle 's mass to come partially from kinetic energy of massless particles ; for example , about half of a proton 's mass is the kinetic energy of its gluons . but the kind of mass that fundamental particles have , the kind that comes from the higgs mechanism , does not appear to be of that kind . maybe someday we will discover that it is . ( this would be the case if string theory turns out to be correct , for instance . ) by the way , scientists do not believe that mass is fundamentally different from energy . mass is just one type of energy .
why do you assume that in the case of torques , the torques must cancel out , in my opinion the best way to deal with this problem is your method 1 , but you can solve by taking torques as follows : when you are balancing along y-axis ( calculating x ) $6a × g × 4 + 29a × g × x = 35a × g × 2.5$ here a is mass per unit area , 4 is the x coordinate of centre of mass of small cut off block , x is the coordinate of centre of mass of left over block and 2.5 is the coordinate of original block we have equate the combined torque of cut off and left off pieces with that of original piece , you can do similar operation for y coordinate $6a × g × 5.5 + 29a × g × y = 35a × g × 3.5$ these operations give the same results as those obtained by method 1 , so there is no error .
for resistors $r_1 , r_2 , \dots , r_n$ in parallel , the equivalent resistance $r_e$ is given by $$ \frac{1}{r_e} = \frac{1}{r_1} + \frac{1}{r_2} + \cdots + \frac{1}{r_n} $$ if two resistors with equal resistance $r_1 = r_2 = r$ are in parallel , then this gives $$ r_e^{ ( 2 ) } = \frac{r_1r_2}{r_1+r_2} = \frac{r^2}{2r} = \frac{r}{2} $$ if three identical resistors $r_1 = r_2 = r_3 = r$ are in parallel , then the equivalent resistance is $$ r_e^{ ( 3 ) } = \frac{r_1r_2r_3}{r_1r_2 + r_2r_3 + r_3r_1} = \frac{r^3}{3r^2} = \frac{r}{3} $$ in fact , for $n$ identical resistors one has $$ \frac{1}{r_3^{ ( n ) }} = \frac{n}{r} $$ so that $$ r_e^{ ( n ) } = \frac{r}{n} $$ and therefore the resistance decreases with the addition of each successive resistor in parallel .
the x-ray diffraction pattern is the fourier transform of whatever is doing the diffracting . if you had an infinite plane of atoms then the spots ( rings in a powder pattern ) would be infinitely sharp because the fourier transform of an infinite wave is a delta function . however a real crystal is the product of an infinite plane with an envelope function , where the envelope is the size of the crystal , so the spot is the convolution of a delta function with the fourier transform of the envelope function . in a powder pattern we have many crystals of differing sizes , so the average fourier transform of the crystal size ends up looking something like a gaussian and the spots have a roughly gaussian profile . re your formula , suppose the average crystal ends up looking like a sphere , i.e. a disk in profile , then it is fourier transform is going to be an airy disk ( but blurred out by the variation in particle size ) . the half angle subtended by the airy disk , $\beta$ , is ( for small angles ) : $$ \beta \propto \frac{\lambda}{d} $$ which is the scherrer equation for small $\theta$ ( i would have to go away and look at the derivation of the scherrer equation to remember why there is a factor of $\cos\theta$ , but for small $\theta$ this factor is approximately $1$ anyway ) . response to comment : the incoming x-rays are scattered by the atoms in the crystal , so each atom acts as an x-ray source . we get the diffraction pattern by summing up the x-rays emitted ( i.e. . scattered ) by all the atoms in the crystal . if you start with one atom then the scattered x-rays will be just be a spherical wave . add a second atom and now the pattern will be the same as the young 's slits experiment . as you add more and more atoms the pattern will tend towards the pattern we expect from a large crystal , however to get an infinitely sharp spot would require an infinite number of atoms . when we have a finite number of atoms the spot will have a finite width . it is a bit like a fourier synthesis ( which is where we came in ) . each atom adds a term to the fourier sum , but to get a perfect transform of the lattice requires contributions from an infinite number of atoms . with a finite number of scatterers the diffraction pattern will only be an approximation to the ft of the lattice .
often , the classical limit of a quantum system may be seen by simply taking the limit as $\hbar\rightarrow 0$ . for example , a quantum harmonic oscillator has energy levels which are multiples of $\hbar$ . $$e_n= \hbar\omega\left ( n+\frac{1}{2} \right ) $$ in the limit as $\hbar\rightarrow 0$ , we can see that energy levels become continuous . another example is heisenberg 's uncertainty principle . the uncertainty in a particle 's position ( $\sigma_x$ ) times its uncertainty in momentum ( $\sigma_p$ ) , obeys the following relation : $$\sigma_x\sigma_p\geq\frac{\hbar}{2}$$ in the limit as $\hbar\rightarrow 0$ , no such restriction between energy and momentum exists .
this document ( nb it is a pdf ) contains details of the beam operation . here 's a key graph nabbed from the presentation : at the end of an experimental run the beam is dumped , and it takes about an hour and a half to get the beam back up to full energy and intensity . once the beam is at full strength the lhc generates data continuously for somewhere between 10 and 20 hours before the beam intensity is too low and the beam needs to be dumped again . note that the lhc is not an experiment that runs once and generates one result , then repeated to generate a second result and so on . once the beam is live it generates data continuously and this data builds up for days and months . because signals like the higgs are so weak you need months and months worth of data , i.e. months and months of beam time , to get enough data to see these small signals . cern have made an animation showing how the higgs signal built up over time , which you can see here .
hints to the question ( v1 ) : let $\hat{z}^i$ be operators that satisfies a heisenberg algebra $$\tag{1} [ \hat{z}^i , \hat{z}^j ] ~=~i\hbar ~\omega^{ij} ~{\bf 1} , $$ where $\omega^{ij}=-\omega^{ji}$ is an antisymmetric real matrix . the important property will be that the commutators $ [ \hat{z}^i , \hat{z}^j ] $ belong to the center of the heisenberg algebra , i.e. commute with everything . let $f ( \hat{z} ) $ and $ ( \partial_{j}f ) ( \hat{z} ) $ denote the weyl-ordered operators corresponding to the functions/symbols $f ( z ) $ and $$\tag{2} ( \partial_{j}f ) ( z ) ~:=~\frac{\partial f ( z ) }{\partial z^j} , $$ respectively . then using e.g. this formula for the weyl-ordered operators , it is possible to show$^1$ $$\tag{3} [ \hat{z}^i , f ( \hat{z} ) ] ~=~\sum_j [ \hat{z}^i , \hat{z}^j ] ~ ( \partial_{j}f ) ( \hat{z} ) . $$ the corresponding classical formula $$\tag{4} \{z^i , f ( z ) \}_{pb}~=~\sum_j \{z^i , z^j\}_{pb}~ ( \partial_{j}f ) ( z ) $$ is a consequence of the poisson property/leibniz rule for a poisson bracket ( pb ) . -- $^1$ equation ( 3 ) actually also holds for many other operator-orderings .
as x-rays and $\gamma$-rays have very low wavelength , one could think of building an x-ray or gamma-ray microscope . but , the problem only arrives at focusing both . they can not be focused as visible light is focused using refractive convex lenses ( in microscope ) thus providing a magnification of about 2000 . another problem with gamma rays is that they have very high ionizing power and interact with matter to the maximum extent thereby destroying it ( causing atomic decay ) . but on the other hand , we have electron microscopes which work on the principle of wave nature of moving electrons . electrons accelerated through a potential difference of 50 kv have a wavelength of about 0.0055 nm . ( which is according to de-broglie relation of wave-particle duality - $\lambda=\frac{h}{\sqrt{2mev}}=\frac{1.227}{\sqrt{v}}$nm ) this is $10^5$ times less than the wavelength of visible light there by multiplying the magnification by $10^5$ . if you have read enough about electron microscopes , you should've known the fact that electrons could be easily focused using electric and magnetic fields than going into a more complex one . . . : ) even if these great physicists try something of focusing the gamma rays , it is production and maintenance would be far too difficult and expensive either . because , we know that $\gamma$-rays could be produced only by means of radioactive decays which is biologically hazardous . . .
it is just as simple as you suggest . at the moment my feet are exerting a pressure on the ground of my weight divided by whatever the area of my shoes is and the pressure exerted by the ground on me is what keeps me stationary . exactly the same applies to your air bag . once the air pressure is the same as the pressure you exert on the bag it will support you . but there are a couple of extra things to consider . when you stand on the bag you will compress the air in it and you will sink until the air is compressed enough to match the pressure of your shoes . so the initial pressure can be lower than your shoe pressure and the bag can still keep you off the ground . you mention the bag size , it is probably easier to compress the gas a lot in a small bag than in a large bag , so a small bag would probably work better . there is nothing especially fundamental about this ; it is just that a large bag allows more room for the air to move into as your feet compress it .
in a word , no . the drift velocity is always very small in common circuits ( ~$10^{-4}cm/s$ ) . in this scenario , when the plates are connected , the electrons still travel slow but all of the electrons along the wire start moving almost at the same time , so even though they move slowly , the electric charge on the plates will vanish quickly . the electrons on the negative plate move into the wire and the positive plate is filled with electrons that were previously in the wire right next to it . this is the difference between the speed of current and the speed of the electrons . current travels very fast ( it is like the speed of sound in the free electron sea ) . electrons drift very slow .
even without going to the com frame , we can solve this problem ( actually since one of the given parameters is an angle in the lab frame , i am not sure if moving to com will help in this case ) . our given relations are : momentum 4-vector length ( for each particle , i.e. photon and electron ) : $$\left ( \frac{e}{c} \right ) ^2-\vec{p}^2=m^2c^2$$ ( for photons $m=0$ ) conservation of momentum 4-vector ( 3 equations ) : $$2m_e c^2=e_{\gamma}'+e_e ' \\ m_e c = p_{\gamma}' \cos \phi+p_e ' \cos \vartheta \\ 0=p_{\gamma}'\sin \phi-p_e ' \sin \vartheta$$ these can be solved to give $e_\gamma'$: $$e_\gamma'=\frac{5-3 \cos ( 2\theta ) }{7-\cos ( 2 \theta ) }m_e c^2\approx 0.335 mev$$
i do not know of any research to find out if skin sunburns faster when wet , though someone did a comparable experiment to find out if plants can be burnt by sunlight focussed through drops of water after the plants have been watered . you need to be clear what is being measured here . the total amount of sunlight hitting you , and a plant , is unaffected by whether you are wet or not . the question is whether water droplets can focus the sunlight onto intense patches causing small local burns . the answer is that under most circumstances water droplets do not cause burning because unless the contact angle is very high they do not focus the sunlight onto the skin . burning ( of the plants ) could happen if the droplets were held above the leaf surface by hairs , or when the water droplets were replaced by glass spheres ( with an effective contact angle of 180º . my observation of water droplets on my own skin is that the contact angles are less than 90º , so from the plant experiments these droplets would not cause local burning . the answer to your question is ( probably ) that wet skin does not burn faster . i would agree with will that the cooling effect of water on the skin may make you unaware that you are being burnt , and this may lead to the common belief that wet skin accelerates burning .
matt strassler goes into detail with lhc data here : http://profmattstrassler.com/articles-and-posts/largehadroncolliderfaq/whats-a-proton-anyway/checking-whats-inside-a-proton/
let 's explain the principle of rotation here from the scratch . say you have the circuit with 1t magnetic field perpendicular to it but the battery is intially switched off . obviously the wheel will not rotate now as there is no moving charge and hence no lorentz force . now when you switch on the battery , electrons will flow through the spokes from outer rim to inner rim . now due to this motion of the electrons , the wheel will start to rotate clockwisely ( just apply lorentz force $\vec{f}=-e ( \vec{v}\times \vec{b} ) $ , where e is positive ) . now the electrons have another velocity due to motion of the wheel in addition to the radial velocity due to the battery . remember this velocity starts from zero as the wheel was initially not rotating . due to this new velocity ( which is increasing from zero ) , the electrons in the spoke will feel a force along the outward direction ( again apply lorentz force due to this new velocity ) . see as the wheel rotational speed increases this force increases , which reduces the velocity of the electrons flowing from outer rim to inner rim . after some time a situation comes when there is no flow of electrons from outer rim to inner rim , which you have described by $\ u_0=u_i$ . at this situation there will be no lorentz force perpendicular the spokes . as there is perpendicular force on the spokes , if you consider torque ( = $\vec{x}\times \vec{f}$ ) , then that will be zero . hence angular momentum $\vec{l} ( =i\vec{\omega}$ ) is constant . " but elsewhere i was told that kirchhoff 's rules do not apply in systems with changing magnetic fields . " but before answering this , at first let me answer the following : " also i am not sure if there still would be an emf induced in stationary conditions since the flow would not change anymore then . " obviously , there will be induced emf in the stationary condition . see lorentz force is still present along radially out on the electrons due to the wheel rotation . see , i do not know why you want to use kvl or something . you have some induced emf in the spoke which is opposing the battery so that no current can flow in the stationary state . nothing else .
please note that in the picture , there are two forces acting : 1 ) the weight , mg , which acts vertically downward , and does not change , and 2 ) the tension in the string , z , which points from the mass to the point the string connects to the ceiling , provided the string remains taut . z varies with time periodically . these two forces combine to give the resultant force , and it is the resultant force which occurs in the same direction as the acceleration , as seen in the gif . the green arrows in the picture are actually just the tangential and normal components of gravity . edit : also , i believe the source of confusion might have lied with assuming the normal component of gravity cancels with the tension . this is not the case : you cannot use the equations of equilibrium if the system is not in equilibrium , i.e. accelerating .
yes , $ ( h^\dagger h ) ^2$ is invariant under $su ( 2 ) \times u ( 1 ) $ because even without the second power , $h^\dagger h$ is invariant under it . by that , we mean $$\sum_{i=1}^2 h_i^* h_i$$ of course that it is invariant under $u ( 1 ) $ because the $u ( 1 ) $ charges of $h^\dagger$ and $h$ are opposite in sign and add up to zero . note that the transformation of a charge-$q$ field is $f\to f\exp ( iq\lambda ) $ . the invariance under $su ( 2 ) $ is also self-evident because $\sum_i z^*_i z_i$ is exactly the bilinear ( with one asterisk ) invariant that defines the unitary groups . no , the invariance of $h^\dagger h$ or its square does not mean that the hypercharge of $h$ itself is zero or isospin is zero . moreover , the op seems to confuse the spin and the isospin .
suppose you have some collection of matter that is so dense it has an event horizon where the escape velocity is greater than the speed of light . the escape velocity is obviously due to the strong gravitational field of the matter inside the event horizon , and equally obviously that matter is also pulled by it is own gravity towards it is centre of mass . also obvious is that because the surface of your collection of matter is nearer to the centre of mass than the horizon is , the gravitational pull on it must be even stronger than the gravity at the event horizon i.e. the ( hypothetical ) escape velocity would be even faster than the speed of light . the reason why this situation is not stable is that the matter making up your object cannot resist the force of it is own gravity and is irrestably pulled inwards until it forms a singularity . at that point we have a standard black hole with an event horizon and a singularity at the centre . to understand why the matter within the event horizon cannot avoid being pulled down into a singularity you have to do some maths . if you are interested my answer to why is a black hole black ? gives a hopefully not too scary explanation of the maths . i think there is a semi-plausible way to explain why the matter can not avoid collapsing into a singularity , but do not take this too literally . i have mentioned above that if the escape velocity at the event horizon is the speed of light , the escape velocity inside the event horizon must be faster than light . but all forces , e.g. the electrostatic forces that hold you in shape , propagate at the speed of light . that means inside the event horizon the electrostatic force can hold matter in shape because it can not propagate outwards fast enough . this also applies to the weak and strong forces , and the end result is that no force is able to resist the inwards fall of the matter into a singularity .
deep lattice limit corresponds to the tight binding limit where the physical entities ( bosons or fermions ) are localized and there is very less hopping allowed and no long range hopping is considered . while shallow lattice limit is the one in which the wavefunction for an entity on one of the lattice sites has significant amplitude on other near lattice sites and thus there is a non-trivial long range hopping involved . the scale for these limits is the depth and half-width of the lattice potential . higher depth and lower width indicate deep lattice limit and vice-versa .
i am guessing this is related to the archaic paris inch , which is $27.069$mm , i.e. $10^8 \times$ the conversion factor . reference : scientific papers vol 2 1881-1887 , john william strutt .
this should be a comment since i am not a string theorist but its too big . when luboš ( luboš correct me if i am wrong ) speaks of the " shape " in his comment : they are spatial dimensions - new temporal dimensions always lead to at least some problems if not inconsistencies - but otherwise they are the same kind of dimensions as the known ones , just with a different shape . to a creature much smaller than their shapes ' size , they are exactly the same as the dimensions we know . the theory implies that the total number of spacetime dimensions is 10 or 11 . we do not have " intuition " for higher-dimensional shapes because the extra dimensions are much smaller than the known ones but otherwise they'er intuitively exactly the same as the known spatial dimensions he means that the higher dimensions are " compactified " . a simple example of a compact space is a circle , or a cartesian product of circles ( a torus ) or a high dimensional sphere . the crucial idea is that they are topologically compact , meaning roughly that they are finite and closed *i . e . * they have no boundary just like the torus or sphere have no boundary . so luboš 's little creatures would return to their beginning point if they walked far enough in the same direction . as i understand it , one of the proposed " shapes " for the compactified dimensions is the calabi-yau manifold . wholly for gazing on beauty 's sake , its worth also looking these here at the wolfram demonstrations site . be aware that you are looking at a projection , hence the seeming " edges " are not the manifold 's boundary . like the torus and the sphere , these manifolds would let a little creature return to their beginning point eventually by travelling in a constant direction and nowhere would they come across a barrier or boundary . actually , it is not out of the question that the three spatial dimensions of our wonted experience are like this too , just that we are talking awfully big distances ( 10s to 100s of billions of light years ) for us to come back to our beginning points if we blasted off into space and kept going in the same direction . as i understand this , this idea is seeming less and less likely since our universe globally is observed to be very flat indeed . see an interesting discussion at mathoverflow on what the fundamental group of the universe might be like update : see also this answer clarifying some of my description of compactified dimensions . if they are big enough ( as for our everyday three spatial dimensions , if they are compactified too ) even though a constant direction vector can be integrated to a closed loop through space , the fact that the universe is expanding means that one cannot traverse this loop in a finite time .
the name " hidden valley " models was coined in 2006 by strassler and zurek http://arxiv.org/abs/hep-ph/0604261 and the paper above , references 1-6 , show you the origin of these papers ; 6 are stringy constructions . the models may be supersymmetric but they do not have to be . the " valley " refers to some place in the hidden dimensions of spacetime where an additional gauge group $g$ added as a factor to the standard model is localized . the new states are uncharged under the standard model and only charged under the new gauge group . the new gauge group and the new particles/fields charged under the new group are ad hoc from the field theory viewpoint and there is no minimal choice . on the other hand , such new gauge groups and states are natural in string theory so it makes to study and realize what consequences such hypothetical states would have .
do not forget that the polarization tensors depend on the gauge choice via reference vectors ( call them $q$ , $q'$ ) now you have to check what happen when you chance the reference vectors from $q , q'$ to some new vectors $r , r'$ . the change of the vectors will lead to the new polarization vectors aquire a term proportional to its momentum $p$ . $$\epsilon ( p , r ) ^\mu \sim \epsilon ( p , q ) ^\mu+p^\mu$$ the contraction of the last term with $m_{\mu\nu}$ vanishes , i.e. you have shown gauge invariance . do you also have to show that $m_{\mu\nu}$ contracted into one of its momenta vanishes ?
the answer is in physics , but not ohm 's law . the lights are dimmed when you turn on the headlights because the car assumes that if the headlights are on , then the cabin is dark . if the cabin is dark , then control lights need to be dimmed so they do not distract the driver . this usually works just fine , unless you turn on the lights when it is not all that dark . the physics of this is called " dynamic range " - the lights will look brighter when you do not have the sun shining around them . in most cars , you can adjust the brightness . check your manual .
all of the ice core methods of measuring temperature in past millenia from gas trapped in bubbles in ice , and measuring concentrations , depend on the fact that permeability is small even in imperfect containers , as the ice in the glaciers . so the answer to " how long " would depend on the exact materials and geometry and temperature , and it will be in any case the gas will disappear if you wait long enough , and long is more than a few million years . there is of course the sublimation of the molecules of the flask which will eventually make holes no matter how perfect the material , but also the quantum mechanical " tunneling effect " will come to play a role if time is long enough .
as anna pointed out in her comment , it really depends on the system . in general , there is not much you can say except that the minimum of kinetic energy corresponds to the minimum speed and the maximum of kinetic energy corresponds to the maximum speed . there are many systems in which the minimum speed ( and thus the minimum kinetic energy ) is zero , and in those systems , finding the times at which the kinetic energy is a minimum tells you when the object stops . mathematically , if you actually take the derivative of kinetic energy with respect to time , you get $$\frac{\mathrm{d}k}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t}\biggl [ \frac{1}{2}mv^2\biggr ] = m\vec{v}\cdot\vec{a}$$ ( using the nonrelativistic expression for $k$ ) . there are three ways this can be equal to zero : $v = 0$: the object is not moving . this corresponds to a minimum of kinetic energy . $a = 0$: the object is not accelerating ( and by $f = ma$ is also experiencing no net force ) . this corresponds to either constant velocity motion , or a maximum of kinetic energy , in a simple harmonic oscillator for example . $\vec{v}\perp\vec{a}$: the object is experiencing pure centripetal acceleration , which means it is moving with a momentarily constant radius of curvature . this can be either a minimum or a maximum of kinetic energy , in an orbit for example . using the relativistic expression , you get $$\frac{\mathrm{d}k}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t}\bigl [ ( \gamma - 1 ) mc^2\bigr ] = \frac{m\vec{v}\cdot\vec{a}}{\bigl ( 1-\frac{v^2}{c^2}\bigr ) ^{\frac{3}{2}}}$$ from which the same conclusions follow .
your question can be translated into " if right now we would send a powerful omnidirectional light pulse from earth into space , would there be galaxies that never see this light pulse ? " the answer is " yes " . due to the accelerated expansion of the universe , as described by the lambda-cdm model , only galaxies currently less than about 16 billion light years ( the difference between the cosmological event horizon and the current distance to the particle horizon ) away from us will at some time observe the light pulse . a nice visual representation of this can be found in figure 1 of this publication .
there are , essentially , three types of ensembles used in thermodynamics : microcanonical ensemble : this is used to describe closed systems . the number of particles and the total energy are constant ( since no interaction with the environment takes place ) . in such a system , the entropy will eventually be maximized . canonical ensemble : now , your system is in contact with a big reservoir and allowed to exchange energy with the system . this means that your system will be in thermal equilibrium with the bath . grand canonical ensemble now , your system can exchange both energy and particles with the system . hence , it will be in thermal equilibrium with the bath and in chemical equilibrium , i.e. the chemical potential for adding a particle to your system equals the chemical potential for adding a particle to the bath . if exchanging particles was not allowed in the grand canonical ensemble , it would become the canonical ensemble .
what force particle mediates electric fields and magnetic fields ? photons , as you have suggested . 1 ) would not that mean that a charged particle ( e . g . an electron or even a polarized h2o molecule ) would constantly be losing energy from sending out photons ? you must describe this process in a quantum field theory . virtual photons emitted by charged particles are reabsorbed in a time consistent with uncertainty principle . hence over some finite amount of time , energy is conserved . 2 ) would not that mean that an electric field is inseparable from a magnetic field , as photons have both - and that one can not have one without the other ? you can already show this in classical electromagnetism - see maxwell 's equations . 3 ) would it be possible , then , to determine the wavelength of magnetic-field-mediating photons ? if so , what is the wavelength - is it random or constant ? the wavelength of a photon is related to it is energy , which is again related to the uncertainty principle . the longer the time borrowed from the vacuum , the lower the energy of the photon , so it has a longer wavelength . hence the wavelength of virtual photons at large distances from the em source is much longer than at short distances . 4 ) how can a photon ( which has momentum ) from one electrically charged particle to an oppositely charged particle cause these particles to be pulled toward each other - or how can a magnetic field cause an electrically charged moving particle to experience a force perpendicular to the source of the magnetic field if a particle with a non-zero mass moving between the two is the mediator of that force ? this become less intuitive depending on your background . richard feynman introduced a trick which offers a way to imagine the process . imagine the photon is emitted between opposite-charge particles in the future and travels ' backward in time'- therefore its momentum minus minus what it really is . this is explained in good detail here . if " virtual photons " are involved , please explain why they work differently from regular photons unlike ' real ' photons ( which have transverse polarisation ) , virtual photons have both transverse and longitudinal polarisations . the energy momentum four vector of the virtual photons , and generally all virtual particles , is not necessarily 0: virtual particles are off mass shell . this means that virtual photons may have non-zero mass - which means that they also have a longitudinal polarisation state . it is important to consider the extra polarisation in your calculations .
what are you saying is completely wrong because einstein just extended the rules of newton for high speeds and velocities only with some new ideas . for normal applications of mechaics newtonian ideas are almost equal to reality .
in order to understand asymptotic freedom , you need to be aware of the concept of renormalization . since you want a qualitative description , just think of renormalization a modification of the coupling strengths and masses of particles at high energies . this is roughly like pushing a ball through the water ; the harder you push , the more the water sticks around it and the harder it is to move . this can be modeled with newton 's 2nd law $f=ma$ by replacing the mass with a slightly larger mass $m+\delta m$ , and this $\delta m$ depends on the velocity of the ball in the water . ( that discussion can be found in section 3.2 of connes and marcolli , " noncommutative geometry , quantum fields and motives" ) once you have the concept of renormalization , asymptotic freedom is a property the strong force has as you scale the coupling constant to high energy . rather then the coupling getting stronger , it gets weaker . this has major consequences for confinement - that is , bound quarks . at low energies , quarks in bound states are forever bound - it becomes harder and harder to pull them apart the further apart you pull them . at high enough energies ( say , colliding two protons at 7 tev like the lhc ) the quark coupling gets small and quarks are essentially free and unbound . it should be easy to see how this would change the cross section . as a sidenote , only the strong force is asymptotically free . the e/m and weak force become stronger as the energy gets higher . in addition , it is important to realize that we cannot solve problems involving the strong force at low energies ( if you could , the clay mathematics institute would give you $1 million ! ) . once they are at high energies , the strong coupling is weak so qcd acts quite a bit like qed .
first of all , i am not an expert on magnetism , so this is more of an additional question than answer ( cannot add pictures to comments , so thats why its here ) . in the case of ferrous materials they generate an magnetic field inside material ( ok ? ) . opposite signs attract each other ( right ? ) . the position of the spring happens to be the local minimum of potential energy by symmetry principle ( or you can actually calculate this ) . all the other phenomena are just corrections to above phenomena ( ? ) . if all above are summed together , the spring is just oscillating around a local potential energy minimum , because of the magnetic field , not because of the spring properties . this is also why the coin oscillates the same way . anyway could you comment on this , i would like to know where i went wrong ( if anywhere ) .
imagine your rod is made up from lots of little bricks stacked on each other . then the total potential energy is the sum of the potential energy of each brick . the diagram shows the rod and one of the bricks of size $dx$ and at a height $x$ . if $\rho$ is the mass per unit length , then the mass of the brick is $\rho \space dx$ , and the potential energy is $\rho \space dx \space g \space x$ . to get the total potential energy we just have to sum up the potential energies of all the bricks . to do the sum we let $dx$ go to zero and replace the sum with an integral so : $$ \begin{align} u and = \rho \space g \int_0^h dx . x \\ and = \rho \space g \left [ \frac{x^2}{2}\right ] _0^h \\ and = \frac{\rho g h^2}{2} \end{align} $$ and since $\rho h$ is just the mass $m$ this gives us : $$ u = \frac{mgh}{2} $$ as you say , this is the same result you get by just considering the centre of mass , but note that we have got this result without involving the centre of mass at all . i think this is a better way to understand why the potential energy is $mgh/2$ without just invoking the centre of mass and waving your arms in the air .
maybe you are getting confused with the notation . notice that $$ \partial_\mu n^\mu \equiv \partial_0 n^0 +\nabla\cdot\vec{n} , $$ where $\nabla\cdot\vec{n} \equiv \partial_in^i$ . now , another way o writting this expression is : $$\partial_\mu n^\mu = \eta^{\mu\nu}\partial_\mu n_\nu=\partial_0 n_0 - \partial_i n_i . $$ vectors are tensor with only one contravariant indice , i.e. upstairs . vectors and covectors are related through the metric by $$v^\mu =\eta^{\mu\nu}v_\nu \leftrightarrow \{v^0 , v^i\}=\{v_0 , -v_i\} , $$where i assumed $\eta_{\mu\nu}= ( 1 , -1 , -1 , -1 ) $ .
yes , the velocity field inside the ellipse is really zero . to convince myself of it i have run a few numeric evaluations of the integral for various $ ( a , b ) $ and $ ( x , y ) $ . here is how can we obtain this result analytically . first , we introduce the elliptical coordinate system with coordinates $\chi$ , $\theta$: $$x = c \cosh \chi \cos \theta , \qquad y = c \sinh \chi \sin \theta , $$ so that the ellipse in question corresponds to a fixed value of $\chi$: $$a = c \cosh \chi_0 , \qquad b = c \sinh \chi_0 , $$ we also assume $a&gt ; b$ . second , we compute the vorticity field . we start with single vortex $$ \mathop{\mathrm{curl}} \left ( \frac{\gamma}{2\pi |\mathbf{x}-\mathbf{x}_0|} \mathbf{e}_\perp \right ) = \mathbf{e}_z \gamma \delta ( \mathbf{x}-\mathbf{x}_0 ) =\mathbf{e}_z \gamma \delta ( x-x_0 ) \cdot\delta ( y-y_0 ) , $$ and integrate the expression over the ellipse : \begin{multline} \mathop{\mathrm{curl}} \mathbf{u} = \mathbf{e}_z \int_0^{2\pi} \delta ( \mathbf{x}-\mathbf{x}_0 ( \theta' ) ) d \theta ' = \mathbf{e}_z \int_0^{2\pi} \delta ( x - a \cos \theta' ) \delta ( y-b \sin \theta' ) d\theta ' = \\= \mathbf{e}_z \int_0^{2\pi} c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \delta ( \chi-\chi_0 ) \delta ( \theta - \theta' ) d\theta ' = \\ = \mathbf{e}_z c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \delta ( \chi - \chi_0 ) , \end{multline} where we used the expression for area element in elliptical coordinates : $da = c^2 ( \cosh^2\chi-\cos ^2 \theta ) d\chi\ , d\theta$ . finally , we calculate the stream function $\psi$ with vorticity as a source : $$\delta \psi = - \omega \equiv - \mathbf{e}_z \cdot \mathop{\mathrm{curl}} \mathbf{u} . $$ in elliptical coordinates this reads as : $$ c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \left ( \frac{\partial^2 \psi}{\partial \chi^2}+\frac{\partial^2 \psi}{\partial \theta^2}\right ) = - c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \delta ( \chi - \chi_0 ) . $$ notice , that the factors on both sides are equal . so we could look for the solution depending only on $\chi$ , compatible with $\mathbf{u} ( \mathbf{0} ) =\mathbf{0}$: $$ \psi = - h ( \chi - \chi_0 ) \cdot ( \chi - \chi_0 ) , $$ wher $h ( x ) $ is heaviside function . this shows , that inside the ellipse velocity field is indeed zero . it also enables us to calculate the velocity field outside the ellipse , right away we see that the streamlines will be confocal ellipses .
no . consider any state with a momentum wavefunction symmetric about zero . it is position-space and momentum-space norm-squared probability distributions are not changed by time-reversal , even though the wavefunction clearly is . here is an explicit example . take the four gaussian wavepacket of mean positions $x_0$ or $-x_0$ , mean momenta $p_0$ or $-p_0$ , and spatial spread $\sigma$: $\psi_{ ( \pm_x , \pm_p ) } ( x ) \propto e^{- ( x \mp_x x_0 ) ) ^2/4\sigma^2 - i x ( \pm_p p_0 ) }$ . ( here , $\pm_x$ and $\pm_p$ are two binary variables taking the values of plus or minus . $\mp_x$ and $\mp_p$ are their opposites . ) now consider the two superpositions $\phi_{\mathrm{away}} = \psi_{ ( + , + ) } + \psi_{ ( - , - ) } $ , $\phi_{\mathrm{toward}} = \psi_{ ( + , - ) } + \psi_{ ( - , + ) } $ . $\phi_{\mathrm{away}}$ is the superposition of two wavepackets separated by $2 x_0$ traveling away from each other with relative speed $2 p_0/m$ . $\phi_{\mathrm{toward}}$ is the same with the packets traveling toward each other . one can check that $\vert \phi_{\mathrm{toward}} ( x ) \vert^2 = \vert \phi_{\mathrm{away}} ( x ) \vert^2$ and $\vert \tilde{\phi}_{\mathrm{toward}} ( p ) \vert^2 = \vert\tilde{\phi}_{\mathrm{away}} ( p ) \vert^2$ . i do not know if there are examples other than with time-reversal .
dear hde , the laser beam obviously has energy and momentum so the laser transmitter gets recoiled due to the conservation of energy and momentum . see also : http://en.wikipedia.org/wiki/poynting_vector the property we call mass is expressed by : $m ~=~ \sqrt{\frac{e^2}{c^4} - \frac{p^2}{c^2}}$ which is zero for photons even though the energy $e$ and momentum $p$ are not zero . the mass m is zero if energy relates to momentum as $e^2=p^2c^2$ . this shows you that photons can not be at rest because in that case both $p$ and $e$ are zero . regards , hans
in general , the answer is no . this type of inverse problem is sometimes referred to as : " can one hear the shape of a drum " . the following extensive exposition by beals and greiner discusses various problems of this type . despite the fact that one can get a lot of geometrical and topological information from the spectrum or even its asymptotic behavior , this information is not complete even for systems as simple as quantum mechanics along a finite interval .
in that case , by symmetry , if you take any loop of the coil , for every field line that goes through it , if it goes to the negative pole , there must be another symmetrical that comes from the positive , so the total flux will be $0$ , and constant , so there will be no current induced .
looks like what you are trying to simulate is a catenary . and this comes under statics - a part of classical mechanics . this paper ( so does the wikipedia article ) introduces to the basics of catenary analysis . also see a similar question on mathoverflow .
i believe you are thinking of the statcoulomb , which is the cgs/gaussian system unit of charge . the article linked above details some methods of conversion , which you can read for yourself , but probably the most important ones to know that : 1 ) $1\ statcoulomb = \frac{{ ( gram ) ^{1/2}} ( cm ) ^{3/2}}{sec}$ 2 ) $1\ coulomb \leftrightarrow \sqrt{4\pi \epsilon_0}\frac{c}{ ( 1m/s ) } statcoulomb$
the loss of magnetism at the curie temperature applies primarily to " induced magnetism like iron sticking to a magnet " . ferrofluid does not really include a molten iron ; ferrofluid is a collection of many small but mezoscopic particles , " sawdust " , and its magnetism does not differ so much from magnetism of normal pieces of iron except that it is easier for the particles to change the orientation . ferrofluid normally have lots of " normal fluid " in it , like water or organic liquids , and some " coating " , so the ferromagnetic material is a relatively minor component and it is surely not melted . when a ferrofluid gets heated to the curie temperature , it loses its magnetism , too . the curie temperature of all sensible materials is and has to be below the melting point . when you approach the melting point while heating the material , the magnetism has been lost for quite some time . yes , molten iron is paramagnetic , much like every ferromagnetic material above the curie temperature .
i admit that the terminology is not as self-explanatory as it should , however a source of confusion is the fact that you are actually looking at consequences of the definitions , instead of at the original definitions that hold in every connected time oriented spacetime . the terminology turns out to be more clear if you use the original definitions which are referring to the nature of the curves connecting the events . for a pair of events in a generic spacetime , time-like related means , by definition , that there is a future directed time-like curve joining the points . in minkowski spacetime , it is equivalent to say that there is a time-like geodesic joining the events and it implies ( it is equivalent in that spacetime ) that there is a minkowski reference frame where the events have the same location at different times . for a pair of events in a generic spacetime , causally related means , by definition , that there is a future directed causal curve joining the events . causal curve means that its tangent vector is not space-like . causal curves are those curves describing the stories of physical points transmitting interactions . finally , for a pair of events in a generic spacetime , space-like separated ( or also , equivalently , causally separated ) means , by definition that there is no future directed causal curve joining the events . in minkowski spacetime , it is equivalent to say ( and it justifies the name ) that there is a space-like geodesic joining the events and , in turns , it implies ( it is equivalent in minkowski spacetime ) that there is a minkowski reference frame where both events occur at the same time in the rest frame of the reference frame .
from the definition of the commutator , $ [ p_x , p_y ] = p_xp_y - p_yp_x$ where , $p_x = -i\hbar\frac{\partial }{\partial x}$ and , $p_y = -i\hbar\frac{\partial }{\partial y}$ therefore , $p_xp_y \psi = i\hbar\frac{\partial}{\partial x} ( ih\frac{\partial \psi}{\partial y} ) $ $ = -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y}$ similarly , $p_yp_x\psi = i\hbar\frac{\partial }{\partial y} ( ih\frac{\partial \psi}{\partial x} ) $ $ = -\hbar^2\frac{\partial^2 \psi}{\partial y \partial x}$ $ = -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y}$ therefore , $ [ p_x , p_y ] \psi = p_xp_y\psi - p_yp_x\psi = -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y} - -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y} = 0$ therefore , $ [ p_x , p_y ] = p_xp_y - p_yp_x = 0$ note the $\psi$ was not used here , but i included it anyway for teaching purposes , since occasionally evaluating commutators is easier when applying them to a test function , $\psi$ .
the total initial internal energy is $u=u_1 + u_2=\frac{\nu_1}{2}c_1rt_1 + \frac{\nu_2}{2}c_2 rt_2 $ where the last equality comes from joules ' first law for ideal gases and where $c_i$ is the number of moles of species $i$ and $\nu_i$ is the number of degrees of freedom of the molecule ( 3 for atoms , 5 for diatomic molecules etc . . ) . now , once equilibrium is reached everybody should have the same temperature $t$ . since you are dealing with an ideal gas it implies that : $u= \frac{ ( \nu_1c_1+\nu_2c_2 ) rt}{2}$ and hence since the whole system is isolated $t = \frac{\nu_1c_1t_1 + \nu_2c_2 t_2}{\nu_1c_1+\nu_2c_2}$ once the temperature is known , the rest follows easily . the pressure can be gotten straightforwardly as $p=\frac{ ( c_1+c_2 ) rt}{v_1+v_2}$ because the ideal gas law is independent of the number of degrees of freedom of the different species .
first , there are too much errors in the context of your question . the last term in the expression $1$ is certainly false , because $h_{\alpha\beta\alpha'\beta'}$ is antisymmetric in the transformation $\alpha \to \beta , \alpha ' \to \beta'$ , while the last term is symmetric for this same transformation . moreover , the terms $h_{ ( \alpha\beta ) }$ are certainly zero because this a multiplication of $\epsilon^{\alpha\beta}$ , antisymmetric ( in $\alpha , \beta$ ) , and $h_{ ( \alpha \beta ) \dot {\alpha }\dot {\beta }}$ , a symmetric quantity ( in $\alpha , \beta$ ) . secondly , it is certainly false that : $\varepsilon^{\dot {\alpha } \dot {\beta } } ( \sigma^{\mu} ) _{\alpha \dot {\alpha}} ( \sigma^{\nu} ) _{\beta \dot {\beta}} = \pm ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) _{\alpha \beta } , $ with your notations , $\sigma^\mu$ has indices $ ( \sigma^{\mu} ) _{\alpha \dot {\alpha}}$ , and $\tilde {\sigma }^{\nu }$ has indices $\tilde {\sigma }^{\nu }_{\beta \dot {\beta}}$ ( the standard notation $\tilde {\sigma }^{\nu }_{\dot \beta {\beta}}$ is preferable ) , anyway you cannot have a matrix $ ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) $ with indices $ ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) _{\alpha \beta }$ . even if you define a matrix $\tilde { ( \sigma }^{\nu } ) ^{\beta \dot {\beta}}$ or $\tilde { ( \sigma }^{\nu } ) ^{\dot \beta {\beta}}$ , this does not work , you are unable to find 2 lower indices $_{\alpha\beta}$
yes , the factor $2\pi$ in eqn 1 should be $4\pi$ – if the area integral is normalized conventionally . for example , for a sphere , the scalar curvature is $r=2/a^2$ . when multiplied over the $4\pi a^2$ surface , we get $8\pi$ which is $4\pi$ times the euler character of the sphere , $\chi=2$ . well , at least i hope that these considerations are not affected by using the metric $\hat h$ instead of $g$ .
they basically measure the intensity of the infrared blackbody radiation in some wavelength region and calculate the temperature needed to give that intensity according to planck 's law .
there is no " better " or worse here . it is just that " work " in physics is defined differently than in chemistry . in chemistry , all quantities follow this sign convention : they are positive if their effect is on the system . so , basically , $du$ is ( infinitesimal ) energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done on the system by the surroundings in physics , the sign convention of $w$ is the opposite $du$ is energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done by the system on the surroundings which means that $du_c = \delta q_c + \delta w_c$ ( $c$ means chemistry ) becomes $d u_p = \delta q_p - \delta w_p$ try to keep these conventions separate in your mind . do not use the physics flt for a chemistry problem and vice versa , many times problems specify values of $w , q , u$ and expect you to know the sign convention . note that these are the iupac/iupap conventions . some books ( as @dmckee mentions , feynman 's lectures is one of them ) use different conventions . in such cases , just make note of the convention and remember that the flt is just a statement of conservation of energy . here 's the menemonic i used to remember it . it is not a great one , but it works : chemists are interested in supplying hear/energy/pressure to a reaction to make it occur . thus , action done by the surroundings on the system is " good " or " positive . physicists are more interested in supplying heat/energy to a system and making it do work . so , supplying heat/energy is " good " , and getting work out is " good " .
your charge distribution is correct , though i usually do not focus on using a " correct " $\pm$ distribution in capacitors--if your sign is incorrect , you get a negative value of charge on the + plate . no biggie . in complex situations , it is sometimes even impossible to predict cjarge distribution without solving the circuit . and your book is incorrect . $c_1-c_2$ are in series , though the entire branch is in parallel with $c_3$ and its opposite branch . parallel is when the current is split , while series is when current is constant . series is a single wire with an in and an out , with components along the wire . parallel is when there are many wires with their ends twisted together . current goes in/out through the twisted ends . you seem to have grasped that , though : ) out of curiosity , which book is this ? ( the capacitors look like they are from resnick )
energy is conserved so it can not be created or destroyed . all we can do is change energy from one form to another . in your example we are changing the potential energy of the mass $m$ into kinetic energy . the increase in kinetic energy must be equal to the decrease otherwise energy would not have been conserved . by an external force i assume you mean some third party outside the system . to give a slightly ridiculous example this could be me standing well away from the earth and the mass and poking the mass with a long pole to accelerate it . in this case the energy of the earth + mass would not be conserved , but also my energy would not be conserved . however the energy of the earth , the mass and me would be conserved . the distinction between internal and external forces is a bit artificial because all systems are closed and all forces are internal if you look on a big enough scale .
general remarks . in general , you cannot " derive " a representation of a given group $g$ on the objects you are considering , but there are some really standard definitions of certain group representations which are given special names like " scalar , " " vector , " and so on . however , given the representation of a lie group $g$ , this induces a representation of its lie algebra $\mathfrak g$ , and determining an explicit formula for this lie algebra representation is precisely what we do when we find the so-called " infinitesimal generators " of the corresponding group representation . an example . $\mathrm{so} ( 2 ) $ let $c^\infty ( \mathbb r^2 ) $ denote the vector space of smooth functions on the plane $\mathbb r^2$ . the scalar representation $\rho$ of $\mathrm{so} ( 2 ) $ acting on $c^\infty ( \mathbb r^2 ) $ is defined as \begin{align} ( \rho_0 ( r ) \phi ) ( \mathbf x ) = \phi ( r^{-1}\mathbf x ) . \end{align} for each $\phi\in c^\infty ( \mathbb r^2 ) $ and for each $r\in\mathrm{so} ( 2 ) $ . what the heck is going on here ? well , notice that this can also be written as follows : \begin{align} ( \rho_0 ( r ) \phi ) ( r\mathbf x ) = \phi ( \mathbf x ) \end{align} so this definition encapsulates the intuitive idea that the transformed field $\rho ( r ) \phi$ evaluated at the transformed point $r\mathbf x$ agrees with the untransformed field $\phi$ evaluated at the untransformed point $\mathbf x$ . in physics , it is common to see " primed " notations for the transformed field and transformed point ; \begin{align} \rho_0 ( r ) \phi = \phi ' , \qquad r\mathbf x = \mathbf x ' \end{align} in which case the definition of the scalar representation can be written as \begin{align} \phi' ( \mathbf x' ) = \phi ( \mathbf x ) \end{align} this probably looks familiar . so basically the " invariance " that is happening is that the value of the field does not change provided the transformed field is evaluated at the transformed point . infinitesimal generators . to find the infinitesimal generators of a given representation , we are really just trying to find a certain representation of the lie algebra of the group . this lie group representation $\rho$ naturally induces a lie algebra representation $\bar \rho$ as follows : \begin{align} \bar \rho ( x ) = \frac{d}{dt}\rho ( e^{tx} ) \big|_{t=0} \end{align} so , for the $\mathrm{so} ( 2 ) $ example , we know that the lie algebra $\mathfrak{so} ( 2 ) $ is generated by the single element \begin{align} j = \begin{pmatrix} 0 and -1 \\ 1 and 0 \\ \end{pmatrix} , \end{align} and we can determine how this element is represented in representation induced by the scalar representation defined above as follows : \begin{align} ( \bar\rho_0 ( j ) \phi ) ( \mathbf x ) and = \frac{d}{dt}\phi ( e^{-tj}\mathbf x ) \big|_{t=0} \\ and = \frac{d}{dt}\phi ( x-ty , y+tx ) \big|_{t=0} \\ and = -y\partial_x\phi ( x , y ) + x\partial_y\phi ( x , y ) \\ and = ( -y\partial_x + x\partial_y ) \phi ( \mathbf x ) \end{align} in other words , in the scalar representation , the generator of rotations on the plane is represented by a differential operator ; \begin{align} \bar\phi_0 ( j ) = -y\partial_x + x\partial_y . \end{align} this same procedure can be extended to find infinitesimal generators of other representations as well , like the vector representation $\rho_1$ of $\mathrm{so} ( 2 ) $ which is defined to act on vector fields $\mathbf v$ on the plane as follows : \begin{align} ( \rho_1 ( r ) \mathbf v ) ( \mathbf x ) = r\mathbf v ( r^{-1}\mathbf x ) \end{align} by the way , you might find the following links interesting and/or helpful as well : tensor operators representations of lie algebras in physics differential realizations of certain algebras generators of poincare groups idea of covering group unitary spacetime translation operator rigorous underpinnings of infinitesimals in physics
your understanding that a change in rotational velocity requires a net torque ( calculated around any convenient axis ) is correct . the only forces acting are gravity , the normal force from the ramp , and the force of friction between the ramp and the block . choosing an axis across the ramp through the center of mass of the block makes life simpler ; the force of gravity ( all of it ) acts through this point , and thus exerts no torque . next , consider the friction force , that is given by $mg \sin \theta$ . this acts up the ramp , at the point of contact of the ramp and block , and thus exerts a torque around the center of mass depending on the dimensions of the block . ( tall blocks are more tippy than squat blocks ) in this case , the torque is a clockwise one . in order for there to be no rotation , there must be a counter-clockwise torque about the cofm . this means that the diagram is wrong ; the normal force no longer is distributed uniformly along the base , and cannot be ignored as acting through the cofm . in fact , the normal force shifts towards the front of the block as the ramp slope increases , and when it reaches the front corner , the rear corner lifts and the block tumbles . . .
in special relativity , you have to choose as frame of reference which is an inertial frame . in this inertial frame , you may consider the movement of any object , whatever this movement is ( accelerated or not ) . let the coordinates of the moving object , relatively to an inertial frame $f$ , be $x$ and $t$ . we can consider an other initial frame $f'$ , which coordinates of the moving object , relatively to $f'$ , are $x'$ and $t'$ the heart of special relativity is that exists an invariant which is $c^2 dt^2 - \vec dx^2 = ds^2$ . this means that : $c^2 dt^2 - \vec {dx}^2 =ds^2= ds'^2 = c^2 dt'^2 - \vec {dx'}^2$ . all inertial frames , when looking at the moving object , agree on the same value $ds^2$ now , at some instant $t_0$ , you may always consider a inertial frame $f' ( t_0 ) $ which has , at this instant , the same speed as the moving object , relatively to $f$ . of course , you will have a different inertial frame $f' ( t ) $ for each instant . however , the key point is , that the instantaneous speed of the moving object relatively to $f' ( t ) $ is zero , that is , you have $dx ' =0$ , so you may write : $ds^2 = c^2 dt^2 - \vec {dx}^2 = ds' ( t ) ^2=c^2dt'^2$ the time $t'$ defined in this manner is called the proper time of the moving object , and is noted $\tau$ ( $c^2 dt^2 - \vec {dx}^2 = c^2d\tau^2$ ) . it represents the time elapsed for a clock moving with the moving object . with your problem , note that if you take the parametrization : $$\left\{ \begin{array}{l l} ct= b ~sh ( \frac{c \tau}{b} ) \tag{1}\\ x= b ~ch ( \frac{c \tau}{b} ) \end{array}\right . $$ you will find , with a little algebra , that , first , $x ( t ) = \sqrt{ ( b^2 ) + ( ( ct ) ^2 ) }$ , and secondly , that $c^2 dt^2 - \vec {dx}^2 = c^2d\tau^2$ ( we suppose here $dy=dz=0$ ) . so , $\tau$ is the proper time , that you are looking for , and you may find a expression of $\tau$ relatively to $t$ , by inversing the first equation of the parametrization $ ( 1 ) $ : $$ \tau = \frac{b}{c}~argsh ( \frac{c t}{b} ) \tag{2}$$
i do not have a figure for the total area subtended by the objects in the oort cloud , and i think it would be very difficult to get even within an order of magnitude estimate for this , since we do not know a lot about the oort cloud . however there is a simple answer to your question . all we have to do is ask how often we see stars occluded by objects in the oort cloud , and the answer is never . that tells us that the amount of light blocked by the oort cloud is insignificant .
i am not quite sure what is your doubt so i will try to describe this as precise as possible what i think the doubt you might have . q . momentum == trajectory . a . yes . . . kind-of ! momentum does describe the trajectory of the free-particle if you knew the starting position in transverse plane ( say $ ( x_o , y_o ) $ ) . as a matter of fact ( in technical words ) momentum is generator of position . if you are okay with this then no need to read forward you could just let this go , but if you are new to quantum then i would suggest that you read further as it might give a clear pathway to understanding of the problem itself which get blurred by weird orthodox interpretation still taught in classrooms . in simple words when you try to find the position of photon 's position in xy-plane ( assuming that light is traveling along z-axis ) , then it is momentum becomes uncertain " in xy-direction only " ( well from energy conservation you might argue that z-momentum as well becomes uncertain , well yes but the effect is more pronounced in x-/y-direction as prior to slit the momentum was exactly zero ) . the momentum ( hence the trajectory in space ) was perfectly known in before photon encounters slit . later the photon get dispersed after it has passed through slit purely due to the quantum effects . following is just mathematical details of " what happens " from " before " to " after " . lets say you start with plane wave , $|\psi_{before}&gt ; = |\vec{p}&gt ; $ . now when photon are passing through the slit measurement is being carried by the atoms making the slit and they are making observation in position basis so ( orthodox interpretation suggest ) we better express the state-ket in terms of position basis $|x&gt ; $ . since the slit is only making the measurement on x and y position of photons we expand the state in $|x&gt ; $ and $|y&gt ; $ . $$ |\psi_{before}&gt ; = \frac{1}{2\pi \hbar^2} \int dp_x dp_y e^{i ( p_x x + p_y y ) /\hbar} |x , y&gt ; $$ it basically means that we have a superposition state i.e. prior to measurement by slit the transverse position-wise incoming photon is dispersed though-out the measuring plane . say the state of photon ( after it has passed through slit ) is $|\psi_{after}&gt ; = |x_\circ , y_\circ&gt ; $ . now we have to evolve it to screen in unitary fashion . with known hamiltonian , $ h = \frac{p^2}{2m} $ it can be seen easily that $ &lt ; x , y|u|x_\circ , y_\circ&gt ; \neq 0 \rightarrow $ the beam spreads . also trajectory is just a colloquial term it sometime helps connecting the quantum mechanics to classical notions , but you might have realised that it is quite confusing to use . that is why you should proceed with more concrete way of expressing the quantum-ideas .
it is widely believed ( but it has not been rigurously proved so far ) that to sustain stable closed timelike curves , you need copious ( i.e. of the order of the total mass of the universe ) quantities of exotic matter . exotic matter is just a generic term to describe matter for which the stress-energy tensor satisfies $$ g^{\mu \nu} t_{\mu \nu} &lt ; 0 $$ that is , exotic matter violates the strong energy condition , which is known to hold in all known quantum physical theories . this applies not only to ctc , but as well as stable wormholes , warp drives , or anything actually fun . this is in itself , outstanding evidence of the catholic nature of god , since interestellar travel would raise the need for some akward explaining from the pope ( i know , bs , but let me have my punchline please )
as you have discovered proper time , $\delta\tau$ , can be either real or imaginary . however , this means that it does not necessarily reflect something measurable with a clock . when it is imaginary , as in the case of a space-like relation of two events , then there is no single clock that can be present at both events . to do so would require having a velocity $v&gt ; c$ . this is , i suppose , a fancy way of saying that the two events are not causal ( event 1 does not cause event 2 and vice versa ) .
is it possible to focus the sun in such way ? yes , as others have pointed out , all of the ideas in your sketch are already used in existing designs - perhaps excepting the shutter ( which actually performs no useful purpose so far as i can see ) . as chris white commented - " this exact design ( with the shutter permanently open ) is a schmidt-cassegrain telescope , probably the most popular high-end consumer scope these days . " is it possible to increase the power of the beam by making it bounce between the mirrors no , focusing a beam of light , or reflecting it , does not increase the power . the amount of power is the amount of light energy entering the system per second . that is limited by the diameter of the entry pupil . energy is conserved .
as djbunk mentions the delta function is symmetric $$\delta ( x ) =\delta ( -x ) $$ so you certainly have $$\delta ( x-x' ) =\delta ( x'-x ) . $$ but you should also know that in general we have $$\langle a | b \rangle = \langle b | a \rangle^* $$ and since in this case the inner product is real , you will also have $$\langle x | x ' \rangle = \langle x ' | x \rangle . $$ so it does not matter which way you write the delta function or the inner product .
as you have in the commutation relations , $\sigma_i \sigma_j= \sigma_j\sigma_i$ e.g. spin operators on different sites commute , so there is no minus sign to pick up .
a simple model that explains the frequency dependency of the resistivity of metals reasonably well is the drude model ( http://en.wikipedia.org/wiki/drude_model ) . there we have frequency dependency because the electrons in a plasma are not moving arbitrarily fast , which is consistent with xurtio 's explanation . the cutoff frequencies are usually in the optical domain . for dielectrics similar models exist , which are often a sum of lorentzian resonances . these have their origin in resonant absorption which is a quantum physical effect . the imaginary part of the permittivity is related to the conductivity . this can be seen as follows : amperes law is $\nabla \times \mathbf{h} = \mathbf{j} +i \omega \epsilon_r \epsilon_0 \mathbf e$ and insert ohms law in differential form $\mathbf{j} = \sigma \mathbf{e}$ then you get $\nabla \times \mathbf{h} = i \omega ( \epsilon_r \epsilon_0 -i \sigma/\omega ) \mathbf e$ which is just of the same form of as original form of amperes law but without the explicit $\mathbf{j}$ term . in conclusion ohms law can be integrated in free space maxwells equations ( without the source terms ) when the relative permittivity $\epsilon_r$ is taken as a complex value ( $\widetilde\epsilon_r = \epsilon_r - i \sigma/ ( \omega \epsilon_0 ) $ ) , where an imaginary part is added related to the conductivity . this essentially models the effect of moving charges under the influence of an oscillating field ( light ) . so the relation between polarization ( $\mathbf d = \widetilde{\epsilon}_r \epsilon_0 \mathbf e = \mathbf p + \epsilon_0 \mathbf e$ ) and conductivity $\sigma$ is given as $\mathbf{p} = \epsilon_0 ( \epsilon_r - i \sigma/\omega - 1 ) \mathbf e$ . since the real part of the permittivity is frequency dependent , so is the conductivity . this is because of the kramers-kronig relations which follow from a causality relation .
you are on the right track in calculating the uncertainty in momentum using the uncertainty principle . the new position will be $$x_2 = x_1 + \frac{p}{m} t$$ there is a well known technique of error propagation which works like $\delta ( f ( x_1 , x_2 , x_i , . . . ) = \sqrt{\sigma\left ( \frac{\partial f }{\partial x_i} \delta x_i\right ) ^2 } $ , where $\delta x_i$ means the uncertainty in $x_i$ , which is an independent coordinate ( including momenta and times ) of the motion . you sum over every measurement that has an uncertainty . this comes from the taylor series . applying this , you will get $$\delta x_2 = \sqrt{\delta x_1^2 + \left ( \frac{t}{m} \delta p\right ) ^2}$$ edit - i thought about this a little more and i think that the addition in quadrature is not so appropriate here . usually you use this for measurement uncertainties , where you look for one-sigma intervals , but for quantum mechanics , where you look for complete uncertainty , it might be more correct to add the components directly . $$\delta x_2 = \delta x_1 + \left ( \frac{t}{m} \delta p\right ) $$
i finally found some prior art . this object has been introduced as the " husimi matrix " by harriman " some properties of the husimi function " harriman , john e . , the journal of chemical physics , 88 , 6399-6408 ( 1988 ) , http://dx.doi.org/10.1063/1.454477 and briefly referred to by morrison and parr " approximate density matrices and husimi functions using the maximum entropy formulation with constraints " morrison , robert c . and parr , robert g . , international journal of quantum chemistry , 39: 823–837 http://dx.doi.org/10.1002/qua.560390607 the treatment was fairly basic . from what i can tell , harriman primarily introduced the husimi matrix to highlight an analogy with density matrices ( since you can use it as the kernel of an integral operator ) . morrison and parr use it for something related to calculating a density matrix as a maximum entropy husimi function , but i do not really understand . i do not believe anyone has explored the relationship to decoherence .
i will try to provide some insight . stationarity of the energy functional ( in your case , beams , elastic energy plus work potential i suppose ) is equivalent to equilibrium : in your case this can be verified quickly by observing how stationarity ( nullity of the first variation ) leads to the euler beam equatuion ( i am assuming you are not dealing with more elaborated beam models , but the essence would still be unaffected ) . stability ( against infinitesimal perturbations , global stability is left aside at this stage ) is a more stringent requirement : on top of equilibrium you want all your nearby accessible configurations to have a higher energy than the equilibrium ( the physical picture of a ball in a well or on a saddle is as valid as ever . . . at least for me ) . that is the intuition behind requiring that the quadratic term in the taylor expansion be positive ( hinting at some " convexity " property related to stability ) : if it were not so an infinitesimal perturbation at the second order would find nearby states of lower energy ( as the ball on top of a half-sphere ) . this verbose prologue of mine might well be obvious , for which i apologise . now to your direct question , how to go from ( 1 ) to ( 2 ) . ( 1 ) contains a derivative of a function ( with respect to $\epsilon$ ) . expand $\hat{p} ( \epsilon ; f ; g ) =p ( f+\epsilon$f ; g ) as your professor suggested : $$\hat{p} ( \epsilon ; f ; g ) =\hat{p}_0 +\epsilon\hat{p}_1 ( 0 ; f ; g ) +\epsilon^2\hat{p}_2 ( 0 ; f ; g ) +⋯$$ plug this expression into ( 1 ) , perform derivatives term by term and you ( almost ) get your result . one would have to define what is meant by taylor expansion of a functional to be fully rigorous , but at the intuitive level i hope the above is of some help ( one can always " imagine " a functional as a function of infinite variables . . . )
note : chocopouce 's answer is the same as mine but is more mathematical . you have a ( spherically symmetric ) probability density distribution $\rho$ in space ( which we get from the square of the amplitude ) . the " radial probability density " is roughly the chance that the electron is at a given radius , say $r = 0.1\mathrm{nm}$ ? in other words , how much of this distribution is in at the $0.1\mathrm{nm}$ shell ? we will not exactly $0.1\mathrm{nm}$ , so we take a thin shell : how much is between $0.1$ and $0.1+ε\mathrm{nm}$ ? amount $=$ ( average prob . density in shell ) * ( volume of shell ) . since the shell is so thin ( $\varepsilon$ is very small ) , the density will be almost constant and the shell 's volume is given by $\mathrm{area}\times\mathrm{thickness} = 4\pi* ( 0.1nm^2 ) *\varepsilon nm$ . thus the probability is $4\pi\rho ( r ) r^2\varepsilon$ , where $\rho$ only depends on $r$ since it is spherically symmetric . but $\varepsilon$ is an arbitrary " small " thickness we defined , and it is best to divide by $\varepsilon$ to get the probability per unit radius , which is called the ( radial ) probability density $4\pi\rho ( r ) r^2$ . the most likely radius ( which is different from the average radius ) maximizes this function .
well , it is not . to our best present knowledge the cosmological constant is positive , which means the universe undergoes accelerated expansion and never crunches . hawking 's book is a little out of date .
the clock placed on the rotating ring would tick slower by a factor of $\sqrt{1-\omega^2 r^2/c^2}$ , where $\omega$ is the angular velocity of the ring , $r$ is the radius of the ring and $c$ is the speed of light .
since the cable is not moving horizontally you know the horizontal component of tension is the same at both ends . the total tension is the horizontal component divided by the cosine of the angle . so the ratio is the tensions is the ratio of the cosines . since you know the shape of the curve you should be able to take it from here . update the general equation for a catenary ( with lowest point at x=0 ) is $$y = a \cosh \frac{x}{a}$$ where $$a = \frac{h}{w}\\ h = \text{horizontal tension}\\ w = \text{weight per unit length}$$ for a given horizontal distance and vertical displacement , we have to figure out the location of the lowest point and the tension - two equations , two unknowns . from wikipedia . org/wiki/catenary#determining_parameters : given $s$ , $v$ , and $h$ , then $a$ can be solved for numerically : $$\sqrt{s^2 - v^2} = 2a \sinh \frac{h}{2a} , a &gt ; 0$$ where $h$ is the horizontal distance between ends , $v$ is the vertical distance between ends , $s$ is the length of the cable , and $a$ is the y coordinate of the lowest point . next , we just need to find the position of the lowest point relative to the ends . to get the actual locations of $x_1$ and $x_2$ ( the horizontal distances from the lowest point to the the left and right ends , respectively ) you now have to solve $$\begin{align}\\ v and = a ( \cosh \frac{x_2}{a} - \cosh \frac{x_1}{a} ) \\ h and = x_2 + x_1 \tag1 \\ v and = a\left ( \cosh \frac{x_2}{a} - \cosh \frac{x_2-h}{a}\right ) \tag2 \end{align}$$ solve ( 2 ) for $x_2$ then substitute into ( 1 ) to get $x_1$ finally , the ratio of tensions comes from the ratio of cosines of the angle at the point of suspension : $$\frac{t_2}{t_1} = \frac{\cos\theta_1}{\cos\theta_2}$$ we know the tangent at $x$ is given by $$tan\ , \theta = \frac{dy}{dx} = \sinh \frac{x}{a}$$ combine with the trig identity $$cos\ , \theta = \frac{1}{\sqrt{1+\tan^2\theta}}$$ you finally obtain $$\frac{t_1}{t_2} = \sqrt{\frac{1+\sinh^2\frac{x_2}{a}}{1+\sinh^2\frac{x_1}{a}}}$$
this is the way in which all physical theories get formulated--- you first acquire certainty regarding the behavior of many special cases you have some experimental data or theoretical insight about , then you try to formulate a precise theory which extends these heuristic laws to a precise understanding , and when you succeed in matching the heuristic laws ( when they apply ) and you can predict everything consistently and correctly , you are done . in fundamental physics , one has a pretty solid understanding of phenomena which are not quantum gravitational , because we have a precise fundamental theory of relativistic quantum fields . so the most significant things that are not fully understood at the precise level are the class of insights deriving from hawking radiation and black hole classical behavior . these are semi heuristic , because there are puzzles that are not yet fully resolved within string theory . this class includes : the near horizon behavior of semiclassical black holes : an observer falling into a black hole sees nothing special when crossing the horizon , and this has not been rigorously demonstrated in string theory , it is only rigorously true classically . now some people claim that it can not work in quantum gravity , that black holes are " firewalls " . i read the arguments , and i find them uncompelling , because they mix assumptions about the semiclassical behavior at late times with full quantum observations on the hawking radiation which are restricted when you measure the semiclassical state at late times , so the argument smells fishy and does not stop smelling fishy even with later clarifications , although understanding what is going wrong is important , and this is at the heuristic level , because we can not reconstruct black hole interiors completely from quantum gravity scattering data . the related holographic principle and black hole complementarity : this is also heuristic for semiclassical thermal black holes , although it is precise in ads/cft , for certain extremal black holes . cosmological horizon entropy : the entropy of the cosmological horizon is not even in-principle understood in string theory , and it is a major clue to understanding how to do quantum gravity in desitter space , because it is understood at the heuristic level--- it is the same as black hole entropy . rindler horizons : the behavior of strings on a rindler background is nontrivial , even though this is just minkowski space . quantum fields are completely understood on rindler , but string theory on rindler is harder , because you do not have an s-matrix ( everything falls into the rindler horizon ) . for string theory , we only have heuristic guidelines regarding compactification and susy breaking . these heuristics are summarized in guidelines about what topologies and matter configurations give rise to which gauge fields and representations , and where one should expect to find the standard model . these are relatively well understood , since there are many explored vacua , but there are always surprises . in cases where we know the fundamental laws without serious doubt , there are still cases where we understand things only at the heuristic level . the following are in qcd : confinement and regge theory : we know the qcd confines to regge trajectories for mesons and topological soliton baryons , but this formulation is not mathematically linked to qcd by any rigorous path . pomerons : this is related--- we know that high energy scattering is dominated by pomeron exchange , and this is heuristic only , because we can not relate this to qcd high energy diffractive scattering except in certain regimes which are not completely diffractive . chiral perturbation theory : again related , this is the low-energy approximation to qcd , and the parameters are from assumptions on low energy condensates . i suppose i should not include this , because you can extract chiral data from lattice data in principle exactly , but i had something else in mind for what constitutes a full understanding--- it would mean that any chiral configuration can be mapped to a qcd configuration , and you could do the path integral for qcd in two steps--- as a path integral over long wavelength chiral configurations plus an additional path integeral over qcd in the given chiral background . nobody did this , although it should not be hard . instanton fluid : there is a class of semiheuristic models that give the qcd vacuum as a dense instanton fluid , and this is not 100% understood . it is essentially the same problem as before--- condensates and confinement . quark condensates : there are condensate models where we can semi-heuristically calculate the effects on hadrons using the svz sum-rules ( qcd sum rules ) , but the condensate values again are not derived from qcd , although they can be measured from experiment . in principle , lattice gives them , but this is not enough--- you want to know the values and effects with more insight . there are no doubts qcd resolves these questions , but the exact best way to make these heuristic things precise is unclear . there are condensed matter systems where the understanding is heuristic in the same way--- the most famous and the one i like best is probably : hightc : the condensate state of the hightc superconductor can be described phenomenologically using a d-wave superconducting condensate . getting this d-wave from the fundamental interactions has not been done in a universally convincing way , although there is no doubt that the fundamental theory will do it . there are many other such things , this is usually all the open problems people work on . historically , if you look at any phenomenon that was understood , it was understood heuristically before it was understood precisely , and learning the heuristic stuff is an important prerequisite for getting certainty about the final explanation , because physical theories are evolved by common sense , they do not emerge fully formed from nothing .
recall that $\left|\psi\left ( x\right ) \right\rangle =\int\left|\varphi_{p}\left ( x\right ) \right\rangle \left\langle \varphi_{p}\left ( x\right ) |\psi\left ( x\right ) \right\rangle dp$ ( for continuous p ) where $\left\langle \varphi_{p}\left ( x\right ) |\psi\left ( x\right ) \right\rangle=\text{φ}\left ( p\right ) $ , which is the amplitude of momentum measurement $p$ then $\text{φ}\left ( p\right ) =\left\langle \varphi_{p}\left ( x\right ) |\psi\left ( x\right ) \right\rangle =\intop_{-\infty}^{\infty}\bar{\varphi_{p}}\left ( x\right ) \psi\left ( x\right ) dx$ =$\frac{1}{\sqrt{2\pi a\bar{h}}}\int_{-\infty}^{\infty}e^{\frac{-\left|x\right|}{a}+ix\left ( k-\frac{p}{\bar{h}}\right ) }dx$ by solving the integral , you get $\frac{1}{\sqrt{2\pi a\bar{h}}}\left ( -\frac{1}{\frac{-1}{a}+i\left ( k-\frac{p}{\bar{h}}\right ) }+\frac{1}{\frac{1}{a}+i\left ( k-\frac{p}{\bar{h}}\right ) }\right ) $ $=\frac{1}{\sqrt{2\pi a\bar{h}}}\left ( \frac{2a}{1+a^{2}\left ( k-\frac{p}{\bar{h}}\right ) ^{2}}\right ) $ which can then be simplified to obtain the answer provided
adding the word " modern " to the title of the question completely changes it . in modern computers you need semiconductors , and the whole theory of solid state physics ( band structures , doping , etc ) is based on a foundation of quantum mechanics - since electrons in semiconducting solids behave in a manner that is more wave-like than particle-like , with each electron occupying its own distinct state . making a semiconductor work well requires in depth understanding of these things .
it does keep accelerating . it is velocity in the direction of the object being orbited keeps increasing . but this direction keeps changing . the reason the satellite 's total speed does not increase , at least in the case of a circular orbit , is that while the it is velocity towards the object increases , it is tangential motion moves it forward so that that direction is always perpendicular to the direction of motion . so while the satellite is undergoing constant acceleration , its always perpendicular to the direction of motion , and thus the speed of the object never changes .
what exactly is a boson ? a boson is a particle whose spin ( = intrinsic angular momentum ) is an integer number . for example , the photon ( the particle that is responsible for the electromagnetic force ) is a boson . contrast this with a fermion , such as the electron , whose spin is a half integer . in everyday terms , the bosons are the microscopic particles that make up the forces : electromagnetism and gravity , as well as the weak and strong forces . fermions are the particles that make up matter : protons , neutrons , and electrons . the fact that forces come from bosons while matter comes from fermions is a very deep observation , and is related ( at least in part ) to something called the ' spin-statistics ' theorem , but let me not go into that . now let me note that all the bosons that are responsible for producing forces have spin 1 or 2 . the higgs boson is another kind of boson , which does not behave like a force . technically , this is because its spin is 0 . in fact , it is the only fundamental particle with spin 0 that we know exists : all the others that are known have spin 1/2 ( fermions ) , 1 , or 2 . is the higgs boson the cause of gravity or a result of it ? does the collision of particles at the lhc create a gravity field or waves or somehow interact with the gravity field of the earth ? the higgs boson has nothing to do with gravity . it does not cause gravity and it is not the result of gravity . the particle that ' causes ' gravity is called the graviton , it is a boson like all the other force-particles , and it has spin 2 . the higgs boson does give mass some to the other fundamental particles , and this mass then interacts with gravity just like all other mass . but this is an indirect relation . the particles at the lhc interact with the gravitational field of the earth just like all other particles . as part of the collisions , certainly some gravitons ( = ' gravity particles' ) are also created . but these are completely negligible effects that can be ignored , because gravity is a much weaker force than all the others . the higgs boson is supposed to be quite massive and equivalent to a large number of protons . were many particles needed to create it or only a few travelling at high speeds ? was the high energy converted into the large mass ? i am not sure what you mean by ' equivalent to a large number of protons ' . it is true that if you weigh one higgs particle it will weigh as much as about 120 protons , but they are not related beyond this simple fact . the higgs particle , like all other particles produced at the lhc , is created by a collision between two highly energetic ( = very fast ) protons . indeed , in the collisions in which a higgs was created , the energy of the protons was converted into the mass of the higgs particle . why is the particle so short lived and what does it decay into ? the higgs is short-lived essentially because it is very massive . roughly speaking , the more massive a particle is , the faster it decays into other particles . this is not a precise statement because it really depends on what the particle can decay to . for example , the proton is massive relative to the electron , but it seems not to decay to anything due to various reasons . the higgs particle has different possibilities to decay to ( these possibilities are called ' decay channels' ) . for example , it can decay to two photons , or to two quarks like bottom and anti-bottom .
because it is structure displays translational symmetry in 2d . atoms themeselves are 3d as in other materials , but their are placed on a 2d flat plane . compare to 1d fullerenes .
this is exactly the approach taken in bernard shutz 's note " gravitational waves on the back of an envelope " ( am . j . phys . 50 vol 5 pp 412 ) . the abstract reads : using only newtonian gravity and a little special relativity we calculate most of the important effects of gravitational radiation , with results very close to the predictions of full general relativity theory . used with care , this approach gives helpful back‐of‐the‐envelope derivations of important equations and estimates , and it can help to teach gravitational wave phenomena to undergraduates and others not expert in general relativity . we use it to derive the following : the quadrupole approximation for the amplitude h of gravitational waves ; a simple upper bound on h in terms of the newtonian gravitational field of the source ; the energy flux in the waves , the luminosity of the source ( called the ‘‘quadrupole formula’’ ) , and the radiation reaction in the source ; order‐of‐magnitude estimates for radiation from supernovae and binary star systems ; and the rate of change of the orbital period of the binary pulsar system . where our simple results differ from those of general relativity we quote the relativistic ones as well . we finish with a derivation of the principles of detecting gravitational waves , and we discuss the principal types of detectors under construction and the major limitations on their sensitivity . ( if you do not have access to am j phys , this talk seems to recapitulate the details . ) a major difference in this newtonian scalar theory from the real gr theory of gravitational waves is in the effect of the waves on an inertial test particle . this newtonian theory predicts that the waves would appear as an oscillating force along the direction between the source and the test particle . by contrast , gr predicts an oscillating differential tidal effect in the plane perpendicular to the line connecting the source and the test mass . as a result , while i think ligo would still detect the " newtonian " form of gravitational waves , the antenna pattern of the detector would be different . the l-shaped ligo detector has optimal sensitivity to a source located directly overhead in the gr case ( allowing the gravitational wave to stretch one arm while it is compressing the other ) . there would be no sensitivity to a " newtonian " source directly overhead . however , you could detect it if the " newtonian " source were aligned with either arm . by the way , " newtonian noise " ( the near-field action of newtonian gravity arising from density waves in the material near the detector ) is a real concern for terrestrial gravitational wave detectors ! p.s. to be pedantic , it is best to avoid the term " gravity wave " ( as opposed to " gravitational wave" ) , since a " gravity wave " ( "newtonian gravity wave " even ! ) is something completely different .
the answers given so far are fine , but to my surprise nobody 's mentioned the most important point : in modern terminology , we generally do not say that the mass of an object increases with speed . " relativistic mass increase " is outdated terminology , not used by most physicists anymore . in general , nowadays , " mass " means " rest mass " and is independent of velocity . igor ivanov 's answer to this question says it all . i have not read the article by lev okun that he refers to , but i like the term " pedagogical virus " for this notion .
however , i had difficulty understanding that answer and would like to understand how to do it this way . that is to say , i would really like to know what property or identity that i am missing before i can use use the bianchi identities to show that it is manifestly zero . the other proof uses the first bianchi identity . that is where the starting assumption $r^a{}_{bcd}\xi^d = \xi^a{}_{ ; bc}$ comes from . if you want to use the second bianchi identity , it is $$ ( \nabla_\xi r ) ( x , y ) + ( \nabla_x r ) ( y , \xi ) + ( \nabla_y r ) ( \xi , x ) = 0\text{ , }$$ and therefore applying it and the leibniz rule produces : $$\begin{align} \underbrace{\nabla_\xi [ r ( x , y ) ] +\nabla_x [ r ( y , \xi ) ] +\nabla_y [ r ( \xi , x ) ] }_\mathrm{foo} = \underbrace{r ( \mathcal{l}_\xi x , y ) + r ( \mathcal{l}_xy , \xi ) + r ( \mathcal{l}_y\xi , x ) }_\mathrm{bar}\text{ , } \end{align}$$ where it was assumed that the torsion vanishes , so that $\mathcal{l}_ab = \nabla_ab-\nabla_ba$ . additionally , $$\begin{eqnarray*} ( \mathcal{l}_\xi r ) ( x , y ) and = and \mathcal{l}_\xi [ r ( x , y ) ] - r ( \mathcal{l}_\xi x , y ) - r ( x , \mathcal{l}_\xi y ) \\ and = and \mathcal{l}_\xi [ r ( x , y ) ] - r ( \mathcal{l}_\xi x , y ) - r ( \mathcal{l}_y\xi , x ) \\ and = and \underbrace{\mathcal{l}_\xi [ r ( x , y ) ] - [ \mathrm{foo} ] + r ( \mathcal{l}_xy , \xi ) }_\mathrm{qux}\text{ . } \end{eqnarray*}$$ so the objective is to show that the right-hand side , $\mathrm{qux}$ , is identically zero whenever $\xi$ is a killing vector field . let 's write $s^a{}_b = [ r ( x , y ) ] ^a{}_b = r^a{}_{bcd}x^cy^d$ , and just crank it out : $$\begin{eqnarray*} \mathcal{l}_\xi s^a{}_b and = and \nabla_\xi s^a{}_b - s^e{}_b\xi^a{}_{ ; e} + s^a{}_e\xi^e{}_{ ; b}\\ and = and \nabla_\xi s^a{}_b + x^cy^d ( r^a{}_{ecd}\xi^e{}_{ ; b} - r^e{}_{bcd}\xi^a{}_{ ; e} ) \\ and = and \nabla_\xi s^a{}_b + x^cy^d ( \nabla_c\nabla_d-\nabla_d\nabla_c ) \xi^a{}_{ ; b}\text{ , } \end{eqnarray*}$$ where the last step is actually valid for arbitrary $z^a{}_b$ , not just $\xi^a{}_{ ; b}$ . the first term of this cancels with the first term of $\mathrm{foo}$ . so far we have not used the fact that $\xi$ is a killing vector field . let 's do so now by considering the other two terms of $\mathrm{foo}$: $$\nabla_x [ r ( y , \xi ) ] ^a{}_b - \nabla_y [ r ( x , \xi ) ] ^a{}_b = \nabla_x\nabla_y\xi^a{}_{ ; b} - \nabla_y\nabla_x\xi^a{}_{ ; b}\text{ , }$$ where the starting identity $r^a{}_{bcd}\xi^d = \xi^a{}_{ ; bc}$ was used . the same identity also gives : $$r ( \mathcal{l}_xy , \xi ) ^a{}_{b} = \nabla_{ [ x , y ] }\xi^a{}_{ ; b}\text{ . }$$ therefore , we have shown that for any vector fields $x , y$ , $$\begin{eqnarray*} x^cy^d ( \mathcal{l}_\xi r^a{}_{bcd} ) and = and \left [ x^cy^d ( \nabla_c\nabla_d-\nabla_d\nabla_c ) - ( \nabla_x\nabla_y-\nabla_y\nabla_x ) + \nabla_{ [ x , y ] }\right ] \xi^a{}_{ ; b}\\ and = and 0\text{ . }\end{eqnarray*}$$ ( if you have trouble with the last step , check christoph 's answer to the other question and modify appropriately . ) thus $\mathcal{l}_\xi r^a{}_{bcd} = 0$ , qed .
the answer provided by ja72 pointed me to the correct direction but did not answer directly my questions . the answers are : the author states rr is the rotation velocity matrix between earth fixed reference frame and body fixed reference frame . i assume the " between " means a rotation from body coordinates to earth coordinates . rt is the translation velocity matrix between earth fixed reference frame and body fixed reference frame . i also assume the " between " means a rotation from body coordinates to earth coordinates . one is right the other not . according to equation $ ( 1 ) $ $r_t$ is a transformation matrix from body frame to earth fixed frame . $r_r$ on the other hand is a rotation matrix from body fixed frame to earth fixed frame so my assumption on $r_r$ was wrong . one question is how do i calculate the value of the angular acceleration described by the partial derivative of $\dot{\phi}$ and $\dot{\theta}$ the answer to this question is through the gradient of the vector , which in matrix from , is similar to ( but not ) the jacobian of the matrix . this can be obtained analytically taking the derivatives of $r_r$ . the other question is in which referential is ζ˙ and why did the author make a rotation and " derotation " on kt ? according to $ ( 1 ) $ the referential of $\zeta$ is earth fixed . the " derotation " is due to symbolic manipulation in $ ( 15 ) $ . regarding the state space formulation i still do not follow how it works out , but from $ ( 9 ) $ i was able to get angular and linear acceleration : $$\begin{cases} \ddot{\eta_{i}} and =\left ( -i_{t}r_{r}\right ) ^{-1}\left ( -t_{pb}+i_{t}\left ( \frac{\partial r_{r}}{\partial\phi}\dot{\phi}+\frac{\partial r_{r}}{\partial\theta}\dot{\theta}+\frac{\partial r_{r}}{\partial\psi}\dot{\psi}\right ) \dot{\eta}+k_{r\_aero}r_{r}\dot{\eta}+\left ( r_{r}\dot{\eta}\right ) \times\left ( i_{t}r_{r}\dot{\eta}\right ) \right ) \\ \ddot{v}_{i} and =\left ( -mr_{t}^{t}\right ) ^{-1}\left ( k_{t}r_{t}^{t}\dot{v}_{i}+mr_{t}^{t}g-f_{pb}\right ) \end{cases}$$ so far this formulation seems to be giving good results but i have not verified against another opinion , and i may be missing something .
do not worry , i did research in surface plasmons and even then i was more than a year into it before i truly understood , on an intuitive level , how the light gets a ' kick ' from the grating . you are correct that it is diffraction at a 90 degree angle to the normal , but there is an easier way to think about it . you say you have never taken a formal course in optics so i will talk a little bit about diffraction gratings in general . you might have come across one before and know that if a beam of light hits it , it is diffracted into several different beams . transmissive diffraction gratings are what one usually encounters in high school physics so i will illustrate one below : the numbers at the end of each beam are known as the order $\nu$ of that beam . the grating equation is $d ( \sin\theta_i + \sin\theta_o ) = \nu\lambda$ , where d is the distance between lines of the grating , $\lambda$ is the wavelength of the light , $\theta_i$ is the angle of incidence , and $\theta_o$ is the angle of the outgoing beam . in the above illustration , $\theta_i$ is zero . next we consider a reflective grating ( for example a piece of metal with 1d periodic grooves ) , as in the following illustration : the same mathematics govern this situation as well . you will notice the $\nu=+2$ order being very close to grazing the grating surface . adjusting the angle of incidence a little bit would cause it to do so . in that case , it would have the required wave vector to launch a surface plasmon , which is the phase matching condition that you started out with . you get the $\beta = k\sin\theta\pm\nu g$ when you convert the grating formula to wave vectors ( reciprocal space ) which i am too lazy to do right now . i suppose you could technically say that the light got a momentum ' kick ' from the $\nu=-2$ order being launched in the opposite direction , but thinking of it as the light getting a ' kick ' is really misleading in my experience .
when an object resonates , it will have a tendency to vibrate in a characteristic way ( it is normal modes ) to produce sounds at its characteristic frequencies , of which there may be more than one . theses frequencies are basically a function of the geometry of the object , as well as the mass density and ' stiffness ' of the material used . for example , an empty bottle will produce a characteristic pitch when air is blown across the top ( similar to a flute ) . the bottle 's length sets up the characteristic wavelength of the pitch produced . half-filling it with water will produce the same pitch an octave higher ( ie : half the wavelength ) . the pitch produced is also a function of the density and velocity of air . blowing harder ( higher velocity ) can produce a different ( alternative ) mode of oscillation with different pitch . in the case of a tuning fork , there is usually just one characteristic pitch ( normal mode ) , which is a function of the density of the fork material , its characteristic length , cross-sectional area , as well as its ' stiffness ' . a heavier ( higher density ) metal will oscillate slower and have lower pitch than a tuning fork made of a lighter metal . a longer tuning fork will also produce a deeper pitch ( longer wavelength ) , all else being equal . using a ' stiffer ' material ( a material with higher young 's modulus which does not ' stretch ' or ' bend ' as easily ) will tend to produce a higher frequency pitch .
in general , decoherence and renormalization are two different things . decoherence is loss of quantum correlations due to lack of information ( either on the interaction , or - on states of some particles ) . the most typical example is when you loss a particle - then you need to trace out with respect to their degrees of freedom , effectively changing all entanglement in ( classical ) randomness . renormalization is a procedure , when you construct ( or deal with ) an effective theory , disregarding some degrees of freedom only to threat them as a single object ( e . g . instead of four particles only one " effective particle" ) . typically you do not " loss " particles , but treat collections of then as a new one . however , the procedure is lossy - i.e. you lose some properties ; in particular , from a pure state you can go to mixed ones . however , typically you rather still hold an approximate pure state , rather than the exact one which ( after neglecting some degrees of freedom ) is mixed ; see e.g. : pietro silvi , tensor networks : a quantum-information perspective on numerical renormalization groups ( 2011 ) , http://www.sissa.it/cm/thesis/2011/silvi.pdf
let me give it a shot : if i interpret this correctly , $\mathbf{f}$ will be the operator for the full spin of the coupled system , $\mathbf{s}$ will be the operator of the electron spin ( usually , one would consider $\mathbf{j}$ , the spin containing also spin-orbit coupling , but we are on the s-shell , hence no angular momentum ) and $\mathbf{i}$ will be the nuclear spin . then it should hold that $\mathbf{f}=\mathbf{s}+\mathbf{i}$ , right ? first , let 's have a look at the hyperfine structure hamiltonian $\mathbf{h}_{hf}$ . by construction of $\mathbf{f}$ , the eigenstates of $\mathbf{h}_{hf}$ will be eigenstates of $f^2$ and $f_z$ . this is just the same as for angular momentum and electron spin ( and we construct $\mathbf{f}$ to have this property - this lets us label the eigenstates by the quantum number corresponding to $\mathbf{f}$ ) . hence the hamiltonian must be diagonal in the $|f^2 , m_f\rangle$-basis . one can also see that $f^2$ commutes with $i^2$ and $s^2$ ( and so does $f_z$ ) , since $\mathbf{f}=\mathbf{i}+\mathbf{s}$ . now we have a look at $\mathbf{h}_b$ , the interaction hamiltonian with a constant magnetic field . we can see that ( up to some prefactor ) $\mathbf{h}_b=s_z$ . hence the eigenstates of $\mathbf{h}_b$ must be eigenstates of $s_z$ and thus also of $s^2$ and , since the two operators are independent ( they relate to two different types of spins , hence the operators should better commute ) also to $i^2$ and $i_z$ , if you want . the crucial problem is that $s_z$ and $f^2$ do not commute . why ? well : $\mathbf{f}=\mathbf{i}+\mathbf{s}$ hence $f^2=s^2+i^2+2\mathbf{s}\cdot \mathbf{i}$ . now $s_z$ and $\mathbf{s}$ do not commute , because $s_z$ does not commute with e.g. $s_x$ , which is part of $\mathbf{s}$ . since $f^2$ commutes with $\mathbf{h}_{hf}$ and $s_z$ commutes with $\mathbf{h}_b$ , but not with $f^2$ , we have that $\mathbf{h}_{hf}$ does not commute with $\mathbf{h}_b$ . this means that $\mathbf{h}_b$ and $\mathbf{h}_{hf}$ cannot be diagonal in the same basis , hence you need to have off-diagonal elements . in order to see how the matrix representing $\mathbf{h}_b$ looks like in the $|f^2 , m_f\rangle$-basis , you can express the $|m_i , m_s\rangle$-basis ( in which $\mathbf{h}_b$ is diagonal ) in terms of the other basis . this is exactly what equations ( 4.21 ) do . these are obtained by ordinary addition of angular momenta . from there , you can construct the unitary transforming the basis $|m_i , m_s\rangle$ into $|f^2 , m_f\rangle$ and $\mathbf{h}_b$ will be the diagonal matrix in the basis $|m_i , m_s\rangle$ conjugated with this unitary . edit : i am not quite sure whether i understand correctly what your problem is , but let me elaborate : we want to find the hamiltonian $\mathbf{h}_b$ in the $|m_im_s\rangle$ basis . in this basis , it is diagonal , because $\mathbf{h}_b$ is essentially $s_z$ ( hence commutes with $s_z$ ) and it must also commute with $i_z$ since $s_z$ and $i_z$ are independent . if we order the basis according to $|\frac{1}{2} , \frac{1}{2}\rangle , |-\frac{1}{2} , -\frac{1}{2}\rangle , |\frac{1}{2} , -\frac{1}{2}\rangle , |-\frac{1}{2} , \frac{1}{2}\rangle$ , then , we can just read off the hamiltonian : the first and fourth vector are eigenvectors to eigenvalue $\mu b$ , the others of $-\mu b$ ( by definition of $s_z$ , since the second component in $|m_im_s\rangle=| ( si ) i_z , s_z\rangle$ tells us the eigenvalue of $s_z$ that the basis vector corresponds to ) , i.e. $$ \mathbf{h}_b=\begin{pmatrix}{} \mu b and 0 and 0 and 0 \\ 0 and -\mu b and 0 and 0 \\ 0 and 0 and -\mu b and 0 \\ 0 and 0 and 0 and \mu b\end{pmatrix}$$ now , as i said , you just have to change the basis . the matrix transforming the above basis into the new basis is given by eqn . ( 4.21a-d ) : $$u:=\begin{pmatrix}{} 1 and 0 and 0 and 0 \\ 0 and 1 and 0 and 0 \\ 0 and 0 and \frac{1}{2} and \frac{1}{2} \\ 0 and 0 and -\frac{1}{2} and \frac{1}{2} \end{pmatrix}$$ where the ordering of the $|fm_f\rangle$-basis is as for $\mathbf{h}$ in your text . now calculate $u\mathbf{h}_b u^{\dagger}$ and that should give you the part of $\mathbf{h}$ coming from $\mathbf{h}_b$ in the $|f , m_f\rangle$-basis and this will be exactly what is written in your book . edit 2: i sort of suspected this , so here is some more linear algebra for the problem . i will use dirac notion since i suspect you are more familiar with this : now suppose you have given two bases $|e_i\rangle$ and $|f_i\rangle$ and suppose they are orthonormal bases . what we want is a matrix $u$ that transforms one basis into the other ( i will call it $u$ , since it'll be a unitary - if the bases are not orthonormal , it'll only be an invertible matrix ) . so we want a matrix such that $$ |f_i\rangle:=u|e_i\rangle \qquad \forall i$$ how to construct this matrix ? well , given an equation for $|f_i\rangle$ in terms of the $|e_i\rangle$ will give you the i-th row of the matrix . you can also see the matrix elements in dirac notation : $$ \langle e_j|u|e_i\rangle=\langle e_j|f_i\rangle $$ in your case , $|e_i\rangle=|m_im_s\rangle$ and $|f_i\rangle=|f^2 , m_f\rangle$ . hence equation ( 4.21a ) will give you the first row of the matrix ( the ordering of the basis vectors $|m_im_s\rangle$ as i proposed above ) , ( 4.21c ) the second ( notice the basis ordering in the matrix $\mathbf{h}$ ! ) ( 4.21b ) the second and ( 4.21d ) the last row of the matrix . using the equation for the matrix elements above , you should be able to check that with not too much trouble . you can also easily check that $u$ is indeed a unitary ( i.e. . $uu^{\dagger}=u^{\dagger}u=\mathbb{1}$ . then we can calculate the matrix elements : $$ \langle e_i |\mathbf{h}|e_j\rangle=\langle e_i|u^{\dagger}u\mathbf{h}u^{\dagger}u|e_j\rangle=\langle f_i| u\mathbf{h}u^{\dagger}|f_j\rangle $$ , which tells you how the matrix looks like in the other basis .
it actually gets a bit complicated , since several effects are involved : evaporating water does require heat , which comes primarily from the hot stones . so throwing water on the stones does cool them down . ( this is where the claim one occasionally hears , that " throwing water on the stones makes the sauna colder " , comes from . technically it is true , if one considers the total heat content of the sauna as a whole . but since most of that heat is in the stones , and since you do not sit on the stones , that is pretty much completely irrelevant to how hot the part of the sauna that you do sit in gets , or feels . ) on the other hand , throwing water on the stones also significantly increases the heat transfer rate from the stones to the air : the evaporation produces a lot of hot steam , which will rise and mix with the ambient air in the sauna . so it is possible for the air temperature in the sauna to increase , even as the stones are cooled down . also , the introduction of steam obviously increases the humidity of the air , which will increase the rate of water precipitation on skin , and/or decrease the rate of sweat evaporation . ( the relative importance of these two effects will depend on the baseline humidity of the air , which can vary quite a lot . my gut feeling , based on experience , is that in all but the driest of saunas condensation probably dominates , simply because human skin is so much cooler than the air . ) in either case , the effect will be to transfer more heat to the skin , and thus to make the air feel hotter . finally , as the hot steam rises off the stones , it will push hot air around the sauna in front of it . while this increase in air movement is slight and transient , it probably does have a noticeable effect : as the hot air flows past the people in the sauna , it will act to disperse the layer of cooler air that forms over the skin , and thus increases heat transfer to the skin . ( if you do not believe me , try blowing some air over your skin in a sufficiently hot sauna . it burns . ) the upshot is that throwing water on the stones increases heat transfer , both from the stones to the air and from the air to your skin . as long as the stones stay hot enough to supply that heat , the net effect will be that you feel hotter . however , if you throw too much water on the stones , it is possible to " kill the stones " by cooling them close to or even below the boiling point of water . at that point , throwing more water is useless , and all you can do is add more firewood or turn up the thermostat and wait for the stones to heat up again . or , if you manage to do this in a smoke sauna , go wash yourself and get dressed up because the sauna is over for the night .
there is no unambiguous correct answer to this question because it is not well posed in terms of logical positivism : what is the difference between the two processes ? there is no way to tell which happens if you do not muck up the intermediate state with a measurement . if you mean this in terms of some quantum field theory with given fields and interactions and asymptotic states , then you can ask how the processes appear in a feynman description . the scattering process in qed is always two-step , the absorption and emission are separate space-time points . but the emission can precede the absorption both in coordinate time and in proper time along the electron 's world line , so you should include " emitted and then absorbed " to the list of possibilities . light does not have to be resonant in order to scatter off an atom . the amount of scattering/emission-reabsorption is smaller away from resonance . a light wave is also a long coherent field , and this field can acquire a phase push from the emission-reabsorption , leading the phase-velocity to be bigger than the speed of light . the issue of " how come the phases add up coherently " is adressed by two things : there is a large scale difference between the atoms and the light wavelength . each atom scatters the light independently and randomly into a spherical waves , which add up coherently in the original direction only to alter the phase velocity by a constant amount . there is no scattering from the bulk of a perfect crystal , for long wavelengths , because there is still a discrete translation invariance which means momentum is conserved up to big jumps , and the big jumps give waves with the wrong frequency for long enough wavelengths . but there are discrete momentum additions which are allowed for a short wavelength x-ray in an atomic crystal , and if the photon momentum comes out different but at the same frequency due to the coherence in a different direction , that is called diffraction . if you want scattering in a crystal , you need to scatter off defects which have a good amount of random variation in a box the size of one wavelength . similarly , if you scatter of a fluid , you need fluctuations in density to be meaningful in one wavelength . this is easier for blue light than for red light , so transparent fluids scatter blue .
there is two important differences between air and water : air is compressible , and the densities are about a factor of ~1000 apart - 1 kg/m³ vs 1 t/m³ ! for most concerns where you use propellers , compression plays no role because the pressure diferences are very low . the densities , however play a large role . the thrust can be described as $f = \dot m * \delta v$ , with $\dot m$ beeing the mass flow - kg and /s or such - and $\delta v$ the difference in velocity a volumeelement of fluid is accelerated . so , to achieve a similiar thrust , the same propeller would have to move 1000 times more air than water by volume . hence the often larger and faster spinning propellers for planes . on the other hand , in a heavier medium each wing of the propeller is subject to stronger torque ( all else beeing equal ) : $$q = \rho v_{a}^2 d^3 f_q ( \frac{nd}{v_a} ) $$ ( source ) $\rho$ is density , $v_a$ rate of advance ( how much the propeller moves forward per revolution ) , d is diameter and n number of revolutions . without going into the math it can probably be shown that in a heavier medium the propeller will experience somewhat more torque for the same thrust - i am to lazy to try now . the propeller will be built with more robust ( and possibly heavier ) material than would be the case id it is an only air propeller . that said , i believe that a propeller for both media is entirely possible , though challenging . however , a propeller for a both media will need a drivetrain that can accomodate speed roughly a factor 1000 apart ( that is not trivial ) . one other reason why do not see a propeller for both media is that there is no vehicle that could make use of one .
it seems that these clouds are in fact reasonably well understood . roger smith , who seems to be something of an expert on these clouds , has several papers discussing the physics of morning glory clouds , many of which are linked from his " list of publications " page and can be viewed for free . this paper seems to be particularly good . i am not a meteorologist , either , but from what i have read they appear to be solitary waves ( a . k.a. , solitons ) which are travelling in a sort of atmospheric waveguide . i am sorry i can not give you a better answer than that ; please do look at the linked references . cheers !
in 3d there are 7 lattice systems which are classes of lattices having the same point group . one of them is the class of cubic lattices . this class contains three different bravais lattices which are distinguished by their translation group .
the reason you are hearing the train farther away is more consequence of the the geometry of different spaces than anything else . it starts with an inversion layer of cold air clinging close to the ground . just as glass bends light by making light move more slowly through it , an inversion layer of cold air bends sound because sound moves more slowly through cold air ( the molecules move more slowly is why ) . so , this inversion layer behaves like the audio equivalent of a large sheet of glass covering the ground and guiding the sound away from the air above it . this is the same principle that allows a fully transparent optical fiber to capture light moving through it and transmit that light for many kilometers with very few losses . ( @hwlau already noticed the inversion in the second of his three possible answers . ) on the ground side of the inversion layer , a fresh fall of snow further helps confine and preserve sound because it looks smooth to the long wavelengths of sound . so , even though snow thoroughly jumbles up the much shorter wavelengths of light , which is why it looks white , it looks very different and much more mirror-like to sound . put those two together -- diffraction on the top and reflection on the bottom -- and you have an example of two-dimensional sound dispersion . by way of contrast , sound dispersion from a train on a summer day that lacks any inversion layer and has sound-absorbing grass on the ground is an example of largely three-dimensional sound dispersion . so , why is the dimensionality of the sound dispersion important ? because sound ( or any other radiation ) disperses at a rate that is dependent on the number of dimensions of the space into which into which it is dispersing . if $l_n$ is the perceived loudness of the sound , $s$ is the distance to the sound source , and $n$ is the number of space dimensions into which sound is dispersing , the general equation for how loud the train will sound is : $l_n = 1/s^{n-1}$ notice that the lower the number of dimensions , the more slowly the sound disperses . ( i discussed this same issue from a slightly different perspective a few months ago in my answer to this question about why objects look smaller when they are farther away . ) this equation explains why optical fibers can transmit light many kilometers without loss any significant of intensity ( "loudness" ) . the dispersion space $n$ for optical fibers is $n=1$ , so $l_1 = \frac{1}{s^{1-1}} = \frac{1}{s^0} = 1$ . that is , there is no diminution of intensity . the audio equivalent would be a long tube , like the ones they used to use as intercoms in old houses ( and still use in some playgrounds ) . now for your inversion layer case , $n=2$ and : $l_2 = 1/s^{n-1} = 1/s$ but because your perception of train distance was tuned to $n=3$ space , you expected the sounds of the train to diminish at the much faster rate of : $l_3 = 1/s^{n-1} = 1/s^2$ the analysis of how much farther away the train really is turns out to be trickier than it might seem . that is because the model i just described assumes that the energy of a 3d sound source can be compressed into a mathematically precise 2d plane . the physical world just does not work that way , since the sound energy in a 3d volume cannot be forced into a true 2d plane without creating infinitely high energy densities in the plane . why ? well , pretty much for the reason that you cannot compress a 3d volume of air into an infinitely thin 2d plane without creating infinite mass densities . crossing dimensionalities is often done a bit casually in physics , but one need to be careful with it . so , in this case , instead of assuming a simple 2d plane , what you have to do is model the problem by using a " pancake " that more realistically represents the thickness of the inversion layer confining the sound . that allows sound intensities that " look " 3d in the immediate vicinity of the train , but then fade off more according to the dimensionality diffusion rules as distances increase to many times the thickness of the inversion layer . so , everything from this point is obviously guesswork about what happened in your case , but a nice ballpark height for your inversion layer might be 10 meters . approximating again , that 10 meters also becomes the " unit of equality " for distance from the train at which the sound of the train is perceived as the same in both cases . this approximation should work reasonably well for any more-or-less point source of sound coming from the train , in particular its whistle . so , call this unit of distance $s_u$ for hearing a similar loudness for the whistle $s_u = s_w = 10 m = 0.01$ km . alas , it gets messier . the sound of the train itself is anything but a point source , since you may be able to hear wheel-on-rail sounds for very long lengths , such as a kilometer for a long train . that also messes up the model and adds even more complexity in the form of orientation and sound delays . so , i am going to wrap all of that complexity up into a single huge approximation and say that for a long train , the sound of the all the train wheels on all the track sounds " about the same " for anyone within a kilometer of the train as it passes by , inversion layer or not . so , the length unit for assessing how train track noises changes over distance becomes $s_u = s_t = 1$ km . the equation now has to be altered slightly so that these " sounds the same " units $s_u$ are factored in : actual : $l_2 = s_u/s^{n-1} = s_u/s$ perceived : $l_3 = s_u/s^{n-1} = s_u/s^2$ solving for the $s$ distances in terms of loudness : actual : $s_2 = s_u/l$ perceived : $s_3 = \sqrt{s_u/l}$ the error factor $e$ for how far off your distance estimate was then is : $e = actual/perceived = s_2/s_3 = \frac{s_u/l}{\sqrt{s_u/l}} = \sqrt{s_u/l}$ for the train whistle , $s_u = s_w = 0.01 km$ . with $l$ in km : $e_w = \sqrt{s_u/l} = \sqrt{0.01/l} = 0.1/\sqrt{l}$ for the track noise from the entire train , $s_u = s_t = 1 km$ . with $l$ in km : $e_t = \sqrt{s_u/l} = 1/\sqrt{l}$ so , finally , a couple of very rough estimate examples are possible . assume the train is actually about $l = 16$ km away . in that case , the whistle sounds like it is $e_wl$ km away , or : $e_wl = 16e_w = 16 ( 0.1/\sqrt{16} ) = 0.4$ km away . in the same case , the train track sound will appear to be $e_tl$ km away , or : $e_tl = 16e_t = 16/\sqrt{16} = 4$ km away . so , not only are sounds moving through a winter inversion layer highly deceptive for estimating distances , they can be deceptive in different ways at the same time ! a point source such as the train whistle may well sound like it is even closer than the train as a whole -- and both perceptions will sound way , way closer than the actual distance .
in 1st case : you do not have a battery , current flows and creates a backward force on the wire given by f = ilb , to overcome this force you need to apply a force and do work against it . this work done gets converted to joules heat . however in case 2 since no current flows , no force is applicable and f = 0 , since there is no force to work against there is no work done in 2nd case and hence no mechanical energy to conserve , if the rod is moving it continues to move without slowing .
what would be the potential of gnd here ? gnd is the reference node which , by definition , measures 0v . to see this , consider that every one of the node voltages in the circuit are referenced to the gnd node . in other words , if you wish to physically measure the voltage at a node in the circuit , you connect the black lead of your voltmeter to the gnd node and your red lead to the node your wish to find the voltage of . clearly , if you put the red lead on the gnd node , you have connected the red and black leads together and thus , you will read 0v there . so , at the node that is labelled $-10v$ , the voltmeter placed between that node and gnd will read $-10v$ . if you move the red lead to node b , you will measure $-9.3v$ since , given the voltage source between the two nodes , node b must be $0.7v$ more positive than the $-10v$ node . but there is no current flowing through r1 there must be a current through $r_1$ since there is $-9.3v$ volts across it . the current is ' up ' through the resistor , ' up ' through the $0.7v$ source and through the $-10v$ source ( not shown ) back to gnd . i think your confusion lies with the $-10v$ node - it appears to be disconnected but , in fact , it is assumed that there is a $-10v$ voltage source connected there .
a sun or a star is not possible to exist on this scale ; to be as massive as a core of a planet , it is just not massive enough . but you did not mention the size of it . so if we put that aside , first of all there is no such thing as no gravity . where there is mass there is gravity , and that gravity has to be strong enough to hold gas ( atmosphere ) . and the rocks will have to sink into the core since they are the denser objects . if however we compared this to an existing example , where the sun is in the center of the solar system and holding planets ( floating chunks of rocks ) , there is still vacuum in between . because at the distances these planets are from the sun , the sun 's gravity is not strong enough to hold gas . where the sun 's gravity is strong enough there is gas , and that ends as far as the outer atmosphere of the sun itself . which mainly does not extend to the planets . therefore it is not possible .
the $b$ in these equations refers to the magnetic fields each individual wire creates . these fields are proportional to the individual wires ' own currents . in other words , the force of wire i on wire ii is $f_{i \to ii} =i_{ii} \ell b_{i} \sin \alpha$ . what is $b_{i}$ , how does it relate to $i_i$ , and how does it differ from $b_{ii}$ ?
every thermodynamic system satisfies the first law during a given process ; $$ \delta e = q-w $$ here $\delta e$ is the change in its internal energy , $q$ is the heat transferred to the system , and $w$ is the work done by the system . for a system undergoing a cyclic process , namely one for which it starts and ends in the same thermodynamic state , one has $\delta e = 0$ , and the first law then tells us that $$ q = w $$ now , suppose that the system is taking some heat $q_h&gt ; 0$ from a reservoir , and turning it into some work $w$ . let 's define $$ q_\mathrm{exhuast} = q-q_h $$ then we can write $$ q_h + q_\mathrm{exhaust} = w $$ the kelvin statement tells us that we must have $q_\mathrm{exhaust}\neq 0$ because otherwise , we would have $q_h = w$ ; the sole result of the process would have been to transform the heat it took in into work . the symbol $q_\mathrm{exhaust}$ is what one commonly calls the exhaust heat .
show me a distribution of remote mass that would provide the behavior we see for both jupiter in orbit around the sun the many moons in orbit around jupiter which both appear to be $1/r^2$ forces . now try to generalize to support all the moons and planets in the solar system . you can not do it because the system is highly over-constrained .
the real space propagator for the massive dirac fermion in $3+1$ dimensions is calculated in r . feynman 's book quantum electrodynamics ( lecture 17 , page 84 in the edition linked to ) . the result is very much as indicated in the question : first solve the wave equation , then differentiate the solution with the dirac operator again . in particular , feynman calculates the propagator of the klein-gordon equation in real space : $$ i_+ ( t , x ) = ∫ \frac{d^4p}{ ( 2π ) ^4} \frac{\exp [ -i ( p\cdot x ) ] }{p^2 - m^2 + i\varepsilon} = - ( 4π ) ^{-1} \delta ( s^2 ) + \frac{m}{8πs}h_1^{ ( 2 ) } ( ms ) $$ here , $s = + ( t^2-x^2 ) ^{1/2}$ for $t&gt ; |x|$ and $s = -i ( x^2-t^2 ) ^{1/2}$ for $t &lt ; |x|$ . moreover , $\delta ( s^2 ) $ is a delta function and $h^{ ( 2 ) }_1 ( ms ) $ is a hankel function . then , you have to differentiate the delta function , indeed . however , note that to be physically meaningful , the propagator $g ( x_2 , t_2 ; x_1 , t_1 ) $ for the dirac equation should only take into account the positive energy states for the retarded time frame $t_2 - t_1 &gt ; 0$ , while the advanced portion $t_2 - t_1 &lt ; 0$ should only take into account the negative energy eigenstates ( holes ) .
the student is right in that energy is the analog of momentum in the " time direction " but i would not go so far as to call it " momentum in the direction of time " . it is analogous in two ways that i can think of off the top of my head : it is the time-component of the 4-dimensional energy-momentum vector in special relativity . noether 's theorem relates a symmetry in the laws of physics with respect to a coordinate to a conserved quantity . momentum is conserved because the laws of physics are invariant with respect to translations in space , and energy is conserved because the laws of physics are unchanging in time . i would not call it " momentum in the direction of time " because that phrase , at least to me , implies that energy is more momentum-like than it really is .
the derivation in landau and lifschitz is making some additional implicit assumptions . they assume that all forces come from pair-interactions , and that the pair forces are rotationally invariant . with these two assumptions , the potential function in the lagrangian is $v ( x1 , . . . , xn ) = \sum_{\langle i , j\rangle} v ( |x_i - x_j| ) $ and then it is easy to prove newton 's third law , because the derivative of the distance function is equal and opposite for each pair of particles . this type of derivation is reasonable from a physical point of view for macroscopic objects , but it is not mathematically ok , because it omits important examples . no rotational invariance , no third law dropping the assumption of rotational invariance , but keeping the assumption of pair-wise interaction , one gets the following counterexample in 2 dimensions , with two particles ( a , b ) with position vectors $ ( a_x , a_y ) $ $ ( b_x , b_y ) $ respectively : $v ( a_x , a_y , b_x , b_y ) = f ( a_x-b_x ) + f ( a_y - b_y ) $ where f is any function other than $f ( x ) =x^2$ . this pair potential leads to equal and opposite forces , but not collinear ones . linear momentum and energy are conserved , but angular momentum is not , except when both particles are on the lines $y=\pm x$ relative to each other . the potential is unphysical of course , in the absence of a medium like a lattice that breaks rotational invariance . many-body direct interactions , no reflection symmetry , no third law there is another class of counterexamples which is much more interesting , because they do not break angular momentum or center of mass conservation laws , and so they are physically possible interactions in vacuum , but they do break newton 's third law . this is the chiral three-body interaction . consider 3 particles a , b , c in two dimensions whose potential function is equal to the signed area of the triangle formed by the points a , b , c . $v ( a , b , c ) = b_x c_y - a_x c_y -b_x a_y - c_x b_y + c_x a_y + a_x b_y$ if all 3 particles are collinear , the forces for this 3-body potential are perpendicular to the common line they lie on . the derivative of the area is maximum by moving the points away from the common line . so you obviously cannot write the force as any sum of pairwise interactions along the line of separation , equal and opposite or not . the forces and torques still add up to zero , since this potential is translationally and rotationally invariant . many body direct interaction , spatial reflection symmetry , crappy third law if the force on k particles is reflection invariant , it never gets out of the subspace spanned by their mutual separation . this is because if they lie in a lower dimensional subspace , the system is invariant with respect to reflections perpendicular to that subspace , so the forces must be as well . this means that you can always cook up equal and opposite forces between the particles that add up to the total force , and pretend that these forces are physically meaningful . this allows you to salvage newton 's third law , in a way . but it gives nonsense forces . to see that this is nonsense , consider the three-particle triangle area potential from before , but this time take the absolute value . the result is reflection invariant , but contains a discontinuity in the derivative when the particles become collinear . near collinearity , the forces perpendicular have a finite limit . but in order to write these finite forces as a sum of equal and opposite contributions from the three-particles , you need the forces between the particles to diverge at collinearity . three body interactions are natural there is natural physics that gives such a three-body interaction . you can imagine the three bodies are connected by rigid frictionless struts that are free to expand and contract like collapsible antennas , and a very high-quality massless soap bubble is stretched between the struts . the soap bubble prefers to have less area according to its nonzero surface tension . if the dynamics of the soap bubble and the struts are fast compared to the particles , you can integrate out the soap bubble degrees of freedom and you will get just such a three-body interaction . then the reason the bodies snap together near collinearity with a finite transverse force is clear--- the soap bubble wants to collapse to zero area , so it pulls them in . it is then obvious that there is no sense in which they have any diverging pairwise forces , or any pairwise forces at all . other cases where you get three body interactions directly is when you have a nonlinear field between the three objects , and the field dynamics are fast . consider a cubically self-interacting massive scalar field ( with cubic coupling $\lambda$ ) sourced by classical stationary delta-function sources of strength g . the leading nonlinear contribution to the classical potential is a tree-level , classical , three-body interaction of the form $v ( x , y , z ) \propto g^3 \lambda \int d^3k_1 d^3k_2 { e^{i ( k_1\cdot ( x-z ) + k_2\cdot ( y-z ) ) } \over ( k_1^2 + m^2 ) ( k_2^2 + m^2 ) ( ( k_1+k_2 ) ^2 + m^2 ) }$ which heuristically goes something like ${e^{-mr_{123}}r_{123}\over r_{12}r_{23}r_{13}}$ where the r 's are the side lengths of the triangle and $r_{123}$ is the perimeter ( this is just a scaling estimate ) . for nucleons , many body potentials are significant . the forces from the crappy third law are not integrable if you still insist on a newton 's third law description of three-body interactions like the soap bubble particles , and you give a pairwise force for each pair of particles which adds up to the full many-body interaction , these pairwise forces cannot be thought of as coming from a potential function . they are not integrable . the example of the soap-bubble force makes it clear--- if a , b , c are nearly collinear with b between a and c , closer to a , you can slide b away from a towards c very very close to collinearity , and bring it back less close to collinear . the a-b force is along the line of separation , and it diverges at collinearity , so the integral of the force along this loop cannot be zero . the force is still conservative of course , it comes from a three-body potential after all . this means that the two-body a-b force plus the two-body b-c force is integrable . it is just that the a-c two body force is not . so the separation is completely silly . absence of multi-body interactions for macroscopic objects in empty space the interactions of macroscopic objects are through contact forces , which are necessarily pairwise since all other contacts are far away , and electromagnetic and gravitational fields , which are very close to linear at these scales . the electromagnetic and gravitational forces end up being linearly additive between pairs , and the result is a potential of the form landau and lifschitz consider--- pairwise interactions which are individually rotationally invariant . but for close packed atoms in a crystal , there is no reason to ignore 3-body potentials . it is certainly true that in the nucleus three-body and four-body potentials are necessary , but in both cases you are dealing with quantum systems . so i do not think the third law is particularly fundamental . as a philosophical thing , that nothing can act without being acted upon , it is as valid as any other general principle . but as a mathematical statement of the nature of interactions between particles , it is completely dated . the fundamental things are the conservation of linear momentum , angular momentum , and center of mass , which are independent laws , derived from translation invariance , rotational invariance , and galilean invariance respectively . the pair-wise forces acting along the separation direction are just an accident .
the affine galilean structure is assigned by the first principle of newtonian dynamics , i.e. by giving the class of inertial reference frames in the spacetime $g^4$ . on the one hand it assigns the structure of an affine space to the spacetime , on the other hand it selects a subclass of permitted transformations between reference frames . a reference frame is a bijective map $g^4 \ni p \mapsto ( t ( p ) , {\bf x} ( p ) ) \in \mathbb r \times \mathbb r^3$ the class of allowed coordinate transformations is that including all of the transformations : $$t ' = t + c \quad ( 1 ) $$ $${\bf x}' = r{\bf x} + t {\bf v} + {\bf c}\: , \quad ( 2 ) $$ where $c\in \mathbb r$ is constant , ${\bf v}$ and ${\bf c}$ are constants in $\mathbb r^3$ and $r \in o ( 3 ) $ . these are the transformation of coordinate between inertial reference frames . in this way a foliation $\{\sigma_t\}_{t \in \mathbb r}$ of $g^4$ turns out to be defined , made of 3-dimensional euclidean spaces . also a surjective map $t : g^4 \to \mathbb r$ is consequently defined , that labels the class of those manifods . the function $t$ ( absolute time ) coincides , up to an additive constant , with the coordinate $t$ of every inertial reference frame , so that intervals of time are absolute . the coordinates ${\bf x}$ of every inertial frame range in each absolute 3-dimensional space $\sigma_t$ that is in common with each of these observers , including the metrical structures that are invariant changing reference frame , in view of ( 2 ) . concerning special relativity . yes , you can completely characterize minkowski spacetime $m^4$ as a real affine 4d space equipped with a pseudo distance with signature $ ( 1 , -1 , -1 , -1 ) $ and a time orientation . the inertial reference frames are a subclass of the affine coordinate systems on $m^4$ where the pseudo distance assumes the canonical form you have written in your question . the transformations between these coordinate frames are the so called poincaré orthochronous transformations . addendum to answer the last question : the relativistic corresponding of galileo group is poincaré orthochronous group and not the lorentz group . orthochronous poincaré transformations are the most general transformations preserving the structure of minkowski spacetime including time orientation . these are , obviously , affine transformations as galilean transformations are .
you are double counting the degenacy in $e_{k}$ by multiplying by $2$ and summing from $-4$ to $4$ . either you sum from $0$ to $4$ $2e_{k}$ or you sum from $-4$ to $4$ $e_{k}$ but not both
the difference is , that if a quantity in your model is secretly theoretically zero , in practice , you will typically still get a small non-zero number , whose size is determined by the scale of uncertainties in measurements , rounding errors , etc . on the other hand , in a naturalness problem in fundamental physics , we typically have a dimensionless quantity , whose measured value is significantly bigger than the error-bars ( and therefore a non-zero quantity ! ) but much much smaller than $1$ .
something moving such that it " measures the proper time " is simply something moving from point 1 at time 1 , to point 2 at time 2 , as seen in s ( you have the data ) . recall the basic definition of velocity .
yes , the unruh effect violates mach 's principle . in effective quantum field theory , the unruh effect arises because the ground state of the hamiltonian $h_a$ naturally associated with an accelerating observer is different than the ground state of the hamiltonian $h_s$ generating time translations in a " static " frame . the static and accelerating frames have different time coordinates $t$ , and because the hamiltonian generates infinitesimal changes of $t$ , it is different . and different operators have different eigenstates , including the lowest-eigenvalue one ( the ground state ) . one vacuum may be viewed as a squeezed/coherent state built upon the other , which may be interpreted as particle production , and creation operators have to be accordingly mixed with the annihilation operators according to the other frame - by the so-called bogoliubov transformation . although mach 's principle has never been properly defined or turned into a realistic theory , it seems pretty clear that according to all of its interpretations , it postulated that it was impossible to have two different notions of a " vacuum " depending on the acceleration of the observer . however , that is exactly the case because of the unruh effect . an acceleration-dependent definition of the ground state ( the vacuum ) , as realized in the derivation of the unruh effect , exactly means that the acceleration may be distinguished from no acceleration even in the vacuum - which is exactly what mach 's principle would like to prohibit . the unruh effect therefore violates mach 's principle . more precisely , it refutes it because mach 's principle is wrong and the unruh effect is real . however , one does not really have to go to the quantum theory to see that mach 's principle has been showed incorrect by subsequent developments . the existence of a dynamical metric tensor - even in the classical , non-quantum theory - is enough . the gravitational waves are the simplest classical entities that show that the gravitational field is real even in the vacuum , and it allows the particles moving through it to distinguish free fall from accelerating motion .
let 's consider the specific type whistle shown in the question . when we blow the whistle , air is forced to rush out through the narrow opening . the flow of air at the center of the stream is significantly faster than the neighboring air close to the main stream . if the air stream is easily deflected ( unstable ) , vortexes are generated . if the same thing happens repeatedly , many more vortexes with similar properties will be generated . these vortexes cause air pressure to vary in a periodic way , so sound wave is produced . the frequency of this sound wave is related to the rate at which the vortexes are shed . since the process is rather chaotic , many different rates or frequencies are produced at a time . as you can see in the picture , the stream is divided into two parts . one part coming out of the opening and the other part stays inside . sound wave trapped inside will interfere with each others . if the frequency of sound does not match any of the resonant frequencies of the chamber , the waves will interfere destructively and vanish quickly . however if the frequency matches the resonant frequency of the cavity , the wave 's amplitude will increase overtime . the rate of increasing will decreases as the amplitude builds up . eventually it will reach a steady state . at this point the amplitude of sound wave is strong enough that the sound becomes very audible . the sound wave come out of the hole , get dispersed strongly , and finally reaches our ears . some whistles have a small ball bounces around inside the cavity . the ball changes the shape of the cavity and at the same time the resonant frequencies . thus it allows us to hear wider range of sound frequency .
yes , it does . when an object floats , its mass is not affected . it only affect the force experienced by it , as the water exerts a " buoyant force " on the object : basically , there is a pressure difference between the top and bottom surfaces , and this corresponds to a force difference , leading to a net upwards force : however , remember that the force exerted by the water on the object leads to an equal and opposite force exerted by the object on the water . since at equilibrium , $m_{obj}g=\text{buoyant force}$ , a force equal to the weight of the body is exerted on the water . when the beaker is weighed , this extra force is balanced by the normal force . here are some free body diagrams . bf is the buoyant force , t is the string tension . note that n is the weight that the weighing platform measures .
at least for $\omega_1=\omega_2$ it is possible to solve the system exactly . our hamiltonian can be written $$\hat h = \frac{\delta }{2}{\hat \sigma _z} + {\omega} ( \hat a_1^\dagger {\hat a_1} + \hat a_2^\dagger {\hat a_2} ) + ( {g_1}{\hat a}_1+{g_2}{\hat a}_2 ) {{\hat \sigma }_ + } + ( {g_1}{\hat a}_1^\dagger+{g_2}{\hat a}_2^\dagger ) {{\hat \sigma }_ - }$$ we apply a change of variables $$a_1&#39 ; =a_1 cos\theta-a_2sin\theta$$ $$a_2&#39 ; =a_1 sin\theta+a_2cos\theta$$ this change of variables preserves the $\delta$ and the $\omega$ terms in the hamiltonian but rotates the $ ( g_1 , g_2 ) $ vector . by choosing an appropriate $\theta$ we can achieve $g_2&#39 ; =0$ . this means the $ ( a_1&#39 ; , \sigma ) $ system decouples from $a_2&#39 ; $ . the former is an ordinary jaynes-cummings model whereas the later is a harmonic oscillator
here i basically do what joshphysics has already mentioned , just in a little more detail , and in a bit more intuitive basis ( which makes effectively no difference ) . so definitely not a slick way . i use the basis $|m_1\rangle\otimes|m_2\rangle$ , where $m_i=\pm1/2$ . keeping only the sign we denote the basis as $\{|++\rangle , |+-\rangle , |-+\rangle , |--\rangle\}$ . now for $s_z=s_{1z}+s_{2z}$ you can check that $$ s_z|++\rangle=\left ( \frac{\hbar}{2}+\frac{\hbar}{2}\right ) |++\rangle $$ and similarly $$ s_z|--\rangle=\left ( -\frac{\hbar}{2}-\frac{\hbar}{2}\right ) |--\rangle $$ so that you and up with the matrix $$ s_z\rightarrow\hbar\left [ \begin{array}{cccc}1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and -1\end{array}\right ] $$ and $$ s_z^2\rightarrow\hbar^2\left [ \begin{array}{cccc}1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 1\end{array}\right ] $$ for $s_x$ and $s_y$ you use raising and lowering operators as josh suggested and obtain the matrix elements by acting on each state in the basis and seeing what has a non-zero dot product with the result , e.g. $$ \begin{align} s_y|+-\rangle=\frac{1}{2i}\left ( s_+-s_-\right ) |+-\rangle and =\\\frac{1}{2i}\left ( s_{1+}+s_{2+}-s_{1-}-s_{2-}\right ) |+-\rangle and =\frac{1}{2i}\left ( \hbar|++\rangle-\hbar|--\rangle\right ) \end{align} $$ thus $\langle++|s_y|+-\rangle=\frac{\hbar}{2i}$ and $\langle--|s_y|+-\rangle=-\frac{\hbar}{2i}$ . once you do this you can get $$ s_x\rightarrow\hbar\left [ \begin{array}{cccc}0 and 1 and 1 and 0\\1 and 0 and 0 and 1\\1 and 0 and 0 and 1\\0 and 1 and 1 and 0\end{array}\right ] $$ and $$ s_y\rightarrow\hbar\left [ \begin{array}{cccc}0 and 1 and 1 and 0\\-1 and 0 and 0 and 1\\-1 and 0 and 0 and 1\\0 and -1 and -1 and 0\end{array}\right ] $$ finally the hamiltonian looks like $$ h=\left [ \begin{array}{cccc}\hbar^2b and 0 and 0 and \hbar^2a\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\\hbar^2a and 0 and 0 and \hbar^2b\end{array}\right ] $$ therefore $|+-\rangle$ and $|-+\rangle$ are eigenvectors with eigenvalue $0$ , and you only need diagonalize $2\times2$ matrix to find that the eigenvalue $\hbar^2\left ( b+a\right ) $ corresponds to the eigenvector $\left ( |++\rangle+|--\rangle\right ) /\sqrt{2}$ , and $\hbar^2\left ( b-a\right ) $ corresponds to the eigenvector $\left ( |++\rangle-|--\rangle\right ) /\sqrt{2}$ .
the mass of jupiter is about $10^{27}$ kg which , via $e=mc^2$ , translates to $10^{44}$ joules . if one turned the planet into thermonuclear fuel in some way and detonated it immediately , about 1% or $10^{42}$ joules would be released . because the diameter of jupiter is about 130,000 km , the blast would last at least half a second or so . so we have $10^{42}$ joules per half a second . it is $2\times 10^{42}$ watts . the sun only releases $4\times 10^{26}$ watts of power , so the blast would be $2\times 10^{16}$ times stronger than the sun . however , looking at the effects on the earth , we must realize that jupiter is about 5 times further from the earth than the sun , reducing the energy flux by a factor of $5^2=25$ . so the half-second blast seems about $10^{15}$ times stronger than the sunshine . the equilibrium temperature is , because of the $\sigma t^4$ law , about $10^4$ times higher than that from the sunshine , about a million degrees . the sun warms the earth by a degree in hours or so . a source that is $10^{15}$ times stronger obviously needs a tiny fraction of a second to reach thousands of degrees and evaporate the matter on the surface . so no doubt about it , the thermonuclear blast of jupiter would burn and evaporate all nearby sides of all the planets – all of them are comparably far from the ground zero . on the other hand , would the incoming energy be able to evaporate the whole earth ? we would be getting $10^{15}\times 342\times 4\pi \times 6,378,000^2\sim 2\times 10^{32}$ watts for half a second , about $10^{32}$ joules per the blast and per the surface of the earth . the specific heats of materials are comparable to $1,000$ joules per celsius degree and kilogram so we have $10^{29}$ kilogram-degrees to be heated . divide it by the earth mass below $10^{25}$ kg to see that you may still heat the material by tens of thousands of degrees by the incoming light . so i do think that this could evaporate the whole earth but not the largest planets like saturn . needless to say , the sun itself would be pretty much untouched . its surface already has 6,000 degrees or so . the strong radiation from jupiter could bring it to a million of degrees , by the calculation above , but it is the same as the temperature of the interior layers . so the sun would get destabilized a bit but it would quickly converge back to the sun we know , i guess . the calculations above are completely unrealistic because at most , one could think about turning jupiter into a small star that would still burn very slowly and would be far weaker than the sun .
you wish to estimate the flux density ( power per unit area ) at a location based on a reading of the received power at a mobile device situated there . the signal transmitted from the router will result in an energy density of radiated power , decreasing with distance . the mobile device has a receive antenna which collects some of this rf power in its vicinity . this receive antenna is characterized by a parameter called its " effective area " , or " effective aperture " . this has the dimensions of an area , and if its value is $a \ cm^2$ , then at a location where the rf power had a flux density of $f \ mw/cm^2$ , the power captured by the antenna would be $fa \ mw$ now the effective aperture is related to a parameter called the antenna gain by $$ g ( \theta , \phi ) = \frac{4\pi a ( \theta , \phi ) }{\lambda^2} $$ where $\lambda$ is the wavelength . both the antenna gain and the effective aperture are functions of direction ( specified by the two parameters $\theta$ , $\phi$ ) . antennas have varying degrees of directionality ( for example a parabolic reflector is highly directional ) . the formula shows that the direction in which the gain is maximum also maximises the effective aperture . now for a few numbers - say the frequency $f = 2.4ghz$ ( depends which wifi band you are using ) , we get a wavelength $\lambda = 12.5cm$ antennas on mobile devices are actually quite lossy , so lets assume a gain of -3db in the given direction ( i.e. . 0.5 in linear terms ) . this gives us $a = 6.2cm^2$ your reading of -50dbm corresponds to $10^{-5} mw$ , so the flux density is $10^{-5}/6.2 = 1.6^{-6}mw/cm^2$
i do not have the book here right now , so i am not sure what he is referring to exactly by that comment . but by modifying the energy-momentum tensor , you will change the central charge and actually get a whole new cft . so i would not call that a symmetry of the free scalar theory . however the theory does have a lot more symmetry , let me focus on one chiral sector only . as you know already , the special feature of two-dimensional cft 's is that the spin-two conserved current ( em-tensor ) $\bar{\partial}t ( z ) =0$ , automatically implies that there is an infinite number of conserved currents $\bar{\partial} ( z^n\ , t ( z ) ) = 0$ in the theory . this is essentially the reason why the conformal algebra , extends to the infinite-dimensional virasoro algebra i two-dimensions . lets use the notation $w_2\equiv t$ , where the index refers to the spin . free-field theories , however , contain an infinite tower of higher-spin conserved currents $w_s ( z ) $ where $s= 2 , 3 , \dots$ is the spin . similar to the $w_2$ case , for any conserved current $w_s$ , there is infinite number of associated conserved currents $\bar{\partial} ( z^n\ , w_s ( z ) ) =0$ . thereby each current $w_s$ extends the virasoro algebra with infinite number of new generators , and there is by itself an infinite number of currents $w_s$ . so this is a vast extension of the virasoro algebra . for example in the case of free complex scalar theory the currents are given by [ a ] $w_s ( z ) = b ( s ) \sum_{k=1}^{s-1} ( -1 ) ^k\ , a^s_k\ , :\partial^k\phi\ , \partial^{s-k}\bar{\phi}: ( z ) $ , where $b ( s ) $ and $a^s_k$ are constants . see in particular equations ( 2.11 ) and ( 2.18a ) - ( 2.18e ) in [ a ] . here $w_2 ( z ) $ is the usual energy-momentum tensor leading to $c=2$ and the virasoro algebra . all the generators combined give rise to the so-called $w_\infty^{prs}$-algebra , which contain the virasoro algebra as a " small " subalgebra . similar things can be done for other free-field theories , i myself used a similar construction for the free ghost system in a recent paper . higher-spin extensions of the virasoro algebra are usually called $\mathcal w$-algebras and they do not lead to conventional lie algebras , but certain types of non-linear algebras . see [ b ] for a review . the free field theories realize the rare type of $\mathcal w$-algebras which are usual ( linear ) lie algebras . in higher dimensions free-field theories also have an infinite tower of higher-spin conserved currents and thereby an infinite dimensional symmetry algebra . but each conserved current only lead to a finite number of generators in the algebra , unlike the the two-dimensional case . maybe polchinski is referring to this vast number of higher-spin symmetries of the free-field theories ? [ a ] : bakas and kritsis - bosonic realization of a universal $\mathcal w$-algebra and $z_{\infty}$ parafermions [ nucl . phys . b 343 , 185 ( 1990 ) ] [ b ] bouwknegt and schoutens - $\mathcal w$ symmetry in conformal field theory [ phys . rept . 223 ( 1993 ) 183-276 ]
it can be shown by conformal compactification of the spacetime , i.e. using coordinates which allow you to draw a penrose diagram . after you have done so , you can analytically continue the geometry and discover the other half ( see chapter 2 of this ) . regarding differences in topology : i do not see any reason for this to be the case .
more smaller triangles are better than a few big ones ( more stiff ) . also think about which triangle shape distributes the loads for evenly . how are you constrained dimensionally ? ideally you create a vertical structure that distributes the weight over $n$ columns of noodles . if you can not do this , then you find which configuration yields the more vertical orientations . also it is important to know if the noodles can rotate about their support making them a 2-force member , or are they fully loaded with the ends fixed in location and orientation ? think of stiffness here . how would you make the truss as stiff as possible ? would long flat triangles be better , or short tall ones ? i guess without more details we cannot qualify an answer .
in your first reference , page $58$ , equation $ ( 3.55 ) $ , there is a personal definition by the author of what it calls " spinor adjoint of a matrix": $\overline m \stackrel {def}{\equiv} \gamma_0 m^\dagger \gamma_0$ with this definition , as you noticed , you have obviously $\overline {\gamma^\mu}= \gamma^\mu$ the above definition of " spinor adjoint of a matrix " is compatible with the definition of the adjoint $ ( 3.54 ) $: $\overline \psi = \psi ^\dagger \gamma_0$ in the following way : $\overline {m\psi} = ( m\psi ) ^\dagger \gamma_0 = \psi^\dagger m^\dagger \gamma_0 = ( \psi^\dagger \gamma_0 ) ( \gamma_0 m^\dagger \gamma_0 ) = \overline {\psi}~ \overline {m}$
the speedometer on an airplane measures air speed , that is speed relative to a big block of air , not ground speed . so if it is traveling at an air speed of 60 miles per hour , that means if there were two balloons 60 miles apart , it could travel between them in one hour . however , if in that hour , the entire block of air moved 10 miles the other way , then at the end of the hour , the plane would still have covered 60 miles of air , but only 50 miles of ground . the speedometer on an automobile measures ground speed , that is speed relative to the ground , not air speed . so if the car is doing 60 miles per hour , that means after one hour , it will have traveled 60 miles measured on the ground . regardless of the wind .
you mistake is that you use the absolute value " of the spatial components " ( your words ) of the velocity only . picking spatial components only is clearly not a lorentz-covariant procedure , so it cannot calculate the invariant " feelings of the astronauts " . instead , the right condition is given by the same inequality but $|d u^\mu / d \tau|$ is the length of the four-vector one obtains by differentiating the four-velocity $u^\mu$ , where $u_\mu u^\mu = 1$ , over the proper time $\tau$ . the vector $d u^\mu / d \tau$ is spacelike and perpendicular ( according to the lorentzian metric ) to the velocity vector $u^\mu$ itself ; but this derivative is not a purely spatial vector in any inertial system . in a frame in which the spatial components of $u^\mu$ are already nonzero , $d u^\mu / d \tau$ contains a nonzero time component , too . when you calculate it correctly , the proper time needed to achieve the speed of light is infinite . the easiest way to calculate it is one that assumes some knowledge of the lorentzian geometry and how it is analogous to the euclidean geometry . a uniformly accelerating object in the euclidean spacetime would produce a circular world line . in the real , minkowski space , the world line is a hyperbola . the coordinates after proper time $\tau$ may be written in analogy with sines and cosines but they are hyperbolic ones : $$ t = \sinh ( \tau/\tau_0 ) , \quad x = \cosh ( \tau/\tau_0 ) $$ here , $\tau_0$ is a constant depending on the acceleration . consequently , the speed after proper time $\tau$ is simply the ratio , $$ v = \tanh ( \tau/\tau_0 ) $$ for a small $\tau$ , this gets reduced to $\tau/\tau_0$ in the limit and the $\tau$-derivative $1/\tau_0$ should be the ( maximum ) acceleration $g_m$ so $\tau_0=1/g_m$: $$ v = \tanh ( \tau g_m ) $$ in the $c=1$ units . you may invert it : $$ \tau = \frac{c}{g_m} {\rm arctanh} ( v/c ) $$ where i restored the powers of $c$ for your convenience . note that arctanh of one is infinity . for a small $v/c$ , one uses ${\rm arctanh}\ , x\approx x$ and the right formula reduces to your nonrelativistic formula from the original question .
when the finite speed of light – the delay of the rays from the source – is taken into account , one encounters many optical effects aside from " how the world is in relativity effects " such as lorentz contraction . flat lines look like arcs or cicles , one may see " behind himself " , and there is of course the doppler shift of the frequencies depending on the relative speed . also , streetcars going from left to right are rotated around a vertical axis , and so on . you may download real time relativity as a great " relativistic 3d game " . for a link and other comments on relativistic optical effects , see http://motls.blogspot.com/2009/02/relativistic-optical-effects.html?m=1
this is a famous period doubling experiment which is reprinted in cvitanovic 's " universality in chaos " , along with other foundational papers on the period doubling route to chaos . the paper you read is probably due to libchaber and maurer " a rayleigh bernard experiment : helium in a small box " proceedings of the nato advanced studies institute on nonlinear phenomena at phase transitions p . 259 ( 1982 ) . or else giglio , musazzi , perini " transition to chaotic behavior via a reproducible sequence of period doubling bifurcations " prl 47 , 243 ( 1981 ) or else libchaber , laroche , fauve " period doubling cascade in mercury , a quantitative measurement " j . phys . lett 43 l211 ( 1982 ) i just copied these from cvitanovic 's table of contents , i read this paper you are talking about and it is one of these . i should point out that it is not surprising theoretically that the period doubling cascade is the same as any other one-dimensional map , because this is the most likely way for a system to make a period doubling cascade--- to have two directions unstable at once is measure zero , so you have a one-dimensional instability from a fixed point , and this is modelled by feigenbaum , and it is universal , so universally applicable .
in 1958 dac was developed in nist by weir , lippincott , van valkenburg , and bunting . this dac article shows an image of a hand palm sized dac in the nist museum . the diamond anvil cell ( dac ) is a refined device , based on the bridgman cell . the latter was developed in the beginning of the first half of the 20th century . your question aims on the updated dac technique . what is the technical distinctive feature ? the bridgman cell has a quatro-plate geometry for applying force . this first developed technique has the feature to allow rays or wires between a pair of pressure plates and is available in a wide pressure range . polished diamond flat-tops allow beam transmission through the sample . this is depicted in the wikipedia article . however beam intensity is extenuated due too high fresnel losses . refractive index $n_{diamond}\approx 2.4$ is high . spectroscopic measurement may be influenced by natural lattice defects in diamond lattice . the refined method , called dac , allows higher pressure . to archive this the bridgman anvil made of tungsten-carbon alloy was replaced by a single-crystal diamond . the problem of breaking anvils was solved and a new pressure range was available . fun fact diamonds used by nist often were destroyed due to high pressure . the diamonds were former property of smugglers . government agents confiscated the gem diamonds . they were given for scientific purpose to the researchers .
since this question is still open and therefore not definitely answerable at present , i save the valuable discussion of the topic in the comments as an answer such that it does not get lost : this is just an accident of 10 dimensions--- there is too much supersymmetry to have a full susy superspace . it is a very good question , but research level , if you answer it fully , everyone will breathe easier . – ron maimon apr 12 at 2:11 3 up voted there are superspace formulations for 4d n=4 theories , the problem is that they only are on-shell formulations . the n=4 multiplet contains the n=2 hypermultiplet and there is a no-go theorem saying that there is no off-shell formulation with a finite number of auxiliary fields . thus you get projective and harmonic superspaces with infinite numbers of auxiliary fields . this works for n=2 , but in n=4 all constraints to reduce the unconstrained superfields down to the physical multiplets force the fields on-shell . – simon apr 12 at 3:47 3 up voted and as @ron says , the reason why it is so difficult to construct such formulations is an open research-level question . if the reason was known , then we had either have a workable n=4 superspace formulation or a no-go theorem by now . . . – simon apr 12 at 3:48 3 up voted dear dilaton , good question . ron and simon have already answered to some extent and i will only offer a different extent , extending ron 's comment in particular . if you want to make n=1 susy in 4d i.e. 4 real supercharges manifest , you need 4 superspace fermionic coordinates . with 16 supercharges , you would probably need at least 16 fermionic coordinates in the superspace but then the fields would have 216=256 components which is pretty high give that you only need 16 on-shell components only . most of the component fields would have to be auxiliary , linked to deritives of others , etc . hard – luboš motl apr 12 at 5:37 4 up voted there is also an interesting twistor-like transform for the 10d super yang-mills by a guy called witten – luboš motl apr 12 at 5:39 thanks guys for these valuable comments and the cool links therein . i would " like " and appreciate them as " partial " answers ( since as you say there is no full answer on this yet ) too . . . :- ) . – dilaton apr 12 at 8:45 @lubošmot l must admit that twistors are one of my black holes of ignorance ( i did not get it from roger penrose 's " road to reality" ) :-/ . . . so i am checking from time to time if i can find a nice " pedagogical " introduction to this on trf ; - ) – dilaton apr 12 at 8:50 1 up voted @dilaton : this is the problem with research level stuff , i can not answer because i do not feel confident enough in my biases about what the answer could or should be to put them in writing , and i would mention a bunch of things that i tried and did not work to answer this , and are not interesting , and i think everyone else is hesitant to answer too for similar reasons . maybe you could copy the comment thread into an answer box , and accept your own answer . – ron maimon 2 hours ago 1 up voted i mean , if you want a little more on this--- there is the question of whether superspace is fundamental in the first place--- it is just a trick for writing multiplets in a way that takes the susy off shell naturally , but the physical reasoning has always eluded me . i tried nicolai maps as an alternative , but it never worked , and it always is tantalizingly close to working , and i tried learning harmonic superspace for on-shell n=4 , but although it is correct , its so annoyingly complicated to work with ! and the s-matrix is simple , so there is a better language out there , i do not know what . – ron maimon 1 hour ago thanks @ron maimon , that is a good idea to save the discussion into an answer . i`m somehow intrigued too by the question if superspace itself could have some physical meaning . . . – dilaton 2 mins ago even though the question is still open , it could probably nevertheless be worthwhile to know what you tried and why it did not work . i mean similar to some kind of " null results " who can be interesting too . . . ? – dilaton
your question gets at two related points : what allows materials to conduct , and what allows materials to be transparent . conductivity requires easy movement of charge through a material . typically this charge is in the form of electrons but in an electrolyte the charge movement is primarily ions moving in the fluid . the quantum-mechanical nature of electrons means they can only exist in certain , discrete energy levels . the specific levels available depend on the electron configuration of the atoms or the ( more complicated ) electron configuration of molecules . good conductors have many closely-spaced energy levels available to electrons and very little energy is required to move them between these levels . this is the band theory of conductivity . the opacity of a material also depends on the electron configurations in the material and the energy levels available . the energy of a photon is proportional to its frequency which means materials tend to act as a low-pass filter . high energy ( high frequency ) electrons have enough energy to kick electrons into higher energy states which cause the photon to be absorbed . if the material has big gaps between the energy bands then photons will not have enough energy to force an electron energy transition so the light will pass through without being absorbed . generally conductivity and transparency are at odds with each other . conductors need closely spaced energy levels and transparency needs widely spaced levels . in the case of indium tin oxide has a mix of both properties . it has electron configurations that are closely spaced at low energies and then a large bandgap at visible light energies . this causes ito to be opaque to low energy light ( infrared ) but transparent to visible light .
the first image shows an object traveling at mach 1 ( $v=c$ ) . the second one shows the object traveling at some supersonic velocity ( $v&gt ; c$ ) . for both the cases , the longitudinal pressure waves pile up . say the observer is standing in the ground and the object is traveling at $c$ . the observer can not hear the pitch of sound because , the waves reach him all at once and hence , he had hear a loud " bash " . the most necessary thing is that he had to wait until the source arrives . when the source is directly overhead , he hears the shock waves . when the object breaks the sound barrier ( supersonic ) , it is somewhat worse . the same loud " thump " is produced here . but , the observer would notice a delay in sound ( i.e. ) he has to wait for the shock waves to reach him . there is also this mach cone produced by these waves since the waves group so fast behind the object . and so , there is a region of high pressure at first followed by a low pressure zone . thus , if the object passes by in some comparable distance , it makes a lot of disturbance , " breaking things " , etc . . . the comic thing is , for someone inside the aircraft , he can still speak with his partner , can hear the bump of a ball on the plane , etc . the problem is only for the distant observer who suffers . . .
the calculation is described in detail in the wikipedia article on recombination . if you consider the ionisation of hydrogen as a reaction : $$ p + e \rightarrow h + \gamma $$ then you can write down an expression for the equilibrium constant as a function of temperature using the saha equation : $$ \frac{n_pn_e}{n_h} = \left ( \frac{m_ek_bt}{2\pi\hbar^2} \right ) ^{3/2} \exp \left ( \frac{-e_i}{k_bt} \right ) $$ if you take 50% ionisation you can work out the corresponding temperature and it turns out to be about 4,000k . so now it is just a matter of relating the temperature of the universe to the time after the big bang . once we are past the various phase transitions that happened in the first few instants after the big bang the temperature is inversely proportional to the scale factor . sadly there is not a simple equation to give the scale factor as a function of time , however it is a straightforward numerical calculation , and the result is that the temperature was 4,000k about 380,000 years after the big bang . that is how the figure of 380,000 years is calculated .
mathematically spoken , since you want your wave functions to be square integrable , your wave functions must be in $l^2$ or some subspace thereof . however , you will not find a function in this space that has a support on a countable set of points , since the lebesgue integral cannot see countable sets ( measure 0 ) , hence there cannot be a function ( i.e. . no wave function ) with support in a single point ( incidentally , the delta function is not a " function " in a way for that reason ) . this tells us that a wavefunction for a particle that is fully localized cannot be defined in the usual setting of square lebesgue-integrable functions , which is not too tragic , because we do not really think it makes physical sense anyway .
useful information : light travels in straight lines . at a distance $d$ between the boy and the point on the ground directly beneath the light , at what distance will the top of the shadow be cast on the ground ? well , draw this scenario on paper and draw a straight line from the light , just touching the top of the boy and see where it hits the ground . simple geometry tells us that this occurs at $2d$ . now we have a formula for the position of the top of the head ( denoted by $h$ ) of the shadow as a function of distance of the boy from the light : $$h=2d$$ you should now be able to see how to calculate the speed . i hope this helped .
you are right and the author is wrong . the problem of p=np is a pure mathematical problem , which has nothing to do with physics . even though quantum mechanics ( or whatever physical system ) can solve all problem in blink of eye , it still does not prove whether p=np or not . the key point is that all computations are based on physics , but not the reverse . in computer complexity theory , they treat these ( existing or imaginary ) superpower machine as an oracle machine , which can give an answer in a single computational step . this formulation allows them to analyze quantum computer . the claim of non-observable macroscopic quantum effects because of p ! =np is based on the following argument : to prove macroscopic quantum effects , we need to compare the physical system with the simulation results of schrodinger equation . so , if we can not simulate schrodinger equation efficiently , then we can not prove any quantum effect . as shown in the paper : this implies that in the case , in which the problem $\phi_\psi$ would be intractable , the deterministic quantum model of a macroscopic system ( built around the exact solutions to the system schrodinger equation ) would be without predictive content inasmuch as there would be no practical means to extract the prediction about the system future state from the schrodinger equation . in this manner , a schrodinger cat state – as a linear combination of the exact ( and orthogonalized ) solutions to the system schrodinger equation – would be predictively contentless and for this reason unavailable for inspection . the author clearly does not familiarize with quantum mechanics , nor the schrodinger equation . schrodinger equation is only a part of qm . he also does not understand the particle concept in the schrodinger equation . a particle is not an atom . this is a basic concept that most physics student should have understand after half dozen courses in qm . the interference of one c$_{60}$ molecule can be described by one particle wavefunction $\psi ( x ) $ . there is no need to solve a 60-particles wavefunction $\psi ( x_1 , . . . , x_{60} ) $ , which is already extreme hard to solve by current computers . if a schrodinger cat state exists , you can always perform a bell-state type measurement , even at the macroscopic level . there is no need to solve schrodinger equation with large number of variables in wavefunction $\psi ( x_1 , . . . , x_{10^{23}} ) $ to know the result , since the system should be effectively described by a two state system .
the significance of the metric : $$ d\tau^2 = dt^2 - dx^2 $$ is that $d\tau^2$ is an invarient i.e. every observer in every frame , even accelerated frames , will agree on the value of $d\tau^2$ . in contrast $dt$ and $dx$ are coordinate dependant and different observers will disagree about the relative values of $dt$ and $dx$ . so while it is certainly true that : $$ dt^2 = d\tau^2 + dx^2 $$ this is not ( usually ) a useful equation because $dt^2$ is frame dependant .
no . it does not necessarily mean that the acceleration of $a$ is greater than the acceleration of $b$ . here 's an explicit counterexample : object $a$ is moving at $10\ , \mathrm{m/s}$ with constant velocity while object $b$ is moving at $5\ , \mathrm{m/s}$ with an acceleration of $1\ , \mathrm m/\mathrm s^2$ . in this case , the acceleration of $a$ is zero , so $b$ 's acceleration is greater , but it is velocity is lower . note that the initial conditions of the motions of the two objects are irrelevant ; we are talking about instantaneous velocities and accelerations , and given any two objects , one can completely independently pick their velocities and accelerations .
it is important to distinguish different phases of the material . while it is true that water and snow consists of the same building blocks of $h_2 o$ , those individual blocks are actually not as important as the resulting material . you can build a storage room and pyramids just out of bricks ( at least in principle ) . gases let 's start with the simplest case . simplest in the sense that there is not much going on , just molecules flying around . let us just assume ideal gas where molecules do not interact with each other . then to explain everything it suffices to look at just one molecule . the color of the molecule of course arises because of its absorption spectrum . this in turn depends on the molecule 's energy levels . for atoms these are pretty nice discrete levels . for more complicated molecules you also have rotational and vibrational degrees of freedom to take into account and besides discrete levels you will also see continuous strips that consist of very fine energy levels corresponding to that . for illustration , see the wikipedia article on $h_2$ hydrogen . in any case , if you are somehow able to obtain that energy spectrum you can then investigate the macroscopic properties by the means of the usual boltzmann statistics $$\hat w = z^{-1} \exp ( -{\beta \hat h} ) = z^{-1} \sum_n\exp ( - \beta e_n ) {\hat p}_n $$ with $z = {\rm tr} \exp ( -{\beta \hat h} ) $ being the partition function , $\beta$ the inverse temperature and $p_n$ projector on $n$-th energy level . using this you can see that as you increase temperature the energy level distribution will change to occupy higher levels and this in turn will change the probability of individual absorption processes between certain levels . now , the only place where you need to talk about qed is the connection between absorption spectrum and energy levels . recall that in quantum mechanics energy levels are stable . they do not change , so there would be neither emission nor absorption . to resolve this apparent paradox we have to recall that we forgot to quantize the electromagnetic field . if you take this into account then atom 's excited energy levels are no longer stable because of the fluctuations of electromagnetic field . or in particle terms : because the number of particles is not conserved and it is easy to create photon out of nothing . of course , energy is conserved so only photon 's corresponding to the energy difference are possible . and the same can be said for absorption . liquids and solids for now i will not talk about these phases because the answer is both becoming long and because optical properties of solids would constitute a book of its own . as for liquids , i have a feeling that the same analysis as for gases should hold except it is of course no longer sufficient to talk about individual molecules because of non-negligible interactions . but i guess starting from ideal gas 's statistics and just switching temperature should give a reasonable first approximation . remark : looking back on the answer , i did not really explain your question completely . but i guess it is because it is quite hard to encompass everything that is going on . maybe another split and specialization ( e . g . to color of solids ) of the question would be in order ? :- )
there is no temperature . if we use the following definition " temperature is the average kinetic energy of the particles " . then no particles - no temperature . as the first sight this answer does not seem to be good enough , but if you want to calculate " average spin " or " average charge " those parameters will have no sense if there is no particles to calculate data on .
you should not think much of his statement about being in two places at once ; it is a bunch of fluffy nonsense he is using to make his work sound sexier . a human-sized object at livable temperatures will never exhibit such behavior . their experiment actually used thousands of measurements to determine what was happening with the lever . i will try to summarize , but please let me know if you need clarification . the device they study is an electrical circuit that connects the macroscopic lever from your picture to a quantum bit ( or " qubit" ) . at very low temperatures both the lever and the qubit are two-state systems , meaning they are individually described by two quantum states ( e . g . " not vibrating " and " barely vibrating " , which i will respectively denote $\left| 0 \right\rangle$ and $\left| 1 \right\rangle$ ) . the coupled system is found in combinations of these states , but the relevant ones in this experiment are when the qubit is barely vibrating and the lever is not , and vice-versa . in my previous notation these states are $$ \left| 1 \right\rangle_q \left| 0 \right\rangle_l , \quad \left| 0 \right\rangle_q \left| 1 \right\rangle_l , $$ where $q$ and $l$ to refer to the qubit and the lever . after cooling down the circuit so the lever and qubit are not vibrating ( $\left| 0 \right\rangle_q \left| 0 \right\rangle_l$ ) they give the qubit a little energy so it is in the barely vibrating state ( $\left| 1 \right\rangle_q \left| 0 \right\rangle_l$ ) . since the qubit and lever are connected , over time the energy moves from one to the other . equivalently , the system evolves to a new quantum state where the lever is barely vibrating and the qubit is not ( $\left| 0 \right\rangle_q \left| 1 \right\rangle_l$ ) . from a quantum mechanical viewpoint this evolution is attributed to the system being in a superposition of the two states $$ a ( t ) \left| 1 \right\rangle_q \left| 0 \right\rangle_l + b ( t ) \left| 0 \right\rangle_q \left| 1 \right\rangle_l . $$ the squares of the numbers $a ( t ) $ and $b ( t ) $ tell us the probability of finding the system in each of its quantum states at a certain time $t$ . when $a=1$ only the qubit is vibrating , when $b=1$ only the lever is vibrating , and when $a$ and $b$ are both non-zero the lever is " simultaneously " vibrating and stationary . o'connell and his collaborators were able to measure the state of the qubit very accurately . they prepared the circuit in the way i described above , waited some amount of time , and looked to see if the qubit was vibrating or not . they repeated this process over and over , tallied up how often the qubit was vibrating , then calculated the probability they would find it vibrating if they performed the experiment again . for certain wait times they found that the qubit was sometimes vibrating and sometimes not vibrating . going back to my expression for the superposition , this means that $a$ and $b$ are non-zero , or that the lever is in a superposition of stationary and vibrating states .
i was always told that to find whether or not a field is conservative , see if the curl is zero . this is almost always true , but not always true . i have now been told that just because the curl is zero does not necessarily mean it is conservative . correct ! to illustrate what is going on , let 's do an example . conside the following vector field : $$\vec{v} ( x , y ) =\frac{-y\hat{x}+x\hat{y}}{x^2+y^2} . $$ note that $\vec{v}$ is not defined at the origin . is $\vec{v}$ conservative ? let 's define " conservative " as follows a vector field $\vec{v}$ is conservative if for any closed path $c$ , the integral $\int_c \vec{v}\dot\ , d\vec{l}=0 . $ consider the path parametrized as $x ( t ) =r\cos ( 2\pi t ) $ and $y ( t ) =r\sin ( 2 \pi t ) $ for $t$ going from 0 to 1 . this path is just a circle of radius $r$ centered on the origin . the displacement on the path is $$\frac{d\vec{l}}{dt} = 2\pi r \left ( - \hat{x}\sin ( 2\pi t ) + \hat{y}\cos ( 2\pi t ) \right ) . $$ if we integrate our example $\vec{v}$ on this path we get $$\begin{align} \int_c \vec{v}\cdot d\vec{l} and =\int_{t=0}^1 \left ( \frac{-y\hat{x}+x\hat{y}}{x^2 + y^2} \right ) \cdot ( 2\pi r ) \left ( -\hat{x}\sin ( 2\pi t ) + \hat{y}\cos ( 2\pi t ) \right ) \ , dt\\ and = 2\pi \end{align}$$ which shows that $\vec{v}$ is definitely not conservative . note that the integral does not depend on the radius $r$ of the path . now , we compute the curl of $\vec{v}$ . for convenience , define $r\equiv x^2 + y^2$ , i.e. $r$ is the radial polar coordinate . $$\begin{align} \nabla \times \vec{v} and \equiv \frac{d\vec{v}_y}{dx} - \frac{d\vec{v}_x}{dy}\\ and =\frac{r^2 - 2 x^2}{r^4} - \frac{-r^2 + 2 y^2}{r^4} \\ and = \frac{2r^2 - 2r^2}{r^4}\\ and = 0 . \end{align}$$ we have now shown that $\vec{v}$ has zero curl . a consequence of this is that if we were to integrate $\vec{v}$ along any little path around a point where $\vec{v}$ is defined , we are guaranteed to get zero . thus , $\vec{v}$ has zero curl but is not conservative . what is going on ? if you picture $\vec{v}$ you will see that it is a swirl of vector lines circling the origin . the magnitude of the lines decreases as you move away from the origin . this decrease is just right so that if you were to integrate around a little loop which does not encircle the origin ( i.e. . if you check the curl ) you get zero . however , because of the global circling around the origin , if you integrate along a loop which does enclose the origin , you get something non-zero . thus , you can think of the integral as either " feeling " the presence of the origin and picking up the $2\pi$ we calculated , or not feeling the origin and giving zero . it is like the origin is a special point worth $2\pi$ . this is really interesting ! our field $\vec{v}$ is conservative everywhere locally , but if you make a path around the origin you can get a nonzero integral , so $\vec{v}$ is not conservative globally . remember we pointed out that $\vec{v}$ is not defined at the origin ? this is not an accident . vector fields which are conservative locally but not globally must have " holes " at which they are not defined . in fact , these vector fields must be approaching infinity near their holes , which $\vec{v}$ most certainly is , as you can check [ 1 ] . those infinite points have " residues " which show up in integrals which go around them . for the experts in the audience , this is exactly the same residue you get from integrating around a simple pole in the complex plane . let 's get back to your question to show this is conservative i would go ahead and take the curl . it will be zero - but that is not definitive proof it is conservative ? how would i show it is ? as you have said , and we have demonstrated , having zero curl does not guarantee that a field is conservative . what does guarantee that a field is conservative is that you can express it as the gradient of a scalar function . in more general mathematical terms , if there exists a function $f$ such that $\nabla f = \vec{v}$ , then $\vec{v}$ is said to be exact . an exact vector field is absolutely 100% guaranteed to conservative . so , one answer to your question is that to show a vector field is conservative , just show that it can be written as the gradient of a function . another answer is , calculate the general closed path integral of the vector field and show that it is identically zero in all cases . let 's go on though , because this is really super interesting stuff . vector fields with zero curl are guaranteed to be exact , meaning that zero curl guarantees conservativeness , unless the vector field has holes ( i.e. . points at which it is not defined ) . so , the mantra you learned that zero curl indicates conservativeness is almost always true , but it fails for vector fields which have holes , like our example $\vec{v}$ does at the origin . now here 's the really amazing part . if i tell you that a vector field has exactly 1 hole , and that it has zero curl but is not exact , there is only one vector field it can possibly be ( up to addition of other exact vector fields ) . in other words , if i tell you that a vector field $\vec{w}$ has zero curl , has one hole , and is not a gradient of any function , then you know for sure that $\vec{w}=\vec{v} + \vec{\lambda}$ where $\vec{\lambda} = \nabla f$ for some $f$ . if repeat the same situation but with two holes , then you know that $\vec{w}$ is expressible as a linear combination of specific curl-less but not exact vector fields associated to the two holes . this whole business generalizes to high dimensional spaces . if you like it , read up on differential forms . you can try the book " analysis on manifolds " by munkres , although that is a very " mathy " book . one last thing . instead of talking about having zero curl and being the gradient of a function , you can talk about having zero divergence and being the curl of another vector field . normally , if a vector field has zero divergence , you can write it as the curl of something else . the electric field of a point charge is conservative and has zero divergence . however , it is not the curl of any vector field . in fact , it is the only $^{ [ 2 ] }$ vector field in three dimensions which has zero divergence and is not a curl of something else . and of course , the electric field of a point charge goes to infinity at the charge point , so this is one of those fields where having a " hole " allows it to break the usual rules . how did nature know to do that ? [ 1 ] the numerator of $\vec{v}$ goes as $r$ while the denominator goes as $r^2$ . therefore the whole thing goes as $1/r$ which diverges near the origin . [ 2 ] this is a slightly incorrect statement , but it is ok for now .
i assume that your question is : " how do i compute the helmholtz free energy given the dependence of the entropy on the intensive parameters $s ( e , n , v ) $ ? " in this case you start by inverting $s ( e , n , v ) $ to get a relation for $e ( s , n , v ) $ . then you can perform the legendre transform , $$ f ( t , n , v ) = e ( s ( t , n , v ) , n , v ) - s ( t , n , v ) \ , t \ , , $$ where the function $s ( t , n , v ) $ is defined through the relation $$ \frac{\partial e}{\partial s} ( s , n , v ) \equiv t \ , . $$
you have a few different but related questions so i will try to explain them in a simple , no-math way . if a radio tunes to a specific frequency , where does the excess energy go ? almost every object that has radio waves ( electromagnetic waves ) around it absorbs some of the radio energy . when the radio waves hit the electrons in the atoms and transfers a bit of momentum and energy to the electrons . if the material absorbing the radio waves is an insulator then most of this energy is turned into heat . if the material is a conductor ( like an antenna ) then a very small electric current is created . the shape , size , and length of an object has some effect on which frequencies it absorbs best so this is why different types of radio antennas are different shapes and sizes . when a radio is designed to tune a specific frequency , it is still absorbing many other frequencies . those other frequencies just get filtered out , meaning that energy is turned into heat . the current created by the frequency being tuned is amplified with a bunch of transistors ( formerly vacuum tubes ) and turned into a strong usable signal . if one continues to hit the resonant frequency , should not the wire begin to melt at some point from too much energy ? you are right that if an object is absorbing a lot of radio energy it will heat up . for normal , every-day settings though the amount of radio energy around us is very low . objects do heat up a tiny bit but this heat gets transfered to the surrounding air and the objects stay cool . if the antenna was in a vacuum with nothing around it for it to transfer heat to , then it would get warmer and warmer . the antenna would get warm until the amount of energy it radiates via black-body radiation matches the amount of radio energy it is absorbing . there are cases where so much radio energy is being absorbed the objects can get very hot . this is how a microwave oven works . the microwave puts out many watts of radio energy which gets absorbed by the objects in the microwave and they heat up because of the huge amount of energy they are absorbing is much faster than they can get rid of the energy to the surroundings . and why is it that one can pick up the radio frequency over the radio 's own self-induced frequency ? i think you mean how is a radio able to transmit and receive at the same time . that is a complicated question but most simple radios can not . they are either transmitting or receiving and they must stop one to do the other . one somewhat easy way to transmit and receive at the same time is to use different frequencies for send and receive . more over , how do you insert information into a wave ? the simplest way to put information in a wave is to use " amplitude modulation " ( am ) . another simple way is with " frequency modulation " ( fm ) . there is a huge branch of math and physics behind the more complicated methods .
there are not mechanical oscillators that have the same frequencies as visible light . the most common oscillators with the same frequencies as visible light are the electrons orbiting atoms , which is why atomic transitions emit and absorb visible light . if you are willing to accept a powered system , you could absorb all the light into an image-processing system and then project that image in another direction , possibly with much more light intensity than the original image had . for instance , folks with telescopes sometimes use video cameras to observe objects too faint to see naked-eye in real time .
the relation $p={h\over \lambda}$ applies to photons , it has nothing to do with the uncertainty principle . the issue is localizing the photons , finding out where the are at any given time . the position operator for a photon is not well defined in any usual sense , because the photon position does not evolve causally , the photon can go back in time . the same issue occurs with any relativistic particle when you try to localize it in a region smaller than its compton wavelength . the schrodinger position representation is only valid for nonrelativistic massive particles . there are two resolutions to this , which are complementary . the standard way out it to talk about quantum fields , and deal with photons as excitations of the quantum field . then you never talk about localizing photons in space . the second method is to redefine the position of a photon in space-time rather than in space at one time , and to define the photon trajectory as a sum over forward and backward in time paths . this definition is fine in perturbation theory , where it is an interpretation of feynman 's diagrams , but it is not clear that it is completely correct outside of perturbation theory . i tend to think it is fine outside of perturbation theory too , but others disagree , and the precise nonperturbative particle formalism is not completely worked out anywhere , and it is not certain that it is fully consistent ( but i believe it is ) . in the perturbative formalism , to create a space-time localized photon with polarization $\epsilon$ , you apply the free photon field operator $\epsilon\cdot a$ at a given space time point . the propagator is then the sum over all space-time paths of a particle action . the coincidence between two point functions and particle-paths this is the schwinger representation of feynman 's propagator , and it is also implicit in feynman 's original work . this point of view is downplayed in quantum field theory books , which tend to emphasize the field point of view .
to show that this measure is lorentz invariant you first need to explicitly write your integral as an integral over mass shell in 4d k-space . this could be done by inserting dirac delta function $\delta [ k^\mu k_\mu-m^2 ] $ and integrating over the whole 4d space . then you could apply the following transformations : \begin{align} \theta ( k_0 ) \cdot\delta [ k^\mu k_\mu-m^2 ] and = \theta ( k_0 ) \cdot\delta [ k_0^2-|\mathbf{k}|^2-m^2 ] \\ and =\theta ( k_0 ) \cdot\delta\left [ ( k_0-\sqrt{|\mathbf{k}|^2+m^2} ) ( k_0+\sqrt{|\mathbf{k}|^2+m^2} ) \right ] \\ and =\frac{\delta\left [ k_0-\sqrt{|\mathbf{k}|^2+m^2}\right ] }{2\ , k_0} , \end{align} where heaviside function $\theta ( k_0 ) $ is used to select only future part of the mass shell .
surface tension is a consequence of fluid molecules that , at the interface with another fluid , have less neighbors than in the bulk fluid . generally speaking molecules will always be more attracted to ' their own kind ' and therefore the molecules with less neighbors at the surface will have an excess energy which results in a surface tension . the order of magnitude of this surface tension can be estimated as shown here on page 15-17 that said , this means that surface tension is the macroscopic effect of molecular forces . because of the immense difference in scale at which these molecular forces and the inertial forces ( your acceleration ) work , the surface tension is not going to change at all .
you are asking the following question : suppose a particle is travelling to the left . if you have a magical reverser , which reverses it is momentum , you can make it go back to where it started . suppose you measure the position of the particle along the way , and then apply the reverser . then the particle will not necessarily go back to where is started . you are asking why this is , why there is an unavoidable disturbance to the state as a result of measurement . first , it must be said that a measurement does not necessarily disturb the particle . if you have a particle travelling in a wavepacket which is a gaussian times a plane-wave , you can suddenly apply an external co-moving potential which is an appropriately sized harmonic oscillator , and check if the particle is in the n-th energy state for n not zero . if you get a null result , you have confirmed that the particle is in the ground state , and then you can release the potential , and let the particle continue on its motion . if you do all this quickly enough so that there would be no significant wavepacket spreading during this time interval for the free particle , you have confirmed the wavefunction is a moving gaussian , and if you use your reverser , you will reverse the motion . this is a simple non-demolition measurement . the issue with measurement in quantum mechanics is simply that when you make a measurement , different states give different macroscopic outcomes . the remaining state is the one consistent with the macroscopic outcome you observe , and this state behaves differently than the original . although you can choose a measurement which will not affect any one given state , for a general state , a measurement will always kick the state into one of the special directions which give a definite result for the measurement . there is no answer to this question which does not involve learning quantum mechanics , unfortunately , since this effect is central to the description .
the phase space dynamics of the discrete dynamical system is just what you describe--- x ( n+1 ) as a function of x ( n ) . the phase space itself is the range of values of the x ( n ) , whatever space they might live on , while the dynamics is the function that specifies the evolution in one step in time . the connection with mechanical phase space is provided by a poincare section . the poincare section describes a dynamical continuous system by its intersections with a given surface in the full phase space . for a 1d motion , you can consider the half-line x=0 , p> 0 , or in canonical action-angle coordinates $\theta$ fixed , j arbitrary . when you have a separable integrable motion , you take any one of the $\theta$ variables and define a surface by setting it to zero . then the motion will intersect this surface once every period . in mechanical phase space , the phase-space volume is conserved , but this is not so for maps . the condition of transversal intersection means that the map from the poincare surface to itself can get the topological properties of maps on topological properties the properties of maps on spaces are as complicated as you like . the question is then which topological properties are you interested in ? the simplest topological theorems on maps is the brouwer fixed point theorem , which can be restated as follows : link the points x and f ( x ) by a path . if you draw a contractible sphere , and you find that as you go around the boundary , this x-f ( x ) map has a nonzero winding , then there is a fixed point inside this sphere . the winding of a sphere around another sphere is the index of the map--- it is how many times the sphere covers the other sphere in the map . the brouwer theorem is classical . another classical theorem of this sort is sharkovskii 's theorem : there is a linear order on periods of periodic cycles in 1d maps , such that each periodic orbit of length l implies that there is a periodic orbit of length l ' whenever l ' is greater than l . some other results are given by symbolic dynamics , the coarse grained position as a function of time . the notions of the entropy of a dynamical system is related to this . these results are not really topological in character , but they are general , and give qualitative insight , so they are similar . many further results can be found here , http://elib.tu-darmstadt.de/tocs/35981431.pdf
the assumption here is that the water is more incompressible than a balloon , so that the density variations in the water are less than the density variation in a balloon . this is true for a balloon , but false for a submarine made of steel . the compressibility of steel is roughly 80 times less than the compressibility of water , so if the submarine has thick steel walls , it will make a stable depth equilibrium using bouyancy alone . you can maintain a depth with a submarine by adjusting the density very slightly to the one appropriate for water at a given depth . for the balloon , the equilibrium is unstable , but for there to be an equilibrium at all requires that the balloon skin be made of a material significantly denser than water , and the depth where there is balance , the air will be compressed enormously . edit : submarine compressibility ( in response to the comments of zassounotsukushi ) when a submarine is submerged in water , the compressibility is not determined by the bulk compressibility of steel , because the pressure stress has to travel through the much thinner hull to get from one side of the submarine to the other . the actual stress in the hull is increased . to see how much it is increased , roughly , consider a model submarine which is a square-box-cylinder , with walls of thickness w and side-length l . the pressure is a flow of momentum per unit length equal to pl from one side ( times the length ) of the box to the other , through the sides of the wall , and if the thing were perfectly solid , the compressibility would be roughly 80 times less than water . but the momentum must flow from right to left through a region of width 2w instead of the full width l , as it would be in a solid steel cylinder , so the actual momentum current in the side of the box is increased by a factor of l/2w . for a round cylinder , up to a factor of order unity ( it will be between . 5 and 2 ) , the analogous factor is r/w . for a typical submarine , i found a hull width w of 30 cm , while a spatious radius is 6m . the ratio is 1/20 , so the submarine is about 20 times more compressible than bulk steel . but this still makes the submarine 4 times less compressible than water , and makes the equilibrium stable . steel balloon in order to make a hollow steel balloon neutrally bouyant , ignoring the density of air , the ratio of unoccupied volume to occupied volume is as the ratio of the density of steel to air volume is as the ratio of the density of steel to water , about 8 to 1 . since this ratio is much smaller than the bulk modulus ratio of 80 to 1 , a neutrally bouyant steel balloon will acheive a stable equilibrium . this was the model i originally had in mind for a submarine , and for this model , the walls are extremely thick , and the bulk compressibility is only changed by a factor of order 10 at most , not 80 . but this model is nonsense : it is assuming an empty submarine ! the accurate description is above .
this problem with $n$ point charges on a sphere is a famous problem in electrostatics known as the thomson problem . for large $n$ , it is in general an open problem still under active research . references : wikipedia . org mathworld . wolfram . com mathpages . com
the space station could shoot itself , but it is extremely unlikely to happen by accident . assuming your space station is in a circular orbit you can calculate it is position in polar co-ordinates as a function of time $ ( r ( t ) , \theta ( t ) ) $ very easily since $r$ is constant and $\theta = 2\pi t/\tau$ where $\tau$ is the orbital period . when you fire the cannon the shell is in a different orbit , and specifically it is in an elliptical orbit $ ( r' ( t ) , \theta' ( t ) ) $ . for the station to shoot itself the two orbits must intersect , i.e. at some time $t$ you have simultaneously : $$r ( t ) = r' ( t ) $$ $$\theta ( t ) = \theta' ( t ) $$ the problem is that for a generic elliptical orbit the expressions for $r' ( t ) $ and $\theta' ( t ) $ are not at all simple so there is no easy way to solve the above simultaneous equations and work out at what time , if ever , they intersect . there is certainly no obvious reason to suppose they should intersect . you can see that it is possible for the spaceship to shoot itself . if you fire the cannon radially outwards the shell will fall behind the space station . if you fire in the direction of motion the shell will move ahead of the space station . so there must be some angle in between where the shell hits the space station . however this will be the exception rather than the rule . i am aware this is not a great answer since i can not solve the equations of motion and give you a rigorous answer . if anyone else can do this i would be very interested to see the calculation .
the mmf due to a current is determined by the current through the surface bounded by the closed path along which the magnetic field is integrated . a closed path within a cross-section of a conductor with , say , a uniform current density , will have a non-zero mmf associated with it and thus , a non-zero magnetic field exists within the conductor .
your equation says that your " vector " is an an eigenvector of your operator , i.e. , that the x-projection of the spin is certain an equal to 1/2 . as well it says that probabilities to find certain z-projections are equal to 1/2 . this " vector " is not an eigenvector $\begin{pmatrix} 0\\ 1 \end{pmatrix}$ or $\begin{pmatrix} 1\\ 0 \end{pmatrix}$ of the spin z-projection $\hat{s}_z=\frac{\hbar}{2}\sigma_z$ , but is a superposition of them , that is why it is an eigenvector of a non-commuting with $\hat s_z$ operator $\hat{s}_x=\frac{\hbar}{2}\sigma_x$ .
two collegues of mine wrote a monograph faragó , karátson : numerical solution of nonlinear elliptic problems where conditioning and preconditioning are an issue . this may be some help though . . . if you want it more elementary , then larsson , thomee is the best . do you have a special equation in mind ? that would help .
permittivity $\varepsilon$ is what characterizes the amount of polarization $\mathbf{p}$ which occurs when an external electric field $\mathbf{e}$ is applied to a certain dielectric medium . the relation of the three quantities is given by $$\mathbf{p}=\varepsilon\mathbf{e} , $$ where permittivity can also be a ( rank-two ) tensor : this is the case in an anisotropic material . but what does it mean for a medium to be polarized ? it means that there are electric dipoles , that units of both negative and positive charge exist . but this already gives us an answer to the original question : there are no opposite charges in gravitation , there is only one kind , namely mass , which can only be positive . therefore there are no dipoles and no concept of polarizability . thus , there is also no permittivity in gravitation .
in a sense yes , if you are very careful about what you are holding constant . stating that a variable is proportional to another variable implies that all other relevant quantities are being held constant . for example , there is a simple relation $d=vt$ that describes the distance $d$ something travels in a time $t$ when traveling at speed $v$ . one might say that $d$ is proportional to $t$ . however , this relation is only valid if the speed $v$ is constant throughout the interval $t . $ in your example , one could say that mass is proportional to displacement if the $k$ and $a$ are constant , but you will need to do some work to figure out what physical system ( s ) meet such requirements .
to add to carl witthoft 's answer : your proposed device would violate conservation of optical extent aka optical étendue unless it were an active device ( i.e. one needing a work input to " uniformise " a given quantity of light ) . the law that optical extent can only be held constant or increased by a passive optical system is equivalent to the second law of thermodynamics for light , because the optical extent of a light source is its volume in phase space . the optical extent $\sigma$ for the light radiated from a surface $s$ is : $$\sigma = \int_s \int_\omega i ( x ) \cos ( \theta ( x , \omega ) ) \ , {\rm d} \omega\ , {\rm d} s$$ where we integrate the intensity $i$ at each point $x\in s$ over all solid angles $\omega$ taking account of the angle $\theta$ each component of the radiation from point $x$ makes with the surface 's unit normal . then we integrate this quantity over all points on the surface $s$ . so , the $\sigma$ for your output would be nought , whilst it would be large for your input , so no passive imaging device can do what you ask . so , another way of putting carl 's answer would be that the proposed device would have to " forget " the state encoded in the input light 's wavefront direction at each point . thus your proposed device , if at all possible , would needfully be an active device , needing work input of $k_b\ , t\ , \log 2$ joules for each bit of light state forgotten in accordance with the landauer principle form of the second law of thermodynamics . i say more about this in my answer here .
to remain on the merry-go-round , the person must be accelerated towards the center . how is the force applied to the person to provide that acceleration ? the static friction provides a way for the floor of the merry-go-round to force the person along the circular path . if the floor were to suddenly become frictionless and the person was not otherwise attached , the person would continue moving along a line tangent to the merry-go-round .
but why is not sr taught with an imaginary time coordinate as standard ? from " gravitation " , page 51 , via google books . i will type up a paraphrase later .
" describes " is used here like " inscribed " note that there is an $s$ underneath the integral sign as well . this means that its a surface integral . the integral is carried out over a surface , analogous to how a line integral is carried out over a path . $\rm ds$ will be a small area element . in most surface integrals , you can write it as a product of two differentials . for example , on a plane ( parallel to $xy$ plane ) , $\rm ds=\rm dx \rm dy$ . for a sphere , $\rm ds=r\rm d\theta \times r\sin\theta\rm d\varphi $ . question : are you trying to learn optics via wikipedia ( looking at your previous questions ) . wikiepdia is a bad place to learn stuff--its a reference--which means that it assumes you know everything and want to know more :/ . i suggest getting a textbook if you are wikipedia-studying . there also are free online lecture videos and material , like those from mit or khan academy .
why do textbooks never mention this ? because in order to travel at supersonic speeds , human beings must be enclosed in a rigid metal tube of some sort . also , these metal tubes they ride in at those speeds generally tend to be insulated against noise from the outside . as for trying to place some sort of microphone outside said metal tube , the propulsion system would risk drowning out any atmospherically transmitted noise ( i.e. . noise can be transmitted through the body of the structure ) . now , if you run the math , it still does not work quite like hearing it backwards ( see the comment by eudoxos ) . although you would encounter the soundwaves in " reverse " order , the shockwaves around you would disrupt anything around you as to make the notion of noise from them irrelevant .
the earth goes around the sun kind of like a ball on a string goes in a circle when you swing it around . instead of the string holding the earth , the sun 's gravity holds it . as the earth goes around the sun , it also spins . this makes day and night . you can see this with a flashlight on the ball when you spin it with your hand . part 2: since gravity is a little flexible , in a sense , the earth 's orbit around the sun is not a perfect circle . sometimes we are closer to the sun , and sometimes we are farther away . the earth 's spin is not directly lined up with the sun . the earth is tilted . so sometimes , the sun hits our part of the earth more directly . when this happens , we get more heat and that makes summer . that is why in summer the sun is higher in the sky at noon .
if you do not care about the direction of the horizontal acceleration , the answer is yes . when the car is stationary ( user acceleration very small , below some limit you define for the rms of the three axes ) you measure the vector $\vec g$ for the total acceleration - this is " down " . now during motion you find the user acceleration perpendicular to this vector with these steps : normalize $\vec g$ to unit length : $\vec n$ take dot product of unit gravity and user acceleration : $d=\vec n \cdot \vec u$ subtract vertical component from user acceleration : $\vec h = \vec u - d \vec n$ finally take the magnitude of this answer ( square root of sum of squares of components ) for the total horizontal acceleration . to separate out the acceleration into lateral ( from car turning ) and linear ( accelerate/brake ) you would have to do a similar procedure to find the remaining orientation by looking for horizontal acceleration when there is no corresponding rotation - this tells you which way the phone is facing .
photoelastic constant i have typically seen this as part of the optical property list of glass or other optical materials . it predicts the birefringence . it is also called the stress-optic constant ( defined in mueller , the theory of photoelasticity , 1938 ) : $$ b=\frac{n_p - n_n}{p} $$ where $p$ is the pressure , $n_n$ is the index of refraction normal to the direction of pressure ( with a ${\bf{k}}$-vector normal to the pressure direction ) , and $n_p$ is the index of refraction parallel to the pressure direction . this equation is for a non-crystalline material , you get two constants and two equations for a uniaxial crystal for example . photoelastic coefficient i found a few papers where this was used to mean photoelastic constant . i also found a definition for photoelastic coefficient in properties of group-iv , iii-v and ii-vi semiconductors by adachi : $$ \alpha_{pe} = \frac{\delta\epsilon_{ij}}{x} $$ for a uniaxial crystal where "$\delta\epsilon_{ij}$ is the change in the dielectric constant parallel and perpendicular to the direction of stress $x$ . " in this definition they are using the dielectric constant ( aka the permittivity ) instead of the index of refraction . this means that the units will be different between the two definitions , but they are basically measuring the same thing in the optical regime ( where $\mu \approx 1$ , $\mu$ is the permeability ) . a lot of materials ( other than glasses ) papers seem to use this definition . acousto-optic coefficient the only reference that i could find to the above term was in a conference paper for cleo/pacific rim 2001 : sound field measurement through the acousto-optic effect of air by using laser doppler velocimeter by nakamura . this paper is so terse that i cannot figure out what he is actually calling the acousto-optic coefficient . i have never heard this term before . i found another definition from a thesis from moscow state university here : morozov thesis $$ \gamma = \frac{\partial n}{\partial p} $$ where $n = n ( p ) $ is the pressure dependent index of refraction and $p$ is the pressure , which is the differential case of the stress-optic coefficient .
the quantum fields in the interaction picture evolve according to the free field equations – the evolution is given just by the quadratic part of the hamiltonian , without the interactions – so green 's functions constructed from these interaction-picture field operators would be those of the free field theory , too . it would not be terribly interesting . we would only " learn " the wick 's theorem and things about the free field theory . the normal green 's ( $n$-point ) functions are supposed to include all the interactions given by feynman 's vertices etc . , so they need to be evaluated from the operators in the normal picture , i.e. the heisenberg picture . the interaction picture is just a " fudged " compromise between the heisenberg picture and the schrödinger picture – a compromise that is useful and convenient but in no way fundamental . it is the heisenberg picture and correlators in it that reduce to classical physics in the $\hbar\to 0$ classical limit .
consider the motion of a magnetic monopole in a completely symmetric maxwell system , where $$ \nabla\cdot {\vec b}~=~4\pi\rho_{mag} , $$ and $$ \nabla\times{\vec e}~=~4\pi{\vec j}_{mag}~-~\frac{\partial{\vec b}}{\partial t} $$ the first equation is then a gauss’ law for magnetic monopole charge , and the second is a magnetic current form of the maxwell-faraday equation . for the occurrence of a magnetic monopole flying through space this will act as a transient current . the last term on the right hand side is a displacement monopole current in this case . the left hand side will by stokes’ law $\int\nabla\times{\vec e}\cdot da~=$ $\int{\vec e}\times d{\vec l}$ , produce an electric current in a loop . so the right hand side could be measured by the torque this magnetic field induces on an ordinary magnetic dipole . the right hand side measured in a solenoid . if the left hand side and the last right hand side term do not equal each other in the standard form of the maxwell equation with ${\vec j}_{mag}~=~0$ , this would be a signal for the detection of a magnetic monopole .
your process will be reversible only if it is a ) quasi-static and b ) non-dissipative . it will be quasi-static if it is carried out infinitely slowly in such a manner that the pressure on either sides of the piston varies only infinitesimally . it will be non-dissipative if the piston is frictionless and there is no viscous heating of the gas as it expands .
euler 's rotational theorem only states , that we can determine a unique axis of the rotation for any given moment . that does not necessarily mean that the axis of rotation 's direction is fixed forever . quite opposite , let 's suppose that we have body with no external force acting upon it . if axis of rotation at some ( starting ) moment does not coincide with one of the three principal axis of moment of inertia , the axis of rotation shall change and you shall have complex rotation of that body ( precession ) . you can see that directly from euler 's equations by putting torque to be zero .
having given it some more thought , there is an unambiguous philosophical difference , with practical implications . the two-slit experiment provides a good example of this . in a classical universe , any particular photon that hits the screen either went through slit a or slit b . even if we did not bother to measure this , one or the other still happened , and we can meaningfully define p ( a ) and p ( b ) . in a quantum universe , if we did not bother to measure which slit a photon went through , then it is not true that it went through one slit or the other . you might say it went through both , though even that is not entirely true ; all we can really say is that it " went though the slits " . ( asking which slit a photon went through in the two-slit experiment is like asking what the photon 's religion is . it simply is not a meaningful question . ) that means that p ( a ) and p ( b ) just do not exist . here 's where one of the practical implications comes in : if you do not understand qm properly [ i am lying a bit here ; i will come back to it ] then you can still calculate a probability that the particle went through slit a and a probability that it went through slit b . and then when you try to apply the usual mathematics to those probabilities , it does not work , and then you start saying that quantum probability does not follow the same rules as classical probability . ( actually what you are really doing is calculating what the probabilities for those events would have been if you had chosen to measure them . since you did not , they are meaningless , and the mathematics does not apply . ) so : the philosophical difference is that when studying quantum systems , unlike classical systems , the probability that something would have happened if you had measured it is not in general meaningful unless you actually did ; the practical implication is that you have to keep track of what you did or did not measure in order to avoid doing an invalid calculation . ( in classical systems most syntactically valid questions are meaningful ; it took me some time to come up with the counter-example given above . in quantum mechanics most questions are not meaningful and you have to know what you are doing to find the ones that are . ) note that keeping track of whether you have measured something or not is not an abstract exercise restricted to cases where you are trying to apply probability theory . it has a direct and concrete impact on the experiment : in the case of the two-slit experiment , if you measure which slit each photon went through , the interference pattern disappears . ( trickier still : if you measure which slit each photon went through , and then properly erase the results of that measurement before looking at the film , the interference pattern comes back again . ) ps : it may be unfair to say that calculating a " would-have " probability means that you do not understand qm properly . it may simply mean that you are consciously choosing to use a different interpretation of it , and prefer to modify or generalize your conception of probability as necessary . v . moretti 's answer goes into some detail about how you might go about doing this . however , while this sort of thing is interesting , it does not appear to me to be of any obvious use . ( it is not clear that it gives any insight into the disappearance and reappearance of the interference pattern as described above , for example . ) addendum : that has become clearer following the discussion in the comments . it seems that it is thought that the alternative formulation may have advantages when dealing with more complicated scenarios ( qft on curved spacetime was mentioned as one example ) . that is entirely plausible , and i certainly do not mean to imply that the work lacks value ; however , it is still not clear to me that it is pedagogically useful as an alternative to the conventional approach when learning basic qm . pps : depending on interpretation , there may be other philosophical differences related to the nature or origin of randomness . bayesian statistics is broad enough , i believe , that these differences are not of any great importance , and even from a frequentist viewpoint i do not think they have any practical implications .
the dimensional prefactor that ends up in your code is $$ \frac{hc^2}{\lambda^5}=\frac{6.626\times10^{-34}\text{ j s}\times ( 3\times10^8\text{ m s}^{-1} ) ^2}{ ( \tilde\lambda \text{ m} ) ^5} , $$ where $\tilde\lambda$ is dimensionless and goes from 0 to 3200×10 -9 . this will come out in terms of a number $p$ , which is what your code calculates , with some units : $$ \frac{hc^2}{\lambda^5} =p\frac{\text{j s}\times \text m^2\text{ s}^{-2}}{\text{m}^5} =p\frac{\text{w}}{\text{m}^3} =p\frac{\text{w}}{\text{m}^2}\frac{1}{\text m}\frac{10^{-9}\ , \text m}{1\ , \text{nm}} =10^{-9} p\:\text w\ , \text m^{-2}\ , \text{nm}^{-1} . $$ note also that you need to distinguish carefully between radiance ( which is the planck 's law quantity you are calculating ) , which is the power flow per unit of solid angle , and irradiance , which is integrated over solid angle , as detailed in carl witthoft 's answer . if you put this together , then , the spectral radiance is $$ b_\lambda ( t ) =10^{-9}\frac{2\times6.626\times10^{-34}\times ( 3\times10^8 ) ^2}{\tilde\lambda^5} \frac{ \text w\ , \text m^{-2}\ , \text{sr}^{-1}\ , \text{nm}^{-1} }{\exp\left ( \frac{6.626\times10^{-34}\times3\times10^8}{\tilde\lambda \times 6000\times 1.38066\times 10^{-23}}\right ) -1} . $$ this peaks at ∼$12\:\text{kw}\ , \text m^{-2}\ , \text{sr}^{-1}\ , \text{nm}^{-1}$ , which is consistent with the graph in wikipedia ; it represents the energy flow from the sun , per unit of solid angle , across a unit area which is right next to the sun 's surface . note that this is not comparable to the graph you give , which plots the solar spectrum as measured on earth . to get to the latter , you need to multiply by the square of the ratio of the solar radius to the earth 's orbital radius , $$ \left ( \frac{r_☉}{a_⊕}\right ) ^2 = \left ( \frac{\phantom{000\ , }696\ , 342\text{ km}}{152\ , 098\ , 232\text{ km}}\right ) ^2 \approx 2.177\times10^{-5} . $$ you then need to account for the fact that the sun emits in all directions . as garyp points out , this is done by means of lambert 's cosine law , which essentially says that after integrating over solid angle you need to put an extra factor of $\pi$ . once you do that , you recover the graph you give : finally , note that $\text{nmm}$ is not an si unit . the matlab function you are basing yourself on has a typo at a crucial place which , in my view , renders it essentially useless , or at least useless without a careful examination of what it is actually calculating . be very careful whenever you see that sort of thing !
in the $s'$ frame , your variables are $x ' = x - t\cdot u \cos\theta $ and $y ' = y - t\cdot u \sin\theta$ . if you do the change of variable , you get that the motion now is described by $$x ' = 0$$ $$y ' = -\frac{g}{2}t^2$$ so in your new frame of reference you have vertical free fall from rest . this is not very helpful in finding out when or where does the projectile hits the ground , but is very relevant if you want to know where will the projectile be after releasing it from a plane moving at constant velocity : right below it all the time . disregarding air resistance , of course . edit the system with a prime is moving with velocity $ ( u \cos\theta , u\sin\theta ) $ , so if you have a velocity in the unprimed system , to convert it to the primed system , you have to substract the velocity of the origin : $$\vec{v'} = \vec{v} - ( u \cos\theta , u\sin\theta ) $$ integrating this , you can get the relation for the position vector : $$\vec{r'} = \vec{r} - ( u \cos\theta , u\sin\theta ) t + \vec{r}_0$$ where $\vec{r}_0$ is the position of the origin of the primed system for $t=0$ . both systems share origin for $t=0$ , so $\vec{r}_0=\vec{0}$ . now replace $\vec{r'}= ( x ' , y' ) $ and $\vec{r}= ( x , y ) $ and you will get the equations above .
there are many common misconceptions of the universe . here 's a brief list : many people think that the big bang was a physical explosion . in fact , the big bang is basically a theory of the beginning of the universe , starting with a creation of matter , energy , and spacetime and spacetime 's expansion . that is it . nothing more . it is commonly believed that stars burn to create energy . burning is combustion , a chemical process that has nothing to do with stars . stars ' crush ' together several lighter nuclei to form new heavier nuclei ( e . g . crushing a deuterium nucleus and a tritium nucleus to form a helium nucleus and a neutron , releasing energy ) , and in the process releases energy , fueling stars , their luminosity , and preventing them from collapsing due to gravitation black holes are popularly thought to be fundamentally different from other gravitational sources . many believe that a black hole , even if it had the same mass as another object , would somehow attract them more strongly and ' eating ' all matter around it . many believe astronomical observations reflect the current . in fact , according to relativity there cannot even exist an objective definition of simultaneity . light takes time to travel from astronomical objects to earth , so by watching the skies we are practically observing the past . most people think that all matter in the universe consists of atoms . in fact , even if you count in plasma ( which does not really consist of atoms ) , they only account for about 4% of the ' mass ' of the universe . the rest are dark matter and dark energy . i think that is about as much as i can think of for now .
use the directions up the ramp and perpendicular to the ramp as the simplest coordinate system . find the component of the vertical force of gravity that acts down the ramp . find the component of the horizontal force f that acts up the ramp . find an expression for the net force up the ramp , and equate to the mass of the cart times the acceleration of the cart up the ramp . solve for f .
you are right , it is wrong to think that in gauge theory " gauge transformations are just a redundancy " . this becomes true only if one abandons locality , ignores all boundary effects , all instanton effects , hence most of what is interesting about gauge theory . of course forming gauge equivalence classes ( say of observables ) is something one wants to do every now and then , but considering only gauge equivalence classes means to kill gauge theory . examples : 1 ) instantons : every gauge bundle on a n-disk is equivalent to the trivial one . yet there are non-trivial gauge bundles on the n-sphere -- the instanton sectors . if you think that only gauge equivalence classes count , then there is only the trivial gauge bundle on one hemisphere , the trivial gauge bundle on the other hemisphere , and you have to glue them trivially on the equator to get a global trivial gauge bundle . instead , what really happens is that gauge transformations are not a redundancy , but are all that makes up the non-triviality of the instanton sector , by the clutching construction . ignoring this means to have non-trivial global structures that are not obtained by gluing local structures , hence means to break the locality principle . 2 ) boundary fields . the way that the wzw model appears on the boundary of chern-simons theory : the gauge transformations of the chern-simons theory on the boundary become the very fields of the wzw model . 3 ) higher codimension defects : wilson loops . similarly , the fields on the wilson loop in chern-simons theory are entirely the gauge transformations of the ambient gauge field , restricted to the loop . see here for review and pointers to the literature . 4 ) generally : locality in gauge theory -- it breaks if one disregards gauge transformaitons , see the pointers here . [ edit : a commenter below points out that this is all fine , but does not seem to address specifically the construction inquied about by the op . in fact it does , here is how : 5 ) gauge fields from local gauging : the traditional physics textbook way of deriving gauge fields from local gauge symmetry is an example of the local relevance of gauge transformations as follows . every fermion bundle is locally gauge equivalent to the trivial such , and so carries the trivial connection , given just by the derivative . but remembering that gauge transformations are a local reality , one observes that these take this trivial connection to one with a non-vanishing vector potential $a$ . while this will still have vanishing field strength , already here something may happen globally : if a bunch of these $a$ are glued by gauge tansformations , we may still have globally a non-trivial instanton sector . given this then we are led to allow general local vector potentials $a$ and find then that by gluing them across patches by gauge transformations , we find the full moduli space of all possible gauge fields . if however , and that is the correct point the op observes , one declares that all local gauge transformations are just redundancies , then that means to replace each local vector potential $a$ by its gauge equivalence class . gluing these across patches never produces all global gauge field configurations ( for instance if the gauge equivalence class was the 0-class , one never finds the global torsion class instanton sectors ) . ] mathematically what is going on here is the statement that gauge fields do not form a moduli space but a " moduli stack " . the mathematical concept of stack is all about what it means to combine locality with the gauge principle . an exposition of this is in our arxiv:1301.2580 . for instance what govers examples 2 ) and 3 ) above , where gauge transformations in higher dimension become genuine fields in lower dimension is essentially an instance of the looping construction on stacks : the moduli stack $\mathbf{b}g$ of $g$-instanton sectors has a single component $$ \pi_0 ( \mathbf{b}g ) \simeq \ast $$ ( hence " there is only one gauge equivalence class" ) but it nevertheless remembers the full nature of gauge transformations $$ \pi_1 ( \mathbf{b}g ) \simeq \pi_o ( \omega \mathbf{b}g ) \simeq g \ , . $$ thinking that " gauge equivalence is a redundancy " means thinking that the moduli stack $\mathbf{b}g$ may just as well be replaced with its 0-truncation $\tau_0 \mathbf{b}g\simeq \ast$ , which means thinking that local $g$-gauge theory is trivial . the same kind of arguments apply to the full moduli stack $\mathbf{b}g_{conn}$ of $g$-gauge fields ( instead of just their instanton sectors ) . then $\pi_0 ( \mathbf{b}g_{conn} ) $ is the sheaf of gauge equivalence classes of $\mathfrak{g}$-valued differential 1-forms . that is more than just the point as before , but still just a faint shadow of what gauge theory is about .
656 beagle is a small ( ~53 km diameter ) asteroid with a 3.15 au semimajor axis . it is luminosity is very faint , with h = 9.92 . nothing seems to be known about its geologic basis , but it certainly does not possess a measurable atmosphere . see http://occsec.wellington.net.nz/planet/2008/updates/080912_656_19331_u.htm for more information .
assuming non-relativistic velocities , the power radiated by a charge accelerating at constant acceleration $a$ is given by the larmor formula : $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c^3} $$ to do the calculation properly is surprisingly complicated , but it is easy show that the effect of the radiation on the electrons fall is negligible . if the electron falls a distance $h$ then the time it takes is given by : $$ h = \frac{1}{2}gt^2 $$ so : $$ t = \sqrt{\frac{2h}{g}} $$ if we assume the electron is accelerating at a constant rate of $g$ , the total energy radiated is just power times time or : $$ e_{rad} = \frac{e^2 g^2}{6\pi \epsilon_0 c^3} \sqrt{\frac{2h}{g}} $$ in your question $h$ is 1000m , so : $$ e_{rad} = 7.83 \times 10^{-51}j $$ the potential energy change is , as you say , just $mgh$: $$ e_{pot} = m_e g h = 8.94 \times 10^{-27} j $$ so the ratio of the radiated energy to the potential energy is about $10^{-24}$ , and therefore the effect of the radiation on the electron 's fall is entirely negligible . response to comment : the power radiated from the electron produces a force that opposes the acceleration due to gravity . assume we can ignore the deviations from accelerating at a constant rate $g$ , then in a small time $dt$ the energy radiated is $pdt$ . the energy is force times distance ( $dx$ ) so to get the force we divide by the distance : $$ f = p\frac{dt}{dx} = \frac{p}{v} = \frac{p}{\sqrt{2gh}} $$ using $v^2 = 2as$ . the acceleration produced by this force is just $f/m_e$ , so the net acceleration on the electron is : $$ a_{net} = g - \frac{p}{m_e \sqrt{2gh}} $$ so the electron does accelerate slightly more slowly than $g$ , but the difference between the acceleration and $g$ is inversely proportional to distance fallen so it gets increasingly negligible the further the electron falls . you have probably spotted that the above equation says the force should be infinite at the moment you release the particle . that is because as you approach the moment of release it is no longer safe to make the approximation that you can ignore the change in the acceleration due to radiation .
the broken gauge symmetry model for the w and z boson masses is correct with scientific standards of certainty , since it is exactly verified by relating the ratio of their masses to the ratio of the coupling constants , and there is no chance for such a precise agreement by accident . the pattern of breaking tells you that there is a charged condensate in the vacuum of the form of the usual higgs , a scalar su ( 2 ) doublet ( "spin 1/2" ) representation with u ( 1 ) charge 1/2 ( the same as the lepton doublet ) . but there is no real reason to think that this must be a fundamental scalar field with these charges . it is good to have as wide a range of models for the higgs sector as possible . it is not possible for this field to just what we have seen so far , just the transverse modes of the w and z , because these modes can not be unitary all by themselves . their dynamics is that of a particular nonlinear sigma model , the limit as the self-coupling $\lambda$ goes to infinity of the standard model higgs , and this limit is inconsistent ( see addendum ) . so you need something to unitarize in the absence of something like the higgs , a good dynamical field theory which reprouces the higgs condensate . so it is just inconceivable that the lhc will find nothing at all . once you get data on the missing component ( s ) of the higgs , it will reveal if the higgs condensate is a composite fermion condensate , as in technicolor , or a scalar condensate as in the standard model and susy variants , or something else entirely ( like a composite scalar made out of technicolor scalars or something even more exotic like an infraparticle of a higher energy banks-zaks theory or whatever ) . the goldstone theorem just does not work when there are gauge fields coupled to the symmetry , this is the whole point of the higgs mechanism . the long-range coulomb interaction coupled to a charged condensate produces no goldstone bosons . the argument is summarized on wikipedia in the page on the higgs mechanism , in the section on superconductivity . the mechanism is often called the " eating " of the goldstone bosons by the gauge bosons . it is no more mysterious than the statement that plasma waves have a finite frequency at infinite wavelength , because of the instantaneous coulomb repulsion ( which is still relativistically instantaneous in dirac gauge ) . addendum : why coupling blow ups to infinity are inconsistent like many field theory arguments , good guidance is provided by the ising model . the ising model is the $\lambda$ goes to infinity version of the scalar with field potential $\lambda ( \phi^2 - 1 ) ^2$ . in the large $\lambda$ limit , when you discretize the action on a lattice , you force the field to be $\pm 1$ . the coupling between neighboring field values reproduces an ising model action of some kind ( perhaps with next-to-nearest neighbor coupling , or whatever , it is the same universality class ) . when you look at this statistical theory at long distances , you reduce to a $\phi^4$ theory with a $\lambda$ which goes down with larger distances . if you back-trace the evolution of $\lambda$ , it gets stronger at shorter distances , and it blows up at the order of magnitude of the lattice scale ( this is completely obvious , because the lattice scale is exactly where $\lambda$ is infinite , so that the ising description is correct . averaging over blocks of spins allows the field to fluctuate away from plus or minus 1 , so it reduces the $\lambda$ . this is also formally well known from the $\beta$ function calculation in $\phi^4$ theory ) . so the bare ising model lattice scale is where the coupling blows up , and we know by construction that there are no shorter distances consistently defined in this model . the lesson learned is that any scalar theory must have some new physics at the scale of its landau pole ( the scale where the coupling blows up ) , otherwise , you will see the lattice , or whatever structure is hiding behind the long-wavelength quantum/statistical field theory . the nonlinear sigma model defined by the classical large $\lambda$ limit of the standard model can be defined at some microscopic scale , but then at long distances , it will flow to the standard model scalar higgs with a weak coupling . if the cutoff is much larger than the tev higgs scale , the coupling is bounded above by this triviality argument and the constraint that the location of the landau pole is higher than the cutoff scale . this gives weinberg 's higgs mass bound . the reason it is a mass bound and not a coupling bound is because the higgs particle mass is determined by the curvature of the higgs potential in the hard direction of the mexican hat ( the soft directions are the goldstone bosons that are eaten by the w an z ) , and the curvature in this hard direction is proportional to $\lambda$ . this is called the unitarity bound , or the triviality bound , depending on who is speaking , and it certainly excludes an infinite coupling at tev scales , as required for a nonlinear sigma model which would keep only the longitudinal models of the w and z , and discard the higgs boson by moving its mass to infinity . note that this argument does not work for abelian higgs mechanism , when the higgsing is of a ( noncompact ) u ( 1 ) gauge theory , because you can take the higgs charge to zero and the higgs self-coupling to infinity and the condensate value to infinity while keeping the mass of the u ( 1 ) photon finite , and the nonlinear sigma model limit ( just a circle ) is the stueckelberg affine higgs mechanism . this limit evades the triviality argument because the circle becomes big at the same time as the coupling becomes big . this does not work in the nonabelian case , because the charges are quantized with a lower bound . this addendum is provided because i did not feel comfortable just saying " weinberg says so . " but if it is too telegraphic , then , well , weinberg says so .
when a particle is deflected by gravity the gravitational field will also be modified by the particle . to form a conservation law for momentum you need to take into account the momentum in the gravitational field as well as the particle . this can be done e.g. using pseudo-tensor methods . this works but remember that momentum is a relative concept . even in newtonian dynamics it depends on the velocity of your reference frame . in general relativity it also depends on the frame but a much wider class of frames is valid . this means that momentum conservation depends on the choice of co-ordinates . locally you can pick an inertial reference frame but over extended regions there is no inertial frame . a momentum in one location cannot be simply added to a momentum vector in another location . nevertheless , momentum conservation laws over extended regions do work correctly in general relativity . for an extended description of the formalism and why it works see my article at http://vixra.org/abs/1305.0034 edit : in the comments below mwt p457 has been cited to support the idea that energy and momentum are only conserved in specific cases . i am adding this to directly refute what has been said there . mwt begin by saying that there is no such thing as energy or momentum for a closed universe because " to weigh something one needs a platform on which to stand to do the weighing " this is pure wheeler rhetoric of the type for which he is greatly admired , but in this case it is simply misleading . weight is a newtonian term with no useful counterpart in general relativity except in the specific case of an isolated system in an asymptotically flat spacetime . for other situations such as the closed cosmology energy and momentum conservation take a different but equally valid form . they go on to say that in a closed universe total energy or momentum or charge is " trivially zero " they justify that it is zero because you can use gauss 's divergence theorem to write the charge , energy or momenta as a boundary integral . for a closed universe the boundary disappears making the result zero . this is of course correct , but they give no justification for calling this answer trivial . energy and momenta are initially defined as a sum of volume integral contributions from each physical field including electromagnetic fields , fermionic fields , gravitational field etc . it is in this sense that we understand that conserved quantities can move and can transform from one form to another but the total remains constant . it is a property of gauge fields that when the dynamical field equations are used the conserved quantity is the divergence of a flux from just the gauge field so that it can be integrated over a volume and be calculated as a boundary surface integral . this gives charge/energy/momenta a holographic nature where they can be considered either as a volume integral over contributions from different fields of a surface integral over the gauge field flux . the important thing to understand is that to go from the volume to the surface form the field equations must be used . this means that the total charge/energy/momenta in a closed universe is zero but that this is not in any sense a trivial result . if you calculate total energy as a volume integral for a configuration of fields that do not satisfy the equations of motion the answer will not necessarily be zero . stating that it is zero is therefore making a non-trivial assertion about the dynamics . this is what conservation laws are all about . mwt go on to explain why it would make no sense to have an energy-momentum 4-vector globally . the invalid assumption they are making is that energy and momenta need to form a 4-vector . a 4-vector is a representation of the poincare group and is the natural form that energy-momentum takes in special relativity where poincare invariance is the global spacetime symmetry . in general relativity the global spacetime symmetry is diffeomorphism invariance so the correct expectation is that all quantities should take the form of a representation of the diffeomorphism group for the manifold . this is what happens . if you demand an energy-momentum 4-vector then of course you will only get an answer locally , and also for an asymptotically flat spacetime where poincare symmetry is valid at spatial infinity , but demanding such a 4-vector is simply the wrong thing to do in general relativity . in the fully general case of any spacetime we can apply noether 's theorem using invariance under diffeomorphism generated by any contravariant transport vector field ( observe that it is invariance of the equations that is required , not invariance of the solutions . some people like to confuse the two ) the result is a conserved current with a linear dependence on the transport vector field . this is the correct form for a representation of the diffeomorphism group . i refer to my cited paper for the mathematical details . this current gives conservation laws for energy , and momenta including generalizations of angular momenta as well as linear momenta depending on the transport vector field chosen . if it transports space-like hypersurfaces in a timelike direction it will give an energy conservation law and if it transports in spacelike directions it gives momenta conservation laws . these energy and momenta do not normally form 4-vectors but they can be integrated to give non-trivially conserved quantities . the global form a conservation law must take is that the total energy and momenta in a volume must change at a rate which is the negative of the flux of the quantity over the boundary , and this is what you get with the currents derived from noether 's theorem . it may be that other people will want to add comments here that dispute the validity of energy conservation in other ways . i refer once again to my new article at http://vixra.org/abs/1305.0034 where i refute all the objections that i have heard . triviality is dealt with in item ( 6 ) and 4-momentum is dealt with in item ( 8 ) . unless someone comes up with a novel objection i will just refer to the numbered objections in this paper in future . remember , there are no authorities in science and any expert may be shown to be wrong either by reasoning or by experiment .
i am not sure why you are using the displacement-time formula if you have the shape of the graph . the distance covered by the particle is given by the area under the graph from point 0 . after t = 2s , the distance the particle would have covered is the area under the trapezium , that is : $$\dfrac12 ( 1+2 ) ( 20 ) = 30$$ the position of the particle becomes $8 m + 30m = 38m$ . maybe you can work out the second part now . explanation : your formula assumes constant acceleration , which is not the case ! for the first 1 s , the acceleration is $0ms^{-2}$ , the next 1s , the acceleration is $-20 ms^{-2}$ ! if you used your formula for $0&lt ; t&lt ; 1$ and then $1&lt ; t&lt ; 2$ , then you would have obtained the desired answer . but it is just making the problem more complicated than it actually is at that point : )
long shot , since lack of context , but here is my attempt . any event can be described with 4 coordinates [x,y,z,t] , where [x,y,z] point in some coordinate system and [t] - synchronized clock at event point . wep is same thing as universality of free fall . the universality of free fall , states that all bodies fall with the same acceleration in a gravitational field , independently of their mass and composition , theory and experiment in gravitational physics , by clifford m . will that means trajectory depends only on its position and velocity . if we need to describe position there is no privileged coordinate system , that will give to us some additional information about event , each coordinate system differ only by some transformation . in same time we need to combine geometrical properties with physical , position and time , this can be done by use of tensor , which combine spatial position of event [x,y,z] and time . this technique also used in analysis of electromagnetic fields .
the conservation of energy means that the potential energy liberated in falling must be present as heat . but there are precisely three ways for the water to lose that heat conduction/convection : by contact with air radiatively : by emitting predominantly infrared light . evaporation : by losing vapor into the air as the water is falling . from experience sweating , it should be apparent that in a fast air-flow environment , evaporative cooling is much more important than the other two effects . but this list is exhaustive--- there is no other place you can put the energy liberated by the fall .
newton 's first law of motion for a point particle states that a particle at rest will stay at rest and a particle in motion will stay in motion unless acted on by an unbalanced force . in other words , if the net force on the particle is zero , then the velocity of the particle will stay constant . newton 's first law of motion for a system of particles states that if the net external force on a system is zero , then the velocity of the center of mass of the system will remain constant . it says nothing about the velocity of each of the particles . so if the center of mass of the rigid body is initially at rest , and there is no net external force , then the center of mass will continue to be at rest . but that does not mean that the individual parts of the rigid body will remain at rest . there is two ways to think about this . one way is to apply newton 's first law to each part of the rigid body : if two forces act on the rigid body , but they act on two different places , then one part of the rigid body will only experience one of the forces , so it can move . the other way to think about this is to use the angular version of newton 's first law : if the net external torque on a rigid body is zero , then the angular velocity of the rigid body is constant . since there is an external torque in our example , there is no requirement that the angular velocity must be constant , so the rigid body can rotate even though it was not rotating before .
it is a direct consequence of equilibrium . it can be proved mathematically that proving the moment is zero at one point of a sytem in equilibrium ensures it is zero everywhere . have a look at the first two pages of this , you can find its proof
when you give the dose of an exposure in sv , you have already taken into account weighting factors related to the organ type being exposed ( some are more sensitive than others ) as well as the radiation type . in other words , the number reported represents " equivalent cancer risk if your entire body had been exposed with . . . " see for example http://en.m.wikipedia.org/wiki/sievert#calculating_protection_dose_quantities so yes - a trans continental plane flight ( new york to la ) on average increases your lifetime cancer risk by more than a chest x-ray . both are absolutely tiny . it is typically considered that 1 sievert carries with it a lifetime risk of cancer of 5% ( this depends of course on the age of exposure . . . all these things are " population risk " not " individual risk " . that means that one flight per week = 2 msv / year for fifty years ( 2500 flights ) will cause one additional cancer in one person in 2000 . and that is assuming linear scaling of radiation risk - there is some evidence that the body has some repair mechanisms that may make the curve nonlinear . so do not get too hung up on the detailed calculations . in radiation protection one uses the principle of alara - as low as reasonably achievable . the person who gets the highest radiation dose in the hospital due to a diagnostic imaging exam is probably the interventional radiologist - it is definitely not the patient getting the chest xray .
to leading order in $\alpha$ , the volume expansion of your container does not depend on its shape , and is equal to $\delta v = 3v\alpha\delta t$ . it is fairly simple to verify this for a cylinder , cube or sphere . also the expansion coefficient for aluminum is going to be quite a bit smaller than glycerol , so you may be intended to simply neglect the expansion of the container .
the expression $$ k_b \frac{\omega}{\bar{\omega}} $$ equals $$ k_b\frac{1}{\bar{\omega}}\frac{d\bar{\omega}}{de} $$ which equals $$ \frac{ds}{de} . $$ in thermodynamics , where $s$ is the clausius entropy , this is equal to $1/t$ where $t$ is the kelvin temperature . in statistical physics , this expression can be taken as a definition of $1/t$ of a system from the microcanonical ensemble $e , a$ .
the technical term for what you are looking for is " fully developed flow " . this means that the velocity does not change in the direction of the flow . i am going to explain for a horizontal pipe here , but it is just as valid for a vertical pipe . when the flow just enters the pipe from a region of significantly larger cross section , the layers of the flow close to the wall are decelerated due to friction with the wall . this retardation creates a shear layer , where the fluid is slower than the rest of the fluid . as the flow " develops " , this shear layer gets bigger until the entire area of the pipe has a velocity gradient . consider $p_1$ , $p_2$ , $p_3$ , and $p_4$ to be four probes in the flow , which can measure the velocity without affecting it . $p_1$ measures a uniform velocity profile entering the pipe . you can see the profile developing at $p_2$ and $p_3$ . the blue area is the area with the unretarded fluid . the grey area is the shear layer , where there is a velocity in the radial direction ( to maintain continuity ) . the green area is the fully developed area . in a horizontal pipe flow , the flow is kept running by a pressure gradient . it becomes stable when the force due to the pressure gradient on the fluid element is equal to the force due to the shear forces on it . at this condition ( in the green zone ) , because there is no net force on the fluid element , its velocity can not change with time . $$\therefore \frac{\partial \vec{v}}{\partial t} = 0$$ additionally , because the element moves along the pipe ( i am going to call this the $z$ direction ) , and its velocity does not change with time , its velocity stays the same with $z$ . $$\therefore \frac{\partial \vec{v}}{\partial z} = 0$$ when the pipe is vertical , gravity replaces ( or supplements ) the pressure gradient as the driving force . the shear force still balances this out in fully developed flow . when we say ignore the " entrance effects " , we mean ignore the blue and grey zones , and consider the flow in the green zone only .
take for example $\mathcal{n}=2$ supersymmetry . the algebra , including a central charge $z$ , is given by $$\{q_\alpha^a , q^\dagger_{\dot{\alpha}b}\}=2\sigma^\mu_{\alpha\dot{\alpha}}p_\mu \delta^a_b$$ $$\{q_\alpha^a , q_\beta^b\}=2^{3/2}\epsilon_{\alpha\beta}\epsilon^{ab}z$$ $$\{q^\dagger_{\dot{\alpha} a} , q^\dagger_{\dot{\beta} b}\}=2^{3/2}\epsilon_{\dot{\alpha}\dot{\beta}}\epsilon_{ab}z . $$ we can now define $$a_\alpha=\frac12\left ( q_\alpha^1+\epsilon_{\alpha\beta}\left ( q_\beta^2\right ) ^\dagger\right ) $$ and $$b_\alpha=\frac12\left ( q_\alpha^1-\epsilon_{\alpha\beta}\left ( q_\beta^2\right ) ^\dagger\right ) , $$ which reduces the algebra to $$\{a_\alpha , a_\beta^\dagger\}=\delta_{\alpha\beta} ( m+\sqrt{2}z ) $$ and $$\{b_\alpha , b_\beta^\dagger\}=\delta_{\alpha\beta} ( m-\sqrt{2}z ) . $$ a bps-state satisfies $m=\sqrt{2}z$ , hence the second part of the algebra reduces to $$\{b_\alpha , b_\beta^\dagger\}=0 . $$ this tells us that the operators in the second half of the algebra , which now vanishes , only generate states of zero norm . this is how you should understand the statement you were asking about .
you will sometimes see the expression for the roche limit written as a ratio of densities , and sometimes as a ratio of masses . the two are obviously interchangable because the mass is equal to the density times the volume ( $m = \rho\tfrac{4}{3}\pi r^3$ ) . in the wikipedia article you link the roche limit is written as : $$ d = r \left ( 2 \frac{\rho_m}{\rho_m} \right ) ^{1/3} $$ but $r$ is the radius of the primary and $\rho_m$ is the density of the primary , so you can rewrite the equation as : $$\begin{align} d and = \left ( 2 \frac{\rho_m r^3}{\rho_m} \right ) ^{1/3} \\ and = \left ( 2 \frac{\frac{3m}{4\pi}}{\rho_m} \right ) ^{1/3} \end{align}$$ where $m$ is the mass of the primary . this allows you to calculate the roche limit for a black hole even though the density is undefined . actually this illustrates a very important point about black holes . as long as you stay outside the event horizon the gravity due to a black hole is exactly the same as any other spherical object with the same mass ( and a radius smaller than your distance from it ) . as long as the primary body is much larger than the secondary ( so the perturbation of the primary by the secondary can be ignored ) you would not expect the size or density of the primary to appear in the expression for the roche limit - only the mass matters .
assuming your pipette has a volume of 15ml , the error in measuring the volume of your sample is 0.02 in 15 i.e. 0.133% . the weight of 15cc of ethanol is 0.789 $\times$ 15 = 11.835g . the error in measuring the weight is 0.002 in 11.835 i.e. 0.025% . hedge physicists like me would immediately note that the error in the weight is a lot lower than the volume , so we can ignore it and just take the error in the density to be 0.13% . however , for the sake of the exercise let 's do this thoroughly . the density is given by : $$ \rho = \frac{m}{v} $$ so the percentage error in the density , $\sigma_\rho$ is given by : $$ \sigma_\rho = \sqrt{\sigma_m^2 + \sigma_v^2} $$ where $\sigma_m$ and $\sigma_v$ are the percentage errors in the weight and volume respectively . this gives : $$\sigma_\rho = 0.136\%$$ so the absolute error in the density of 0.789 would be 0.136% of 0.789 or 0.0011 . there is one last step to do : to distinguish the two fluids we need to measure their densities ( with an error of 0.0011 ) then subtract the measurements . because we are subtracting two measured densities , each with an error of 0.0011 , the error in the result is given by an expression similar to the one above : $$ \sigma_{diff} = \sqrt{0.0011^2 + 0.0011^2} = 0.0016 $$ note that this time we are using absolute errors not percentage errors . when you are multiplying or dividing you combine the percentage errors and when you are adding and subtracting you combine the absoluite errors . anyhow , the difference in the densities is 0.789 - 0.785 = 0.004 , so our result would be 0.004 plus or minus 0.0011 . assuming the errors quoted are the 1$\sigma$ errors , the error in the result is 4$\sigma$ , so we had be 98% confident we could tell the difference .
no it is not . this is a mysterious thing in quantum field theory on curved space , as first noted by ' t hooft . if you assume there is a certain amount of entropy in the quantum fields surrounding the black hole , due to their thermal nature , you might estimate that there is a local contribution to the entropy from each approximate mode at the correct local hawking temperature of the black hole . this entropy is divergent in quantum fields in curved space , because the time dilation factor makes it that at a fixed energy , the number of modes diverges as you approach the horizon . this is one of the paradoxes that led t'hooft to the holographic principle . within ads/cft models , it is easy to give an answer-- the entropy of a black hole is the entropy of it is cft description . this includes systems like stacked branes , in which case , the entropy of the black hole is the number of vacuum states . this is strominger and vafa 's famous calculation of 1995-96 . this entropy coincides with the extremal horizon area ( although in this case , the black hole is extremal , so the temperature is zero ) . within string theory , this mystery is essentially resolved . the entropy is the entropy of the microscopic constituents of the black hole . it is not resolvable in curved-space qft because of the ' t hooft divergence , and it is not well resolved in an agreed upon manner in any other approach ( this means loops ) .
there is much more matter in the interstellar medium than in the visible stars . our best estimate of the total matter/energy content of the universe is shown in this image from nasa : now the dark energy is not really matter - it acts like an anti-gravitational vacuum energy which is responsible for the accelerating expansion of the universe . the dark matter is probably some kind of matter that does not have any strong nuclear or electromagnetic interactions with our ordinary ( baryonic ) matter ( atoms ) . it may possibly only have gravitational interactions , but all the current searches for dark matter assume that it also has a weak nuclear interaction with ordinary matter . so that leaves $4.6\%$ of the universe that is made from atoms . it is believed that only about $\frac{1}{2}\%$ of the mass of the universe is in the form of the atoms that are inside stars ( and planets ) . so for every atom in a star , there are 9 atoms in the interstellar medium . some of the atoms in the interstellar medium would be gas clouds inside of galaxies and some of it would be the diffuse interstellar gas between galaxies and even clusters of galaxies . the amount of mass/energy in black holes is a more difficult question ( and about which i know much less ) . some of the atoms have already collapsed into black holes such as the super-massive black holes in the center of most galaxies and the black holes that may result when stars go supernova . the super-massive black holes in the centers of galaxies are still only a very small fraction of the total mass of the galaxy so all of these " known " black holes are probably only a small fraction of the $\frac{1}{2}\%$ of the atoms that are in the form of stars . however , there could be some primordial black holes left over from the big bang . these primordial black holes could constitute part of the dark matter of the universe . most standard cosmological theories do not predict any ( or many ) primordial black holes and there have been some searches for them with gravitational lensing . in this area , i know much less , but i believe these primordial black holes cannot be a big fraction of the dark matter , but i do not know what the upper limit is precisely .
a good source is likely to be nasa - have a look through the basics of space flight for some starting pointers on what is required and how that work is done at nasa for each mission .
diamond dust ( or dust of any other material ) will not conduct heat anywhere close to as well as the solid material . at a molecular level the dust is not in very good contact with other grains of dust . there is plenty of separation and air in between the particles that will retard heat conductivity . if you were to compress the dust so significantly that it did conduct as well , i am certain the pressure would be great enough to cause the dust to bond with other dust into a larger solid . for more a more technical treatment of the thermal conductivity of powder beds , see this paper .
assuming the first was also tap water at the same temp and the pot was room temperature , then all that can be said given your question is " less than 20 minutes " . it depends on the thermal capacity of the pot . put another way , the first time you boil the water you have to do two things : heat the pot to boiling temp heat the water to boiling temp the second time all you have to do is : heat the water to boiling temp what really happens when you put the tap water in the hot pot for the second time is that thermal energy from the pot flows into the cooler water and warms it up . this lowers the temperature of the pot and raises the temp of the water until they are roughly equal temperatures ( thermal equilibrium ) . after this balancing the whole system starts out warmer than the first system did and less energy must be put into the system to heat it to the same boiling point . if the first one took 20 minutes then the second one will take less time . the actual amount of time saved depends on how much heat energy was stored in the pot and that depends on the size of the pot , what it is made out of , etc .
you may have encountered it in a different context , but i recognize it from the topic of singularities of correlation functions in quantum field theory . in massless field theories such correlation fucntions becomes singular for points which are lightlike separated , and the structure of such singularities is determined by good physical principles such as locality and unitarity . then again , i may be completely off the mark . in any event , except for the linguistic similarity , i do not think it has to do with null singularities in spacetime .
yes , you can make a unitary asymptotic s-matrix ( so asymptotic measurements ) when the intermediate states do not evolve in a unitary way . this is what ghost fields do--- the intermediate states in ghost-descriptions include negative probability objects , but when you make asymptotic measurements you do not see the ghosts , you only see the positive probability objects . in cases where you have a ghost description , there are often no-ghost formulations , like light-cone or axial gauges . in these formulations , the hamiltonian is well defined , so that you can ask about measurements on the intermediate states and get well defined answers . these formulations have a reduced symmetry compared to the ghost formulation , but they are manifestly unitary . in ghost formulations , you assume that every measurement is made on asymptotic states which have no ghosts . even if it is not true that every measurement is of an s-matrix quantity , the existence of the unitary formulation guarantees that anything you build out of asymptotic states will only end up measuring a quantity which has a reasonable positive probability interpretation .
acuriousmind has it right . the properties of a scalar field would not in principle be different from any other scalar field theory . it depends on the lagrangian for the theory of course , but the form of the lagrangian would still be constrained by the same restrictions on other theories ( general covariance , renormalizability , phase invariance of some sort , etc . ) . the interactions would arise from a term like $\mathcal{l}_{int}\sim \lambda\phi ( x ) f ( g_{\mu\nu} ) $ where $\phi ( x ) $ is your scalar field and $f$ is some function of the tensor field in question . in the case of brans-dicke theory , this term is $\phi r$ with $r$ the ricci scalar for the metric tensor . the coupling constant would naturally determine the strength of the interaction .
in $y$ direction you have accelerated movement with constant acceleration , thus $$v_y = v_{y0} - g t$$ and after putting initial conditions $$|v_y| = g t$$ i have no idea whatsoever what did you want to do with your calculation .
indeed , your question has nothing to do with the distinction between 1pi and wilsonian . the answer is that the terms which contain nontrivial dependence on $d^2\phi$ are to be dropped if the breaking of supersymmetry is small compared to the natural ( "supersymmetric" ) mass scale in the problem . you can see this by noting that the effective potential has to be of the form $f^2 f ( f/m^2 ) $ where $f$ is the susy breaking scale and $m$ is some supersymmetric scale ( which can the vev of some modulus , too ) . another way to see this is that terms with more powers of $d^2\phi$ have a higher engineering dimension and thus have to be divided by some susy scale , so their effect disappears as f/m^2-> 0 . in some physical scenarios these corrections could be important , but since in dynamical models having rigorous control over the physics usually entails having susy-breaking as a small effect , in most of the literature these terms are dropped .
after some amount of on and off thinking here 's what i have come up with . please pardon the coarse picture . the interpretation of the dispersion as energy is applicable to non-interacting particle . in general , for interacting particles , $e ( \vec{k} ) $ cannot be interpreted as energy ( of ? ) . however , frequency $\omega$ is always proportional to energy of the system . one could see it in the following way : the schrödinger’s equation , $$i \frac{\partial}{\partial t} \psi = \hat{h} \psi , $$ on fourier transforming is given by , $$ \omega \psi = \hat{h} \psi . $$ therefore , the set of ( discrete ) frequency $\omega$ is the set of eigenvalues of the hamiltonian operator $\hat{h}$ . so the conjugate variable to time $t$ should always correspond to energy of the system . in non-interacting case $\omega \propto e ( \vec{k} ) $ . but , generically in the presence of interactions it should not be the case . comments and corrections are very welcome .
apart from the spatial translations corresponding to the momentum operator , the other symmetries ( that i can think of ) that are relevant in particle physics i.e. things like spatial rotations phase transformations flavour transformations colour transformations are represented by the action of compact lie groups . the irreducible unitary hilbert space representations of compact lie groups are finite dimensional and this is reflected in the discrete labelling .
water does not form a liquid at very low pressure . here is the this phase diagram : ( image from http://www.lsbu.ac.uk/water/phase.html and appears to be under the creative commons non-commercial no-derivatives ) the lowest pressure at which water is liquid is that of the triple point : 273.15 k and 611.73 pa . that pressure is about 0.006 atmospheres . wikipedia puts the surface pressure on mars at around 636 pa , from which we conclude that there is just barely room for water to be a liquid in a very narrow temperature band above the earthly freezing temperature ( and even this relies on getting the full pressure quoted above which will not be true except at the lowest elevations ) . no asteroid has sufficient atmosphere to maintain liquid water . if you have ice and heat it , it will sublime ( convert directly to vapor ) .
the biggest thing about this supernova is how close it is . a mere 21 million light years away ( as opposed to being a billion light years away ) . the folks at john hopkins think that studying a ype ia supernova is valuable for several reasons . sne ia are also very bright compared to other standard candles , which means they can be seen at high redshifts and so are important to cosmology . this is due to the following : the expansion of the universe is inferred from the observation of a correlation between recession velocity and distance -- the farther away an object is , the faster it is moving away from us . velocity relates to redshift , and so the ability to determine distances out to high redshifts allows us to measure the rate of expansion . this rate is given by hubble 's constant in the local universe where the expansion is a linear relationship : v=h_0*d . however , this relation can in general be much more complex , as it depends on the densities of the various components of the universe . for example , matter tends to slow down the expansion , and if the universe is curved that will also affect the expansion rate . we have observed that the expansion is accelerating , and since we do not know of anything that can cause this acceleration , we call it dark energy . they conclude with type ia supernovae are not only an odd astrophysical phenomenon , they are also an important tool for studying cosmology . the jhu-led mission adept hopes to study the nature of the dark energy using both a bao study and high redshift sne ia observations . in order for this to happen , we need to determine whether adept will be able to delineate between different cosmological models , and to do this realistically requires complex simulations , of which the list in section 4 is only a basic outline . the true conclusion will not be until nasa decides adept 's fate at the end of 2008 , so stay tuned . now that is from 2008 , so this is a wonderful opportunity to get an " up close " look at what a type ia will tell them . caltech also talks about using a type ia as a standard candle , as well as getting a better understanding of dark energy and the hubble constant .
it depends on the hamiltonian , i.e. , the interactions the beam is subject to after the first measurement . if the beam is allowed to propagate freely , then a pure state , say $|0\rangle$ , will remain in that state always because the hamiltonian only contains a momentum operator that will not affect the spin component of the state vector . if on the other hand , after the first measurement , there are other interactions like a magnetic field , then it can lead to a non-trivial schrodinger evolution of the pure state ( spin part ) which will change the outcome at the second measurement .
i think there may be a better forum to ask this question in and it will likely be closed , but information theory is important to many branches of physics in , so here 's a quick answer . the bandwidth of a channel is simply the number of symbols you can send through it per unit time . by symbol , i mean here a single , real number , and this meaning arises through the shannon sampling theorem . see the wikipedia page for this theorem , and go through the proof so you will understand exactly what i mean . now , just one lone noiseless real number can in theory encode as much information as you like . there are $\aleph_0$ digits in a real number ! write out the whole of wikipedia as 0s and 1s and call it a binary fraction between 0 and unity and the whole of wikipedia is still a finite precision , rational binary number ! so you can see in theory that you can send heaps of data over channels that can send only a low number of symbols each second . this theoretical ideal is , of course , limited by noise . it effectively " coarse grains " the real numbers . if i have noise with an amplitude of 0.1units , and can send symbols with an amplitude of up to 1 units , then i roughly have 10 amplitude levels i can encode data on . otherwise put , i can tell apart ten levels . so i can encode $\log_2 10$ bits per symbol in this example . if my noise amplitude is 0.01 units , i can tell apart roughly 100 different levels per symbol . so i can encode $\log_2 100$ bits per symbol in this example . i think you should now be able to see what is going on : the number of bits you can send per unit time is roughly $$b \log_2 s/n$$ the actual shannon-hartley theorem is a little more complicated , but that is the idea . edit : for interest : 64-qam modulation is commonly used for digital communications . this is essentially where the " symbols " are one of 64 points on a regularly spaced grid in the argand plane representing the amplitude and phase of the signal . so this scheme has a spectral efficiency of six bits ( $\log_2 64$ ) per hertz ( i.e. . symbol per second ) . the ultimate spectral efficiency of a typical optical fibre link is of the order of 20 to 25 bits per hertz : see my answer here .
the direct calculation of the derivatives is not that hard . but one can also quickly see the values of the riemann tensor for a sphere – and similar simple shapes – by using the definition of the riemann tensor via the parallel transport of vectors . $$\delta v^\alpha = r_{\alpha\beta\gamma\delta}v^\beta d\sigma^{\gamma\delta}$$ around a point of the sphere $s^d$ , the transport around an area given by $d\sigma^{\gamma\delta}$ for fixed values of the indices ( locally orthonormal basis ) allows you to see that all of this is happening on an $s^2$ only . the other dimensions are unaffected . that is why you get $r_{\alpha\beta\gamma\delta}$ equal to $ ( 1/a^2 ) $ times $g_{\alpha\gamma}g_{\beta\delta}-g_{\alpha\delta}g_{\beta\gamma}$ . effectively , the antisymmetrized pair of indices $\alpha\beta$ has to be the same as the pair $\gamma\delta$ . i did not assume anything special about the point ; all points on a sphere are equally good by a symmetry . so the ansatz for the riemann tensor has to hold everywhere . note that it is multiplied by $1/a^2$ , the inverse squared radius of the sphere . many of your formulae omit it ; moreover , you are using a confusing symbol $r$ for the radius which looks like the ricci scalar – a different thing . in $d=2$ , the riemann tensor only has one independent component and the formula for the riemann tensor in terms of the metric tensor above actually holds for any surface if $1/a^2$ is replaced by $r/2$ . note that the two-sphere has ( ricci scalar ) $r=2/a^2$ . also , the ricci tensor is $r_{ij}=rg_{ij}/2$ in $d=2$ so that the vacuum einstein equations are obeyed identically . for $d=3$ , the riemann tensor has 3 independent components , just like the ricci tensor , so the riemann tensor may be written in terms of the ricci tensor . that is not true for higher dimensions , either .
your intuition about the charge repulsion and strong force acting on protons more is less important that you think . the strong nuclear force is a few orders of magnitude greater than electromagnetism so coulomb repulsion just does not contribute much . what matters most is the nuclear binding energy to separate a proton from the nucleus . if the resulting system is below the proton separation energy it is possible for the proton to tunnel out . see proton emission and proton drip line for more information about this . it does happen but remember neutron emission is also rare . $\beta^+$ and $\beta^-$ and alpha emission are much more common .
i call it inertial force , but to be exact it should be called the rate of momentum change . $$ \vec{f} = \frac{{\rm d}}{{\rm d}t} ( m \vec{v} ) = m \vec{a} $$
choosing uniformly distributed points on the two dimensional bloch sphere : $0 \leqslant \phi &lt ; 2\pi , 0 \leqslant \theta \leqslant \pi$ , we can construct a random vector $ |\psi_0\rangle = cos\frac{\theta}{2} |0\rangle + e^{i\phi} sin\frac{\theta}{2} |1\rangle $ in order to obtain a uniform distribution over the sphere 's surface , $\phi$ should be uniformly distributed in the interval $ [ 0 , 2\pi ) $ and $cos ( \theta ) $ should be uniformly distributed in $ [ -1 , 1 ] ) $ . to see that , please notice that in terms of the height of the unit sphere $z = cos\theta$ , the surface element is uniform : $ ds = sin ( \theta ) d\theta d\phi = -dz d\phi$ this is called archimedes ' spherical sampling theorem as it was known already to archimedes . the required expectation : $ p = \mathrm{tr} ( |\psi_0\rangle\langle\psi_0|\phi ) = cos^2\frac{\theta}{2} = \frac{1+z}{2}$ since $p$ is linear in $z$ , it is uniformly distributed in $ [ 0 , 1 ] ) $ .
the wave function is a mathematical solution of a specific quantum mechanical equation different for different potentials and boundary conditions . this formulation is validated by a plethora of experimental data , not only the two slit experiment . since we are familiar with sound and light interference we call the mathematically similar patterns of the two slit experiment with electrons " interference " patterns . this terminology just reflects that the probabilities in space of finding an electron on the screen are affected by the boundary condition of the two slits to create an interference pattern . the wavefunction given by the solution for " two slits and incoming electron " has the patterns .
i am in part trying to understand this myself . the berry phase is computed from differential forms , such as the one-forms $\omega$ constructed from states $$ \omega~=~\langle\psi|d\psi\rangle $$ and with the covariant differential $d~=~d~+~\omega$ the two-forms $$ \omega~=~d\omega~=~d\omega~+~\omega\wedge\omega $$ the tensor components of the 2-form $f$ are elements of a self-adjoint principal bundle $p$ . the determinant of these elements $$ det\big|1~+~\frac{ixf}{2\pi}\big|~=~\sum_nc_jx^n $$ which is a characteristic polynomial which represents the chern class . each $c_n ( p ) $ is an element of $h^{2n} ( m ) $ . so the curvature form for the berry phase , or the fubini-study metric $\omega=~dz\wedge d{\bar z}/ ( 1~+~|z|^2 ) ^2$ is evaluated $\int\omega~=~2\pi i$ and gives $c_1~=~1$ so there is a nontrivial cocycle on the “2-level . for this projective geometry there are alternating betti numbers $1 , ~0$ for even and odd . if you had some product of states $\prod_n |\psi_n\rangle$ , say in an entangled state etc , you could apply the differential $d$ up to $n$ times and form and $n$-form . for instance the product $|\psi_1 , ~\psi_2\rangle$ $=~|\psi_1\rangle|\psi_2\rangle$ defines the one-form $$ \omega~=~d|\psi_1 , ~\psi_2\rangle~=~d|\psi_1\rangle|\psi_2\rangle~+~|\psi_1\rangle d|\psi_2\rangle $$ and one could then build up a system of differential forms on various chains . the analogue of the projective geometry for this is a $g_2 ( v ) $ grassmannian and this continues up for n-product spaces .
laser light is spatially and temporally coherent . the stimulated emission is mainly responsible for the temporal coherence . so the answer is yes , you can create an electromagnetic beam that is spatially but not temporally coherent by placing a pinhole close to the source , and then another pinhole in the far field of the first pinhole . this beam will not spread out very much . ( but also remember that laser light does spread out . ) note that for rf frequencies , a " pinhole " is probably several meters in diameter . the far field distance is given by this inequality : $l \gg a^2/\lambda$ , where l is the distance , a is the diameter of the hole , and $\lambda$ is the wavelength . however , creating a rf pencil beam is probably not practical . the term " pencil beam " mentioned in the wikipedia article is explained as being diffraction-limited . the size of a diffraction-limited beam gets larger with , i believe , the square root of the wavelength . it would be more like a gas-pipeline beam than a pencil beam .
ball lightning could definitely be some atmospheric pressure plasma phenomenon . you can make a pretty impressive ball plasma by discharging a kilojoule-scale capacitor bank into a bucket of salt water . check out free-floating atmospheric pressure ball plasma . in most of those pictures they are using a copper sulfate solution , but that is not essential ( sodium chloride also works ) . these ones only last a ( significant ) fraction of a second , but i am sure if you made a larger one ( e . g . by a lightning strike ) , they could last longer . btw , this was the subject of a killer science fair project : http://www.youtube.com/watch?v=se6sbanskoc
the fact is , in the context of ideal circuit theory , the inductor voltages are equal in the circuits below : in the lower circuit , the inductor current has a constant component , i.e. , the lower circuit is equivalent to your $c \ne 0$ case . but , there is nothing remarkable or surprising about this . is there something else to your question that i am missing ? [ i ] am asking that why in in elementary physics text books the author directly writes c=0 for ideal inductor without mentioning the initial conditions ? the initial conditions are mentioned when the context is transient analysis . for example , from wikipedia : however , when the context is ac ( sinusoidal ) circuit analysis , the underlying assumptions are ( at least ) : ( 1 ) all sources are sinusoidal and of the same frequency ( 2 ) the circuit is in sinusoidal steady state , i.e. , all transients have decayed . when these conditions hold , can we use voltage and current phasors and the notion of impedance to analyze circuits .
no , the lhc does not violate the uncertainty principle . the principle only affects position and momentum ( not actually velocity ) in the same direction , but in a particle accelerator , you do not have to constrain both position and momentum in any one direction . it is only important to constrain the position in the transverse direction ( perpendicular to the beam ) and the momentum in the longitudinal direction ( along the beam ) .
i claim that the partial and total time derivatives of the hamiltonian are equal whenever the hamiltonian is evaluated on a solution to hamilton 's equations of motion . for conceptual simplicity , let 's restrict the discussion to systems with a two-dimensional phase space $\mathcal p$ with generalized coordinates $ ( q , p ) $ . it is important to note what the total time derivative and partial time derivative mean in this context . in particular , recall that the hamiltonian is a function that maps a pair consisting of a point $ ( q , p ) $ in phase space and a point $t$ in time , to a real number $h ( q , p , t ) $ . when we say that we are taking the partial time derivative of $h$ , we mean that we are taking a derivative with respect to its last argument ( in my notation ) . when we say that we are taking a total time derivative , we have in mind evaluating the phase space arguments of the hamiltonian on a parameterized path $ ( q ( t ) , p ( t ) ) $ in phase space , then then taking the derivative with respect to $t$ of the resulting expression , like this ; \begin{align} \frac{d}{dt}\big ( h ( q ( t ) , p ( t ) , t ) \big ) \end{align} if we use the chain rule , we find that this total time derivative can be related to the partial time derivative of $h$ as follows : \begin{align} \frac{d}{dt}\big ( h ( q ( t ) , p ( t ) , t ) \big ) = \frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \dot q ( t ) + \frac{\partial h}{\partial p} ( q ( t ) , p ( t ) , t ) \dot p ( t ) + \frac{\partial h}{\partial t} ( q ( t ) , p ( t ) , t ) \end{align} i have deliberately not abbreviated notation here to make explicit what exactly is going on so that there is no confusion . for example , the expression \begin{align} \frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \end{align} means that we take the partial derivative of $h$ with respect to its first argument ( which i labeled $q$ ) , then then i evaluate the resulting function on $ ( q ( t ) , p ( t ) , t ) $ . now the question is , when are the total and partial time derivatives the same ? well , the relationship between them that we derived above shows that this happens if and only if the other stuff in the equation vanishes ; \begin{align} \frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \dot q ( t ) + \frac{\partial h}{\partial p} ( q ( t ) , p ( t ) , t ) \dot p ( t ) =0 \end{align} notice , now , that this equation definitely does not hold for a general path $ ( q ( t ) , p ( t ) ) $ in phase space . i will leave it to you to find a simple counterexample . so , for what paths does this relationship hold ? well , notice that this relationship is satisfied provided the path satisfies hamilton 's equations ; \begin{align} \dot q ( t ) and = \frac{\partial h}{\partial p} ( q ( t ) , p ( t ) , t ) \\ \dot p ( t ) and = -\frac{\partial h}{\partial q} ( q ( t ) , p ( t ) , t ) \end{align} in other words , we have demonstrated the claim i started with .
the phase constant is needed only if you have a specific initial condition , e.g. if i told you where $x$ was at time $t = 0$ , you could solve for $\varphi$ . otherwise you can just choose whatever you want for it : note that it is the same in all functions . choosing some value for $\varphi$ is analogous to you manually setting the time origin to something you like . imagine observing the harmonic motion for a while , and then deciding that $t = 0$ should be the time where the piston is all the way up . or you could decide that $t = 0$ should be the time where the piston is all the way down . or right in the middle . thus , for your problem , you can just set $\varphi = 0$ .
for this post i will assume that the observation of neutrinos faster than light is accurate and not further mention this assumption below . of the particles whose speeds have been accurately measured , the neutrino is unique in that it does not participate in electromagnetic interactions . the graviton also does not participate in electromagnetic interactions and so we might expect its velocity to also exceed that of light . this has immediate impact on quantum gravity and would open up a wide range of new theories . since neutrinos can be used to send a signal , their moving faster than light destroys the relationship between causality and arbitrary reference frames which forms an important part of the special theory of relativity . to get causality to function again , we have to choose one reference frame as the preferred one and then let that reference frame define causality for all others . this variation of the special theory of relativity is sometimes called " lorentzian relativity " or " neo-lorentzian relativity " or " lorentz ether theory " , and has been explored by fringe physicists for most of the last 100 years . the presence of a preferred reference frame in the special theory of relativity implies that one must also be present in general relativity . this makes theories that assume a " background metric " more interesting . of such theories , my favorite is the one found by the cambridge geometric algebra research group . for example , see gravity , gauge theories and geometric algebra , anthony lasenby , chris doran , stephen gull , phil . trans . r . soc . lond . a 356 , 487-582 ( 1998 ) . this theory gives the observable predictions of gr exactly , but avoids wormholes and other topological things as it is built on a flat background metric . rather than tensors , it uses the gamma matrices used in the rest of particle physics . in terms of the damage to relativity as compared to the damage to quantum mechanics , an accepted observation of superluminal neutrinos would be more damaging to relativity . so i would think that the general effect on quantum gravity would be to make it more quantum and less ( traditional ) gravity .
lets suppose that all n magnets are identical . assume that they are quite far from each other so we can replace them with n magnetic dipoles with dipole moment $\overrightarrow{m}$ . also assume that the dipoles are placed along the x axis in such a way that they repel each other and all n moments $\overrightarrow{m}$ are parallel . the first step is to find the interaction force between two dipoles . fortunately , wikipedia has a formula for the force so that we can save a large amount of work . link here i write only its x axis component which we need , to save space : $$f=\frac{6\mu_0 m^2}{4\pi x^4}$$ now , to find " spring rate " between two magnets , it is sufficient to find the differential $df$: $$df=-\frac{6\mu_0 m^2}{\pi x^5}dx$$ so for the small displacement the " spring rate": $$k ( x ) = \frac{6\mu_0 m^2}{\pi x^5} $$ now , since we know $k$ it is easy to find the effective spring rate for the top magnet . let $f$ be the force on the top magnet , other end magnet fixed . then the displacement of the top magnet is $\delta x= ( n-1 ) \frac{f}{k}$ so the effective spring rate : $$k_e ( x ) = \frac{6\mu_0 m^2}{ ( n-1 ) \pi x^5} $$
yes , the comination $j_1 + j_2$ determines the spin of the particle . note however , that this is an addition of angular mementum which may be complicated . furthermore , you can count the degrees of freedom : in $ ( j_1 , j_2 ) $ , each contribute $2j_1 + 1$ states and we construct a tensor product , so $ ( j_1 , j_2 ) $ gives $ ( 2j_1 + 1 ) * ( 2j_2 + 2 ) $ degrees of freedom . for the vector we have $ ( 1/2 , 1/2 ) \mapsto 2 * 2 = 4$ degrees of freedom . if the representation is reducible , i.e. of the form $ ( j_1 , j_2 ) + ( k_1 , k_2 ) $ , then you simply add the d.o.f. you get from each pair . the dirac spinor has $ ( 1/2 , 0 ) + ( 0 , 1/2 ) \mapsto ( 2 ) + ( 2 ) = 4$ degrees of freedom , the field-strength tensor has $ ( 1 , 0 ) + ( 0 , 1 ) \mapsto 3 + 3 = 6$ d.o.f. as a sidenote : the representations $ ( 1 , 0 ) $ do not corresond to vectorlike degrees of freedom , but rather to antisymmetric self-dual tensors . $ ( 0 , 1 ) $ is the antisymmetric anti-self-dual tensor . the vector ( and the only way to get a vector out of this ) is $ ( 1/2 , 1/2 ) $ !
no . johannes kepler published what is now known as his third law of planetary motion in 1619 ( in his treatise harmonices mundi ) , but discovered it already on may 15 , 1618 . he simply related mean distance of a planet from the sun to its mean angular motion , without a word about a mass , i think . he did write on gravity and mass ( not the precise physical term ) in a foreword to his earlier book astronomia nova . thanks to people ( rafael gil brand , roger ceragioli and r . h . van gent ) from h-astro discussion forum i have the following update #1: 1 , the original form of the third law ( formulated for planets ) , freely traslated to english reads approximately : " . . . it is absolutely certain and perfectly correct , that the ratio which exists between the periodic times of any two planets is precisely 3/2 of the ratio of the mean distances , i.e. of the spheres themselves , bearing in mind , however , that the arithmetic mean between both diameters of the elliptic orbit is slightly less than the longer diameter " 2 , although ( as far as i know from my own experience with early observations of double stars by galileo ) it is virtually impossible to prove that an earlier observation/idea did not exist , it seems that the first application of kepler 's third law to the jovian satellite system , is found in newton 's philosophiae naturalis principia mathematica ( 2nd ed . of 1713 ) , lib . iii , prop . 8 , resulting in 1/1033 solar mass . it is possible that riccioli had something about the topic in one of his monumental treatises published around the middle of the 17th century . update #2 riccioli seem to discuss relation between elongation of galilean satellites of jupiter and their orbital periods both in his almagestum novum and astronomia reformata , and cites vendelinus ( godefroy_wendelin ) . the wikipedia entry for him states : " in 1643 he recognized that kepler 's third law applied to the satellites of jupiter . " without further details . update #3 - final answer i repost here the final answer by christopher linton from h-astro : " kepler , in the epitome of copernican astronomy ( 1618-1621 ) , did apply his third law to the jovian satellites ( in art . 553 ) . he got the data from simon mayr 's world of jupiter ( mundus jovialis , 1614 ) . he establishes that $t^2/a^3$ is roughly constant and concludes that the physical mechanism which causes the planets to move as they do is the same as that which causes the jovian satellites to rotate around jupiter . "
this sounds suspiciously like a homework problem , which means we are only allowed to discuss general concepts . your professor would not be best pleased if we just gave you the answer to their problems :- ) the amplitude $y$ is a function of time and distance . since you just want the frequency the $x$ dependance does not matter and you can just look at the amplitude at a fixed value of $x$ . i would choose $x = 0$ as the simplest option . now you have the amplitude as a function of just time . you have to work out how long it takes for the amplitude to go through one cycle . for example suppose you start at $t = 0$ , then as you increase $t$ the value $y ( t ) $ will fall to zero , go negative , start rising again and eventually return to it is original value . that time is the period . once you know the period the frequency is just the reciprocal of the period .
was taught that it has more to do with specific heat than with heat conduction . 1 gr of water requires 1 cal of heat to raise its temperature by 1 °c . by comparison , sand only takes about 0.2 cal to get the same effect . you can find some values of common substances here .
it is conservation rules ( mostly that of energy ) that controls what decays are possible , and what channels are allowed ( the strong interaction respected quark flavor , but the weak interaction does not ) . if the final state has higher energy than the initial state there can be no spontaneous process leading from one to the other . the energy environment of a nucleus is a complicated place because it is affected by the strong nuclear force , electromagnetic forces and by the limits on state occupation imposed by pauli exclusion . there are a lot of question already on the site about when various processes can proceed and related subjects $\alpha$ decay to more than one nuclear state what stabilizes neutrons against beta decay in a neutron star ? how can a proton be converted to a neutron via positron emission and yet gain mass ? why is the ( free ) neutron lifetime so long ? nuclear fission and half life adding many more neutrons to a nucleus decreases stability ? . . . your question about the case where a nucleus has multiple modes have different half-lives is tricky . the measured result is the same half-life for each mode , but we can ask " if we could magically block modes and study the isolated half-life of each mode , what would they be ? " if you perform the ( often approximate ) calculation for that question you generally find different half-lives . the interesting part is the the branching fractions for each decay are related to ratios of the calculated " isolated " half-lives , so you are seeing the effect of the different probabilities when you look at the frequency of different decays .
another technique for this type of question is to assume that the final result is all the water in some specific state , and find the net energy change to achieve this . if it is non-zero , as is likely , you have one uniform system to make any necessary correction . for example , assume that the final situation here is all liquid water at 100° c . this is wrong , as shown by wojciech morawiec , but no matter . to reach this state , we must add heat energy to the kilo of ice:$$heat_{added}=1000\times 334+1000 \times 100\times 4.184=752,400 joules$$ to condense the kilo of steam to liquid water at 100° c , we must remove heat:$$heat_{removed}=1000 \times 2230=2,230,000 joules$$ so , now we have 2 kilograms of boiling hot liquid water , but we need , for zero net heat flow , to stuff back in $ ( 2,230,000-752,000 ) $ or $1,471,000 joules$ . ( see , the guess $was$ wrong ) this is enough to " re-vaporize " $\frac{1,471,000}{2230}=660$ grams of water .
interesting , i was just studying the fourier decomposition of vowels for half an hour yesterday . first , you must distinguish vowels and consonants . words like " bee " ( which is how we spell the letter " b" ) of course are not uniform sounds . they start with a consonant , in this case one created by lips . depending on the way how they are created , consonants are divided to many groups . see a table of consonants here . in general , consonants are types of noise because they do not have a well-defined basic frequency . it means that the fourier series for a consonant is composed pretty much of all sufficiently high frequencies . the color of the noise – whether higher frequencies tend to be more strongly represented than the lower one etc . – determines the type of the consonant . there are consonants with a throat sound added , like l , m , n , which may be fourier decomposed similarly to the vowels , and noise-based consonants such as b , d , g , v , z which are the sound-equipped cousins of p , t , k , f , s , and so on , and so on . the most monochromatic sounds are the vowels . they can be sung so they have a well-defined base frequency . whether one gets u , o , a , e , i – or , in english , oo , aw , ah , eh , ee etc . – depends on how the mouth is opened . this modifies the shape of the resonance cavity and therefore the preferred additional frequencies that are excited by the action of the base frequency coming from the throat . the presence of higher harmonics is essential for the difference between vowels . i recommend you to look at a page about it , for example this one : http://hyperphysics.phy-astr.gsu.edu/hbase/sound/vowel.html the vowel u ( oo in english ) has the lowest representation of the higher harmonics . it is the closest one to the harmonic sound and it is achieved by changing one 's mouth into a passive tube through which the sound penetrates . on the other hand , a ( ah ) and i ( ee ) have a huge , important contribution of higher harmonics and when i say higher , i do not mean the 2nd or 3rd . the 20th harmonic etc . etc . are very important . in fact , it is more accurate to talk about the absolute frequencies . the vowel a ( ah ) has lots of those higher harmonics that are close to 1,000 hz ( cycles per second ) which are already suppressed in u ( oo ) and partly suppressed in o ( aw ) . as you continue to go towards e ( eh ) and i ( ee ) , the contribution from frequencies close to 3,000 hz starts to increase . these frequencies are calculable from the size of the mouth opened in the right way , from the length of the resonant cavity that becomes comparable to the wavelength of the sound waves that become important . in principle , all vowels may be emulated by the fourier expansion using just the base frequency ( pressure goes like $\sin \omega t$ ) plus higher harmonics but the very harmonics including $\sin 20\omega t$ are still very important for the character of the vowel . phonetics is the portion of linguistics that studies how language sounds ; the experts partially learn some physics although fourier series are not their primary tool . still , to understand phonetics , one has to accept various basic things . i found out that native english speakers misunderstand phonetics because they do not really decompose the language into " pure sounds " . your representation of " b " as a sound analogous to " a " may be an extreme example because you may have meant " b " in the sense of " bee " which is clearly composed of two sounds , the consonant " b " and the vowel " ee " . but even when it comes to vowels only , english ( and french and some other languages ) is deliberately obscuring the reality as it pronounces many vowels in a variable way . for example , " my " is pronounced as " mai " where the vowel gradually changes from " ah " at the beginning to " ee " at the end . some native english speakers do not even realize that this " y " in " my " is not a single uniform vowel . there are many other examples , of course .
there are two related but distinct questions : how do you keep a wormhole stable ? how do you make the wormhole in the first place ? courtesy of matt visser we can give one answer to the first question . matt 's example is to make the wormhole cube shaped , and in that case all you need to do is construct a cube from string i.e. the twelve edges of the cube are made from string . however the string would have to have a negative tension , and indeed it would have to have the ridiculously high negative tension of $−1.52 \times 10^{43}$ joules/metre . this is where your exotic matter comes in since the tension in any string made from normal matter would always be positive . the second question is harder . matt 's analysis applies to a time independant wormhole , i.e. one that has existed for an infinite time . constructing your cube of exotic string would warp spacetime in the manner required for a wormhole , but calculating what happens as you tie the strings into a cube is probably impossible at present . response to comment : this is going to be a bit hard to explain , but the space inside the cube does not exist . it is not part of the manifold on which the universe exists . if you travelled towards the cube you would not hit anything - you had just keep going without feeling anything as you past where it is wall is , but now you had be travelling in the other region of spacetime on the other side of the wormhole . re your comment it does not sound like there is a lot of control , the wormhole matt describes is not the same as the sort of wormhole sci-fi writers use to allow interstellar travel . as far as i know there is no theoretical support for the interstellar travel type wormhole . the wormhole matt describes connects two regions of spacetime but makes no statement about the global topology , so the region of spacetime the other side of the wormhole need not be , and almost certainly is not , some distant region of the universe around us . the wormhole does not allow ftl travel to e.g. alpha centauri . it just allows travel ( at up to the speed of light ) to the new region of spacetime on the other side of the wormhole .
i have read that true steam is clear ( transparent ) water vapor . according to this theory , the white " steam " you see is really a small cloud of condensed water vapor droplets , a fine mist in effect . so what you are seeing is not more steam , but more condensation and more mist . the speed with which the steam/vapor/mist rises and disperses may also change .
let me attempt a more " popular science " answer ( ron please be gentle with me ! ) . in gr a geodesic is the path followed by a freely moving object . there is nothing especially complex about this ; if you throw a stone ( in a vacuum to avoid air resistance ) it follows a geodesic . if the universe is simply connected you had expect to be able to get anywhere and back by following geodesics . however in a static black hole , described by the schwartzchild geometry , something a bit odd happens to the geodesics . firstly anything following a geodesic through the event horizon can not go back the way it came , and secondly all geodesics passing through the event horizon end at a single point i.e. the singularity at the centre of the black hole . if you now rotate the black hole , or you add electric charge to it , or both , you can find geodesics that pass through the event horizons ( there are now two of them ! ) , miss the singularity and travel back out of the black hole again . but , and this is where the separate universes idea comes in , you now can not find any geodesics that will take you back to your starting point . so you seem to be back in the normal universe but you are in a region that is disconnected from where you started . does this mean it is a " separate universe " . that is really a matter of terminology , but i would say not . after all you just got there by coasting - you did not pass through any portals of the type so beloved by scifi films . and there is no reason to think that physics is any different where you ended than where you started . if you are interested in pursuing this further i strongly recommend the cosmic frontiers of general relativity by william j . kaufmann . it claims to be a layman 's guide , but few laymen i know could understand it . however if you know sr you should not have any problems with it .
thermal neutrons capture on hydrogen and carbon with reasonable ( i.e. . not large , but significant ) cross-sections ( this is the delayed event detection methods of most organic liquid scintillator anti-neutrino detectors--i . e the one that do not dope their scintillator with gadolinium ) . so though a " cloud"--meaning a localized diffuse gas--of neutrons can develop in the neighborhood of a strong source ( size of the cloud is driven by how far they go as they thermalise ) , their dissipation is driven by their mean capture time , not their half-life . confession : here i am presuming that the mean capture time is significantly shorter than the half-life , but i have not measured it in a " near the laboratory " setting . in organic liquid scintillator the capture time is on order of $200\text{ }\mu\text{s}$ , but air has a lot less hydrogen and carbon in it . note that the neutrons also go into the ground , the building , nearby vehicles and passers-by ( if any ) where they may find things to interact with . at my grad-school we had a 2 curie ( i.e. . huge ) ambe source . the source vault would register unusually high back-grounds on a survey meter for a few minutes after it was returned from the moderator tank to the shielded vessel , so that may be a rough measure of the time scale . it also says something about the strength of the radiation field : a few times the in-the-basement background level . shielding methodology for strong neutron sources generally incorporates a great deal of boron in various layers to help suck up the thermal neutron flux ; not incidentally this means that most of the capture gammas are generated inside the shielding . borated plastics are common as are borated concretes . these days gadolinium is cheep enough that i imagine we will start seeing it used in shielding design . the source vault in grad school was built of borated cinder block---two layers with a meter air-gap between . another not-very-quantitative story that might shed some light on this . i was friends with one of the radiation safety guys at jlab . part of his job was monitoring the radiation level at the fence around the secure area with the accelerators , experimental halls , etc . mostly they just put out general purposes detectors and compared the results with background reading from nearby , but early on they built a more sophisticated detector out there to understand the various contributions to the dose ( probably trying to tune their monte carlos , those guys are really big into modelling ) . he told me two interesting things if they ran the accelerator at high current and high duty cycle they could about double the dose at the fence ( i.e. . the accelerator related dose was as big as the background at the fence ) . neutron sky-shine was the single biggest contributor . sky-shine means that the neutrons got out through the lightly shielded roofs of the halls ( only 50 cm of concrete and 2 meters of packed earth ) , and their detectors saw radiation coming from the captures/decays that occurred above them . the fence was about 40 meters from the beam dumps .
it depends on the fan , but i would guess the majority of domestic fans will use less power at lower speeds . i can state with authority that the fan in my car ( a ford focus ) uses roughly the same power regardless of speed because i have just had to replace the ballast resistor that is uses to control fan speed . when you select a lower speed the fan dissipates power as heat in the ballast resistor so the speed setting makes little difference to the power drawn . i can not be sure about domestic fans , but in the car fan the heat dissipated in the ballast resistor is very noticable and indeed theresistor gets too hot to touch . the fan on my desk does not get hot when used at a lower speed , so i think it is very likely it does not simply dissipate power to lower the speed and therefore it will use less power at lower speeds . it is probably significant that the car fan is dc while domestic fans are ac . it is much easier to control power in ac circuits because you can use a thyristor or something similar to control the power delivery in a lossless way . the only way to be sure is , as energynumbers suggests , to measure the power drawn . a simple power meter like this one is all you need . unfortunately i am working away from home this week otherwise i could measure the power drawn by my own fan and give you a definitive answer . however i am sure there must be some fan owning , power meter armed , experimental physicists reading this :- )
the data set you show is a jumble of multiple contributions ( and the cern fits agree , see how many times those lines intersect and how they all lie close together in the horizontal band on the right side of the plot ? ) , and that makes for a problem . you need some way to disentangle these bits . the first thing i would tell a grad student to try is to separate the data-set on the basis of some other variables and fit each line to a sub-set of the data that she was confident contained mostly the particle they were interested in . this would generally be done by taking advantage of other data recorded about each hit in the plot{*} . if no additional data was available i would suggest using geometric cuts in the $de/dx$--$p$ plane to select the data to fit . that is fit to the parts of the plot which identifiable belong to a single particle species . if you are ambitious you could use a track density-and-width-model to sequentially subtract the results of each fit you get in order to make the next one easier ( at the cost of making the uncertainty analysis high correlated---yuck ! ) . additional options : start with a model for a particular particle and impose strict limits on the fitting parameters . consider the more fitting and log-likelyhood options to the fitter . also consider giving an artificially high weight to the data in the regions where the bands are well separated ( the is an alternative to the geometric cuts ) . if your monte carlo is good fit the lines to mc and simply plot them on the data . . ( this is not an uncommon thing to do ; does the caption on the cern data say where the lines come from ? ) {*} possibly something as simple as that plot representing the overlay of multiple particle-specific data-sets , or by taking advantage of pid detector systems , or particle species identifications arising from event topology . frankly you do not care what you get and you do not care if you lose a lot of the data in the selection ( statistics are not your problem when you have 7 million measurements ) .
well the answer is that the body will indeed loose the momentum . but since the mass of the body will decrease as well due to radiation , the velocity should not change .
the curie temperature or curie point is the temperature at which a ferromagnetic or a ferri-magnetic material becomes paramagnetic when heated . the effect is reversible . on the other hand , the curie-weiss temperature is the temperature at which a plot of the reciprocal molar magnetic susceptibility against the absolute temperature t intersects the t-axis . the curie-weiss temperature can adopt positive as well as negative values . i hope , now you will get the difference .
the mean anomaly relates position and time of an orbiting body . it is zero at the perihelion and increases with time . its formula is : $m=m_0+nt$ . so , in this case $m_0=-3.289°$ is the mean anomaly when the measurement was made . $n=0.9856$ is the mean motion , which is $2\pi$ divided by the duration of the full orbit . more information here .
a string is a " particle with a complicated internal structure " . to see the rough emergence of particle species , you may start with a hydrogen-string analogy . the hydrogen atom is a bound state of a proton and an electron . it may be in various energy eigenstates described by the quantum numbers $ ( n , l , m ) $ . they have a different angular momentum and its third polarization and different energies that mostly depend on $n$ . it is similar for a string . a string may be found in various states . the exact " spectrum " i.e. composition of these states depends on the background where the string propagates and the type of string theory ( more precisely , the type of the string theory vacuum ) . but for the rough picture , consider string theory in the flat space , e.g. in the 26-dimensional spacetime . take an open string . its positions $x^\mu ( \sigma ) $ may be fourier decomposed and each of the fourier modes , labeled by a positive integer $n$ , produces coordinates of a 24-dimensional harmonic oscillator . so an open string is equivalent to a $24\infty$-dimensional harmonic oscillator ( yes , it is twenty-four times infinity ) . each of the directions in this oscillator contributes $nn/ \alpha'$ to the squared mass $m^2$ of the resulting particle where $n$ is the total excitation level of the harmonic oscillators that arise from the $n$-th fourier mode . at any rate , the possible values of the squared mass $m^2$ of the particle are some integer multiples of $1/\alpha'$ . this dimensionful parameter $1/\alpha'$ is also called $1/l_{string}^2=m_{string}^2$ . the ground state of the string , $|0\rangle$ of the harmonic oscillator , is a tachyonic particle with $m^2=-1/\alpha'$ in the case of bosonic strings . these tachyons are filtered away in the superstring . the first excited state of an open string is $\alpha^\mu_{-1}|0\rangle$ which carries one spacetime lorentz vector index so all these states behave as a vector with $m^2=0$ . they give you a gauge boson . and then there are massive modes with $m^2\gt 0$ . closed strings of similar masses have twice larger number of indices , so for example , the massless closed string states inevitably produce a graviton . so different masses of the resulting particles arise from different values of $nn$ – and the very fact that the values may be different for different excitations is analogous to the same feature of the hydrogen atom or any other composite particle in the world . in string theory , however , one may also produce states with different values of the angular momentum – also somewhat analogous to the hydrogen atom which is a sufficient model – or different values of the electric charge and other charges . for example , in some kaluza-klein-like vacua , the number of excitations of $x^5_{n}$ , the fourier modes of the ( circular ) fifth dimension $x^5$ , will be interpreted as the electric charge and it will behave as the electric charge in all physical situations , too . there are other ways how $u ( 1 ) $ electric-like charges and other charges arise in string theory . see e.g. this popular review http://motls.blogspot.com/2012/08/why-stringy-enhanced-symmetries-are.html?m=1 of ways how yang-mills gauge groups and charges may emerge from different formulations and vacua of string theory . if even this review is too technical , you will have to be satisfied with the popular brian-greene-like description stating that particles of different mass , spin , or charges emerge from strings vibrating in different ways . i am sort of puzzled about your question – and afraid that my answer will be either too simple or too off-topic given your real question – because you must have heard and read these basic insights about string theory about hundreds of times already .
realistically , because the light emitted from the infalling object is quantized , you will observe the " last " photon emitted from the infalling object in ( very much ) finite time . if you keep waiting after that , you will eventually observe the black hole hawking-radiate away , and any " information " carried by the infalling object will be " encoded " in the hawking radiation . here 's an additional argument : when the infalling object passes through the event horizon , it will contribute to the mass of the black hole , which will cause the event horizon to expand outward even more . this ensures even further that the infalling object will no longer be observable from the outside in a relatively short time , compared with the lifetime of the black hole .
naively , both temperatures ( curie and curie-weiss temperature ) are equal and they are the constant temperature $t_c$ entering the curie-weiss law : $$ \chi = \frac{c}{t-t_c} . $$ however , the behavior is often more complicated and the formula above does not describe the susceptibility $\chi$ well for all temperatures . when it is so , the curie temperature $t_c$ is the temperature at which the susceptibility actually blows up , so $\chi=c/ ( t-t_c ) $ holds for $t\sim t_c$ while the curie-weiss temperature is the temperature for which the law $\chi=c/ ( t-t_0 ) $ holds for $t\gg t_0$ , i.e. one reconstructed from the " shape of the hyperbola far away " . the temperatures are close $t_0\sim t_c$ for materials for which the transition is first-order ; the temperatures are very different if the transition is second-order . since it is the curie-weiss temperature that may be negative and it is extracted from the behavior of a curve far enough from the actual transition , and the behavior may be rather complex , your simple positivity argument ( more heat means higher entropy ) is not really able to determine the curie-weiss temperature , not even its sign . at any rate , the curie-weiss temperature is just a constant with the units of temperature that happens to enter some more complicated law . we are actually not adjusting any physical system to this temperature – it is not possible if it is negative – so the wikipedia page about the negative temperatures is not really relevant . we are not studying any system at a negative temperature ; just a system at a positive temperature , much higher than $t_c$ or $t_0$ , where the behavior reproduces some curves that may be parameterized by the curie-weiss temperature . something special would occur at the curie-weiss temperature if the hyperbolic shape of $\chi$ could be extrapolated . but it cannot so all the special things actually occur at positive temperatures . i am not saying that " negative temperatures " are indefensible as a generalized concept at all situations ; i am just saying that this generalization is not needed when discussing the curie-weiss temperature , not even if the latter is negative .
as for you distribution , i think it should be correct , because you can note $p^0=\frac{m}{\sqrt{1-v^2}}$ , so your distribution is actually $$ f_k=\frac{1}{m}n'_k\delta ( p-p_k ) , $$ where $n'_k$ is the particle density in your lab frame , which is ( up to your normalization $m$ which is unclear to me ) really the phase-space distribution of dust . i think that a possible source of errors can be the following . if you know that $$ \delta ( p-p_k ) f ( p ) +\delta' ( p-p_k ) g ( p ) =0 $$ it implies that \begin{align} g ( p_k ) =0\\ f ( p_k ) -g' ( p_k ) =0 \end{align} this is because $g ( p ) \delta' ( p-p_k ) \neq g ( p_k ) \delta' ( p-p_k ) $ , but rather \begin{align} g ( p ) \delta' ( p-p_k ) = and \left ( g ( p ) \delta ( p-p_k ) ) \right ) '-g' ( p ) \delta ( p-p_k ) =\\ = and \left ( g ( p_k ) \delta ( p-p_k ) ) \right ) '-g' ( p_k ) \delta ( p-p_k ) =\\ = and g ( p_k ) \delta' ( p-p_k ) -g' ( p_k ) \delta ( p-p_k ) \end{align} where you now have the coefficients before linearly-independent functions as real numbers , not functions ( this is what you need ) . the above is just the leibniz rule , if you recall the definition of distributions ( functions like $\delta$ ) , and the definition of ' a distribution is zero ' . when you do your derivation , you should be carefull . you should keep the distinction between $p$ and $p_k$ up to the very end . look . you have derived the first equation by using the composition rule , and then just set the coefficient before the $\delta'$ to zero . it is ok , but you either have to assume that $p_0$ in your definition of $f$ is a function of $p_k$ or add the $-g' ( p_k ) $ term to the second equation . i gather that you have not added this term . then , in deriving the second equation we should consider $p_0$ to be a function of $p_k$ and thus to have derivatives wrt $x$ . lets see , lhs comes from differentiating $n_k$ wrt $x$ , and rhs comes from differentiating $p_0$ wrt $p$ . so , you do not follow one of the possibilities , hence the problems .
the binding energy curve , again in wikipedia , shows iron as the one with the smallest binding energy per nucleon . though in the table , the following is stated : 56fe has the lowest nucleon-specific mass of the four nuclides listed in this table , but this does not imply it is the strongest bound atom per hadron , unless the choice of beginning hadrons is completely free . iron releases the largest energy if any 56 nucleons are allowed to build a nuclide—changing one to another if necessary , the highest binding energy per hadron , with the hadrons starting as the same number of protons z and total nucleons a as in the bound nucleus , is 62ni . thus , the true absolute value of the total binding energy of a nucleus depends on what we are allowed to construct the nucleus out of . if all nuclei of mass number a were to be allowed to be constructed of a neutrons , then fe-56 would release the most energy per nucleon , since it has a larger fraction of protons than ni-62 . however , if nucleons are required to be constructed of only the same number of protons and neutrons that they contain , then nickel-62 is the most tightly bound nucleus , per nucleon . one sees that there is a leeway when constructing models in end of the universe scenaria . there is so much speculation in the time lines . observation tells us that ni-62 is not abundant , while fe is . it seems that in the sequence of supernova explosions iron wins out ; according to the quote above , this would mean that it is the number of nucleons that is important and the charges statistically arrange themselves . anyway , in a continuously expanding universe with a stable proton it is hard to see how the expanding gases of helium and hydrogen can tunnel into anything as they expand so that " all matter " would end up as fe or ni atoms . not to worry , the proton will decay according to all models of particle physics anyway .
i simply misfactorised the quadratic - i knew it was a stupid mistake . i am amazed that i did not see it , but even more amazed that nobody else did ! here is the correct solution . $$\phi_{01} ( t , x , y , z ) = \frac{1}{2\pi i}\oint d\xi \frac{\xi}{ ( x^{01'} ) ^2 ( \xi -\xi_1 ) ^2 ( \xi - \xi_2 ) ^2}$$ the residue at $\xi_1$ is \begin{align*} r_1 and = \rho_{\xi_1}\frac{d}{d\xi}\frac{\xi}{ ( x^{01'} ) ^2 ( \xi - \xi_2 ) ^2} \\ and = \frac{1}{ ( \xi_1 - \xi_2 ) ^2 ( x^{01'} ) ^2} -2 \frac{\xi_1}{ ( x^{01'} ) ^2 ( \xi_1-\xi_2 ) ^3} \\ and = \frac{\xi_1 - \xi_2 - 2\xi_1}{ ( x^{01'} ) ^2 ( \xi_1-\xi_2 ) ^3} \\ and = -\frac{x ( y+iz ) ^2}{2r^3 ( y+iz ) ^2} \end{align*} this is now the correct answer : ) .
the answer is definitely yes . the ground states ( and low-lying eigenstates ) of many-body systems are generically entangled . examples include the ground states of local quantum field theories ( which describe the fundamental particles and forces of nature ) and ground states of fermionic lattice models ( which describe much of the solid matter we see around us ) . the reason for this is simple : these systems are composed of many interacting constituents . take for example the magnetic dipoles associated with the spins of atoms in a lattice . these spins are quantum mechanical , which means the orientations of their dipole moments fluctuate even at low temperatures , due to the uncertainty principle . in addition , the dipoles interact with each other because of the magnetic fields that they produce . therefore these intrinsic quantum fluctuations become correlated with each other , via the mutual magnetic interaction of the spins . the appearance of local quantum fluctuations that are globally correlated is one of the essential features of entangled states . the entanglement of many-body ground states is highly relevant , since many systems are nearly in their quantum ground states even at everyday temperatures . examples include the electrons in a typical metal , or the electromagnetic radiation field at optical frequencies . this is true whenever the typical energy scales of the system are large compared to room temperature ( i.e. . the fermi energy for a metal , or the frequency of optical radiation ) . interestingly , the principle of locality places quite severe restrictions on the kinds of entangled ground states that actually appear in nature . typically , these states obey area laws . that is , the amount of entanglement between a sub-region of the system and the rest scales with the area of the boundary of the sub-region , rather than its volume . for more information on area laws , see j . eisert et al . , rev . mod . phys . 82 , 277 ( 2010 ) .
for someone with perfect vision , the lens in their eye focuses a point source in their field of vision to a point source on their retina . for someone with less than perfect vision , the focus lies either before or behind the retina , resulting in a large spot on the retina , instead of a point . see the top illustration . when you squint , or roll your fingers into a tube in front of your eye , you are placing an aperture in the way of the light . see the bottom illustration . as the diagram shows , the focus stays the same distance away from the retina , but since the angles are smaller , the spot on the retina is also smaller . a smaller spot is closer to a point , and therefore seems " sharper " - even though the vision is just as bad . the camera obscura works on a similar principle .
the answer is in front of you $$ a = \frac{{\rm d} ( u ) }{{\rm d} t} = \frac{{\rm d}}{{\rm d} t} \left ( \frac{{\rm d} ( r ) }{{\rm d} t} \right ) = \frac{{\rm d}^2 ( r ) }{{\rm d} t^2}$$ as a matter of convention . it is just the way we write 2nd derivatives .
a spacecraft in space should only lose heat by radiation to space , since it is not in direct contact with anything . near shoemaker actually soft-landed on the surface of eros . after the landing , its contact with the surface meant that it would lose heat by direct conduction , making it more difficult to maintain the temperature necessary for operation . conditions near an asteroid probably would not be a problem , except for the lack of sunlight that you mentioned . disclaimer : i do not know how much of the spacecraft was in contact with the surface , or how much heat it would have lost through the contact area . this is largely speculation .
the most elementary reason is that the dirac field hamiltonian is bounded below only when you use anticommutation relations on the creation/annihilation operators instead of commutators . a free quantum field theory with energy unbounded below has no stable vacuum . it is easiest to demonstrate this in two dimensions , where there are no polarization issues . instructive 2d example in two dimensions ( one space one time ) , there is a nice dimensionally reduced analog , which is the right-moving ( necessarily massless ) majorana-weyl fermion ( the argument also works with 2d dirac fermions with two components , but this is the simplest case ) . this is a single component field $\psi$ which obeys the equation of motion $$ ( \partial_t -\partial_x ) \psi = 0 $$ this simple equation is derived from the 2d dirac equation using the ( real convention , explicitly real ) 2d dirac matrices ( 0,1 ; -1,0 ) an ( 0,1 ; 1,0 ) , which are $\gamma_0 = \sigma_x$ and $\gamma_1 = i\sigma_y$ . they square to 1 and -1 respectively , and they anticommute , so they reproduce the 1+1 dimensional metric tensor . the $\gamma_5$ analog , which i will call $\gamma$ to accommodate different dimensions , is diagonal in this explicit representation , and $\gamma=\sigma_z$ . the two eigenvectors of $\gamma$ propagate independently by the 2d massless equation of motion $$ \gamma_i \partial_i \psi = 0 $$ and further , because the $\gamma$ matrices are real , this is a majorana representation ( most physicists write the dirac equation with an i factor in front of the derivative , so that the dirac matrices for a majorana representation are purely imaginary . i am using a mathematician 's convention for this , because i like the equations of motion to be real . others like the k-space propagator to not have factors of i in the k part . unfortunately , physicists never settled on a unique sensible convention--- everyone has their own preferred way to write dirac matrices ) . so it is sensible in the equation of motion to restrict $\psi$ to be hermitian , since its hermitian conjugate obeys the exact same equation . so that the field has a k decomposition $$ \psi ( x ) = \int a_k e^{ikx - ikt }dk$$ and the reality condition ( hermiticity ) tells you that $a^{\dagger} ( -k ) = a ( k ) $ ( one should say that the normalization of the $a$ operators expansion is not completely conceptually trivial--- the $a$ 's are both relativistically and nonrelativistically normalized , because the spinor polarization $\sqrt{w}$ factor cancels the mass-shell hyperbola factor , so that the dk integration is not weighted by anything , it is just the normal calculus integral with uniform measure ) an operator with definite frequency , which ( heisenberg picture ) evolves in time according to $$ \partial_t o = i\omega o$$ has the property that it is a raising operator--- acting with this operator adds $\omega$ to the energy . if $\omega$ is negative , $a$ is an annihilation operator . the condition that the vacuum is stable says that all the annihilation operators give 0 when acting on the vacuum state . but notice that the frequency in the expansion of $\psi$ changes sign at $k=0$ . this came from the linearity of the dirac hamiltonian in the momenta . it means that the operator $a_k$ acts to raise the energy for k> 0 , but acts to lower the energy for $k&lt ; 0$ . this means that the $k&gt ; 0$ operators create , and the $k&lt ; 0$ operators annihilate , so that the right way to $a^{\dagger} ( -k ) $ are creation operators , while the $k&lt ; 0$ operators are annihilation operators . the energy operator counts the number of particles of momentum k , and multiplies by their energy : $$ h = \int_{k&gt ; 0} k a^{\dagger} ( k ) a ( k ) dk $$ and this is manifestly not a local operator , it is defined only integrated over k> 0 . to make it a local operator , you need to extend the integration to all k , but then the negative k and positive k contributions have opposite sign , and they need to be equal . to arrange this , you must take anticommutation relations $$ \{ a^{\dagger} ( k ) , a ( k ) \} = i\delta ( k-k&#39 ; ) $$ and then $$ h = {1\over 2} \int k a^{\dagger} ( k ) a ( k ) = \int \psi ( x ) i \partial_x \psi ( x ) dx $$ note that this looks like it is a perfect derivative , and it would be if $\psi$ were not anticommuting quantity . for anticommuting quantities , $$ \partial_x \psi^2 = \psi \partial_x \psi + \partial_x \psi \psi $$ which is zero , because of the anticommutation . deeper reasons although this looks like an accidental property , that the energy was negative without anticommutators , it is not . the deeper reason is explained with euclidean field theory using a feynman-schwinger formalism , but this requires understanding of the euclidean and path integral versions of anticommuting fields , which requires being comfortable with anticommuting quantities , which requires a motivation . so it is best to learn the shallow reason first .
the other answers are correct , but i think that you might benefit from a more " microscopic " view of what is happening here . whenever one substance ( a solute ) dissolves in another ( a solvent ) , what happens on the molecular scale is that the solute molecules are surrounded by the solvent molecules . what causes that to happen ? as @chris described , there are two principles at work - thermodynamics , and kinetics . in plain terms , you could think of thermodynamics as an answer to the question " how much will dissolve if i wait for an infinite amount of time , " whereas kinetics answers the question " how long do i have to wait before x amount dissolves . " both questions are not usually easy to answer on the macroscopic scale ( our world ) , but they are both governed by two very easy to understand principles on the microscopic scale ( the world of molecules ) : potential and kinetic energy . potential energy on the macroscopic scale , we typically only think about gravitational potential energy - the field responsible for the force of gravity . we are used to thinking about objects that are high above the earth 's surface falling towards the earth when given the opportunity . if i show you a picture of a rock sitting on the surface of the earth : and then ask " where is the rock going to go ? " you have a pretty good idea : it is going to go to the lowest point ( we are including friction here ) . on the microscopic scale , gravitational fields are extremely weak , but in their place we have electrostatic potential energy fields . these are similar in the sense that things try to move to get from high potential energy to lower potential energies , but with one key difference : you can have negative and positive charges , and when charges have the opposite sign they attract each other , and when they have the same sign , they repel each other . now , the details of how each individual molecule gets to have a particular charge are fairly complicated , but we can get away with understanding just one thing : all molecules have some attractive potential energy between them , but the magnitude of that potential energy varies by a lot . for example , the force between the hydrogen atom on one water molecule ( $h_2o$ ) and the oxygen atom on another water molecule is roughly 100 times stronger than the force between two oxygen molecules ( $o_2$ ) . this is because the charge difference on water molecules is much greater ( about 100 times ) than the charge difference on oxygen molecules . what this means is we can always think of the potential energy between two atoms as looking something like this : the " ghost " particle represents a stationary atom , and the line represents the potential energy " surface " that another atom would see . from this graph , hopefully you can see that the moving atom would tend to fall towards the stationary atom until it just touches it , at which point it would stop . since all atoms have some attractive force between them , and only the magnitude varies , we can keep this picture in our minds and just change the depth of the potential energy " well " to make the forces stronger or weaker . kinetic energy let 's modify the first potential energy surface just a little bit : now if i ask " where is the rock going to go ? , " it is a little bit tougher to answer . the reason is that you can tell the rock is " trapped " in the first little valley . intuitively , you probably can see that if it had some velocity , or some kinetic energy , it could escape the first valley and would wind up in the second . thinking about it this way , you can also see that even in the first picture , it would need a little bit of kinetic energy to get moving . you can also see that if either rock has a lot of kinetic energy , it will actually go past the deeper valley and wind up somewhere past the right side of the image . what we can take away from this is that potential energy surfaces tell use where things want ( i use the term very loosely ) to go , while kinetic energy tells us whether they are able to get there . let 's look at another microscopic picture : now the atoms from before are at their lowest potential energy . in order for them to come apart , you will need to give them some kinetic energy . how do we give atoms kinetic energy ? by increasing the temperature . temperature is directly related to kinetic energy - as the temperature goes up , so does the average kinetic energy of every atom and molecule in a system . by now you might be able to guess how increasing the temperature of water helps it to clean more effectively , but let 's look at some details to be sure . solubility we can take the microscopic picture of potential and kinetic energies and extract two important guidelines from it : all atoms are " sticky , " although some are stickier than others higher temperatures mean that atoms have larger kinetic energies going back to the coffee cup question , all we need to do now is think about how these will play out with the particular molecules you are looking at . coffee is a mixture of lots of different stuff - oils , water-soluble compounds , burnt hydrocarbons ( for an old coffee cup ) , etc . each of these things has a different " stickiness . " oils are not very sticky at all - the attractive forces between them are fairly weak . water-soluble compounds are very " sticky " - they attract each other strongly because they have large charges . since water molecules also have large charges , this is what makes water-soluble compounds water-soluble - they stick to water easily . burnt hydrocarbons are not very sticky , sort of like oils . since molecules with large charges tend to stick to water molecules , we call them hydrophilic - meaning that they " love " water . molecules that do not have large charges are called hydrophobic - they " fear " water . although the name suggests they are repelled by water , it is important to know that there are not actually any repelling forces between water and hydrophobic compounds - it is just that water likes itself so much , the hydrophobic compounds are excluded and wind up sticking to each other . going back to the dirty coffee cup , when we add water and start scrubbing , a bunch of stuff happens : hydrophilic compounds hydrophilic compounds dissolve quickly in water because they stick to water pretty well compared to how well they stick to each other and to the cup . in the case where they stick to each other or the cup better than water , the difference is not huge , so it does not take much kinetic energy to get them into the water . so , warm water makes them dissolve more easily . hydrophobic compounds hydrophobic compounds ( oils , burnt stuff , most stains ) do not stick to the water . they stick to each other a little bit ( remember that the forces are much weaker compared to water since the charges are very small ) , but water sticks to itself so well that the oils do not have a chance to get between the water molecules . we can scrub them , which will provide enough energy to knock them loose and allow the water to carry them away , but if we were to increase the kinetic energy as well by increasing the water temperature , we could overcome both the weaker forces holding the hydrophobic compounds together , while simultaneously giving the water molecules more mobility so they can move apart and let the hydrophobic compounds in . and so , warmer water makes it easier to wash away hydrophobic compounds as well . macroscopic view we can tie this back to the original thermodynamics vs . kinetics discussion . if you increase the temperature of the water , the answer to the question " how much will dissolve " is " more . " ( that was the thermodynamics part ) . the answer to " how long will it take " is " not as long " ( kinetics ) . and as @anna said , there are other things you can do to make it even easier . soap for example , is made of long chain molecules with one charged end and one uncharged end . this means one end is hydrophilic , while the other end is hydrophobic . when you add soap to the picture , the hydrophilic end goes into the water while the hydrophobic end tries to surround the oils and burnt stuff . the net result is little " bubbles " ( called micelles ) made up of soap molecules surrounding hydrophobic molecules that are in turn surrounded by water .
yes , coherent radiation means that the phases of two ( or more ) waves representing the radiation differ by a known constant . incoherence means that the phase differences are unknown/random . laser radiation is coherent because stimulated emission assures phase differences are constant . radiation from an incandescent lamp is incoherent because the electromagnetic waves are generated in a statistically random manner depending on which atoms are excited and de-excited . to develop intuition think of the classical example of soldiers marching in step . their motion is coherent and so are the vibrations they set up . they break step over ancient bridges so as to become incoherent , there were cases where ancient bridges resonated to the step and were damaged .
no . you are correct that the kinetic energy is equal to the change in the potential energy , $mgh$ , where $h$ is the distance fallen , but because the object is accelerating $h$ is not simply velocity times time . if the object starts at rest then ( ignoring air resistance ) : $$h = \frac{1}{2} g t^2 $$ so substituting this into $mgh$ gives : $$e_{kinetic} = \frac{1}{2} m g^2 t^2 $$ note that $gt$ is just the velocity at time $t$ , so this expression is the same as : $$e_{kinetic} = \frac{1}{2} m v^2 $$ which may look familiar :- ) note however that the velocity $v$ is a function of time .
( 1 ) first argument an ordinary object that is spinning on an axis has an angular momentum which is determined by how the mass of the object is distributed about the axis , and how fast the object is spinning . for a fixed angular momentum , if the mass is distributed farther from the axis the angular velocity is lower ; if the mass is distributed closer to the axis , the angular velocity is higher . think of a spinning ice skater who turns at one , slower speed with arms extended and at another , higher speed with arms pulled in overhead ( on the axis ) . no size has been found for electrons ; they appear to be point particles . if an electron has finite size ( which happens to be too small to see ) , it introduces issues in any classical description , like charge self-repulsion having infinite energy or having a surface which spins faster than the speed of light . if the electron is a point particle , i.e. , has no finite size , then it cannot have angular momentum due to spinning about it is own center of mass because the entire object is on its rotational axis . how to get out of this conundrum ? you cannot " see " an electron to determine whether it is spinning or not . the " spinning " of the electron is not measurable ; it makes no sense to speak of it in science . however , you can measure an electron 's angular momentum ; it makes sense to speak of angular momentum in science . therefore , do not think of the electron as a " spinning " object ( which we can never know or observe ) ; think of it as simply having " intrinsic " angular momentum . ( 2 ) second argument the usefulness of an analogy in science is determined by whether you can make inferences or understand other features in a first system under study by way of the analogy and knowledge of a second system . thinking of spin as " spinning " introduces conceptual problems classically , does not generalize easily to massless objects , does not handle half-integer spin well ( the representations of a classical spinning object do not look like a spin 1/2 object ) , and allows you to infer almost nothing correctly about the electron , other than feeling like you know where the angular momentum comes from . this point of view does not work well to start with , and is a dead end as you keep studying physics . on the other hand , thinking of spin as intrinsic angular momentum avoids all the above-identified issues and , you will find with further study , fits nicely into the rest of physics . ( 3 ) regarding spin-1/2 and the value ℏ√3/2 , these come from group theory and a particular choice of units for angular momentum . this can not be really explained well in a post ; study group theory for physicists . good luck !
if we assume that all the matter that is only visible in star systems ( baryonic matter ) , then we are not able to account for how galaxies rotate . similarly , we have problems trying to calculate movement in a cluster of galaxies , or the rate of expansion of the universe . it seems as if there is more gravitational interaction happening than just the matter we see . so , we postulate something called " dark matter " that is essentially matter which exerts gravitational force but is not visible to us since we can not see it ( any form of electromagnetic radiation ) . other observations such as gravitational lensing ( light bending around a heavy object ) due to invisible/dark objects also indicate the existence of dark matter . as for you question of why this mass is not to be found in black holes . . . if this mass were in a black hole , it would be highly localized . also , ( super ) massive black holes are typically found only in the centre of galaxies , whereas to explain the observed effects , we need dark matter to be quite well diffused all around the galaxy and some in between galaxies . we take into account gravitational interactions due to the mass which such black holes can have and ascribe the remaining discrepancy to dark matter . as always , the wikipedia page on dark matter gives more details if you are interested .
you could make an analogy between the pressure distribution of a sound wave and the mass density distribution of a realistic spring undergoing vibrations , but it would not give you the explanation you are looking for . as a matter of fact , that would be more like explaining a sound wave in terms of springs , rather than what you are trying to do , i.e. explaining a spring in terms of waves . although i am not intimately familiar with the details , basically what goes on at the microscopic level of a spring is that , when the spring is at equilibrium , the atoms are set in some sort of rigid structure . any given pair of atoms has a potential energy which is a function of the distance between those two atoms , so the entire spring has a potential energy determined by all the distances between every possible pair of atoms : $$u = \sum_{i , j} u_{ij} ( r_{ij} ) $$ in equilibrium , the spring will take a shape which minimizes this total potential energy . if you think about it , a metal spring might typically be formed by heating some metal to make it malleable ( or even melting it ) , and then forming it into the desired shape before it cools . the heat allows the atoms to move around relatively freely so that they can reach the equilibrium configuration that minimizes their potential energy , then once the spring cools , they are frozen in place . of course , the atoms are not completely frozen in place . as i see that georg has already written in his answer , the potential energy between two atoms ( $u_{ij} ( r_{ij} ) $ ) has a minimum at their equilibrium distance and goes up on either side . if you add some energy into the system , say by exerting a force on it , you can get the atoms to move closer together or further apart . when you stretch or compress a string , you are really just doing this to all the ( pairs of ) atoms in the spring simultaneously . the atoms will , of course , " try " to return to their equilibrium position , i.e. they will " try " to minimize their potential energy , and this is what you feel as the restoring force of a spring under tension .
i think this is largely just terminology . strictly speaking the coordinate is $ct$ not $t$ , and of course $d ( ct ) = cdt$ . in any case we usually choose units where $c = 1$ and just ignore it .
$\vec{e}$ has not different phase , but different polarisation : $\vec{b} = \vec{n}\times\vec{e}$ yes , this solution cannot be a solution of maxwell 's equations for all $\vec{k}$ , cause $\nabla{ \vec{e}} =0$ for $\vec{e} = \vec{e_0} e^{-i ( \omega t - \vec{k}\vec{r} ) } $ implies $\vec{k} \vec{e_0} = 0$ , so electromagnetic waves in the empty space are transverse . the profound reason of this is the gauge invariance of the em field and , finally , the masslessness of photons .
first , your wave equation is wrong . you can see this from dimensional analysis . it should be \begin{align} \frac{\partial^2 \phi}{\partial t^2} = c^2 \frac{\partial^2 \phi}{\partial x^2} \end{align} second , you made a mistake in the cross terms for the $\partial^2 /\partial x^2$ term . the cross term should have the coefficient $-2\gamma^2 v/c^2$ . third , use the fact that $\gamma = \frac{1}{\sqrt{1 - v^2/c^2}}$ . you will get the desired result .
you are right . in a space-time with one time dimension and $d$ spatial dimensions , finding possible different statistics is equilalent to look at the fundamental group ( first homotopy group ) of $so ( d ) $ for $d=1$ , the fundamental group is trivial . for $d=2$ , the fundamental group is $\mathbb{z}$ . for $d&gt ; =3$ , the fundamental group is $\mathbb{z}_{2}$ so , it explains , why , in 3 spatial dimensions , there are only 2 kinds of statistics ( fermions and bosons ) , while the situation is different with 2 spatial dimensions . for quantum point of view , you will have to find unitary representations of this fundamental group .
the symmetry breaking just breaks the symmetry in the electroweak sector of the standard model , it does not turn off the interaction--neutrinos become different from electrons , the gauge bosons get mass , etc , but all of those things are still there . in essence , what the symmetry breaking does is make the weak interaction short-ranged .
i am confused . bernoulli 's eqn . says the static pressure inside the jet should be less than atmospheric . as you go further out and the jet slows down , then it should approach atmospheric pressure . the pressure gradient between the atmosphere outside and the low pressure inside the jet leads to air getting sucked into the jet ( entrainment ) .
it is really not clear what hypothetical limits you are imposing . i take your question to mean that in the process of baryogenesis the various baryons like protons and neutrons highly favored up quarks ( lots more protons than neutrons ) . remember , quarks are subject to confinement so other than a quark-gluon plasma , quarks are confined to baryons . since stellar formation depends on hydrogen and hydrogen formation depends on protons and electrons primarily , it is unlikely that there would be any meaningful impact . also , remember that free neutrons are unstable and will undergo beta decay and convert to protons . if there is an excess of protons the reverse can happen via electron capture . i am pretty sure even if there were some wild physical law that forced there to always be a huge imbalance of up to down quarks there would still be enough hydrogen formation for stars to form . it is not clear what this restriction would do to hydrogen to helium fusion though .
let $x$ denote the length of the rope that is on the table , then $$ m ( x ) = \frac{m}{l}x $$ is the mass of the rope on the table . it follows that the force of friction on the rope on the table is $$ f ( x ) = \mu_k m ( x ) g = \mu_k\frac{m}{l}xg $$ if the rope moves an amount $dx$ then the work done by friction is $$ dw = f ( x ) dx = \mu_k\frac{m}{l}gx dx $$
it is easy to forget that , in the context of relativity , there is no universal time . you write : for an outside observer the time seems to stop at the event horizon . my intuition suggests , that if it stops there , then it must go backwards inside . is this the case ? but your intuition does not seem to take into account that , for an observer falling into the hole , time does not stop at the event horizon . the point is that one must be much more careful in their thinking about time within the framework of general relativity where time is a coordinate and coordinates are arbitrary . in fact , within the event horizon , the radial coordinate becomes time-like and the time coordinate becomes space-like . this simply means that , to " back up " inside the event horizon is as impossible as moving backwards in time outside the event horizon . in other words , the essential reason it is impossible to avoid the singularity once within the horizon is precisely that one must move forward through time which , due to the extreme curvature within the horizon , means moving towards the central singularity .
33 mj is the electrical energy . i think the projectile is about 1kg , so the efficiency is about 10% , not so bad . plasma from electrical breakdown , which then gets accelerated the same way the projectile does , with exb force
if a body moves only because of the influence of a torque , then it will rotate about the center of gravity . there is no location for torques , only directions . you you take the equations of motion as seen here ( stackexchange-url you will see that the location of the torque does not enter into the equations . only the location of the forces . as a result the acceleration of the center of mass is zero , and only angular velocity will exist . the body will rotate about its center of mass . note that these two statements are equivalent : a pure force thorugh the center of gravity ( with no net torque ) will purely translate a rigid body ( any point on the body ) . a pure torque any point on the body ( with no net force ) will purely rotate a rigid body about its center of gravity consider a motionless rigid body with a pure instantenous torque $\vec{\tau}$ applied on it . the motion of any point a not on the center of gravity is $$ 0 = m \vec{a}_a - m \vec{c}\times \vec{\alpha} \\ \vec{\tau} = i_c \vec{\alpha} - m \vec{c} \times \vec{c} \times \vec{\alpha} + m \vec{c} \times \vec{a}_a $$ where $\vec{c}$ the position vector of the center of gravity relative to point a . the soltution to the above is $$ \vec{a}_a = \vec{c} \times \vec{\alpha} \\ \vec{\tau} = i_c \vec{\alpha}- m \vec{c} \times \vec{c} \times \vec{\alpha}+ m \vec{c} \times \vec{c} \times \vec{\alpha} = i_c \vec{\alpha} $$ $$ \vec{\alpha} = i_c^{-1} \vec{\tau} \\ \vec{a}_a = \vec{c} \times i_c^{-1} \vec{\tau} $$ from the above is it obvious that the only point a not moving is at $\vec{c}=0$ and all points parallel to $ \vec{\alpha}$ through the center of mass .
the virasoro algebra is a centrally extended algebra . this means that in every representation , its central element must be represented by the unit operator . thus ( for a nonvanishing central charge ) it cannot fully implemented at the quantum level as a symmetry of the vacuum , otherwise one can get a contradiction of the type $1 |0\rangle = 0 |0\rangle $ . the algebra $\frak{su} ( 1,1 ) $ generated by $l_{0 , \pm1}$ is the largest subalgebra not containing the central element , thus the largest subalgebra which can be implemented as a symmetry of the vacuum . the correct way is to view the ( centrally extended ) group $ g = \widetilde{diff ( s^1 ) }$ as the dynamical group of the theory ( please see souriau 's book page 100 ) . ( although this book treats only finite dimensional groups ) . this means that this group can be implemented classically by means of a canonical transformation . the hamiltonian will be an element of universal enveloping algebra of the group . both these conditions are valid in our case . the importance of this construction is that upon quantization , the action of the dynamical group can be ( optimistically ) lifted to a unitary representation on the quantum hilbert space . this representation is induced from a representation of the small group ( vacuum preserving subgroup for example a subgroup $h$ corresponding to the lie algebra $\frak{su} ( 1,1 ) $ ) . in the modern terminology , this construction is called a quantization of the coadjoint orbit $g/h$ . the representation of the small group actually fixes a symplectic structure on the coadjoint orbit . please see the article by : gay-balmaz on the virasoro group coadjoint orbits and references therein ( the article is available online in the following page ) . ( it should be mentionioned that the theory of coadjoint orbits in the finite dimensional case is much more straightforward , please see for example the following review by kirillov ) . this is not the whole story in our case because of the infinite dimensionality of the group and the representations . as mentioned in the question , the broken generators in this case cannot be unitarily implemented because they drive the vacuum outside the quantum hilbert space . this situation was considered by bowick and rajeev and also by kirillov and yuriev , please see the following review by sergeev . they constructed the hilbetrt space and then acted on it by an arbitrary element of $diff ( s^1 ) $ shifting the vacuum state . the vacuua generate a line bundle over $diff ( s^1 ) $ with a hermitian connection . they found that the condition that this connection becomes flat is exactly the when the quantum theory becomes $diff ( s^1 ) $ invariant at the quantum level ( for example in the flat case of the bosonic string this happens when $d=26$ ) . the quantization they performed is a kähler quantization . they found that the coadjoint orbit canonical bundle contribution to the anomaly is just equal to the contribution of the ghosts ( this is understandable because the ghost term originates from the volume measure of the path integral ) . the flatness condition of the vacuum bundle connection can be interpreted as a unitary equivalence of the quantization hilbert spaces . in this case the virasoro algebra can be exponentiated since it does not have a net central charge . this type of quantization was used by hitchin and later by axelrod , della pietra , and witten in the quantization of the chern simons theory . thus the solution of the implementability of the full virasoro group at the quantum level is by taking the vacuum as a tensor product of the matter fock vacuum and the ghost sectors such that its full central charge vanishes .
i understand that a general interpretation of the $1/r^2$ interactions is that virtual particles are exchanged [ . . . ] general relativity does not suppose that zero-mass particles exchanged . you do not need quantum field theory for this . in a purely classical field theory , we have field lines , and the field lines from a spherically symmetric source should radiate outward along straight lines . in a frame where the source is at rest , we expect by symmetry that the field lines are uniformly distributed in all directions . the strength of the field is proportional to the density of the lines , which falls off like $1/r^2$ in a three-dimensional space . this whole description is complicated by the polarization of the field . gravitational fields have complicated polarization modes . nevertheless , the $1/r^2$ result is unaffected . finally , we have an issue unique to gr , which is that the field is the metric , and this means that the field itself affects the measuring tools that we use to measure things like $r$ , the field , and the area of a surface through which we are counting the number of field lines that penetrate . these are all strong-field issues , so for large $r$ , they do not affect the $1/r^2$ argument . is it a postulate ? no . in the standard formulation of gr , the main postulate is the einstein field equations . from it , we can prove birkhoff 's theorem , which says that the schwarzschild metric is the external field of a static , spherically symmetric source . the weak-field limit of the schwarzschild metric corresponds to a $1/r^2$ field .
[ this is now a long answer . in summary , generally you need a physical assumption , the clock postulate , which people tend to omit , but is necessary , and can not be argued for a priori . however sometimes special relativity plus a restricted version of the postulate suffices . mundane experience is sufficient to verify this restricted version . ] let $\lambda = t$ the time according to inertial $o$ , and let $\vec{x}'$ be the spatial position of $o'$ according to $o$ , while $t'$ is the time measured by $o'$ . if $o'$ is piecewise inertial , then along each piece , $$c^2 ( \delta t'/\delta t ) ^2 = c^2 - ( \delta\vec{x}'/\delta t ) ^2\qquad [ 1 ] $$ and what you are trying to justify is that , even if $o'$ is not piecewise inertial , $$c^2 ( dt'/dt ) ^2 = c^2 - ( d\vec{x}'/dt ) ^2\qquad [ 2 ] $$ so , the problem is , special relativity strictly speaking only makes claims about inertial observers . and if you do not make any assumptions whatsoever about the experience of accelerated observers , then i think you are just stuck , mathematically i do not think you can go from $ [ 1 ] $ to $ [ 2 ] $ . ( for example , we can not rule out that proper acceleration itself further contributes to time dilation . ) i suggest : the motion of $o'$ is smooth . [ a1 ] for every $\epsilon&gt ; 0$ , there is a $\delta&gt ; 0$ such that , if , from the point of view of a certain unaccelerated observer $a$ , the magnitude of the velocity of another observer $b$ never exceeds $c\delta$ betwen time $t_0$ and $t_1$ , then time lapse $\delta t_b$ on $b$ 's clock satisfies $ ( t_1-t_0 ) ( 1-\epsilon ) &lt ; \delta t_b &lt ; ( t_1-t_0 ) ( 1+\epsilon ) $ . [ a2 ] pick $\epsilon&gt ; 0$ , use [ a2 ] to get $\delta$ ; use [ a1 ] to break the motion of $o'$ into intervals small enough such that , from the frame of reference of an interial observer travelling between the endpoints of a piece , the velocity of $o'$ never exceeds $c\delta$ ; use [ a2 ] to make $ [ 2 ] $ true within $\epsilon$ . since this works for all $\epsilon&gt ; 0$ , [ 2 ] is simply true . now [ a1 ] might look suspect , since we have used a piecewise inertial observer , whose motion is obviously not smooth ! so we can not even assume anything about what this piecewise inertial observer experiences at the corners ! but that is okay , [ a2 ] only refers to the individual pieces and not the whole . use a family of ( truly ) inertial observers that meet at the appropriate points . as for [ a2 ] , it is a bit opaque , but what it says that if you are not moving too fast relative to an inertial observer , your experience of time is almost the same . this does not follow logically from anything in particular , it is just a physical assumption . but note that special relativity is so hard for many people to accept precisely because [ a2 ] is a fact of life , for reasonably small $\epsilon$ . to make it true for even smaller $\epsilon$ requires more than everyday experience , but it is still " common sense " , and presumably testable to quite small values . now , to believe it literally for arbitrarily small $\epsilon$ requires quite a leap , but do not take differential equations literally . ( added : ) aha ! i found the clock postulate for accelerated observers , and i do believe [ a2 ] is interchangeable with it . and yes , it is often omitted but cannot be derived from other assumptions . it has been tested . ( second addendum ) : even though they are interderivable , mine is better :- ) i have given the accuracy of [ 2 ] directly in terms of the accuracy of [ a2 ] . for example , we do not need the full clock postulate for the twin paradox ( which you mention as a motivating example in a comment ) : the proper acceleration of $o'$ is continuous and its magnitude is bounded by $a_{max}$ . [ a1' ] ( over any finite interval , [ a1 ] does imply [ a1' ] for some value of $a_{max}$ . and [ a1' ] is sufficient for the above argument . ) now , even with mundane accelerations , the twin paradox can produce a sizeable mismatch in ages within a human lifetime . ( besides , if they are not survivable accelerations , the travelling twin 's lifetime ends ! ) so , there is a usable $a_{max}$ for [ a1' ] . and , mundane experience alone proves [ a2 ] up to that $a_{max}$ and down to fairly small $\epsilon$ . so [ 2 ] holds sufficiently accurately to give the twin paradox . we only need special relativity plus a mundane restricted clock postulate . ( i realise you can bypass the whole acceleration question by altering the paradox so that there are three inertial observers who compare clocks as they pass . but then it is not the twin paradox anymore , duh ! )
if you take your lagrangian , including the $a^\alpha a_\alpha$ and vary it with respect to $a^\alpha$ , you will get the classical equation of motion : $\partial_\beta \partial^\beta a^\alpha + \mu^2 a^\alpha = 0 $ . if you use a plane wave as a trial solution for this : $a^\alpha = e^{i p\cdot x} \epsilon^\alpha $ where the $\epsilon^\alpha$ 's are polarization vectors that obey your gauge ( calibration ) condition , you will get : $ ( p^2 -\mu^2 ) e^{i p\cdot x} \epsilon^\alpha= 0 $ . which enforces : $ ( p^2 -\mu^2 ) = 0 $ . expanding out the four momentum we get : $e^2-|\vec{p}|^2-\mu^2 = 0 $ . after rearranging we get : $e^2 =|\vec{p}|^2+\mu^2 $ which is the dispersion relation for a relativistic particle of mass $\mu$ .
let 's see with help of an example . let the particle is at $ ( 0,0 ) $ moving with speed of $2m/s$ at $t=0$ and is subjected to acceleration $-2\hat i\ \text{m/s}^{-2}$ . now see after $2 seconds$ . we see displacement is zero , and distance travelled =$2m$ . also acceleration is constant but still $\langle speed\rangle\not=0$ whereas $\langle velocity \rangle=0$ now let 's see the graphs for this scenario . you see distance traveled is area under v-t curve ( all positive ) but displacement is are with signs . the area under x-axis is negative . so , graphs can help in cases where velocity changes direction .
fundamental particles are identical . if you have two electrons , one from the big bang and the other freshly minted from the lhc , there is no experiment you can do to determine which one is which . and if there was an experiment ( even in principle ) that could distinguish the electrons then they would actually behave differently . electrons tend to the lowest energy state , which is the innermost shell of an atom . if i could get a pen and write names on all of my electrons then they would all fall down into this state . however since we can not tell one electron from another only a single ( well actually two since there are two spins states of an electron ) electron will fit in the lowest energy state , every other electron has to fit in a unique higher energy level . edit : people are making a lot of comments on the above paragraph and what i meant by making electrons distinguishable , so i will give a concrete example : if we have a neutral carbon atom it will have six electrons in orbitals 2s1 2s2 2p2 . muons and tauons are fundamental particles with very similar properties to the electron but different masses . muons are ~200 times more massive than electrons and tauons are ~3477 times more massive than an electron . if we replace two of the electrons with muons and two of the electrons with tauons all of the particles would fall into the lowest energy shell ( which can fit two of each kind because of spin ) . if in theory these particles only differed in mass by 1% or even 0.0000001% they would still be distinguishable and so all fit on the lowest energy level . now atoms are not fundamental particles they are composite , i.e. composed of " smaller " particles , electrons , protons and neutrons . protons and neutrons are themselves composed of quarks . but because of the way that quarks combine , they tend to always be in the lowest energy level so all protons can be considered identical , and similarly with neutrons . to take the example of carbon , there are several different isotopes , different number of neutrons , of carbon ( mostly $^{12}c$ but also ~1% $^{13}c$ and ~0.0000000001% $^{14}c$ {the latter which decays with a half life of ~$5,730$ years [ carbon dating ] but is replaced by reactions with the sun 's rays in the upper atmosphere} ) . if we take two $^{12}c$ atoms , and force all of the spins to be the same . this is not too difficult for the electrons of the atom since the inner electrons do not have a choice of spin because every spin in every level is already full . so only outer electrons matter . the nucleons also have spin . with our two $^{12}c$ atoms with all of the same spins , we now have two indistinguishable particles which if you set up an appropriate experiment ( similar in principle to the electrons not being able to occupy the same state ) we will be able to experimentally prove that these two atoms is indistinguishable . answer time : are atoms unique ? no . do atoms have any uniquely identifying characteristic besides their history ? their history of a particle does not affect it* . no particles are unique . atoms may have isotopes or spin to identify one from another , but these are not unique from another particle with the same properties . would it contain information with which we could positively identify that they two are the same ? yes only because we could positively identify that this carbon atom is the same as almost every other carbon atom in existence . *unless it does , in which case it may be considered a different particle with different properties .
i do not know if this is what you mean to ask , but what i know as a quantum quench is a sudden change in the potential , sufficiently fast that it can be considered instantaneous . in that case the state does not change instantaneously , but obviously its time evolution does : from that point it evolves according to the new hamiltonian . if originally you were in an eigenstate , generally you will be in a superposition of states for the new hamiltonian after the quench .
one revolution per minute is one sixtieth of a revolution per second . you have it the opposite way .
there were many models overturned throughout history , i will list some of the most salient ones . i will ignore the ones that predate modern science , the most prominent one being the geocentric model of the solar system , and i will confine myself to wrong ideas that were scientifically accepted as probably true at some point in history . phlogiston : this is the fluid that carries heat . it was imagined to be a quantity , like electric charge , that is conserved . experiments with cannon-boring machines showed that the amount of heat that you can make by doing mechanical work is only limited by your patience and force , so that the phlogiston fluid was discredited . this work is associated with the name of joule . the replacement for phlogiston is the modern notion of energy , which includes both mechanical and heat energies as one , and heat is no longer a quantity conserved separately from mechanical energy . lumineferous ether : this is the idea that light propagates in a medium , which fills all of space , and picks out a preferred frame , in which the speed of light is equal in all directions . relativity showed that such a frame does not exist--- that light moves in the same speed in all directions regardless of the motion of the source or the observer . this new symmetry removed the need for a non-relativistic light ether . modern etherlike ideas are relativistically invariant , and include the qcd condensates and the higgs mechanism . the drude model : when electrons were known , but quantum mechanics did not exist , drude proposed that a current is a drift of electrons . the idea was that electrons make a gas inside a metal , and for some reason they drift along as if they were in free space . the result predicted a very slow drift velocity of electrons , the drude velocity . drude 's model was made quantum mechanical , and this required that the electrons make a quantum fermi gas . the fermi gas is very cold at room temperature , since the typical temperature at which it becomes classical is of the order of the melting temperature of the metal . the behavior of a degenerate fermi gas explained the specific heat of metals , and gave a correct velocity for the current carrying electrons . the ether-knot theory : this idea was due to kelvin , that atoms are vortices in the ether , different atoms are different kinds of knots , and molecules are links . it was very popular at the turn of the century , because it could explain why atoms were discrete , but was shown to be wrong when bohr 's atom qualitatively and quantitatively explained the periodic table and the spectrum of hydrogen . the plum-pudding model : this imagined that electrons were embedded in a big diffuse positive charge , which was the atom . this theory predicted why atoms can have special resonance frequencies where they scatter light strongly . these frequencies were the resonant motions of the electrons inside the plum-pudding . the theory is incorrect , and the correct laws of resonant scattering are only provided by quantum mechanics . the bohr-sommerfeld quantum theory : this is the old quantum mechanics , where the action was given integer values . it was an important intuition for developing modern quantum mechanics , but it is now recognized as just an approximation to the actual quantum mechanical laws . the modern quantum mechanical laws were intuited from the bohr-sommerfeld laws by heisenberg in 1925 , and the same laws were derived by a different path , using wave-particle duality by debroglie , einstein and schrodinger . the bohr-sommerfeld theory is still a useful approximation , but it is not considered fundamental anymore . bohr-kramers-slater theory : this idea was that electron orbits are quantized , but the electromagnetic field is not . the theory predicted that energy is not conserved . the reason is that electromagnetic waves which do not come in photons can excite electrons in quantized orbits on many atoms at once , even if there are too few photons in the wave to do that . the modern quantum theory , plus the observation of compton scattering , demonstrated that photons are real , and killed the theory . nuclear electrons : in the 1920s and 1930s , before the neutron was discovered , the mass of nuclei were known to be approximately integer multiples of the mass of the proton . since it was considered unlikely that there would be a neutral particle with the same mass as the proton , people assumed that there were electrons tightly bound in the nucleus . this idea was inconsistent with quantum mechanics , which predicted that an electron confined to a nucleus would be about as massive as a proton , and when the neutron was discovered , the idea was jettisoned . you see the theory of nuclear electrons in old papers . integer charged sakata quarks : in 1957 , japanese physicist sakata explained the structure of the known strongly interacting particles by assuming they were all made of the proton , neutron , and lambda baryon . this idea is roughly successful , but only because the proton , neutron and lambda are surrogates in this theory for the up , down , and strange quark . the idea was fixed up to the quark model by gell-mann and zweig , but sakata is strangely neglected in this story , perhaps because of his strong marxist political leanings . landau mean-field exponents : in the field of critical phenomena , second order phase transitions are those which have power-law divergences in the correlation length and the averaged field fluctuations at the transition . the power laws for the divergences were predicted by landau using general principles of analyticity , later made more precise by thom in catastrophe theory . these predictions fail , and landau recognized that this was a sign of an important new discovery to be made . the discovery was modern renormalization , due to widom , kadanoff , wilson , fisher and developed by many others . kolmogorov turbulence theory : kolmogorov proposed an approximation to turbulence which derived the energy in each mode from an argument which does not require much more sophistication than dimensional analysis . the same theory was reproduced by onsager and heisenberg , probably completely independently , during the wwii years . the k41 theory predicts the velocity-velocity correlation functions in fully developed turbulent flow . the theory was first thought to be exact , but in the 1960s , it was slowly shown to be only a rough first approximation , with new phenomena of intermittency altering the correlation function power laws . steady state cosmology : this was the idea that the expansion of the universe is produced by a field with a positive cosmological constant , so that we live in a desitter vacuum . further , as the universe expands , new h atoms are produced from the expansion so that the universe is in a steady state . this was killed by evidence for a hot big bang , the cosmic microwave background , plus the observation that old galaxies are noticibly different in their statistical properties from modern ones , they are immature and irregular , contrary to steady state predictions . frozen star black-holes : before the black hole theory was advanced , many people , including einstein , proposed that something terrible happened at the horizon , which either led to blow ups in energy , or to freezing matter on the exterior . this is somewhat true , in that it takes forever for an object to cross the horizon from an exterior point of view , but the modern understanding requires that objects have an interior to go into . black hole remnants : this was the short-lived idea the black holes leave behind a small pointlike object with a huge entropy when they decay in the final stages . this was designed to fix the black-hole information paradox . black holes make other universe : this idea was associated with the information loss puzzle--- how can black holes lose information ? they must be making a new universe . cosmological constant suppression due to diverging mode in quantum gravity path integral : this idea is due to coleman , and unlike many of his other brilliant contributions , it turned out to be wrong . the idea there was that the diverging scale-factor path-integral factor in the quantum gravity path integral leads to a zero cosmological constant . this theory was killed by the observation of a nonzero cosmological constant . one could continue giving examples , but it is easier to look at the old literature and find all claims . many of these claims are incorrect , and each one is usually an example of this sort . it is good to know the wrong turns , so as not to rediscover an old new idea . for the most challenging physical phenomena , those which are currently the most difficult to understand , i will have to choose one of the many mysteries . i would probably say : high temperature superconductivity : what makes ceramics superconduct ? it is clearly a purely electronic thing , nothing to do with phonons , but the attraction mechanism is not completely understood . this has been an active subject for 20 years , but i do not see a good answer in the literature . regge theory : how do you produce a regge trajectory from a confining field theory ? this quetion lies at the intersection of string theory and quantum field theories , like qcd . how do clouds form ? how do they separate charges ? the phenomena in clouds is completely ill understood , and important for giving climate science more precise predictions . there are many approximate models here again , there are too many to list , you just explore the literature and find open questions .
i am not quite sure which literature are you looking for , but it should be written in standard textbook . anyway , you are right that it is not that useful except a formulation and exercise . usually , it is written in the following form . given a charge distribution $\rho_{1} ( \vec{\mathbf{r}_{1}} ) $ for object with volume $v_1$ and $\rho_{2} ( \vec{\mathbf{r}_{2}} ) $ for object with volume $v_2$ , we have the electric field : $$\vec{\mathbf{e}} ( \vec{\mathbf{r}_{2}} ) =\frac{1}{4\pi\epsilon_{0}}\int_{v_{1}}\frac{\rho_{1} ( \vec{\mathbf{r}_{1}} ) }{|\vec{\mathbf{r}_{2}}-\vec{\mathbf{r}_{1}}|^{3}} ( \vec{\mathbf{r}_{2}}-\vec{\mathbf{r}_{1}} ) d\vec{\mathbf{r}_{1}} \tag{1}$$ and force $$\vec{\mathbf{f}}=\frac{1}{4\pi\epsilon_{0}}\int_{v_{2}}\int_{v_{1}}\frac{\rho_{1} ( \vec{\mathbf{r}_{1}} ) \rho_{2} ( \vec{\mathbf{r}_{2}} ) }{|\vec{\mathbf{r}_{2}}-\vec{\mathbf{r}_{1}}|^{3}} ( \vec{\mathbf{r}_{2}}-\vec{\mathbf{r}_{1}} ) d\vec{\mathbf{r}_{1}}d\vec{\mathbf{r}_{2}} \tag{2}$$ because of practicality , it is not used that often : it is hard to apply on non-fixed charge distribution . for example , for conductor , the charge itself would change as two object approaching . this internal dynamics is not captured in eq ( 2 ) . when the objects are far away , it is just like a point charge . why do you want to do the cumbersome integration ? there are better tools to handle this situation : multipole expansion . as two objects become closer and closer , you include the monopole first ( i.e. . a point charge ) , then dipolar , then quadrupole . . . this expansion is very systematical and have good physical meaning . people care more about electric field rather than force , as it is more fundamental , so the eq ( 1 ) is more emphasized than eq ( 2 ) .
if you have watched any of the popular science programmes on the higgs boson you have almost certainly got the wrong idea about it . in particular , the title to your question suggests you have heard the description " god particle " once too often since the higgs is not the root of all elementary particles . physicists believe that particles acquire mass through a process called electroweak symmetry breaking . i am not sure that this has been proven , but the theory fits observations so well that everybody believes it . anyhow , the higgs mechanism is one of many mechanisms by which the symmetry breaking could occur . actually it is the simplest and most elegant mechanism , and all the other suggestions have various problems associated with them , so most people 's money is on the higgs , especially now there is a hint of it at the lhc . but there are various other possible mechanisms including a composite higgs theory known as the little higgs , and it is possible that experiment could prove the little higgs theory to be correct . alternatively another symmetry breaking mechanism like technicolor , that does not have a higgs boson at all , could turn out to be correct . so while it currently seems most likely that the simple higgs boson model is correct , the higgs might not be a fundamental particle or might not exist at all . at the end of the day electroweak symmetry gets broken and particles acquire masses , so all the alternatives to the higgs have to do basically the same thing . failing to find a simple higgs boson would not be the end of physics .
measure the angular distance between a star and the distant background stars . repeat 6months later when the earth is on the opposite side of the sum if you know the length of the baseline ( the earth 's orbit ) and the angle then you know the distance to the star . in fact we define the distance to stars in terms of this angle and the earth 's orbit - see http://en.wikipedia.org/wiki/parsec because of the blurring effects of the atmosphere it is difficult to measure angles much less than 1 arcsec , and so determine the distance to stars more than a few parsecs away directly by this method . the hipparcos satelite was able to make much more accurate measurements ( less than 1 milli-arcsec ) and so measured distances 1000x further
notice first that the phase space of any theory is nothing but the space of all its classical solutions . the traditional presentation of phase spaces by fields and their canonical momenta on a cauchy surface is just a way of parameterizing all solutions by initial value data -- if possible . this is often possible , but comes with all the disadvantages that a choice of coordinates always comes with . the phase space itself exists independently of these choices and whether they exist in the first place . in order to emphasize this point one sometimes speaks of covariant phase space . this is well known , even if it remains a bit hidden in many textbooks . for more details and an extensive and commented list of references on this see the $n$lab entry phase space . then notice that the phase space of every field theory that comes from a local action functional ( meaning that it is the integral of a lagrangian which depends only on finitely many derivatives of the fields ) comes canonically equipped with a canonical liouville form and a canonical presymplectic form . the way this works is also discuss in detail at phase space . a good classical reference is zuckerman , a more leisurely discussion is in crncovic-witten . this canonical presymplectic form that exists on the phase space of every local theory becomes symplectic on the reduced phase space , which is the space obtained by quotienting out the gauge symmetries . this quotient is often very ill-behaved , but it always exists nicely as a " derived " quotient , and as such is modeled by the bv-brst complex ( as discussed there ) . the whole ( lagrangian ) bv-brst machinery is there to produce the canonical symplectic form existing on the reduced phase space of any local action functional . since the einstein-hilbert action and all of its usual variants with matter couplings etc . is a local action functional , all this applies to gravity . recently fredenhagen et al . have given careful discussions of the covariant phase space of gravity ( and its liouville form ) , see the references listed here . it follows that the " dimension " of the covariant phase space of gravity does not depend on the " size of the universe " , nor does it make much sense to ask this , in the first place . a given cosmology is one single point in this phase space ( or rather it is so in the reduced phase space , after quotienting out symmetries ) . however , you might be after some truncations or effective approximations or coarse graining to full covariant gravity . for these the story might be different .
check your algebra in the part where you need to compare $ ( \partial b ) c$ between the two normal orderings , i got $\lambda ( \lambda-1 ) / ( 2z^2 ) $ for the difference . the result follows immediately . ( by the way , thanks for the symbol ${}^{{}_\circ}_{{}^\circ}$ , i have been looking for that forever . )
for clarity , there is a common misconception about plasma here . plasma when being introduced for the first time to someone who does not know what it is , it is called " the fourth state of matter " which is an inaccurate description of it . since this term is used for introducing some one to plasma , it is no big deal . when a material changes from a distinct phase to another , it goes through a physical process called phase transition . when gas becomes plasma , it does not go through the standard phase transition . hence plasma-in a general sense-can not be regarded as a distinct phase as solid , liquid and gas phases . it is a phase of the gaseous state . in certain rare cases however , transition from gas to plasma can be described as phase transition . plasma by definition is a mixture of free electrons and their ions ( possibly negative ions ) . you need enough energy to liberate electrons from atoms . roughly speaking , when you put that energy in a solid , energy might be dissipated as heat . if you put that energy in a liqued , energy might be dissipated in vaporization . if you put it in a gas it goes into breaking atoms and molecules ( creating plasma ) . the following figure makes it clearer hopefully that was useful
in your post when you say you ' know ' the position and momentum of a single photon you really do not know anything , you are just making a prediction , not making a measurement . in your head you are basically assuming classical physics and using the initial parameters of the system to calculate the final parameters . in order to actually know any properties about a system you will have to perform a measurement , and to really say anything conclusive you will have to do this many times . take your single photon source and measure the momentum and position of the outgoing photons numerous times - the product of the standard deviation in momentum and position will be greater than $\frac{\hbar}{2}$ .
you are quite correct , you had write the opposite velocity with a negative sign . you just need to decide what sign convention to use . in your example you are only considering motion in one dimension , so you could take motion to the right to be positive in which case motion to the left would be negative . or you could take motion to the left positive and motion to the right negative . it does not matter what convention you use as long as you are consistent . suppose we take motion to the right to be positive so in your first example both a and b are moving to the right . the velocity that a measures is $v_b - v_a$ , so in your first example it is 105 - 100 or 5 m/s . if b is moving in the other direction the velocity $v_b$ is -105 m/s . it is negative because it is moving to the left . the velocity a measures is now -105 - 100 or -205 m/s . the minus sign tells us the motion is to the left so a sees b moving to the left at 205 m/s .
so the key is to understand that $\nabla^\dagger = - \nabla$ . to see why this should be true , we go back to the definition of the adjoint of an operator , namely $$\left&lt ; \phi \right|\left . a \psi \right&gt ; = \left&lt ; a^\dagger\phi \right|\left . \psi \right&gt ; \implies \int d^dx \phi ( x ) ^* {\hat a} \psi ( x ) = \int d^d x \left ( {\hat a}^\dagger \phi ( x ) \right ) ^* \psi ( x ) $$ in particular , for the differential operator $$\int \phi^* \nabla \psi = - \int \nabla \phi^* \psi = \int \left ( - \nabla \phi \right ) ^* \psi \implies \nabla^\dagger = - \nabla$$
the diagram looks like the fat man bomb that was dropped on nagasaki . the wikipedia article gives lots of info on the design if you are intereted in pursuing it further . the casing is just to make it aerodynamically stable so it fell in a controlled way . the bomb itself is spherical so the case could be spherical as well if it were not for aerodynamic requirements .
if the magnetic dipoles in a material are ordered , the material has a lower entropy because there are many fewer ways how the spins may be oriented if most of them ( or all of them ! ) are required to be aligned . such an alignment also reduces the heat capacity because before the dipoles got aligned , the orientation ( direction ) of each dipole was a degree of freedom that was storing something like $o ( kt ) $ of energy where $k$ is boltzmann 's constant and $t$ is the temperature in kelvins . however , when the dipoles are kept aligned , the direction is no longer variable so the heat capacity from each dipole decreases by $o ( k ) $ or so , or $o ( r ) $ if expressed per mole . it is a similar difference as the heat capacity of monoatomic vs diatomic gas , which carry $3kt/2$ vs $5kt/2$ per molecule , respectively . ( $3/2$ is from 3 linear momenta and the extra $2/2$ is from the longitude and latitude remembering the rotational motion and/or direction of the diatomic molecule . ) note that the diatomic gas has a greater energy per molecule which scales with $t$ , and therefore also steeper dependence of the energy on the temperature ( the capacity is $5r/2$ ) because the energy may also be stored in the random rotations of the diatomic molecule that do not exist for the monoatomic molecule . the random chaotic thermal rotation of the dipoles is analogous to the random rotation of the diatomic gas molecules and it becomes banned or indistinguishable for monoatomic gas as well as the magnetic material with aligned spins which is why the heat capacity decreases analogously to the decrease from $5r/2$ to $3r/2$ in the transition from a diatomic molecule to a monoatomic one .
if you observe a star of radius $r$ from the distance $l$ , you will see it as a small disk under the angle $2r/l$ so the solid angle the photons from the star will cover will scale like $ ( r/l ) ^2$ . that is the percentage of the retina that will be receiving photons : the solid angle measures the " percentage of directions " in which the photons from the star are flying . the number of photons from the star that hit your eyebulb scales like $ ( r/l ) ^2$ as well ( the dependence $1/l^2$ is what i care about here ) , because they are divided to all points on the sphere of area $4\pi l^2$ , so by dividing these two expressions , you may easily see that the number of photons per unit area of the retina is actually independent of $l$ . the star will look smaller as it gets further but the number of photons per unit time that hit a small area of the retina is $l$-independent . if you are really worried that the star does not emit enough photons to satisfy your eyes , note that the sun emits roughly $4\times 10^{44}$ photons each second . the earth-sun distance is roughly 150 million km which is 15 trillion times the radius of the eyebulb . square it and you will still get that the number of eye-sized areas on the surface of the 1-au-radius-large sphere is just of order $10^{26}$ , still giving you $10^{18}$ photons to each eyebulb per second . so if you allow 100 photons per eyebulb to be enough to see it , you may still allow the star to be $10^{8}$ times further than the sun . the sun is 8 light minutes and if multiplied by 100 million , you get something like 200 light years . so with this minimal required number of photons per eyebulb ( 100 per second ) , you may see stars up to hundreds of light years away ( i can not ) . of course , telescopes are collecting starlight from a much larger area than the eyebulb ( and they may also patiently collect the photons for a much longer time ) so they may see stars much further than that .
1 ) is correct . the wrong reasoning about 2 ) is that what you have in mind is probably newtons law for point masses . when the sphere is close to the doghnut the gravitational force will be more complicated , but still point towards the centre of the doughnut due to the symmetries in the situation . it will however stay finite , because all points of the sphere still have a finite distance to all points of the doughnut .
if the moon were exactly the same as the earth , then sure , there is no major reason to suspect it would be any different . it is in the same orbit around the sun as us , so it gets heated by the same amount . this would place it in the habitable zone . however , habitability is not the same as being in the habitable zone , and the detailed answer depends on how you make the surface gravity match that of earth . the surface gravity of a sphere of radius $r$ and average density $\rho$ is $$ g = \frac{4\pi}{3} g \rho r . $$ most rocky bodies in the solar system have about the same density - that of a rock - so making the moon 's gravity match the earth 's is just a matter of making it bigger . essentially it would become earth 's twin in every way . on the other hand , maybe you intended to keep the size the same . in that case you would have to increase the density . it is not clear what you would make the interior out of , but it is pretty certain you will not get the same geology as on earth . for one , smaller bodies cool off too fast to be geologically active at this age ( roughly 5 billion years ) . you see , when the planets condensed out of the gas and dust swirling around the sun billions of years ago , they were hot - gravitational potential energy went down , and so thermal energy went up . their heat capacity is proportional to their volume , but their heat losses are proportional to their surface areas . thus objects with high surface area-to-volume ratios ( i.e. . small things ) cool quickly . the thing is , earth 's geologic activity probably had a large role in building up and maintaining the atmosphere and oceans we know and love . in either case , there is also the problem of tidal locking . it is suspected by some that having tides was crucial for the development of life . the moon is already tidally locked with the earth - we only ever see one face of it - so it has no tides . if you scaled it up , you might tidally lock the earth as well . the moon would essentially be in a geostationary orbit , and we would not have tides . this is the case for the pluto-charon system , for instance .
it is a stupid mistake . when r is less than the radius of the cylinder , the electric field will be 0 . i solved the problem not taking this into account and just plugged in the numbers . my method is perfectly valid for any r greater than 4cm .
you say : terminal velocity depends on two things- surface area and speed but i think you are getting slightly mixed up about the terminology . the drag ( i.e. . air resistance ) depends on surface area and speed , but the terminal velocity is the speed and it just depends on the surface area ( and air temperature , density , etc , etc that we will assume is constant ) . you say ; with the parachute you have a larger surface area but lower speed and this is quite correct but the speed is the terminal velocity so with the parachute you have a larger surface area but lower terminal velocity .
waves appear in nature and are described by wave equation s , second-order linear partial differential equation for the description of waves – as they occur in physics all such equations have time dependent solutions and what they have in common is the oscillating behaviour in time that allows to assign a wavelength and describe the waves observed . now in quantum mechanics the description of the behavior of nature is dependent on differential equations of this type . the first studied is the schrodinger equation and the link provides a good historical description of how it became evident that in the microcosm the behavior of particles was following a wave equation . what is important to keep in mind is that in quantum mechanics the waves are probability waves , i.e. the probability if you do an experiment , like the double slit experiment , to find a particle in space at the time you look is governed by a wave solution . this is in contrast to other waves in physics which are variations in time on a medium , or in classical electromagnetism on changing fields . so the relationship is the mathematical formulation of the differential equations describing nature in the two frameworks , not a one to one correspondence . as i mentioned in my comment in electromagnetism , the frequency of the electromagnetic field described by classical electrodynamics appears in the energy of the photon ( the particle form of electromagnetism ) in the identity e=h . nu . the interference pattern seen in the double slit experiment will display the frequency of light nu . if you continue your studies in physics you will be able to understand how the microcosm described by quantum mechanics leads to the macrocosm we call " classical physics " smoothly .
yes current does flow until $q/v$ equals $c$ . in the case of two capacitors in series , the effective capacitance is $1/ ( 1/c_1 + 1/c_2 ) $ , because the voltage $v$ is effectively divided between them . maybe an easier way to see this is to define inverse capacitance $k = 1/c = v/q$ which is , for a particular capacitor , the amount of voltage $v$ needed to put a given charge $q$ on one plate ( and $-q$ on the other ) . then it is easier to see that $k = k_1 + k_2$ , because the voltages across them sum , because they are in series . the charge on the right plate of $c_1$ comes from the left plate of $c_2$ .
yes , your friend is right . within electrostatics , an electric field $\vec{e}$ should be curl-free $\vec{\nabla} \times\vec{e}= \vec{0}$ . the drawn electric field lines looks like the electric field is of the form $$ e_x=e_x ( y ) , \qquad e_y=0 , \qquad e_z=0 , $$ cf . the rule that to depict the magnitude $|\vec{e}|$ , a selection of field lines is drawn such that the density of field lines ( number of field lines per unit perpendicular area ) at any location is proportional to $|\vec{e}|$ at that point . here the $x$-axis is horizontal , the $y$-axis is vertical , and the $z$-axis perpendicular to the plane . this is only curl-free if $e_x=e_x ( y ) $ is independent of $y$ , which it is not on the figure .
the formula is just a force balance . if the contact line is stationary the forces at it must balance so taking the horizontal component of the forces gives you : $$\sigma_{gas-liquid} cos \theta + \sigma_{liquid-solid} = \sigma_{gas-solid}$$ and hence the formula you quote . if you replace the gas with a liquid the force balance calculation is just the same , so the formula remains valid .
if you mean special conformal transformation x-> 1/x conformal invariance of maxwell equations is known since 1909 . see here : http://cts.iisc.ernet.in/personnel/pages/asinha/draft1shouvik.pdf or here : http://arxiv.org/pdf/hep-th/9701064
the oldest population i stars are about 10 billion years old . those stars have 0.1 times the metal abundance of the sun ( source ) .
it is the " latent heat of fusion " produced when water freezes that protects the crop , see p . 3 here : http://fruit.wisc.edu/wp-content/uploads/2011/02/understanding-frost-in-fruit-crops1.pdf
i will once again state that string theory , any theory , cannot be proven right by any experiment . the experiment might validate the theory , i.e. come as a result of a prediction from the theory . at the moment there does not exist one string theory in the manner that there exists one general relativity theory . there are many models based on string theory , though . why such an interest ? because at the moment string theories are the only theories that can accommodate the standard model of particle physics and at the same time allow for the quantization of gravity , which has been the holy grail of theoriticians the past fifty years . that is they promise a " theory of everything , toe ) . what might disprove the usefulness of string theories for a toe would be if supesymmetry were falsified at the lhc , for example . if nothing is seen other than the higgs at the lhc , ss would seem as a nice try but bad luck . then the usefulness of strings becomes doubtful . if ss is seen in the lhc and studied as well as the sm in the international linear collider to be built in the future , then strings will be good as candidates of a toe .
for those who upvoted me , i am sorry . i just checked my equations again and i found that i made stupid mistakes when writing the torque equations . i made a mistake in writing the length of the lever arms of $w-w_{arm}$ and $w_{board}$ , it should have been $l-x_a$ and $d-x_a$ instead of $l$ and $d$ . it turns out that the method in my previous answer does not work because every time we find $w_{arm}$ , we will not find it alone . it always sticks together with $l$ as one $w_{arm}l$ . at best we can only get $w_{arm}l$ as a single variable . so unless we conduct another measurement using a different method that has nothing to do with measuring center of mass , we will not be able to find $w_{arm}$ . even worse , even if we come up with other experiment of locating the center of mass or anything related to torque with various body orientations . we will always end up with $w_{arm}$ sticking with another quantity with dimension of length . and adding the number of measurements also will not help . so we have to give up trying to find $w$ with torque method and be satisfied with $w\times [ length ] $ . we have to figure out another experiment to find $w\times f ( length ) $ . the only mechanical experiment that i can think of is an experiment involving centrifugal force , since it is a dynamical experiment it is hard to measure . so i think perhaps hennes ' method is better . but if we still insist to use mechanical method here is one way to do it : let 's say we want to calculate the weight of someone 's arm . first , place a thin board horizontally and support it with a pivot and a weigh scale . ask that guy to stand up on it but with his hands oriented straight horizontally . where $n$ is the reading of the scale times the gravitational acceleration $g$ . the balance in torque gives $w_{arm} ( l-x ) +n ( l+x ) = ( w-w_{arm} ) x+w_{board} ( d+x ) $ all quantities involved in two equations above can be measured using a scale and a meter stick except $w_{arm}$ and $l$ . now for the second experiment , we need two high speed camera and a scale with high refresh rate . hang a paper above the guy 's head and ask him to swing his arms quickly while keeping them straight until he hits the paper . while he is doing that , record the movement of the hand using one camera and record the reading of the scale using another camera . the angular velocity $\omega$ of the hand just before hitting the paper can be obtained from the video recorded by the first camera . angular velocity is much easier to calculate than velocity since we do not need to worry about parallax so much , but still it is not an easy task . then we can also obtain the normal force at the instant the hand hits the paper from the second video from the reading of the scale when the voice of hitting paper is present . let 's say the reading is $n'$ $\sigma f_y$ at the moment when the hand hits the paper gives $n'=w-w_{arm}+w_{arm} ( 1+\frac{\omega^2 l}{g} ) $ now we can substitute $l$ from one equation to the other equation to obtain $w_{arm}$ . note that $w_{arm}$ that we get is the mass of two arms , so we need to divide the final result by a factor of two to get the mass of an arm . and we can also measure the mass of a leg using the similar method .
0 . caveat lector : this was done before i drank my morning coffee , so there may be some errors in the reasoning ( well , the physical reasoning , the mathematics should be kosher ) . 1 . perfect fluid . so we have two stress-energy tensors here . one is the stress energy tensor for a perfect fluid $$\tag{1}t^{\alpha\beta}_{\text{fluid}} = \rho \ , u^\alpha \ , u^\beta + p \ , h^{\alpha\beta}$$ where we have the worldlines of the fluid 's particles have velocity $u^\alpha$ the projection tensor $h_{\alpha\beta} = g_{\alpha\beta} + u_\alpha \ , u_\beta$ projects other tensors onto hyperplane elements orthogonal to $u^\alpha$ the matter density is given by the scalar function $\rho$ , the pressure is given by the scalar function $p$ . we had need extra terms if there were heat flow or shear involved . 2 . scalar field . now , we have another distinct stress-energy tensor for a massless scalar field : $$\tag{2}t^{\mu \nu}_{\text{scalar}} =\partial^{\mu}\phi\ , \partial^{\nu}\phi-\frac{1}{2}g^{\mu \nu}\partial_{\rho}\phi\ , \partial^{\rho}\phi$$ we would use this equation when modeling , e.g. , massless pions ( or some other massless spin-0 field ) . 3 . problem : are these two related ? now if we take our matter density to be , in the appropriate units , $$\tag{3a} \rho = 1 + \frac{1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ and the pressure $$\tag{3b} p = \frac{-1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ then ( 2 ) resembles ( 1 ) . this is after pretending $\partial^{\mu}\phi=u^{\mu}$ , which terrifies the original poster ( but that is what condensed matter physicists do , so i suppose i could end here content ) . is this kosher ? we should first note if we wanted to take the derivative of some function along the worldline $x^{\mu} ( s ) $ with respect to the " proper time " ( length ) $s$ we have $$\tag{4} \frac{\mathrm{d}f}{\mathrm{d}s}=\frac{\mathrm{d}x^{\mu}}{\mathrm{d}s}\frac{\partial f}{\partial x^{\mu}}$$ by the chain rule . for general relativity , we use the " comma-goes-to-semicolon " rule , but for a scalar quantity $f$ we have $$ \nabla_{\mu}f = \partial_{\mu}f . $$ ( if this is not obvious , the reader should consider it an exercise to prove it to him or herself . ) the punchline : identifying $\partial^{\mu}\phi=u^{\mu}$ is kosher . how ? observe in equation ( 4 ) the guy in front , the $\mathrm{d}x^{\mu}/\mathrm{d}s$ is just some vector . so in the very , very special case that equations ( 3a ) and ( 3b ) hold , and $\mathrm{d}x^{\mu}/\mathrm{d}s= ( 1,0,0,0 ) $ , we see that we can indeed recover the first stress-energy tensor as a special case of the scalar field 's stress-energy tensor .
yes , this should be possible using a chiral material or the faraday effect . first example . second example . however , the calcite + wave plate system is probably a lot easier .
yes there is . i use the summation convention throughout ; repeated indices are summed over 1,2,3 . begin with the canonical commutation relations ( ccrs ) \begin{align} [ x_i , p_j ] = i\hbar\delta_{ij} i , \qquad [ x_i , x_j ] = 0 , \qquad [ p_i , p_j ] = 0 . \end{align} define the components of orbital angular momentum as follows : \begin{align} l_i = \epsilon_{ijk}x_jp_k \end{align} prove your desired identities by applying the definition of the angular momentum components and by repeatedly using the ccrs . i will leave the details to you ; it is a good exercise to get comfortable with using the ccrs to prove stuff . it actually turns out to prove useful to first prove the following identities which encode the fact that the $x_i$ are components of a " vector operator " and so are the $p_i$ . \begin{align} [ x_i , l_j ] =i\hbar\epsilon_{ijk}x_k \end{align}
you are using standard si units for all the other terms ( no unit multipliers ) . if you look at the specific heat of water ( at standard atmospheric pressure ) you will find the specific heat is $\approx 4.2 \mathrm{kj\ , kg^{-1}\ , k^{-1}}$ or $\approx 4200 \mathrm{j\ , kg^{-1}\ , k^{-1}}$ . it is the latter you want to use . i hope this helps .
the localization or delocalization of the excess charge in conductors and insulators can be understood in a way similar to the uncharged case using band theory . please refer to the first diagram in : http://en.wikipedia.org/wiki/work_function for simplicity let us consider the zero temperature case . the fermi energy $e_f$ can be thought of as a level which separates the occupied states from the empty states . now , as you may know , the fermi energy in a metal would lie in a band . in other words , in metals you have half filled bands ; this is why electrons , near the fermi energy , are delocalized in metals . in semiconductors , however , the fermi energy lies within the band ( as shown in the figure ) leaving all the bands either completely filled or empty . as a result , the electrons are localized . since the fermi energy separates the filled and empty states , it can , intuitively , be thought of as the surface of a fluid in a container ; the fluid is analogous to the electrons in the system . now , as you add or remove the fluid , its surface will either rise or drop respectively . changes in the fermi energy can be visualized in the same way . there is , however , one caveat : the vacuum energy $e_{vac}$ will also change , in addition to $e_f$ , as excess charge is introduced . $e_{vac}$ is considered as the energy at which the electron is no longer bound to the solid . if the solid is charged positively or negatively , it will be harder or easier for the electron to escape respectively . as a result , one only considers changes in work function $\phi$ or electron affinity $e_{ea}$ . the former is often used in the case of metals and the latter in the case of semiconductors or insulators . to sum it all up , excess charges will result in changes in $\phi$ and $e_{ea}$ . after these changes have occurred , by the introduction of access charges , it is a question of where the fermi energy sits . depending on that , the electrons will either be localized or delocalized . for a reasonable value of excess charge the fermi energy will only move by a small amount , and will still typically lie in one of the bands or in the band gap in a metal or insulator respectively . this is why the excess charge will be delocalized , and cover the entire surface of the metal ; whereas the excess charge will be localized in an insulator . if you want to get a better feel for how $e_f$ , $e_{vac}$ , $\phi$ , and $e_{ea}$ change as excess charge develops on metals , insulators , and semiconductors , you can take a look at chapter 2 of : http://www.amazon.com/field-effect-devices-volume-edition/dp/0201122987/ref=sr_1_1?ie=utf8qid=1348762717sr=8-1keywords=field+effect+devices
the stability argument is as follows--- the geon system will have some mass , and it is made out of massless fields orbiting in closed orbits , so if you make the geon a little smaller with the same total energy , you expect the gravity to win and the massless fields to collapse into a black hole , and if you make the geon a little bigger , you expect the massless stuff to disperse to infinity . this argument is hard to make rigorous , because you need to find a way to rescale the nonlinear gravitational theory . so wheeler studied this situation extensively , with the hope of finding a stable geon . he did not find one , and even if there were one , we already have a good model of elementary particles in the black hole solutions and their quantum counterparts , so it is not clear that such a solution would be useful . but it is a strangely neglected field . perhaps there is an easy argument that establishes instability of all geons , but it is going to be tough , because the geons can make arbitrarily complicated links of light going through each other , pulling each other into stable orbits .
the morally correct answer is that the measurement of one spin in the epr-entangled pair eliminates the entanglement as it picks a particular factorized basis vector for the measured spin , and the total wave function therefore has to factorize to $\psi_\text{just measured}\otimes \psi_{\rm something}$ . if you parameterize the multi-fermion states in terms of " fermion 1" and " fermion 2" , you will have to antisymmetrize , so no multi-particle wave function will ever tensor factorize . ( this is true even for bosons with the exception of the states where all the bosons are placed to the same one-particle state . ) however , as you say , this obstacle to factorization ( in the form of the required antisymmetrization , and similarly symmetrization for bosons ) is sort of " trivial " . there is a technical way to support the claim that this state is really not entangled . if you write the " subsystems " not as " fermion 1" and " fermion 2" but as " region around $r_a$" and " region around $r_b$" , using your notation , then you will see that the wave function ( al ) for the whole space is almost accurately tensor factorized to the wave functional that only depends on the fields near $r_a$ ( where a spin-down electron excitation is added ) and the fields around the point $r_b$ ( where the spin-up electron excitation lives ) . $$ \psi_{qft} = c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) \otimes c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) $$ of course , the formula above is just a suggestive way to show that the ordinary , correct state of a quantum field theory $$ c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) |0\rangle $$ is really a simple tensor product of a sort . for this reason , the notion of " entanglement " is usually modified for identical particles so that the unentangled states are not just the states having the form of the simple tensor product but all states obtained as ( anti ) symmetrization of a tensor product .
kamland borexino has set moderately strict limits of the total power of a central geo-reactor . see for instance geo-neutrino : experiments ( pdf link ) a talk by one of my colleagues . ( jalena notes that borexino 's limit is the strongest one going , but kamland was the leader for a while . ) the upper extreme of these limits is less than half the total geological power , but quite non-trivial . the bottom goes all the way to zero . there is also a recent paper ( that i have yet to read ) on a variation of this idea : the kamland-experiment and soliton-like nuclear georeactor . part 1 . comparison of theory with experiment . i have no idea , how the rest of these ideas stand .
you dont give magnitude of pounds , you give magnitude of , say mass ( as here ) . so the magnitude is 120 pounds .
at the fundamental level there are four forces associated with the interactions of particles in the microcosm : the strong , the weak , the electromagnetic and the gravitational one . the last two are long range forces and influence the behavior of matter macroscopically too , in a collective manner . macroscopically phenomena of absorption can be observed which at the microscopic level end up being interactions of fundamental particles . for example : a sponge absorbs the water . a black wall absorbs the visible light falling on it etc . at the microscopic level all these are interactions of the electromagnetic field of the molecules and atoms with the incoming particles ( water molecules or photons in the two examples ) . one has to be clear about the scale of the phenomenon .
as richard points out , you can derive the second equation by setting $\psi$ to be a position eigenstate in the first one . doing that , you turn the general case $$\langle \mathbf{r}\lvert \mathbf{l}\rvert \psi\rangle =\mathbf{r} \times ( -i\hbar\nabla\langle \mathbf{r}|\psi\rangle ) $$ into the relation $$\langle \mathbf{r}\lvert \mathbf{l}\rvert \mathbf{r}'\rangle =\mathbf{r} \times ( -i\hbar\nabla_\mathbf{r}\langle \mathbf{r}|\mathbf{r}'\rangle ) =\mathbf{r} \times \left ( -i\hbar\nabla_\mathbf{r}\delta ( \mathbf{r}-\mathbf{r}' ) \right ) . $$ in here , you can change the $\mathbf{r}$ 's into $\mathbf{r}'$s using the fact that both vectors are equal at the support of the delta function . thus you can change $\mathbf{r}\times$ for $\mathbf{r}'\times$ , but the derivative is a bit trickier : since the argument of the delta fuction is $\mathbf{r}-\mathbf{r}'$ , its derivatives w.r.t. $\mathbf{r}$ differ from its derivatives w.r.t. $\mathbf{r}'$ by a sign , and you must change $\nabla_\mathbf{r}$ for $-\nabla_{\mathbf{r}'}$ . with this , then , $$\langle \mathbf{r}\lvert \mathbf{l}\rvert \mathbf{r}'\rangle =\mathbf{r} \times \left ( -i\hbar\nabla_\mathbf{r}\delta ( \mathbf{r}-\mathbf{r}' ) \right ) =\mathbf{r}' \times \left ( +i\hbar\nabla_{\mathbf{r}'}\delta ( \mathbf{r}-\mathbf{r}' ) \right ) =\mathbf{r}' \times \left ( +i\hbar\nabla_{\mathbf{r}'}\langle \mathbf{r}|\mathbf{r}'\rangle\right ) . $$ once it is in this form , you simply have a global factor of $\langle\mathbf{r}|$ , which you can simply " cancel out " . ( more formally , since the $|\mathbf{r}\rangle$ are a complete set , the projections on the $\langle \mathbf{r}|$ completely determine any vector . or , if you prefer , simply multiply the equation by $|\mathbf{r}\rangle$ and integrate over all $\mathbf{r}$ . ) doing that , then , and dropping the primes , you get , finally $$\mathbf{l}\rvert \mathbf{r}\rangle =\mathbf{r} \times \left ( +i\hbar\nabla_{\mathbf{r}}|\mathbf{r}\rangle\right ) \tag1$$ as you wanted to get . i must say , though that this relation is not particularly useful . what is useful , though , is its adjoint relation , which you can get from the original $$ \langle \mathbf{r}\lvert \mathbf{l}\rvert \psi\rangle =\left ( \mathbf{r} \times ( -i\hbar\nabla ) \langle \mathbf{r}|\right ) |\psi\rangle $$ by simply " cancelling out " $|\psi\rangle$ . ( or , more formally , by noting that the linear functionals on both sides coincide for all $|\psi\rangle$ , and must therefore be equal as linear functionals . ) this gives simply $$ \langle \mathbf{r}\lvert \mathbf{l} =\mathbf{r} \times ( -i\hbar\nabla_\mathbf{r} ) \langle \mathbf{r}| , \tag 2$$ which is evidently the adjoint of ( 1 ) . ( what is remarkable is that the vector calculus remains valid . ) the reason i say that this is the form that is actually useful is that you very , very rarely deal with position ket $|\mathbf{r}\rangle$ , as they are very much not physical states , but you do deal regularly with position bras $\langle \mathbf{r}|$ , as they are an essential ingredient in well-written position representations . the form ( 2 ) then lets you find the position-representation wavefunction of the transformed vector $\mathbf{l}|\psi\rangle$ from the original wavefunction $\langle \mathbf{r}|\psi\rangle$ . this is analogous to the way to make precise the intuition that $\mathbf{p}$ equals the derivative $-i\hbar \nabla$ , by considering its actions on bras instead of kets , to get $$\langle \mathbf{r}|\mathbf{p}=-i\hbar\nabla_\mathbf{r}\langle \mathbf{r}| , $$ as i have said before in this answer . while this looks slightly unintuitive at first , it is actually more useful if you use it right .
assume that the point mass , $m$ has two tiny thrusters , mounted so as to exert purely tangential force in the plane of the circular motion , one clockwise , and the other counter-clockwise . the magnitude of the constant velocity of the mass is $v$ , and the radius of the circle is $r$ . measure the position of the point mass in the standard cartesian coordinate way : angles are measured from the positive x-axis , counter-clockwise positive . at the point where the mass is at a position angle $\theta$ . the total radial force inward on the mass , $f_r$ is given by the centripetal force equation:$$f_r=\frac{mv^2}{r}$$ there are two forces that supply this radial force : the tension , $t$ in the string , and the inward radial component of the force of gravity:$$f_{g-r}=mg\sin ( \theta ) $$so:$$\frac{mv^2}{r}=t+mg\sin ( \theta ) $$and:$$t=\frac{mv^2}{r}-mg\sin ( \theta ) $$note that this implies that:$$v &gt ; =\sqrt{rg}$$ or the string tension will become negative near the top of the circle , an impossibility . the conditions of the question also require that at all times the net tangential force , $f_t$ , be zero . the tangential component of the force of gravity , $f_{g-t}$ is given by:$$f_{g-t}=-mg\cos ( \theta ) $$where a positive force implies counter-clockwise force . the thrusters are needed to supply the exact opposite force to the mass at all times .
think in units of $c=1$ . if $\frac{∂x}{∂t}=\frac{∂t}{∂t}$ , then you are moving at the speed of light . the faster you are , the more your world line is tangent to the light cone . the condition just says that you are moving at non-relativistic speed , i.e. about tangent to the time axis in a minkowski-diagram . the term is $g∂g= ( \eta+h ) ∂ ( \eta+h ) = ( \eta+h ) ∂h==\eta∂h+o ( h∂h ) $ . in minkowski coordinates , you have $∂\eta=0$ in any case . and he probably says somewhere , that the pertubation $h$ does not vary too much , so that $∂h$ is also small and the second order expression $h&#183 ; ∂h$ clearly does not contribute . you also messed up an index in "$-\frac{1}{2}\eta^{\mu\lambda}\partial_{\gamma}h_{00}$" .
current cosmological theorists suppose that the universe is exactly identical , no matter where it is viewed from , so long as it is viewed at the same time . at the time of the big bang , the distances between any two given points seems to shrink to zero ( or some nonzero value that we supposedly will derive from quantum mechanics ) . the conclusion is that the big bang happened everywhere , all at once . this is also how you get out of the ' was the big bang a black hole ? '-type questions : even though you had large concentrations of matter at times close to the big bang , they were spread out over all space , which is different than just having a clump of matter with finite extent ( the second thing would collapse to a black hole ) .
1 . not all states produced by $\text{cnot} \ ; ( h \otimes i ) $ are entangled . for the first part of your question : no , not all states which arise as the output of $\text{cnot} \ ; ( h \otimes i ) $ are entangled . specifically , you can consider $$\begin{align*} |\psi\rangle \ ; and =\ ; ( h \otimes i ) \ ; \text{cnot} \ ; \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \bigr ] \\ \ ; and =\ ; ( h \otimes i ) \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle \bigr ) \bigr ] \\ \ ; and =\ ; \tfrac{1}{2} \bigl ( |00\rangle + |01\rangle + |10\rangle - |11\rangle\bigr ) , \end{align*}$$ which is a maximally entangled state ( notice from the second line above that it differs from a bell state by a local unitary ) . however , by construction , if you apply $\text{cnot} \ ; ( h \otimes i ) $ to $|\psi\rangle$ , you will get back out the state $$ \text{cnot} \ ; ( h \otimes i ) \ ; |\psi\rangle \ ; =\ ; \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \ ; =\ ; |+\rangle\otimes|0\rangle\ ; , $$ which has no entanglement at all . even if you are interested only in inputs which are product states , we can see that the circuit maps $|+\rangle \otimes |0\rangle$ to another product state &mdash ; specifically , $|0\rangle|0\rangle$ . 2 . all maximally entangled two-qubit states can be easily described by a similar circuit to $\text{cnot} \ ; ( h \otimes i ) $ acting on the standard basis . for the second part of your question : if you allow yourself arbitrary single-qubit unitaries , then for any maximally entangled two-qubit state $|\psi\rangle$ , you can certainly construct a circuit which constructs $|\psi\rangle$ from standard basis states , and which allows you easily to see that the state is maximally entangled . every two-qubit state has a schmidt decomposition , $$ |\psi\rangle \ ; =\ ; u_0|\alpha_0\rangle|\beta_0\rangle \ ; +\ ; u_1|\alpha_1\rangle|\beta_1\rangle \ ; , $$ where $u_0 \geqslant u_1 \geqslant 0$ &mdash ; in particular , $u_1 = 0$ for $|\psi\rangle$ a product state &mdash ; and where $\{ |\alpha_0\rangle , |\alpha_1\rangle \}$ and $\{ |\beta_0\rangle , |\beta_1\rangle \}$ are each orthonormal bases for $\mathbb c^2$ . in order for $|\psi\rangle$ to be maximally entangled , we need $u_0 = u_1 = \tfrac{1}{\sqrt 2}$ . consider single-qubit unitary matrices $a$ and $b$ , such that $$\begin{alignat*}{4} a |x\rangle \ ; and =\ ; |\alpha_x\rangle and \quad and \text{for $x \in \{0,1\}$} , \\ [ 1ex ] b |y\rangle \ ; and =\ ; |\beta_y\rangle and and \text{for $y \in \{0,1\}$} ; \end{alignat*}$$ then we can describe $|\psi\rangle = ( a \otimes b ) \ ; \text{cnot}\ ; ( h \otimes i ) \ ; |0\rangle|0\rangle$ , that is , the effect of applying $ ( a \otimes b ) $ to the bell state $|\phi^+\rangle = \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle ) $ . furthermore , it is not hard to show that any standard basis state is mapped by that circuit to some maximally entangled state similar to $|\psi\rangle$ , and that any circuit of this form ( whatever $a$ and $b$ might be ) maps any standard basis state to a maximally entangled two-qubit state .
first of all , nantennas in general do not violate the second law of thermodynamics , so they are not perpetual motion machines of second kind . as long as the total entropy goes up , the second law is obeyed . in other variables , it really means that a part of the incoming heat has to heat the nantenna up but there may still be a lot of energy left for energy production , much like in any other heat engine . the wikipedia suggestion that natennas could violate the second law only referred to a particular application hypothesized by mr novack . if he could be cooling the room while getting energy out of it , and if the gadget to cool the room were not connected to any cooler heat bath , then it would indeed be a perpetual motion machine of second kind and it would be impossible . the reason why nature makes it impossible is kind of trivial . if the room has temperature $t$ , then the nantenna or " power plant " may only be kept at the same temperature $t$ if there is equilibrium . but if that is the case , the nantenna emits thermal radiation , too . so even if it absorbs some incoming radiation , it still radiates its own . they are balanced and the energy gain is zero . solar cells and " legitimate applications " of nantennas can only create energy because they work with incoming light whose " own " temperature is higher than the temperature of the solar cell or nantenna itself . for example , solar radiation has the temperature comparable to 5,500 celsius degrees . the solar cells are effectively heat engines operating between this high temperature and a much lower temperature of the ground . the same is really true about life on earth , too . the energy from the sun may be converted and is often converted to useful energy or work because the high-energy photons from the sun – which correspond to a high temperature and therefore a low entropy per unit energy ( $e\sim ts$ ) – are processed on earth and the energy is finally emitted in much lower-temperature " infrared " thermal photons – which carry a higher entropy . so the entropy can go up even if a part of the incoming energy is converted to useful work . the temperature inequality between the solar surface ( and the solar radiation ) on one hand and the cool temperature of the outer space is necessary for the sun to play this often praised beneficial role .
from wiki : approximately 90% of the power consumed by an incandescent light bulb is emitted as heat , rather than as visible light . so , yes , the incandescent bulb can be used as a heater and , in fact , has been used as a heater . for example , see : easy-bake oven . the original toy used an ordinary incandescent light bulb as a heat source
if you take the simplest form of capacitor , two parallel plates , the the capacitance is proportional to the area of the plates and inversely proportional to the distance between the plates : $$c \propto \frac{a}{d}$$ when you are making a capacitor out of a snapple bottle you are actually making something similar to the simple " two plate " capacitor but the " plates " are curved round the surface of the bottle , with the foil as the external plate and the ( conducting ) salt water in the bottle acting as the internal plate . the $d$ in the equation above is the thickness of the glass . so you can make a capacitor out of any bottle , jar or anything similar . all that matters is the area of the foil and the thickness of the glass . if you want to increase the total capacitance you can just link any number of bottles in parallel i.e. link the external foil covers as one electrode and the internal brine solution as the other electrode . when you join up capacitors in this way , " in parallel " , you get the total capacitance just by adding up the individual capacitances of all the bottles you have joined . you ask about the effect of the increased voltage : the charge stored in a capacitor is given by : $$q = cv$$ where $c$ is the capacitance and $v$ the voltage , so using 12kv instead of 9kv just means you get 33% more charge for a given capacitance , or alternatively you can get away with a smaller capacitance to hold the same charge . do i sound a bit like a grandma if i point out you need to be careful with this experiment . a typical tesla coil can not generate enough current to kill you , but if you gang together enough capacitors the stored charge in them will kill you ! finally , i normally point people to wikipedia if they want to learn more , so see http://en.wikipedia.org/wiki/capacitor and http://en.wikipedia.org/wiki/leyden_jar for the sort of capacitor you are making . however be warned that the wikipedia article on the capacitor is a bit technical .
so the frequency response has a little more punch at the bottom end i think this one should do . of course , you will only need one .
the answer is yes , and the up and down quarks in the proton are already close enough to massless that the proton is hardly affected by making them completely massless . the proton mass would shift by a few mev in this case , and the proton and neutron would be very close in mass , with the difference in their mass due entirely to the proton 's electromagnetic field , so the proton with it is electric field would end up slightly more massive than the neutral neutron . the confinement mechanism with massless fermionic particles is very interesting and complicated compared to qed , this is why it took so long to understand historically . the fermions are massless , so they do not want to bind into anything , while the confinement means that they are not allowed to propagate separately , because they carry a qcd string along . this tension is hard to resolve . what qcd does to resolve it is to make a quark condensate in the vacuum . this condensate is actual stuff--- it is like a relativistically invariant superfluid filling all space , and it makes it so that quarks are not chirally symmetric , so while they are still massless , the vacuum does not allow them to propagate while keeping their chirality , when you take their mixing with other quarks in the vacuum into account . this is analogous to a nematic fluid breaking rotational invariance . the nematic molecules , if they were , in free space can rotate with a fixed angular momentum , since they are rotationally invariant . but in the nematic , if you try to spin up one of the long molecules , you get oscillations in the nematic order parameter . the only sign that it was originally rotationally invariant is that the associated nematic orientation waves are long-range goldstone-like bosons whose correlations fall off as a power . because of the condensate 's presence , it is completely wrong to treat quarks as isolated particles--- the low energy particles are collective excitations of the quark fluid ( and the glue fluid ) , just like sound-waves in a solid or orientation waves in a nematic . all the low-energy particles ( with the exception of the broad resonances , the f0 ( 660 ) , and the f ( 980 ) ) can be understood as shaking of this low energy fluid . the pions are the goldstone bosons of the broken chiral symmetry , the rho is the effective gauge bosons of the remaining isospin , and the a ( 1260 ) isotriplet is the gauge boson for chiral isospin . these effective gauge bosons emerge because the qcd string is like a fundamental string , and global symmetries end up effectively gauged . they are massive because they come with a scalar partner , because they are in 5d by ads/cft , not in 4d . the rest of the particles are from extending this picture to strangeness , and the nucleon is the skyrmion . this picture relates the low energy particles only to the symmetry properties of quark field products . this means that the quarks are not composing these particles the way that a proton and a neutron make a hydrogen atom . they make up these particles the way that iron atoms make up a sound-wave in iron . only the symmetry properties matter , and the exact atom that makes up the wave changes as the wave propagates . similarly , as the pion propagates , the quarks that make it up constantly change , as they flow in and out of the dynamical vacuum . the same is true for rho mesons and for protons and neutrons . in more qualitative detail the qcd glue field is random on a scale slightly larger than the proton radius , and this is the origin of confinement . what this means is that if you look at two boxes separated by more than a proton radius , the gauge field is completely uncorrelated in the vacuum in the two boxes ( this is true for 4 dimensional boxes in the euclidean probabilistic version , but the square-root of the euclidean vacuum probability distribution is the ground state wavefunction--- so you should think of the glue correlation in spacelike separated 3d boxes to make this precise ) . a random gauge field makes lots of quarks and antiquarks from the vacuum , because it has a flat power-spectrum at energies less than the randomness scale . this means that at any energy less than 1gev , quarks and antiquarks are freely created and annihilated by the random gauge field . these quarks and antiquarks fill up space with a sea , whose qualitative properties are difficult to predict from the fundamental theory . this sea is like a conduction electron sea in a metal , in that there is fermionic stuff there , but it is not like a conduction band because the total number of vacuum quarks and antiquarks is equal , so you do not have a fermi surface , which would be something which would pick out a rest-frame . instead you have a pair-order parameter , which is very much like the pair-order parameter in a bcs superconductor ( except this pair-order parameter is neutral , so that you get a paired fermion superfluid instead of a supercounductor ) . the chiral condensate is a soup of quarks and antiquarks which make an expectation value for q-qbar . this vacuum condensate breaks the chiral symmetry of the quarks , meaning that quarks are not chirally symmetric below the condensation scale , on the order of 1gev . the shaking of the condensate consists of light pions , while the vector mesons emerge from the shaking condensate linked by a qcd string , which gives emergent massive gauge bosons because string-theory strings turn flavor symmetries into gauge symmetries in one higher dimension , and qcd strings are not qualitatively too different from string theory strings . in addition , you can tie a knot in the pion condensate , the knot is the skyrmion , and this knot is the nucleon . the pionic field of a proton is not simply described by high-energy calculations in qcd , it is better described by the classical configuration of the long-range effective condensate theory . the skyrme model is not very precise , it is only good to something like 30% accuracy ( quantitatively , this is very bad ) in predicting the field distribution in protons and the structure of small nuclei . but it is at least qualitatively consistent with the known vacuum structure of qcd , unlike the picture of the proton as 3 quarks making a bound state . the picture that the low-lying excitations of qcd are simple bound states made of quarks is completely dynamically wrong . it has no justification , and it is sold to the public , because until recently , the concept of a dynamical vacuum was too far-out . it was nambu who introduced the dynamical vacuum in 1960 , and this pioneering work has too often been buried in the succeeding decade . with nambu 's recognition with a nobel prize , some of this outrageous recent history can be put right .
yes it is redundant . this is exactly what ads/cft is not . the degrees of freedom of the bulk are the degrees of freedom of the horizon . this is also why condensed matter analogs are rare--- the most common idea of identifying ads/cft boundary theories with condensed matter boundary theories is wrong , because in tranditional condensed matter systems , the boundary degrees of freedom are in addition to the bulk degrees of freedom , they are not dual to these degrees of freedom , as in ads/cft . the exception , where the condensed matter analog is right , is where the bulk theory is topological , like the chern-simons theory for the quantum hall fluid , where you can consider edge-states as describing interior physics . there might be more analogs of this sort . one has to be careful , because a lot of people have this wrong picture of ads/cft in the head , that it is boundary stuff in addition to bulk stuff .
thermal conductivity of ice is higher than thermal conductivity of snow , so ice is the winner . ice has higher thermal conductivity because it is more dense than snow . when snow is as dense as ice then it is no longer snow , it is ice . thank you for your answers , without them i would not guessed it right : ) .
" focus " is an inconvenient word if you are thinking of changing the potential , because if you do then the orbits are no longer conics and the word kind of loses its meaning . that aside , let me see if i understood your question correctly : given a gravitational potential that is spherically symmetric around a central point $\mathbf{r}_0$ , and which has a gravitational potential $v ( |\mathbf{r}-\mathbf{r}_0| ) $ , what is the fundamental reason that orbital speeds decrease as $|\mathbf{r}-\mathbf{r}_0|$ increases ? is this due to the gravitational field being weaker at longer distances ? in that case , the answer is that orbital speeds decrease because $v$ itself increases at longer distances . this is simply conservation of energy : $$\frac12m\mathbf{v}^2+mv ( |\mathbf{r}-\mathbf{r}_0| ) =e , $$ and if $v$ becomes less negative then $v^2$ must be smaller . thus , potentials where this does not happen must have regions where the potential is repulsive from the origin . one such example is $$v ( r ) =-\frac1r -r , $$ though of course there is no physical system with that behaviour .
the most probable position would be such as where the global maximum of the distribution is located . this is different to the expectation value of a distribution , but it happens that for a gaussian function the mean and the most probable value are the same .
the riemann tensor encapsulates all information about the 4-dimensional space-time . this information can generally divided into two sectors : information about the curvature of space-time due to the existence of matter . this is given by the ricci tensor according to the einstein equation $$ r_{\mu\nu} - \frac{1}{2} g_{\mu\nu} r = 8 \pi g t_{\mu\nu} $$ information about the structure of gravitational waves in the space-time . this is given by the trace-free part of the riemann tensor , namely the weyl tensor . often , we are not quite interested in the exact structure of the space-time , but only if gravitational waves can exist or their structure . in these cases , one studies the weyl tensor rather than the ricci tensor . for example , in the setup of quantum gravity , one requires to study the asymptotic structure of spacetime . in these theories , a good understanding of the weyl tensor is more important .
i believe that the " roughly " term is applied because of the associated experimental error when measuring its charge . the same cannot be said to the electron because " we " decided to make the electron the reference charge . so , the reference charge is definitely -1 . however the muon charge must be measured . according to this paper , muon mass and charge by critical absorption of mesonic x rays . s . devons , g . gidal , l . m . lederman and g . shapiro . phys . rev . lett . 5 no . 7 , 330–332 ( 1960 ) . the measured relative charge of a muon is $e_\mu / e_e = 1 \pm 10^{-5}$
the unusual thing is the really high absorption of microwaves by bulk water , whereas the ice behaves more normal like most solids and liquids . in liquid water we have an effect of relaxation of orientational polarisation . the polarisation is achieved not by rotation ( not possible in liquid water ) but by shift of hydrogen atoms along the hydrogen bonds . this is a kind of kohlrausch conduction mechanism . this process is extremely fast , so polarisation of water is one of the fastest processes in liquids . there is debate , whether tunneling plays a role to enhance the shift of the protons . the same mechanism is responsible for the extraordinary ( about tenfold ) mobilities of h+ ions in water . here is a plot of water spectrum in microwave domain . note the incredible absorption maximum with k about 3 ! http://books.google.com/books?id=bj1enqpb0cmcpg=pa184#v=onepageqf=false
the device you are describing is a cardan suspension .
hopefully david bar moshe can give a more rigorous explanation in terms of cohomology , but i have the following intuitive understanding of the difference between the two situations . in the aharonov-bohm effect , the particle is constrained to move around an ( effectively ) infinite solenoid . it then suffices to consider a problem on a plane , but with a hole in the middle that is threaded by a magnetic flux ( the solenoid ) . a path on which the charge loops $n$ times around the solenoid cannot be continuously deformed into a path with $m\neq n$ loops . in topological language , we say that the two paths are not homotopic to each other , because the winding numbers $n$ and $m$ are different . in physical terms , paths with different winding numbers correspond to quite distinct physical situations , so it is reasonable that the two paths could have different phases even if the actual lengths of the paths are the same . in dirac 's monopole , the singularity is now just a point in 3d space rather than a cylinder . the configuration space of the problem is essentially the surface of a sphere , with the monopole at the centre . any path that takes the particle on a closed path around the monopole is homotopic to the trivial path , in which the particle just stays where it is . put another way , it is not possible to reasonably define which paths constitute ' going around ' the monopole and which do not , since any path which goes exactly around the equator of the sphere is just an $\epsilon$ away from a path that goes ' above ' or ' below ' the monopole . if a non-trivial phase was picked up on spanning a loop around the monopole , you could envisage the following confusing situation . assume that the charge sits very close to the monopole . paths which are infinitesimally close to the trivial path now pick up a large phase relative to the amplitude for just staying still . put in terms of the path integral formulation of quantum mechanics , where $\mathrm{phase} \sim \mathrm{action}$ , this would basically imply a discontinuity in the action functional . see zee 's book quantum field theory in a nutshell for a nice explanation of this in path integral language ( i do not have the book to hand but will post an exact chapter reference when i can ) . incidentally , the same topological reasoning explains why anyons can exist in two dimensions but not in three or more . a path where two identical particles exchange positions twice is topologically equivalent to a path where one spans a loop around the other . since in three dimensions , this path is homotopically trivial , this restricts the phase acquired on a double exchange to be $+1$ . therefore the phase on a single exchange can only be $\pm 1$ , leaving fermionic and bosonic statistics as the only possibilities . in two dimensions , however , double exchanges are homotopically nontrivial ( so long as the particles cannot pass right through each other ! ) , meaning that in principle any phase can be acquired on exchange of identical particles ( hence the name anyons ) . see the beautiful paper by leinaas and myrheim for more on this geometrical viewpoint on particle statistics .
yes , this equations applies to all waves . . . with the caveat that you replace c by the speed of the wave you are studying ! in a water wave , the product of the wavelength and the frequency will be the speed of the water wave , not of light . for sound waves in air , it will be the speed of sound , etc . because of this , the general form of the equation you provided is : $$\lambda \nu ~=~ v_{wave} . $$ another interesting thing is that the speed of the wave need not be constant . the equation is always valid , but it might be possible that the wavelength depends on the frequency , in which case the speed will also depend on the frequency . this happens with water waves ; you can notice not all waves travel at the same speed on the ocean . it also happens with light ; light of different colours travel at different speeds through glass , which is what allows a prism to disperse white light into a rainbow . when the wavelength depends on the frequency , we call it " dispersion " . if it does not , then the wave speed is the same for all waves of that type .
as cumrun vafa explains in the video linked to below the picture of him in this article , f-theory works in a total of $10+2$ dimensions . the signature of the last two infinitesimal dimensions is ambiguous , so that they can indeed both be timelike . since these are only infinitesimal dimensions , any causality issues etc are not a problem in this case . and as cumrun vafa nicely explains in his talk , f-theory gives quite a nice phenomenology with an astonishingly realistic ckm-matrix , coupling constants , etc ; so it is not true that theories that operate in more than one time dimension are completely off base , as some people claim . there is no reason to dogmatically dismiss every theory that has more than one time dimension . btw , the talk is very accessible and enjoyable .
i do not think it is that tough to analyse . if a conductor is present in a uniform electric field then there will be redistribution of charges to counter electric field inside the conductor ( so that the net field inside the conductor is zero ) . however in uniform electric field this redistribution of charges will not cause any net force on the conductor . why ? because the amount of +ve charge on the conductor is equal to the -ve charge . hence f = q*e will be countered ( or balanced ) by equal and opposite force ( -q*e ) . the geometry on conductor will not play any role at all . ( nature of coulomb in force . ) so centre of mass will not experience any acceleration . what about torque ? it turns out torque = r×f . ahh . . . " r " . interesting . so will it experience any angular acceleration ? : )
they would be linearly dependent if and only if there exist complex numbers $\alpha$ and $\beta$ such that $\alpha x_{1} ( t ) + \beta x_{2} ( t ) = 0 \forall t$ clearly , if $\omega_{0}=0$ then this is the case for $\alpha = 1$ and $\beta = -c_{1}/c_{2}$ . so then they are linearly dependent . however , if $\omega_{0}\neq0$ , you can not find a combination of $\alpha$ and $\beta$ that fulfills this requirement for all $t$ . the importance lies in the fact that ( 1 ) any linear combination $\gamma x_{1} ( t ) + \delta x_{2} ( t ) $ of the two functions is also a solution . ( just plug the linear combination into the equation to see this . ) ( 2 ) these are the only solutions . namely , if you would find a solution $y ( t ) $ you could always write it as a combination of $x_{1}$ and $x_{2}$ . so these are the only solution to care about , all the dynamics of the system is contained in them .
there is nothing wrong with your calculations . from the wikipedia article on supermassive black holes : " the average density of a supermassive black hole ( defined as the mass of the black hole divided by the volume within its schwarzschild radius ) can be less than the density of water in the case of some supermassive black holes " given that black hole masses scale with linear size , while objects we encounter in daily lives have a mass proportional to the cube of their linear size , makes it inevitable that beyond a certain size black holes are characterized by mass densities that we label as ' small ' . in other words : when growing an object while keeping it is mass density fixed , there is a maximum to how far you can grow such an object . beyond a certain size , the object acquires a gravitational horizon that starts expanding proportional to the object 's mass , thereby reducing the object 's mass density .
the solution for this problem for a dust equation of state and spherical symmetry is known as the oppenheimer-snyder solution . you model the interior of the distribution as a frw universe with positive spatial curvature , zero pressure , and zero cosmological constant . you model the exterior of the solution as the schwarzschild solution cut off at a time-dependent radius . so long as the matter distribution is dust , the thing satisfies all of the junction conditions you need . see poisson 's relativity book or mtw . a more general solution requires numerics . but one thing we can say for sure is that there is no need for the black hole to shed its ' hair ' in the case of spherical symmetry--the radial dependence of the solution will just compress into the singularity eventually , or scatter out to infinity . birchoff 's theorem tells us every spherically symmetric vacuum solution must be the schwarzschild solution ( perhaps with an electrostatic charge , which is technically not vacuum ) . this is related to the fact that there can be no monopole radiation in relativity . also , the general case for this problem is very likely chaotic . already , if the equation of state of the matter is that of a classical , spherically symmetric , klein-gordon field , which is a relatively simple generalization , the system exhibits a ( link is a large postscript file ) second-order phase transition , a result found by matt choptuik , and related to the settling of the hawking naked singularity bet .
so you want to know how much water a certain surface adsorbs . this is really dependent on the surface material/conditions . check adsorption and relative humidity on wikipedia . to where i have analyzed , it seems that there is about enough information in the two articles . i am not a specialist on the subject so i might be missing some important factor .
you have to be precise with the definition here . you mention a temperature difference at both sides , but this is a 1d problem . you have different temperatures at both sides , and a temperature difference across the rod . the heat flux is proportional with the temperature gradient , this is referred to a fourier 's law ( see e.g. http://en.wikipedia.org/wiki/fourier's_law#fourier.27s_law ) now i understand your question as : why is $q=-k \frac{du}{dx}$ ( u is temperature in your case ) , and not just proportional to $u$ . now , suppose the flux would be proportional to the temperature . then you get some problems . first , suppose you have a rod of uniform temperature . the heat flux is constant across the rod everywhere , but in what direction ? and what is the new equilibrium than ? just the notion that the energy should flow from high to low temperature , is given by the gradient , making the energy always traveling down the gradient , trying to reach a more uniform state . to make it work out , some constant of proportionality was introduced , which is most often a system ( or material ) property .
conservation of energy follows from invariance under translation in time , not inversion . this symmetry states that no matter when you do your experiment , it will give the same results . all isolated systems obey this symmetry ( and therefore conserve energy ) and no violation of it has ever been detected . ( needless to say , it would be a huge event if it were . ) in classical physics , only continuous symmetries - that is , symmetries that can be continuously connected to the identity transformation - have a corresponding conservation law . quantum physics does permit conservation laws for discrete symmetries but these laws are far harder to visualize . an example of this is conservation of parity , $p$ , which corresponds to invariance under inversion in space , and which gives the parity - even or odd - of wavefunctions . temporal inversion , $t$ , is even harder to turn into a physical quantity because it requires a full relativistic treatment in which time is a coordinate like space and not a parameter ( as it is in non-relativistic quantum mechanics ) . a third discrete symmetry is charge conjugation , $c$ , which exchanges particles for their antiparticles . it turns out that any consistent field theory must be invariant under all three operations when taken together - i.e. under $cpt$ . thus violation of parity - an experiment and its mirror image behaving differently - is possible , for example , if it comes together with violation of $c$ - i.e. the mirror experiment behaves like the original one if it is made of antimatter - , as was discovered in the sixties . violations of $c$ and $p$ together have also been discovered in recent years , which means that in some situations violations of $t$ must occur . the recent $b$-meson experiments confirm this . since the $t$ symmetry does not correspond to energy but to a far more abstract quantity ( which is not conserved ) , this does not lead to a nonconservation of energy .
calories are the energy released when food is burnt . they are not a very accurate measure of the energy you get form eating them because they do not consider the actual biological process - just what happens if you burn them . if you drink cold water or eat ice then your body must use energy to heat it to body temperature - so eating enough ice could be a diet ! see how much more energy does it take for a human body to heat 0c ice vs 0c water ?
you have to interpret $|\frac{d}{dq} \psi\rangle$ . knowing that decomposition of the basis $|q'\rangle$ gives : $$|\psi\rangle = \int dq ' \psi ( q' ) |q'\rangle \tag{1}$$ you have : $$|\frac{d}{dq}\psi\rangle = \int dq ' \frac {d\psi ( q' ) }{dq'} |q'\rangle\tag{2}$$ so , applying it to $|\psi\rangle = |q"\rangle = \int dq ' \delta ( q"-q' ) |q'\rangle$ , you get : $$|\frac{d}{dq}q"\rangle = - \int dq ' \delta' ( q"-q' ) |q'\rangle\tag{3}$$ so , for instance , you have : $$\int dq " \langle \phi |\frac{d}{dq} q"\rangle \psi ( q" ) = -\int dq"dq'\delta' ( q"- q' ) \langle \phi|q'\rangle \psi ( q" ) \\= -\int dq"dq'\delta' ( q"- q' ) \phi ( q' ) \psi ( q" ) =-\int dq"dq'\delta ( q"- q' ) \frac{d\phi ( q' ) }{dq'} \psi ( q" ) \\=-\int dq ' \frac{d\phi ( q' ) }{dq'} \psi ( q' ) \tag{4}$$ this corresponds to the formula $ ( 15 ) $ in the dirac paper , and we have made use of integration by parts relatively to the variable $q'$ on the second line of the equations . for your last side question , as soon as you have the equality $\int dq ' f ( q' ) \psi ( q' ) =\int dq ' g ( q' ) \psi ( q' ) $ , for all possible functions $\psi ( q' ) $ , the only possibility is $f ( q' ) =g ( q' ) $
in the simplest approximation , an explosion is a shockwave moving out from some locus . the shockwave may be a compression front in a ambient medium , or may be a wave of gas propagating from the explosive into a vacuum . so that is the first thing you need to tell us : in air , water , vacuum , or what ? when the shockwave arrives at some material thing , it is the pressure exerted by the shockwave that transfers momentum ( i.e. . applies a force ) to the target . the target object then accelrates as per newton 's law : $$ \vec{f} = m\vec{a} . $$ the vector part of the above is the trigonometry that you show . i am simply going to assume that you have your coordinate system squared away . however , we still have not said how much force . to a first approximation it goes by the shock pressure $p$ times the area $a$ the object presents to the shock wave . so that gets us to $$ \vec{a} = \frac{p a}{m} \hat{n} $$ where the unit vector $\hat{n}$ is normal to the surface of the shockwave . we are still not done because we do not know $p$ . again , we will take the simplest approximation . assume you know the shock pressure $p_0$ at the surface of the explosive ( this is presumably something you can look up for chemical explosives ) . when the shockwave has total area $\mathcal{a}$ , it is pressure is $$ p = p_0 \frac{\mathcal{a}_0}{\mathcal{a}} $$ where $\mathcal{a_0}$ is the area over which $p_0$ applied . for small sources ( like a bomb or stick of dynamite ) this will give you a $1/r^2$ type dependence for the pressure . for long linear sources you get a $1/\rho$ dependence ; and for large surface charges a pressure that is independent of distance . that is not a good approximation , because you can expect the shockwave to spread out of it is own accord over time , and i have not yet modeled that . the really , really simple thing to do here is to choose a maximum range $l = \tau v_s$ equal to some time interval times the speed of shock propagation , and apply a hard cutoff at that point . now we know how much force as a function of distance from the explosive and geometry of the explosive , but to get to change in velocity we need to know for how long the force is applied . obviously that will be related to the depth of the shockwave and to it is speed of propagation : $ \delta t \approx v_s * d $ . unfortunately i am hazy on how to estimate $d$ . for a simple explosive i might go with half the " size " of the charge ( radius , thickness , whatever ) . the speed of propagation depends on the medium . again , for game purposes you could simple chose some number . once we have $\delta t$ we get $$ \delta v = \delta t * a . $$ since you say that your engine works on a applied impulse , you get $$ i = \delta t * p * a = \delta t * \frac{p_0 \mathcal{a}_0}{\mathcal{a}} * a , $$ which is actually pretty simple . and that should get you on your way .
first of all , the 21-centimeter line is " cold enough " so that its presence is universally connected with cold hydrogen , namely with interstellar gas . this spectral line may appear both as emission and as absorption . we see emission lines from the interstellar gas if there is no source behind the interstellar gas ; we may also see absorption lines if there is a source of all frequencies behind the interstellar gas . in the latter case , the interstellar gas ' absorption exceeds its emission . this is the dominant process in the case of the cas-a spectrum on the image so ( 2 ) is basically correct . otherwise the lines ' frequencies are not sharply determined in the graph . in the context of astrophysics , virtually all deviations from the " precisely right frequency " of the 21-centimeter line are due to the doppler shift i.e. the relative motion of the interstellar gas with respect to us . the graph mostly shows absorption and one may calculate the radial speeds of the clouds that are reducing the function in the graph in this way ( from the frequencies ) . the explanation ( 1 ) contains almost no component of the truth because cas a is a remnant of a type iib supernova that exploded a few centuries ago ( more precisely , that is when the light describing the explosion reached the earth ) . such supernova remnants have virtually no hydrogen in the shells – because even the star that led to this explosion , probably a red giant , had already have just the helium core and almost no hydrogen envelope . so pretty much all the spectral features of the hydrogen have to be due to the interstellar gas . it is not true that the emission and absorption always cancels . once again , emission dominates when the interstellar gas is " warmer " at the given frequency than the environment behind it ; absorption wins when the source behind the interstellar gas is " warmer " at the frequency ( literally think about the intensity as if it were temperature and realize that the heat goes from a warmer body to a cooler one ) .
here 's a slightly different but equivalent way to think about it . forces describe interactions between two objects . if two objects are interacting , they exert forces on each other . if two objects are not interacting , they do not exert forces on each other . thus , an object does not " carry around " a force with it . a force is not a property of an object , just as dmckee explains . instead , we describe interactions between two objects using the more-abstract concept of force . in your block-hits-other-block scenario , it is tempting to ask where did the force come from if colliding object had $f_\text{net}=0$ ? but when forces are viewed as interactions , it becomes more apparent that the force did not come from anywhere within one of the objects . there simply was not an interaction before they collided , so we would not ascribe the existence of a force force .
instead of the circular object i use a cardboard with a circular cavity that will behave same as the object , instead the lighted portion on screen would be shadow in that case : $$\tan\theta=\dfrac r d =\dfrac r{d+d} $$
you are right that friction points up the hill . what happens when you solve this is that you get a friction force that is negative . a negative force pointing down the hill is the same as a positive force pointing up the hill , so everything works out okay . it would have been more clear if the diagram author showed the friction vector pointing uphill to begin with , though .
mathematically , qft and dft are exactly the same thing . you can verify this by comparing the first equation on each of the wikipedia pages . on wikipedia these two differ by a minus sign in the exponent and by a normalization factor , but these are just conventions . the difference between quantum fourier transform and classical fast fourier transform is in the speed and in the way the data is represented physically . classically , a dimension $n$ vector is represented using $n$ floating point numbers . on a quantum computer , the qft operates on the wave function of $\log_2 n$ qubits , an exponential space savings . the classical fft runs in time $o ( n\log n ) $ and the qft runs in time $o ( ( \log n ) ^2 ) $ where again $n$ is the dimension of the vector . note that the classical fft must take time $o ( n ) $ to even read the input ! it should also be noted that the classical and quantum versions differ in another important respect , as a consequence of the way the data is stored . after the classical fft algorithm completes , you have the entire output vector to do whatever you want with ( e . g . print out the $100^{\textrm{th}}$ component of the vector or whatever ) . the qft leaves the answer in a quantum state , which you can think of as a sort of generalized probability distribution , and you can do certain measurements to get certain types of information out , but you have to be crafty . shor 's algorithm is of course the canonical place to look for an example here , although simon 's algorithm illustrates many of the same concepts without having to deal with the computational tedium of shor 's algorithm ( simon uses the hadamard rather than fourier transform but conceptually there is not much difference ) . by the way , the circuit that implements the qft is a reflection of cooley and tukey 's divide and conquer algorithm for fft . when cooley-tukey splits the input in half , the qft inspects one qubit . ponder this and you will understand something about the power of quantum computers .
the hamiltonian has the very legal definition that it is the legendre transform of the lagrangian function . so , in any physical case to find the hamiltonian of a system , you have to take $l = t - v$ and then perform the ritualistic legendre transform process shifting coordinates from $q , q'$ to $q , p$ . the time symmetry of the system leads to the conservation of the so called jacobian function and not the hamiltonian . the hamiltonian will however be the total mechanical energy if thesystem is conservative and in this case the jacobian function also equals the total mechanical energy . the jacobian is the mechanical one and not the mathematical one . ref goldstein .
disclaimer : i have no engineering background , so if anything i write is in error , definitely point it out . however , if the 3-axis accelerometer only returns the proper acceleration vector $\mathbf{a}$ , then if the object is moving around and physically accelerating , it is impossible to determine the orientation of the object without additional information . here is a formal counterexample . state 1: object at rest , roll is $\pi/4$ , pitch is 0 the object experiences a proper acceleration vector of $$\mathbf{a}=\left ( 0 , \frac{g}{\sqrt{2}} , \frac{g}{\sqrt{2}}\right ) . $$ state 2: object accelerating , roll is 0 , pitch is 0 with an inertial acceleration vector $\ddot{\mathbf{r}}=\left ( 0 , \frac{g}{\sqrt{2}} , -\frac{g}{\sqrt{2}}\right ) $ , the proper acceleration measured will be $$\mathbf{a}=\left ( 0,0 , g\right ) +\ddot{\mathbf{r}}=\left ( 0 , \frac{g}{\sqrt{2}} , \frac{g}{\sqrt{2}}\right ) . $$ since both state 1 and state 2 return the same acceleration vector but have different spatial orientations , the act of converting proper acceleration to orientation under the influence of an outside acceleration is not a one-to-one correspondence , and thus is underdetermined . in other words : how is the accelerometer supposed to tell the difference between state 1 and state 2 ? it can not , unless you have additional information .
orbital angular momentum is a good quantum number for the atomic problem because the coulomb potential between the electron and nucleus is rotationally invariant , but the potential an electron feels in a crystal is not . a non-spherically-symmetric potential can couple states with different $l_z$ , and so if $\psi_{l_z}$ were the eigenstate of the spherically symmetric problem with angular momentum $l_z$ , then the correct atomic eigenstates in , for example , a cubic or tetragonally symmetric potential separate into linear combinations like $\psi_{l_z} \pm \psi_{-l_z}$ which measures out to total $l_z=0$ .
your friends are correct . if there is no force in the left-right direction , then linear momentum will be conserved in that direction . because the new composite object has more mass than the original object , it will have a lower speed to the right . what about energy ? kinetic energy is not conserved in this case , because the collision is inelastic . the kinetic energy lost in the collision ( and the downward momentum lost , for that matter ) is absorbed by whatever wall is preventing the composite object from continuing to move downward .
the wikipedia diagram is giving the breakdown by mass not by volume . baryonic/leptonic ( i.e. . non-dark ) matter is only about 5% of all matter and of that four fifths of it is in the form of free hydrogen and helium . of the remaining 1% about half is neutrinos or heavy elements . that means only 0.5% of the mass/energy in the universe is in stars .
there are a variety of methods used to measure distance , each one building on the one before and forming a cosmic distance ladder . the first , which is actually only usable inside the solar system , is basic radar and lidar . lidar is really only used to measure distance to the moon . this is done by flashing a bright laser through a big telescope ( such as the 3.5&nbsp ; m on apache point in new mexico ( usa ) , see the apollo project ) and then measuring the faint return pulse with that telescope from the various corner reflectors placed there by the apollo moon missions . this allows us to measure the distance to the moon very accurately ( down to centimeters i believe ) . radar has been used at least out to saturn by using the 305&nbsp ; m arecibo radio dish as both a transmitter and receiver to bounce radio waves off of saturn 's moons . round trip radio time is on the order of almost 3 hours . if you want to get distances to things beyond our solar system , the first rung on the distance ladder is , as wedge described in his answer , triangulation , or as it is called in astronomy , parallax . to measure distance in this manner , you take two images of a star field , one on each side of the earth 's orbit so you effectively have a baseline of 300 million kilometers . the closer stars will shift relative to the more distant background stars and by measuring the size of the shift , you can determine the distance to the stars . this method only works for the closest stars for which you can measure the shift . however , given today 's technology , that is actually quite a few stars . the current best parallax catalog is the tycho-2 catalog made from data observed by the esa hipparcos satellite in the late 1980s and early 1990s . parallax is the only direct distance measurement we have on astronomical scales . beyond that everything else is based on data calibrated using stars for which we can determine parallax . and they all rely on some application of the distance-luminosity relationship $m - m = 5log_{10}\left ( \frac{d}{10pc}\right ) $ where m = apparent magnitude ( brightness ) of the object m = absolute magnitude of the object ( brightness at 10 parsecs ) d = distance in parsecs given two of the three you can find the third . for the closer objects , for which we know the distance , we can measure the apparent magnitude and thus compute the absolute magnitude . once we know the absolute magnitude for a given type of object , we can measure the apparent magnitudes of these objects in more distant locations , and since we now have the apparent and absolute magnitudes , we can compute the distance to these objects . it is this relationship that allows us to define a series of " standard candles " that serve as ever more distant rungs on our distance ladder stretching back to the edge of the visible universe . the closest of these standard candles are the cepheid variable stars . for these stars , the period of their variability is directly related to the absolute magnitude . the longer the period , the brighter the star . these stars can be seen in both our galaxy and in many of the closer galaxies as well . in fact , observing cepheid variable stars in distant galaxies , was one of the original primary mission of the hubble space telescope ( named after edwin hubble who measured cepheids in m31 , the andromeda galaxy , thus proving that it was an “island universe” itself and not part of the milky way ) . beyond the cepheid variables , other standard candles , such as planetary nebula , the tully-fisher relation and especially type 1a supernova allow us to measure the distance to even more distant galaxies and out to the edge of the visible universe . all of these later methods are based on calibrations of distances made using cepheid variable stars ( hence the importance of the hubble mission to really nail down those observations .
the answer seems to near 60 degrees . additional description here i have found this mathematica description of the ecliptic plane relative to the galactic plane . wolfram . com
in principle yes . but to make a good $\text{zn}_3\text{n}_2$ homojunction led you need the capability to incorporating both p-type and n-type dopants ( normally oxide materials are naturally n-type ) which might not be possible . from what i have read , this material has been proposed as a way of making p-type zno ( which is naturally n-type ) by a post growth annealing step . this means that via $\text{zn}_3\text{n}_2$ you maybe able to make a p-zno/n-zno homojunction led . more info here , http://pubs.rsc.org/en/content/articlehtml/2013/ra/c3ra46558f .
without the internal part : the divergence $$\nabla_\mu ( x_\nu \theta^{\mu\nu} ) = x_\nu \nabla_\nu \theta^{\mu\nu} + \frac12 ( \nabla_\mu x_\nu + \nabla_\nu x_\mu ) \theta^{\mu\nu} $$ where i used that $\theta^{\mu\nu}$ is symmetric . recalling that the energy-momentum tensor is divergence free , the first term drops out . assuming that $x^\nu$ generates a dilation/scaling symmetry ( and not a bona fide symmetry ) , we know that its deformation $$ \nabla_\mu x_\nu + \nabla_\nu x_\mu \propto \mathcal{l}_x g_{\mu\nu} \propto g_{\mu\nu} $$ where $\mathcal{l}$ is the lie derivative . ( in the case $x^\nu$ generates a symmetry the term vanishes from killing 's equation . ) hence in this case for the current to be conserved ( that is , divergence free ) , we need that $g_{\mu\nu} \theta^{\mu\nu} = 0$ ; that is , the energy momentum tensor is tracefree .
i think a very good orignally german qm book is : straumann : " quantenmechanik : ein grundkurs über nichtrelativistische quantentheorie " there is also a second volume : straumann : " relativistische quantenfeldtheorie " another good book is from g . grawert : quantenmechanik you may also have a look at thirrings books about qm ( it is mathematically more advanced ) . i think straumanns book is not " canonical " . often used by students are the books by nolting and greiner . from the experimental point of view you may have a look at demtröders book .
when you look at this video you can see that the lower leg seems to maintain a constant velocity . this is probably partially due to the higher total mass of the leg compared to the ball . the ball leaves the foot at a higher velocity . this due to the deformation of the foot and the ball . this deformation is caused by acceleration ( initial velocity difference ) . when the ball tries to return to its original spherical shape , it will push against the foot , which pushes back and accelerates the ball . now if the initial velocity difference would be bigger , due to the ball moving towards you , the deformation will be bigger . assuming that the leg still can maintain roughly a constant velocity , this means that the ball will be accelerated for longer and thus have a higher velocity . and other way to look at this is that the ball bounces of a wall . however in this case the wall is moving . relative to the wall the ball reverses its velocity normal to the wall , but with a slightly lower amplitude due to inelastic collision . but this means that if the ball initially is moving faster towards the wall the ball will move away from the wall faster . this can be done by moving the wall faster ( kicking with a higher speed ) or by increasing the incoming velocity of the ball .
this question was featured on this month 's safari magazine . it says : the electrons of a permanent magnet are aligned in a strict and disciplinary order . these electrons of a magnetic domain rotate harmoniously in the same direction and that is what is responsible for the magnetic power of a magnet . the magnetic waves travel through the domain walls , passing from one domain to the next and provide strength to the domain structure . the structure is so tight and coherent that unless the magnet is subjected to extreme temperatures or extreme shock , the disciplinary order of the magnetic domain is not disturbed . as long as the domain structure remains intact , the magnetic power of a permanent magnet does not diminish . under the normal cause of usage , a magnet made of samarium cobalt will lose only about 50% of its magnetic power in 700 years . i am not sure about what causes it to lose its magnetism .
think about this : a function that maps points on a 2d space to numbers can describe the shape of terrain , but i would not say that it is the terrain . in the same way , a mapping of points to objects ( scalars , vectors , tensors , etc . ) is the mathematical description of a field , but if you think of the field as just the mapping , you are missing out . fields can have various physical properties . for example , just as a particle can have a certain amount of energy , so can an electromagnetic field . the difference is , since the field is spread throughout space , so is the energy ; therefore , it makes more sense to talk about the density of energy rather than the amount . same applies for momentum , or any other physical quantity carried by the field . just as the field could be described by a mapping of points to vectors , so the energy density can be described by a mapping of points to numbers . given the vector value of the field at any point , you can calculate the numeric value of the energy density at that point . but remember that these numbers ( i.e. . the mapping ) are just a mathematical description of the energy density . now , you may notice that the mapping that describes the energy density ( $u ( x ) $ ) satisfies the naive definition of a mathematical description of a field : it associates a number with each point in space . but physicists would not normally call that mapping a " field , " because in a sense , it is not really independent . mathematically , you can calculate $u ( x ) $ from $a ( x ) $ ; physically , the energy " field " is completely determined by the em field . in physics , we tend to reserve the term " field " to talk about something that can not be obtained by a simple calculation from some other field . does that mean , that the " numbers " that make up the field in each point in space stay constant with temperature or time ( or both , i am not sure . ) i am not sure how you got that from the quote you listed . . . no , the numbers that make up the mathematical description of the em field do not stay constant with either time or temperature . in fact , one of the things that characterizes a physical field is that it has dynamics - mathematically , this means that the numbers ( or whatever ) making up the field change with respect to time and space , but in a predictable manner which can be described with differential equations . but there are things you can calculate from a field that do stay constant . for instance , you can calculate the total energy stored in the field by calculating the energy density and then integrating it over the volume of the field . you could also calculate the temperature of the field , by some mathematical procedure . in many cases , these quantities are more closely based on the manner in which the field changes than the actual values that describe it . ( in fact , in some sense , it turns out that you can describe a field by the way that the numbers change , just as well as you can with the numbers themselves . read up on the fourier transform and momentum space if you are interested . )
consider a ball of water floating in zero g , as demonstrated on the iss . ignoring for the moment the surface tension of the water ( i will come back to that ) the pressure inside the water is the same as the pressure of the air around it . this is simply because without any forces , like gravity , acting on the water there is nothing to cause a pressure gradient . well the pressure is not quite the same . in my first paragraph i mentioned that the air-water interface has a surface tension , and this acts a bit like the elastic of a balloon . it compresses the water inside the drop and increases the pressure slightly . the hyperphysics site has an article deriving the pressure and the result is that the extra pressure is given by : $$ \delta p = \frac{2\gamma}{r} $$ where $\gamma$ is the surface tension , which is about 0.07 n/m for pure water , and $r$ is the radius of the ball of water . if we take a radius of 1 cm we get $\delta p = 14$ pa . this is negligable compared with the atmospheric pressure of $101,325$ pa , so it is a very good approximation to say the pressure inside the ball of water is the same as the pressure of the air around it .
if you are careful about how you define the surface , then you will get the correct direction out of maxwell 's equations . in vector calculus ( and generally in math and physics ) , a surface has an orientation , which also specifies the orientation of the loop that forms its boundary . so you can not just pick a direction to go around the loop at random . the direction in which you go around the loop is related to the orientation of the normal vector to the surface . of course , you do not actually need to do a surface integral to figure this out . the electric and magnetic fields in an em wave ( at any given position and moment in time ) are related by the equation $$\mathbf{b} = \frac{1}{c}\hat{\mathbf{k}}\times\mathbf{e}$$ where $\hat{\mathbf{k}}$ is a unit vector that points in the direction of propagation of the wave . if you do not know which direction the wave is moving , you can not tell which way the magnetic field points .
if i understand the question , you are wondering how to justify the statement that a ( reverible ) adiabatic process is isentropic from the point of view of statistical mechanics ( the classical thermodynamics definition makes sense to you ) . let us then start with the entropic fundamental relationship , s = s ( u , v , n ) , where u stands for energy , v for volume , n for number of particles . in many a statistical mechanics texts you will find the explicit definition of s for a system of particles ( under usual simplifying assumptions ) . inyour example n is constant , but u and v are not : i would be glad to help further if needed , but if you looked at the expression for s as a function of v and n this would answer your question alone . i believe the discussion at isentropic processes be useful .
answering my own doubts in order:- no . it got to the correct answer , but was wrong . i think she was getting confused between cause ( force ) and effect ( acceleration ) . when the train brakes , the ball and train acquire a relative acceleration . no other force comes into play when one 's inertial frame of reference is the train 's frame . she was basically saying $ a_{iron} = a_{rubber} $ and $ m_{iron} &gt ; m_{rubber} $ , so therefore $ f_{iron} &gt ; f_{rubber} $ which is correct . but then she also says that the iron ball will move a longer distance which implies that both the balls will eventually come to a stop while simultaneously claiming to ignore air resistance and friction . so then which force eventually stops the balls ( as otherwise they will keep moving at the constant speed otherwise as dictated by one of newton 's laws of motion ) ? none that i could think of ( which are probably all of them considering this is such a simple question ) . not mentioned , but probably not . in most idealized physics questions ( especially textbook ones ) , air resistance is not something to be considered . this is to some extent implied by the phrase " on a smooth floor " . if we do not consider it , then no external forces will be acting on the ball ( barring friction ) and both the balls will move at the same speed till they like slide off the train 's surface or hit a wall or something meaning this was a trick question ( which seems atypical as it is not the modus operandi of the crappy , unenlightening educational system ) . if we do consider it , then b'coz of the reasons in the ( my ) original answer , the iron ball will have a slight greater speed than the rubber ball and thus will move a bit further . probably not as the question mentions the phrase " a smooth floor " . although it could be said that while this would affect the absolute values of friction , it would not change the relativity , i.e. , the fact that the rubber ball will experience more friction that the iron one . also , considering it would mean we should probably also consider air resistance . and then the question becomes unsolvable as there will be unknown variables required then . for that and the reason above , it should probably not be considered . 10x , @nathaniel and @tromik .
when it is explained as opposite , it is usually the heat released when a crystal is formed , and since the word " released " already makes up for the sign of heat ( i.e. . if heat is -10 that means the heat released or evolved is +10 ) , it is taken as positive . nonetheless , some books also present a negative value to it but it does not matter unless you are careful about it in calculations .
in this context , a " current " is an object obeying an affine lie algebra , also called current algebra and a special case of a kac-moody algebra . it is an algebra formed by unit weight operators : take for example a current $j^a ( z ) $ , where $a$ is a label and $z$ is a complex coordinate . the algebra is given by $$ [ j^a_n , j^b_m ] =i{f^{ab}}_cj^c_{n+m}+mkd^{ab}\delta_{n+m} , $$ where $$j^a_n=\frac{1}{2\pi i}\oint dz \ , z^{- ( n+1 ) }j^a ( z ) . $$ the integer $n$ denotes the mode number , the integer $k$ is the level and $d^{ab}= ( t^a , t^b ) $ defines the inner product between generators . the word " boundary " refers to the fact that the symmetry group underlying the algebra preserves a certain structure at the boundary of the geometry at infinity . in the case of the paper you are reading , the symmetry group is $u ( 1 ) $ and the boundary is given by $\mathcal{i}^+$ . additional information : affine lie algebras play a role in string theory/conformal field theory , where they can be used to generate states in certain representations of a group . for example , the state $$j^a_{-1}\tilde{\alpha}^{\mu}_{-1}|0\rangle$$ corresponds to a massless vector $a^{\mu a}$ in the adjoint representation of the underlying group ( $\tilde{\alpha}^{\mu}_{-1}$ is a creation operator ) .
the equation is correct--- the ( laminar ) flow at small reynolds number is given by making the flow be along the pipe , and substituting into the navier stokes equations , which reduces to your thing . the one issue is the sign--- $\delta p$ is negative if you mean the flow is going to be in the positive z direction . i will absorb the constants and consider the problem on the box [ -1,1 ] x [ -1,1 ] . there is no analytic solution in elementary functions for this , because the problem is equivalent to solving laplace 's equation with certain dirichlet boundary conditions on the square . but there is a simple and rapidly convergent series which gives you the answer . the equation is $$ \nabla^2 \phi = -a $$ where a is the ( negative ) pressure gradient over the viscosity , in length units where the size of the box is 2 . to solve this , first note that the quadratic function $$ \phi_0 ( x , y ) = {a\over 4} ( 2 - x^2 - y^2 ) $$ works , but does not satisfy the boundary conditions . this flow ( plus a constant ) gives the parabolic cylinder laminar flow profile , it satisfies no-slip on the circle of radius $\sqrt{2}$ , just touching the corners of the square . you replace $\phi$ by $\phi_0 - {a\over 4} \phi$ , and the new $\phi$ satisfies laplace 's equation : $$ \nabla^2 \phi = 0$$ with the boundary conditions $$\phi ( x , 1 ) = 1-x^2 $$ , so as to subtract out the nonzero velocity on the boundary square . this is the dirichlet problem . so you need to solve the dirichlet problem on the square . in principle , the interior of the unit square can be conformally mapped into the circle , but the tranformation is ugly . so it is best to give a direct approximation . write $\phi$ as the real part of an analytic function $f ( z ) $ , where $z=x+iy$ . the symmetry of the problem tells you that the real part of $f ( iz ) $ is the real part of $f ( z ) $ , so that ( by analyticity ) $f ( iz ) =f ( z ) $ and the analytic function f is an expansion in powers of $z^4$ . $$ \phi = \mathrm{re} f ( z ) $$ $$ f ( z ) = a_0 + a_1 z^4 + a_2 z^8 + a_3 z^{12} $$ then you know that on the boundary , $f ( 1+iy ) = 1 - y^2 $ , and this fixes the coefficients . to lowest nontrivial order , you keep only the constant and the $z^4$ term , and you find $$ \mathrm{re} f ( 1+iy ) = a_0 + a_1 ( 1 - 6 y^2 + y^4 ) = 1-y^2 $$ which gives ( just by setting the lowest order terms equal ) $a_1 = 1/6$ and $a_0 = 5/6$ . the flow is , to quartic order : $$ v_z ( x , y ) = {a\over 4} ( 7/6 - x^2 - y^2 - {1\over 6} ( x^4 - x^2 y^2 + y^4 ) ) $$ this is not a great approximation , but you can go to order 8 , order 12 , or order 16 and do the same thing to get polynomial approximations to the flow of any order . i should add that there is a slowly converging solution by expansion in box modes , and it obeys the boundary conditions but it is inferior to the analytic method above--- the fourier series of a constant function only falls of as 1/n .
given a arbitrary metric $g_{\mu\nu}$ you can introduce a reference ( background ) metric $\bar{g}_{\mu\nu}$ ( in the paper notation it is just the minkowski metric $\eta_{\mu\nu}$ ) in a way that $\delta{}g_{\mu\nu} = g_{\mu\nu} - \bar{g}_{\mu\nu}$ is small ( in some sense ) . you can reintroduce the background metric in a way that the perturbation remains small , this action is parametrized by an small diffeomorphism ( you can see why in the mukhanov 's review of perturbations : dx . doi . org/10.1016/0370-1573 ( 92 ) 90044-z ) generated by an arbitrary vector field $-\xi^\alpha$ , thus , the background change by $$\bar{g}_{\mu\nu}\rightarrow\bar{g}_{\mu\nu}-\mathcal{l}_\xi\bar{g}_{\mu\nu} , $$ where $\mathcal{l}_\xi$ is the lie derivative with respect to $\xi^\alpha$ . given this transformation , the perturbation transform as $$\delta{}g_{\mu\nu}\rightarrow\delta{}g_{\mu\nu} + \mathcal{l}_\xi\bar{g}_{\mu\nu} . $$ if you choose a covariant derivative compatible with $\bar{g}_{\mu\nu}$ , say $\bar{\nabla}_\alpha\bar{g}_{\mu\nu} = 0$ , the lie derivative can be written as $$\mathcal{l}_\xi\bar{g}_{\mu\nu} = 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$ in the paper the background metric is just the minkowski metric , then in cartesian coordinates $$\mathcal{l}_\xi\eta_{\mu\nu} = 2\partial_{ ( \mu}\xi_{\nu ) } = 2\xi_{ ( \mu , \nu ) } , $$ where the comma represent the partial derivative . so in general you can write a metric in terms of the background , perturbation and a gauge transformation as $$g_{\mu\nu} = \bar{g}_{\mu\nu} + \delta{}g_{\mu\nu} + 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$
spin1/2 particle ususally , in this kind of hamiltonian , people uses $s=s_z$ , where $$s=s_z=\left [ \begin{array}{cc} 1 and 0 \\ 0 and -1\end{array} \right ] . $$ then , your unperturbed hamiltonian $h_0$ is : $$h_0=-\mu s\cdot b_0 = -\mu \left [ \begin{array}{cc} 1 and 0 \\ 0 and -1\end{array} \right ] b_{0 , z} . $$ then the eigen vectors of energy are : $$|\psi^0_+\rangle=\left [ \begin{array}{c} 1 \\ 0\end{array} \right ] , $$ $$|\psi^0_-\rangle=\left [ \begin{array}{c} 0\\ 1 \end{array} \right ] . $$ perturbation solution then you want to compute $|\psi_+\rangle$ and $|\psi_-\rangle$ for the perturbed hamiltonian $h=h_0-\mu b_1 s_x$ , where $$s_x=\left [ \begin{array}{cc} 0 and 1 \\ 1 and 0\end{array} \right ] . $$ as you said , you have to compute the following quantities ( note i use $+ , -$ instead of $n=0,1$ . which became : $$\psi^{ ( 1 ) }_+=\sum_{n\neq +} \psi^{ ( 0 ) }_{n'}\frac{\langle\psi_{n'}^{ ( 0 ) }|-\mu b_1s_x|\psi_{+}^{ ( 0 ) }\rangle}{e_+^{ ( 0 ) }-e_{n'}^{ ( 0 ) }}=\psi^{ ( 0 ) }_{-}\frac{\langle\psi_{-}^{ ( 0 ) }|-\mu b_1s_x|\psi_{+}^{ ( 0 ) }\rangle}{e_+^{ ( 0 ) }-e_{-}^{ ( 0 ) }}$$ $$\psi^{ ( 1 ) }_-=\sum_{n\neq -} \psi^{ ( 0 ) }_{n'}\frac{\langle\psi_{n'}^{ ( 0 ) }|-\mu b_1s_x|\psi_{-}^{ ( 0 ) }\rangle}{e_-^{ ( 0 ) }-e_{n'}^{ ( 0 ) }}=\psi^{ ( 0 ) }_{+}\frac{\langle\psi_{+}^{ ( 0 ) }|-\mu b_1s_x|\psi_{-}^{ ( 0 ) }\rangle}{e_-^{ ( 0 ) }-e_{+}^{ ( 0 ) }}$$ put here vectors and matrices we just found and let me know if you get zero .
your expression for the velocity looks right ; but we have to get a few other things taken care of . first - the center of the marble does not move from 0 to 2R , it moves from r to 2R-r - so the potential energy due to this is smaller than mg(2R) which is what you had in your expression . on the other hand , you need to take account of the energy of the sphere rolling ( which is stated explicitly ) . the moment of inertia for a solid sphere ( the usual case for a " marble" ) is $$i=\frac25 m r^2 \\$$ this leads to rolling energy $$e=\frac12i\omega^2=\frac12\frac25 m r^2 ( \frac v r ) ^2 = \frac15mv^2\\$$ thus your energy equation has to be corrected to $$\frac12 m v^2 + m g ( 2r-2r ) + \frac15mv^2 = m g h \\$$ also - note that the marble is moving in a path with radius R-r not radius R ; you need to take that into account when you compute the limiting velocity ( "fast enough to stick to the track" ) - you have to put R-r where you have R in your velocity equation : $$\frac{mv^2}{r-r}=mg\\ v=\sqrt{g ( r-r ) }$$ combining these : $$\frac{7}{10}mg ( r-r ) +2mg ( r-r ) =mgh\\ h = \frac{27}{10} ( r-r ) $$ note that it is sometimes said that " you can ignore the rotational energy of the marble if it is very small " , but that is emphatically not true - the rotational energy for a solid sphere is always 2/5 of the ( linear ) kinetic energy , regardless of the size of the marble . it can therefore only be ignored for the case of a ( frictionless ) sliding object . finally - since no physics problem is complete without a diagram : this shows clearly where the R-r term is coming from . thus if you ignore the rolling , the diagram explains the " correct " answer ( which in a way was your question ) . if you include rolling , then you need to modify the solution as shown .
obviously things vary from subfield to subfield , so canonical advice is probably to much to hope for . with that in mind , let me try to give you something of an answer . in principle when a person finishes a phd , if they remain in academia , there are a number of ways this might happen . they might end up with : a teaching job with little or no research emphasis , a postdoc supported from a faculty members grant , a personal fellowship , or a junior faculty position . now , ( 4 ) is essentially unheard of these days , so you can more or less forget that . of the rest , what is feasible will depend on a number of factors , but the impact of your research is going to play a big role in your success in applying for either ( 2 ) or ( 3 ) . i presume , since you are only looking at what to apply for now , that you have not finished the phd yet , and have something on the order of six months to a year left ( do correct me if i am wrong ) . with this in mind , it seems quite possible that you will have more than one publication by the end of the phd . if this is possible , you should put in the effort to write up results . if you get 3 or 4 papers ( preprints definitely count , so make sure you are using the arxiv and including them on your cv ) , this will substantially alter your odds of getting a postdoc from someone you have not previously worked with . personal fellowships are the most sought after form of postdoc , as they usually allow substantial autonomy , and so provide something of a springboard for a research career . this means that they are competitive and realistically , i can not see someone getting one on a single publication . i was rejected from some of the ones i applied for with 8 publications including a number of prls . postdocs supported from someones grant are easier to get , but still competitive . if you have only one publication , it might prove hard to convince someone you do not already know of your potential , though it is perhaps still worth trying , especially if your paper is particularly interesting . the best way to get a position if you are really limited on publications is as a postdoc on a grant supported by someone you have worked with before , or who knows your work well . these people are more likely to hire you on potential or other factors , rather than on your publication record . the scope for this is of course limited , so as pieter points out the more people who know you and your work , the better . that said , my advice would definitely be to start applying and get your results written up as soon as possible .
one must keep in mind also that it is the particle , not the shower that goes through the astronaut in dmckee 's estimate above , where he treats the relativistic particle going through matter . the shower in your question which gave the energy estimate of the parent particle is generated by cascade/sequential collisions of deep angle scattering over a long path . the energy is not released in one go unless the astronaut is very unlucky . the deep inelastic scattering crossection at those energies is still not up to barn values ( a barn is about the size of a uranium nucleus ) so the astronaut would have to be very unlucky even to get one energetic scatter let alone to start a shower .
this equation is wrong . as it has been pointed out in the comments , i can not equate the integrands of two integrals just because the integration limits are the same . the equation which is of relevance in the context of probability density and quantum mechanics is perhaps the well known continuity equation , $$ \frac{\partial \rho}{\partial t} + \nabla \cdot \textbf{j} = 0 $$ , where $\rho = \vert \psi \vert^2$ and $ \mathbf{j}=\frac{\hbar}{m}\text{im}\left [ \psi^*\nabla\psi\right ] $ is the probability flux .
your second equation , $p ( \nu , t ) = \frac{2 h {\nu}^3}{c^2}$ $\frac{1}{\exp\bigl ( \frac{h \nu}{kt}\bigr ) - 1}$ is what is commonly referred to as planck 's law for radiation , although a more standard symbol used is $b_\nu ( t ) $ . this is the energy radiated per time , per area , per frequency interval , per steradian . it is a formula for the ' specific intensity ' of a source , which intuitively is the energy flux along a ray of radiation in a given direction , and so you must normalize by the solid angle subtended by that ray . to get the total energy per time per area radiated by a patch of a black body , integrate over solid angle and over frequency . be careful performing the solid angle integral , however , because you must include the geometric factor $\cos \theta$ that accounts for the projected area of the patch ( $\theta = 0$ corresponds to a ray emitted in the normal direction ) . rays leaving one side of a patch can only be directed into the upper hemisphere of the solid angle sphere . so the solid angle integral looks like this : $$ f_\nu = 2 \pi \int_0^{\pi/2} b_\nu ( \theta ) \ , \cos\theta \ , \sin \theta \ , d \theta$$ the $2 \pi$ out in front is for the azimuthal angle . here , $f_\nu$ is what is commonly referred to as the specific flux ( 'specific ' because it is still per unit frequency interval ) . then , either by reading up on the riemann $\zeta$ function , or just using a computer to tell you the answer , you can perform the frequency integral and get $$ f = \sigma \ , t^4$$ here $f$ is what we commonly think of as the flux ( energy per area per time ) , and $\sigma$ is the stefan-boltzmann constant , $$\sigma \equiv \frac{2 \pi^5 \ , k_\mathrm{b}^4}{15 \ , h^3 \ , c^2}$$
i will keep things simple . keep in mind gravity tends to pull everything together and never repels objects . gravitational field of the object depends on the shape and the matter distribution . a deformed planet with the same mass as that of the earth will behave just like the earth from a distance . the non-spherical effects will become more obvious on approaching the object . due to lack of spherical symmetry , objects will definitely accelerate with different values and directions around the surface object . things would definitely have different weights in different regions on the surface . gravity wants to smash everything together into smallest size possible , only non-gravitational forces can cause repulsion . for the slingshot question , it depends on many factors that are both unique to the object and that are same for all planets . if you are too far from the planet , you will not be able to slingshot . if you are moving too fast , you will not be able to slingshot . but if you get your initial approach distance and velocity just right , you will be able to slingshot around the non-spherical planet . there are many combinations of the two that could make you slingshot . the translational motion of any free rigid object can be analyzed by looking only at the motion of its center of mass . we can thus concentrate the whole mass of the object at a point . basically , the orbit is the path traced by this point ( the center of mass ) . so if the non-spherical planet is rigid enough , it will trace a uniform path around the sun ( an ellipse ) . the shape of the ellipse will in general depend on the mass of the object and its energy . so the orbit will in general be a squashed or stretched variation of earth 's orbit . remember the earth was not always shaped like a ball ( well almost ) . it became a big ball over billions of years . however , if the object is rigid enough it will not turn into a ball . this is off topic , but having read the books and seen the movie , i can tell you that the former are much much better than the latter .
you are right that we do not know if the universe is finite or infinite in space . cosmologists do now think that it has an infinite future because of the accelerated expansion rate due to dark energy but this does not tell us anything about the question of infinite space . to answer the question for space we first have to assume spacial homogeniety , i.e. that space looks the same everywhere on large scales . if this assumption is wrong then we are stuck because we cannot see beyond the horizon of the observable universe which is due to the finite speed of light and the finite age of the universe . if we can not assume anything about what happens beyond the horizon then obviously we cant tell if it is finite or infinite . however , within the observable universe space does appear to be homogeneous so it is usual to assume , rightly or wrongly , that it is homogeneous everywhere . if that is the case then the question of the finiteness depends on the curvature of space and this is something that can be estimated with some precision using cosmological observations , especially that of the cosmic microwave background . if the curvature of space takes a positive value then it must be finite . if it is zero or negative then it is probably infinite though the possibility of a finite universe remains if it has an unusual topology like a tessellation of a finite polyhedron . when we measure the curvature we find that it is close to zero and we cannot tell whether it is positive , zero or negative with the present error bars , so we do not know if the universe is finite or infinite in space . this flatness is expected as a prediction of inflation theory . the best we can do is set a lower limit to how small the universe can be given limits on its measured curvature and that is what the papers you linked to are trying to do . they do not claim to settle the question of whether it is finite or infinite . your final question about quantum mechanics and the outside of the universe is unrelated and should have been asked as a separate question , but the answer is no .
the gravitation lens generates smaller angular deflections at large impact factors . a converging lens generates larger angular deflections at larger impact parameters . so , a first guess would be " no way " .
yes , the electrons are in the same state and yes they interact ( in the sense that identical bosons interact to create a bec ) . the explanation is kind of involved . in a collection of identical atoms , it is not possible for us to distinguish between them . this also applies to their electrons . this is true whether or not the atoms are cold enough to be in a bec state . now the rules of qm state that when you have identical particles ( electrons or atoms ) you have to symmetrize over them . thus a literal answer to your question " but what about the electrons or other fermions of the bec atoms ? are they in the same state ? " is " yes , they are in the same state but this does not have anything to do with the bec state . your second question : " do electrons of one atom interact with those of another ? " can be answered similarly . if you consider the electrons as identical particles which must be symmetrized over , then it is impossible to distinguish one electron from the other . therefore they must all be in the same state . so anything you do to " one " electron will effect all of them . therefore they do interact ; their waveforms are shared . now let 's be more specific about your questions in terms of the nature of the bec state . consider just a single atom . it is described by a wave function . as the atom is placed in a cooler environment it loses energy . this loss of energy changes the shape of the wave function . it makes the wave function bigger . the wavelength for the wave function of a gas atom is called the " thermal de broglie wavelength " . the formula is : $$\lambda_t = \frac{h}{\sqrt{2\pi m k t}}$$ where $h$ is the planck constant , $k$ is the boltzmann constant , $t$ is the temperature , and $m$ is the mass of the gas atom . the thing to note about the above formula is that as the temperature gets lower , the wavelength $\lambda_t$ gets bigger . when you reach the point that the wavelengths are larger than the distances between atoms , you have a bec . the reference book i have got is " bose-einstein condensation in dilute gases " by c . j . pethick and h . smith ( 2008 ) which has , on page 5: " an equivalent way of relating the transition [ to bec ] temperature to the particle density is to compare the thermal de broglie wavelength $\lambda_t$ with the mean interparticle spacing , which is of the order $n^{-1/3}$ . . . . at high temperatures , it is small and the gas behaves classically . bose-einstein condensation in an ideal gas sets in when the temperature is so low that $\lambda_t$ is comparable to $n^{-1/3}$ . " when the wavelengths are longer than the inter-particle distances , the combined wave function for the atoms ( which one must symmetrize by the rules of qm ) becomes " coherent " . that is , one can no longer treat the wave functions of different atoms as if they were independent . to illustrate the importance of this , let 's discuss the combined wave function of two fermions that are widely separated with individual wave functions $\psi_1 ( r_1 ) $ and $\psi_2 ( r_2 ) $ . for fermions , the combined symmetrized wave function is : $$\psi ( r_1 , r_2 ) = ( \psi_1 ( r_1 ) \psi_2 ( r_2 ) - \psi_1 ( r_2 ) \psi_2 ( r_1 ) ) /\sqrt{2} . $$ now the pauli exclusion principle says that exchanging two fermions causes the combined wave function to change sign . that is the purpose of the "-" in the above equation ; swapping $r_1$ for $r_2$ gives you negative of the wave function before the change . another , more immediate , way of stating the pauli exclusion principle is that you can not find two fermions in the same position . thus we must have that $\psi ( r_a , r_a ) = 0$ for any fermion wave function where $r_a$ can be any point in space . but for the case of the combined wave function of two waves that are very distant from one another there is no point $r_a$ where both of the wave functions are not zero ( that is , the wave functions do not overlap ) . thus the pauli exclusion principle does not make a restriction on the combined wave function in the sense of excluding the possibility that both electrons could appear at the same point . that was already intrinsically required by the fact that the two wave functions were far apart . now i used the above argument about " far apart fermions " because the pauli exclusion principle has a more immediate meaning to a lot of people than the equivalent principle in bosons . the same argument applies to bosons , but in reverse . bosons prefer to be found near one another , however if we write down the combined wave function for two widely separated bosons , there is still a zero probability of finding both bosons at the same location . again the reason is the same as in the fermion case : $\psi_1 ( r_b ) $ is only going to be nonzero in places where $\psi_2 ( r_b ) $ is already zero . the bosons are too far apart to interact ( in the sense of changing the probability of finding both at the same point from what you had otherwise expect classically ) . another way of saying all this is that in qm , there is no interaction between things except if their wave functions overlap in space . you use the thermal de broglie wave function to determine how big a wave function has to be . if the atoms are closer than that , they are interacting in the sense of bose ( or fermi ) condensates . so let 's apply our understanding to the question " do the electrons interact " in a bec . consider the $\lambda_t$ formula to the electron . since $m$ appears in the denominator , replacing the atom with the electron decreases $m$ by a factor of perhaps 3 or 4 orders of magnitude . this increases $\lambda_t$ by perhaps 2 orders of magnitude . therefore , any gas which is cold enough to be a bec will be composed of electrons that are much more than cold enough to also be coherent .
it is a common misconception to think that because the higgs mechanism is the origin of mass it is also the origin of gravity . this is a misconception because the origin of gravity is not simply mass . instead it is a quantity called the stress-energy tensor . the stress-energy tensor is usually represented as a 4 $\times$ 4 matrix containing 10 independant entries ( 10 not 16 because the matrix is symmetric ) and in most cases the only significant entry is the top left one , $t_{00}$ , which gives the energy density . the key point is that as far as the stress-energy tensor is concerned mass and energy are the same thing related by einstein 's famous equation $e = mc^2$ . immediately before the electroweak transition all particles were massless , and immediately after they had a finite mass , but this change did not make any difference to the stress-energy tensor and therefore to gravity . before and after the transition the energy density was the same ( well similar anyway ) so the contribution of the particles to the stress-energy tensor and therefore to gravity was the same . so there is no contradiction between the higgs mechanism and the idea of entropic gravity .
there are lots of possibilities , depending on the energy of the antiproton beam . the hadron spectrum is quite complicated . probably the most likely channel is pion production : $$ \bar p \to \bar n + \pi^- . $$ this reaction requires a " spectator " nucleus to exchange energy and momentum with the $\bar p$ , and so might be more properly written as $$ a + \bar p \to a^* + \bar n + \pi^- $$ where by $a^*$ i mean that the spectator nucleus might also end up in an excited state . the negative pions will eventually either decay ( mostly $\pi^-\to\mu^-+\bar\nu_\mu$ ) or be captured on another nucleus in a reaction like $$ \pi^- + p \to n . $$ that is not the only available channel : with a spectator nucleus , you can make other antibaryons and mesons , for instance \begin{align} \bar p and \to \bar\delta + \pi and and \text{ ( which could make $\pi^\pm$ or $\pi^0$ ) } \\ \bar p and \to \bar\lambda + k^- \\ \bar p and \to \bar\sigma + k and and \text{ ( could be a $k^0$ or a $k^-$ ) } \\ and \vdots \end{align} here 's a review of low-energy nucleon-antinucleon interactions , which i have not yet read .
well , by the presentation you give , you are going to have $\frac{d^{2}x^{i}}{d\tau^{2}}\neq 0$ , because you have those $\gamma_{0i}{}^{j}$ terms . for instance ${\ddot y} + 2\frac{\dot a}{a}{\dot y}{\dot t} = 0$ ( i abuse notation and mean the obvious things with dots , but obviously , $a = a ( t ( s ) ) $ and $y=y ( s ) $ ) the condition you want is $\gamma_{\mu\nu}{}^{i} = 0$ , with at least one $\gamma_{\mu\nu}{}^{0}\neq 0$ . i am sure there are metrics that satisfy this condition , but i do not know any ( non-trivial ones${}^{1}$ ) off of the top of my head . edit : note that even the minimally coupled " perturbative spherical potential " metric $ds^{2} = - ( 1-2\phi ( r ) ) dt^{2} + dr^{2} + r^{2}d\theta^{2} + r^{2}\sin^{2}\theta d\phi^{2}$ will have a nonzero component for $\gamma_{tt}{}^{r}$ , so it might be a bit tricky to find a nontrivial example . ${}^{1}$for instance , you could define $g_{ab} = -f ( t ) dt^{2} + \delta_{ij}dx^{i}dx^{j}$ . this will have a nonzero value for $\gamma_{tt}{}^{t}$ , but the space is really just minkowski space because it can be changed to it by the substitution $t = \int \sqrt{f ( t ) }dt$
any tensor $a_{ij}$ can be decomposed into symmetric and antisymmetric parts , regardless of whether or not it is diagonalizable . $$ \begin{align} s_{ij} and = \frac{1}{2}\left ( a_{ij} + a_{ji}\right ) \\ \omega_{ij} and = \frac{1}{2}\left ( a_{ij} - a_{ji}\right ) \end{align} $$ so that $$ a_{ij} = s_{ij} + \omega_{ij} $$
provided the intervals between all events are spacelike they can appear in any order . see this article for a popular science level description , or this paper for the full details .
your procedure gives : $$ a_{xz} = \sqrt{a_x^2 + a_z^2} $$ then : $$ a_{total} = \sqrt{a_y^2 + a_{xz}^2} $$ but if you substitute for $a_{xz}$ in the second equation you get : $$ a_{total} = \sqrt{a_y^2 + ( \sqrt{a_x^2 + a_z^2} ) ^2} = \sqrt{a_y^2 + a_x^2 + a_z^2}$$ so you do not need to split the calculation into two steps . your accelerometer may already exclude the acceleration due to gravity . if it does not then yes you need to use the inclination to work out the three components of gravity then subtract them from $a_x$ , $a_y$ and $a_z$ . it is hard to say exactly how to do this without knowing how your phone reports it is inclination . response to comment : suppose you have your device held flat so $a_z$ = -1 . now move the device downwards at and angle of $\theta$ as shown below : assuming it is moving in the $xz$ plane the value of $a_z$ will be decreased a bit and the value of $a_x$ will increase from zero . suppose you are applying an acceleration to the phone of $2g\space cos ( \theta ) $ - you will see why i have chosen this value in a moment . now the values of $a_x$ and $a_z$ are : $$ a_x = 2g\space cos\theta \space sin\theta $$ $$ a_z = g - 2g \space cos^2 \theta $$ you now calculate $a_{total}$ by just squaring and adding as we discussed above to get : $$a_{total}^2 = 4g^2 \space sin^2\theta \space cos^2\theta + g^2 + 4g^2 \space cos^4\theta - 4g^2 \space cos^2\theta $$ and a bit of rearrangement gives : $$a_{total}^2 = g^2 + 4g^2 \space cos^2\theta \left ( sin^2\theta + cos^2\theta - 1\right ) $$ and because $sin^2\theta + cos^2\theta = 1$ the quantity in the brackets is zero so you end up with : $$a_{total}^2 = g^2 $$ that is : $$a_{total} = g $$ which is the same as when the phone is stationary . so it is possible to be accelerating the phone and still have the total acceleration come out as $g$ ( $g$ = -1 in the phone 's units ) . that is why just subtracting one is not a reliable way to tell if the phone is accelerating .
conformal field theories do not have a mass-gap , which is one of the assumptions [ for the strong conclusions of non-mixing of poincare spacetime symmetries vs internal symmetries ] of the coleman-mandula no-go theorem . similarly , for its superversion : the haag-lopuszanski-sohnius no-go theorem . [ in the supercase , the poincare algebra is replaced with the super-poincare algebra . ]
a magnet is in essence made up of atoms with orbital and spin angular momentum , ( mainly due to the electrons ) and the forces that act on these electrons can be derived from dirac 's equation , but give you what is essentially the lorentz force law . you can think of the magnetic field acting on every single electron individually , and all these forces added up will apply a net force and a net torque to the magnet as a rigid body . if you want to do some calculations , you can imagine each atom as having a microscopic current proportional to the dipole moment of that atom ( basically , the magnetization ) . this " bound current " is $\vec{j} = \nabla \times \vec m$ . again , imagine an electron orbiting the nucleus as providing a " current " flowing around the nucleus -- the bohr magneton . it is just an idealization but you can make it rigorous if you want . now , currents of adjacent atoms cancel out where they intersect , because they are going in opposite directions , but on the boundary of the magnet they do not cancel out , because there is no neighboring atom there . thus it is perfectly mathematically and physically sensible to model a permanent magnet as a sheet of current moving along the magnet 's surface , roughly like a solenoid does , and given by $\vec{k} = \vec{n}\times \vec{m}$ , where $\vec{n}$ is the normal to the surface . now this surface current is a real current , and therefore it will experience a force when subjected to an external magnetic field ( and it is own magnetic field , btw . this is why transformers hum ) .
the classical field has a straightforward interpretation in the bosonic case--- it is determined by the density and phase of the superfluid condensate of the particles and both are simultaneously measurable in the thermodynamic limit . if there is no superfluid , the field is zero . where there is a superfluid , it is the square-root of the density , with a phase whose gradient is the local superflow . you can measure the value of psi-squared at x , that is determining the superfluid condensate density , which can be done by shining light through the fluid to get the index . you can also measure the value of the velocity by the exact way light refrects ( or if it is dense enough , by putting little dust specks in the fluid ) . a gaseous bose-einstein condensate , or even liquid helium , is described precisely by the classical limit of this formalism in the thermodynamic limit . if you take the classical limit wrong , by insisting that the particle number n is fixed , you do still get hbars lurking around . in order to take a good classical field limit , you need enormous occupation numbers for the field , which requires that you take n to infinity and the mass to zero keeping the density fixed . in this limit , the conjugate variables density/phase become commuting . as for measuring the " x-projection of operators " , i could not figure out what that meant . you can measure the field momentum too , of course , by just taking the complex conjugate of the measurement of the field ( the imaginary part of the field and the real part are not independent , and you can describe the whole system using only the real part and its time derivative , as described in the wikipedia article ) .
the distance where light has a circular orbit is actually $1.5r_s$ not the event horizon . this distance is known as the photon sphere . in principle a shell observer hovering at this distance could indeed see their own back . the proper distance is indeed just $2\pi r$ , however the object would look bigger than expected because the curvature of spacetime has a focussing effect . light slightly outside the photon sphere will spiral outwards away from the black hole while light slightly inside the photon sphere will spiral inwards towards the black hole . therefore the light will not travel in a straight line . if we unroll the circular object to make it easier to draw the light rays we get something like :
first of all , the standard model does not treat bosonic fields as classical . they are quantum mechanical i.e. non-classical , they are just not anticommuting or grassmann-odd . second , a consistent theory just requires the relationship between spin and statistics , see e.g. the http://en.wikipedia.org/wiki/spin-statistics_theorem combining integer spin with fermi statistics leads to ghost or energy or the norm that is not positively definite , and vice versa ( half-integer spin with bose statistics ) . it was proved by pauli . however , your very question did not actually talk about the integer vs half-integer spin at all . it was talking about the relationship between fermions and anticommuting fields . this is almost a tautology . a fermion is a particle whose wave function for many particles is antisymmetric , $\psi ( x_1 , x_2 ) =-\psi ( x_2 , x_1 ) $ etc . , so the fields that create these particles must be anticommuting , $a^\dagger ( x_1 ) a^\dagger ( x_2 ) =-a^\dagger ( x_2 ) a^\dagger ( x_1 ) $ . the multiparticle state in qft is written as $$ |\text{2 fermions}\rangle = \int d^3 x_1 d^3 x_2\ , \psi ( x_1 , x_2 ) a^\dagger ( x_1 ) a^\dagger ( x_2 ) |0\rangle $$ because the wave function $\psi$ is antisymmetric , only the antisymmetric combination $a^\dagger ( x_1 ) a^\dagger ( x_2 ) - a^\dagger ( x_2 ) a^\dagger ( x_1 ) $ contributes to the state , and in fact , only this combination is nonzero . the sum – the anticommutator – vanishes . that is why the antisymmetry of $\psi$ is " automatic": if there were a non-antisymmetric part of $\psi$ , it would vanish in the integral above because the product of the creation operators is antisymmetric . the same for bosons and " commuting " , without the minus sign . the answer to your " why " question is that your statement is really a tautology , pretty much a definition of bosons and fermions , up to the possibly confusing comments about " one antisymmetry " implying the " other antisymmetry " above . of course , you could also ask why one uses commuting or anti-commuting fields to describe particles at all . well , nature just works in this way . quantum fields naturally reduce to multi-body quantum mechanics with the automatic symmetry or antisymmetry – and they may give rise to automatically lorentz-invariant theories , too ( something that would be hard in the " non-relativistically styled " multiparticle quantum mechanics ) .
okay , this was really cool and i got some help from my physics professors on this one ( apparently i will not learn this until next semester ) and to find the magnitude of the square of a complex number you take it times it is complex conjugate . so in this case $$\frac{v_0}{i \omega l +r}$$ is multiplied with $$\frac{v_0}{-i \omega l + r}$$ leaving you with simply $$\frac{v_0^2}{w^2l^2+r^2}$$ , then just take it times your $$r_2$$ giving you $$\frac{v_0^2}{\omega^2 l^2+r^2} \times r_2$$ for the average power . sorry that my equations are not very pretty , latex is not working on my ubuntu install yet . . . . . hope this helps ! !
the book has a minus sign typo in the previous equation . the commutation in the step you are looking at does not introduce a minus sign , but the previous line had a term of the form $$ - c^r c^i b_r k_i$$ with a minus sign in front , which requires you to commute the two c 's past each other . the next step writes $$ - c^i c^r b_r k_i $$ incorrectly , it should be a plus sign , because the c 's are commuted , and then the next step says $$ c^i k_i c^r b_r $$ which fixes the error . you assumed that the minus sign came from moving the k , which does nothing , when it actually comes from commuting the c 's past each other . thank you for linking the book , it would have been impossible to find the error otherwise . given the unmotivated and needlessly formal introduction of brst in this book , i would recommend that you read the introduction to brst in the appendices of polchinski . the exercise in question does not require formal symbol manipulation gymnastics in an operator calculus , it is a simple question .
alright , let 's go on a thrilling tour through the theory of representations . notation in the following is the same as in the op , except that we call the trivial representation $\boldsymbol{1}$ , as is canon . we start from my expression in the question $$ i := \int \alpha ( g ) ^i_{i'}\beta ( g ) ^j_{j'}\gamma ( g ) ^k_{k'} = \sum_\rho \sum_{\mu = 0}^{n ( \rho , \gamma , \boldsymbol{1} ) } c ( \alpha , \beta , \rho ) ^{ij}_\zeta c ( \rho , \gamma , \boldsymbol{1} ) ^{\zeta k}_\mu c^* ( \alpha , \beta , \rho ) _{i'j'}^\eta c^* ( \rho , \gamma , \boldsymbol{1} ) _{\eta k'}^\mu $$ now , the first thing we must find out is : how often does $\boldsymbol{1}$ appear in $\rho \otimes \gamma$ ? the answer is depressingly simple . by schur 's lemma , every morphism of vector spaces $v_\rho \rightarrow v_\gamma $ that commutes with the group action is either an isomorphism of the zero map . let us denote the set of morphism that commute with the group action by $\mathrm{hom}_g ( v_\rho , v_\gamma ) $ . now , $n ( \rho , \gamma , \boldsymbol{1} ) $ is the dimension of $\mathrm{hom}_g ( v_{\boldsymbol{1}} , v_\rho \otimes v_\gamma ) $ . for finite vector spaces , $v_\rho \otimes v_\gamma \cong \mathrm{hom} ( v_\rho , v_\gamma^* ) $ , where the star denotes the dual space on which we have the dual representation $\gamma^*$ . thus , we seek the dimension of $\mathrm{hom}_g ( v_{\boldsymbol{1}} , v_\rho \otimes v_\gamma ) \cong \mathrm{hom}_g ( v_{\boldsymbol{1}} , \mathrm{hom} ( v_\rho , v_\gamma^* ) ) \cong \mathrm{hom}_g ( v_\rho , v_\gamma^* ) $ . but schur 's lemma tells us that this space is only non-zero when $v_\rho$ and $v_\gamma^*$ are isomorphic ! ( and it is easy to see that even then , it is only one-dimensional . ) so , we have found a precise criterion for the occurence of the trivial subrep : $\rho = \gamma^*$ . thus , the sum over $\mu$ and $\rho$ is killed , and all that is left is $$i = c ( \alpha , \beta , \gamma^* ) ^{ij}_\zeta c ( \gamma^* , \gamma , \boldsymbol{1} ) ^{\zeta k}c^* ( \alpha , \beta , \gamma^* ) ^\eta_{i'j'}c^* ( \rho , \gamma , \boldsymbol{1} ) _{\eta k'}$$ now , we must think a bit more carefully than i did before about the factors at a vertex : the three representations associated with the $\epsilon$ factor are the $\alpha_1$ of the wilson line and the $\beta_1 , \beta_4$ of the regions adjacent to it . but for one of the two regions , the wilson line runs against its natural orientation , so it will be w.l.o.g. a factor $\beta_4 ( g^{-1} ) $ in the integral . now , since we consider unitary representations , $\beta_4 ( g^{-1} ) = \beta_4^* ( g ) $ , and the factor associated to that part of the wilson line at the vertex is $$\epsilon ( \alpha_1 , \beta_1 , \beta_4^* ) ^{ij}_k := c ( \alpha_1 , \beta_1 , \beta_4 ) ^{ij\zeta}c ( \beta_4 , \beta_4^* , \boldsymbol{1} ) _{\zeta k}$$ which is the $3jm$-symbol for the reps $\alpha_1 , \beta_1 , \beta_4$ . thinking carefully about the four lines meeting at the vertex and the relative orientations of the regions and the lines , one finally finds the total factor at one vertex to be $$ g ( \alpha_1 , \alpha_2 , \beta_{1,2,3,4} ) = {\epsilon ( \alpha_1 , \beta_1 , \beta^*_4 ) ^{ij}}_k{\epsilon ( \alpha_2 , \beta_2 , \beta_1^* ) ^{lm}}_j {\epsilon^* ( \alpha_1 , \beta_2 , \beta^*_3 ) _{im}}^n {\epsilon^* ( \alpha_2 , \beta_3 , \beta^*_4 ) _{ln}}^k $$ which has exactly the right structure of being the product of 4 $3jm$ symbols summed over their $m$ to be a $6j$ symbol . thus , as is unsurprising , witten is correct in asserting that we get the $6j$ symbol at a vertex , though i still consider this to be highly non-obvious .
it is not the case because the gamma matrices do not commute and neither do the covariant derivatives . we can always write a 2-index tensor as the sum of its antisymmetric and its symmetric part , $$d^\mu d^\nu = \frac{1}{2} ( d^\mu d^\nu + d^\nu d^\mu ) + \frac{1}{2} ( d^\mu d^\nu - d^\nu d^\mu ) . $$ the latter part vanishes precisely when the curvature ( = field strength ) vanishes . and we also have similarly for the gamma matrices $$\gamma^\mu\gamma^\nu = \frac{1}{2} ( \gamma^\mu\gamma^\nu + \gamma^\mu\gamma^\nu ) + \frac{1}{2} ( \gamma^\mu\gamma^\nu - \gamma^\mu\gamma^\nu ) = g^{\mu\nu} + \frac{1}{2} [ \gamma^\mu , \gamma^\nu ] . $$ since an antisymmetric tensor contracted with a symmetric tensor gives 0 , $$\gamma^\mu\gamma^\nu d_\mu d_\nu = g^{\mu\nu}\frac{1}{2} ( d_\mu d_\nu + d_\nu d_\mu ) + \frac{1}{2} [ \gamma^\mu , \gamma^\nu ] d_\mu d_\nu = d^\mu d_\mu + \frac{1}{2} [ \gamma^\mu , \gamma^\nu ] d_\mu d_\nu . $$ the one half in the first term is cancelled by the metric being symmetric .
dmckee 's right : the picture you link to shows stars , not planets . the color of a star is almost entirely determined by its temperature . the light coming from a star is , to a good approximation , blackbody radiation ( except for absorption lines in its spectrum , which are very important tools for learning about the star but have little effect on its color ) . the spectrum of a blackbody depends on its temperature , in such a way that it shifts from longer to shorter wavelengths as the temperature increases . so hot stars look blue , and cool stars look red . even cooler objects , such as you , do not glow significantly at all in the visual part of the spectrum but do in the infrared . at the moment , we know little or nothing about the colors of planets other than those in our solar system . extrasolar planets are detected indirectly , via their effect on the star they orbit . they are not yet seen directly themselves .
forget the webcam . attach the secondary mirror to the wall , at a height that is near the height of the center of the primary mirror . then adjust the horizontal and vertical tilt of the primary mirror to center the multiple images of the secondary mirror within each other on the primary mirror . if you have a cheap laser pointer and a carpenter 's square , you can set up the laser pointer so that it is exactly square to the wall ( vertically and horizontally ) and then adjust the primary mirror such that the laser beam goes back to the laser .
actually , the answer is a bit more subtle than just density . the principle that is behind floating objects is archimedes ' principle : a fluid ( liquid or gas ) exerts a buoyant force , opposite apparent gravity ( i.e. . gravity + acceleration of fluid ) on an immersed object that is equal to the weight of the displaced fluid . thus , if you have an object fully immersed in a fluid , the total force it feels is given by ( positive sign means down ) : $f = gravity + buoyancy = \rho_{object} v g - \rho_{fluid} v g = ( \rho_{object} - \rho_{fluid} ) v g$ thus , if the average density of the object is lower than that of the water , it floats . if the object is partially immersed , to calculate the buoyant force you have to consider just the immersed volume and its average density : $f = \rho_{object} v g - \rho_{fluid} v_{immersed} g$ note that when i was talking about density , i was talking about the average density of the object . that is its total mass divided by its volume . thus , a ship , even if it is made out of high-density iron it is full of air . that air will lower the average density , as it will increase the volume considerably while keeping the weight almost constant . if you want to understand this better you can give the following problem a try : ) what is the height an ice cube of side l floats in water ?
there is a more physically intuitive way to do this in my opinion . note that in your picture , the longer stick has length $l_0 = 1\ , \mathrm{m}$ and the shorter stick has length $l=l_0/\gamma_u$ because of length contraction . therefore , the time $t = 12.5\ , \mathrm{ns}$ given in the problem corresponds to the shorter stick traveling a distance $l_0-l_0/\gamma_u$ . this gives the equation $$ l_0 - \frac{l_0}{\gamma_u} = u t $$ now simply solve for $u$ . i checked this numerically by the way and it gives $u=c/2$ .
i found this to be interesting , because at least pedagogically people will write down long range electronic interactions which totally break gauge invariance . i mean we have all seen someone write down a " general four point interaction": $$\int\psi ( 1 ) \psi ( 2 ) \bar{\psi} ( 3 ) \bar{\psi} ( 4 ) v ( 1,2,3,4 ) $$ . this breaks gauge invariance , which is usually horrible , but frequently this does not seem to lead any obvious issue . why is that ? it is bothered me before . to return specifically to your question , first let 's rewrite in real space : $$\int\delta ( r ) u ( r , r' ) \bar{\delta} ( r' ) $$ where $\delta = c_{\uparrow}c_{\downarrow}$ has $u ( 1 ) $ charge $2$ . this is not gauge invariant unless $u$ transforms properly . now we started with a gauge invariant system at some level , so we must have gotten to this point by " integrating out " out some charged degrees of freedom . in this simplest scenario we did second order perturbation theory and $u$ is just some correlator : $$ u ( r , r' ; a ) = \langle o ( r ) \bar{o} ( r' ) \rangle_{a}$$ where $o$ has the right charge . or it could be something more complicated , the details do not matter . i have explicitly noted this correlator must depend on the gauge field $a$ . this is not surprising since $u$ measures a charge being released at $r$ and destroyed at $r'$ - regardless of the details there must be ahranov-bohm phases . it is this dependence of $u$ on $a$ that maintains gauge invariance , clearer below . there is a sort of minimal coupling prescription for $u$: $$u ( r_1 , r_2 ; a ) = \int \mathcal{dp}\exp ( i\ ! \int_{r_1}^{r_2}\ ! \ ! \ ! \ ! a ( r' ) \cdot dr' ) $$ where the $\mathcal{dp}$ is some measure on the space of paths from $r_1$ to $r_2$ , and this measure does not depend $a$ . this is minimal in the sense of being , well , minimal and in the sense that if you expand $u$ as a polynomial in derivatives you recover the usual minimal coupling prescription . you can see that this has right gauge transformations properties . in the special case where the motion is essentially semiclassical you just a finite sum over wilson lines . okay so that is the way this interaction should be written , now the real question is when can we ignore all this , since we surely do not want to estimate some path integral measure on the atomic scale when we can barely estimate single interaction energies on the atomic scale . now violating gauge invariance leads to horrible things , i need not remind . but clearly there is a sense where if $u ( r , r' ) $ has a small radius of effect and if we probe with a very long wavelength then we should not know that it is not a delta function , and hence we should not know that its violating gauge invariance . this is not really correct : if we follow our " minimal coupling " prescription we realize what we need is not that the distance between $r$ and $r'$ is small , but that the region explored by the bulk of paths in $\mathcal{dp}$ is small . this makes sense , since the issue comes from ahranov bohm phases . it does not help if $r$ and $r'$ are close if we get between them by travelling on circuitous paths that are enormously sensitive to the gauge fields . so the correct criterion is if the magnetic flux threading the region with paths is much smaller than a flux quantum , then we can " straighten out " all the paths in our path integral and just write it as : $$ u ( r_1 , r_2 ; a ) \approx \tilde{u} ( r_1 , r_2 ) \exp ( i\int_{r_1}^{r_2}\ ! \ ! \ ! a ( r' ) dr' ) $$ where the integral is taken over whatever path in the region we choose . note this approximation is gauge invariant , since the error depends on the magnetic field , and its as simple a gauge invariant as thing as we get . now in terms of linear response , at this point one could simply plug in an external field and decide whether or not it was sufficiently small to be ignored . or one could simply make peace with the wilson line and proceed . if you wanted to operate more generally but still wanted to ignore that wilson line you can do it as long as you restrict yourself to a sufficiently smooth gauge . if you are in a gauge where $a$ does not vary appreciably over the range of $u$ then $\int_{r_1}^{r_2}\ ! \ ! \ ! a ( r' ) \cdot dr ' \approx a\cdot ( r_2 - r_1 ) $ . and the point this is approximately pure gauge : it corresponds to the gauge transformation with field $\chi = a ( r ) \cdot r$ . so we may essentially ignore it . to have a smooth gauge one must have slowly varying fields small magnetic field . which are fairly intuitive physical requirements . and then additionally we must not introduce gauge transformations which vary quickly . so one way to think of all this is that we can manipulate such apparently non-gauge invariant expressions because we have actually gauge fixed the high frequency modes of the gauge field . probably could have thought of that without all this work , but such is life .
please be aware that plutonium cores are supposed to be plated with another metal ( nickel or silver , if my memory serves me right ) . machining plutonium is very hazardous and is done with remote manipulators , since it increases risk of inhalation . source : http://toxnet.nlm.nih.gov/cgi-bin/sis/search/r?dbs+hsdb:@term+@na+@rel+plutonium,+radioactive : absorption through the skin can occur through occupational exposure . experiments show that the skin is an effective barrier and the percentage absorbed /seldom/ exceeds 0.05% for intact skin . [ seiler , h.g. , h . sigel and a . sigel ( eds . ) . handbook on the toxicity of inorganic compounds . new york , ny : marcel dekker , inc . 1988 . , p . 724 ] peer reviewed source : plutonium anl factsheet oct 2001 plutonium metal . plutonium isotopes are primarily alpha-emitters so they pose little risk outside the body . here the plastic bag , gloves , and outer ( dead ) layer of skin would each alone stop the emitted alpha particles from getting into the body . what happens to it in the body ? when plutonium is inhaled , a significant fraction can move from the lungs through the blood to other organs , depending on the solubility of the compound . little plutonium ( about 0.05% ) is absorbed from the gastrointestinal tract after ingestion , and little is absorbed through the skin following dermal contact . after leaving the intestine or lung , about 10% clears the body . the rest of what enters the bloodstream deposits about equally in the liver and skeleton where it remains for long periods of time , with biological retention half-lives of about 20 and 50 years , respectively , per simplified models that do not reflect intermediate redistribution . the amount deposited in the liver and skeleton depends on the age of the individual , with fractional uptake in the liver increasing with age . plutonium in the skeleton deposits on the cortical and trabecular surfaces of bones and slowly redistributes throughout the volume of mineral bone with time . what is the primary health effect ? plutonium poses a health hazard only if it is taken into the body because all isotopes but plutonium-241 decay by emitting an alpha particle , and the beta particle emitted by plutonium-241 is of low energy . minimal gamma radiation is associated with any of these radioactive decays . inhaling airborne plutonium is the primary concern for all isotopes , and cancer resulting from the ionizing radiation is the health effect of concern . the ingestion hazard associated with common forms of plutonium is much lower than the inhalation hazard because absorption into the body after ingestion is quite low . laboratory studies with experimental animals have shown that exposure to high levels of plutonium can cause decreased life spans , diseases of the respiratory tract , and cancer . the target tissues in those animals were the lungs and associated lymph nodes , liver , and bones . however , these observations in experimental animals have not been corroborated by epidemiological investigations in humans exposed to lower levels of plutonium . as a note , the common myth that plutonium is the “deadliest substance known to man” is not supported by the scientific literature . it poses a hazard but is not as immediately harmful to health as many chemicals . for example , for inhalation – the exposure of highest risk – breathing in 5,000 respirable plutonium particles , about 3 microns each , is estimated to increase an individual’s risk of incurring a fatal cancer about 1% above the u.s. average “background” rate for all causes combined . ) edit : as an aside : i recommend reading eileen welsome 's the plutonium files : america 's secret medical experiments in the cold war to get some idea of what early plutonium health safety experiments really meant ( e . g . injecting a solution of plutonium salt into a patient 's leg ) .
it is because once the higgs couples two weyl fermions together , they become the two chiralities of a massive charged fermion . the standard 4-component spinor formalism disguises how natural this coupling is because it makes every fermion into a dirac fermion , and projects out the unphysical states at every vertex . if you do not do that , if you only include the 2 spinor corresponding to each physical field , the higgs coupling is extremely natural : it is the most general renormalizable gauge-invariant coupling of an su ( 2 ) doublet higgs with hypercharge 1/2 ( in one usual normalization ) to chiral weyl 2-spinors . you can think of a yukawa coupling as a mass term with a scalar field taking the role of the mass . the mass term for a charged fermion has to be of dirac type , because it must be invariant under phase rotations , and only the term $\bar\psi\psi$ is invariant . this means that higgs fields will always couple opposite chiralities of charged massive fermions , i.e. everything in the standard model that couples to the higgs . to see a situation where the coupling is for one chirality only , consider the nonrenormalizable two-higgs two lepton interaction : $$ h h l l $$ where l is the su ( 2 ) doublet left-handed lepton field , h is the higgs field , also an su ( 2 ) doublet , and each l 's su ( 2 ) index is contracted with one of the h 's su ( 2 ) index using the su ( 2 ) epsilon tensor , and the l 's lorentz indices are contracted with each other using the space-time epsilon tensor ( in 2 index formalism ) . this nonrenormalizable term becomes a neutrino majorana mass , and it is suppressed by a large scale , but it is the right order of magnitude to explain the majorana masses . in this case , the two-higgs field is coupled to a single chirality . the allowed couplings are always determined by matching all the su ( 2 ) , su ( 3 ) and lorentz indices together in a 2-spinor formalism , and it becomes the chirality restriction only by coincidence in the standard model . if you had a fundamental su ( 2 ) symmetric tensor higgs field t ( it would be the su ( 2 ) " spin 1" representation , not spin 1/2 ) with twice the hypercharge of the standard model higgs , it could give neutrinos a majorana mass with renormalizable yukawa couplings , just by replacing $hh$ with $t$ above , and contracting the indices the same way .
normal matter structure is entirely constructed from the electronic bindings , so it is in the realm of the possible to engineer how the atoms are binded together exactly and this is the aim of lower-level nanotechnology . however , it is with current technogy and physics , impossible to create complex structures at lower scales ( i.e. : nuclear scale ) . and i do not think that is something that is in principle possible unless some dramatic breakthrough occurs in how we obtain larger-scale nuclear matter , which from the experimental limitations that there are to obtain high z nucleus that might probe the regions of higher islands of stability , one can safely infer that this is a very challenging problem in of itself update : this is not entirely related , but it shows an example of how assumptions as the one i have made above about manipulation of small scales being out of engineering reach can be twisted : this slide about non-homogeneous diffraction crystals shows how to do something that most physicists have thought for long to be essentially impossible ; x-ray and gamma-ray optics
you surely need to consider einstein 's field equations at some point because the stress-energy tensor in general relativity is defined as the object that is set equal to the einstein curvature tensor ( with the appropriate coefficient ) . so if you vary the whole action $$ \int \left ( \frac{r}{16\pi g} +{\mathcal l}_{\rm matter} \right ) \sqrt{-g}\ , d^d x $$ with respect to the metric , you will simply obtained einstein 's equations that has a multiple of the einstein tensor and a multiple of the stress-energy tensor defined in the way you mentioned . what you could have asked is why this definition of the stress-energy tensor is equivalent to other definitions you may know – i.e. non-gravitational ones . to answer this question , you would have to mention which other definition you mean . well , you would probably mean the stress-energy tensor that is locally covariantly conserved , $\nabla_\mu t^{\mu\nu}=0$ in the context of general relativity . you could derive this stress-energy tensor by the noether procedure by considering spacetime translations in non-gravitational theory , and then by covariantizing the result that you get in some right way . why is the conserved stress-energy tensor the same thing as the stress-energy tensor obtained by varying the metric ? they have to be the same because einstein 's equations set the stress-energy tensor equal to a multiple of the einstein tensor and the latter is covariantly conserved as a matter of identity . the equation $$\nabla_\mu g^{\mu \nu}=0$$ holds identically , for any metric tensor field configuration , and because the einstein tensor is set proportional to the stress-energy tensor , the same condition ( vanishing of the covariant divergence ) has to hold for the stress-energy tensor , too . this is no accident and this fact may be formulated in other ways . for example , we say that gravity has to use the diffeomorphism symmetry ( it is doubly important as the gauge symmetry in the quantum mechanical framework to get rid of the unphysical , negative-norm polarizations of the gravitons ) . such a local symmetry has to be coupled to a " conserved current " ( conserved stress-energy tensor ) for consistency . you will therefore typically end up with the same formula for both stress-energy tensors . however , they may end up differ by certain terms whose covariant derivative is zero identically as well . for example , the non-gravitational conserved stress-energy tensor is not always quite canonical and it does not even have to be a symmetric tensor . the gr definition of the tensor is automatically symmetric .
imagine that you are on a train , traveling at a steady speed of 50 miles per hour ( mph ) . your physics textbook on the table in front of you . now , you and the textbook ( and the train ) are all moving at the same speed . to an outside observer standing next to the train tracks , you and the book are each rushing by at 50mph . but , from your point of view , the book is not moving at all . that is , it is not getting closer to or farther from you . you are moving at 50mph relative to the observer next to the tracks . you are not moving , relative to the textbook . the book , the train , and yourself are not moving at all , relative to each other .
your brain has mainly 3 tricks for determining how close an object is . pure picture analysis . this is the part where the brain combines perspective and past experience of how big objects should be , to decide how far away something is . this is the only manner of depth perception you have available to you when watching conventional pictures and movies , and it can be toyed with , as in this video by richard wiseman . comparing the perspective of your two eyes . this is the one that becomes active in 3d movies . the easiest way to understand how it works is to close one eye , and bob your head from side to side , while focusing on a single object ( this might make you look like a fool ) . you can see things appear to move relative to one another , and things that are farther back seems to be more stationary . when you have both eyes open , you do not need the bobbing , as you already have input from two different perspectives , and a brain well versed in interpreting it . focal strength . the third way your brain percieves distance is it gauges the lenses in your eye . this in turn becomes a measure of how far away an object is . if you close one eye and hold one object quite close to your open eye , and shift focus from that object to the background immediately behindto it , i can almost promise you that your closed eye moves away from your nose . this is because your brain is anticipating that the lesser focus means you are looking at something farther back , and thus moves the other eye to try to focus on that instead ( even though it can not see anything ) . i have heard that number the reason many people have trouble with headaches etc . after watching 3d-movies , is that because no matter how much the perspective and eye movement tries to fool your brain into thinking that objects are closer and further back , your focal muscles are always focused on a single distance some 50-200 feet away ( the canvas ) . this dissonance makes your brain uncomfortable , and it will have a real physiological effect , just as dissonance between what your eyes see and your inner ear balance organ feels makes you nauseous ( that is motion sickness ) .
i do not know the exact number but i want to support johannes ' claim that the percentage is way smaller by a calculation . most of the light arguably comes from the milky way - especially the strip that gave name to the galaxy . the diameter of the milky way is 100,000-120,000 light years so the median star 's distance is something like 50,000 light years away from us . that is approximately $3\times 10^{9}$ times longer a distance than those 500 seconds for the sun . one must square the distance ratio to get the light power ratio , about $10^{17}$ , between the sun and the typical milky way star . even when $10^{-17}$ is multiplied by the number of stars in the milky way , about $1-4\times 10^{11}$ stars , one gets $1-4$ parts per million of the light , also assuming that the sun is an average-size star . my estimate is 3 orders of magnitude greater than johannes ' but it is still vastly smaller than 0.5% . just to check , sirius is the brightest star in the sky . it is 25 times brighter than the sun but it is 9 light years away , which is $500,000$ times further than the sun . square it and divide 25 by it to get $10^{-10}$ . that is the fraction of the sunlight obtained from sirius . you see that it is much smaller than the result for the generic milky way stars above , so individual bright ( and mostly nearby ) stars are unlikely to topple the statistical estimate . the weakest point of the statistical estimate is that the sun is not quite the average star . one may also check the contribution from other galaxies . there are about $2\times 10^{11}$ galaxies in the universe . however , even if you decide that the average distance from us is 5 billion years only , shorter than half of the age of the universe , it is 100,000 times further than the average milky way star discussed above ( 50,000 light years ) . square it to get $10^{10}$ for the ratio . if you multiply $10^{-10}$ by $10^{11}$ , you actually conclude that the total light from other galaxies is about 10 times greater than the total light from the milky way . but that is probably an overestimate because much of the very distant galactic light is redshifted , absorbed , and the older galaxies may have a lower luminosity . at any rate , it is unlikely that they will drive us above 1/100,000 of the sunlight . finally , instead of trying even more distant stars , let me mention that there is also the moon in the sky . it is actually dominating or almost dominating the luminosity at night , except for the new moon or eclipses . in average , we get 1 milliwatt from the moonlight which is 1/300,000 of the sun 's 342 watts ( averaged over places , seasons , day cycles ) . that is about the same what i got for the total strip of stars in the milky way – 3 parts per million of the sun – but my estimate of the stars was probably an overestimate and i believe the moon is brighter than the milky way combined .
there are a couple of nonlinear magnetic material effects that might be at play here , although this answer must be described as speculation without more detail . both effects are more pronounced if your inductor is ungapped . 1 ) at very low current levels ( corresponding to very low levels of magnetic field h ) , the inductance can be lower than nominal . ( the b-h characteristic of the magnetic core material has a lower slope right at the origin . ) as current increases from these low levels , the calculated inductor impedance $z_l =2 \pi f l$ would increase and then stabilize at the nominal value . 2 ) as current continues to increase , eventually the inductor starts to saturate . ( the b-h characteristic flattens at high fields . ) inductance then becomes a decreasing function of current , so the calculated inductor impedance would decrease . you can see some b-h curves illustrating these effects in the wikipedia article on " saturation ( magnetic ) " . introducing a gap in the magnetic core reduces the component 's inductance but stabilizes it against these effects .
yes , it is simple to prove using moment generating functions . and yes , the mathematics is very closely related to that of quantum field theory . you compute $g ( j ) = &lt ; exp ( \sum j_i x_i ) &gt ; $ where each $j_i$ is a " source " for the corresponding $x_i$ . this is easily shown to be something like $g ( j ) = exp ( \sum j_i \mu_{ij}^{-1} j_j ) $ to get expectation values you then take $ &lt ; x_i x_j . . . &gt ; = \frac{\partial}{\partial j_i} \frac{\partial}{\partial j_j} . . . g ( j ) |_{j=0}$ . the rest follows simply . in particular , you can see how the variables must be grouped in pairs to get a nonzero results when you set $j=0$ after taking derivatives . you essentially have a feynman expansion of a non-interacting 0-dimensional field theory . this is covered nicely in zee 's " quantum field theory in a nutshell " , where it is a simple application of what he calls the " central identity of quantum field theory "
both formulas are equivalent , if you are in the electrostatic approximation and your dipole vector does not depend on the position $\mathbf{r}$ . let 's consider the expression $\mathbf{f}=\nabla_{\mathbf{r}} ( \mathbf{p} \cdot \mathbf{e} ) $ which can be easily obtained from the potential energy function $u=-\mathbf{p} \cdot \mathbf{e}$ and its relation with the force $\mathbf{f}=\nabla_\mathbf{r} u$ . now , recall the vector identity $\nabla_\mathbf{r} ( \mathbf{a}\cdot \mathbf{b} ) = ( \mathbf{a} \cdot \nabla_\mathbf{r} ) \mathbf{b}+ ( \mathbf{b} \cdot \nabla_\mathbf{r} ) \mathbf{a} + \mathbf{a} \times ( \nabla_\mathbf{r} \times \mathbf{b} ) + \mathbf{b} \times ( \nabla_\mathbf{r} \times \mathbf{a} ) $ for $\mathbf{a}=\mathbf{a} ( \mathbf{r} ) $ and $\mathbf{b}=\mathbf{b} ( \mathbf{r} ) $ two arbitrary vectors . for $\mathbf{p}=\mathbf{a} \neq \mathbf{p} ( \mathbf{r} ) $ [ independent of the position ] and $\mathbf{b}=\mathbf{e} ( \mathbf{r}$ ) we have $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}+ ( \mathbf{e} \cdot \nabla_\mathbf{r} ) \mathbf{p} + \mathbf{p} \times ( \nabla_\mathbf{r} \times \mathbf{e} ) + \mathbf{e} \times ( \nabla_\mathbf{r} \times \mathbf{p} ) $ as the dipole vector does not depend on the position we can drop the second and the fourth terms . in the electrostatic approximation , faraday 's law reads $\partial_t \mathbf{b}=\mathbf{0}\leftrightarrow \nabla_\mathbf{r} \times \mathbf{e} ( \mathbf{r} ) =\mathbf{0} $ [ this is known as ''carn 's law'' ] so that the electric field is irrotational and the curl vanishes . then we can drop the third term and $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}$ so that your definitions agree .
yes , if it is not a plastic covered car it is an effective faraday cage . if the tires are such that the car is insulated electrically , if it is hit it will take some time to discharge to the ground , but still the passengers would be safer than standing next to it outside . i have learned that modern tires are particularly constructed so that the static charge generated by the friction on the road is discharged so that would also help . ( in olden times they used to have chains trailing from the trucks in order to discharge the static . recently i saw a car with a discharger too , trailing on the road ! ) . lightning is essentially just an huge electric arc from the clouds to the ground , correct ? wrong , the current actually may start from the ground . that is the rational of the lightning rods , to create a path for a current to be generated by the potential difference to the cloud and to meet the current from the clouds in a prefered location instead of a random one . it is not wise to stand next to a rod , read in the link the amount of power dissipated by a bolt . the average peak power output of a single lightning stroke is about one trillion watts — one " terawatt " ( 1012 w ) , and the stroke lasts for about 30 millionths of a second — 30 " microseconds " . [ 18 ] and it is not wise to stand , because you may also give rise to leaders that will meet the lightning path . if in the open it is best you fall on the ground as much sheltered as possible . a colleague once was about 20 meters from a lightning bolt , and he was so shocked by the sound and fury , it took him a week to come down to normal .
i think you are sort of reversing the logic of chirality and helicity in the massless limit . chirality defines which representation of the lorentz group your weyl spinors transform in . it does not ' become ' helicity , helicity ' becomes ' chirality in the massless limit . that is , chirality is what it is , and it defines a representation of a group and that can not change . this other thing we have defined called helicity just happens to be the same thing in a particular limit . now once you take the massless limit the weyl fermions are traveling at the speed of light you can no longer boost to a frame that switches the helicity . i think its best to think of a fermion mass term as an interaction in this case and remember that the massive term of a dirac fermion is a bunch of left and right- handed weyl guys bumping up into one another along the way . conversely if you want to talk about a full massive dirac fermion that travels less than c and you can boost to change the helicity , but that full dirac fermion is not the thing carrying weak charge , only a ' piece ' of it is . see this blog post on helicity and chirality . as far as the left-right symmetry being broken people have certainly built models along these lines but i do not think they have worked out . does this answer your question ?
i think you are misinterpreting the statement that " it does not have any effect " . this statement does not mean that the faddeev-popov methodology " does not work " , as you wrote later . instead , it means that it is completely unnecessary . if you look at the faddeev-popov ghosts ' lagrangian , you will see that for abelian groups , the structure constants $f_{abc}$ vanish and we are left with $$ {\mathcal l}_{\rm ghost} = \partial_\mu \bar c^a \partial^\mu c^a $$ which means that the ghosts are completely decoupled . they do not interact with the gauge fields ( photons ) . you may still use the faddeev-popov machinery and the brst formalism based upon it to identify the physical states as the cohomologies of $q$ , the brst operator . but what this brst machinery tells you is something you may easily describe without any faddeev-popov ghosts , too . it just tells you that the excitations of $\bar c , c$ are unphysical much like the excitations of time-like and longitudinal photons . that is why the brst problem in the case of abelinan gauge groups is " solvable " in such a way that you may simply eliminate the ghosts completely , together with 2 unphysical polarizations of the photon . and that is why qed may be taught without any faddeev-popov ghosts and one may still construct nice feynman rules for any multiloop diagrams . for non-abelian theories , the counting still works – ghosts , antighosts , and two polarizations of gluons etc . are unphysical . however , because there are interactions of ghosts with the gluons in that case , there is no easy way to describe the physical states without the faddeev-popov ghosts .
actually , there are two different viscosity coefficients . you can see this from the stress tensor $$ \sigma_{ij} = -p_0 \delta_{ij} + \eta \left ( \frac{\partial v_i}{\partial x_j} + \frac{\partial v_j}{\partial x_i} - \frac{2}{3} \delta_{ij} \frac{\partial v_k}{\partial x_k} \right ) + \zeta \delta_{ij} \frac{\partial v_k}{\partial x_k} $$ which has the two coefficients of viscosity $\eta$ and $\zeta$ ( see landau and lifshitz , fluid mechanics , for example ) . the pressure $p_0$ is given by the thermodynamic equation of state , but this is not the whole pressure $p$ . the latter is given by the mean normal stress $$ p = - \frac{1}{3} \sigma_{ii} = p_0 - \zeta \frac{\partial v_k}{\partial x_k} $$ so that the stress tensor is $$ \sigma_{ij} = -p \delta_{ij} + \eta \left ( \frac{\partial v_i}{\partial x_j} + \frac{\partial v_j}{\partial x_i} - \frac{2}{3} \delta_{ij} \frac{\partial v_k}{\partial x_k} \right ) . $$ that is why sometimes you do not see the coefficient $\zeta$ ( often called second viscosity ) in the navier-stokes equation . it is hidden in the pressure , but it is there .
how galilean transformations which are wrong ( are approximately correct ) give the correct answer for k ? the lorentz prediction and the galilean prediction must agree in the limit that $v \to 0$ ( or in the limit that $c \to \infty$ ) . this is because $v=0$ corresponds to no transformation at all , so they had better both agree there . so if you take the transformation and evaluate it for smaller and smaller $v$ , you will find that $k=1$ still has to be true . why we should assume that there are two electric fields , one in the lab frame and one in the other , but just one magnetic field in both frames ? that is just the galilean transformation of the em field . to see how it relates to the relativistic case , the lorentz transformation of the em field is : $$\mathbf{e}' = \gamma \left ( \mathbf{e} + \mathbf{v} \times \mathbf{b} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{e} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ $$\mathbf{b}' = \gamma \left ( \mathbf{b} - \frac {\mathbf{v} \times \mathbf{e}}{c^2} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{b} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ when you take the limit that $c \to \infty$ , we know that $\gamma \to 1$ , so it just becomes : $$\mathbf{e}' = \mathbf{e} + \mathbf{v} \times \mathbf{b}$$ $$\mathbf{b}' = \mathbf{b}$$
it is tempting to think of an electron as a little point , but in most systems electrons are delocalised , that is they do not have a well defined position . as a general rule , if an electron is tightly bound to something then it more localised than if it is nearly free . so for example the electron in a hydrogen atom is bound to the proton so it is localised to a sphere with a radius of a tenths of nanometres ( though it is still impossible to say exactly where within that sphere the electron is . by contrast electrons in the conduction bands of metals behave as if they are almost free , and indeed you can calculate their properties assuming they are free and get reasonably good answers . the point of all this rambling is that if you add two electron to a conductor those two electrons do not have a precisely defined position but are ( at least in principle ) spread out over the conductor . so the charge density on the surface can be even because the two electrons are delocalised over the whole surface of the conductor . i should add that this is a somewhat idealised scenario and would apply only to a perfectly spherical superconductor that is totally isolated . in real conductors imperfections would change the delocalisation of the electrons . likewise bringing up test charges or anything similar to the conductor will interact with the electrons and partially localise them .
i ) the $x$-boost formula of the stand-up physicist ( also known as doug sweetser ) can be simplified to $$\tag{1} q^{\prime}-bq\bar{b}~=~ \bar{q} \frac{\bar{b}^2-b^2}{2} $$ $$~=~ - \bar{q} i {\rm im} ( b^2 ) ~=~ - \bar{q} i\sinh ( 2\alpha ) ~=~ ( q_{\perp}-\bar{q}_{\parallel} ) i\sinh ( 2\alpha ) , $$ where $$b~:=~\cosh ( \alpha ) +i \sinh ( \alpha ) ~\in~ \mathbb{c} , \qquad b^2~=~ 1+ i \sinh ( 2\alpha ) , \qquad |b|^2~=~\cosh ( 2\alpha ) , $$ $$q~:=~q_{\parallel}+q_{\perp}~\in~ \mathbb{h}~\cong~m ( 1,3 ; \mathbb{r} ) , \qquad q_{\parallel}~:=~t+ix , \qquad q_{\perp}~:=~jy+kz . $$ formula $ ( 1 ) $ indeed reproduces the well-known lorentz transformation formulas for $x$-boosts with rapidity $2\alpha$ , $$ t^{\prime}~=~t\cosh ( 2\alpha ) - x \sinh ( 2\alpha ) , \qquad x^{\prime}~=~x\cosh ( 2\alpha ) - t \sinh ( 2\alpha ) , $$ $$ y^{\prime}~=~y , \qquad z^{\prime}~=~z , $$ or equivalently , $$ q^{\prime}_{\parallel} ~=~q_{\parallel}\cosh ( 2\alpha ) -\bar{q}_{\parallel} i \sinh ( 2\alpha ) , \qquad q^{\prime}_{\perp}~=~q_{\perp} . $$ this is because $$bq\bar{b}~=~b ( q_{\parallel}+q_{\perp} ) \bar{b}~=~q_{\parallel}|b|^2+ q_{\perp}\bar{b}^2~=~q_{\parallel}\cosh ( 2\alpha ) + q_{\perp} ( 1- i \sinh ( 2\alpha ) ) . $$ ii ) as is well-known , the $x$-boosts $b$ form an abelian non-compact $u ( 1 ) $ subgroup of the lorentz group $so ( 1,3 ; \mathbb{r} ) $ . since the pertinent group acts freely on minkowski space , the $x$-boost formula $ ( 1 ) $ must also respect this group structure . in terms of rapidity $2\alpha$ , this abelian subgroup is just the additively written group $ ( \mathbb{r} , + ) $ , cf . this and this phys . se post . iii ) there are many important descriptions of the minkowski space $m ( 1,3 ; \mathbb{r} ) $ and the lorentz group ( meaning both rotations and boosts ) , but as far as i can tell , the quaternions $\mathbb{h}$ are not too useful in this respect . for instance , the set of hermitian matrices $u ( 2 , \mathbb{c} ) $ seems to be a much more powerful description of minkowski space $m ( 1,3 ; \mathbb{r} ) $ , cf . this phys . se post . in contrast , the quaternions $\mathbb{h}$ play an important role for the euclidean compact counterpart $so ( 4 , \mathbb{r} ) $ of the lorentz group $so ( 1,3 ; \mathbb{r} ) $ , because the lie group $u ( 1 , \mathbb{h} ) \times u ( 1 , \mathbb{h} ) $ is ( a double cover of ) $so ( 4 , \mathbb{r} ) $ . iv ) the lie group $$u ( 1 , \mathbb{h} ) ~:=~\{r\in \mathbb{h} \mid \bar{r}r=1\}~\cong~ su ( 2 , \mathbb{c} ) $$ is ( a double cover of ) the rotation group $$so ( 3 , \mathbb{r} ) ~\cong~ so ( {\rm im} ( \mathbb{h} ) , \mathbb{r} ) , $$ which in turn is both a subgroup of $so ( 4 , \mathbb{r} ) $ and $so ( 1,3 ; \mathbb{r} ) $ . a rotation $r$ is implemented as $$\tag{2} \tilde{q}~=~rq\bar{r} , \qquad q\in \mathbb{h} , \qquad r\in u ( 1 , \mathbb{h} ) . $$ similarly , $$ \qquad \tilde{q}^{\prime}~=~rq^{\prime}\bar{r} , \qquad \tilde{b}~=~rb\bar{r} , \qquad \tilde{i}~=~ri\bar{r} . $$ hence , the formula $ ( 1 ) $ behaves covariantly under rotations , i.e. formula $ ( 1 ) $ holds not just for boosts $b$ in the $x$-direction , but for boosts $b$ in any direction ! v ) potential problems lie elsewhere . is this formula $ ( 1 ) $ interesting ? i fail to see how formula $ ( 1 ) $ could be useful ( as compared to more potent standard approaches ) . it is apparently not lorentz covariant . why have a formula that works for boosts , but not for rotations , cf . formula $ ( 2 ) $ ? note that two successive boosts in different directions do in general not constitute a pure boost , i.e. boosts in generic directions do not form a proper subgroup .
your professor is correct , but i agree with you that the statement “vorticity can’t be destroyed or created” seems jarring - i would prefer to think of this as “vorticity is conserved” because the conservation of vorticity derives from the navier-stokes eq and the conservation of angular momentum . i confess this is splitting terminology hairs ( don’t push it with your professor ) but i think it helped me . so , i think , maybe i can understand this as an analogy with linear momentum , because linear momentum is conserved too . i remember the problem of a car of mass m , traveling toward the right at velocity v , and on the same road an identical car traveling to the left at velocity –v . they collide head-on and smash and stick together . velocities after the crash – zero . momentum after the crash – zero and of course , momentum is conserved . the total momentum of the system was zero before and after . let say your container filled with water is a long annulus with thick steel walls . the flow is initially a circular flow around the axis ( i.e. . 2d flow . ) what is initial total angular momentum of the system ? eventually the fluid stops moving , so the final total angular momentum of the system must be zero . how do we show that the initial angular momentum is zero too ? at this point you need to recognize that the vorticity vector in the moving fluid is everywhere parallel to the axis of the container . and you need to use stokes’ theorem to write an integral equation with a line integral on the lhs ( the circulation ) and a surface integral on the rhs ( vorticity integrated over the container cross section . ) \begin{align*} \oint_{c} v \cdot dl = \int_{s} w \cdot ds\ \end{align*} take your integration path ( the closed path c ) entirely inside the steel wall of your container . the velocity inside the container wall is always zero , and so the circulation along the path is always zero , and so the total vorticity across the cross-sectional area ( the area s ) of the container and fluid is always zero too . you can calculate for yourself that the boundary layer vorticity is equal and opposite to the bulk viscosity using a similar approach . imagine spinning the container about its axis at a constant angular velocity . eventually the entire viscous-fluid and container system will be rotating like a rigid body around the axis . every point has the same angular velocity , and there is now a vortex located at the center . compute the circulation for any closed path that includes the vortex inside it – this will be the strength of the vortex , and the magnitude and sign of vorticity in the bulk fluid . you can show yourself that the strength of this vortex is the total vorticity . compute the circulation around any path that does not include the vortex , this will always come out to be zero . pick a path near the fluid-container boundary , s’ , so half of it is in fluid and half is inside the container wall , as long as the container and fluid are still rotating together the circulation around this path will be zero too . now stop the container’s rotation . the fluid continues to move . compute the circulation around the path s’ again , it is no longer zero and is the vorticity at the boundary layer . it’s sign is opposite that of the vortex at the center . every point along the fluid-boundary can be associated with path like s’ and a small amount of boundary layer vorticity . integrate around the entire boundary and the sum will be equal in magnitude and opposite in sign to the strength of the vortex at the center . eventually , boundary layer vorticity will diffuse towards the center and annihilate the center vortex . @isopycnal_oscillation is correct to point out that in 3d , and particularly near turbulent conditions , vorticity is not conserved . the second term on the rhs of your ‘transport equation’ says that the stretching and tilting of vortex tubes can change vorticity too . however , i expect that in the classes where your professor is fond of saying that “vorticity cannot be destroyed” turbulent flow is seldom if ever encountered . finally , assuming that the lhs of your ‘transport equation’ equals zero does not necessarily require that the fluid be inviscid or that the problem be 2d – you are assuming that the terms on the rhs happen to cancel exactly and the vorticity is fortuitously ‘steady-state . ’ so yes , that is a very strong assumption to accept .
first normalize the state to find $a$ . then you need to express the state as a superposition of the stationary states of the infinite square well : $$ \psi\left ( x\right ) = a x \left ( a-x\right ) = \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) , $$ where $\psi_n\left ( x\right ) = \sqrt{2/a} \sin\left ( n \pi x / a\right ) $ is the $n$-th stationary state . you can do this using the orthogonality of the stationary states , $$ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) = \frac{2}{a} \int_0^a dx \ \sin\left ( \frac{m \pi x}{ a}\right ) \sin\left ( \frac{n \pi x}{ a}\right ) = \delta_{mn} , $$ by integrating the equation above : $$ \begin{align} \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ a x \left ( a-x\right ) \right ] and = \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \left [ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \delta_{m n} \\ and = c_m \end{align} $$ i will leave the $c_n = a \sqrt{2/a} \int_0^a dx \ \sin\left ( n \pi x / a\right ) x \left ( a-x\right ) $ integral for you to work out . once you have the $c_n$ 's , the most likely value of a measurement of the energy is the energy corresponding to the stationary state with maximum $c_n$ . to find the probability of measuring $9 \hbar^2 \pi^2 / 2 m a^2$ for the energy , determine the stationary state that this energy corresponds to , and compute $\left|c_n\right|^2$ . for the time evolution , since the potential is $0$ everywhere after $t=0$ , it is a free particle , and the general solution is : $$ \psi\left ( x , t\right ) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty dk \ \phi\left ( k\right ) \exp\left [ i\left ( k x + \frac{\hbar k^2}{2 m} t\right ) \right ] , $$ where $$ \phi\left ( k\right ) = \frac{1}{\sqrt{2 \pi}} \int_0^a dx \ \psi\left ( x , 0\right ) \exp\left ( -i k x\right ) = \frac{a}{\sqrt{2 \pi}} \int_0^a dx \ x\left ( a-x\right ) \exp\left ( -i k x\right ) . $$ so , now you just have to do this integral .
firstly , as the wedge is movable , it will move when the object slides onto it ( i guess that is what they meant ) . the steps you may follow : momentum is conserved . so , the initial momentum of the system will be $mu$ . the final momentum will be the combined momentum of the wedge and the block $ ( m+m*eta ) v$ . from here , you can find an expression for the final velocity in terms of u 2 . using conservation law of energy , $$k . e_i = k . e_f+mgh$$ 3 . by solving this , you can solve for minimum value of u
the best approximation i found so far is in bloembergen : nonlinear optics . it is stated that successive orders of polarisation are reduced by a factor $|e|/|e_{at}|$ , with the applied electric field $e$ and the atomic electric field $e_{at}$ . this will be sufficient for my case .
the sign of the gauge part of the covariant derivative is a convention , you can choose it any way you want , it just defines the sign of a . this sign has nothing to do with the metric convention , mostly + or mostly - . its arbitrary in either convention . the second part is just differentiating both sides of the previous equation for $d_0\phi$ , there is a t in the right hand side . so it is $\partial_0 ( t d_0 \phi ) $ , since a_0 is infinitesimal and gives a higher order correction , and he keeps the first part of this , where you differentiate t with respect to t , and ignores the second part , since time derivatives of $\phi$ are small by assumption that the monopole is stationary at t=0 and slowly accelerating .
the blades of a ceiling fan are pitched out of plane slightly . as a result , when the fan spins , the blades push air either up towards the ceiling or down towards the floor . which direction it pushes air is determined by the direction the fan is spinning , and the direction the blades are pitched . the usual convention is given by the right hand rule : if you hold your right hand so that you can curl your fingers in the direction the fan is spinning , then it will push air in the direction that your thumb is pointing . 1 when it is pushing air down on you , it will then be spinning in a counter clockwise direction as you look up at it . you can make it follow a left hand rule by reversing the pitch of the blades . once you have set the direction of the pitch of the blades , you can reverse the airflow by reversing the direction the fan spins . many ( most ? ) modern ceiling fans provide some mechanism to do this . the fans in my house have a small black switch that slides up and down . @ignacio vazquez-abreams mentions using a pull chain in a comment to another answer . in warm weather , you set the fan so that it pushes air down towards the floor . this causes you to feel a breeze , which cools you . in cold weather , you set the fan so that it pushes air up into the ceiling . you do not feel a breeze , 2 but it circulates warm air from the ceiling towards the walls and down towards the floor . box fans usually follow the right hand rule as well . you may find it easier to check some of this if you have a box fan handy . you can walk around the fan and hold a tissue in front of it from both side while watching the blades spin . you can check this with a box fan . set the box fan on the floor , and turn it on . it will pull air from one side and push it out the other . you feel a breeze on the " out " side and feel a much weaker breeze ( if any ) on the " in " side .
this was intended to be a comment , but is too long so i will post it as an answer . first of all , a disclaimer , i am a physict and all that i know about quantitative finance comes from self-learning , so please feel free of correcting me if i am mistaken ( also in the physics stuff , of course ! ) . i have been doing a little of research , and perhaps you are right in your last point . the black-scholes pde relies in the assumption of ( i ) the option prize is a continuous function of time and the undelying asset and ( ii ) the current stock price follows a [ geometric ] brownian motion . from the physics point of view , there is a deep connection between brownian motion and the diffusion equation which is exemplified in the famous einstein relation . as the heat equation is a particular form of the diffusion equation , it is not so surprising that the heat kernel appears in some of the solutions of the black-scholes pde . however , there is no physical similarity between these two phenomena as they both do not describe physical processes but only are based in the same mathematics [ stochastic calculus ] . as a curious fact , i was reading not long time ago the book when genius failed : the rise and fall of long-term capital management . there it is said that myron scholes and specially robert c . merton developped all their option pricing theory mimicking physical models [ taken from research papers and books of statistical mechanics ] and having faith in the so-called " efficient market hypothesis " . the history of ltcm is well-known in finance , they lose billions of dollars after having been leveraged $250$ to $1$ [ that is they invested $250$ dollars while having actually $1$ ] .
lense-thirring frame dragging has been measured to about 10 or 20% accuracy by two satellite experiments . look up lageos and gp-b ( gravity probe b ) .
a superposition sector is a subspace of the hilbert space ${\mathcal h}_i$ such that the total hilbert space of the physical system may be described as the direct sum $$ {\mathcal h} = {\mathcal h}_1 \oplus {\mathcal h}_2 \oplus\cdots \oplus {\mathcal h}_n$$ where $n$ may be finite or infinite such that if the state vector belongs to one of these superselection sectors $$|\psi ( t ) \rangle\in{\mathcal h}_i , $$ then this property will hold for all times $t$: it is impossible to change the superselection sectors by any local operations or excitations . an example in the initial comments involved the decomposition of the hilbert space to superselection sectors ${\mathcal h}_q$ corresponding to states with different electric charges $q$ . they do not talk to each other . a state with $q=-7e$ may evolve to states with $q=-7e$ only . in general , these conservation laws must be generalized to a broader concept , " superselection rules " . each superselection rule may decompose the hilbert space into finer sectors . it does not mean that one can not write down complex superpositions of states from different sectors . indeed , the superposition postulate of quantum mechanics guarantees that they are allowed states . in practice , we do not encounter them because the measurement of total $q$ – the identification of the precise superselection sectors – is something we can always do as parts of our analysis of a system . it means that in practice , we know this information and we may consider $|\psi\rangle$ to be an element of one particular superselection sector . it will stay in the same sector forever . in quantum field theory and string theory , the term " superselection sector " has still the same general meaning but it is usually used for different parts of the hilbert space of the theory – that describes the whole spacetime – which can not be reached from each other because one would need an infinite energy to do so , an infinite work to " rebuild " the spacetime . typically , different superselection sectors are defined by different conditions of spacetime fields at infinity , in the asymptotic region . for example , the vacuum that looks like $ads_5\times s^5$ ground state of type iib string theory is a state in the string theory 's hilbert space . one may add local excitations to it , gravitons , dilatons ; - ) , and so on , but that will keep us in the same superselection sector . the flat vacuum $m^{11}$ of m-theory is a state in string theory 's hilbert space , too . there are processes and dualities that relate the vacua , and so on . however , it is not possible to rebuild the spacetime of the $ads$ type to the spacetime of the $m^{11}$ time by any local excitations . so if you live in one of the worlds , you may assume that you will never live in the other . different asymptotic values of the dilaton ; - ) or any other scalar field ( moduli . . . ) or any other field that is meaningful to be given a vev define different superselection sectors . this notion applies to quantum field theories and string theory , too . in particular , when we discuss string theory and its landscape , each element of the landscape ( a minimum of the potential in the complicated landscape ) defines a background , a vacuum , and the whole ( small ) hilbert space including this vacuum state and all the local , finite-energy excitations is a superselection sector of string theory . so using the notorious example , the f-theory flux vacua contain $10^{500}$ superselection sectors of string theory . in the case of quantum field theory , we usually have a definition of the theory that applies to all superselection sectors . a special feature of string theory is that some of its definitions are only good for one superselection sector or a subset of superselection sectors . this is the statement that is sometimes misleadingly formulated by saying that " string theory is not background-independent " . physics of string theory is demonstrably background-independent , there is only one string theory and the different backgrounds ( and therefore the associated superselection sectors – the empty background with all allowed local , finite-energy excitations upon it ) are clearly solutions to the same equations of the whole string theory . we just do not have a definition that would make this feature of string theory manifest and it is not known whether it exists .
classically a non-pointlike spinning charged object possesses a magnetic dipole moment due to the fact that charged particles in the object are spinning around some axis . in contrast , the electron has a dipole moment that arises from its intrinsic spin angular momentum . as you point out , the electron has no internal structure , so the spin does not refer to actual physical spinning . the dipole moment has spatial dimensions outside of the point where the electron exists because it arises from the quantum spin property of the electron , it is not itself a property of the electron . the magnetic dipole moment of the electron is related to its spin in the way described here .
it really depends on how more gas is being " pumped in " . for example , a typical pump will take air in at ( roughly ) one atmosphere , and pressurize it by doing work on it , which will increase the temperature . on the other hand , if the gas is coming from a cylinder of compressed air , it will cool on entering your volume , so the temperature would actually drop . you can even imagine a " maxwell 's demon compressor " , which just allows a molecule to enter whenever no other molecule would leave . this should not change the temperature at all .
i believe your mistake is with units , and it is the following : $$t=\dfrac{m [ c_{rms} ] ^2}{3r} = \dfrac{\left ( 1 \text{amu} \right ) [ 11.2 \frac{km}{s} ] ^2}{3 \left ( 8.3144621 \frac{\mathrm{j}}{\mathrm{\text{mol} k}} \right ) } $$ this does not even cancel out because you are left with a $\text{mol}$ unit . add avogadro 's number . $$t = \dfrac{\left ( 1 \text{amu} \right ) [ 11.2 \frac{km}{s} ] ^2}{3 \left ( 8.3144621 \frac{\mathrm{j}}{\mathrm{\text{mol} k}} \right ) } \left ( 6.022 \times 10^{23} \frac{1}{\text{mol}} \right ) = 5,028 k $$ this is the case for earth . for the moon : $$t = \dfrac{\left ( 1 \text{amu} \right ) [ 2.4 \frac{km}{s} ] ^2}{3 \left ( 8.3144621 \frac{\mathrm{j}}{\mathrm{\text{mol} k}} \right ) } \left ( 6.022 \times 10^{23} \frac{1}{\text{mol}} \right ) = 230 k $$ this is negative in celsius units . this is not a problem . it is merely saying that even freezing temperatures are enough for a lone hydrogen atom to escape the gravity of the moon with . all we required was that this number be less than the temperature of the sun , which it is . any surface that faces away from the sun will again see those photons within a month , unless it is in a crater on one of the poles , which we know can have ice near the surface . so that makes sense .
you do not need gauss ' law . coulomb 's law will do the trick here along with some calculus .
the electric field is the vector sum $$ {\vec e}~=~\frac{1}{4\pi\epsilon_0}\big ( \frac{q_1{\bf n}_1}{2d^2}~+~\frac{q_2{\bf n}_2}{d^2}\big ) $$ so the components of the field are $$ e_x~=~\frac{1}{4\pi\epsilon_0}\frac{q_1}{2\sqrt{2}d^2} $$ $$ e_y~=~\frac{1}{4\pi\epsilon_0}\frac{q_1}{d^2}\frac{1~+~2\sqrt{2}}{2\sqrt{2}} $$ the rest is plug and grind on numbers .
the reduced matrix is defined as the partial trace of the density matrix . be $a$ , $b$ finite dimensional hilbert spaces and be $t$ $\in$ $l ( a \otimes b ) $ ( linear operators on $a \otimes b$ ) , then the partial trace of t is defined as $\rm{tr}_b [ t ] $ in $l ( a ) $ is defined by \begin{equation} \langle a | \rm{tr}_b [ t ] | b \rangle = \sum_n \langle a | \langle n | t| n\rangle | b \rangle \end{equation} where $| n \rangle$ is an orthonormal basis in $b$ . finally , note that the reduced matrix is not the correct way of the describing a quantum state , is just a way to describe it as seen by looking just at a subsistem . this usually involves ignoring part of the information of the state and therefore the reduced density matrix of a pure state may be a mixed state . this is spectacular for the bell states , as their reduced matrix is $\rm{id}/2$ , the most disoredered state .
well a beautiful way to understand why would we choose a quantity and say that it is conserved is looking at symmetries . if you look at the euclidean space , $\mathbb r^3$ , with only one particle . you clearly see that space is homogeneous , i.e. , there is nothing that differs from a point in space to another ( right ? ) . well , symmetry implies that there is a conserved quantity as the noether 's theorem states . the quantity that is conserved due to that symmetry ( space homogeneity ) is what we call momentum . this can be readily checked if you formulate classical mechanics using the hamilton 's principle known as lagrangian mechanics . for example , conservation of energy is verified by the homogeneity of time , i.e. , if any instant is equivalent to any other , and angular momentum conservation is due to isotropy of space . more on this topic can be seen in landau and lifshitz course of theoretical physics - volume 1: mechanics chapters 1 and 2 .
yes , everything generates a gravitational field , whether it is massive or massless like a photon . the source of the gravitational field is an object called the stress-energy tensor . this is normally written as a 4 x 4 symmetric matrix , and the top left entry is the energy density . note that mass does not appear at all . we convert mass to energy by multiplying it by $c^2$ ( as in einstein 's famous equation $e = mc^2$ ) and then put in the energy . so even a photon generates a gravitational field because although it has no mass it does have energy . it is surprising what else is in the stress-energy tensor and can therefore generate a gravitational field . for example pressure and shear stress appear . it is even been suggested that a gravitational field could be generated by gravty itself i.e. the energy of the gravitational field generates the curvature that creates the field . the resulting object is called a geon , though i should emphasise that no-one has proved these could exist and most of us think they probably can not .
when you mention solar power , it makes me think you are thinking about photo-voltaic power or power extracted from solar panels . the power put out by the sun is about $3.95*10^{26}w$ per second . but solar panels can only capture a fraction of that energy . even so , in 2008 humans used about $4*10^{13}w$ per second which is many orders of magnitude less than the sun puts out . the suns life cycle will last billions of years fusing hydrogen , then helium and slowly working its way down to a white dwarf star . as the temperature and radius changes of the sun , it is power output will fluctuate . but it will always put out more power than humans could use until the earth is heated greatly during the red giant phase . considering homo sapiens have only been around 200,000 years and the sun will not expand for another ~7,000,000,000 years that is approximately infinite in terms of human life spans which are about 80 years . what will exactly happen during this red giant phase is still under debate , but it is a long long time from now .
the number of physical degrees of freedom ( dof ) or dynamical variables is simply the number of generalized positions whose evolution is given by a second order in time differential equation . using the op 's notation , the number of dof is $${1\over 2} ( n-2m-s ) $$ for instance , in electrodynamics the phase-space is six-dimensional $\{a_i , f_{0i}\}_{i=1}^3$ and the gauss law is a first class constraint . thus $n=6 , \ , m=1 , s=0$ . so that there is two dof corresponding to the two polarizations of electromagnetic waves or the two photon 's helicities . one can take an alternative and equivalent point of view in which the phase-space consists of $\{a_{\mu} , f_{0\mu}\}_{\mu=0}^3$ and besides the gauss law one has the first class constraint $f_{00}\approx0$ ( the symbol $\approx$ is read " weakly zero " and means zero when the constraints are verified , you may perfectly write $=$ ) which poisson commutes with the gauss law and both are therefore first class constraints . then $n=8 , \ , m=2 , s=0$ and the number of dof is still two , of course . in the case of the gravitational field , the counting of dof is analogous . the phase-space consists of $\{h_{ab} , p_{ab}\}_{a=1 , b=1}^{a=3 , b=3}$ , with $h_{ab}$ the components of the spatial metric and $p_{ab}$ their conjugated momenta . the four $ ( 0 , \mu ) $ einstein equations are not dynamical equations —since they do not contain second order temporal derivatives— but first class constraints . hence $n=12 , \ , m=4 , \ , s=0$ so that the number of dof is two corresponding to the two polarizations of gravitational waves . however , consider the case of the procca field ( a vectorial field of mass $m$ ) . now the phase-space consists of $\{a_{\mu} , f_{0\mu}\}_{\mu=0}^3$ and there are two constraints $\partial_i\ , f_{0i}=m^2a_0$ —i am considering a theory with no matter fields besides the vectorial field , if one added other fields , then there would be a density of charge $\rho$ in the right hand side— which reduces to the gauss law when $m=0$ and $f_{00}=0$ like in the electromagnetic case . however , now due to the mass term , the two constraints do not poisson commute , thus the constraints are second class . hence $n=8 , \ , m=0 , s=2$ and the number of degrees of freedom is three corresponding to the three helicities of a massive vectorial particle .
if you consider a homogeneous piece of silicon the total flow of electrons through it is : $$ i = \frac{u}{r} = n \mu \frac{s}{d} u $$ where $r$ - the resistance of the piece , $u$ - external voltage applied to it . the resistance depends on : $n$ - the concentration of electrons ( number of electrons per m$^3$ ) , $\mu$ - the mobility of electrons ( ratio of velocity of the electron and electric field that makes it move ) , $s$ and $d$ - cross-section and length of the sample . the changes of geometric size with temperature are negligible . so the values that affect the resistance are concentration $n$ and mobility $\mu$ . the mobility depends on the temperature and also on the concentration of various defects in the sample . at 100 f the temperature dependence is dominating . the concentration is the most complicated point . there are the following cases : pure silicon . all the electrons ( and the same amount of holes ) are thermally generated . their concentration depends on the temperature exponentially . if you need total current do not forget about the holes . silicon doped with donors . the amount of thermally generated electrons is negligible . the concentration does not depend on the temperature . semiconductor device with p-n junction or/and heterojunction ( connection of different materials ) . the laser/led of optical computer mouse is this case . the sample is not homogeneous and the concentration is determined mainly not by temperature but by more interesting things like voltage polarity . this case requires more formulas and exact data concerning the sample structure . ! the laser is made of gaas and similar materials not silicon . the attempts to make silicon laser never stop though . edited ( 2011/12/15 ) : for the temperature dependence of electron mobility wikipedia gives $$ \mu ( t ) \approx \mu_0 t^{-2.4} $$ where $\mu_0 = 9.46 \cdot 10^{6} \text{m}/\text{ ( v s ) }$ , hope i have calculated it correctly from first point set ( black circles ) here this formula takes into account only electron scattering on the oscillations of the ions of the crystal . this effect is dominating at room temperature and higher . the temperature must be in kelvin degrees here . for the electron $n$ and hole $n_h$ concentration in pure silicon ( case 1 . ) at room temperature and higher one can use the following formula : $$ n = n_h = n_\text{eff} \ ; t^{\ ; 3/2} \exp \left ( -\frac{e_g}{2k_b t} \right ) $$ where $n_\text{eff}$ - some constant describing the shape of conduction and valence bands of silicon ( i have not found explicit value yet ) , $t$ - temperature in kelvin degrees , $e_g = 1.12\ ; \text{ev} = 1.79 \cdot 10^{-19} j$ - energy gap of silicon , $k_b$ - boltzmann constant .
you can prove it with a little trick . for a particle on a circle of radius $r$ we have $\vec{x} ( t ) \cdot\vec{x} ( t ) =r^2$ at each point in time . differentiating with respect to time we get $\vec{x}\cdot\vec{v}=0$ . differentiating again we get $v^2+\vec{x}\cdot\vec{a}=0$ . but $\vec{x}\cdot\vec{a} = - r a_{\mathrm{rad}}$ , and the relation $a_{\mathrm{rad}} = v^2/r$ follows .
take a look at this article . the authors present an elementary explanation for the two-to-one ratio of wobble to spin frequencies . update : in case you do not have easy access to the ajp , here it is : article .
if you want to make a small very-hot spot , spatial coherence is important . contrary to what you say , the sun has quite high spatial coherence ( not as high as a laser , but higher than most other bright light sources ) . that is why you can focus sunlight very well . if you focus sunlight perfectly , you get a spot the shape of the sun . that spot would have the same light intensity as if you were standing on the surface of the sun , looking down . if you perfectly focus the light from an incandescent light bulb , you can get a spot shaped like the tungsten filament . that spot would have the same light intensity as if you were a tiny person standing on the surface of the tungsten filament , looking down . ( this intensity is much lower than the the sun 's ) . an extreme example of low spatial coherence is the blue light from the blue sky . you cannot use a lens to focus that light into a bright blue spot on the ground . try it ! this blue light has almost no spatial coherence , which means you cannot focus it . there is plenty of blue light coming at you , but it already has as much intensity as it is capable of having . this is quantified by the law of conservation of etendue . light starts out with a certain radiant intensity , and then it can never be increased , no matter what kind of lenses or mirrors you use . lasers can have far higher radiant intensity than any other light source . it is not just how many watts they emit , it is how they emit it -- with high spatial coherence , which means it can be focused very effectively . lasers come in all shapes and sizes , and intensities , and wavelengths , and form-factors , and prices . red diode lasers cost a few cents each . other lasers cost $100,000 or more . lasers are basically a generic way to create coherent ( and therefore high-radiant-intensity ) light , in many different systems . so if you need a high-radiant-intensity light source , the best one for the job is quite likely to be a laser .
more clues ? :- ) this is harder then the isobaric process because now the pressure is a function of volume . you need to write the pressure as a function of volume , then integrate it from the initial to final volume . for some clues see the wikipedia article on adiabatic expansion . although the question does not say so , you will need to assume the expansion is reversible as the question can not be answered otherwise .
my gut feeling is that the pressure loss will be noticable , but managable with a small heating pump . 16mm diameter , however , seems awfully thin to me . you need to consider several things : how much thermal power will your heating system deliver , and at what temperatures ? from this you get the neccesary flow : $q=\frac{p_{therm}}{c_{heat}*\delta t}$ with this flow you can calculate the pressure loss with the formula found here : http://en.wikipedia.org/wiki/darcy%e2%80%93weisbach_equation - preferably in the pressure loss form . for such a small pipe diameter it is pretty safe to assume laminar flow , then you do not even need to check for the friction factor in a moody diagram . if the hose goes mainly straight up , you can calculate just the pressure loss from the distance and ignore fittings , bends , etc for a first shot . this is not a complete walkthrough , but i am quite confident you find everything you need one link from the wikipedia page i linked .
first of all , in lower dimensions ( 2+1 and 1+1 ) the gravity is much simpler . this is because in 3d curvature tensor is completely defined by ricci tensor ( and metric at a given point ) while in 2d curvature tensor is completely defined by scalar curvature . this means that there are no purely gravitational dynamical degrees of freedom , in particular no gravitational waves . general note : horizon ( which is the defining feature of black hole ) representing our inability to obtain information about events under it would always imply the entropy of corresponding solution . so , in all of black hole models there is some black hole thermodynamics . for hawking radiation one needs to include quantum effects into consideration and also radiative degrees of freedom ( if there are no gravitons or photons or any other '-ons ' than nothing would radiate ) . let us start with case of 3d ( that is 2+1 ) . the einstein equations in 2+1 spacetime without any matter fields would simply imply that spacetime is flat , that is ' constructed ' from pieces of minkowski spacetime . it may have nontrivial topology , so 2+1 gravity is a topological theory , but no black hole solutions exist . this model ( in mathematical sense ) is exactly solvable . to introduce non-trivial 2+1 solutions we can add matter or cosmological constant ( which could be considered the simplest form of matter ) . it turns out that the spacetimes with negative cosmological constant ( which would locally be composed of pieces of anti-de-sitter spacetimes ) do admit the black hole solution : btz black hole ( name after authors of original paper ) . this solution shares many of the characteristics of the kerr black hole : it has mass and angular momentum , it has an event horizon , an inner horizon , and an ergosphere ; it occurs as an endpoint of gravitational collapse ( for that , of course , we need to include matter beyond cosmological constant in the consideration ) ; and it has a nonvanishing hawking temperature and interesting thermodynamic properties ( see , for instance , paper by s . carlip ) . the hawking temperature of btz black hole $t\sim m^{1/2}$ which , in contrast to the ( 3+1 ) -dimensional case , goes to zero as $m$ decreases . additionally , the simplicity of the model allows quantum treatment of it including statistical computation of the entropy ( see references in paper by e . witten ) . there are many other variations of solutions in 2+1 gravity theories ( for instance by including dilaton and em fields , scalar fields etc . ) but all of them require negative cosmological constant . this is because dominant energy condition forbids the existence of a black holes in 2+1 dimensions ( see here ) . now to 1+1 dimensions . locally all gr models in 1+1d are flat . so to include nontrivial spacetime geometry we need to modify gravity . this can be done by including dilaton field . the resulting models often admit nontrivial geometries with black holes ( see paper by brown , hennaux , teitelboim , wiki page on cghs model , paper by witten on bh in gauged wzw model , and this review ) . these black hole solutions also admit nontrivial thermodynamics and hawking radiation . in particular the hawking temperature is proportional to mass , so as the black hole evaporates it becomes colder ( unlike 4d case where $t \sim m^{-1}$ ) . now to higher dimensional gravity . gravity itself is much richer than in lower dimensional cases , so analogues of all 4d black holes also exist in higher dimensions , as well as some new black hole-like solutions such as black strings and black p-branes . there are also multi-black hole configurations where multiple black holes are placed along the ring or line such that the total force on each of them is zero , resulting in equilibrium configuration . since many uniqueness theorems for black holes only work for 3+1 dimensions there are even solutions with nontrivial horizon topologies such as black rings . i suggest to look at the living review recommended by ben crowell or to this lectures by n . obers . the simplest black hole would be schwarzschild–tangherlini solution ( analogue of schwarzschild black hole ) which is vacuum solution to einstein field equations : here $\mu = r_s^{d-3} = \frac{16 \pi g m}{ ( d-2 ) \omega_{d-2}}$ is mass parameter . this gives us the relationship between mass and schwarzschild radius : $r_s \sim m^{1/ ( d-3 ) }$ . the entropy is given by bekenstein-hawking formula : $$ s = \frac {\cal a}{4g}=\frac 14 \left ( \frac{\omega_{d-2} r_s^{d-2}}{ g} \right ) . $$ temperature could be found from the first law $ ds = d m / t $: $$t = \frac{d-3}{4 \pi r_s} . $$ rotating solution ( generalization of kerr metric ) would be myers-perry metrics . note , that rotations in higher dimensions are more complex , so the angular momentum is represented by several parameters . also note , that many solutions with horizons elongated in one direction ( such as black strings or black rings ) turn out to be unstable via the gregory-laflamme instability , where the smooth ' tubular ' horizon evolve growing perturbations of certain wavelengths . so possibly black strings and black rings would tend to decay into droplets-like black hole along them ( the exact mechanics is yet unknown ) . but of course , the second law of thermodynamics would be observed , meaning that total area of the horizons would increase .
the electric field of a negative point charge points towards the point charge as a result of the definition of the electric field of a point charge . to see this , recall that the electric field of a point charge $q$ is defined as $$ \mathbf e = \frac{1}{4\pi\epsilon_0}\frac{q}{r^2}\mathbf e_r $$ where , $r$ is the distance to the charge , and $\mathbf e_r$ is the outward pointing radial unit vector field emanating from the location of the charge . if $q$ is negative , then we see that the electric field points in the direction of $-\mathbf e_r$ , and therefore it points radially inward . you might then ask " well this is fine , but why is the electric field defined in this way ? could not we have defined the electric field in such a way that the electric field due to a negative point charge points radially outward ? " well , the definition of the electric field is motivated by coulomb 's law , the empirical fact that the electrostatic force exerted by a point charge $q_1$ on a point charge $q_2$ is $$ \mathbf f_{12} = \frac{1}{4\pi\epsilon_0}\frac{q_1q_2}{r^2}\mathbf e_{12} $$ where $\mathbf e_{12}$ is the unit vector pointing from charge 1 to charge 2 . the idea in defining the electric field is that we want to associate a vector field ( which we call the electric field ) to each point charge individually such that if we multiply that field by any other charge ( which is usually called a test charge ) , then we obtain the force that would be exerted on the test charge due to the original charge . if we factor $q_2$ out of the right hand side of coulomb 's law , then the rest of the stuff is precisely such a vector field associated to the charge $q_1$ $$ \mathbf f_{12} = q_2\underbrace{\left ( \frac{1}{4\pi\epsilon_0}\frac{q_1}{r^2}\mathbf e_{12} \right ) }_{\text{stuff that only depends on charge 1}} $$ so we define the electric field as the stuff in parentheses . notice , however , that we could just as well have factored out $-q_2$ in which case the electric field would have been opposite in sign to the conventional definition , and in this case , the electric field of the negative charge would have pointed radially outward by definition .
i am going to make some cosmetic changes to your equation : your magnetic field should be $\vec b = \vec e_x b_0 x/x_0$ , so that $\vec b$ and $b_0$ have the same dimensions . this field does not obey $\vec\nabla\cdot\vec b=0$ , but we will leave that alone for now . i will write $\mu \vec\sigma\cdot\vec b$ for the magnetic energy . notice that the neutron 's magnetic moment is pretty tiny , about 50 nev/t . so-called " ultra-cold " neutrons , with energies below about 100 nev , can be confined by a magnetic field minimum , but for cold or thermal ~mev neutrons , stern-gerlach steering is generally negligible . ( if it were not negligible , you could use stern-gerlach separation to turn one neutron beam into two polarized beams going in different directions ; alas , real neutron polarizers all absorb one spin state . ) your equation of motion is also separable in time , so i will consider only the spatial part of it ; we can tack on a factor of $e^{-i\omega t}$ later . i will use $\phi$ and $\chi$ to give separate names to the two components of your spinor . in matrix notation , your time-independent schrödinger equation is $$ \left ( \begin{array}{cc} \frac{-\hbar^2}{2m}\partial_x^2 and \frac{\mu b_0 x}{x_0} \\ \frac{\mu b_0 x}{x_0} and \frac{-\hbar^2}{2m}\partial_x^2 \end{array}\right ) {\phi\choose\chi} = e {\phi\choose\chi} . $$ this makes it a little more obvious what is happening . when you defined your initial ensemble to have $\phi = \chi = \psi_0 ( x ) $ , you set your system into an eigenstate of $\sigma_x$ ! so of course the entire ensemble moves together : it is polarized along the field direction ! if your initial condition is $\phi = -\chi$ , you should get a packet moving in the other direction . when you replace $\sigma_x$ with $\sigma_{y , z}$ , you are effectively changing the direction of the field . remember that the energy is $\mu\vec\sigma\cdot\vec b$ ; if the only nonzero term in this product is $\sigma_y b_0 x/x_0$ , you are telling your model that the field varies in strength with $x$ but points along the $y$ direction . ( since those fields do obey $\vec\nabla\cdot\vec b=0$ , you should prefer them anyway . ) so in your second and third figure you are putting an $x$-polarized sample into a field along $y$ or along $z$ , and it separates . the separation is along the direction of the field strength gradient , rather than along the direction of the field ; the ucn trappers talk about " strong field seekers " and " weak field seekers " . in the field pointing along $z$ , your sample separates into your spinor basis , which gives you the nice colors . in the field pointing along $y$ , your sample still separates . but instead of separating into the $\phi , \chi$ basis that you are using for colors , the diagonal states are $\phi\pm i\chi$ . this gives you interference when the two wavepackets overlap .
leibniz rule holds for covariant derivatives , both in gauge theories and gravity . mathematically , a derivation is one for which the leibniz rule holds . how does it work for non-abelian covariant derivatives . i will give you an example . let $\phi^\dagger \phi$ be invariant under local non-abelian gauge transformations . then $$ \partial_\mu ( \phi^\dagger \phi ) = ( d_\mu\phi ) ^\dagger \phi + \phi^\dagger ( d_\mu\phi ) = [ d_\mu ( \phi^\dagger ) ] \ \phi + \phi^\dagger ( d_\mu\phi ) $$ the term on the left hand side is the ordinary derivative as the combination is gauge invariant . by construction , it is easy to see that each term on the right hand side is invariant under local gauge transformation . you need to further show that the terms linear in the gauge field cancel . that more or less follows from representation theory . it is also easy to work out the rules for integrating by parts using the above formula . for your lagrangian , you would consider the ordinary derivative of the scalar , i.e. , , $\partial_\mu ( \phi^\dagger d^\mu \phi ) $ . remarks : the ordinary derivative satisfies an additional property i.e. , $\partial_\mu\partial_\nu\ f ( x ) = \partial_\nu \partial_\mu f ( x ) $ for all smooth functions . this is not assumed for all derivations . in fact , the obstruction to commutativity of the derivatives defines the field strength . $$ [ d_\mu , d_\nu ] \ \phi \sim g\ ( f_{\mu\nu}^a t_a ) \ \phi\ . $$ in supersymmetry , one uses a graded version of the leibniz rule for ( covariant ) derivatives involving grassmann coordinates .
three-phase has two main reasons to exist : driving n . tesla 's polyphase induction motors used throughout industry . reducing the total cost of metal in cross-country power lines : w/single-phase lines , more metal would be needed to transfer the same rate of kilowatts . you are right : lighting as well as ac motors will briefly turn off at 120 times per second ( that is for usa 60hz line frequency . ) for this reason , metal-vapor streetlights and older inductor-ballast fluorescent tubes have significant " hum " modulation in their light output . this strobe effect can be very visible when your eyes sweep across strings of led-based xmas lights . the hum is greatly reduced with incandescent lamps , since the filament does not cool down significantly during the millisecond-long low point in the 60hz wave . and you will probably hear a 120hz sound when using high-speed carbon-brush motors under high mechanical loads , such as electric mixers and carpentry routers . these high-rpm products can not use ac induction motors which are naturally limited to 1800rpm by the 60hz drive frequency . the 120hz variation ends up in the torque output of high-speed brush motors . single-phase induction motors should have much less 120hz mechanical buzz , since they are essentially 2-phase motors with magnetic field torque which rotates rather than oscillates . either capacitance or inductance is used to give the motor a second electromagnet pole having shifted phase .
let 's find the complete solution of the problem . a complete solution of the problem would be the solution to the linear ode , $m \dot{\mathbf{v}} =q\mathbf{v} \times \mathbf{b}-k \mathbf{v}$ assume without loss of generality that the magnetic field is pointed along the z-axis , so $\mathbf{b} = b \mathbf{\hat{z}}$ . so our equation simplifies to , $m \dot{\mathbf{v}} =qb\mathbf{v} \times \mathbf{\hat{z}}-k \mathbf{v}$ dividing both sides of the equation by $m$ and for simplicity in the notation , let $\omega=\frac{qb}{m}$ and $\gamma=\frac{k}{m}$ . so , $\dot{\mathbf{v}} =\omega\mathbf{v} \times \mathbf{\hat{z}}-\gamma \mathbf{v}$ using $\mathbf{v}=\begin{pmatrix} v_{x}\\v_{y}\\v_{z} \end{pmatrix}$ and writng the given eqaution in matrix form we have , $\dot{\mathbf{v}} =\begin{pmatrix} -\gamma and \omega and 0 \\ -\omega and -\gamma and 0 \\ 0 and 0 and -\gamma \end{pmatrix} \mathbf{v}=a \mathbf{v}$ this is linear ode which can be solved using the matrix exponenetial as , $\mathbf{v}=e^{at} \mathbf{v_0}$ to simply this equation we can find the eigenvaules of a , use a similarity transform to convert it to a diagonal matrix which this greatly simplifies the matrix exponential . the eigenvalues and the corresponding eigenvectors are , $\lambda_{1}=-\gamma , \mathbf{v_1}=\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$ $\lambda_{2}=-\gamma-i\omega , \mathbf{v_2}=\begin{pmatrix} 1 \\ i \\ 0 \end{pmatrix}$ $\lambda_{3}=-\gamma+i\omega , \mathbf{v_3}=\begin{pmatrix} i \\ 1 \\ 0 \end{pmatrix}$ using $s= [ \mathbf{v_1} \mathbf{v_2} \mathbf{v_3} ] $ and performing a similarity transform on the matrix a , $s^{-1}as=d$ where d is diagonal matrix with the eigenvalues as the diagonal elements . and here we witness the power of similarity transformations as , $e^{at}= s \begin{pmatrix} e^{\lambda_{1}t} and 0 and 0 \\0 and e^{\lambda_{2}t} and 0 \\ 0 and 0 and e^{\lambda_{3}t} \end{pmatrix} s^{-1}$ ( after some tedious calculations ans using $e^{ix}=\cos{x}+isin{x}$ ) $=\begin{pmatrix} e^{-\gamma t}\cos ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\ -e^{-\gamma t}\sin ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\0 and 0 and e^{-\gamma t}\end{pmatrix}$ therefore , $\mathbf{v}=\begin{pmatrix} e^{-\gamma t}\cos ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\ -e^{-\gamma t}\sin ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\0 and 0 and e^{-\gamma t}\end{pmatrix} \mathbf{v_0}$ writing out the components , $v_{x}=e^{-\gamma t} ( v_{x_0} \cos{\omega t}+v_{y_0} \sin{\omega t} ) $ $v_{x}=e^{-\gamma t} ( -v_{x_0} \sin{\omega t}+v_{y_0} \cos{\omega t} ) $ $v_{x}=e^{-\gamma t} v_{z_0}$ this the just the equation of a helix with both the pitch and radius decreasing exponentially with $\gamma$ . however , the angular frequency is the same as that without drag , $\omega$ . here is a sample trajectory ,
assuming you are in orbit around the sun ( presumably a highly elliptical orbit ) you will not feel any force due to gravity . in principle you might feel tidal forces , but for an object the size of a spaceship these are negligable even if you graze the surface of the sun . the most obvious problems are the heat from the sun and the radiation it emits . the radiation is a mixture of electromagnetic radiation and charged particles , both of which are not good for anything relying on it is dna remaining intact . it is difficult to do much about the heat because in space the only way you can cool is by radiation . what you had probably do is surround your spaceship with a mirrored shell and keep a layer of vacuum between the shell and your ship . even with very good mirroring the shell will heat up , but for a while at least it will keep the heat off your spaceship . the messenger probe in orbit round mercury uses a reflective shield , and contains internal refridgeration - i do not knw exactly how this works but presumably it uses a radiator on the side of the probe pointing away from the sun . there is not a lot you can do about the radiation except surround your spaceship with a thick layer of lead , and that much lead would be difficult to put into space . the solar probe plus is planned to get within 4 million miles of the sun 's surface , and this will be the closest we have managed to get any spacecraft . however the spp does not have any human passengers to worry about . i suspect radiation is the real problem for human passengers . even for a hypothetical manned mars mission the radiation dose the astronauts would receive is a worry , and the intensity of the radiation goes up as the inverse square of the distance .
the angular momentum $l_{a/b}$ of a rigid body $a/b$ about its center of mass is $$l_{a/b} = i_{a/b} \omega_{a/b} , $$ where $i_{a/b}$ is the inertia matrix of $a/b$ about its center of mass in the world frame and $\omega_{a/b}$ is the angular velocity of $a/b$ . the angular momentum $l_{a/b}^0$ of a rigid body $a/b$ about the origin of the world frame is $$l_{a/b}^0 = l_{a/b} + x_{a/b} \times p_{a/b} , $$ where $x_{a/b}$ are the coordinates of the center of mass in the world frame . then the total angular momentum in the system with the rigid bodies $a$ and $b$ about the origin of the world frame is $l_{total}^0 = l_a^0 + l_b^0$ , which is supposedly conserved . the total linear momentum is $p_{total} = p_a + p_b$ . the total linear momentum is conserved as soon as the forces $f_a$ and $f_b$ are of equal magnitude and opposite direction ( $f_a = f = -f_b$ ) : $$ \frac{\mathrm d}{\mathrm{d}t} ( p_a + p_b ) = m_a \dot{v}_a + m_b \dot{v}_b = f_a + f_b = f - f = 0 , $$ where $v_{a/b}$ is the translational velocity of $a/b$ in the world frame . the derivative of the total angular momentum with respect to time is $$\begin{split} \frac{\mathrm d}{\mathrm{d}t} ( l_a^0 + l_b^0 ) and = \frac{\mathrm d}{\mathrm{d}t} ( l_a + l_b + x_a \times p_a + x_b \times p_b ) = \dot{l}_a + \dot{l}_b + x_a \times \dot{p}_a + x_b \times \dot{p}_b \\ and = \tau_a + \tau_b + x_a \times f_a + x_b \times f_b = r_a \times f_a + r_b \times f_b + x_a \times f_a + x_b \times f_b \\ and = ( x_a + r_a - x_b - r_b ) \times f . \end{split}$$ thus the total angular momentum is conserved if : $x_a + r_a - x_b - r_b = 0$ , that is the force pair acts at the same coordinates in the world frame , $f = 0$ , that is no force acts , $ ( x_a + r_a - x_b - r_b ) \ ||\ f$ , that is the force acts along the line of connection . fig . 1 satisfies condition number 3 and thus conserves the total angular momentum . fig . 2 satisfies none of the three conditions and thus does not conserve the total angular momentum . edit : the so post is angular momentum always conserved in the absence of an external torque ? contains a proof for point particles , which has an analogous requirement for the conservation to hold ( forces along the line of connection ) . the author of the proof corrected it by now .
$ ( 2.7.7 ) $ is about the closed string , while $ ( 4.1.11a ) $ is about the open string . you can compare the expansions of the closed string and the open string in $ ( 2.7.4 ) $ and $ ( 2.7.26 ) $ . you see that the term in front of $-ip^\mu \ln|z|^2$ is $\frac{\alpha'}{2}$ for the closed string , and $\alpha'$ for the open string . when looking at the stress-energy tensor ( see $2.4.4$ ) , you have quadratic quantities of $\partial_a x^\mu$ divided by $\alpha'$ , so you get a term $\frac{\alpha'}{4}$ for the closed strings and a term $\alpha'$ for the open string . now , the constant part of $l_o$ ( the zero mode of the stress-energy tensor ) ( $\sim p^2$ ) is directly related to this term , so it explains the difference .
question 1: you are running into a problem because you are applying the uniqueness theorem to too large of a volume . the bottom of an appropriate volume lies on the $z=0$ plane , where $\phi=0$ because there is a grounded conducting plate there . the rest of the volume 's boundary is an arbitrary surface that completes the enclosure of the volume and consists of points such that $z&gt ; 0$ and $x^2 + y^2 + z^2 \gg d$ , the latter condition being to ensure that $\phi \approx 0$ is a good approximation at that point of the boundary . you can effectively think of all but the bottom of the volume 's boundary as being " at infinity " , loosely speaking . question 2: $\rho_1 ( \vec{r} ) =\rho_2 ( \vec{r} ) $ at all points for which you need to solve for $\phi$ . for all points for which you need to solve for $\phi$ , $z&gt ; 0$ , strictly . you do not need to solve for $\phi$ at points precisely on the boundary , because you already know that $\phi=0$ everywhere on the $z=0$ plane , because there is a grounded conducting plate there . question 3: no , a single charge does not satisfy the same boundary conditions , which is presumably obvious now that the bottom of the boundary is understood to be on the $z=0$ plane , instead of the volume in question being all of space .
first discretize the spacetime , assign a fermion pair $\bar{\psi} ( i ) $ and $\psi ( i ) $ at each point i . then assuming operator $\hat{a}$ is symmetric , hence which can be diagonalized by a unitary operator whose determinant is one , the path integral can be written in the following way : $$\int \pi_{i} d\psi ( i ) d\bar{\psi} ( i ) e^{i \delta \sum \bar{\psi} ( i ) \lambda_i \psi ( i ) }$$ where $\delta$ is the discretized spacetime volume element and $\lambda_is$ are the eigenvalues of $\hat{a}$ . now the path integral can be written as a product of many grassmanian integrals . in particular each one is : $$\int d\psi ( i ) d\bar{\psi} ( i ) e^{i\delta \lambda_i \bar{\psi} ( i ) \psi ( i ) }\\ =\int d\psi ( i ) d\bar{\psi} ( i ) ( 1+i\delta \lambda_i \bar{\psi} ( i ) \psi ( i ) ) = i\delta \lambda_i $$ multiplying all these together you get the determinant of $\hat{a}$ up to some irrelevant constant factor .
as far as i know , gallavotti proved the ergodicity of the lorentz gas , while sinai proved that of a system of $n \leq 5$ rigid spheres . anyway , this is a minor detail . for certain aspects , a more suitable model for the drude model is the boltzmann gas . lanford has shown ( in 1970s , i think ) that the entropy for this model is always increasing , but anyone has proved that boltzmann gas is ergodic . so the answer to your question is : if , at our level of accuracy , lorentz gas was an appropriate mathematical scheme for the drude model , than it would be ergodic . else we can not conclude , since sinai 's result is very important but too limited . ( at the present day . ) however , it is an interesting question from a mathematical point of view ( for me , for example , it really is ) , but for a physicist it is not very important , since drude model does not provide an appropriate level of precision for most of calculations carried out nowadays in solid state physics ( at least , concerning what i have seen in my course of condensed matter , i am not a specialist in solid state physics ) . moreover , any of that models takes in account coulombian interactions between charged particles . ( this remark restricts - in principle - a lot the domain of applicability of such schematizations . ) i think you could find very interesting the treatments of this subject ( ergodic theory ) by halmos and arnold in their classical monographs . references . p . r . halmos , lectures on ergodic theory v.i. arnold , ergodic problems of classical mechanics and mathematical methods of classical mechanics g . gallavotti , statistical mechanics and the elements of mechanics
$f=ma$ . if $f=0$ , and $m=0$ , $a$ can be anything . most physical laws are not " a causes b " . they usually say that " a and b can coexist in these conditions " . so , it is not necessarily " force causes acceleration " . it is " an accelerating body can coexist with a force if $f=ma$" the net force on a massless string is always 0 -- it has to be ( otherwise it will have infinite acceleration ) . whenever we draw free body diagrams of systems that contain massless strings , we always take a tension force $t$ that represents the string " pulling " the body . take the reaction force of $t$ on the string and you will notice that the string is always in equilibrium . for example , take this system , where someone is pulling a set of two boxes interconnected by a string : note that the reaction force of $t$ ( in red ) on the string balances itself . for a more complicated system , take the following : ( i have taken a massless smooth pulley here . if the pulley was not smooth , then the tensions in the two portions of string would be different . if it was not massless , the force from the ceiling would be different ) in this system $t$ balances out on the string as well . this is precisely why we say that a section of string exerts $t$ on both ends &mdash ; to maintain equilibrium .
newtonian mechanics is not quite as predictable as it is made out to be , both in theory and in practice . in theory , it is an easy task to set up a newtonian system of particles that will eventually violate the lipschitz continuity condition . you can toss reversibility and predictability out the window when that happens . in practice , we can not know state perfectly , and that means the complicated dynamic systems you mentioned eventually become unknowable . they will eventually entire a region of strong chaos , and there is no telling what happens after that . with regard to entropy , consider an interstellar gas cloud . the jeans instability can make a portion of the cloud collapse to form a protostar and a protoplanetary disk . you will not see a protostar and protoplanetary disk undo themselves and reform that gas cloud because of entropy . that is yet another gravitational example , but there are plenty of non-gravitational examples . consider a semi-rigid body with three distinct principle moments of inertia rotating in space . eventually that body will end up spinning about the axis with the largest moment of inertia . this is an irreversible process , and it lies almost entirely within the realm of newtonian mechanics . the only non-newtonian aspect is that the body radiates heat generated by internal friction away into the universe .
$ [ f , p_k ] =\frac{\partial f}{\partial q_i}\frac{\partial p_k}{\partial p_i}-\frac{\partial f}{\partial p_i}\frac{\partial p_k}{\partial q_i}$ . second term is zero and first term is $\frac{\partial f}{\partial q_i }\delta_{ik}=\frac{\partial f}{\partial q_k}$
thermal velocity is the velocity that a particle in a system would have if its kinetic energy were equal to the average energy of all the particles of the system . take an ideal gas in three dimensions as an example . in equilibrium there is a distribution of velocities among all of the particles . some move " fast " others move " slowly " . that means that there is a distribution of kinetic energy as well . you can calculate the average kinetic energy , call it $e_{avg}$ by adding up the energies of all of the particles and then divide that by the number of particles . it turns out that the average kinetic energy can be calculated for systems in equilibrium . for an ideal gas , $e_{avg} = \frac{3}{2} kt$ . average energy is proportional to the temperature of the system . there are similar expressions for more complicated systems , but the average energy is always proportional to $t$ . the thermal velocity is the speed that a particle would have if its kinetic energy happened to be equal to the average value . thus for an ideal gas $$ \frac{1}{2}mv^2 = \frac{3}{2}kt$$ and $$ v = \sqrt{\frac{3kt}{m}}$$ for a system of particles of known mass , the thermal velocity depends only on the temperature and a universal constant . thus , it is an indirect measure of temperature . as you can see from the wikipedia article , there are different ways to express average energy . this is one of them . the different ways lead to slightly different expressions .
formally , the gauge-invariant observables do not depend on the choice of gauge-fixing condition ( such as , e.g. , lorenz gauge , coulomb gauge , axial gauge , temporal gauge , etc ) . similarly , the hamiltonian can formally be gauge-fixed in any gauge . however , it is my understanding that to avoid the gribov problem , an algebraic ( rather than a differential ) gauge-fixing condition is preferred . see also the footnote on p . 15 in s . weinberg , quantum theory of fields , vol 2 .
yes , a hole with energy $e$ is the same as an electron with a negative energy $e$ missing - that is why it is called a hole and that is how paul dirac first encountered it in the relativistic context ( in the form of positrons ) . a positively-charged positron may look more " particle-like " but one may describe it as the very same holes in the otherwise filled sea of negative-energy electron states . in both cases - semiconductors and positrons - you may assume that the negatively-charged electrons are the only " real " particles . however , you will always derive the existence of positively-charged holes that behave " just like electrons " . if you find states such that the energy $e$ is an increasing function of the momentum , the system will first try to fill the low-momentum , low-energy states , and you may add additional higher-energy , higher-momentum particles ( electrons ) . but some part of the spectrum may have the property that the energy $e$ is a decreasing function of the momentum $k$ . in that case , the electron states with a higher value of momentum are filled first , and you are adding them " inwards " . this is counterintuitive , so it is more logical to exchange the convention for what we mean by a " filled state " and what we mean by an " empty state " . once we do so , we also change the charge and energy of the particle in each state . consequently , we will deal with positive-charge holes whose energy $e$ behaves just like $-e$ of the original electrons , and increases with $k$ just like for ordinary electron states . the only difference will be in the charge .
what does it mean ? the reason they are conservative or non-conservative has to do with the splitting of the derivatives . consider the conservative derivative : $$ \frac{\partial \rho u}{\partial x} $$ when we discretize this , using a simple numerical derivative just to highlight the point , we get : $$ \frac{\partial \rho u}{\partial x} \approx \frac{ ( \rho u ) _i - ( \rho u ) _{i-1}}{\delta x} $$ now , in non-conservative form , the derivative is split apart as : $$ \rho \frac{\partial u}{\partial x} + u \frac{\partial \rho}{\partial x} $$ using the same numerical approximation , we get : $$ \rho \frac{\partial u}{\partial x} + u \frac{\partial \rho}{\partial x} = \rho_i \frac{u_i - u_{i-1}}{\delta x} + u_i \frac{\rho_i - \rho_{i-1}}{\delta x} $$ so now you can see ( hopefully ! ) there are some issues . while the original derivative is mathematically the same , the discrete form is not the same . of particular difficulty is the choice of the terms multiplying the derivative . here i took it at point $i$ , but is $i-1$ better ? maybe at $i-1/2$ ? but then how do we get it at $i-1/2$ ? simple average ? higher order reconstructions ? those arguments just show that the non-conservative form is different , and in some ways harder , but why is it called non-conservative ? for a derivative to be conservative , it must form a telescoping series . in other words , when you add up the terms over a grid , only the boundary terms should remain and the artificial interior points should cancel out . so let 's look at both forms to see how those do . let 's assume a 4 point grid , ranging from $i=0$ to $i=3$ . the conservative form expands as : $$ \frac{ ( \rho u ) _1 - ( \rho u ) _0}{\delta x} + \frac{ ( \rho u ) _2 - ( \rho u ) _1}{\delta x} + \frac{ ( \rho u ) _3 - ( \rho u ) _2}{\delta x} $$ you can see that when you add it all up , you end up with only the boundary terms ( $i = 0$ and $i = 3$ ) . the interior points , $i = 1$ and $i = 2$ have canceled out . now let 's look at the non-conservative form : $$ \rho_1 \frac{u_1 - u_0}{\delta x} + u_1 \frac{\rho_1 - \rho_0}{\delta x} + \rho_2 \frac{u_2 - u_1}{\delta x} + u_2 \frac{\rho_2 - \rho_1}{\delta x} + \rho_3 \frac{u_3 - u_2}{\delta x} + u_3 \frac{\rho_3 - \rho_2}{\delta x} $$ so now , you end up with no terms canceling ! every time you add a new grid point , you are adding in a new term and the number of terms in the sum grows . in other words , what comes in does not balance what goes out , so it is non-conservative . you can repeat the analysis by playing with altering the coordinate of those terms outside the derivative , for example by trying $i-1/2$ where that is just the average of the value at $i$ and $i-1$ . how to choose which to use ? now , more to the point , when do you want to use each scheme ? if your solution is expected to be smooth , then non-conservative may work . for fluids , this is shock-free flows . if you have shocks , or chemical reactions , or any other sharp interfaces , then you want to use the conservative form . there are other considerations . many real world , engineering situations actually like non-conservative schemes when solving problems with shocks . the classic example is the murman-cole scheme for the transonic potential equations . it contains a switch between a central and upwind scheme , but it turns out to be non-conservative . at the time it was introduced , it got incredibly accurate results . results that were comparable to the full navier-stokes results , despite using the potential equations which contain no viscosity . they discovered their error and published a new paper , but the results were much " worse " relative to the original scheme . it turns out the non-conservation introduced an artificial viscosity , making the equations behave more like the navier-stokes equations at a tiny fraction of the cost . needless to say , engineers loved this . " better " results for significantly less cost !
you are almost there . i am assuming your square well $v_0 ( r ) $ is nonzero and negative only on some interval $-r_0 &lt ; r &lt ; +r_0$ about the origin . in that case , for positive $a , b$ you already have that the isosinglet well is deeper than the isotriplet well . remember that the finite square well has a finite number of bound states , each with energy $-|v_0| &lt ; e &lt ; 0$ . find the width and depth of a well with a single bound state ( for fun , with the correct binding energy , 2.2 mev ) . next find the minimum $b$ so that a same-radius well , shallower by $\frac{3}{4}v_0b$ , has zero bound states . tada : a bound isosinglet with no excited states , and an unbound isotriplet .
a ) cuold someone briefly explain to me why the block on top can accelerate , but the one hanging on the pulley does not ? both blocks do accelerate , and in fact they have the same acceleration . you will see that if you write out the full set of both the x and y components of newton 's second law for each system ( i.e. . for each block ) . in the given solution , they only wrote out the components that are directly needed to solve the problem . notice how the 3kg block is actually " attached " tot he 8kg block , yet the solution here did not include it in its free-body diagram and they even included the reaction force from the 3kg on the 8kg . compared with the cart problem where the hanging mass is also touching the cart , no reaction force was drawn and they even treated the three mass as a single mass . it is not attached . the only interactions between the 8kg and 3kg blocks are the normal force and static friction , neither of which qualifies as " attachment . " this is exactly analogous to the interaction between $m$ and $m_2$ in the first problem . the reason no reaction force was drawn in the first case ( by " reaction force " i assume you are referring to the reaction to the normal force acting on $m_2$ ) was that they did not draw a free body diagram for the object the reaction force acts on , namely $m$ . again , the given solution omits some pieces ( in this case , a free body diagram ) which are not necessary for solving the problem . if you draw a free body diagram for $m$ , you will see the reaction force . could you have solved the first problem with the cart without taking all three mass into the cart 's fbd ? is there another way of applying newton 's second law to the cart problem ? if by this you are asking whether you could have solved the first problem by drawing separate free body diagrams for each of the three masses $m_1$ , $m_2$ , and $m$ , then yes , you certainly can .
the main question is " relative to what ? " for space probes and the like , the speeds that matter are be either with respect to the earth , the target object ( s ) ( mars , some asteroid , space station , etc . ) , and/or the sun ( or solar system barycenter ) . these speeds are measured mostly by doppler shifts in radio waves emitted by a radar the probe carries , reflected by the surface of some target the communication signal between probe and earth ( see for instance , the deep space network ) . other methods have been used ( image analysis between consecutive images taken by the space probe , the temperature of the heat shield on atmospheric entry , etc . ) but these are all much less precise than doppler measurements . space telescopes will measure redshift to some object ( star , galaxy , etc . ) ( which is very similar to doppler ) , which is more an indication of how fast that object is moving with respect to the entire solar system , rather than just the space telescope . parallax methods are also used ( see @ignaciovazquez-abrams 's answer ) , but such methods can only be used for objects relatively close by ( the parallax for most galaxies is too small to measure ) . other methods include cepheid variables , and of course the famous type 1a supernovae , which were used to conclude that the expansion of the universe is accelerating . but these are primarily measures of distance , and only crude measures of speed -- for objects at large distances , redshift is the only accurate way to measure the speed with respect to those objects .
the terminology of collapse of the wavefunction is an unfortunate one . take an oscillating ac line and use a scope to measure it and display it . is the ac 50 herz wavefunction collapsed because we observe it on the scope ? the ac wave function is just a mathematical description of the voltage and current on the line and allows us to calculate the amplitude and time dependance of the energy it carries . an equally unfortunate concept is the matter wave . the particle is not a continuous soup distributing its matter in space and time the way of an ac voltage or other classical wave . you will never find 1/28th of a particle , it is either there in your measuring instruments or it is not , and it is governed by a probability wave mathematical description , not a " matter wave " even more so , the wavefunction manifestation of a particle does not collapse when we measure it the way a balloon collapses when pierced by a pin , because it is just a mathematical description of the probability to find a particle in a particular ( x , y , z ) with a particular ( p_x , p_y , p_z ) within the constraints of the heisenber uncertainty principle . when wavefunction collapse , does not σx become 0 ? , as we will know the location of the particle . or does standard deviation just become smaller ? we know the location of the particle at that specific coordinate where we had our measuring instrument with the specifice momentum that our insturments measured , within the instrument errors . the probability of finding it there after the fact is 1 . it is the nature of all probability distributions that after the detection they become one . example : the probability i will die in the next ten years is 50% . at the instant of my death the probability is one that i am dead . σx is not a standard deviation in the error sense . σxσp≥ℏ2 says that : if i want to know the location of my particle within a region about the x point with uncertainty/accuracy σx , the σp i can measure simultaneously is constrained to be within an uncertainty that follows the constraint σxσp≥ℏ2 . does the particle resurrect into a wavefunction form ? the particle keeps it dual nature of particle or probability wave according to the momentum it still carries and will be appropriately detected as a particle or a probability wave by the next experimenter . it is not a balloon to have been destroyed by the measurement . what can be an observer that triggers wavefunction collapse ? ( electron wavefunction does not collapse when meeting with electrons ; but some macroscopic objects seem to become observers . . . . ) in principle , any interaction of a particle that changes its momentum and position is an observer except that some interactions are quantum mechanical because of the hup and the nature of the interaction and some are macroscopic manifestations in our instruments of the passage of a particle or probability wave of a particle . we usually call observers the classical macroscopic detectors , be they people or instruments . at the microcosm quantum level we have interactions governed by the probability wave functions . what happens to the energy of a particle/wave packet after the collapse ? energy and momentum are conserved absolutely , so it will depend on what sort of detection of the particle took place . some will be carried off by the particle if it has not been absorbed into the detector , as for example these particles in this bubble chamber photograph which continually interact with the transparent liquid of the bubble chamber . in this case a tiny bit of the energy is taken by kicked off electrons ( first detector atom of liquid , final detector photographic plate ) which show by the ionisation the passage of the particle , which is certainly not idiotically " collapsing " .
the direction of reaction on the wheels of car or any body can be simply known by rotating the axis of spin to the axis of reactive gyroscopic couple .
it might be worth taking a look to the original text , galileo 's discourses on two new sciences . the reasoning you are looking for is on the third day , a translation of which may be found online . the relevant parts are labelled theorem i and theorem ii in the above-linked translation . to derive that distance in a uniformly accelerated motion ( e . g . free fall ) goes as time squared , galileo first argues that the time in which any space is traversed by a body starting from rest and uniformly accelerated is equal to the time in which that same space would be traversed by the same body moving at a uniform speed whose value is the mean of the highest speed and the speed just before acceleration began . this is argued on a graphical basis ( see above link ) . however , even though the pictures may look pretty similar to modern functional representations ( e . g . of velocity vs . time ) and arguments involve finding equal areas in different situations , the arguments never involve an actual calculation of an " area " with mixed units , which was not yet conceivable at the time ( e . g . $m/s \cdot s = m$ ) . in fact , the whole third day seems very convoluted precisely because the notion of velocity was not clearly numerical yet , since only commensurable ( same unit ) quantities could conceivably be operated with ( added , divided , . . . ) . proportions of non-commensurable quantities , however , could be compared ( today we had say they are dimensionless ) , as in if a moving object traverses two distances in equal intervals of time , these distances will bear to each other the same ratio as the speeds ( earlier in the third day )
the assumptions of the famous earnshow 's theorem on the impossibility of levitation have some " loopholes " ( http://en.wikipedia.org/wiki/earnshaw%27s_theorem ) , one of them is alternating current electromagnets ( http://en.wikipedia.org/wiki/magnetic_levitation#oscillating_electromagnetic_fields ) . probably , that is the principle used for the installation . another " loophole " is diamagnetism ( not necessarily the meissner 's effect in superconductors mentioned by l . motl ) .
since the vector space is finite dimensional , say of dimension $n$ , then $\hat{a}_-$ is represented by a $n\times n$ matrix $a$ with respect to the basis that op mentions . as valdo suggests , define $\hat{a}_+$ via the hermitian conjugate matrix $a^\dagger$ . now use that any matrix $a$ can uniquely be written as $$ a~=~h+ik , $$ where $h$ and $k$ are hermitian . it is not hard to see that $$h~=~\frac{a+a^\dagger}{2} , \qquad k~=~\frac{a-a^\dagger}{2i} , \qquad a^\dagger~=~h-ik . $$
the cosmic background radiation was always with us , it is not reaching us now . it just became cooler and cooler as time went on . one has to understand that when we are talking big bang and general relativity we are talking of the universe starting from one ( x , y , z , t ) point and as time goes on , expanding . this means that all ( x , y , z ) points of our universe trace back to that one point singularity that went " bang " . envision the two dimensional surface of a balloon , as shown in the wiki link . at time=0 the balloon is one point , call it r=0 . as it expands the points on its surface start receding from each other , and all points on that surface were at r=0 at t=0 . their neighborhood expands as the balloon blows up , and this means the electromagnetic radiation that started in the earth 's neighborhood hot , cools due to the expansion and becomes the cosmic microwave background . that is what i mean it is not coming from anywhere , it just is .
there is a more general statement : all 4d lorentz invariant field theories flow to cfts in the uv and the ir . a proof was given last year by luty , polchinski , and ratazzi in http://arxiv.org/pdf/1204.5221.pdf . their argument has some assumptions but they are fairly weak .
you can callculate the standart deviation as show in this link http://en.wikipedia.org/wiki/standard_deviation#generalizing_from_two_numbers the standart deviation $\sigma=\sqrt{\frac{\sum_{i=1}^n a_i^2}{n}-\left ( \frac{\sum_{i=1}^n a_i}{n}\right ) ^2}$ , where $a_i$ is the $i$-th number in your set and $n$ is the number of numbers you have in your set ( in your example $a_1=32.5$ , $a_2=32.0$ , $a_3=32.3$ so $n=3$ ) using the numbers from your question i got $\sigma \approx 0.20548$
first of all write down an explicit expression for the summation over all microstates . edit since you are treating the system classically this includes an integral over phase-space and a summation over all possible spin-configurations . $$ \sum_{\mu_s} = \sum_{\{s_i\}}\int\frac{d^np d^nq}{h^{3n}n ! }$$ the second thing is to realize that your hamiltonian is non-interacting and the canonical density $e^{-\beta h} $ is just a product of one-particle hamiltonians $$ e^{-\beta h} =\prod_{i=1}^ne^{-\beta h_i} $$ where of course $$ h_i = {\frac{ ( p_i + \gamma s_i ) ^2}{2m} -b s_i} $$ ( i renamed $\beta$ to $\gamma$ , because you do not mean the inverse temp . here ) so you have to evaluate $$ \frac{1}{n ! }\sum_{\{s_i\}}\prod_{i=1}^n\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = $$ because $h$ ist non-interacting , the n-particle phase-space-integral factorizes into n integrations over a 1-particle phase-space . similarily one may interchange the spin-summation and the product ( convince yourself that this is true ! e . g that one ends up with the same terms , whether you sum over spin first or not . ) that means , instead of summing over all the many-body microstates , one first sums over the possible configurations of a single particle and accounts for the fact , that there are many afterwards . additionally all $h_i$ are equivalent . they each just carry a different but redundant index : $$ z = \frac{1}{n ! }\prod_{i=1}^n\sum_{s_i=\pm1}\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = \frac{1}{n ! }\left ( \sum_{s=\pm1}\int\frac{dp dq}{h^{3}} e^{-\beta h}\right ) ^n$$ where $h$ is $h_i$ but without an index , because the reference is not to a specific particle anymore . /edit you will have to think about , what to do with the momentum integration . i have not calculated the result , but it might be you will not end up with a solution in closed form . there might be an approximation needed to do the momentum summation . edit i think a gaussian integration will do the trick . /edit let us know what you end up with !
things are not always in states of minimum energy . this is something that applies to equilibrium states . simple examples of this idea are certainly familiar to you already - a ball comes to rest at the bottom of a valley , not partway down the side . if we want to find the equilibrium state for a white dwarf , there must be no forces on it . if there are no forces , then small changes to the white dwarf do not change its energy , since the change in energy is force*distance . that means the energy should be a " stationary point " , where the energy curve is flat . ( in this case , we are looking at a curve that describes energy as a function of radius . ) we want a minimum so that the equilibrium will be stable . you can apply different reasoning to get to the same result . for example , the white dwarf is in a cold universe , so thermodynamics says it will give away energy as much as it can since this increases the total entropy . this only stops when the white dwarf is at a minimum energy , or at least gets down to a few degrees k .
the hilbert space for a particle in a box has no associated momentum operator ( as a momentum operator implies that the state space is invariant under space translations ) . so your attempt do define one leads to artifacts . [ edit ] in general , the hamiltonian must be a densely defined operator on the physical hilbert space . but a square well potential is too singular as an operator on the space of all square integrable functions on $r$ ( i.e. . , no densely defined subspace is mapped by it into the hilbert space ) , hence its hilbert space cannot be the standard hilbert space . [ edit2 ] note that the usual formula for the momentum is not a canonical momentum on the restricted space of wave functions defined only in the box , as it fails to satisfy either the commutation relation or does not preserve the boundary conditions . the appropriate replacement for the momentum operator is the mode counting operator $\hat n$ with $\hat n|n\rangle=n|n\rangle$ ; see also the derivation of qmechanic .
i have only skimmed the wikipedia article you link to . from a quick look i would say the paragraphs you quote are making points about what a theory of gravity needs to look like . for example you say " curvature of spacetime in only required in order to explain tidal forces " , but what that really means is that it is impossible to have a theory of gravity without curvature . that is because any theory of gravity inevitably has to describe tidal forces . you go on to say " as long as you ignore tidal forces , you can explain gravity without curvature " , but you can not ignore tidal forces so you can not explain gravity without curvature . to take your two specific questions : question 1 . gravity i.e. general relativity is not a theory of forces : it is a theory of curvature . by focussing on the " fictitious forces " you are getting the wrong idea of how gr works . when you solve the einstein equation you get the geometry ( curvature ) of space . this predicts the path a freely falling object will take . we call this a geodesic and it is effectively a straight line in a curved spacetime . if you want the object to deviate away from a geodesic then you must apply a force - and there is nothing fictitious about it . for example , gr predicts that spacetime is curved at the surface of the earth , and if you and i were to follow geodesics we had plummet to the core . that we do not do so is evidence that a force is pushing us away from the geodesic , and obviously that is the force between us and the earth . but , and it is important to be clear about this , the force is not the force of gravity , it is the force between the atoms in us and the atoms in the earth resisting the free falling motion along a geodesic . question 2 . again this is really just terminology . when you are free falling " gravity " is not eliminated . remember that " gravity " is curvature , and in fact the curvature is the same for all observers regardless of their motion . that is because the curvature tensor is the same in all co-ordinate frames . the existance of tidal forces is proof that gravity/curvature is present . when you are free falling you are moving along a geodesic . it is true to say that there are no forces acting , but this is always the case when you are moving along a geodesic . remember a geodesic is a straight line and objects move in a straight line when no forces are acting . there would only be a force if you deviated from the geodesic e.g. by firing a rocket motor . response to fiftyeight 's comment : this got a bit long to put in a comment so i thought i would append it to my original answer . i am guessing your thinking that if you accelerate a spaceship it changes speed , so when you stop something has happened , but when the earth accelerates you nothing seems to happen . the earth can apply a force to your for as long as you want , and you never seem to go anywhere or change speed . is that a fair interpretation of your comment ? if so , it is because of how you are looking at the situation . suppose you and i start on the surface of the earth , but you happen to be above a very deep mine shaft ( and in a vacuum so there is no air resistance - hey , it is only a thought experiment :- ) . you feel no force because you are freely falling along a geodesic ( into the earth ) , while i feel a force between me and the earth . from your point of view the force between me and the earth is indeed accelerating me ( at 9.81ms$^{-2}$ ) . if you measure the distance between us you will find i am accelerating away from you , which is exactly what you had expect to see when a force is acting . if the force stopped , maybe because i stepping into mineshaft as well , then the acceleration between us would stop , though we had now be moving at different velocities . this is exactly what you see when you stop accelerating the spaceship . it is true that a third person standing alongside me does not think i am accelerating anywhere , but that is because they are accelerating at the same rate . it is as though , to use my example of a spaceship , you attach a camera to the spaceship , then decide the rocket motor is not doing anything because the spaceship does not accelerate away from the camera .
in principle , you can charge a conductor indefinitely . but remember that in order to cause a flow of charges from a body ( call it a ' source' ) to another ( the conductor in question ) , the potential of the former has to be lower than the potential of the latter . this potential difference causes a current to flow from the source to the conductor , resulting in a transfer of charges . all the charges that are already in the conductor will exert an electrostatic repulsion on the incoming ones , slowing them down and making it more and more difficult ( as more and more charge accumulates in the conductor ) to charge it further . as charge continues to flow out of the source and into the conductor , the potential difference decreases and the potentials of the two objects become more and more similar . at some point , the potential difference will reach 0 . the current will stop flowing when the charge in the source is equal to the charge in the conductor , which corresponds to the situation in which the electrostatic repulsion from the charges in the conductor is equal to the force attempting to put more charges in . here , the potential difference is 0 . if you want to force more charges to flow out of the source and into the conductor , therefore increasing its charge , you have two options : 1 ) you either increase the potential of the source , so as to create a non-zero potential difference and thus causing current to flow . 2 ) introduce a new force that pushes the charges out of the source and into the conductor : this force ( eg chemical force ) has the job of bringing a charge of the source against the electric field exerted by the charges of the conductor . the real limit to the charging of a conductor would be when there is no physical space available for the electrons . electrons are fermions and they cannot occupy the same position in space , so it will become harder and harder to squash them together . now , in the context of capacitance : imagine a circuit with a generator ( battery ) and a capacitor . dc current will flow only in half of the circuit ( left or right depending on the choice of charge carries , electrons or ions , that is conventional ) . let 's say protons carry the charge ( this is the convention for current , although physically it is the electrons that to the moving ) . protons leave the + terminal and accumulate on the closest plate of the capacitor . the plate is now the conductor , and the + terminal of the battery is the source . the plate has initially 0 charge and therefore 0 potential . the charges accumulating on the plate will exert an electric field that is going to oppose incoming protons . the potential of the + terminal decreases , the potential of the plate increases , the potential difference reaches 0 and current stops flowing : there are as many protons in the + terminal as there are on the plate . unless you increase the charge in the + terminal ( therefore increasing its potential ) or apply another force on the protons , no more current will flow . if you now disconnect the battery and close the circuit , you have one of the capacitor plate charged ( so at a non zero potential ) and the other one with no charge ( 0 potential ) . potential difference => current will flow until the charge on both plates is the same .
it can be justified using distribution theory .
it depends what you consider " radiation " . ultrasound does not involve radition . mri involves radiowaves , which are electro-magnetic radiation ( but not ionizing radiation ) . proton imaging and neutron imaging do not directly involve electro-magnetic radiation , but the term ionizing radiation is usually defined to include particles that cause ionization regardless of whether the particles are photons ( electro-magnetic radiation ) .
i will attempt an answer , though someone knowing the precise ground realities will most likely improve on my answer . you make a very good point about the speed staying constant if the space ship can be treated as a closed system . that is the sole point that we need to worry about . naturally , the atmosphere within a space ship has to be maintained ( at the values that can support human beings ) . if it was just a case of filling up the shuttle once with $21 \%$ oxygen and being done with it , astronauts would keep consuming it so that its levels would fall , and percentage of ${\rm co}_2$ would keep increasing . that is undesirable and in a simplified description , one can get around this by removing ${\rm co}_2$ via a chemical reaction with lithium hydroxide ${\rm lioh}$ . ( by the way , this is a fairly common use of ${\rm lioh}$ , as a carbon dioxide scrubber in breathing purification systems , as can be seen here . ) upon the reaction , these ''canisters'' can be stored and disposed off later . all the excess water ( i.e. . discounting the potable variety ) is directed to tanks , which can again be disposed later . excess heat is handled by converting to ammonia vapor and subsequent storage . ( though somewhat simplified , a description of this process can be found in the first link of this article . ) so , while space shuttles would ''maintain'' a requisite atmosphere , ( apparently ) nothing gets dumped on there an then basis . now , your question pertained to what would happen if this release happens ( or does not happen ) while the shuttle continues moving ahead at a uniform velocity $v$ - if it got dumped , then $v$ would change . does not seem to be the case . see , everywhere in physics , we make all sorts of approximations , the question is how valid they are in real situations . irrespective of which materials you may choose to build the spacecraft with , it will not make a perfect thermal insulator . while they may try to reduce this radiation loss to as low a value as possible , there will be some amount of heat radiated by the craft . so , while not absolutely ideal , it may be be a good approximation to an insulated body , or in the context , let 's say a closed system . in textbook situations , one always considers simplified descriptions . thus , armed with these links , i think you can safely go and pester your instructor , telling him that his original logic had a flaw ! !
for the lower atmosphere , where most of the air is , the temp , pressure , and density of air is given by : $t=t_0-lh$ $p=p_0\left ( \frac{t}{t_0}\right ) ^{\frac{gm}{rl}}$ $\rho = \frac{pm}{rt} $ using the following constants : sea level standard atmospheric pressure p0 = 101325 pa sea level standard temperature t0 = 288.15 k earth-surface gravitational acceleration g = 9.80665 m/s2 . temperature lapse rate l = 0.0065 k/m universal gas constant r = 8.31447 j/ ( mol·k ) molar mass of dry air m = 0.0289644 kg/mol the force due to air resistance can be written : $f = -\rho v^2 c_d a$ where $c_d$ is the coefficient of drag , $v$ is the velocity , and $a$ is the surface area of the projectile . the goal is to get out of the atmosphere ( where force of gravity is roughly constante ) with the earth 's escape velocity , $11.2$ km/s . for a bullet-shaped 1-kg projectile of steel , $c_d \approx 0.04$ and $a \approx 4\times 10^{-4}$ m$^2$ . this leads to a initial velocity of $13.5$ km/s . while not much higher than the vaccuum value , it is still high enough that the bullet would probably vaporize in the atmosphere . interestingly , if one used a sphere instead of a bullet , then $c_d=0.4$ , $a=4\times 10^{-3}$ m$^2$ , the air resistance is 100 times higher , and thus a much greater velocity is needed . but since the drag scales like $v^2$ , this leads to much higher drag . so much so , that even if one could launch the ball at the speed of light ( newtonians only , please ! ) , it still could not make it out of the atmosphere !
there are two reasons why it may not be necessary . . . firstly the intensity of light from the candle is low unlike that of the laser . secondly , the laser encounters two mirrors while light from the candle only encounters one before reaching the detector .
wind load formula : $f_d = \frac{1}{2} \rho v^2 a c_d$ where $f_d$ is the force of drag ( or in this case force against the flat plate ) $\rho$ is the density of the air $v$ is the speed of the air against the object $a$ is the area of the object which the air is blowing against $c_d$ is the drag coefficient
the laughlin state alone does not explain the plateau . there is a lot more to the story . firstly at filling factor=1/3 the many-body ground state of the interacting electron gas is " approximately " the laughlin wavefunction . by this i mean that the overlap between the laughlin state and the numerically found ground state ( for any realistic interaction like coulombic ) is very large , i.e. their inner product is quite close to 1 . using the plasma analogy one can show that this state corresponds to uniform electron density . ( see girvin 's les houches notes for details on plasma analogy . ) secondly the transport phenomena are decided by charged excitations in the system . for the filling factors 1/3,1/5,1/7 , etc . the charged excitations are quasiholes and quasielectrons . while the former has a dip in the density profile at some point z ( say ) in the 2d plane , the latter has the opposite thing in its density profile ( as opposed to the earlier uniform case ) . the plasma analogy can again be used to show that these quasiparticles will have fractional e/3 charge in our case . ( atleast for now let us avoid justifying why they are excited states . ) now lets say we are sitting exactly at 1/3 filling factor and then we add an electron to the system . it will break into 3 quasielectrons which can be separated at no extra energy cost ( the idea of fractionalization ) . similarly if some more electrons are added they will produce more quasiparticles . now start thinking in terms of the ' semiclassical percolation picture ' that is applied to electrons to explain integer qhe ( again see girvin 's notes ) . instead of electrons we give the same arguments using quasiparticles to explain the plateaus around 1/3 filling factor . the conductivities stop changing when the added quasiparticles are either going into the valleys of the disorder potential or are ending up on shorelines at the 2 well separated edges . let me clarify things a bit more . think of starting with the 1/3 filling factor ground state . now let us add adiabatically 1 flux quanta through a thin solenoid at the origin of space ( see laughlin 's nobel lecture ) . he shows that in this process e/3 charge flows towards the origin and gets collected there . thus we have ended up with an exact ground eigenstate of the original hamiltonian + e/3 charge . so quasiholes are ' charged excitations ' , not the excited state when sitting at 1/3 filling . in fact the low energy gapped excitations at 1/3 filling are ' neutral collective excitations ' ( again see girvin 's notes ) and the existence of this gap is necessary for adiabaticity to work fine in the above thought experiment . ( in the words of laughlin the usage of the word quasiparticles here was " unfortunate " . ) now if i just move the filling factor a bit in an experiment the new ground state is made up of new " quasiparticles " .
electrically charged particles interact via their fields and so there is , in general , wide range interaction throughout the gas . the electromagnetic interactions between particles of the gas/plasma can give raise to effects which are significantly different from neutral gas , such as e . g waves . so to what extend the ideas gas law can be considered to " hold " for a plasma will depend on the parameters of the system , temperature , pressure , etc . , but foremostly of the ionization degree of the gas/plasma . it is an involved issue , as this quantity will depend on all the other parameters . one commonly cited relation for certain parameter ranges is the saha equation , which relates temperature and particle density - which are both part of $pv=k_b t\cdot n$ too . microscopic considerations in such a " chemical system " , where the constituents can be ionized and thereby change their properties , lead you to the observation that the value charge density depends on the surroundings . so e.g. the poission equation takes a nonlinear form $\delta\phi=\rho ( \phi ) $ . it is then also related to new'ish system parameters like the debye length , which caracterize the overall bahaviour you ask for . i am sure there are debye length-temperature ranges where it is perfectly reasonable to apply a gas law , just watch out which part of the system makes up charged particles or neutral ones . e.g. i think in space , there are a whole lot of charged particles , but people work with ideal gas laws . a general rigourous classical look at it will lead you to $pv=k_b t\cdot \text{ln} ( \mathcal z ) $ , where the partition function contains the hamiltonian of the system , which include the potentials $\phi$ = energy expressions involving multiple variable-integrals over statistically weighted interaction potentials , see this link .
take 4 points arranged in a square . the middle of the square is a critical point by symmetry , and the midpoint of the four sides of the square would be a critical point just for the two vertices it joins together , ignoring the other two . but the other two are little more than twice as far away , so eight times weaker force , so if you bring the point closer to the center of the square by an amount about 1/8 of the way to the other side , you will cancel out the force from the far pair from the force on the near pair . so there are 5 critical points for four points on a square , and this holds for all sufficiently fast-falling-off forces . using squares-of-squares , i believe it is easy to establish that the number of critical points is generically $n^2$ . i believe it is a difficult and interesting mathematical problem to establish any sort of nontrivial bound on the number of critical points . the trivial bound is from the order of the polynomial equation you get , and it is absurdly large---- it grows like $2^n$ . edit : the correct growth rate the answer for a general configuration is almost certainly n+c critical points ( this should be an upper bound and a lower bound for two different c 's --- i did not prove it , but i have a , perhaps crappy , heuristic ) . for the case of polygons , there are n+1 critical points . for squares where the corners are expanded to squares , and so on fractally , the number of critical points is n+c where c is a small explicit constant . the same for poygons of polygons . i found a nifty way of analyzing the problem , and getting some good estimates , but i want to know how good the mathematicians are at this before telling the answer . perhaps you can ask this question on mathoverflow ?
you are not getting your facts right at all . how do we know from this $\langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx$ or this $\hat{h} = -\frac{\hbar^2}{2m}\frac{d^2}{dx^2} + w_p$ that we have an eigenfunctiuion and eigenvalue . answer : we do not . all i know about operator $\bar{h}$ so far is this equation where $\langle w \rangle$ is an energy expected value : \begin{align} \langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx \end{align} no , you do not . here 's the mathematical side of what an eigenfunction and eigenvalue is : given a linear transformation $t : v \to v$ , where $v$ is an infinite dimensional hilbert or banach space , then a scalar $\lambda$ is an eigenvalue if and only if there is some non-zero vector $v$ such that $t ( v ) = \lambda v$ . here 's the physics side ( i.e. . qm ) : we postulate that the state of a system is described by some abstract vector ( called a ket ) $|\psi\rangle$ that belongs to some abstract hilbert space $\mathcal{h}$ . next we postulate that this state evolves in time by some hermitian operator $h$ , which we call the hamiltonian , via the schrodinger equation . what is $h$ ? you guess and compare to experimental results ( that is what physics is anyway ) . next we postulate for any measurable quantity , there exists some hermitian operator $o$ , and we further postulate that the average of many measurements of $o$ is given by $ \langle o \rangle = \langle \psi | o | \psi \rangle$ . connection to wavefunctions : we pick the hilbert space $l^2 ( \mathbb{r}^3 ) $ to work in , so $\psi ( x ) = \langle x | \psi \rangle$ , and $\langle o \rangle = \int_{-\infty}^{\infty} \psi^* ( x ) o ( x ) \psi ( x ) dx$ . ok , that is the end . the form of $h$ does not follow from the energy expected value . wait ! i have not even talked about eigenvalues and eigenfunctions . this is a useless post ! answer : well you do not have to . but it is useful to find the eigenvalues and eigenfunctions of $h$ , because the eigenfunctions of $h$ form a basis of the hilbert space , and certain expressions become diagonal/more easily manipulated when we do whatever calculations we want to do . so to find the eigenvalues of $h$ , we simply solve the eigenvalue equation as stated above : solve \begin{align} h | \psi_n \rangle = e_n | \psi_n \rangle . \end{align} this is in the form $t ( v ) = \lambda v$ . so as alfred centauri says , we simply want to find the eigenfunctions of $h$ . a more subtle question would be , how do we know they exist ? the answer lies in spectral theory and sturm-liouville theory but nevermind for now , as physicists we assume they always exist . so your additional question : $\hat{a} \psi$ is an eigenfunction of operator$\hat{h}$ with eigenvalue $ ( w-\hbar \omega ) $ . well . . . . that just follows straightaway . you said you already proved that $h a^\dagger \psi = ( w - \hbar \omega ) a^\dagger \psi$ . so here $t$ = $h$ , $a^\dagger \psi = v$ , and $\lambda = ( w - \hbar \omega ) $ . which is an eigenvalue equation $t ( v ) = \lambda v$ . thus , $a^\dagger \psi$ is an eigenvalue of $h$ with eigenvalue $ ( w-\hbar \omega ) $ .
it is a direct consequence of the usual ( weak ) spatial markov property enjoyed by the ising model . namely , if $\lambda$ is a ( deterministic ) finite subset of $\mathbb{z}^d$ , and $f$ a local function with support inside $\lambda$ , then the expected value of $f$ in the box $\lambda$ , with a given frozen configuration $\omega$ outside $\lambda$ only depends on the value taken by $\omega$ along the boundary of $\lambda$ ( assuming nearest-neighbor interactions , as in the linked paper ) . this follows immediately from the dlr equation . the strong markov property is the analogous property when $\lambda$ is a random finite subset of $\mathbb{z}^d$ . in order for the property to be satisfied in this case , it is necessary that knowing $\lambda$ only imposes conditions on the spins outside $\lambda$ ( otherwise that would have an effect on the expected value of $f$ ) ; this is the reason for asking that $\lambda$ be determined from outside . an example would be : " let $\lambda$ be the outermost circuit of $+$ spins surrounding the support of $f$ in a region $\delta$ of $\mathbb{z}^d$" . this imposes constraints outside $\lambda$ ( since , e.g. , there cannot be a circuit of $+$ spins surrounding $\lambda$ inside the region $\delta$ ) , but not inside . the proof of the strong version is then a corollary of the ( weak ) markov property : just decompose the expectation of $f$ according to the realization of the random set $\lambda$ , and , for each realization , apply the weak markov property . ps : forgive the plug , but i would like to point out two recent works that provide alternative approaches to the aizenman-higuchi theorem , and yield considerably stronger results : arxiv:1003.6034 ( ising model ) and arxiv:1205.4659 ( potts model ) . edit : i have looked at your slightly more detailed question on math . stackexchange , but i still do not see precisely where your difficulty lies . so , let me try to give an explicit example . to keep things easy , let us only consider the nearest-neighbor model , and work in the finite box $\lambda=\{-n , \ldots , n\}^2$ with $+$ boundary conditions . let us consider the random variable $\sigma_0$ giving the spin at the center of the box . given a contour $\gamma$ ( i assume you know what this is ) surrounding the origin , let $\mathcal{e}_\gamma$ be the event that $\gamma$ is the outermost contour surrounding the origin , and $\mathcal{e}_\varnothing$ be the event that no such contour exist . then the expectation of $\sigma_0$ can be decomposed according to $\gamma$: $$ \mu^+_\lambda ( \sigma_0 ) = \sum_\gamma \mu^+_\lambda ( \mathcal{e}_\gamma ) \mu^+_\lambda ( \sigma_0 \ , |\ , \mathcal{e}_\gamma ) , $$ where the sum is over all contours $\gamma$ in $\lambda_n$ surrounding the origin , including the degenerate case $\gamma=\emptyset$ . the point , now , is that , when $\gamma\neq\emptyset$ , $$ \mu^+_\lambda ( \sigma_0 \ , |\ , \mathcal{e}_\gamma ) = \mu^-_{\rm int ( \gamma ) } ( \sigma_0 ) , $$ since , conditionnally on $\mathcal{e}_\gamma$ , there is a path of $-$ spins along $\gamma$ ( on the " inner " side of $\gamma$ ) , which decouples the configuration inside $\gamma$ from the configuration outside . note , and this is crucial , that the fact that we chose the outermost contour implies that the configuration inside $\rm int ( \gamma ) $ is unconstrained ( while the configuration outside $\gamma$ is very much constrained , since it must not destroy the fact that $\gamma$ was outermost ) . the last identity is the strong markov property for the finite-volume nearest-neighbor ising model . the property in your question is the analogous one for the infinite volume gibbs measure .
parameters of the surface in order to calculate the size of the sphere we need the equation for gravity field : $$ \nabla\vec{g} = -4\pi g \rho \qquad ( 1 ) $$ and the hydrostatic version of newton 's 2nd law : $$ \rho \vec{g} = \nabla p \qquad ( 2 ) $$ where $g$ is the gravitation constant , $\rho$ is the density of the " cover " , $p$ is the pressure inside the " cover " . in order to solve the first equation one can use gauss 's theorem . in spherical coordinates this will give $\vec{g} = \bigl ( -g ( r ) , 0 , 0\bigr ) $ . the second equation will give the distribution of pressure in the " cover " . on the internal surface it is equal to the pressure of the gas . on the external surface it is zero . stability the surface described by the equation ( 2 ) acts like it is made of water . my intuition says it is unstable . if we put a small piece from the external surface to the internal one it will have less gravitational potential energy . the change of the pressure will be negligible especially for small particles . this is known as rayleigh–taylor instability ( thanks to @mmc ) . in order to get a stable surface we need some solid material . in that case additional forces appear in equation ( 2 ) . then if we increase the pressure of the gas , the cover will expand but not fly away . it will be held by the elastic forces . in this case , though , the pressure should be so high that the balloon stability would be based on the strain not gravity . so this would be a usual balloon .
let 's assume mass of the person plus spacesuit to be $m_1$=100kg asteroid density : $\rho=$2g/cm$^3$ ( source ) that is 2 000kg/m$^3$ 15km/hour is a good common run . that is roughly v=4m/s the orbital height is negligible comparing to the radius , assume 0 over surface . linear to angular velocity ( 1 ) : $$ \omega = {v \over r } $$ centripetal force ( 2 ) : $$ f = m r \omega ^2 $$ gravity force ( 3 ) : $$ f= g \frac{m_1 m_2}{r^2} $$ volume of a sphere ( 4 ) : $$ v = \frac{4}{3}\pi r^3 $$ mass of a sphere ( 5 ) : $$ m_2 = v \rho = \frac{4}{3}\pi r^3 \rho $$ combining ( 1 ) , ( 2 ) , ( 3 ) , reducing : $$ { m_1 r v^2 \over r^2 } = g { m_1 * m_2 \over r^2 } $$ $$ r v^2 = g m_2 $$ combining with ( 5 ) $$ r v^2 = g \frac{4}{3}\pi r^3 \rho $$ $$ r^2 = \frac{v^2}{\rho g \frac{4}{3}\pi} $$ $$ r = v ( {\frac{4}{3}\pi g \rho} ) ^{-{1 \over 2}} $$ substituting values : $$ r = 4 ( {1.33333*3.14159* 6.67300*10^{-11} * 2000} ) ^{-{1 \over 2}} $$ that computes to roughly 5.3 kilometers more interestingly , the radius is directly proportional to the velocity , $$ r [ m ] = 1.337 [ s ] * v [ m/s ] = 371.51 [ h/1000 ] * v [ km/h ] = 597 [ m*h/mile ] * v [ mph ] $$ so , a good walk on a 2km radius asteroid will get you orbiting . something to fit your bill would be cruithne , a viable target for a space mission thanks to a very friendly orbit . note , while in rest on cruithne , the astronaut matching the m_1=100kg would be pulled down with force of 4.5n while not in motion . that is like weighing about 450g or 1lbs on earth .
what you have worked out so far is the left hand side of the answer . you have written that the sum of the accelerations is zero , but that is not correct because the system is being driven by the roller moving across the sinusoidal surface . parameterize in terms of time by writing $x=ut$ , then rewrite your $y_0 ( x ) $ equation as $y_0 ( t ) $ . now that you have the vertical position of the roller as a function of time , you can find its acceleration by taking the derivative twice with respect to time . the result should look comforting . this new acceleration term goes with the force that drives the system .
the best calculation of hawking radiation is the one given on wikipedia , in the article for hawking radiation . this is the method implicitly described by unruh in his famous paper on accelerated observers in qft , and it is mathematically equivalent to the method hawking uses in papers of the 1977-78 period , which use periodicity of the analytically continued solution in imaginary time . hawking 's original calculation assumes that the emitted field quanta are non-interacting . the result is clearly true for interacting theories as well , but this is not completely clear in hawking 's original calculation . further , hawking 's 1976 method uses a pure-black hole solution ( not an eternal solution , no white hole ) , meaning that the horizon was created at a finite affine parameter in the past . the outgoing radiation , when traced back in time , is all coming from a sub-microscopic infinitesimal region right at the moment the horizon first formed . during the sojourn next to the even horizon , the peeling-off property of the horizon ( the fact that outgoing light rays peel away from stationary light rays right on the horizon ) lead to a big magnification of the tiny region where the black hole is first formed . any outgoing photon mode , extended back , comes from a place where its wavelength is trans-trans-trans planckian , this is right the moment of formation of the black hole horizon . the trans-planckian property led many people to be skeptical of hawking 's result . analytic continuation is impossible for general metrics , and it seems strange to use an analytic continuation to find a physical temperature . the calculation below is equivalent to the analytic continuation to imaginary time , but explains why this method works . unruh radiation when an observer is accelerating in minkowski space , the trajectory in space time is a hyperbola , which is the relativistic analog of a circle . the natural coordinates for an accelerating observer are the relativistic analog of polar coordinates . choosing the x coordinate to be the direction of the acceleration , these coordinates are given by $$ x = r\cosh ( \tau ) $$ $$ t= r\sinh ( \tau ) $$ the y , z coordinates transverse to the acceleration are unchanged , and the metric in these coordinates is the relativistic polar metric : $$ ds^2 = dr^2 - r^2 d\tau^2 + dy^2 + dz^2 $$ and it is important to compare this the standard euclidean polar metric $dr^2 + r^2 d\theta^2$ . it is obvious then that the time coordinate is like the angle , while the r coordinate is radial . a green 's function for a quantum field in the $x , t$ coordinates analytically continues to imaginary time , this is wick rotation , and it is the whole point of the feynman formalism . the natural hamiltonian for the $\tau$ coordinate analytically continues to a generator of rotations in the x-t plane , so that the $\tau$ coordinate is periodic with period $2\pi$ . this means that the accelerated observer measures a thermal green 's function for the field with a temperature that is diverging as $1\over r$ ( the $\tau$ period is $2\pi$ , but $\tau$ is dimensionless-- the physical local time coordinate is $r\tau$ , so that the physical energies of high frequency quanta ( those that can be localized ) are $1/r$ bigger ) . the divergence of the unruh temperature at the rindler acceleration horizon is obvious--- it is just the statement that a family of observers at different r in the $r , \tau$ coordinates have an acceleration that diverges as $1\r$ . the unruh radiation , when back-traced , also has a trans-planckian wavelength right at the acceleration horizon . the reason is just because any outgoing mode is redshifted an infinite amount from its value near the horizon , and the constant magnification of the near-horizon region is the analytic continuation of the constant rotation in imaginary time ( rotation is by cosines and sines , which continue to cosh 's and sinh 's , giving the magnifying at the acceleration horizon . the bogoliubov coefficients are given by these near-horizon blow-up factors . the bogoliubov formalism is not particularly enlightening . ) . since the unruh radiation is in flat space , there is no debate about it is correctness . you can model an accelerating detector in ordinary minkowski coordinates to see that the detector goes off as if it is immersed in thermal radiation . another important point about unruh radiation is that the only reason you see it all around you is because the radiation going out of the acceleration horizon free-falls back in , to hit you going the other way on its way back to the horizon . none of this thermal background can escape , because the rindler ( pseudo ) gravitational potential well is infinitely steep--- you need to do an infinite amount of energy to get away from an accelerating observer in the direction of acceleration . equivalence principle by the equivalence principle , a near-horizon observer by a black hole can not tell the difference between the black hole and nothing at all . the black hole horizon is identified with the rindler horizon for a nearby stationary observer , which must be accelerating quickly to keep from falling in . this is a powerful statement , because knowing that the black-hole metric is locally rindler ( using the coordinate $s=\sqrt{r-2m}$ ) means that one can continue into the black hole just by assuming that the horizon is not a special location . this continuation past the horizon is standard , it is how the black hole interior is derived from the exterior solution . it is sometimes called " analytic continuation " , but this is a misnomer , as it does not require analyticity to work . it just needs that the metric is asymptotically rindler , so that the spacetime is really still locally minkowksi space in different coordinates , there are no curvatures or singularities , and so continues into the interior . there are not many different stationary parametrizations of minkowski space , if you want the space to look stationary you need to use a rindler coordinate of some type . this means that any locally flat horizon is locally rindler . extremal black holes are not locally flat , and their near horizon limit is ads , not minkowksi , reflecting the diverging curvature at extremality , due to the pinching off of the interior . the local temperature seen by a near horizon observer diverges as $1/s$ , which is proportional to the radial distance to the horizon . this local temperature is equal to the local periodicity of the solution in imaginary time , because this is true in rindler space--- the $\tau$ periodicity translates into a $t$ periodicity near the horizon . but the black hole solution is stationary , which means that once you know the period in imaginary time near the horizon , if you extend the thermal radiation using the gravitational redshift factor , it stays in equilibrium . any outgoing radiation is redshifted to a lower temperature , while any incoming radiation is blueshifted to a higher temperature . the operator which moves $t$ to $t+a$ is not quite a hamiltonian , since it generates rotations near the horizon ( just like the rindler $\tau$ ) , so you find that for equilibrium , the thermal radiation is the unruh radiation plus a bath at a temperature that goes as the inverse redshift . for unruh radiation , the redshift factor , the square root of $g_{00}$ , is $r$ , it goes to infinity at $r=\infty$ . in the schwartzschild solution , the square root of $g_{00}$ is $\sqrt{1-{2m\over r}}$ , and it asymptotes to a finite limit at infinite r . the analytic continuation matching the unruh temperature gives the hawking temperature at infinity to be ( see wikipedia for the simple calculation , it is an exercise ) : $$t_h = { 1\over 8\pi m}$$ the argument from matching to the unruh limit shows why analytic continuation to imaginary time is correct--- the unruh periodicity continues to the long distance periodicity . the generator of time translations is a symmetry , and if it has a period at some position , imposing the condition that the period is constant gives a consistent background for a path integral , and this path integral is necessarily thermal , with asymptotic temperature given by hawking 's formula . physical emission the argument above only shows that the black hole is in equilibrium with a thermal gas whose temperature at infinity is $1\over 8\pi m$ . it also shows that the correct thermal ensemble for the exterior observer living with an interacting quantum field theory is the one from the path integral in an imaginary-time background which is periodic in t with period $8\pi m$ everwhere , with a background metric equal to the analytic continuation of the schwartschild metric . but what if there is no radiation at infinity ? then there is no infalling radiation , and the thermal emissions from the black hole are not compensated , so the black hole glows thermally . the actual radiation from a black hole is given by detailed balance--- the black hole is in equilibrium with a thermal gas at infinity at the hawking temperature , so the emission rate for a free field theory is related to the absorption by the condition that they balance out in thermal equilibrium , and , since the quanta are free , they are independent processes . this gives the graybody factors for black hole emission from a calculation of the absorption at the same wavenumber .
when one proves that a small part of a greater system is described canonical ensemble , even though the greater system is described by a microcanonical ensemble , the key point is that the density of states of the greater system has the exponential form you mention , over a certain interval of energy . specifically what is important is that $\log \rho ( \eta ) = {\rm const} + \beta \eta$ over the range $\eta = \langle\eta \rangle \pm \delta e$ , where $\delta e$ is the energy fluctuation of the small system , and $\langle \eta \rangle = e - \langle e \rangle$ is the expected energy . the value of $\rho ( \eta ) $ is not really important for energies that are very far outside this range , since those energies never occur . in practice the form of $\rho ( \eta ) $ is usually something like $\log\rho ( \eta ) \approx {\rm const} + n \log \eta$ for some very large $n$ ( such a form is found for gases , in particular ) . this log can be taylor expanded to yield the needed form for the canonical ensemble . for a large environment ( one that is essentially in the thermodynamic limit and much larger than the attached system ) , the first order expansion is very accurate over the relevant energy range .
your calculations are correct . the reason we do not feel it is because the 1 atmosphere of pressure will be applied to all surfaces of our body , including the soles of our feet . in fact the interior of our body is also pushing out against our skin with 1 atmosphere of pressure so there is no net force being applied in either direction at the skin 's surface . the only way you could feel this pressure would be if part of your body was in contact with a vacuum while the rest of your body remained in air at 1 atmosphere of pressure . imagine that you were standing on an opening to a vacuum chamber that was almost , but slightly less than the size of the soles of your feet . if there was a perfectly air tight fit between the vacuum opening and your feet , you would indeed feel that pressure as that much force on your feet just as you calculated . as a simpler experiment , just put your hand over the hose of a vacuum cleaner - that is only a very partial vacuum but the force you need to exert to remove your hand is due to the 1 atmosphere of pressure around us all pushing your hand onto the partial vacuum . in fact if you measured that force needed to pull your hand off of the vacuum cleaner hose , you could compute what the air pressure in the vacuum hose was .
you have actually solved the problem yourself already ! the method you described , solving for the acceleration and plugging it into the formula for displacement , work just fine .
a wave packet/train of the form $f ( x\mp v t ) $ is a $\begin{array}{c}\text{right}\\ \text{left}\end{array} $ mover with velocity $\pm v$ , respectively . how do we deduce which sign is which ? well , pick some fixed argument , e.g. $0$ , of the wave profile $f$ . let us observe this section of the wave profile , where it takes the value $f ( 0 ) $ . we want to ride with this portion of the profile as the wave packet/train moves forward in time $t&gt ; 0$ . to keep the argument fixed $x\mp v t=0$ at zero , we would have to change our position $x=\pm vt$ as time $t&gt ; 0$ passes . in other words , we would move to the $\begin{array}{c}\text{right}\\ \text{left}\end{array}$ .
in most cases , this is related to an assumption of small displacements from equilibrium . assume that the system is described by a potential function $v ( s ) $ , where $s$ represents the coordinate ( s ) associated with the normal modes . let $s_0$ represent value of the coordinates the equilibrium state . taylor expanding the potential about this point yields $$ v ( s-s_0 ) \approx v ( s_0 ) + v' ( s_0 ) ( s-s_0 ) + ( 1/2 ) v'' ( s_0 ) ( s-s_0 ) ^2 + . . . $$ the key feature is that we know $v' ( s_0 ) =0$ , since that is the the definition of equilibrium . we can also ignore the first term since it is independent of the state of the system . thus the resulting form of the equation of motion of the form $$ 0 = \ddot{ s } + \omega^2 s^2 $$ with $\omega^2$ a function of $v'' ( s_0 ) $ and the masses/moments of inertia of the system . this equation has $\sin ( \omega t ) , cos ( \omega t ) $ as its solutions . thus , simple harmonic motion is a generic feature of small oscillations about any mechanical equilibrium .
the speed of light in vacuum is constant and does not depend on characteristics of the wave ( e . g . its frequency , polarization , etc ) . in other words , in vacuum blue and red colored light travel at the same speed c . the propagation of light in a medium involves complex interactions between the wave and the material through which it travels . this makes the speed of light through the medium dependent on multiple factors which include the frequency ( other example factors being refraction index of the material , polarization of the wave , its intensity and direction ) . the phenomenon due to which the speed of a wave depends on its frequency is known as dispersion and is the reason why prism and water droplets separate white light into a rainbow .
i think your question is perfectly fine , i do not think this forum is only for advanced research level questions . assuming you are not actually receiving small droplets of water , the air around the droplets is cooled by the water , which will then cool your skin . this is strongly accentuated by the fact the water create air currents . while the effect you suggest of feeling an absence of radiating heat is physically sound , my intuition would tell me it is negligible in this case . radiation of objects at room temperature would not be very strong compared to the diffusive and convection effects induced by your cold shower .
a positive cosmological constant leads to positive scalar curvature by definition . just trace over the einstein equation and you end up with $$ r = 4\lambda - 8\pi t $$ which is just $$ r = 4\lambda &gt ; 0 $$ in vacuum . the implicit , but more interesting questions are probably the following ones : why can we interpret the cosmological constant as dark energy ? modelling matter as a fluid in equilibrium , ie $$ t_{\mu\nu} = ( \rho + p ) u_\mu u_\nu + p g_{\mu\nu} $$ the einstein equation reads $$ r_{\mu\nu} - \frac 12 r g_{\mu\nu} = 8\pi ( \rho + p ) u_\mu u_\nu + ( 8\pi p - \lambda ) g_{\mu\nu} $$ now , if we want to fold the $\lambda$ term into the matter terms , we require $$ \rho_\lambda + p_\lambda = 0 \\ 8\pi p_\lambda = -\lambda $$ which is $$ \rho_\lambda = -p_\lambda = \frac\lambda{8\pi} $$ a positive energy density with negative pressure . take note that this pressure is not directly responsible for any acceleration or deceleration of the cosmological expansion : it is uniform across space and stays constant in time , and lacking a gradient , does not induce any forces . its effect is purely gravitational in nature - after all , this is just the cosmological constant in disguise . does positive spacetime curvature actually lead to parallel geodesics converging ? not necessarily due to the lorentzian signature of the metric . take 1+1 de sitter space , which can be realized as a hyperboloid in minkowski space and would look like this ( picture taken from wikimedia commons ) : we get geodesics from the intersections of planes through the origin of the ambient minkowski space with the hyperboloid , and time-like ones from those which are angled less than 45° towards the time axis . the vertical lines thus correspond to time-like geodesics and clearly do not converge . this is where slicing into space-like hypersurfaces comes in : in flrw cosmology , there is a preferred slicing where the galactic fluid is homogeneous . in de sitter space , there is no matter and thus no preferred slicing , but we can nevertheless use it to illustrate various features of the cosmological standard model . the horizontal circles , which we obtain by intersecting a parallel family of planes in the ambient space with the hyperboloid , correspond to a spatially closed universe . choosing appropriate coordinates yields the metric $$ ds^2 = -dt^2 + \alpha^2\cosh^2\left ( \frac t\alpha\right ) d\omega^2 $$ where $d\omega$ is the metric of the euclidean sphere and $\alpha=\sqrt{3/\lambda}$ . by tilting our planes , we can also create flat slicings with corresponding metric $$ ds^2 = -dt^2 + e^{2t/\alpha}dy^2 $$ and open slicings with metric $$ ds^2 = -dt^2 + \alpha^2\sinh^2\left ( \frac t\alpha\right ) dh^2 $$ where $dh$ is the metric of the euclidean hyperbolic space . while the light-like geodesics shown above - corresponding to particles at rest in case of the closed slicing - diverge , the spatial curvature will determine what happens to particles in parallel motion through space . however , this is not something that can be shown in our picture of a 1+1 spacetime . how does this result in an accelerated expansion of the universe ? looking at the spatial part of the metrics , all three slicings ultimately lead to an exponential expansion of space , which , in case of a de sitter universe , is just a matter of geometry . in the closed case however , accelerated expansion happens only after a decelerating collapse to some minimal size determined by the value of the cosmological constant . in friedmann models , as long as the cosmological constant dominates over the matter content , we will eventually approach de sitter geometry and thus also exponential expansion .
you can see from the q over x diagram that the p-doping is stronger than the n-doping . ( more aceptors per volume in the p-region than donators per volume in the n-region . ) the overall depletion zone is neutral ( e . g . , the diode is not electrically charged ) . therefore , the overall charge must be zero . the positive charge of the depleted n-region with less donators per volume must be the same as the negative charge of the depleted p-region with more acceptors per volume . therefore , the space charge region must penetrate the n-region deeper than the p-region .
in the picure below , you can see how the row-column grid correspond to the graphene structure . to have a ( n , m ) nanotube , you " just " have to roll your graphene sheet so that the ( 0,0 ) hexagon coincides with the ( n , m ) hexagon . of course , it is much easier said than done !
it is $\sqrt{\frac {gm}{r}}$ , where $m$ is the mass of the body ( earth ) the spacecraft is revolving around , $r$ is the distance from the center of that body ( earth ) , and $g$ is the universal gravitational constant . this can be easily derived from equating centripetal acceleration and gravitational attraction , i.e. $$\frac {mv^2}{r} = \frac {gmm}{r^2}$$ note : this is only applicable for spherical bodies .
this thread will inevitably descend into a semantic and/or philosophical discussion unless we have some at least somewhat precise notion of what it means for particles to be the " same " . in modern physics , elementary particles are fundamentally treated quantum-mechanically , and in quantum mechanics , they are modeled as being exactly the same in the following precise sense : if a system consists of two or more elementary particles , then the state of the system only changes by a multiplicative constant ( which happens to be $+1$ for bosons an $-1$ for fermions ) when one permutes the labels of all of the particles . now , it is also the case that in quantum mechanics , two states that differ by such a multiplicative constant are physically equivalent , so permuting the labels of all of the particles leads to a physically equivalent state of the system .
as you write , the geometric structure factor is $$f_{hkl} = \sum_{j=1}^n f_j e^{i \delta k \cdot \vec r_j}$$ you also correctly state that $\vec{r}_j$ denotes the location of the $j$-th atom in the cell . now , you incorrectly say that $\vec{r}_2 = \vec{a}_1$ , $\vec{r}_3 = \vec{a}_2$ . but $\vec{a}_1$ does not tell you where in the cell the 1st atom is , $\vec{a}_1$ tells you in what direction the next cell starts . the $\vec{a}_i$ are called the lattice vectors and are just there to define the cubic structure . they can be chosen the same for the simple cubic , the bcc and the fcc lattice . what you need are the basis vectors . in a simple cubic lattice , you only have one basis vectors , $\vec{0}$ . in the fcc lattice , we have four atoms per unit cell , and therefore we have four basis vectors , and those are the vectors that you have written down above : $\vec{0} , a/2 ( \vec{x}+ \vec{y} ) $ and so on , where $\vec{x}$ is the unit vector in $x$-direction . in a cubic lattice , we have $\vec{a}_1 = a\vec{x} , \vec{a}_2 = a\vec{y} , \vec{a}_3 = a \vec{z}$ , so your basis vectors read $\vec{r}_1 = \vec{0} , \vec{r}_2 = 1/2 ( \vec{a}_1 + \vec{a_2} ) $ etc . now if you insert those into the exponential and use the fact that $\vec{a}_i \cdot \vec{b}_j = 2\pi\delta_{ij}$ you should get the correct result .
first , the speed of other galaxies is not too helpful . for example , the radial velocity of the andromeda galaxy relatively to us is 300 km/s , i.e. 0.1% of the speed of light only . moreover , internally , everything in that galaxy moves by pretty much the same speed and is confined to the vicinity of that galaxy which makes us pretty sure that no piece will reach us before andromeda galaxy will . more importantly , macroscopic systems in outer space are not moving with the same huge speeds near the speed of light as the cosmic rays essentially because of the second law of thermodynamics . when cosmic rays are accelerated to high speeds , we may treat them statistically and the high energy of these elementary particles may be assigned a high temperature . but the physical systems with many degrees of freedom prefer to evolve to the most likely , high-entropy configurations . that is why the excess energy ( e . g . in a supernova ) tends to divide between the elementary particles chaotically . in particular , the individual particles ' kinetic energy results from velocities that have a random direction . at these huge temperatures ( kinetic energies per particle ) , the atoms are unbound ( well above the ionization energy ) and macroscopic matter does not exist . so the likelihood that a large object will move towards the earth at a near-luminal velocity is as unlikely as the possibility that the numerous atoms in the large objects are assigned velocities with the exactly equal direction although the directions are being chosen from an isotropic , random distribution . after some time for " thermalization " ( interactions between atoms are allowed to change the system with the conservation laws ' being the only absolutely constraints ) , the greater object you consider , the less likely it is that all the atoms in that object will be doing the same thing . this is a form of the second law of thermodynamics . the previous paragraph prevents the creation of " coherent macroscopic cosmic rays " , macroscopic objects that would move in the same direction , from the thermal chaos of hot environments such as the supernova . but even if some astrophysical process managed to eject a chunk of matter at these high speeds , the previous paragraph will still guarantee that we will not receive it on earth . instead , the individual atom of that speedy object would still have some residual mutual velocities so the object would split into individual atoms and we would observe cosmic rays only once again . i could summarize the situation in this way : to shoot a large object by a huge speed from a very distant celestial body to the earth requires one to have the same and huge radial velocities of all atoms but almost vanishing relative transverse velocities . the likelihood of that goes to zero exponentially in any thermal environment . if one assumes that the source of the initial speed is not thermal , then one must accept that the projectiles will derive their speed from their broader environments – speeds of astrophysical bodies that already exist – and those are simply of order 0.1 percent of the speed of light as in the andromeda example or lower . these modest speeds boil down to the inhomogeneities during the structure formation and , ultimately , to inflation , or to the planetary speeds derivable for a star . whenever you want to locally violate the modesty of the speeds , e.g. by a gravitational collapse , you do not " shoot " any new particles before the collision and when the collision happens , the excess energy of the collision also inevitably creates a high temperature and we are back to the previous paragraph . so near-luminal macroscopic bodies are not observed . i would even go further and despite my being a believer that life in the universe is extremely rare , i would say that an object moving by a near-luminal speed would prove the existence of an advanced civilization . i should be a bit careful : the gravitational slingshot is a process that allows the speed to be enhanced even naturally . but even if the source of the speed were a gravitational slingshot and the speed would be really high , like 99.9999% of the speed of light , chances would be high that some intelligence was behind the optimization of the gravitational slingshot because it is extremely unlikely for such an outcome to occur naturally .
this is really an open problem . even for the class of problems which quantum computers are known to be fast at - and shor 's algorithm in particular - there is no ' hard ' proof that classical computers must be slow there . ( to be clear : i do not think any serious computer scientist expects factoring to be in p , but there is no formal proof that it is not . ) it is not clear to me what you mean by ' finite state machine ' . shor 's algorithm does require a universal quantum computer , but any implementation must have finite registers and their size will determine the size of integer they can factor out . from what i can make out , you are asking whether there are special-purpose quantum devices that provide quantum speed-ups , compared to the best classical algorithms , for a particular problem . if that is the case , you may want to look at linear optical quantum computing implementations of the boson sampling problem , which is exactly in that class . some places to look are the computational complexity of linear optics . s aaronson and a arkhipov . proceedings of the 43rd annual acm symposium on theory of computing ( stoc '11 ) , pp . 333-342 . full paper ( 96 pages ) at arxiv:1011.3245 [ quant-ph ] . for a more understandable reference , try this blog entry by aaronson .
as sebastian henckel said , there are many such conservation laws in non-relativistic physics which prohibits particle creation and annihilation . the number of particles of each allowed type is conserved separately . to see the symmetry associated with these numbers , e.g. the total number $n$ of elementary particles , one has to realize that the symmetry variation of an observable $l$ is equal to $$\delta l = \epsilon\{n , l\}$$ where $n$ is the symmetry generator and the bracket is a poisson bracket . clearly , $n$ depends neither on positions $x$ nor on the momenta $p$ which is why the bracket vanishes and the symmetry acts trivially – nothing transforms under it . so you may say that there is a symmetry and it is formally isomorphic to $u ( 1 ) $ but nature only allows objects that are invariant under it so you will never see any " change " linked to the symmetry . quantum mechanically , the bracket is replaced by $1/i\hbar$ times the commutator which is nonzero . the ket vectors simply transform as $$|\psi\rangle \to \exp ( i\lambda n ) |\psi\rangle$$ under the symmetry you are looking for . this is just the overall change of the phase which does not change the physical ( measurable ) properties of the ket vector . because the number of particles is conserved so perfectly , any complete enough measurement of the system will find it in an eigenstate of $n$ . the conservation law implies that the final state will be an eigenstate , too . so after one complete enough measurement , the subspaces of the hilbert space with different eigenvalues of $n$ are pretty much separated from each other – we say that they live in different superselection sectors . let me comment on a word in the title , " approximate " . because the conservation law is exact , the symmetry has to be exact as well , of course . it is uninteresting not because it is violated or approximate but because it acts trivially .
physically what is happening is this : when you touch the positively charged source to the conductor ( the metal sphere ) , electrons leave the conductor through the point of contact . this leaves the point of contact on the conductor with a large deficit of electrons , and thus the point has a positive charge density . the positive charge density produces an electric field in the conductor , which immediately pulls on remaining electrons in the conductor . the electrons remaining spread out until they have eliminated all of the electric fields in the conductor ( if there were remaining fields , the electrons would continue to rearrange ) . the electrons will now be ' more spread out ' than the protons ; the difference between the new electron surface density and the original tells you the distribution of ' excess positive charge ' on the surface . i hope this helps , let me know if you have an application in mind for this ; i often times find it helpful in thinking about problems to temporarily ignore the fact that in practice there is only one charge carrier ( the electron ) and just think about excess positive charge as positively charged particles spreading out .
most materials contract on cooling . the notable exception to the rule are some phase transitions and water . but even ice contracts on cooling . water expands on cooling only between $0^\circ\text{c}$ and $4^\circ\text{c}$ ( including phase transition ) . this corresponds to the part of the graph below , in which density rises with temperature ( note suppressed zero ) . as regarding to what material contracts most , what are you looking for is the coefficient of linear thermal expansion $\alpha$ $$\frac{\text{d}l}{l} = \alpha \delta t . $$ there is plentiful of various tables available on web , e.g. http://www.engineeringtoolbox.com/linear-expansion-coefficients-d_95.html it seems plastic materials contract most on cooling . ethylene ethyl acrylate ( eea ) for example has the largest one coefficient among the solids in this table .
this is really a chemistry question , but i do not think there is a chemistry se at the moment . anyhow , there may not be clearly defined secondary bonds . if you take silicone ( dimethicone ) it has a glass transition temperature about -125c . there are not any specific bonds involved : it is just due to the van der waals forces that you get in any solid . when you have polymers with polar groups , e.g. polymethylmethacrylate you can get hydrogen bonding between the polymer chains . these are more what you had describe as a " secondary bond " and are indeed broken by rising temperature . you generally find hydrogen bonded polymers have much higher glass transition temperatures than polymers that cannot form hydrogen bonds .
an interesting question . you are right , the stress in a crystalline solid , or any solid , is treated by engineers as a macroscopic property of matter assuming matter is a continuous medium . it is given in terms of the external forces acting on the solid per unit area at some direction . hence the distinction of $\sigma_{xy} , \sigma_{yz}$ etc . this goes with the definition of stress $\sigma_{ij}={\frac {df_i}{da_j}}$ where $df_i , da_j$ are cartesian tensors ( not like the general contravariant and covariant tensors in general tensor analysis as in gr . ) this derivative is meaningful locally and , as said above , treats the solid as a continuous medium . in order to define stress in terms of the crystal structure you can relate it via the strain tensor , which can be defined in terms of relative infinitesimal displacements of the atoms in the cell . there are a couple of ways of doing this , one of which is the lagrangian description . in this , the coordinates $ ( x_1 , x_2 , x_3 ) $ of the atoms in the unrestrained state are taken as the independent variables , while $ ( u_1 , u_2 , u_3 ) $ are the relative displacements of the atoms , and they are the dependent variables . this leads to the following definition for the strain tensor ( langrangian strain ) $\eta_{ij}={\frac 1 2} ( {\frac {\partial u_i}{\partial xj}}+{\frac {\partial u_j}{\partial x_i}}+\sigma {\frac {\partial u_r}{\partial x_i}} {\frac {\partial u_r}{\partial x_j}} ) $ where the summation is over the index $r$ . note that this is a function of the coordinates of the atoms in the lattice , so $\eta_{ij}$ is a locally defined quantity . macroscopically strain and stress relate via young’s modulus $e$ in the relation $\sigma=\epsilon e$ ( hooke’s law , ) which would be ok for an isotropic material , in which direction of application of the force is irrelevant . for anisotropic materials , such as crystalline solids in condensed matter physics , this relation is generalised to the following $\sigma_{ij}=\sigma c_{ijkl}\eta_{kl}$ and the summation is assumed over the $kl$ indices . the coefficients $c_{ijkl}$ are the second order ‘elastic constants’ or elastic stiffness coefficients of the material , and they are fourth order tensors , defined as derivatives of the elastic energy of the material , i.e. the potential energy of the atoms in the crystal lattice due to their relative displacements . i think this is what tms meant in his comment . i hope this helps to understand the notion of locality of stress in crystalline solids .
good question , and the answer is that $w ' &gt ; w$ if $f &gt ; mg$ . the reason for this is that if $f = mg$ then the net force is zero so the particle travels at a constant velocity . that means it is kinetic energy has not changed so the only change is the potential energy . however if $f &gt ; mg$ then the net force is positive and the particle is accelerating upwards . that means when it reaches the height $h$ both the potential energy , $mgh$ , and the kinetic energy , $0.5mv^2$ , have increased . the difference between $w$ and $w'$ is the extra kinetic energy of the particle .
consider the system in equilibrium , with the rod hanging straight down . imagine taking a marker and drawing a vertical diameter across the disk . if the rod were fixed to the disk 's center so that the disk could not rotate , then would this marker line still be vertical mid swing ? what does this tell you about the rotation of the disk around its central axis relative to its starting position ? if the disk were not rigidly attached as in the former case , would there now be any torque on the disk around its central axis ? what would the marker line look like mid swing in this case ?
just use the free-fall equation . the time spent in falling from a height $ h $ verifies this : $ h - \frac{g}{2} t^2 = 0 $ so you get : $ g = \frac{2h}{t^2} $ note that you have to determine the height $ h $ . how ? , maybe you can estimate it thinking that the game 's character is about 1.80 m . the $ g $ you are obtaining here is the acceleration of the gravity . in order to get the " universal " gravity constant for the game 's universe , $ g $ , you will need to know , or at least estimate , the mass of the planet and the radius of the planet . . . or the ratio $\frac{m}{r^2}$
as jan noted , the hamiltonian should have a minus sign : $h=\frac{ ( p-qa ) ^2}{2m}$ where $p$ is the canonical momentum , and the expression $p-qa$ is the kinetic momentum $p$ . a homogenous magnetic field is an interesting case , because the vector potential in a given gauge does not exhibit translation invariance , but the physical system clearly does . the solution to this dilemma is that you can preserve translational invariance by changing the gauge as you move the coordinates . there is a conserved quantity associated with this symmetry , but it turns out not to be the canonical momentum ( or the kinetic momentum , either ) . i do not know if it has a particular name , and since it is gauge-dependent there is no universal expression you can write for it . but , for example , in the gauge where $a=\frac{b}{2} ( -y , x ) $ , it is just $ ( p+qa ) $ . if anyone has an insight into any physical interpretation of this quantity i would be interested to hear it . there is a very nice example of all this , worked out concretely , in these notes .
turns out the book ( not paper ) is on google books ( hopefully the link works , otherwise go to books . google . com and search " viscosities of solutions and mixtures" ) . it says for salt ( nacl ) , the values for the three-parameter equation , $$ \frac{\eta_s}{\eta_0} = 1+a\sqrt{c}+bc + cc^2 , $$ are given by $$ a=0.0062 \quad b=0.0793 \quad c=0.0080 $$ unfortunately , the one-parameter values are not given in that book ( at least from my perusal ) , but i imagine you might be able to approximate it by comparing estimations with the above three-parameter values . ps : sorry about not responding to that comment , seems i totally missed that in my feed . hopefully this answer makes up for that !
no , em radiation cannot be completely dissipated . it simply expands to fill the increasing size of the universe , so that each cubic meter contains less of its original free em energy as the universe grows larger . that reduction is reflected by a decrease both in the frequency of the light within that volume and by an overall dimming of the light . there is no minimum frequency or intensity of em radiation due to planck 's constant . all planck 's constant does is decrease the likelihood of finding a photon of em energy in any one volume of space . that is not the same as disappearing , since until you look for it , the em photon is just as likely to be in one such volume of space as another . planck 's constant only quantizes the detection of photons , not the existence ( or minimum level ) of the em field itself . it is one of the odder features of quantum mechanics , that . particles also become less likely to find , both classically and ( if you choose ) if viewed as quantum particles ( field excitations ) . however , in the case of particles that have mass and thus location in space , taking a simple classical scattering the particles over an increasingly huge expanse of space will give you about the same answer as the quantum approach , but with a lot less complexity . ( there are some exceptions to that , e.g. certain types of molecules are far more likely to form in cold space due to quantum tunneling . )
best advice is : do not try to clean them ! a little dust has absolutely no effect on the images , but a bad cleaning can damage the lenses or their coatings irreparably . if you must clean them , use the techniques here : http://www.televue.com/engine/tv3_page.asp?return=adviceid=103 cleaning the mirrors of reflector telescopes is even less desirable and requires even more care : http://www.backyardastronomy.com/backyard_astronomy/chapter_15__polar_alignment,_collimation,_cleaning_and_testing_of_telescopes_files/appendix%20b-collimation.pdf
there are at least two issues : @lagerbaer 's comment points out that that you have to convert the bin numbers into time intervals . the time for your experiment is already fixed , so if you use more bins , then each bin indicates a shorter time interval . the final bin in your third plot means a time twice as much as the final bin in your second plot , and three times as much as the final bin in your first plot . so when you fit your data for the half-life , you are actually getting the half-life in terms of bins . correcting this might be as simple as multiplying by the width of each bin to convert the half-life from bins to $\mu\mathrm{s}$ . to properly fit your data , you have to account for the uncertainty in your measurements . each of your bins is a count or a count rate , and will have some uncertainty in that count or count rate . the decay of a particle follows poisson statistics , where the uncertainty in the number of counts increases with the number of counts . but it increases more slowly , so the relative uncertainty goes down as your measured count rate goes up . including the uncertainties in your analysis re-weights your bins according to their relative uncertainty . by leaving that out , you are inadvertently over-weighting the later bins . over-weighting the later bins like that will give you a larger value for the half-life . one way to look at this is to look at bin 15 in your third plot . for an exponential decay , it should be less than bin 14 , and greater than bin 16 . but it is less than bin 16 . that is a reflection of the counting statistics . the weighting issue in point 2 is a subtle effect ; i do not think it would double the half-life you get from your fit . the bin size in point 1 is much larger , and could account for an error of that size . you should correct that first . and depending on the level of the course , you might not even be expected to do the full uncertainty analysis .
your angular velocity vector is $$ \vec{\omega} = \omega \frac{ \vec{r}_d - \vec{r}_a }{|\vec{r}_d - \vec{r}_a|} $$ where $\vec{r}_a = ( 0,0.2,0.12 ) $ , $\vec{r}_d = ( 0.3,0,0 ) $ , $\vec{r}_b = ( 0.3,0.2,0.12 ) $ in meters and $\omega = 90\ ; {\rm rad/s}$ . your velocity kinematics is $$ \vec{v}_b = \vec{\omega} \times ( \vec{r}_b - \vec{r}_a ) $$ and acceleration kinematics $$ \vec{a}_b = \dot{\vec{\omega}} \times ( \vec{r}_b - \vec{r}_a ) + \vec{\omega} \times \vec{\omega} \times ( \vec{r}_b - \vec{r}_a ) $$ $$ = \dot{\vec{\omega}} \times ( \vec{r}_b - \vec{r}_a ) + \vec{\omega} \times \vec{v}_b $$ from here you plug-and-chug .
the answer is well-known from the analysis of open strings in perturbative string theory : at the end points , the coordinates $x ( \sigma ) $ must also satisfy either neumann or dirichlet boundary conditions for the very same reason that you mentioned . in fact , i think that you have almost answered your own question . the variation of the field is zero . but the very point of the variational calculus - or the principle of the least action - is that the variations may change an allowed configuration to any other nearby allowed configuration . so if you want a constraint of the kind $\delta x ( 0 ) =0$ - or , in your ads case , $\delta \phi|_{y*} = 0$ , the only reason to guarantee it is to impose the condition $x ( 0 ) =const$ or $\phi|_{y*}=const$ for the allowed configurations . you may imagine that the constant is zero - it is just a convention . but because it is a constant , the variation - the difference of the value of $x ( 0 ) $ between two nearby allowed configuration - vanishes . setting a scalar field at the boundary equal to a constant is called the dirichlet boundary condition . alternatively , the boundary term in the variation - the second term in equation 7 - may vanish because the arbitrary nonzero $\delta \phi$ is multiplied by a vanishing coefficient . the coefficient is called $b_{curly}\phi$ in the equation but $b_{curly}$ is an operator proportional to $\partial_5$ . so this method to make the boundary term vanish says that $\partial_5 \phi=0$ at the boundary , and this condition is called the neumann boundary condition . just to be sure , the $d_{curly}$ in the first term of equation 7 is also an operator - and it contains second derivatives . note that the first term has second derivatives integrated over the 5d space , while the second term has first derivatives integrated over the 4d space , so the dimensions agree .
i do not know the answer for the case with two horizons ( non-extremal case ) , but when there is only one ( that is the extremal case : q=m in suitable units and common notation for charge and mass ) , here is how it works . let us first note that in the extremal case , the timelike coordinate outside the black hole remains timelike inside it . this is a major difference with the non-extremal case and with the schwarschild black hole . let us look at what happen to probes and observers in this universe . as you mention , an observer that is at rest in the rn coordinate system and outside the horizon will never see a falling object ( say a massive probe , that is a body with a small mass compared to the black hole mass ) cross the horizon . on the other hand , it takes a finite amount of proper time for the falling body to cross the horizon . the probe , however , will not meet the ( time-like ) singularity . it will automatically , at some finite distance of the singularity , " bounce " back and " return " towards the horizon . this is not in contradiction with causality as the radial coordinate is still space-like in this region . the probe will then again cross the horizon ( but this time from inside to outside ! ) , and one can show that all this is achieved in finite proper time . we thus arrive at the puzzle : where is the probe now ? we know it is outside the horizon . but on the other hand we know it cannot be in the same patch it was before falling into the black hole , as for static observers in this region the probe nerver crossed the horizon . the solution to this puzzle is simply that the probe has arrived a new space-time patch corresponding to the outside of the horizon . ( this is somewhat similar to the schwarshild black hole , where you see that there are light rays that seem to come " from " the black hole , but since it is impossible to escape from it , the light rays must come from a new patch that correspond to the inside of the black hole , that is then called a white hole . ) repeating the above reasoning , we see that consistency of the different viewpoints in the extremal rn universe requires that it contains an infinite number of patches . i do not know the discussion for the non-extremal case , but i hope that at least some aspects of your question are now clearer . cheers
poking around on google with various search terms that included " parachute shape " i came upon " the parachute manual " by dan poynter . table 8.1.7 from that book catalogs empirical data on a host of parachute shapes . assuming that the parachute is round , as you say , and does not have any holes ( apparently , many designs purposefully include gaps to improve stability ) , the first section of that table ( on page 457 ) is the one to look at . to interpret the terms of the table , i tried reading the glossary . it gives the following three definitions : nominal diameter : the diameter of a circle made with the same amount of cloth as the parachute . projected diameter : the diameter of the real parachute when it is inflated . constructed diameter : the diameter of the real parachute when it is not inflated . from this it is clear that the table says " nominal diameter " but means " constructed diameter . " the table is in fact using a nominal diameter of 1 for all the parachutes it lists . using this correction , the table reveals that the ratio between projected diameter and nominal diameter for round parachutes varies between 0.6 and 0.7 this means that given the projected radius , the area is , roughly speaking , somewhere between $2\pi r^2$ and $3\pi r^2$ .
the sound that reaches your ear is just air pressure fluctuating over time . you can use a transducer of some sort to convert the value of air pressure to some other form - for example : to the depth of a groove being cut into a helical track on a layer of wax on a rotating drum to the depth of a groove being cut into a spiral track on a circular disc of metal from which other plastic disks are pressed . to a strength of magnetisation of a magnetic layer on a plastic tape being wound onto a spool to a series of numbers representing the pressure at regular tiny intervals of time . the idea that the variations in pressure over time are due to , or consist of , a collection of frequencies is just a mathematically equivalent description but it does not represent extra information , it is just a different way of describing the same information . here 's some diagrams from a synthesizer manual above are three very different sounds with apparently the same frequency ( say 440 hz ) above is shown how you can add sine waves of two frequencies to produce a more complex waveform above is shown how you can continue adding sine waves of differing frequencies to construct an arbitrary waveform ( a sawtooth ) . the sawtooth waverform can be recorded directly as depths in a groove on a record . but you could " record " the same thing as a set of numbers that represent the frequencies of a dozen sine waves you could add together to produce a single pressure wave that varies over time in the same way . see fast fourier transform
the white cloud you see in the water is steam bubbles . the grains of salt provide nucleation sites that allow the water to vaporize as they fall through the superheated liquid ( so bowlofred had it right--although it is steam that is forming , not dissolved gasses coming out of solution ) . if you raise a pot of water to near boiling and toss in a handful of salt , the water can explode out of the pot due to this effect . counter to what you might think , the addition of salt to water actually raises the boiling point by about one half degree celsius for every 58 grams of salt dissolved per kilogram of water . so the steam is not caused by salty water around the dissolving salt crystals boiling at a lower temperature . the nucleation effect diminishes as the salt diffuses throughout the water . note that the effect is not limited to salt . if you toss in something that does not dissolve--like sand--you will see the same nucleation effect .
as stated , $\mathbf{n}$ is a unit vector and $n_x$ , $n_y$ and $n_z$ are its cartesian components . $\mathbf{n}$ is just a vector pointing in an arbitrarily direction with magnitude 1 . taking $\mathbf{n} \cdot \mathbf{\sigma}$ , we have \begin{equation} \mathbf{n} \cdot \mathbf{\sigma} = n_x\sigma_x + n_y \sigma_y + n_z \sigma_z \\ = n_x \left ( \begin{array}{cc} 0 and 1\\1 and 0\end{array}\right ) + n_y \left ( \begin{array}{cc} 0 and -i\\i and 0\end{array}\right ) + n_z \left ( \begin{array}{cc} 1 and 0\\0 and -1\end{array}\right ) \end{equation}
light passing through a moving medium undergoes a shift due to the difference in frames between the two media . this problem is quite simple to solve in the frame of the river . in this frame the light is moving at an angle and the river is still . the air is moving relative to the river but since air has an index of refraction of $1$ , its movement does not have any effect on the behaviour of light . then you can use the ordinary snell 's law and finally boost back to the original frame . the only subtlety here is that we are in some sense using both the particle and wave viewpoints of light since we will discuss momenta as well as snell 's law , however i can not see an issue with doing so . i denote the lab frame without a prime and the river frame with a prime . the initial light momenta was , \begin{equation} p _i = e \left ( 1,1,0,0 \right ) ^t \end{equation} boosting into the river frame we have , \begin{equation} p ' _i = e\left ( \gamma , 1 , - \beta \gamma , 0 \right ) ^t \end{equation} therefore the angle of incidence is \begin{align} and \tan \theta _i ' = \beta/ \sqrt{ 1 - \beta ^2 } \\ \rightarrow and \sin \theta _i ' = \beta \end{align} now using snell 's law we have , \begin{equation} \sin \theta _f '= \frac{ \beta }{ r} \end{equation} where $ r $ is the ratio of indices of refraction , $ n _f / n _i $ . therefore the momenta of light in the water in the frame of the water is , \begin{equation} p _f ' = e \gamma \left ( 1 , \sqrt{ 1 - \beta ^2 / r } , \beta / r , 0 \right ) ^t \end{equation} boosting back to the lab frame we have , \begin{equation} p _f = e\left ( \gamma + \beta ^2 \gamma /r , \sqrt{ 1 - \beta ^2 / r ^2 } , \beta \gamma + \beta \gamma / r , 0 \right ) ^t \end{equation} to find out how the light will behave once it exits the river we note that the angle of incidence at the second interface is the same as the angle of refraction in the water ( still in the river frame ) . we have , \begin{equation} \sin \theta _{ \mbox{out }} '= r \sin \theta _f ' = \beta \end{equation} this is the same as the initial angle in the water frame . after boosting back to the lab frame we should get back the same perpendicular light arrow . in total the journey of the light should take the form , $\hspace{1cm}$ where the shade of red is proportional to the speed of the river , the lightest being $0.1c$ and the darkest being $0$ . as we would expect in the limit that $\beta\rightarrow 0 $ the refraction effect goes to zero and we note that the effect is only significant for huge river speeds .
let us consider the velocities of the ball at some interim height $h$ on the ball 's way up and down . the absolute value of the velocity is higher on the ball 's way up , as the potential energy of the ball is the same and the total energy is less on its way down due to air resistance . as this applies to any height $h$ , the conclusion is that the time the ball goes up $t_1$ is less than the time the ball goes down $t_2$ .
the sealing effect is caused by pressure differences . once you put the lid back on , the warm air in the box cools , reducing the gas pressure inside the box below atmospheric pressure . so the atmospheric pressure outside the box tends to seal the box if there is contact all around the lid to prevent air moving between inside and outside . water vapour condensing inside the box adds to the effect . the same sort of thing can happen when you close the door of a refrigerator and the air inside cools , again leading to a partial seal .
water molecules don’t carry an electric charge ( and if they do , you don’t want them on your hands… ) . the dipole moment of water molecules can only be used to rotate them in space , not to move them . additionally , the forces that apply to water molecules on your hands also apply to water molecules in your hands . so even if you somehow managed to apply a sensible force on these water molecules , this would get rather uncomfortable . the same problem arises if you attempt to heat them up by means of electric resonance ( similar to a microwave ) . i therefore doubt that it would be possible to build a device based on electric fields rather than moving air , that removes water molecules from the surface of your skin . however , it might be possible to vaporise the water on your hands using strong infrared lamps . this might lead to other problems , though , such as the focusing of infrared radiation on small areas of the skin by water drops .
2012-07-11 addendum based on excellent inputs , in particular from @annav , my answer is now " no , even a direct worst-case hit by the oh-my-god particle would not kill you , even by radiation , because there is insufficient distance and angle to generate a fatal radiation cone . thanks all , and be sure to look at the earlier answer that @dmckee pointed out . ** original answer** ( my answer seems to differ from the earlier ones that @dmckee aptly pointed out , so i will go ahead and risk posting it . my main difference is that i suspect that a head on collision with a large nucleus could produce a wide enough horizontal-splatter radiation cone to produce a fatal event . ) since the 1991 oh-my-god particle was most likely a proton and had the kinetic energy of a fast baseball , i am going out on a limb and saying yes , you could be killed by a single particle . this harvard physics site suggests an approximate energy transfer of about 0.2% in transfers with heavy nuclei , which as i discuss below may be enough to do you in with that kind of particle . but it would be the ensuing radiation event and cone that would do you in , not the kinetic energy of the particle . the main issue is that your head does not have anything in it remotely solid enough to stop or even slow down a particle with that much momentum . so , like a locomotive passing through a cloud of fog , it is going to zip through pretty much as if your head is not there . the question , then , is to ask not what the fog will do to the locomotive , but what the locomotive will do to the fog -- that fog being your head . even a single solid , exactly head-on collision with a nice fat iron nucleus just as an ultra cosmic ray proton enters your head would probably not be a pretty event in terms of the resulting secondary radiation shower . i am guessing ( nothing more , i have not tried to calculate anything ) that outward splattering of a nice little quark plasma , one created as the iron nucleus vaporizes during the transition event , could produce a sufficiently wide cone of particle-zoo ejecta to irradiate a fatal percentage of your brain . rapid heating of your brain would not be a problem , however , since 1/500 of the approximately 50 joules of kinetic energy would work out to be only about 0.1 j of heat energy tops . by comparison a standard firecracker releases about 500 j of energy . and what are the real odds on such a dead-center strike on a large nucleus near the surface of your brain , assuming you were an astronaut unprotected by out atmosphere ? low almost beyond belief . look at the date on the oh-my-god particle : 1991 . we have not seen one quite that feisty since . as @dmckee aptly notes , ordinary cosmic rays or their secondary outputs hit us all the time , and astronauts watch direct collision buzz through their retinas without much harm .
let me guess : you take the spring as it is and hang your objects , right ? then measure the displacement . try to do the following : hang any arbitrary object so that the string will stretch a bit from its initial state . then add you 100g and 200g objects to the initial mass and measure the difference in spring 's length . i will be surprised if you will not get good results . explanation : there are other forces involved when the spring is in its initial condition ( as in the picture ) . when you initially stretch it a bit , you neutralize these forces and the only force left is hooke 's one .
if you leave aside quantum mechanics , the laws of physics are deterministic . that means if you have a perfect description of a system you can calculate the properties of the system arbitrarily far into it is past or future . so if you know the properties of the system at some time you can tell what the system was like in the past . however just because we are calculating past properties from future properties i would not say that makes the properties of the system dependant on it is future . which is all very well , but many interesting systems are chaotic and since it is impossible to characterise even a classical system perfectly we can not calculate the systems properties for more than a short distance into the future . and of course in quantum mechanics we have the collapse of the wavefunction , which means we can not calculate the results of future observations . the best we can do is assign them a probability .
i doubt this is your answer : what is the frequency $f$ doing , in the problem of a magnet sliding down an incline ? what you should ask yourself is : how is changing the thickness of the aluminum foil going to modify the time down the incline ? increasing the thickness of the aluminum foil increases/decreases/leaves unaltered the foil 's resistance ( you pick one ) ; a larger resistance increases/decreases/leaves unaltered the power dissipated by eddy currents . a larger/smaller power dissipated by eddy currents reduces/increases/leaves unaltered the kinetic energy of the magnet at the end of its run . at some point , you will have to decide which formula for the power dissipated it is best to use , $i^2r$ or $v^2/r$ . try to decide whether $i$ or $v$ can be considered identical between two experiments . . . good luck with your homework .
the copenhagen interpretation consists of two parts , unitary evolution ( in which no information is lost ) and measurement ( in which information is lost ) . decoherence gives an explanation of why information appears to be lost when it in actuality is not . " the decay of the off-diagonal elements of the wave function " is the process of turning a superposition : $$\sqrt{{1}/{3}} |{\uparrow}\rangle + \sqrt{{2}/{3}} |{\downarrow}\rangle$$ into a probabilistic mixture of the state $|\uparrow\rangle$ with probability $1/3$ , and the state $|\downarrow\rangle$ with probability $2/3$ . you get the probabilistic mixture when the off-diagonal elements go to 0 ; if they just go partway towards 0 , you get a mixed quantum state which is best represented as a density matrix . this description of decoherence is basis dependent . that is , you need to write the density matrix in some basis , and then decoherence is the process of reducing the off-diagonal elements in that basis . how do you decide which basis ? what you have to do is look at the interaction of the system with its environment . quite often ( not always ) , this interaction has a preferred basis , and the effect of the interaction on the system can be represented by multiplying the off-diagonal elements in the preferred basis by some constant . the information contained in the off-diagonal elements does not actually go away ( as it would in the copenhagen interpretation ) but gets taken into the environment , where it is experimentally difficult or impossible to recover .
the superscript $*$ is a common notation for complex conjugate . going back to check , ( 3.53 ) in the blue english edition states $$y_{l , m} = \sqrt{\frac{2l+1}{4\pi}\frac{ ( l-m ) ! }{ ( l+m ) ! }}p^m_l ( \cos\theta ) e^{im\phi}$$ which is followed by ( 3.54 ) $$y_{l , -m} ( \theta , \phi ) = ( -1 ) ^m y^*_{l , m} ( \theta , \phi ) , $$ making is clear that it has to be complex conjugation .
poisson brackets play more or less the same role in classical mechanics that commutators do in quantum mechanics . for example , hamilton 's equation in classical mechanics is analogous to the heisenberg equation in quantum mechanics : $$\begin{align}\frac{\mathrm{d}f}{\mathrm{d}t} and = \{f , h\} + \frac{\partial f}{\partial t} and \frac{\mathrm{d}\hat f}{\mathrm{d}t} and = -\frac{i}{\hbar} [ \hat f , \hat h ] + \frac{\partial \hat f}{\partial t}\end{align}$$ where $h$ is the hamiltonian and $f$ is either a function of the state variables $q$ and $p$ ( in the classical equation ) , or an operator acting on the quantum state $|\psi\rangle$ ( in the quantum equation ) . the hat indicates that it is an operator . also , when you are converting a classical theory to its quantum version , the way to do it is to reinterpret all the variables as operators , and then impose a commutation relation on the fundamental operators : $ [ \hat q , \hat p ] = c$ where $c$ is some constant . to determine the value of that constant , you can use the poisson bracket of the corresponding quantities in the classical theory as motivation , according to the formula $ [ \hat q , \hat p ] = i\hbar \{q , p\}$ . for example , in basic quantum mechanics , the commutator of position and momentum is $ [ \hat x , \hat p ] = i\hbar$ , because in classical mechanics , $\{x , p\} = 1$ . anticommutators are not directly related to poisson brackets , but they are a logical extension of commutators . after all , if you can fix the value of $\hat{a}\hat{b} - \hat{b}\hat{a}$ and get a sensible theory out of that , it is natural to wonder what sort of theory you had get if you fixed the value of $\hat{a}\hat{b} + \hat{b}\hat{a}$ instead . this plays a major role in quantum field theory , where fixing the commutator gives you a theory of bosons and fixing the anticommutator gives you a theory of fermions .
they are variants , different kinds of quantum field theory , but they are not mutually exclusive . the different adjectives you mention separate quantum field theory to " pieces " in different ways . the different sorts of variants you mention are being used and studied by different people , the classification has different purposes , the degree of usefulness and validity is different for the different adjectives , and so on . conformal quantum field theory is a special subset of quantum field theories that differ by dynamics ( the equations that govern the evolution in time ) , namely by the laws ' respect for the conformal symmetry ( essentially scaling : only the angles and/or length ratios , and not the absolute length of things , can be directly measured ) . conformal field theories have local degrees of freedom and the forces are always long-range forces , which never decrease at infinity faster than a power law . they are omnipresent in both classification of quantum field theories - almost every quantum field theory becomes scale-invariant at long distances - and in the structure of string theory - conformal field theories control the behavior of the world sheets of strings ( here , the cft is meant to contain two-dimensional gravity but the latter carries no local degrees of freedom so it does not locally affect the dynamics ) as well as boundary physics in the holographic ads/cft correspondence ( here , cfts on a boundary of an anti de sitter spacetime are physically equivalent to a gravitational qft/string theory defined in the bulk of the anti de sitter space ) . conformal field theories are the most important class among those you mentioned for the practicing physicists who ultimately want to talk about the empirical data but these theories are still very special ; generic field theories they study ( e . g . the standard model ) are not conformal . topological quantum field theory is one that contains no excitations that may propagate " in the bulk " of the spacetime so it is not appropriate to describe any waves we know in the real world . the characteristic quantity describing a spacetime configuration - the action - remains unchanged under any continuous changes of the fields and shapes . so only the qualitative , topological differences between the configurations matter . topological quantum field theory ( like chern-simons theory ) is studied by the very mathematically oriented people and it is useful to classify knots in knot theory and other " combinatorial " things . they are the main reason behind edward witten 's fields medal etc . axiomatic or algebraic ( and mostly also " constructive " ) quantum field theory is not a subset of different " dynamical equations " . instead , it is another approach to define any quantum field theory via axioms etc . that is why it is a passion of mathematicians or extremely mathematically formally oriented physicists and one must add that according to almost all practicing particle physicists , they are obsolete and failed ( related ) approaches which really can not describe those quantum field theories that have become important . in particular , aqfts of both types start with naive assumptions about the short-distance behavior of theories and are not really compatible with renormalization and all the lessons physics has taught us about these things . constructive qfts are mainly tools to understand the relativistic invariance of a quantum field theory by a specific method . then there are many special quantum field theories , like the extremely important class of gauge theories etc . they have some dynamics including gauge fields : that is a classification according to the content . qfts are often classified according to various symmetries ( or their absence ) which also constrain their dynamical laws : supersymmetric qfts , gravitational qfts based on general relativity , theories of supergravity which are qfts that combine general relativity and supersymmetry , chiral qfts which are left-right-asymmetric , relativistic qfts ( almost all qfts that are being talked about in particle physics ) , lattice gauge theory ( gauge theory where the spacetime is replaced by a discrete grid ) , and many others . gauge theories may also be divided according to the fate of the gauge field to confining gauge theories , spontaneously broken qfts , unbroken phases , and others . string field theory is a qft with infinitely many fields which is designed to be physically equivalent to perturbative string theory in the same spacetime but it only works smoothly for open strings and only in the research of tachyon condensation , it has led to results that were not quite obtained by other general methods of string theory . we also talk about effective quantum field theories which is an approach to interpret many ( almost all ) quantum field theories as an approximate theory to describe all phenomena at some distance scale ( and all longer ones ) ; one remains agnostic about the laws governing the short-distance physics . that is a different classification , one according to the interpretation . effective field theories do not have to be predictive or consistent up to arbitrarily high energies ; they may have a " cutoff energy " above which they break down . it does not make much sense to spend too much time by learning dictionary definitions ; one must actually learn some quantum field theory and then the relevance or irrelevance and meaning and mutual relationships between the " variants " become more clear . at any rate , it is not true that the classification into adjectives is as trivial as the list of colors , red , green , blue . the different adjectives look at the framework of quantum field theory from very different directions - symmetries that particular quantum field theories ( defined with particular equations ) respect ; number of local excitations ; ability to extend the theory to arbitrary length scales ; ways to define ( all of ) them using a rigorous mathematical framework , and others .
when you put the ice cubes in , the temperature of the cubes is ( much ) below freezing temperature . the drink in the glass is above freezing temperature . the interface between the ice and the liquid ( the surface of the ice cube ) is cooled by the ice cube , but heated by the liquid . the ice cube heats up in this process and the liquid cools down in this process ( which is the main reason why you put the cubes in your drinks anyway ) . usually the liquid wins this competition and the ice cube starts shrinking . however , at places where two ice cubes are very close together , the liquid in between the cubes cools very fast ( as it is cooled from two sides and cannot easily be replaced by new , warmer , liquid since it is contained between two walls of ice ) . at these places , the ice cube is able to win and starts expanding . the liquid close to the cubes therefore freezes , forming a bridge . note that bridges have the highest probability to form if the ice cubes come from a very cold freezer and if your drink is not too warm .
the direction of the thrown object is not important since there is not air resistance . whatever upward vertical component the object had will be downward when it passes you . also , the horizontal component does not change . so it does not matter if you throw the object straight up , or at 37 degrees , or at 0 degrees , the object will always have the same energy when it is at the same level . so the potential energy plus the kinetic energy at the start equals the kinetic energy at the end . $\frac {1}{2}m{v_i}^2 + mgy = \frac {1}{2}m{v_f}^2$
nothing , your book did it wrong .
the problem is that heat flow ( in or out of an object ) is related to the temperature difference between the object and it is environment . for the sort of cooling usually found in the kitchen ( convective cooling ) the heat flow , and therefore the rate of temperature change is proportional to the temperature difference . so let 's take some example like a bottle of milk . if you want to heat it quickly that is pretty easy because it is easy to generate a large temperature difference on the hot side . just burn some gas . however to cool the milk quickly we need to generate a large temperature difference on the cool side , and that is hard . you mention liquid nitrogen , and indeed that is a good way to cool things quickly . however you are forgetting all the hours the liquid nitrogen supplier had to put in to cool nitrogen enough to make it liquify . in general it is hard to cool things quickly unless you cheat and start with something ( like liquid nitrogen ) that is already been cooled . response to comment : this started as a comment , but it got a bit involved so i thought i would put it in here . the temperature of anything ( e . g . milk ) depends on how much heat it contains . let 's not get into exactly what heat is , but basically if you add heat it increases the temperature and if you remove heat it reduces the temperature . the problem is that the milk is surrounded by the rest of the world , and this is around room temperature . heat will not flow from a cold place to a hot place , so heat will not flow out of the milk into it is surroundings unless we do some work . typically what we do is use energy to pump heat around . the area we have pumped the heat from becomes cooler and the area we have pumped it to becomes hotter . this is what you do to liquify nitrogen . you have to pump the heat out of it so te nitrogen gets cold and liquifies while the rest of the world gets hotter . once we have the liquid nitrogen we can use it to cool the milk , but it took a lot of work to make the liquid nitrogen . if you are interested in pursuing this , the mechanism for pumping heat around is called ( unsurprisingly :- ) a heat pump . it is basically a heat engine that runs backwards . heating things is easy because there are lots of systems that have stored energy that can be easily converted to heat . for example a gas/air mixturer has chemical energy that is converted to heat by burning it . you mentioned a microwave : this uses electricity that came from chemical energy i.e. from a power station burning gas or coal , so the heat froma microwave originally came from chemical energy . you might wonder why we can not easily convert heat to chemical energy e.g. mix carbon dioxide and water and have it convert to gas and oxygen and cool down in the process . if we could do this it would be an easy way to cool things . the reason why we can not do this is the second law of thermodynamics . explaining this would be a long essay in it is own right , but in brief it is highly probable that a gas/air mixture will convert to carbon dioxide and water ( i.e. . burn ) but it is very improbable that a carbon dioxide/water mixture would convert back to gas and air .
the page to which you linked suggests that the earth will explode due to global warming . what about venus ? not only is it closer to the sun , but its greenhouse effect is an order of magnitude stronger than ours . yet venus has not exploded . the page also suggests that the earth 's core is some kind of nuclear reactor . it can not be a fusion reactor because the earth is not massive enough to start fusing hydrogen like our sun does . there is some fission happening in the earth 's core , but it is decreasing as the radioactive elements are depleted , and the planet is generally cooling as time progresses . the heat energy contribution from the sun has virtually no effect on the internal temperature of the planet . and even if some of the fissionable material in the earth 's core somehow condenses into a critical mass , the explosion would probably not be felt on the planet 's surface . it would take a ridiculous amount of fissionable material to literally " blow apart " the planet ; much more than actually exists inside of it .
rocket fuels initially , followed by a series of gravitational assists ( slingshots ) : http://en.wikipedia.org/wiki/gravity_assist the linked article mentions voyager 1 mission as an example .
there are many ways to answer this question with varying levels of sophistication but here 's an attempt at a short and relatively non-sophisticated answer . assume the classical field obeys a wave equation such that each mode of the field obeys the equation of motion of an independent harmonic oscillator . it is straightforward to show that promoting the classical equation of motion of the field to an operator equation of motion is equivalent to quantizing each mode of the classical field as an independent quantum harmonic oscillator . this allows the quanta of each mode , which are created and destroyed by associated ladder operators for each mode , to be interpreted as " particles " with definite energy and momentum .
protons are positively charged , and neutrons are neutral , so large nuclei are highly positively charged . a postively charged sphere will energetically prefer to break up into two separate charged droplets which move far apart , this reduces the electrostatic energy , since the electrostatic field does work during this process . this thing , spontaneous fission , is usually phase-space unlikely , since you need to have a large chunk of the nucleus tunnel away from another large chunk , and it is unlikely for all those particles to tunnel out together . but at large atomic numbers , you are unstable even just to shooting out an alpha-particle , and this does not require a conspiracy , so large z nuclei are alpha unstable , usually with long half-lives . the positive charge on nuclei puts a limit to the stable ones . the reason is simply that the electrostatic force is long range , while the cohesion force is short range . the same phenomenon causes the instability of water droplets , so that if you charge one up , it will break into a fine mist . the cohesion of the droplets is local , while the electrostatic repulsion is long range . the scale at which you get a fission instability directly can be estimated from surface-tension considerations . if you break a sphere into two adjacent spheres of same total volume , the radius is reduced by the cube-root of two , so that the surface area is decreases by the square of this , and you multiply by 2 ( since there are two spheres ) so the net factor is the cube-root of 2 , which is around 1.3 . so the extra surface tension energy is increased by a factor of 1.3 , or 30% . but in separating the two spheres , you have taken one ball of charge , with an energy of $q^2\over r$ and separated it into two adjacent balls of reduced radius and half the charge . adding up the electrostatic energy , it is about 80% of the original electrostatic energy in the single sphere . so spontaneous droplet fission will happen when you have a charged ball for which 30% of the surface tension energy is less than 20% of the charge energy . since charge goes up almost as the volume ( not quite , but close ) while the surface tension goes up as the area , there is a crossover , and charged droplets will spontaneously separate when they are too big . the surface tension can be found from the binding energy curve of nuclei , and these simple considerations limit stable nuclear size to about that of uranium . the u nucleus can spontaneously fission at an extremely low rate , but the transuranics become progressively more unstable because their electrostatic energy is increasing as the volume to a power greater than 2/3 , while their surface tension energy is increasing as the surface area , which grows as the 2/3 power of the volume . these considerations , in much more sophisticated form , are due to niels bohr in the seminal liquid drop model of the 1940s . this model explained the nuclear binding energy curve quantitatively , and accounted well for fission phenomena . the only major thing left out of this was the shell model and magic numbers , which was supplied by mayer .
for an individual droplet the term is drop deformation and break-up . this can be caused by both shear and elongational stress . for the breakup of a fluid jet into droplets - you are looking at the rayleigh-plateau instability .
ok , i am getting to this a little later than i originally thought i would but hopefully it is ok : ) so let 's go back to the first equation you wrote down , but rearrange it a bit : \begin{equation} \nabla = \partial + \gamma \end{equation} now if i was teaching a first year gr course and someone showed that to me , i would yell at them because , for example , $\nabla \phi =\partial \phi$ with no christoffel symbols . however , if we are willing to be a bit loose with our notation , then there is a sense in which that equation is ok . we just have to say that $\gamma$ does not refer to the christoffel symbols explicitly , but rather is a schematic statement saying that there is a connection piece of the covariant derivative that depends on the representation of the object that the covariant derivative acts on . if the object is a scalar , $\gamma=0$ . if the object is a vector , you get the normal christoffels . if the object is a tensor , $\gamma$ stands for the specific combination of christoffels appropriate for that tensor . it is just like writing $d_\mu=\partial_\mu+ig a_\mu$ for a yang mills theory , strictly speaking you do not know what $a_\mu$ is until you know the representation of the object that the covariant derivative acts on . so with that in mind , let 's switch to the einstein cartan formalism . instead of working with the metric $g_{\mu\nu}$ and associated connection $\gamma^\mu_{\rho\sigma}$ as our variables , we work with the vielbein $e^a_\mu$ and the associated " spin connection " $\omega^{ab}_\mu$ . the vielbein is defined by \begin{equation} g_{\mu\nu} = \eta_{ab} e^a_\mu e^b_\nu \end{equation} and it has a lot of nice properties you can read about elsewhere . the $\mu$ index is the standard tangent space index . both $e$ and $\omega$ are spacetime one-forms , as you can see . the $a , b$ indices can be thought of as internal indices corresponding to an internal lorentz group . the main point is that there is an analogue of $\nabla$ for objects with local lorentz indices . we can call it $d$ , and we can write a similarly schematic equation \begin{equation} d = d + \omega \end{equation} this formalism is most useful when dealing with forms ( ie , with objects who can have any number of local lorentz $a , b$ indices , but all of whose spacetime indices $\mu , \nu$ are ( 1 ) downstairs and ( 2 ) totally antisymmetric ) , so the $\gamma$ piece of the covariant derivative drops out . hence i used the exterior derivative $d$ instead of the partial $\partial$ . this $d$ is a covariant derivative acting on the local lorentz indices . if i do a local lorentz transformation ( which as it says on the tin is a local symmetry ) , this covariant derivative behaves like a local lorentz tensor . now acting on local lorentz scalars , $\omega=0$ . acting on local lorentz vectors you can work out the appropriate expression for $\omega$ . just for clarity , the covariant derivative acts on on lorentz vectors as \begin{equation} d_\mu v^a = \partial_\mu v^a + \omega^{ab}_\mu v^a \end{equation} where you have to work out the components of $\omega^{ab}_\mu$ . you should suspect that there should be some relation between the components of $\omega$ acting on a lorentz vector , and the components of the usual christoffel symbols which are the connection relevant for acting on a spacetime vector . indeed there is such a relationship , it is \begin{equation} \omega^{ab}_\mu=e_\nu^a \partial_\mu e^{\nu , b} + e_\nu^a e^{\sigma b} \gamma^\nu_{\sigma\mu} \end{equation} ( the $e$ with an upper spacetime index $e^\mu$ is an inverse vielbein , ie the matrix inverse of the original vielbein ) . now the nice thing about this derivative $d$ is that unlike $\nabla$ , it can act on spinors . the spinor carries no spacetime index but has 1 local lorentz index that lives in the spinor representation . in that representation , \begin{equation} d_\mu \psi = \partial_\mu \psi - \frac{i}{4}\omega^{ab}_\mu [ \gamma^a , \gamma^b ] \psi \end{equation} here $\omega^{ab}_\mu$ is the same $\omega^{ab}_\mu$ used above . the factor of $-i/4$ depends on the specific gamma matrix conventions you used , i just stole this from wikipedia so i do not know the precise convention . so in the schematic notation $d=d+\omega$ , we see $\omega^{spinor} = -\frac{i}{4}\omega^{ab}_\mu [ \gamma^a , \gamma^b ] $ . ( it might be confusing that $\omega^{spinor}$ involves $\omega^{ab}_\mu$ , but that is the cost of using schematic/sloppy notation . the point is that the connection piece of $d$ involves a $\gamma$-like object when $d$ acts on local lorentz vectors , and involves the $\gamma$ matrices acting on local lorentz spinors ) . so that is at least one sense in which your original statement is correct : there is a connection between the christoffels and the dirac matrices , due to working out properties of covariant derivatives . there is an object , $\omega$ , which in different representations reduces to ( 1 ) essentially the chrisoffel symbols , or ( 2 ) the commutator of dirac matrices .
quantum mechanics ( qm – also known as quantum physics , or quantum theory ) is a branch of physics which deals with physical phenomena at microscopic scales , where the action is on the order of the planck constant . quantum mechanics departs from classical mechanics primarily at the quantum realm of atomic and subatomic length scales . quantum mechanics provides a mathematical description of much of the dual particle-like and wave-like behavior and interactions of energy and matter . quantum mechanics is the non-relativistic limit of quantum field theory ( qft ) , a theory that was developed later that combined quantum mechanics with relativity . quantum field theory ( qft ) is a theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics , by treating a particle as an excited state of an underlying physical field . some of the relativistic quantum field theories would be qed , qcd , and the standard model . references : http://en.wikipedia.org/wiki/quantum_mechanics http://en.wikipedia.org/wiki/quantum_field_theory
i think this is a proper question to be answered with wiki . resources for job seeker general : jobs for phd academic jobs physics : spires american physical society ( aps ) astrophysics : american astronomical society ( aas ) european astronomical society ( eas ) european space agency ( esa ) european southern observatory ( eso ) gravitational waves : gravitational wave detection jobs wiki
your second method is correct . to compare , say , the magnetic field with what you find in jackson , you really need to realize that there is an assumption that you have unit basis vectors there , and that the cross product is actually a hodge dual ( which will invoke factors of the square root of the determinant of the metric ) . these will make direct comparisons a bit tricky when going from one notation to the other . of course , ultimately , both methods will work . the second method is far more error proof and is also coordinate ( and the only metric dependent step is the lowering of the index of $a^{\mu}$ ) independent . ( note that what i"m saying above is that it does not matter if you replace the ordinary derivatives with covariant derivatives , since : $$\nabla_{a}v_{b} = \partial_{a}v_{b} - \gamma_{ab}{}^{c}v_{c}$$ , which means that $$\nabla_{a}v_{b} - \nabla_{b}v_{a} = \partial_{a}v_{b} - \partial_{b}v_{a} - \gamma_{ab}{}^{c}v_{c} + \gamma_{ba}{}^{c}v_{c} = \partial_{a}v_{b} - \partial_{b}v_{a}$$ , so they really are the same thing . )
the crucial fact about these idealized circuits and electric potential differences that leads to the assertion you want to justify is wires are modeled as perfect conductors ( ohmic resistors with negligible resistance ) for which there is zero potential difference between any two points . ( this was edited from " perfect conductors are equipotentials . " ) if we assume that the wires are perfect conductors ( zero resistance ) , then ohm 's law immediately gives the result above . having this fact in hand , we first notice that we have the following mathematical identity ( which is basically the " loop rule" ) : $$ v_a + ( v_d-v_a ) + ( v_c - v_d ) + ( v_b-v_c ) + ( v_a - v_b ) = v_a $$ which we can rewrite as $$ \delta v_{da} + \delta v_{cd} + \delta v_{bc} + \delta v_{ab} = 0 $$ now using the fact above , we note that the potentials at any two points connected solely by a wire are the same , so that $$ \delta v_{da} = 0 , \qquad \delta v_{bc} = 0 $$ which gives $$ \delta v_{cd} + \delta v_{ab} = 0 $$ and therefore since $$ \delta v_{ab} =- \delta v_{ba} $$ we get the desired result $$ \delta v_{cd} = \delta v_{ba} $$
the term sector in quantum field theory typically refers to a portion of the lagrangian . for example , in the standard model lagrangian , we encounter a term , $$\mathcal{l}_{sm}= ( d_\mu \phi ) ^{\dagger} ( d^\mu\phi ) -\frac{m^2_h}{2v^2}\left ( |\phi|^2-v^2/2\right ) +\dots$$ plus others involving couplings with the higgs doublet $\phi$ , and we can refer to the subset as the higgs sector . the hidden sector features all the interactions and terms in the lagrangian which we expect exist , but have not explicitly written in the full standard model lagrangian .
this problem has four separate regions of operation impact : the pen applies an impulse over a short period of time at $b$ and the coin pivots about $a$ as the center of gravity starts to lift off . initial speed is given from the impulse magnitude and the geometry . pivot : the coin pivots about the contact location as the friction force is enough to keep the coin from sliding along the contact . contact forces are found from the pivot constraint . sliding : as the friction is overcome by the motion of the coin it starts to slide along the contact . the reaction forces are found from the friction relationship and the force components along the coin and perpendicular to the coin . flight : the coin flies through the air without contacting the laptop anymore . now you can use the projectile equations to track its position .
no , euclidian space is not necessary . you can " model " intrinsic curvature using the beautiful language of riemannian geometry , whose great triumph was formulating a vocabulary that lets you talk about the curvature of a space without making reference to an extrinsic space in which the curved space is embedded : hence the term intrinsic . this is crucial to general relativity , since embedding a curved 4-space in flat euclidean space requires that the euclidean space be ten-dimensional , and dealing with that embedding would suck .
the special conformal transformations as well as the translations , dilations and rotations are all continuously connected to the identity . this means that they contain parameters such that at some particular value the trasformation becomes trivial . for example , for $b=0$ the special conformal trasformation you write is simply $x_i\mapsto x_i$ . the inversion map $$x_i \mapsto \frac{x_i}{x^2} , $$ on the other hand , is not connected to the identity . we can also note that an inversion changes the orientation of the space , while the other conformal transformations preserve the orientation . in $d$ ( euclidean ) dimensions the special conformal transformations , translations , dilations and rotations together form the lie group $so ( d+1,1 ) $ . this is what , at least in physics , is normally referred to as " the conformal group " . if we also allow for inversions this group is extended to $o ( d+1,1 ) $ .
yes . from clausius theorem the following inequality can be deduced : $$\delta q \le tds$$ where the equality holds in the reversible case . so , a reversible adiabatic process is necessarily isentropic , but irreversible adiabatic processes are not so . to put it in another way , in an irreversible process , according to the above inequality , either entropy changes , or heat must be somehow removed from the system to make it possible to have zero change in entropy . so an irreversible isentropic process can not be adiabatic .
this was previously a comment to space_cadet 's answer but became long ( down-vote was not me though ) . i do not understand space_cadet 's talk about unstable orbits . recall that two-body system with coulomb interaction has an additional $so ( 3 ) $ symmetry and has a conserved laplace-runge-lenz vector which preserves the eccentricity . because interactions between planets themselves are pretty negligible one needs to look for explanation elsewhere . namely , in the initial conditions of the solar system . one can imagine slowly rotating big ball of dust . this would collapse to the sun in the center a disk ( because of preservation of angular momentum ) with circular orbits and proto-planets would form , collecting the dust on their orbits . initially those planets were quite close and there were interesting scattering processes happening . the last part of the puzzle is mystery though . if there were still large amount of dust present in the solar system it would damp the orbits to the point of becoming more circular than they are today . the most popular explanation seems to be that the damping of the eccentricity was mediated by smaller bodies ( like asteroids ) . read more in " final stages of planet formation " - peter goldreich , yoram lithwick , re'em sari .
as mentioned in the comments , you need one more piece of information to determine the magnitude of the velocity . you said that you might use the eccentricity , so in that case you can use the formula given here and deduce a quadratic equation on the velocity which yields : $$ v= \sqrt{\frac{g m}{r \sin ( \alpha ) } ( 1 \pm \epsilon ) } , $$ where $g$ is the gravitational constant , $r$ is the distance between the two masses , $m$ is the bigger mass ( i assumed here that one mass is much bigger than the other ) , $\alpha$ is the angle between the velocity vector and the radius , and $\epsilon$ is the eccentricity . note that since we had a quadratic equation , you still have two options for the velocity , both consistent with the given eccentricity .
the magnitude of the work done by friction in linear motion is equal to the work done by the torque of friction only if the wheel is smooth rolling . in smooth rolling , we have $$v_{cm}=\omega r$$ ( this is equivalent to the fact that the point of contact is at rest ) or equivalently : $$dx=d\theta r$$ now the work done by friction in linear motion : $$dw_f=-f_f \cdot dx$$ and the work done by the torque of friction is : $$dw_f=\tau \cdot d \theta=f_frd \theta=f_fr \frac{dx}{r}=f_fdx$$ . so the the " two works " are equal in magnitude and no energy is dissipated .
neutral quarks can be excluded experimentally ( up to a certain mass ) , because the neutrinos match the invisible decays of z bosons perfectly . if there were neutral quarks below half the z mass , the z boson should also decay into these . for higher masses , things get more involved . finding out that you are not seeing something going on is hard , but our experimental physicists can do just that . and they do not see any " invisible " particles . finally , neutral quarks would also affect the running of the strong coupling which is measured to good precision . new quarks , i.e. new color charged particles , would change the rate at which the strong interaction becomes weaker with higher energies ( it would become weaker even more quickly ) . again , calculations using particles from the standard model match the observation excuisitely , just like for the invisible z dacays .
all these dilemmas with different speed of light as observed by different observers are not real . they never occur in reality . they are only imaginary and can never be tested experimentally . observers in two different inertial frames cannot see/measure the same ray of light . this is physically impossible , for in order for you to see the light it must come exactly to your eye ( measuring device ) . light " exists " only for the one to whom it comes directly . if light is " passing you by " , you cannot see it . therefore , two different observers will never actually measure different speeds of the same light . if one will measure it , the other will not be able to . if i understand your set-up correctly , the person being in the other spaceship ( not the one where the tank is located ) will see only the light sent to him from the tank through vacuum separating the two spaceships . therefore , as the speed of light in vacuum is $c$ , than that is exactly what he will measure . ( and yet , the person in the spaceship with the tank in it will never see light sent to the other rocket , so he will not be able to make any direct comparisons . )
it is because you would not hide in the corners like your kitty does ! a electric radiator is designed to be directional and therefore it does not heat the unnecessary part of your room . it makes you feel warming in front of it , but some part of the room do not get heated like those corner and the ceiling . in comparison , a vacuum cleaner heating the gas instead , so it is much more uniform , but less efficient from your point of view because you do not feel it ( your kitty might be happy about it though ) . as what @johnrennie , they dissipate the same amount of heat ( and some become noise ) .
short answer , no . long answer , sort of . short answer : no , the e and m fields may be coupled by the lorentz transformations , but it is only when they work together to make a self-propagating wave that we can call it a particle . to separate them as individual fields is physically meaningless . so giving each field their own particle is equally meaningless . long answer : this can be thought of as an " is the moon really there when we are not looking at it ? " problem . the only way we can observe that an e or m field is present is when it interacts with something via the lorentz force . so if the field is not interacting with anything , is it really there ? in advanced physics , that answer turns out to be a resounding " no " . what we can say is that the e or m field interacts with objects via a photon . that is , the magnetic or electric field can be said to exists , but we can also say that the source of this field is interacting with our sensors or other particles by exchanging photons to produce a force . thus , to answer your question , we can in a way " quantize " the separate fields and visualize them as a particle , but only if we visualize them not as fields but the exchange of photons between the sources of the field and the objects influenced by it . but full photons , not half a photon .
this is simply the sum of the gravitational potential energy over all the points that make up the body . each point has a mass $\rho dv$ , meaning the mass density times the infinitesimal volume element , and this is multiplied by g and h , because the potential energy of a point at height h is $mgh$ . if you are asking why the potential energy is $mgh$ for a point , this can be argued using reversible elevators attached by pullies . if you want to raise a mass m by a certain amount h , you can do this by putting an equal mass on the other side of a pulley-elevator and lowering the mass by an equal amount . this process is easy--- you do not have to do work--- because the masses balance on the two sides . so if there is a conserved energy , it must be a quantity which is unchanged when you lower a mass by a given amount , so long as you raise the same amount of mass somewhere else by the same amount . since you can cut up big masses into small pieces , you can raise a big mass of mass 2m by h units of height by lowering two masses of size m each by h units . in all these processes , the sum of the mass times the height is conserved . when you have different gravitational forces , like deep in the interior of the earth , you can compensate by using a bigger mass . so it is mg which is the right unit to balance , the force , not the mass itself . this argument establishes that the potential energy is proportional to mgh , and that there is no numerical constant , that the potential energy is just equal to mgh , is just a best convention for defining the unit of energy from the unit of force . this argument is presented in detail in feynman 's lectures on physics vol 1 , in an early chapter . it is essentially due to archimedes , and it is also discussed in this answer to a related question : why does kinetic energy increase quadratically , not linearly , with velocity ?
given some distribution or density $\rho ( x ) , $ a moment is the ' expectation value ' of some power of $x \in \mathbb{r}$ . to be precise , the $n$-th moment $m_n$ is given by $$m_n = \int_{\mathbb{r}} x^n \rho ( x ) \mathrm{d}x . $$ in the mechanics case , $\rho ( x ) $ is simply the mass density . you can extend this to vectors in $\mathbb{r}^d$ in a straightforward way ; for example , for the moment of inertia you replace $x^2$ by $\mathbf{x}^2 = x_1^2 + \ldots x_d^2$ to obtain $$i = m_2 = \int_{\mathbb{r}^d} \mathbf{x}^2 \rho ( \mathbf{x} ) \mathrm{d}^dx$$ which should match the definition given in your mechanics textbook . for the first moment of mass , you need to distinguish different directions . as you indicate , you can choose your coordinates such that $$\int_{\mathbb{r}^d} x_i \ , \rho ( \mathbf{x} ) \mathrm{d}^d x = 0$$ where $i$ runs over the coordinates . in three dimensions , you have $x_1 = x , x_2=y$ and $x_3=z . $
if i remember correctly they only do this in the turns and they use both arms in the straights . it is the outer arm that is active . this helps them turn in two ways . it helps accelerate the outer side more than the inner which is what is what turning really is . the reaction force at the shoulder also helps them lean into the turn which helps them stay stable through the acceleration of turning . this the same reason why bicycles and motorcycles lean into turns and why race tracks for some sports are banked . the mechanics of swinging arms in skating are similar to those in running and even walking . the main benefit is during the back/down part of the stroke . since the arm is moving back , down and also out , there is an opposing forward , up and inward reaction force on the torso at the shoulder . the forward component helps with acceleration , upwards helps extend the stride and the inward helps with balance . doing this only on one side helps turn to the opposite side as described above . there are ( at least ) two reasons why the return part of the arm stroke does not completely undo the benefit of the first part . one reason has to with timing . the active part of the stroke occurs while the foot on that side of the body is off the ground and the other foot is planted so it is helping propel the side of the body that is up moving and there is a longer lever from the shoulder to the fulcrum at the planted foot on the far side . during the return stroke that side of the body is on the ground so it does not slide back due to returning forces . also the leverage from the shoulder to the planted foot is shorter . when the opposite foot is off the ground the return stroke actually helps move the opposite site of the body forward . a simple experiment to demonstrate these effects is to swing just one energetically arm back and forth while standing on one foot and then on the other . observe the different ways your body responds to swinging the same arm in while standing each foot . the other possible reason the return strong does not undo the benefit of the initial stroke is that the arm can be in a more relaxed position during the return stroke so it has smaller moment of inertia and thus the reverse effect is lesser . also return stroke can be less aggressive . this is similar to the way falling cats turn themselves in mid air without violating conservation of angular momentum . i am not certain how big a role this mechanism plays in skating or running though . edit my explanation is only valid in cases where the skaters move as i described . according to @pulsar below , this may not be universally true . however , arms do go out of phase with legs and both arm are used at least some of the time
have a look at http://en.wikipedia.org/wiki/sunlight as this gives the irradience of sunlight as a function of wavelength . the spectrum is very similar to a black body temperature of 5,600k . i was not sure from your question if you were asking about the units used for irradience . the units used in the wikipedia article ( and in the astm page it references ) are power per square metre per nanometre of bandwidth . for example a value of 1.5 at 400nm means that the total power of all the light between 400nm and 401nm is 1.5w/m$^2$ . to model photosynthesis you need to multiply the spectrum by the absorption spectrum of chlorophyll then integrate across all wavelengths to get the total power per square metre .
it is simple . the dilaton-axion ( complexified ) field in supergravity ( and similar classical theories with a noncompact symmetry ) is invariant under $sl ( 2 , r ) $ transformations $$\tau \to \frac{a\tau+b}{c\tau+d} , \quad ad-bc=1$$ however , the same transformation must also transform the charges of objects . for example , one-dimensional branes always carry the charge like $m$ fundamental strings superposed on top of $n$ d1-branes . so the general charge ( density ) of a d1-brane is given by two numbers $ ( m , n ) $ . under the transformation above , they transform to $$ ( m , n ) \to ( am+bn , cm+dn ) $$ because the $sl ( 2 , r ) $ transformation mixes the two types of one-brane charges ( and similarly for other dimensions of branes , including the instantons ) . in the classical theory , the charge of a black $p$-brane , including the one-branes above , is ( a generalization of the charge of a charged black hole ) given by any real numbers ( charges ) $m$ and $n$ . however , quantum mechanically , $m$ and $n$ have to be integers in certain units . consequently , we only get allowed one-branes after the transformation if the final charges , $ ( am+bn , cm+dn ) $ , are integers for all integers $ ( m , n ) $ . this requirement of the integrality of charges implies that $a , b , c , d$ have to be integers by themselves and only the $sl ( 2 , z ) $ subgroup of $sl ( 2 , r ) $ maps allowed states in the hilbert space ( a superselection sector ) to other allowed states in the same hilbert space . note that the quantization ( integrality ) of the charges such as $m , n$ above is required by quantum mechanics . the one-branes are electromagnetic duals of five-branes that carry their charges , too – a combination of d5-brane and ns5-brane charges ( completely analogous to the two one-branes ) . because all these four types of charges ( d1 , f1 , d5 , ns5 ) are allowed to be nonzero but quantum mechanics enforces the dirac quantization rule that the spacing of the d1-brane charge is inverse up to a $2\pi$ factor to the spacing of the d5-brane charge , and similarly for f1 and ns5 , it follows that all these four charges must belong to a lattice . in other words , there has to exist a linear redefinition or convention in which all these four charges are integers .
probably not , but it depends on the geometry of your coil . for a couple of dollars at the hardware store you can get a big stack of those coin magnets . if the answer to your question were yes in general , it would be harder to break apart the big stack of magnets that to separate two of them . that is not consistent with my experience . in general for a dipole $\vec m$ in a field $\vec b$ , the force is $\vec f=-\nabla ( \vec m \cdot \vec b ) $ . if the dipole moment is a constant , and the dipoles are free to rotate , they will orient themselves so that $\vec m$ is antiparallel to $\vec b$ . in that special case , the force simplifies to $$\vec f \approx m \vec\nabla b . $$ in other words , the dipoles " want " to align antiparallel to the field , then to move up the gradient into the strongest part of the field . a dipole in a uniform field feels a torque , but not a net force . if your coil is set up to generate a uniform field , your two stacked magnets will feel the same force — but that force will be zero . if your coil is generating a magnetic field with some dipole component , your stacked magnets will rotate to align with the field , then ( since we have already assumed there is a field strength gradient ) one of them will see a weaker gradient than the other . the gradient $\vec\nabla b$ typically vary like $1/r^4$ , where $r$ is the distance from the center of the coil , so the force on the two coin magnets can vary considerably over a short distance . this is why strong magnets like to suddenly " leap " together and pinch your fingers when you are playing with them . now you could put your two coin magnets next to each other , symmetrically about the axis of your coil , and they would see the same $|\vec\nabla b|$ in slightly different directions . but with the two coin magnets next to each other , they would exert torques on each other , since they prefer to be stacked pole-to-pole ; you could engineer a solution like this , but there would be extra parts involved . i actually happened to have some magnets with this shape on my desk . in response to carl 's comment , i built a little string lifting harness to measure the force on a single magnetized paper clip : adding a second magnet increased , but did not double , the weight that the harness could hold : stacked number of clips magnets that stayed up 1 11 2 17 3 23 4 24 5 24  the dominant effect in calculating the force is the field is right at the surface of the stack of magnets ; it looks like that gets saturated , an effect i did not consider in the first part of my answer .
the technical name for the structure in a battery is known as the " electrochemical cell " . it consists of an electrolyte ( or more of them ) , a liquid with ions that carry the electric charge . the asymmetry in the battery , the main reason why it works , is that these two types of carriers like to participate in chemical reactions in two different containers , near the two electrodes . positively charged ions – imagine $na^+$ from salt , although it is not the most realistic example – like to to get " attached " to one electrode while the negatively charged ions – imagine $cl^-$ ions from salt – like to get " attached " to the other one . these two types of reactions are called reduction and oxidation , respectively , according to the sign of the charge that the ion is gaining or losing . i do not want to get lost in sign errors so i have not assigned them to the first sentence of this paragraph . as these chemical reactions are running , they are producing a charge asymmetry , and therefore a discrepancy between the potentials of the two electrodes , and this asymmetry is compensated by the flow of electrons through the wires of the circuits ( outside the battery ) . in the long run , one is converting chemical energy ( sort of an electrostatic potential energy of ions – they are " higher " , using a gravitational analogy , before they react with the electrodes ) to a hopefully useful energy done by the circuit . the total energy is conserved . the energy deposited to a charged particle moving across voltage $v$ is $e=vq$ . emf , the electromotive force , is a special type of voltage , so its si unit is 1 volt , just like for any other voltage . i say it is a special kind of voltage because it is used for the " energy generating " parts of the circuits only , namely for batteries or parts that produce electricity by electromagnetic induction ( following faraday 's law ) like coils in a variable magnetic field . there is also voltage on resistors and capacitors but it is " passive " , or " consuming " the power created elsewhere , so we do not count it as emf . emf is meant to be the " ultimate source of life " in electrical circuits . concerning the derivation of the relation between current and voltage , i suppose you mean ohm 's law $v=ir$ . its microscopic form is $\vec j = \sigma \vec e$ , i.e. the current is proportional to the electric field where the coefficient $\sigma$ is known as conductivity . this formula holds for metals rather well because the electric field accelerates electrons up to an average speed dictated by the trade-off between the electric field acting on the electrons ; and the decelerating speed from the collisions . this trade-off leads to a velocity that is proportional to $\vec e$ as well , and $\vec j$ is the product of the electron density and the average velocity of these carriers .
the simple answer is that the average galaxy spacing is around a few megaparsecs , while the biggest galaxies are around 0.1 megaparsecs in size . so the average spacing is somewhere in the range of 10 - 100 times the size of the biggest galaxies . the peas i had for lunch today were ( at a guess - i did not measure them ! ) 5mm in diameter so the interpea spacing would be 5 - 50cm , or between 8,000 and 8 per cubic metre . but this is a very misleading statistic . galaxies are not distributed uniformly , but instead are grouped into clusters , which are themselves grouped into superclusters . also galaxies vary enormously in size , with dwarf galaxies around a thousand times smaller than the biggest galaxies . i would resist the temptation to assign any significance to my figures above . however there is a take home message i.e. galaxies are much , much , much closer relative to their size than stars are . that is why galaxy collisions are quite frequent while stellar collisions are rare to the point of non-existance .
let 's start out using notation similar to the example you linked to : $$ \ddot{y}+b\dot{y}+\sin ( y ) =a\cos ( ct ) +d $$ as in the example , we will write this in autonomous form : $$ \mathbf{x}=\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=\begin{pmatrix}y\\\dot{y}\\ct\end{pmatrix} $$ the differential equation can now be written : $$ \mathbf{\dot{x}}=\begin{pmatrix}\dot{x_1}\\\dot{x_2}\\\dot{x_3}\end{pmatrix}=\mathbf{f ( x ) }=\begin{pmatrix}x_2\\-bx_2-\sin{x_1}+a\cos{x_3}+d\\c\end{pmatrix} $$ now , suppose we study the evolution of two different starting points in this system that are arbitraily close . let 's denote the trajectory of one of them with $\mathbf{x} ( t ) $ and the other one with $\mathbf{x} ( t ) +\mathbf{z} ( t ) $ , where $\mathbf{z} ( t ) $ is the vector difference between the two points . the time derivative of $\mathbf{z} ( t ) $ can be written : $$ \mathbf{\dot{z}}=\mathbf{f ( x+z ) }-\mathbf{f ( x ) } $$ if $\mathbf{z}$ is sufficiently small this can be written : $$ \mathbf{\dot{z}}=\mathbf{jz} $$ where $\mathbf{j ( x ) }$ is the jacobian of $\mathbf{f ( x ) }$: $$ \mathbf{j}=\begin{pmatrix} 0 and 1 and 0\\ -\cos{x_1} and -b and -a\sin{x_3}\\ 0 and 0 and 0 \end{pmatrix} $$ over a short period $\delta t$ , the change in $\mathbf{z}$ is given by : $$ \mathbf{z} ( t+\delta t ) =\mathbf{z} ( t ) +\delta t\mathbf{jz} ( t ) = ( \mathbf{i} +\delta t\mathbf{j} ) \mathbf{z} ( t ) $$ assuming $\mathbf{z}$ and $\delta t$ are small enough for the linearizations to hold we can decribe the change in $\mathbf{z}$ from time $t_1$ to $t_2$ with : $$ \mathbf{z} ( t_2 ) =\left [ \prod_i ( \mathbf{i} +\delta t_i\mathbf{j} ( \mathbf{x}_i ) ) \right ] \mathbf{z} ( t_1 ) $$ where $\delta t_i$ are sufficiently small time intervals from $t_1$ to $t_2$ and $\mathbf{x}_i$ is $\mathbf{x}$ at those times . what you need to do to calculate the lyaponov exponents of the this system is to numerically solve the trajectory $\mathbf{x} ( t ) $ for an arbitrary starting point $\mathbf{x}_0$ over a long time interval $t$ . then you need to calculate the matrix $\mathbf{a}$: $$ \mathbf{a}=\prod_i ( \mathbf{i} +\delta t_i\mathbf{j} ( \mathbf{x}_i ) ) $$ using the values for $\delta t_i$ and $\mathbf{x}_i$ from the solved trajectory . you then need to calculate the eigenvalues of $\mathbf{a}$ . let 's call them $\alpha_1$ , $\alpha_2$ and $\alpha_3$ . the lyaponov exponents are given by : $$ \lambda_i=\frac{1}{t}\log{\alpha_i} $$ assuming $t$ is long enough . you need to experiment with different $\mathbf{x}_0$ and $t$ . hopefully , the variations in the lyaponov exponents you calculate for different $\mathbf{x}_0$ will decrease with larger $t$ and converge for sufficiently large $t$ . one problem you will almost certainly run into is that the values of $\mathbf{a}$ will become too big to handle numerically . the way to get around this is to keep track of the magnitude of $\mathbf{a}$ as it is iteratively calculated and normalize it whenever its highest value reaches some threshold . the normalizing factors needs to be saved each time this is done . assuming this has been done you will in the end have a matrix $\mathbf{b}$ and a series of normalizing factors $n_j$ ( in my notation $n_j$ are the values you have divided by when normalizing ) relating to $\mathbf{a}$ as : $$ \mathbf{a}=\left ( \prod_j n_j\right ) \mathbf{b} $$ if $\beta_i$ are the eigenvalues of $\mathbf{b}$ , the lyaponov exponents can be calculated as : $$ \lambda_i=\frac{1}{t}\left ( \log{\beta_i}+\sum_j\log{n_j}\right ) $$ it is usually the largest of the lyaponov exponents that is of interest as it will dominate the exponential growth in $\mathbf{z}$ . at least one of the lyaponov exponents must be positive for the system to be chaotic . it is important to understand that $\mathbf{z}$ should be thought of as a vector that remains very small throughout the whole period $t$ . this might be a bit counter-intuitive since it grows exponentially . however , for any finite $t$ , the starting value of $\mathbf{z}$ can always be chosen small enough to make sure it reamins sufficiently small thoughout the time period .
as can be seen in this pie chart taken from the wikipeia article on " universe " , the significant parts of the universe are , in descending order , dark energy , dark matter , gas , stars , and the ghostly subatomic particles called neutrinos . that is the most laymanish terms that can be used , because nobody knows what the first two actually are .
in fact it depends on the type of glass . depicted is solar transmittance through glas . for greenhouses special heat absorbing glas may be used . the diagram shows a dip of transmittance at $\approx 1200\ , $nm . lower transmittance is because absorption ( as in the name of the glas ) and as well in reflection ( see fresnel equations ) . your reasoning is right : the glas of a greenhouse may reflect infrared light . the most common types have a transmission band up to the nir ( near infrared ) regime . e.g. crown glas ( $350\ , \text{nm}&lt ; \lambda &lt ; 2\ , \mu$m ) and fused quartz ( $200\ , \text{nm}&lt ; \lambda &lt ; 3\ , \mu$m ) .
you might need to say some more about what you want to do with this equation , because you can descend into as much complexity as you like . do you , for example , want to think about variable length strings , i.e. those where the tension lengthens the string and the tension itself is a function of position along the string ? do you want to think about a general , nontangential force ? you could pull out landau and lifshitz " theory of elasticity " or stephen timoshenko " strength of materials volume 2" or " theory of elasticity " and build something pretty complicated , but each new effect modeled is going to yield diminishing returns . assuming a constant tension $t$ in the string of linear density $\mu$ along its length $z$ and assuming still predominantly transverse motion $y ( z , \ , t ) $ in one plane , i get : $$t\ , \cos\theta ( z , t ) \ , \kappa ( z , t ) = \mu\ , \partial_t^2 y\quad\quad\quad ( 1 ) $$ where $\theta$ is the string 's angle made with the horizontal and $\kappa$ its curvature . substituting for $\cos\theta$ and $\kappa$ yields : $$t\ , \partial_z^2 y = \mu\ , ( 1+ ( \partial_z y ) ^2 ) ^2\ , \partial_t^2 y\quad\quad\quad ( 2 ) $$ which will give you a nice nonlinearity to chew on . next , you might consider a constant tension , constant length string with vibration but with motion in both transverse directions . so you are going to get two coupled nonlinear differential equations . let our two transverse displacement components be $x ( z , t ) $ and $y ( z , t ) $ , then the local tangent to the string be defined by the unit vector components $x = \partial_z x/\sqrt{1 + ( \partial_z x ) ^2+ ( \partial_z y ) ^2}$ and $y = \partial_z y/\sqrt{1 + ( \partial_z x ) ^2+ ( \partial_z y ) ^2}$ so that ( here $s$ is the arclength ) : $$t\ , \mathrm{d}_s x = t\ , \partial_z\left ( \frac{\partial_z x}{\sqrt{1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2}}\right ) \mathrm{d}_s z = \mu\ , \partial_t^2 x\quad\quad\quad ( 3 ) $$ $$t\ , \mathrm{d}_s y = t\ , \partial_z\left ( \frac{\partial_z y}{\sqrt{1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2}}\right ) \mathrm{d}_s z = \mu\ , \partial_t^2 y\quad\quad\quad ( 4 ) $$ whence ( since $\mathrm{d}_z s = \sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2}$ ) : $$\left ( 1+ ( \partial_z y ) ^2\right ) \ , \partial_z^2 x- \partial_z x\ , \partial_z y\ , \partial_z^2 y = \frac{\mu}{t}\ , \left ( 1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2\right ) ^2\ , \partial_t^2 x\quad\quad\quad ( 5 ) $$ $$\left ( 1+ ( \partial_z x ) ^2\right ) \ , \partial_z^2 y- \partial_z x\ , \partial_z y\ , \partial_z^2 x = \frac{\mu}{t}\ , \left ( 1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2\right ) ^2\ , \partial_t^2 y\quad\quad\quad ( 6 ) $$ which reduce to eq . ( 2 ) when there is vibration in one plane only . you will get some really interesting effects from these coupled equations : whirling , coupling of energy from $x$ to $y$ and back again and so forth . the next step would be to think of the axial motion of the string and the attendant variable tension along the string 's length . this would only be apparent well into the nonlinear régime and likely ( 5 ) and ( 6 ) should model most of the nonlinear effects you will need . energies in the string if you are seeking to find out the work done by the end of the string , then you would need a model of what it is linked to and therefore a tension to displacement expression - likely a differential equation , which will be a differential equation . now the tension $t$ is a function of time , so you are beginning to get seriously interesting ! you might also be interested in looking at a tension varying with length at this point , with the local tension defined by $e\ , a\ , \epsilon ( z , t ) = k_t\ , \epsilon ( z , t ) $ , where $e$ is the string 's young 's modulus , $a$ its cross-sectional area and $\epsilon ( z , t ) $ the strain . it makes more sense to use $k_t$ and measure experimentally : it is not going to be easy to work out $k_t$ from first principles from the material elastic constants for a braided or stranded string ! the sting 's curvature begets the strain : $\mathrm{d} s = \sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2} \mathrm{d} z$ so that $$\epsilon ( z , t ) =\sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2} - 1 \approx \frac{1}{2}\left ( ( \partial_z x ) ^2 + ( \partial_z y ) ^2\right ) \quad\quad\quad ( 7 ) $$ if you looking for loss in the string , a good model of air drag force is $−\lambda\ , \partial_t x$ , $−\lambda\ , \partial_t y$ ( i.e. . proportional to transverse velocity ) , which terms you will need to include in the dynamical equation , then work out loss from the power dissipated by these terms . internal material bending losses are complicated to model : often you can do this kind of thing by replacing material elastic constants with lossy elastic operators - so you would replace the young 's modulus $e$ for example by something of the form $e+e_t \partial_t$ , for some loss constant $e_t$ ; equivalently , you would work with $k_t + k_1 \partial_t$ for the string 's effective srping constant . but , at last , if you , as i now understand from your questions , are looking simply to find out the energy needed to set the vibration up ( the energy stored in the string ) in a lossless string , then you can work as follows . the kinetic energy per unit length is obvious : it is simply : $$k ( z , t ) = \frac{1}{2}\ , \mu\ , \left ( ( \partial_t x ) ^2 + ( \partial_t y ) ^2\right ) \quad\quad\quad ( 8 ) $$ now , if we assume that the displacement is small , such that the at first high tension $t$ does not change much as the string vibrates , then the work done by $t$ in straining a length $\mathrm{d}z$ of string is $t\ , \epsilon\ , \mathrm{d}z$ , so that the potential energy stored per unit length is , from eq . ( 7 ) : $$u ( z , t ) = t\ , \epsilon\ = \left ( \sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2} - 1\right ) \ , t\approx\frac{1}{2}\ , t\ , \left ( ( \partial_z x ) ^2 + ( \partial_z y ) ^2\right ) \quad\quad\quad ( 9 ) $$ the approximation holding when $|\partial_z x| , \ , |\partial_z y|\ll 1$ . these are the general equations . to find the dispersion relationship for the uncoupled linear vibration equations $t\ , \partial_z^2 y = \mu\ , \partial_t^2 y$ , $t\ , \partial_z^2 x = \mu\ , \partial_t^2 x$ we study solutions of the form $\exp ( i\ , ( k\ , z\pm\omega\ , t ) ) $ where $k$ is the wavenumber and $\omega$ the angular frequency ; on substitution into the linear equations , we get $t\ , k^2 = \mu\ , \omega^2$ or : $$c = \left|\frac{\omega}{k}\right| = \sqrt{\frac{t}{\mu}}\quad\quad\quad ( 10 ) $$ so for such a wave , eq . ( 8 ) and eq . ( 9 ) ( the latter in the small vibration $|\partial_z x| , \ , |\partial_z y|\ll 1$ approximation ) can be combined to show that $u ( z , t ) = k ( z , t ) $ , as you state . likewise , by using this relationship as well as parseval 's theorem for fourier series for any superposition of frequencies such that the waveshape is periodic , you can prove that the total kinetic and potential energies integrated over a wavelength are equal . but this is for the linear régime only . more generally , you must use eq . ( 5 ) and eq . ( 6 ) together with eq . ( 8 ) and eq . ( 9 ) separately . even with these equations , it would be altogether reasonable to assume the small vibration approximation with eq . ( 9 ) , because none of the above considers $z$-directed components of the force , which will become significant with angles that are big enough to make the small vibration approximation of eq . ( 9 ) invalid . therefore , your final set ( approximating the rhs of ( 5 ) and ( 6 ) in the same way as ( 9 ) ) might be : $$\begin{array}{rcl} \left ( 1+ ( \partial_z y ) ^2\right ) \ , \partial_z^2 x- \partial_z x\ , \partial_z y\ , \partial_z^2 y and = and c^2\ , \left ( 1+2\ , ( \partial_z x ) ^2+2\ , ( \partial_z y ) ^2\right ) \ , \partial_t^2 x\\ \left ( 1+ ( \partial_z x ) ^2\right ) \ , \partial_z^2 y- \partial_z x\ , \partial_z y\ , \partial_z^2 x and = and c^2\ , \left ( 1+2\ , ( \partial_z x ) ^2+2\ , ( \partial_z y ) ^2\right ) \ , \partial_t^2 y\\ k ( z , t ) and = and \frac{1}{2}\ , \mu\ , \left ( ( \partial_t x ) ^2 + ( \partial_t y ) ^2\right ) \\ u ( z , t ) and = and \frac{1}{2}\ , \mu\ , c^2\ , \left ( ( \partial_z x ) ^2 + ( \partial_z y ) ^2\right ) \end{array}\quad\quad\quad ( 11 ) $$ with $c$ defined by eq . ( 10 ) .
following larry 's response , but with approximate numbers : assume an " average " night means september 21 / march 21 ( nights about that length are more common than others ) . from http://www.sunrisesunset.com/calendar.asp , i get that the night lasts 11:45 in boulder , co ( which is at 40n ) . i would assume that everything about 10 degrees above the horizon is visible , and everything below is not - that is true in many locations either because of trees , atmosphere opacity , mountains , or city lights . if you are using a telescope , i would not ever go below 20 degrees . a general formula using a surface integral on the surface of a sphere : $a = \text{angle above horizon something is considered visible}$ $b = \text{latitude}$ $c = \text{number of degrees in the night = number of hours in the night / 24 * 360}$ $d = 180 - 2*a + c$ $e = ( 90-b ) -a$ $ \text{visible fraction} = \left ( \int_0^d \int_{90-e}^{180} \sin ( \varphi ) d\varphi d\theta \right ) / 4 \pi$ $= - ( \cos ( 180 ) - \cos ( 90-e ) ) * d / ( 4\pi ) $ $= d ( \sin ( e ) + 1 ) / ( 4\pi ) $ ( d and e must be converted to radians ) for an 11h45m night , that comes out to 38.0% , 29.9% , 22.0% for $a=$10 , 20 , and 30 degrees respectively . if you consider that $\cos ( 90-b ) / 2$ of the sky is never visible ( because it is always below the horizon ) , these become 61.6% , 48.5% , 35.7% of the sky that you could ever see . these calculations were somewhat hasty . . . i expect to be brutally corrected . the real answer is much more complicated - you need to do an integral over a sphere after rotating the pole , which gets into euler parameters and quaternions . still , i think my first guess is probably correct to within about 5-10% .
it is true that time slows down for someone travelling near the speed of light , so you could use this to travel into the future i.e. only age one day while the earth ages one year . however this is not really time travelling in the usual science fiction sense . for physicists time travel generally means a closed timelike curve ( ctc ) . see http://en.wikipedia.org/wiki/closed_timelike_curve for a good article on this . basically it means being able to visit the same point in spacetime more than once . there are solutions of the general relativity equations that allow ctcs , but it is not clear how physical these are . einstein 's equations allow solutions that do not have anything to do with the real universe . there is a suggestion called the chronology protection conjecture that the universe does not allow ctcs , but as far as i know no-one has proved this . as of right now no-one knows for sure if ctcs , and therefore time travel , are possible or not , though i suspect most physicists think not . i did not see the discovery channel programme you mention , but as far as i know no-one has sent anything back in time whether it is a photon or anything else .
turns out there are two methods that i have found out . . . there many be others : 1 ) repeat the simulation n times with different initial conditions and use the usual statistic techniques to calculate the mean and error of the variance quantities . 2 ) bootstrapping , sub sample the data as follows : randomly choose n frames from m frames , where m > n n [ n defined below ] . calculate quantities in the set n . repeat n times calculate statistical variances among the n sets of calculations . source : dr peter olmsted @ leeds university
regarding supersymmetric quantum mechanics : by all means you should read and absorb the source of all supersymmetric quantum mechanics , which is still probably the deepest article on this topic , namely the original edward witten supersymmetry and morse theory this is one fourth of the work that won witten the fields medal in 1990 . when i learned this stuff back then , i was very much enchanted by the perspective of spectral geometry ( i.e. . connes ' spectral triples ) that is highlighted in the survey jürg fröhlich , oiver grandjean , andreas recknagel , supersymmetric quantum theory , non-commutative geometry , and gravitation i would say if you read that in parallel with witten , you will end up with a rather profound understanding of what the deep aspects here are . because , as you will notice , the topic of susy qm has the tendency to make many people write many rather shallow articles . stay clear of them and focus on the substantial stuff here . speaking of substance , of course the full conceptual impact of supersymmetric qm rests in its relation to index theory and k-theory . you may not need that for your physics project ( depends ) , but for your own education i would suggest to at least look at the surveys and get the basic idea . this is tremendous stuff . finally a fun fact to know -- in particular when somebody quizzes you on applications or is sceptical about the value of supersymmetry -- is that every fermion particle is described by relativistic supersymmetric quantum mechanics on its worldline . see here . this is in itself a somewhat trivial fact really , but drastically underappreciated . it is of course directly related to the way that witten initially bumped into susy qm in the first place , namely by considering the point-particle limit of superstrings .
( 1 ) as for the ( a ) the total force of the ground/hinge ( e . g . thrust or normal force + friction ) is generally neither vertical nor horizontal . edit : you can obtain the force of the ground/hinge by calculating the force of the rope first , and then add all three forces together to get zero . as for the ( b ) you have three forces acting to beam , force of the ground , gravitational force and force of the rope . since problem suggests " considering equilibrium " , torques of these three forces must equal zero . ( 2 ) force at point b is simply the force of rod bd to rod ac ( and vice versa ) . effectively , you have three forces acting on rod ac . note also that the force of the rod bd is along its direction ( because it is limited by two joints at its ends and there is no force in between ) .
i believe you have a mistake in your formula as the self-inductance of a coil is given by $$l\approx\mu_0 \frac{n^2 a}{\ell} ; $$ here $n$ is the number of windings , $a$ is area of the cross-section , and $\ell$ is the length of the coil . your task is to maximize $l$ with the constraint that the length of the copper wire is $w$ . assuming that the solenoid is a cylinder , the cross-section read $a=\pi r^2$ with $r$ the radius of the cylinder . a solenoid with $n$ windings needs a wire of length $w= 2\pi rn$ . thus , $$ l \approx \mu_0 \frac{w^2}{\ell} . $$ we see that the inductance of the solenoid decreases with increasing length ( keeping the total length of the wire fixed ) . thus , we obtain the largest self-inductance having the smallest length which is a single loop with $n=1$ . for a single loop the formula given above is not correct ( as it assumes $\ell \gg \sqrt{a}$ ) and thus we have $$l\approx \mu_0 r \ln ( r/r ) \approx \mu_0 \frac{w}{2\pi} \ln ( w/r ) $$ with $r$ the radius of the wire .
use wheatstone bridge : http://en.wikipedia.org/wiki/wheatstone_bridge this is classical way of measuring resistivities with no error due to imperfectness of measurement instruments ( e . g . galvanometer ) .
even considering the same fabric with different colours it will depend a lot on optical properties of the dye which we cannot tell only by the colour that we can see . i would need to know " how much the silver paint would reflect and in what spectral range " and also " how is the absorption coefficient as a function of the wavelength ( from near-uv , visible to near-infrared ) " . i believe that this is the most important point and not the colour that we perceive with our eyes . nevertheless , keeping things simple , i think that the reflective coating being outside or inside would make the same amount of radiation that passes through . the only difference is that putting the reflective layer inside you would be increasing the path length of the radiation , thus increasing the absorption , and therefore the temperature edit : in order to clarify my idea i added this picture . dashed layer is reflective and the the grey layer is the normal dark layer . i would like to divide this discussion in two : " radiation protection " and " heat protection " regarding the radiation protection , i believe that it depends on how the the dark layer absorbs light in all spectral range of the sun light , i.e. absorption coefficient and thickness of the material+dye . it also depends on how the reflective layer reflects the light , i.e. reflectance . i believe that the radiation protection does not depend on the arrangement , so both schemes protect from radiation as good . ( i am not considering far infrared radiation ) regarding heat protection , the thought is the same , except we should consider that in the first case the length of the radiation in the dark layer is longer , thus absorbing more radiation thus heating more . also , the protection from the heat will depend on the heat conductivity of the layers and surface morphology which will affect how well the layer will cool down . i understand that having a reflective coating outside an umbrella would be unpleasant for other people . but it would be the best choice for heat protection . however , regarding the emergency heat blankets , i do not know why they also have a reflective layer on the inside .
you are quite right that einstein gravity is not renormalizable by powercounting . be careful though , this is not a rigorous proof , it is a mere estimation . in fact there is not proof to this date which once and for all proves that gravity is really not renormalizable . if you think in terms of feynman diagrams ( which are a nightmare for einstein gravity ) , there might be non-trivial cancellations hidden within the sum of graph which tame divergences . it might also be that the potential counterterms are related by some non-obvious symmetry , so that in the end only a finite number of field redefinitions is necessary to get rid of the divergences -- or in other words that a sensible implementation of renormalization is possible . in fact , the question about uv finiteness is currently being addressed by zvi bern and friends who could show using sophisticated techniques that maximally supersymmetric quantum gravity is much less divergent than one would naively think . the buzzwords here are color-kinematics duality and the double copy construction which basically says that a gravity scattering amplitude is in some sense the square of a gauge theory amplitude . check the arxiv , there is a plethora about this . now , regarding powercounting the reasoning is roughly as follows : the eh action is basically $$\mathcal{l} = \frac{1}{\kappa} \int d^4x \sqrt{-g}r $$ with $g$ the determinant of the spacetime metric $g^{\mu\nu}$ . the mass dimension of the ricci scalar $r$ is $ [ m^2 ] $ , that of the integral measure $ [ m^{-4} ] $ , i.e. in order for the whole expression to be dimensionless $\kappa$ has to have mass dimension $ [ m^{-2} ] $ . if you now do a perturbative expansion around a flat background of the metric , you will encounter at each step more and more powers of one over $\kappa$ . graphically , this expansion is an expansion in numbers of loops in feynman diagrams . at each step , i.e. at each loop level the whole expression should be dimensionless , i.e. at each step you need more and more powers of loop momentum ( at each loop level two more powers , to be precise ) , s.t. in the end your expressions become the more divergent the higher you go in the perturtabive expansion . in order to cancel these ever sickening divergences you had have to introduce an infinite number of counterterms which -- in terms of renormalization -- makes no sense , hence this theory is by powercouting non-renormalizable . there are nice lecture notes about this , cf http://arxiv.org/abs/1005.2703 ( notes by lance dixon about supergravity but the introductory bit is quite general ) .
surely . lets consider scattering of a 1-d particle on a small potential barrier . to solve the problem , we will find energy eigenstates : $$ h|\psi\rangle=e|\psi\rangle $$ set $h=p^2/2m+\epsilon u=h_0+\epsilon u$ , where $\epsilon$ is a small parameter . consider the equation $$ ( h_0-e ) |\psi\rangle=|\phi\rangle $$ let us write a solution as $$ |\psi\rangle=|\psi_0\rangle +g_0 ( e ) |\phi\rangle $$ where $|\psi_0\rangle$ lies in $e$-eigenspace of $h_0$ and $g_0 ( e ) $ is the operator with kernel ( in coordinate rep ) being the casual green function of $h_0-e$ . now we write the original problem : $$ ( h_0-e ) |\psi\rangle=\epsilon u|\psi\rangle$$ so $$ |\psi\rangle=|\psi_0\rangle +\epsilon g_0 ( e ) u|\psi\rangle $$ we want this to reduce to a free particle moving from left to right for $\epsilon=0$ , so we write in the coordinate representation $|\psi_0\rangle=\exp ( ip_ex ) , \ , p_e^2/2m=e , p_e&gt ; 0$ , and the equation : $$ \psi ( x ) =\exp ( ip_ex ) +\epsilon\int_{-\infty}^x g_0 ( e , x-x' ) u ( x' ) \psi ( x' ) dx ' $$ this is just what you want , with $k ( x , y ) =g_0 ( e , x-y ) u ( y ) , \lambda=\epsilon$ , it is a volterra second kind integral equation , and partial sums of liouville-neuman series give a perturbative solution to 1-d scattering .
the reason is that the constant time slice ends on the cosmological horizon . if you look at desitter space in t , r coordinates , the metric is ( in appropriate units ) : $$ ds^2 = - ( 1-r^2 ) dt^2 + {1\over 1-r^2} dr^2 + r^2 ( d\theta^2 + \sin^2\theta d\phi^2 ) $$ the time coordinate freezes at r=1 , so that the entire history of the region is contained in $0&lt ; r&lt ; 1$ . this is the reason for the finite volume . the distance between the spheres at $r$ and $r+dr$ is read off from the metric : $$ ds = {dr\over \sqrt{1-r^2}}$$ and this has an inverse square-root singularity at r=1 , which has a finite integral . so if you integrate for the volume , you multiply the radial distance by the area of the sphere at radius r , or $4\pi r^2$ , to find $$ v = \int_0^1 {4\pi r^2 \over \sqrt{1-r^2}} = \pi^2 $$ to do the integral , change variables to $u^2=1-r^2$ and it becomes $4\pi$ times the area of a quarter-circle of unit radius . the result is finite because you are looking at a causal patch of desitter space . the geodesics separate exponentially due to the positive curvature ( in euclidean positively curved space , they converge , but this is minkowski signature ) . this means that the full space disconnects into regions which are not in causal contact with each other . each such region is finite volume . when you slice at constant r , you produce an infinite cylinder . this is not a contradiction . the reason this is counterintuitive is because the t-coordinate is bad at r=1 . the killing vector defining t becomes null at a finite distance away from the origin . the finite volume is the volume of the region where the t-coordinate is timelike and a killing vector , it is not the full volume of the space ( in the gr maximally extended way of looking at things ) . the euclidean continuation of desitter space is just the 4-sphere . this comes from changing the sign of the dt term in the metric . the physics of desitter space is best thought of as a path integral on the sphere , not in some sort of maximal extension . this is a point of contention , as a lot of people do not like cutting off the universe at the visible universe , but this is the way suggested by the holographic principle . the periodicity of the desitter space sphere in imaginary time tells you the hawking temperature of the cosmological horizon , and the finite volume of the t=0 slice just becomes the volume of the 3-sphere section of the 4-sphere at imaginary time 0 .
your question is a classical college exercise . the limit is supposed to be the melting of the base of the artificial mountain under the pressure , which is linked with the energy of chemical bounds . you have an example of such a calculation here . you can also look it for an arbitrary planet size : the smaller the planet , the smaller the gravity is , and the bigger the biggest mountain can be . and when the mountain can be as big as the radius of the planet , you have roughly the dwarf planet / asteroid boundary .
try the following two thought experiments . eliminate the branch of the circuit containing y . you now have three resistors : the internal resistance of the battery ( $5\omega$ ) , and the two given resistors . whatever current you determine to flow through the circuit , it must be the same through all three of those resistors . because of that , the voltage drop across the $60\omega$ resistor has to be exactly twice the resistance drop across the $30 \omega$ resistor . eliminate the branch of the circuit containing x . all of the above still holds , except that the ordering of the resistors is different . now reassemble the circuit . because the two branches have the same total resistance ( $30 \omega + 60 \omega = 60 \omega + 30 \omega = 90 \omega$ ) , they have the same current running through them . if you change the values of the resistors , then the current might not be split equally between the two branches . but the voltage drop across the two branches will always be equal , regardless of what the total current is or how it is split between the two branches . and the voltage drop within a given branch will always be divided between that branch 's resistors proportionally to their resistance values .
i have confirmed with zurek , he told me that it was wrong ( at least the period-wise ) and it has been pointed out many times by other people including animesh dutta .
when they mention that speed they are speaking of the minimum speed he will need to run in order to keep contact with the loop . i believe they calculated that number simply as a reference point so that 1 ) they knew looping the loop was theoretically possible , and 2 ) so he had an estimate of about how fast he needed to run . imagine trying this stunt all day just to find later that in order to run around the loop you would have needed to achieve a speed of $50 {km \over h}$ for the stunt to be even theoretically possible . in fact , if you watch the video you can tell that he does run faster than what he theoretically needed to in order to run safely around the loop . theoretically , the minimum velocity needed at the apex of the loop is the velocity which will result in a normal force of zero at that point . this means at the very top of the loop his feet would not be exerting any force upon the loop at all ( meaning he would just barely make it through the apex without falling ) . you can see from the video that this is not the case . if you watch the slow motion clip his foot clearly presses against the track very near to the apex of the loop , meaning he must have been traveling faster than the theoretical minimum velocity . so , in answer to your question , yes , running as fast as he could would have worked just fine , since that would clearly have been faster than the minimum velocity . of course , the faster he runs the more leg strength he would need at the apex to resist the normal force of the track pushing down on him .
this actually comes down to a question about the interpretation of units , and it is kind of a tricky issue . on one hand , you can view any equation in physics as nothing more than a mathematical relationship between some numbers . this was the view taken in the early days of quantitative physics , * back in the 17th and 18th centuries , when the concepts of force and momentum were just being quantified . since there was very little in the way of collaboration , the idea of a standardized unit system had not really taken off , so if you were doing an experiment to establish the relationship between force , momentum , and time , the units you used would have been determined by your equipment . in other words , all you would know is that you have a force meter ( scale ) which will give you a number proportional to the force , a " momentum meter " which will give you a number proportional to the momentum change , and a clock of some sort ( perhaps a pendulum ) which will give you a number proportional to the time . let 's say it is been established that the relationship is linear . you would probably run an experiment in which you apply a certain quantity of force for a given number of ticks of the clock , and change in momentum off your measuring device . then you would plug those numbers - it is important to notice that you are only dealing with numbers , since there are really no meaningful units to speak of - into the discrete approximation of newton 's second law : $$f^{ ( n ) } = k_f^{ ( n ) }\frac{\delta p^{ ( n ) }}{\delta t^{ ( n ) }}$$ here i have used the superscript $^{ ( n ) }$ to indicate pure numerical value , i.e. the number you read off the scale/clock/meter . plugging in these numbers will allow you to determine the value of the constant $k_f^{ ( n ) }$ . hopefully it is obvious that the value of this constant will depend on how your equipment is calibrated , or in other words , which unit system you are using . if hypothetical-experimenter you decided to label your unit of force the pound $\mathrm{lb_f}$ , your unit of momentum $\mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-1}$ , and your unit of time the second , you would find that $$k_f^{ ( n ) } = 32.174$$ back in those days , before people started really thinking about units , all multiplicative equations in physics were thought of as simple relationships between numbers . accordingly , they included constants of proportionality which were customized to each lab 's equipment . eventually , as more people started doing physics , there arose a need for standardized unit system so you could compare data from different labs . then you could write the above equation as $$\frac{f}{\mathrm{lb_f}} = \frac{k_f}{\mathrm{lb_f}/ ( \mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-2} ) }\frac{\bigl ( \frac{\delta p}{\mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-1}}\bigr ) }{\bigl ( \frac{\delta t}{\mathrm{s}}\bigr ) }$$ because the numerical value of a measurement is just the measurement divided by its unit . you can algebraically rearrange this to $$f = k_f\frac{\delta p}{\delta t}$$ where $$k_f = k_f^{ ( n ) }\frac{\mathrm{lb_f}}{\mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-2}}$$ so newton 's second law has shifted from being a simple relationship between numbers , to being a relationship between physical quantities which are expressed as multiples of a reference value . however , you still have a conversion constant in the equation . symbolically , it is independent of units , but you still do have to plug in a different number depending on which combination of units you want to work with . this is the sense in which $f$ is only proportional to $\frac{\mathrm{d}p}{\mathrm{d}t}$ . in the modern scientific community , on the other hand , i think most people would agree that units are a human invention , and that physical quantities should exist in some sense independently of the units we choose to use for them . taking that view , there should be some " natural " way to express the equations of physics that does not incorporate any " unit system artifacts " like these proportionality constants . the way we do this is to define the units as abstract objects and develop a set of rules for manipulating them ( kind of like unit vectors ) . we can then incorporate the conversion constants into those rules . for example , let 's again consider the discrete approximation of newton 's second law , but this time without the conversion constant written into it : $$f = \frac{\delta p}{\delta t}$$ you can still use seconds for time and $\mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-1}$ for momentum in this equation . when you read the numbers off your measuring equipment and plug them into the formula , you will do it like this : $$f = \frac{\delta p^{ ( n ) }\ \mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-1}}{\delta t^{ ( n ) }\ \mathrm{s}} = \frac{\delta p^{ ( n ) }}{\delta t^{ ( n ) }}\mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-2}$$ suppose you want your answer in pounds of force . you would look up the multiplication rule for $\mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-2} \to \mathrm{lb_f}$ , which in this case can be found on wikipedia : $$\mathrm{lb_f} = 32.174\ \mathrm{lb_m}\ ; \mathrm{ft}\ ; \mathrm{s}^{-2}$$ ( in general you might have to chain a few rules together to get the right conversion ) . so you wind up with $$f = \frac{1}{32.174}\frac{\delta p^{ ( n ) }}{\delta t^{ ( n ) }}\mathrm{lb_f}$$ it works out to the same thing as before , but this time the conversion constant $k_f$ is part of the unit system , not the equation . this means that if you are not plugging actual values into the equation , you do not have to think about units or proportionality constants at all . and if you look at it this way , $f$ is actually equal to $\frac{\mathrm{d}p}{\mathrm{d}t}$ . so what is the verdict ? unfortunately , there is no unassailable answer to this question of whether newton 's second law is a proportionality or an equality . depending on how you think about it , either answer could be valid . but i would say the " equality " answer , which corresponds to the modern view of units , is conceptually cleaner . it is accepted by all competent modern physicists , as far as i know ( for mechanics , at least ; electromagnetism is a whole different story ) , and it is certainly the interpretation we try ( however unsuccessfully ) to instill into introductory physics students ' minds . i would definitely agree that the examiner was being unreasonably picky ( though to be fair , he did give credit for it ) . *i do not have an explicit source , so i am not entirely sure this is the way things were really developed ; i am basing my description on some fuzzy memories . that being said , the backstory does help clarify the various ways in which we treat units , so consider it historical fiction if you must .
the oscillatory part is nothing but thomas-fermi approximation or more riguresly , this is a version ( someone should correct me if i am wrong ) weyl 's formula regrading on how to obtain the wkb from the trace formula : you can read the 2 papers by berry and tabor on how they derived a trace formula ( like that of gutzwiller ) but to the case of integrable systems . from the derivation there you can see how the ebk pop up . . .
the mass of a quantum of a field is defined from the second derivative of the potential term $$ m^2 = \left . \frac{\partial^2 v ( \phi ) }{\partial \phi^2} \right|_{\phi=0} $$ and similar for fields with spin ( fields that are not scalar fields ) . the general form of the potential – or the whole lagrangian – is always more complicated but only this leading term determines how non-interacting wave packets and the particles of the field propagate . the higher-order terms only affect the interactions . so the potential may be expanded as $$ v = v_0 + 0\phi +\frac{m^2}{2}\phi^2+ o ( \phi^3 ) $$ here , the constant term does not matter except for causing curvature in general relativity ( the cosmological constant ) . the linear term may be set to zero by redefining the field $\phi$ additively so that its vev is $\phi=0$ , the quadratic term is the first important nontrivial term , and the higher-order terms do not affect small " waves " i.e. masses of the particles at all because the equations of motion are of the form $$\box \phi =m^2 \phi + o ( \phi^2 ) $$ and the higher-order terms may be neglected for a small $\phi$ . the quantization of the field from which the particles ' masses may be extracted effectively deals with the infinitesimal values of $\phi$ , too . to calculate the leading non-trivial ( non-constant and nonzero ) term in the potential , it is enough to linearize the dependence of all other things on the fields at the relevant point . so there is no inaccuracy introduced whatsoever .
since all the balls are accelerating together , this problem is equivalent , by the non-relativistic equivalence principle , to the problem of balls moving without gravity , or on a horizontal surface , which are free to sort themselves out according to the same force law . this reduced problem is interesting and widely studied . depending on the force law , you can get either a 1-d integrable model , or a 1-d statistical equilibrium , and both have a massive literature .
answer is in link below . short version : refractive gradients in the mixed atmosphere , thousands of meters above the surface , are far too small to act as lenses and prisms , and scintillation is not a geometric optics phenomenon . it results from interference effects of plane-wave light ( spatially coherent light , like starlight or laser light ) as the wavefront is distorted by tiny refractive gradients associated with kolmogorov turbulence . this accounts for both the intensity and color fluctuations seen by eye , and also explains why the larger planets , whose light is far from coherent , do not scintillate . little 's paper anticipates by a decade the works of kolmogorov , tatarski and rytov . it also anticipates speckle , seen in larger apertures , where the distorted wavefront breaks up the airy disc into tiny pieces that fluctuate on millisecond time scales . http://adsabs.harvard.edu/full/1951mnras.111..289l
just because the maximum speed is $6\pi\text{ cm/s}$ does not mean that $6\pi = 6\pi \cos ( 3\pi t ) $ . keep in mind that speed is the absolute value of velocity $x&#39 ; $ .
one alkaline aa cell has about 11 kj of energy . for a laptop battery , it is 360 kj . chevrolet equinox fuel cell has 58 mj of energy . one kilogram of tnt carries about 4.184 mj of energy . divide the numbers from the previous paragraph by this constant to see that the aa cell , laptop battery , and electric car battery have 2.6 grams , 86 grams , or 14 kilograms of tnt . note that tnt usually releases all the energy abruptly . gunpowder has 3 mj per kg or so . it means you have to add about 35% to get the right estimate for the mass of equivalent gunpowder . if you could release the energy from the batteries very quickly , the explosion could be equally devastating as the corresponding gunpowder and tnt except that batteries can not release energy this quickly .
you have a typo that the amperes output is 4.936 , but that is corrected later on . you also slipped a decimal when you did 4.936 * 0.2 , which should give 0.9872 . the battery is at 130v , so the current out of the battery is $\frac {235}{130}\cdot 4.936=8.922$ and it is really a little higher for the losses in the conversion . let 's say it is 9a out of the battery . then you would have 1.8 a-hr in the battery to run for 12 minutes . i do not see where it says you can in fact run for an hour , so it looks to me like about 1.8 a-hr . if you can run for an hour , it would be 9 a-hr .
the entropy of the measured system decreases , at the expense of the entropy of the detector , which goes up . the total entrob=py balasnce is positive ( as irreversibly fixing a detection results usually costs more entropy than is gained from the reduction in the measured system . thus the second law is not violated . edit : about deriving such results : in a fully microscopic description the entropy remains constant , but nothing can be measured as there is no microscopic concept of a permantent record of measurement results . one therefore needs appropriate coarse graining assumptions , which take the place of boltzmann 's 19th century stosszahl ansatz for classical molecular systems . coarse graining means that the density matrix is restricted to take a form depending only on macroscopically measurable parameters , and deviations from this form ( due to the exact dynamics ) are swept under the coarse graining carpet . this leads to an approximate macroscopic dynamics . the resulting description is dissipative in the markovian limit : the entropy $tr ( -\rho\log\rho ) $ strictly increases with time unless the system is already in equilibrium . a book covering this nicely and in full detail for a number of coarse graining recipes is grabert 's ''projection operator techniques in nonequilibrium statistical mechanics'' . for a readable summary of the basic technique , see , e.g. , http://arxiv.org/pdf/cond-mat/9612129 .
by definition an orbit occurs when gravity balances with the " centrifugal " force . it is essentially a free fall situation . so the answer is the same reason why you do not get stuck to the ceiling of a free falling elevator . both the spacecraft and the occupants are moving in-sync .
no they will not . space is intrinsically isotropic , so assuming they are not aware of any specific reference points , and they are far enough away from a massive body as to experience an insignificant amount of gravity , there would be no way of knowing their orientation . gravity essentially provides observers with a force field that the body can utilise to establish orientation etc . hope this helps .
it is not really the legendre polynomials you should be calculating against , but more precisely the spherical harmonics $$y_{lm}=\sqrt{\frac{ ( 2l+1 ) ( l-m ) ! }{4\pi ( l+m ) ! }}p_l^m ( \cos ( \theta ) ) e^{im\phi} . $$ exactly what you integrate will depend on what exactly is the object you are studying , but the essentials will be the same . none of the coefficients by themselves will mean much , but in general bigger coefficients mean bigger anisotropies ; the different varieties of spherical harmonics represent different modes of anisotropy . for example , if the coefficients for $l=1$ are nonzero then it probably means you chose the wrong origin , and the cluster centre is in some particular direction . if the coefficients for $l=2$ are nonzero then you will have some sort of ellipsoidal distribution ; the moments then tell you if it is oblate or prolate , how much , and in which direction . ( this is a common problem in nuclear physics , where the quadrupolar moments of nuclei are essential properties . ) nonzero $l=3$ moments could imply some sort of triangular or tetrahedral structure , and so on . a good way to picture these anisotropies is to graph the surface $r=1+\epsilon y_{lm}$: you should also be careful because the different moments are not necessarily comparable , and it could be hard to tell which one is more important . an example of this is in electrostatics . say you have some localized density of electrical charge $\rho ( \mathbf{r} ) $ which you are trying to examine from far away . then you can decompose the far field into its multipolar components as $$ \phi ( \mathbf{r} ) =\sum_{l=0}^\infty\frac{4\pi}{2l+1}\sum_{m=-l}^l q_{lm}\frac{y_{lm} ( \theta , \phi ) }{r^{l+1}} , $$ where $$q_{lm}=\int\mathrm{d}\mathbf{r} \rho ( \mathbf{r} ) r^ly_{lm}^\ast ( \theta , \phi ) $$ are the multipole moments . these measure the different anisotropies of the distribution and imprint it on the different anisotropic components of the far field . however , their relative importance changes with the distance from the distribution , because the different anisotropic ( multipolar ) components decay at different rates . thus the ratio between different coefficients essentially tells you the distance you need to be at for the more anisotropic component to be negligible . in your case , i gather you want to study the cluster boundary as a surface $r=r ( \theta , \phi ) $ . my first shot at that would be to attempt a decomposition of the form $$r ( \theta , \phi ) =\sum_{l=0}^\infty\sum_{m=-l}^l r_{lm}y_{lm} ( \theta , \phi ) , $$ where you can find the coefficients $r_{lm}$ using the orthogonality properties of the harmonics . here the $r_{lm}$ are all distances and are thus comparable , so that if some coefficient is much smaller than another you can probably ignore it . each coefficient then tells you how much of some particular type of anisotropy ( multipolarity ) your surface has . you should be careful when judging them as absolutes because they are distances and therefore carry dimensional information ; to remove that you could try dividing by $r_{00}$ , which is up to constants the radius of the cluster . a useful measure of total anisotropy could then be $$\sum_{l\neq0}\left|\frac{r_{lm}}{r_{00}}\right| , $$ or squaring each term , or some such .
i am not sure what you mean by medium here , but i believe i can still provide an answer to your question . a fuel-thruster works by pushing the fuels reactants back and thus pushing the thruster forward . a human cannot swim in the vacuum because there arms do not push anything back . in water , a human can swim by pushing water back and thus pushing the human forward . by your use of words , i guess you can consider the " fuel " in this case as a " medium " for the reactive force . in answer to your first question , which i interpret as " does a force require both something that caused the force and something that receives the force " , the answer is yes . however , we can write quantities , such as the potential , that only depend on a source .
it is easy to check if your calculations are right from this ( rather crude ) drawing : there you see that the size of an object is related to the size of the image according to $$h_o=h_i\frac{d}{f}$$ where $h_o$ and $h_i$ are the sizes of the object and image . $f$ is not a focal distance , but the distance from the pinhole to the sensor or film . so , if you have a 10cm object at 10m from the camera , and you want that to be 0.5mm ( 50px width ) , you need to place the sensor 0.5mm*10m/10cm=5cm behind the pinhole . if you do that , the angle your camera will see is $\tan ( \alpha ) =h_\mathrm{ccd}/ ( 2f ) $ . the field of view is defined as twice this angle ( $\mathrm{fov}=2\alpha$ ) . with the above numbers , the vertical fov is 4.1º and the horizontal fov is 5.5º .
first problem : you say $v ( t ) = a x^2$ , but that is a function of position , not time . putting the definition right : $$ v = \frac{dx}{dt} = a x^2 $$ you can regroup terms on the same variable : $$ \frac{dx}{x^2} = a dt$$ and then do the integration : $$ \int \frac{dx}{x^2} = \int a dt$$ this is homework , so i will leave the integral limits and the following details to you , but i think this should clarify it enough . the key to your mistake is that you cannot simply do $\int x dt$ , because $x$ is a function of $t$ , but you do not know which one .
no . i can take a ball and swing it back and forth periodically with my hand . the motion is periodic , but the situation is not conservative - my body generates a lot of heat . a simple mathematical example is a forced , damped harmonic oscillator . it has a steady-state periodic solution that dissipates energy . if you want to know whether a force field is conservative , take its curl . time-independent force fields ( force is a function of position but not time ) are conservative iff their curl is zero .
the main idea behind polaroid sunglasses is that reflexion from water , snow and other glary reflectors is mainly polarized in one direction . to understand this , witness the behaviour foretold by the fresnel equations ( the graph below taken from the wikipedia " fresnel equations " page ) : so that you can see for a wide range of scattering angles from these surfaces , the reflected light reaching your eyes is mainly in the $s$-polarized direction ( electric field vector orthogonal ( "senkrecht " in german ) with the plane of polarization ) , so if you quell this polarization , you get rid of most of the glare from these surfaces . why are your lenses twenty degrees off in their polarization axes ? i would say that this is a simple question of production economics . the power through a polaroid varies like $ ( \sin \theta ) ^2$ , where $\theta$ is the angle between the actual polarizing axes and their ideal directions for quelling a given linear polarization . this functional dependence is very flat for a wide angle range around the null , so , if there is a twenty degree error , the attenuation ratio is still 0.1 . so a polarizer that is twenty degrees off is still almost as good as an ideally aligned one for the lower-the-glare-in-human-sight application . therefore , a manufacturer simply will not go to the extra cost of the quality control needed to align the polarisers more accurately : it really would not make the product any better for the application at hand .
there are several reasons for using the hamiltonian formalism : 1 ) statistical physics . the standard thermal states weight pure states according to $$\text{prob} ( \text{state} ) \propto e^{-h ( \text{state} ) /k_bt}$$ so you need to understand hamiltonians to do stat mech in real generality . 2 ) geometrical prettiness . hamilton 's equations say that flowing in time is equivalent to flowing along a vector field on phase space . this gives a nice geometrical picture for how time evolution works in such systems . people use this framework a lot in dynamical systems , where they study questions like ' is the time evolution chaotic ? ' . 3 ) generalization to quantum physics . the basic formalism of quantum mechanics ( states and observables ) is an obvious generalization of the hamiltonian formalism . it is less obvious how it is connected to the lagrangian formalism , and way less obvious how it is connected to the newtonian formalism . [ edit to answer javier 's comment ] this might be too brief , but the basic story goes as follows : in hamiltonian mechanics , observables are elements of a commutative algebra which carries a poisson bracket $\{ , \}$ . the algebra of observables has a distinguished element , the hamiltonian , which defines the time evolution via $d\mathcal{o}/dt = \{h , \mathcal{o}\}$ . thermal states are simply linear functions on this algebra . ( the observables are realized as functions on the phase space , and the bracket comes from the symplectic structure there . but the algebra of obserbables is what really matters : you can recover the phase space from the algebra of functions . ) on the other hand , in quantum physics , we have an algebra of observables which is not commutative . but it still has a bracket $\{ , \} = \frac{i}{\hbar} [ , ] $ ( the commutator ) , and it still gets its time evolution from a distinguished element $h$ , via $d\mathcal{o}/dt = \{h , \mathcal{o}\}$ . likewise , thermal states are still linear functionals on the algebra .
we have to differentiate between radiant flux and luminous flux . the former refers to power carried by all emitted photons ( watts ) while the latter refers to brightness as perceived by the human eye ( lumens ) . with that out of the way it is clear that by using filters to change the perceived color is equivalent to wasting all the luminous flux associated with the unwanted wavelengths as filters work by absorption . so if there was a way to produce only the desired wavelengths so that filters are unnecessary then for the same amount of power in , there is more useful power out . the problem is that filament based lighting produce a continuous spectrum of wavelengths and must be filtered to produce specific colors . fluorescents and leds actually produce at least 3 individual colors from phosphors/dopants respectively . this is why the latter 2 technologies tend to be more efficient--they do not have to produce true whites , do not produce significant ir , and yet their light is still perceived as white . of course , if we obtain single phosphor fluorescents or single die led , these will always be more energy efficient than to use 3 phosphors only to filter 2 out . additionally , luminous flux pertains to the biology of human eyes , which have been estimated to be most sensitive to 555nm ( green ) and taper off at red and blue . this means a pure red lamp producing 5w radiant flux of 600nm light does not appear as bright as 5w radiant flux of 555nm light .
the major problem with ultrasound as a mechanism of purification is that it does not break molecules . heat at least denatures proteins and breaks hydrogen bonds , but ultrasound is of a just smaller order of magnitude of energy at the atomic scale , which can be of the order of the adhesive forces holding the liquid together , but not of stronger molecular bonds . but i think you can do it a different way : use a sound waves intensity gradient to move the biological impurities in the water to a part of the container , by having them walk down an effective potential gradient , like optical tweezers move molecules . this requires only that the density/stiffness of the molecules be different from water , so that the sound energy at a given mode is different inside the molecules than in the water . if it is greater , the molecule will move to the regions of greater intensity . if it is less , it will move towards the regions of smaller intensity . by arranging the sound wave to have an intensity gradient , you can make all the molecules segregate towards/away from the microphone , leaving water in the middle with only ionic or small molecule impurities , which are not affected by the sound . you can flush the sides away , and repeat to make a purer water . this might work for getting rid of prions , which are not disinfected by boiling . this article is the only thing i found that might be relevant , but it is paywalled : http://www.annualreviews.org/doi/pdf/10.1146/annurev.bb.20.060191.001541 i think this might be a very useful idea .
i work with stellar models , so i thought i would chip in here . my instant reaction is that you should not worry too much : determining the age of a star is difficult and different models will disagree ( sometimes significantly ! ) on that age . how reliable is this research ? i can not see an obvious reason to doubt the conclusion . what method do they use to measure the age of such a star as methuselah ? basically , one tries to measure as many properties about the star as accurately as possible , and then find the best fitting stellar model . these models are solutions to a set of differential equations ( in time and one spatial dimension ) that tries to capture all the relevant physics that determines how stars evolve . the bulk physics is a fairly well-defined problem but there are several potentially important components that are lacking in these models . ( i will expand on this if desired . . . ) the usual difficulty here is breaking down the degeneracy between brightness and distance . that is , a distant object is fainter , so it is hard to know whether a certain object is intrinsically faint or just further away . the principal result in this paper is the hubble-based parallax measurement , which makes a big improvement on that distance measurement and , therefore , the brightness of the star . the other things they use are proxies for the surface composition and the effective temperature of the star , as far as i can see . incidentally , this is where i would suspect the tension can be resolved . if you look at fig . 1 of the paper , they show the evolution of different stars for different compositions . what you are looking for , roughly speaking , is lines that go through the observed points . that figure shows that if the oxygen content is underestimated , then the best fit is actually about 13.3 gyr , which is no longer at odds with the age of the universe . take note of table 1 , where the sources of error ( at 1$\sigma$ ) are listed . it is interesting that , not only is the star 's oxygen content the largest source of error , but even the uncertainty of the oxygen content of the sun is a contributor ! which is more likely to be wrong , the age of methuselah or the current estimate of the age of the universe ? the age of methuselah , definitely . i would describe our estimates of the age of the universe as in some way " converegent": different methods point to consistent numbers . sure , planck shifted the goalpost by 80 myr or so , but it would be a real shock to see that number change by , say , half a billion years . could relativistic effects account for some of the age ? i have no idea and have not really thought about it . since i am pretty sure this is not a big problem , i do not think relativistic effects are necessary to explain the discrepancy .
yeah , susskind has done a load of quantum mechanics videos aimed at f***wits like me and my peers : lecture 1 | modern physics : quantum mechanics ( stanford ) http://www.youtube.com/watch?v=2h1e3yjmkfa
the hup holds for elementary particle frameworks . a lamda goes into a pion and a proton , and when we calculate the rest frame we never define an ( x , y , z , t ) , we are interested in the momentum and energy four vectors of the produced particles . when we say a kaon hit a proton in the target , and assume the target at rest , the magnitudes of spatial uncertainty of the target , microns in this picture for the target and velocity the proton has due to the temperature of the target , plus the measurement errors on the momentum of the kaon , a few mev/c , are below our experimental discrimination and fulfill inevitably the hup .
i am not a physicist either . as i understand it , heat can be lost by conduction , by convection and by radiation , the purpose of the bottle is to reduce all three . if you half the amount of liquid , the question is whether you also half the loss of heat , or do more or less . analysis is difficult because the weak part of the bottle is the cork . if it is full , there is hot liquid near the cork that looses heat faster , and then gets conduction and convection heat fron the rest . there is also a lesser problem with the bottom , since it is an additional surface where heat can be lost . when the bottle is half full , the liquid is further away from the cork . but the air inside will conduct some of the heat ( conduction , and convection ) to the empty part of the bottle , and radiation may internally add some . if the empty part became as hot as the liquid , the heat loss would be the same as before , for a lesser mass of liquid . hence it would cool faster , if it does not get as hot , it means that some heat is lost to keep it cooler . if the bottle were homogenous ( no cork effect , no bottom effect ) , that would mean that , in addition to its normal heat loss through the side , the remaining liquid has to provide for the heat loss in the empty space above it . hence it cools down faster . the bottom is a disadvantage for the half full bottle , since its loss is the same in both case , and thus contributes comparatively more to cooling when the liquid mass is lower . now , i would need more data and/ or knowledge to analyse the effect of the cork . with a very conducting cork , the full bottle would loose heat quickly ( assuming the liquid touches it ) through convection and conduction in the liquid . with a totally insulating cork , it would at worse balance the effect of the bottom of the bottle , so that the analysis without cork or bottom would be valid . so with a reasonnably good cork , my conclusion is that a half full bottle will cool faster .
it sounds like you may have been confused because intuitively , you would think a reaction occurs after the action , in response to it . as you have found , that is not what newton 's third law is saying . the reaction force is not a response to the action . for this reason , some people do not like the statement " for every action , there is an equal and opposite reaction . " a better statement of newton 's third law is to say that forces always occur in pairs . it is impossible for object a to exert a force on object b without object b also exerting a force on object a . the two forces are simultaneous , of equal magnitude , and in opposite directions . mathematically , this is stated as $$\mathbf{f}_{ab} = -\mathbf{f}_{ba}$$
this is cool because $e=mc^2$ can act as some sort of uncertainty relation ; if you have a population of photons with energy $e$ , they are engendered with a mass $\frac{e}{c^2}$ , no matter what my intuition says . ( is that right ? ) no , it is not quite right . in relativity , it turns out that energy and momentum are parts of a single four-dimensional vector , imaginatively called " four-momentum " , and mass is simply the length of that vector . the general relationship is $ ( mc^2 ) ^2 = e^2 - ( pc ) ^2$ , which simplifies to $e = mc^2$ just in the case of zero momentum . thus , if you have photons in one direction , then each of them have energy $e = pc$ , and there is no mass . but if you have photons in opposite directions but with equal energy , then the total momentum is zero , and you have have total mass . intuitively , think of a box of negligible mass , and fill it with lots of photons with zero total momentum . if you try to accelerate the box in some direction , then the photons coming to one side will be redshifted , thus impinging less momentum on that side , and on the other side will be blueshifted instead . thus , accelerating it will require a force , so we can measure the positive mass of the filled box ( more than an empty one ) . however , your main mistake is that you are thinking that mass is the gravitational charge . it is not , despite newtonian thinking--it is actually energy . it is just every mass has energy $mc^2$ and this usually dominates all other forms of energy under normal conditions . because the gravitational field is spin-2 , not just energy is important , however--see stress-energy tensor . so my real question is , in any model of particle interactions , can a photon directly generate a gravitational field ? for example , something like a feynman diagram with a photon and a massive particle linked with a graviton . the einstein field equation is completely classical , so it does not even mention gravitons . but a photon can generate a gravitational field because it has energy , which is the gravitational charge . this is similar to how maxwell 's equations do not care about where the electrical charge comes from , whether it is continuous or made of discrete particles , or what kind of exchange interactions are going on under the hood--it just says that if you have such-and-such charge distribution , then the electromagnetic field will be related to it in such-and-such way .
the field inside the sphere will not be zero if it is hollow and there is a point charge in the hollowed out part . the field will be zero in the conductor , because the field is always zero in a conductor in electrostatics . what you might be refering to is that the field will be zero inside the hollow sphere if it is charged , because the charges will distribute symmetrically over the sphere .
joule dissipation ( equivalently , ohmic heating ) is a statistical process . it does not occur at the microscopic scale , and a single travelling electron in an electrical field is certainly microscopic . resistivity depends on particle collisions , which turns translational kinetic energy into thermal kinetic energy . ohm 's law relates the current $\vec{j}$ in a medium to the electric field $\vec{e}$ via the conductivity $\sigma$ , $$ \vec{j}=\sigma \vec{e} $$ another way of looking at this is that dissipation $p$ is given by $$p=\frac{j^2}{\sigma}$$ written this way , we have an expression very similar to that for ohmic heating in a resistor , namely that $$p=i^2 r$$ since the conductivity is the inverse of the resistivity , this is an intuitive result . the advantage of joule 's expression is that it allows for the determination of joule heating at a particular point in space , rather than over the entire resistor ( whatever that may be ) . this is useful in plasma physics , for example , where it may be the case that joule heating is localized , rather than uniform throughout the plasma . nonetheless , it is predicated on the medium being strongly collisional , and still must refer to a macroscopic , rather than microscopic effect .
it looks like p and s have been a little bit dirty at this point . go back to equation 4.29 . . . you really need to take the limit $t\to \infty ( 1-i\epsilon ) $ of this expression . physically what is happening is that the true vacuum $|\omega\rangle$ is not the perturbative vacuum $|0\rangle$ . you are extracting the true vacuum contribution by evolving to large imaginary times . equality only holds in the limit . your first term is an increasing phase divided by $t$ so it goes like a constant , but the second term is a $t$ independent constant divided by $t$ so it disappears .
it depends on what you mean by completely . ab initio calculations of ferromagnetism are routinely done for a wide range of systems . googling for ab initio calculation of ferromagnetism or some similar terms will find you lots and lots of examples . so in this sense the answer to your question is yes . however in many materials whether the system is ferromagnetic or not depends on a delicate balance between the electron magnetic interactions and the exchange interaction , and when the system is finely balanced it would require an extremely accurate calculation to get the correct answer . for example iron ( which is the archetypical ferromagnet ) exists in three slightly different crystal structures called ferritic , martensitic and austenitic . the ferritic and martensitic are ferromagnetic but the austenitic form is not . so the same element can switch between a ferromagnet and paramagnet just by changing it is crystal structure . i am not sure whether the theoreticians can honestly say they can predict the correct behaviour for these borderline cases .
the parameters $g^2$ and $n$ are independent of each other so it is meaningless to ask whether $g^2$ goes like $1/n$ for large or small coupling " in general " . in general , it may go but it does not have to go . but if it does go , i.e. if $g^2 n$ is kept fixed , then one may say new interesting things . if one introduces a new symbol $\lambda = g^2 n$ for the ' t hooft coupling , the condition simply says that $\lambda$ is kept fixed – which is natural , especially in the dual stringy interpretation of the same physics . i need to dedicate a special paragraph to an error in your question which is probably not just a typo , due to the complicated work you had to go through to write the fractions . what is kept fixed in ' t hooft 's limit is not $g^2/n$ ; it is $g^2 n$ . it is the product , not the ratio ! so $g^2$ indeed goes like $1/n$ and not $n$ if $n$ is sent to infinity .
looks like neutrons can cause visual perception : visual phenomena noted by human subjects on exposure to neutrons of energies less than 25 million electron volts , science . 1971 may 21 ; 172 ( 3985 ) :868-70 , " six subjects reported multiple starlike flashes and short streaks on exposure to neutrons of energies up to 25 million electron volts . the probable mechanism is interaction with the retinal rods by proton recoils and by alpha particles released from neutron reactions with carbon and oxygen . these observations are similar to light flashes and streaks seen by astronauts who are exposed to high-energy cosmic rays on translunar flight . "
i think you generally have the wrong formula : $t = \frac{1}{2}m\vec{a}\cdot\vec{a}+\frac{1}{2}\vec{\omega}\cdot\vec{\omega}i+m\vec{a}\cdot ( \vec{\omega}\times\vec{r} ) $ , where $\vec{a}$ is the linear velocity of the point you are calculating i around . so for the point of contact of a rotating wheel , you have $\vec{a}=0$ , as the instantaneous linear velocity of the point of contact is zero , $t= \frac{1}{2}m\vec{a}\cdot\vec{a}+\frac{1}{2}\vec{\omega}\cdot\vec{\omega}i+m\vec{a}\cdot ( \vec{\omega}\times\vec{r} ) = 0 + \frac{1}{2}\vec{\omega}\cdot\vec{\omega}i + 0 = \frac{1}{2}\omega^2i$ which should give you the same answer as before .
no , elements of $spin ( n ) $ do not obey the clifford algebra . instead , it is the gamma matrices that obey it . and no , the commutator of the $spin ( n ) $ lie algebra is not the commutators of the elements of the group but elements of the lie algebra . now positively . the spinor representation is the representation on which the generators $j_{ij}$ ( the basis of the lie algebra ) act , $s\mapsto j_{ij}\cdot s$ . the elements of the $spin ( n ) $ group may be obtained by exponentiation : $$ g = \exp ( \sum_{i , j} i\omega_{ij}j_{ij} ) $$ where $j_{ij}$ is the basis of the lie algebra . while in the vector representation , $j_{ij}$ is given by a nearly vanishing $n\times n$ matrix with entries $\pm i$ on the $i , j$ and $j , i$ position , respectively , the matrices $j_{ij}$ have a completely different form in the spinor representation of $spin ( n ) $ . they may be written as $$ j_{ij} = \frac{\gamma_i \gamma_j - \gamma_j \gamma_i}{4} $$ where $\gamma_i$ are gamma matrices that do obey the clifford algebra $$ \gamma_i \gamma_j + \gamma_j \gamma_i = 2\delta_{ij}\cdot {\bf 1}$$ so the matrices obeying this algebra may be combined to bilinear expressions , the antisymmetric tensor $j_{ij}$ with two indices , and these $j_{ij}$ obey the $spin ( n ) $ lie algebra , and as with every lie algebra , the elements of the lie groups may be obtained by exponentiating combinations of the lie algebra matrices . ( equivalently , the lie algebra is the tangent space of the lie group manifold in the vicinity of the unit element of the group . )
you just use the lens formula : $$ \frac{1}{u} + \frac{1}{v} = \frac{1}{f} $$ do the calculation for the first lens to find the position of the image , then do the calculation for the second lens taking $u$ to the distance of the first image from the second lens . make $u$ negative if the image is on the far side of the second lens .
you are right about your understanding of these terms . this terminology appears in extensions of the randall-sundrum type brane world models . the original model contains a single compact extra dimension bounded by two branes and is known as a hard wall model with the " hard wall " referring to the hard cutoff of space by the ir brane . with such a geometry it is found that the kaluza klein ( kk ) masses of particles that live in the bulk scale as $m_n^2 \sim n^2$ ( like the energy levels of a particle in a box ) . attempts were made to use rs type setups to be dual to qcd in order to calculate meson masses etc . this is known as ads/qcd . however the meson mass spectrum is what is called a regge spectrum i.e. $m_n^2 \sim n$ and so the rs type model needed to be adapted . this paper first introduced the idea of a soft wall to solve this problem . one of the branes in the hard wall model is removed and a dilaton field $\phi$ is introduced which dynamically cuts off the space-time $$s= \int d^5x \ , \sqrt{g}\ , e^{-\phi}\mathcal{l} . $$ the profile of the dilaton in the extra dimension then determines the kk spectrum of bulk fields and for a quadratic dilaton profile ( $\phi ( z ) \sim z^2$ ) a regge spectrum is produced . the removal of one of the branes ( hard spacetime cutoff ) and replacement by a smooth dynamical cutoff coming from the dilaton coined the name " soft wall " . following this idea , people decided to model electroweak physics with such a geometry ( see e.g. here ) . all the standard model fields , including the higgs must now propagate in the bulk . the new setup offered unique phenomenology and is far less constrained by electroweak precision observables and fcncs which cause severe tensions in the original rs . note that since the dilaton field is not normally given a kintic term in such models , it is not a true dynamical field and one may simply consider the effect as being a different form of metric than rs . so essentially the difference between hard wall and soft wall is just a different geometry of the extra dimension which produces different phenomenology .
the mass of the black hole only grows by $ ( 1-\epsilon ) m$ , i.e. the mass that has not been radiated away yet . that is guaranteed by the mass conservation . however , one must be careful about dividing mass and energy to " individual places " in general relativity ; in this case , it can be kind of done , but more detailed questions " where the mass/energy resides " could be meaningless . only the total mass/energy is conserved in general relativity ( in asymptotically flat and similar spaces ) . the local physics of electrons moving in magnetic fields etc . is always the same . the electron mass is always the same constant . to describe what electron is doing in a situation like this , go to a freely falling frame , find out what the values of the electromagnetic fields are in this frame , and use exactly the same electron mass etc . as you would use in the absence of any black hole . if you wanted to use a non-freely-falling frame ( or coordinate system ) to describe the behavior of an electron near the event horizon , you must be very careful to do it right . for example , the gravitational field near the event horizon makes the usual static coordinates extremely deformed relatively to the flat metric – a component of the metric tensor goes to zero or infinity near the event horizon – and there is a nonzero curvature etc . so i am sure that all people who think that general relativity is still essentially the same newtonian mechanics – and in between the lines , you make it likely that you belong to this set – would almost certainly make the calculations incorrectly in a curved system . that is why i am urging you to go to a freely falling frame .
your formula for the generating function is wrong in a crucial sense . the formula you are after reads $$ \frac{1}{|\mathbf{r}-\mathbf{r}'|}=\sum_{l=0}^\infty \frac{r_&lt ; ^l}{r_&gt ; ^{l+1}}p_l ( \cos\theta ) . $$ note that the numerator and denominator of each term are powers of the lesser and greater , resp . , of $r$ and $r'$ . for the multipolar expansion your question asks about , you need the point of evaluation to be further away from the centre than the disk radius $r$ , which means that the powers will be in $r/p$ instead of $p/r$ . that will solve the divergence issues on the integrals . i find the key to quickly seeing which way the expansion will go is seeing it as a taylor series ( for fixed $\theta$ ) in the relevant small paramenter .
a majorana fermion would be a fermion that is it is own anti-particle . this is a common trait for bosons - the photon , gluon , and z bosons are all their own anti-particles . obviously , a majorana fermion must not interact through the electromagnetic force , that is , it must have zero charge . more technically , the wave equation that governs majorana particles is a real wave equation . you can change a particle into its anti-particle by using complex conjugation ( reversing the sign of complex numbers ) . therefore , since it is a real wave equation , majorana fermions would be their own anti-particles . so , i think you are misunderstanding something - a majorana fermion is a label applied to a fermion that is its own anti-particle . it is not that fermions have a majorana fermion that they are related to , it is just a name for particles that are equivalent to their anti-particles . for example , the neutrino may be a majorana fermion . the way this can be tested is to see if double beta decay can occur without neutrinos . in double-beta decay , two neutrons in the nucleus are converted to protons , and two electrons and two electron antineutrinos are emitted . in neutrinoless double beta decay , the two neutrinos annihilate each other to produce two electrons , which is obviously only possible is the neutrino is a majorana particle . this is a feynman diagram of neutrinoless double beta decay . some more information on the wiki page : http://en.wikipedia.org/wiki/majorana_fermion
linacs come in several types , but the kind you are talking about are segmented devices . the device is divided into multiple regions , each developing a strong electric field , but to avoid needed million volt potentials ( as in a van de graff accelerator ) the regions have alternating fields at any given moment . then you arrange for a bunch of charged particles to enter the device at a time when the field has the polarity that will accelerator those particles . however , the particles are moving , so if you just left the field that way the beam would enter the next region ( where the field points the other way ) and lose all the energy the gained in the first region . instead , you swap the field as the particles move between regions . that way they pick up more energy in the second region . then you swap feilds again before they move into the third region and they get another boost and so on . all of this happens very fast , of course , so in modern devices the power is provided at radio frequencies . indeed the super conducting klystrons that are all the rage are resonant cavity devices working in the radio band .
the flight actually took 25 minutes , the video you linked to is edited down , and the later stages of it look pretty turbulent . i would guess it was the shaking that eventually broke off the chair leg , possibly due to metal fatigue . note that the leg broke off after the balloon had burst and while the chair was falling back to earth . as anna says , the low temperature may well have been a factor . the ductile-brittle transition for mild steel is around -50c so at -60c it would be a lot more brittle than at room temperature . from the way the leg broke off i would guess it was the weld that failed rather than the steel itself .
theoretically ? sure . practically ? no . the primary problem is not lack of knowledge about how to manipulate individual atoms , though this is very tricky and it might not currently possible to manipulate the right kind of atoms in the right kind of way for this sort of task . the central problem is one of scale . for an item like a milky way bar , you are talking about billions and billions of atoms . your problem would not be that the candy bar was melty , it would be that you would die thousands of years before there was enough of it put together to take a bite . also , the techniques for atom manipulation work reasonably well for placing atoms on some sort of substrate like printing circuits on silicone chips . when you start talking about things like assembling complex sugars atom by atom , the situation gets significantly more difficult .
if you want to prove that $\vec{l}=\vec{r}\times \vec{p}$ is constant with respect to time for a particle in a central force field $\vec f = \phi ( r ) \vec r$ , just show that the angular momentum does not change with time , i.e. $\frac{d}{dt}\vec{l}=0$ . using the product rule we get two terms : $\frac{d}{dt}\vec{l}=\frac{d}{dt} ( \vec{r}\times \vec{p} ) = \frac{d\vec r}{dt} \times \vec p + \vec r \times \frac{d \vec p}{dt}$ . since $\vec p = m \frac{d \vec r}{dt}$ and $\frac{d \vec r}{dt}$ are obviously parallel , the first term vanishes . in the special case of a central force $\vec f = \phi ( r ) \vec r$ the second term vanishes too : we have $\frac{d\vec p}{dt} = \vec f \propto \vec r$ , so the two vectors in the second term are parallel , causing the cross product to become zero . therefore $\frac{d}{dt}\vec{l}=0$ and $\vec{l}$ is a constant with respect to time . to answer your questions : ( 1 ) no , you can not integrate like that . the position of the particle $\vec r$ changes with the time , so you can not treat it as a constant in your integration . if you want to solve this integral , solve the equations of motion $\frac{d \vec p}{dt} = \vec f$ first . ( 2 ) if your integration would have been correct ( for instance if the particle position were constant ) , the integration constant would have been a vector too . then the cross product would make sense again .
if anyone is interesed , the answer to my question is $$i\mathcal{m}=\frac{-ie^4\epsilon_{\eta} ( k ) \gamma^{\eta}g_{\eta \delta}\gamma^{\delta}\epsilon_{\delta} ( k' ) g_{\delta \beta}\gamma^{\beta}\epsilon^*_{\beta} ( k'' ) g_{\alpha \beta}e^*_{\alpha} ( k''' ) \gamma^{\alpha}g_{\alpha \eta}}{ ( q^2+i\epsilon ) ^4}$$
you can think of it in this way : to find position of any object we use reflected light from that object . for day-to-day life objects there is no problem . but for subatomic particle it means that we are giving them considerable amount of momentum and energy through photons . thus the very moment we measure their position we are also changing their momentum . thus both cannot be known with absolute certainty at the same time . hope this helps .
yes it is , because null areas ( and only null areas ) are invariant to the time-slicing , although it is not completely obvious to someone who is not day-to-day familiar with minkowki geometry . this is proved here in the first titled section of this answer : second law of black hole thermodynamics perhaps i should say how this proves the intended result : no matter what spacelike surface you slice a stationary black hole with , so long as matter did not fall in ( so that the geodesics on the black hole surface did not spread apart ) , the area you cross with the slice is invariant . a boosted observer has a tilted simulteneity plane , so this observer crosses the horizon with a different natural coordinate ( although which natural coordinate to use is not specified by gr--- you can use isotropic schwarzschild coordinates , then boost these by a naive lorentz transformation ( this keeps the asymptotic metric flat ) , and then continue a little bit into the interior by any method , this requires bending the t-coordinate away from the symmetry direction so that it can cross the horizon ) . the area of each little triangle on the crossing surface can be slid up and down to match each little triangle on another crossing surface , so the area is independent of the frame .
let 's look to your own statements . first , time derivative after transformations is not equal to an " old " derivative : for $\mathbf r ' = \mathbf r - \mathbf u t = \mathbf r - \mathbf u t ' \rightarrow \mathbf r = \mathbf r ' + \mathbf u t'$ $$ \partial_{t'} = ( \partial_{t'}\mathbf r ) \partial_{\mathbf r} + ( \partial_{t'}t ) \partial_{\mathbf t} = ( \mathbf u \cdot \nabla ) + \partial_{t} , \quad ( \mathbf u \cdot \nabla ) = u^{i}\partial_{x_{i}} . $$ so , with $\nabla ' = \nabla$ , " bianchi " equations transforms to $$ ( \nabla \cdot \mathbf b' ) = 0 , \quad [ \nabla \times \mathbf e ' ] + \frac{1}{c}\partial_{t}\mathbf b ' + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b ' = 0 . \qquad ( . 1 ) $$ second , the form of $\mathbf {e}' ( \mathbf r ' , t' ) , \mathbf b ' ( \mathbf r ' , t' ) $ is not equal to $\mathbf e ( \mathbf r , t ) , \mathbf b ( \mathbf r , t ) $ . let 's use the lorentz force expression , $$ \mathbf f = q\mathbf e + \frac{q}{c} [ \mathbf v \times \mathbf b ] . $$ it does not depend on acceleration , so the statement that $\mathbf f ' = \mathbf f$ under galilean transformation is true . it means that $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v ' \times \mathbf b' ] . $$ by using galilean transformation for speed , $\mathbf v ' = \mathbf v - \mathbf u$ , this equation can be rewritten as $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v \times \mathbf b ' ] - \frac{1}{c} [ \mathbf u \times \mathbf b' ] , \qquad ( . 2 ) $$ so the statement that $\mathbf e = \mathbf e ' , \quad \mathbf b = \mathbf b '$ is not correct . so you need to find expressions $\mathbf e ' $ and $\mathbf b'$ via $\mathbf e $ , $\mathbf b$ . by rewriting $ ( . 2 ) $ , $$ \mathbf e + \frac{1}{c} [ \mathbf v \times ( \mathbf b - \mathbf b ' ) ] = \mathbf e ' - \frac{1}{c} [ \mathbf u \times \mathbf b ' ] , $$ in a reason of arbitrary $\mathbf u $ you can get the solution : $$ \mathbf b ' = \mathbf b , \quad \mathbf e ' = \mathbf e + \frac{1}{c} [ \mathbf u \times \mathbf b ] . $$ by substitution these equations to $ ( . 1 ) $ you will get $$ ( \nabla \cdot \mathbf b ) = 0 , \quad [ \nabla \times \mathbf e ] + \frac{1}{c} [ \nabla \times [ \mathbf u \times \mathbf b ] ] + \frac{1}{c}\partial_{t}\mathbf b + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b = [ \nabla \times \mathbf e ] + \frac{1}{c}\partial_{t}\mathbf b = 0 , $$ because for $\mathbf u = const$ $$ [ \nabla \times [ \mathbf u \times \mathbf b ] ] = \mathbf u ( \nabla \cdot \mathbf b ) - ( \mathbf u \cdot \nabla ) \mathbf b = - ( \mathbf u \cdot \nabla ) \mathbf b . $$ so the first pair of maxwell 's equations is clearly invariant under galilean transformations . let 's look to the other pair of maxwell 's equations : $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e = 0 , \quad ( \nabla \cdot \mathbf e ) = 0 . \qquad ( . 3 ) $$ by using an expressions which were derived above , you can rewrite $ ( . 3 ) $ as $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e ' - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e ' = $$ $$ = [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e - \frac{1}{c^{2}}\partial_{t} [ \mathbf u \times \mathbf b ] - \frac{1}{c^{2}} ( \mathbf u \cdot \nabla ) [ \mathbf u \times \mathbf b ] = 0 , $$ $$ ( \nabla \cdot \mathbf e ) + \frac{1}{c} ( \nabla \cdot [ \mathbf u \times \mathbf b ] ) = ( \nabla \cdot \mathbf e ) -\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) = 0 . $$ the requirement of galilean invariance of second equation leads to te state that $\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) $ , which is not true in the general case . analogically reasoning can be used for the first equation . so the second pair of maxwell 's equations is not invariant under galilean transformations .
the velocity calculation was distance* ( change in angle ) . however , this does not take into account the changing time-delay of light : we see it sped-up because the time delay is decreasing , like a tv recording where you are fast-forwarding as you gradually catch up with real time . fortunately , all we need to do to calculate the real speed is to account for the time delay , no weird relativity is necessary . suppose a far away object is approaching at $0.8c$ and has a transverse velocity of $0.25c$ ( total velocity of $0.84c$ ) . it emits a burst of light ( in our frame ) at time $t$ and $t+5$ seconds . it is $d$ light seconds from us at time $t$ and $d-4$ at $t+5$ . accounting for the time delay of light , we see flashes at time $t+d$ and $t+5+ ( d-4 ) = t+d+1$ ; we see them only $1$ second apart . in those $5$ seconds , it moved transversely a distance of $5\times0.25 = 1.25$ light seconds . since it appeared to move $1.25$ light seconds in $1$ second , we see apparent superluminal motion .
the problem is you have the wrong relations between $\{n_\mathrm{h} , n_\mathrm{he}\}$ and $\{n_\mathrm{p} , n_\mathrm{n}\}$ . every hydrogen contains 1 proton , and every helium contains 2 , so $n_\mathrm{p} = n_\mathrm{h} + 2 n_\mathrm{he}$ . the neutrons are only contributed to by helium in the accounting : $n_\mathrm{n} = 2 n_\mathrm{he}$ . inverting these relations yields \begin{align} n_\mathrm{h} and = n_\mathrm{p} - n_\mathrm{n} \\ n_\mathrm{he} and = \frac{1}{2} n_\mathrm{n} . \end{align} it looks like you got the direction wrong , in the sense that there should be fewer helium nuclei than protons or neutrons ( the 2 's are on the wrong side ) . also , " hydrogen " means ${}^1\mathrm{h}$ not ${}^2\mathrm{d}$ unless otherwise stated .
regarding compactification -- perhaps you are looking for the notion of the " unit tangent bundle " , which would be compact provided the manifold is compact . in general , the tangent bundle is a manifold of dimension twice the original and naturally projects onto the original manifold ; in other words , if the original manifold is not compact , even the unit tangent bundle is not compact . you can read more about the unit tangent bundle here . regarding changes of coordinates : to be completely frank , i understood neither the motivation nor the technical details . nevertheless , when dealing with like questions , one has to be careful to not mix the categories that one is working in . from the point of view of bare set theory , all the spaces you described , and in addition $\mathbb{c}^2$ , are equivalent ( in the sense that there exists a one-to-one and onto map from any one of the spaces in question to any other ) . however , in addition to the set structure , our spaces have a much richer structure : a geometric one . thus , as soon as we pass to the smooth category ( think of it , very roughly speaking , as a context of smooth manifolds and smooth maps defined on them ) , then questions arise as to whether changes of coordinates preserve the structure we are interested in . to make matters more complicated , the spaces $\mathbb{r}^k$ , $\mathbb{c}^k$ and the projective spaces all carry an algebraic structure . in short , there is no reason to change the coordinates from , say , $\mathbb{r}^2$ to $\mathbb{c}^1$ , unless one wants to benefit from the additional structures carried by $\mathbb{c}^1$ . in this case , then , one has to make sure that the change of coordinates is canonical ( in the sense that whatever is proven in the new coordinates can be carried back to the old coordinates ) . thus the transformation that manifests this change of coordinates must possess certain structural integrity ( loose language alert ! ! ) . this makes the question difficult , especially when it is not clearl exactly what one is looking for . here is a simple example : let us view $\mathbb{r}^2$ as $\mathbb{c}^1$ via the transformation $\phi : \mathbb{r}^2\rightarrow \mathbb{c}^1$ with $\phi ( a , b ) = a + bi$ . no tricks here . moreover , this transformation is continuous with a continuous inverse , so a homeomorphism . thus all the topological properties are preserved ( i.e. . whatever one can prove about $\mathbb{c}^1$ and the natural topology thereof , as well as continuous maps defined on them , one can formulate analogous results for $\mathbb{r}^2$ ) . notice also that both $\mathbb{r}^2$ and $\mathbb{c}^1$ are manifolds , but here one has to be careful : a manifold is always modeled on a " model manifold " ( for example , a smooth surface embedded in $\mathbb{r}^3$ is modeled on $\mathbb{r}^2$ in the sense that it is locally , around any point , essentially a copy of $\mathbb{r}^2$ . now , when we say that both $\mathbb{r}^2$ and $\mathbb{c}^1$ are manifolds , what do we mean by that ? here one has to make a choice . we can either model both on $\mathbb{r}^2$ , or model $\mathbb{r}^2$ on $\mathbb{r}^2$ , and model $\mathbb{c}^1$ on $\mathbb{c}^1$ . in the former case they are equivalent via the change of coordinates $\phi$ ; in the latter case , they are not , since there are functions , say $f$ , on $\mathbb{r}^2$ which are infinitely differentiable and constant on some open set , while the corresponding " push forward " of this function under $\phi$ , namely $f\circ \phi$ , is not differentiable in $\mathbb{c}^1$ . that is , in the analytic category , $\mathbb{r}^2$ and $\mathbb{c}^1$ are not equivalent ( analytic category is stronger than continuous ; also , being differentiable in $\mathbb{c}^1$ is equivalent to being infinitely differentiable , is equivalent to being analytic ) . the latter complication comes from the fact that in addition to a topological structure of $\mathbb{c}^1$ ( which , as i have commented above , is the same as that of $\mathbb{r}^2$ ) , $\mathbb{c}^1$ carries an algebraic structure ( namely , a field ) which is compatible with its topological ( even smooth ) structure in the sense that the field operations of multiplication and addition are smooth maps ( in fact analytic ) . $\mathbb{r}^2$ , on the other hand , as it is usually defined , does not enjoy these properties . you could argue , of course , that we can " redefine " $\mathbb{r}^2$ to reflect the properties of $\mathbb{c}^1$ . sure we can do that , but then $\mathbb{r}^2$ becomes $\mathbb{c}^1$ that goes by the name of $\mathbb{r}^2$ . when we write $\mathbb{r}^2$ , we specifically mean the two-dimensional vector space with the underlying field being $\mathbb{r}$ . the vector space operations in this case are incompatible with the field operations of $\mathbb{c}^1$ . long story short : whenever a change of coordinates is introduced , it is usually for the sake of transforming a representation into a simpler form , to simplify computations or to relate to a known problem/solution in the other coordinates . you know you are doing something that does not " feel right " when you are changing coordinates for the sake of benefiting from some additional ( or principally different ) mathematical structure of the new coordinates , because whatever you manage to prove in the new coordinates that requires this additional ( or different ) structure , you probably will not be able to pull back to the original coordinates ( there are some exceptions to this rule , of course ! ) . thus , it seems to me , that what you are doing is either ( a ) ultimately incorrect in the sense of making a category error , or ( b ) simply " renaming " your spaces without losing or gaining anything .
finally , i got it . the sketch is as following : in order to understand the fermionic measure , suppose that we did something suitable so that the spectrum of the appropriate dirac operator is discrete ( for exapmle , take the volume to be finite , work on compact manifold ) . let $\psi_n ( x ) $ be the eigenbasis for the operator . then we use $\det c = \exp ( tr \ln c ) $ for $c$ the transition operator . in our case it multiplies by $\exp ( \phi ) $ , so $\ln c$ is multiplication by $\phi$ . trace $\sum_n\int d\mu ( x ) \phi ( x ) \bar{\psi}_n ( x ) \psi_n ( x ) $ is regularised either by $\zeta$-regularization or by introducing $\exp ( -d^2/m^2 ) $ with $d$ the dirac operator . basically the same as the chiral anomaly in inspirehep . net/record/17430 , but we take the finite transformation instead of infinitesimal one .
yeah , convert it to meters . other units are used for quick-and-dirty conceptual stuff , but when it comes to the math , you need to keep your units consistent . $\nu$ is frequency . hence the exchange : " hey , what is new ? " asked alan . "$\nu$ is frequency ! " answered betty . alan blinked twice , and began looking for an exit to this conversation .
special relativity " says " that a clock with a straight worldline through two events records a larger elapsed time between the events than a clock with any other worldline through the same two events thus , the " stay at home " lc oscillator shows the most counts .
among the base units of the international system , the kilogram is the only one whose name and symbol , for historical reasons , include a prefix . names and symbols for decimal multiples and submultiples of the unit of mass are formed by attaching prefix names to the unit name " gram " , and prefix symbols to the unit symbol " g " ( cipm 1967 , recommendation 2 ) . bipm the reason why " kilogram " is the name of a base unit of the si is an artefact of history . louis xvi charged a group of savants to develop a new system of measurement . their work laid the foundation for the " decimal metric system " , which has evolved into the modern si . the original idea of the king 's commission ( which included such notables as lavoisier ) was to create a unit of mass that would be known as the " grave " . by definition it would be the mass of a litre of water at the ice point ( i.e. . essentially 1 kg ) . the definition was to be embodied in an artefact mass standard . after the revolution , the new republican government took over the idea of the metric system but made some significant changes . for example , since many mass measurements of the time concerned masses much smaller than the kilogram , they decided that the unit of mass should be the " gramme " . however , since a one-gramme standard would have been difficult to use as well as to establish , they also decided that the new definition should be embodied in a one-kilogramme artefact . this artefact became known as the " kilogram of the archives " . by 1875 the unit of mass had been redefined as the " kilogram " , embodied by a new artefact whose mass was essentially the same as the kilogram of the archives . the decision of the republican government may have been politically motivated ; after all , these were the same people who condemned lavoisier to the guillotine . in any case , we are now stuck with the infelicity of a base unit whose name has a " prefix " . bipm the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
any equations you set up will be uglier than the cross product ( they will basically by expansions of $\times$ ) remember , vectors only can define a direction . the issue with the magnetic field is that it is heavily related to planes , and is a perpendicular force . we can not assign a magnetic field analogous to the electric field ( as in : a field which shows the direction of force ) , as it depends upon the direction of velocity . the cross product enables us to define a plane with a vector perpendicular to it . aside from that , it gives perpendicular stuff . this clearly points to the use of the cross product . besides , the cross product is not that dirty . it makes stuff simpler , if anything . if you want to do it all via equations , use this : $$\vec{a}\times\vec{b}= \begin{vmatrix} \hat{i} and \hat{j} and \hat{k} \\ a_x and a_y and a_z \\ b_x and b_y and b_z \\ \end{vmatrix}$$ these rules may help you avoid the determinant form for simpler cross products : $$\hat{i}\times\hat{j}=\hat{k} ; \:\hat{j}\times\hat{k}=\hat{i} ; \:\hat{k}\times\hat{i}=\hat{j} ; \:\vec{a}\times\vec{a}=0 ; \:\vec{a}\times\vec{b}=-\vec{b}\times\vec{a}$$ over here , the right hand rule is implied by our choice of a right handed system of $\hat{i} , \hat{j} , \hat{k}$ or $x , y , z$ . so you do not need to touch it , it is already inherent in the coordinate system .
this link : http://alienryderflex.com/polarizer/ has an excellent explanation ; much better than anything i could write here . essentially , it says that this occurs because the 45 degree filter outputs a projection of the vertical rays at 45 degrees . this , in turn , has a horizontal component , which the final filter projects in its output .
is it true that gravitational lensing only occurs for objects made of plasma ? most of space contains a plasma , or at least some ionisation , at some density or other , making the claim hard to dispute at that level . also , gases and/or plasmas will tend to exist with higher densities in deeper gravity wells . what is the merit of claim that bending could be caused by plasma and not by gravity ? in general it is possible for light paths to be changed by interaction with matter ( a glass lens being an obvious example ) . in principle similar distortion could happen with a rarefied plasma over large distances . a density variation at the right scale , and with a simple shape would be required . i expect however that the claim in the link is just a conjecture/challenge , and not mathematically or scientifically robust ( although i do not personally have the knowledge and skills to assess that ) . i suspect that the lensing seen in an einstein ring would be very hard to duplicate using realistic values for plasma density near the lensing object . to me , there are plenty of warning signs that this is a crackpot claim from the link . unfortunately the world is full of them . often such people cannot be corrected by scientific evidence , in fact they will self-promote that there is " controversy " when engaged in conversation , or complain of being " suppressed by the establishment " when ignored . i am not saying those thing apply to the author ( i have not checked other publications ) , but i would be wary of the " a nasa physicist " label - it is likely correct on some technical label ( he worked for nasa and has a qualification in physics ) but is not particularly meaningful . it does not help the lay audience ( nb i include myself here ) when real scientific controversies and useful scepticism can follow a similar pattern . popular proponents of badly-thought-through or outlier theories often manage to pick up on the form of this and produce " science-y " work , making it harder to filter out good from bad . update : i have struggled through a few pages of http://www.extinctionshift.com and stand by this - the work appears to be random picking of equations plus handwaving arguments about how they connect . nothing is properly analysed . does this mean general relativity is invalid ? no , even if somehow the claim had merit , there is plenty of evidence for gr outside of gravitational lensing . for example : time-dilation effects of general relativity can be measured directly by atomic clocks . they have got so accurate that we can actually measure the time dilation of a few metres height difference at ground level . the gps system needs to account for this effect . however it is well known that general relativity and the standard model of quantum mechanics , although our best current models in physics , cannot be the last word . they are not compatible , and where they overlap ( typically when considering cosmology or black holes ) there is a need for a combined or replacement theory . that does not make the theories " wrong " any more than newtonian physics was " wrong " , but it does mean we can expect some extensions or a deeper theory to emerge at some point .
the phenomenon you describe is ferromagnetism not paramagnetism . ferromagnetic materials like iron behave as if they contain many tiny bar magnets ( called magnetic domains if you are interested to pursue this further ) , but because the magnet domains are aligned randomly the fields cancel out and there is no net magnetic field . however if you put a ferromagnetic material in a magnetic field the external field will cause partial alignment of the magnetic domains . this induces a magnetic field in the originally unmagnetised iron , and that is why your paper clip sticks to the ball . however if you remove the external magnetic field the domains will go back to their original alignment , the net magnetic field will go back to zero and the paper clip will fall off again . if you apply a very strong field and/or combine it with heating and cooling you can permanently change the alignment of the magnetic domains so they remain aligned when the external field is removed . this is how you make permanent magnets .
i will stick my neck out and say that the answer to your question is simply " yes . " first off , these detailed thermal models are complex and hard to do , so we want confirmation from independent groups . we have that : rievers and lämmerzahl , " high precision thermal modeling of complex systems with application to the flyby and pioneer anomaly , " gr-qc/1104.3985 second , we could ask whether these results contradict previous work . the answer is basically no . previous work was simply sloppy . there is a nice talk on this topic here by toth : http://streamer.perimeterinstitute.ca/flash/a2cc528b-1d36-4a2e-af73-5f81b8b17477/viewer.html there is a long history where people did back-of-the-envelope estimates of the thermal effects and said , " look , the order of magnitude is too small to matter ! " it just turns out that the back-of-the-envelope were wrong . finally , all of this stuff is very tough to be sure of , because there are so many uncertainties about things like the degradation of the white paint on the rtgs . therefore it would be good to have independent ways of testing the hypothesis of a gravitational anomaly , without having to use the pioneer data at all . we do have these independent tests . if the effect obeyed the equivalence principle , it would have had effects on the outer solar system that are not in fact observed : iorio , " does the neptunian system of satellites challenge a gravitational origin for the pioneer anomaly ? , " gr-qc/0912.2947 it is dead , jim .
per kittel 's " elementary statistical physics": $$dq=du_a+mdh$$ . where $du_a$ does not include the magnetic field energy as part of the system . an alternative , equivalent , formulation is $$dq=du_b-hdm$$ where $du_b$ does include the field energy : $u_a=u_b-\mathbf{h \cdot m}$ . with $dq=tds$ , i think you are there . . .
i will try to give a simple answer without going too deep into physics details . localization of light is the confinement of a light wave so that it is very intense in one particular location . usually it vanishes rapidly as you get farther away from that location . ( note that by " light wave " i am not talking about the traveling plane waves that you would use to describe a light beam ; this is more of a stationary oscillation mode . ) this crops up in the context of plasmonic nanoparticles because surface plasmons ' wavelengths are smaller than normal light waves of the same frequency , allowing surface plasmons to be confined into even tinier spaces . there are two major advantages that lead to applications . one is that confining the light into a small space allows easy manipulation of it , for example by tailoring the shape of your nanoparticle . the other is that confining the light into a small space also squeezes all the energy it carries into that small space . this is called field enhancement . you can then do processes that require a lot of electric field strength ( e . g . nonlinear things ) without needing a huge amount of light to achieve that field strength . ( i adapted some of this answer from the introduction to my doctoral dissertation . )
the event horizon is a lightlike surface , and so its area is coordinate-invariant . for a schwarzschild black hole , $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2 + \left ( 1-\frac{2m}{r}\right ) ^{-1}dr^2 + r^2 ( d\theta^2+\sin^2\theta\ , d\phi^2 ) $$ the horizon suface is at $r = 2m$ of schwarzschild radial coordinate , and so at any particular schwarzschild time ( $dt = 0$ ) has the metric $$ds^2 = ( 2m ) ^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) , $$ which is just the metric on a standard 2-sphere of radius $2m$ . you can find the square area element explicitly as the determinant of the metric , $da^2 = ( 2m ) ^4\sin^2\theta\ , d\theta^2d\phi^2$ , and integrating : $$a = 16\pi m^2 = \frac{16\pi g^2}{c^4}m^2 . $$ the volume of the black hole is not invariant , as jerry schirmer says . if you try to apply anything the above schwarzschild coordinates above , then since the coefficients of $dt^2$ and $dr^2$ switch signs across the horizon , $t$ is spacelike and $r$ is timelike . therefore , since the black hole is eternal , it could be said to have infinite volume ( classically , but a real astrophysical black hole would have a finite but still extraordinarily high lifetime ) , as you will be integrating $dt$ across its lifetime . technically , the above argument is a bit flawed , because the schwarzschild coordinate chart is not defined across the event horizon , so one should be more careful how they are continued across the horizon ( e . g . , with kruskal-szekeres coordinates ) . but this can be made more rigorous . in another coordinate chart , e.g. , the gullstrand-painlevé coordinates adapted to a family of freely falling observers , $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2-2\sqrt{\frac{2m}{r}}\ , dt\ , dr + \underbrace{dr^2 + r^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) }_{\text{euclidean in spherical coord . }} , $$ at any instant of time ( $dt = 0$ ) , space is precisely euclidean ; since the horizon is still $r = 2m$ in these coordinates , " the " volume is $$v_{\text{gp}} = \frac{4}{3}\pi ( 2m ) ^3 . $$ if you pick yet another coordinate chart , you may get a yet different answer .
yes , that is ok ! you have stumbled upon one of the basic strange phenomena of relativity .
in short , no , not all fluorophores are dipoles in the permanent dipole sense . as a counterexample , anthracene has zero permanent dipole moment , but fluoresces blue under uv illumination . the reason why many fluorescent molecules are called " dipoles " is because electric dipole transitions between quantum states occur due to nonzero values of the transition dipole element between those states . so while anthracene 's ground state has zero permanent electric dipole , the transition dipole element between the ground state and the relevant excited state turns out to be nonzero .
all you need to do is set up a multipole expansion of the gravitational waveform . you will find that the monopole moment is proportional to the time derivative of the mass of the stress-energy tensor , and the dipole moment is proportional to the second time derivative of the momentum from the stress-energy tensor , both of which are conserved . thus , the first nonzero moment comes from the quadrupole moment . this is worked out in great detail in mtw .
one of the best recent references is the 2008 rmp article by nayak et al . non-abelian anyons and topological quantum computation a somewhat less technical reference is an anyon primer by sumathi rao . there are many others but these two are good for someone starting out .
the outcomes are not supposed to be the same . there are two ways to interpret your question : 1 . you want to calculate the kinetic energy in different reference frames . let 's think for example of a point-like body moving in a constant velocity $\mathbf{v}$ . it is kinetic energy is $\frac{1}{2}mv^2$ , but if we calculate it in a reference frame that is moving with the body , in that frame the body is at rest and we get zero . so we do not expect the same outcome when calculating kinetic energy in different reference frames . 2 . you want to stay in the laboratory reference frame , but pick different points as the axis of rotation for your calculations . here there is a subtle point we need to be aware of . it is true that some calculations involving the rotation of rigid bodies can be done in several different ways , each time picking a different axis , and still resulting in the correct outcome . for example this works if we want to calculate the linear and angular acceleration of a body when a given force is applied to it . however , if a rigid body has linear movement and rotation simultaneously , and we want to calculate it is kinetic energy , we need to be careful when using the formula : $$ e_k = \frac{1}{2}m v^2 + \frac{1}{2} i \omega^2 , $$ where $\mathbf{v}$ is the velocity of the axis point and $\omega$ is the angular velocity of rotation around that point . this is the formula you used in your calculations , but in fact it is valid only in the following cases ( and i give a proof of this below ) : when we use the center of mass as the axis of rotation . when the axis of rotation is at rest ( i.e. . $\mathbf{v}=0$ ) . when the velocity is parallel to the line connecting the axis point to the center of mass . regarding the calculations you showed in your question : when you used the fixed point of the cylinder as the axis case 2 applied . when you used the center of mass case 1 applied . when you used the moving extreme none of the cases applied , and you cannot use the above formula in this case . proof : we model the rigid body as a collection of point-like masses $m_i$ with their positions relative to the axis of rotation denoted as $\mathbf{r}_i$ . the velocity of mass $i$ is : $$\mathbf{v}_i = \mathbf{v} + \boldsymbol\omega \times\mathbf{r}_i . $$ the total kinetic energy is then : $$e_k = \sum_i \frac{1}{2} m_i v_i^2 = \frac{1}{2} mv^2 + \frac{1}{2} i \omega^2 + m \mathbf{v} \cdot ( \boldsymbol\omega \times \mathbf{r}_{cm} ) , $$ where $m=\sum_i m_i$ is the total mass , $i=\sum_i m_i |\hat{\boldsymbol\omega} \times \mathbf{r}_i|^2$ is the moment of inertia and $\mathbf{r}_{cm} = ( \sum_i m_i \mathbf{r}_i ) /m$ is the center of mass relative to the axis of rotation . we see that we need the last term to vanish in order to get the formula we want to prove , and we can get this if $\mathbf{r}_{cm}=0$ ( case 1 ) , $\mathbf{v}=0$ ( case 2 ) or $\mathbf{v} \cdot ( \boldsymbol\omega \times \mathbf{r}_{cm} ) =0$ ( case 3 ) .
the concept you are looking for is the attenuation length of light in water . this is the quantitative measure of how strongly different wavelengths get absorbed in water . the absorption of light is fairly simple to describe : it mostly depends on the material , but also if the light transverses a longer length of material , the attenuation will be stronger , and if there is more light to begin with , then more will be absorbed . in fact , the amount of light absorbed is proportional to the amount of light present , so that the decay will be exponential . to get rid of these factors and to get a constant which is purely a property of the material and not of how much of it there is or of the intensity of the light , we do two things : we measure absorption per unit length of material transversed , and we measure absorption as a percentage of the incident light . this is what the absorption length is : the length $l$ it takes for a given light intensity to decrease to 36.7% of its initial value . why this weird number ? since the decay of the intensity is exponential , it can be written as $$ i ( l ) =i ( 0 ) e^{-l/l} . $$ when $l=l$ , the intensity has decreased to $e^{-1}=0.367\ldots$ . an alternative , more useful quantity is the inverse of the attenuation length , $\kappa=1/l$ , which is called the attenuation coefficient of the medium , and this gets quoted somewhat more often as i understand it . here is a sample absorption spectrum of liquid water , which as you can see depends strongly on the wavelength : to find out the incident light spectrum at a given depth , you need to apply this attenuation factor to the initial spectrum , which is the one at sea level . to get the spectral irradiance ( solar power per unit area per unit wavelength ) at a wavelength $\lambda$ and at depth $l$ , you start with the spectral irradiance at the sea surface , $\sigma ( \lambda , 0 ) $ , and then you apply the beer-lambert law of exponential decay to it : $$ \sigma ( \lambda , l ) =\sigma ( \lambda , 0 ) \exp ( \kappa ( \lambda ) l ) ) . $$ this is the equation you were looking for . you might notice , though , that it does not get you that much closer to plotting a graph of this quantity , and that is because $\kappa ( \lambda ) $ is a complicated quantity with a complicated spectral dependence . to get this data , you can start e.g. here , though of course the amount of work you will need will depend on exactly how much detail you want in your final answer .
in ideal circuit theory , kvl holds period . consider the series rlc circuit driven by an arbitrary voltage source $v_s$ . the canonical differential equation for the series current $i ( t ) $ is : $$\dfrac{d^2i}{dt^2} + \dfrac{r}{l}\dfrac{di}{dt} + \dfrac{1}{lc}i = \dfrac{1}{l}\dfrac{dv_s}{dt}$$ where does this equation come from ? it comes from writing the kvl equation around the loop in terms of the series current $i$: $$v_s ( t ) = ri ( t ) + l\dfrac{di ( t ) }{dt} + \dfrac{1}{c}\int_{-\infty}^ti ( \tau ) d\tau$$ now , it is true that if the assumptions of ideal circuit theory do not hold , kvl does not hold . however , understand that ac circuit analysis is under the umbrella of ideal circuit theory thus , in that context , kcl holds for ac circuit analysis . in response to a comment : then say for example , the current in a circuit with resistance and a capacitance is $i$ . then $v_r=ir$ and $v_c=i/ωc$ . but the supply voltage $v=\sqrt{v^2_r+v^2_c}$ and not $v=v_r+v_c$ . does this not violate kvl ? first , the phasor voltage across the capacitor is $\vec v_c = \dfrac{1}{j \omega c}\vec i$ . phasors are complex numbers . the sum of magnitudes is generally not equal to the magnitude of the sum : $$|z_1| + |z_2| \ne |z_1 + z_2|$$ thus , the sum of resistor and capacitor phasor voltage magnitudes is not meaningful but the sum of the resistor and capacitor phasor voltages is . let $v_1 ( t ) = v_1 \sin \omega t$ and $v_2 ( t ) = v_2 \cos \omega t$ be the time domain voltages across two series circuit elements . by kvl , the voltage across the series combination is $$v_s ( t ) = v_1 ( t ) + v_2 ( t ) = v_1\sin \omega t + v_2 \cos \omega t = \sqrt{v^2_1 + v^2_2}\cos ( \omega t - \phi ) $$ where $$\tan\phi = \frac{v_1}{v_2}$$ note that , using phasors , the above is $$\vec v_s = \vec v_1 + \vec v_1 = -jv_1 + v_2 = e^{-j \phi}\sqrt{v^2_1 + v^2_2}$$ thus , the sum of the phasor magnitudes , $v_1 + v_2$ , is not meaningful and certainly is not an application of kvl .
spin wave theory simply does not apply for 1d spin system . the starting point of the spin wave theory is a magnetically ordered ground state . but mermin-wagner theorem states that 1d spin system can not order even at zero temperature , due to the strong quantum fluctuation . so 1d heisenberg model does not lead to an antiferromagnetically ordered ground state , and hence the spin wave is not well defined , and the spin fluctuation does not follow the dispersion relation $\omega\sim k$ . it is known [ 1 ] that 1d spin chain is gapped , as conjectured by haldane . [ 1 ] z . -c . gu and x . -g . wen , phys . rev . b 80 , 155131 ( 2009 ) .
this is actually a rather subtle point never fully appreciated by students the first time they learn relativity . there is a difference between someone at a receiving the photons from event x , and event x actually occurring . in physics classes ( as distinct from astronomy ) , we are almost always talking about the occurrence of the event , not its observation . the idea is you set up your canonical grid of rulers and clocks throughout space . events happen . later on , at your own convenience , you go and visit the clocks ( or have them send you data ) and assemble the whole picture . this god 's-eye perspective is what we usually have , where we witness " simultaneous " events simultaneously only because we are post-processing the data . to emphasize : events are single points in spacetime , and they do not change place or time . event x may have occurred at b 's location - fine - but then do not confuse x with y = ( the observation of x by a ) , which is another event entirely , occurring at a different place ( a rather than b ) at a different time ( 1 light-second later than x , in the frame of a/b ) . so : yes they are in the same frame . both will agree , in post-processing , that the simultaneous event happened at the same time . a will see the flash at a first , and ditto for b , but there will not be any confusion after the fact when they get together and discuss what their grids of clocks said . see 1 . carefully . if you ever take your physics " rocketships and grids of rulers " training and go into astronomy , you will learn to appreciate the fact that we are looking out along our past light cone , rather than " horizontally " in a spacetime diagram . we do not have an infinite grid of clocks spread throughout the universe to query , but rather just our telescopes right here . you already know that the further away an event , the longer the elapsed time between it and its observation . but the key is to internalize that , and get used to adding and subtracting corrections to account for it .
light cannot move outwards inside the event horizon . i would guess you are thinking that an outgoing light ray might leave you in the outgoing direction , then slow to a halt and return - hence you would see yourself . however this does not happen . the light leaving you moves inwards not outwards , but since you fall inwards faster than the light does , the light still leaves you ( at velocity $c$ ) and never returns . this is discussed in some detail in the question if you shoot a light beam behind the event horizon of a black hole , what happens to the light ? . to show what happens to you and the light we draw a spacetime diagram . we’ll assume all motion is radial , so the diagram will just show distance from the singularity and time . the trajectory of any object in spacetime is a curve on the diagram called a worldline , and when two objects meet their worldlines intersect . so to show the black hole cannot act as a mirror we draw your worldline and the worldline of the light and show that they only intersect once . the problem is that we can’t use the usual coordinates $r$ and $t$ because these are singular at the event horizon . instead we use kruskal-szekeres coordinates $u$ and $v$ . i’m not going to go into how these coordinates are defined , see the wikipedia article for details , because we’d be here all day . the $u$ coordinate is spacelike both outside and inside the event horizon , and likewise the $v$ coordinate is timelike both outside and inside the event horizon . using these coordinates the spacetime diagram of the black hole looks like this : on this diagram the diagonal dashed lines are the event horizon , and the red hyperbola at the top is the world line of the singularity . the blue curve is your worldline as you fall into the black hole . we’re only interested in the top right half of the diagram – the bottom half shows a white hole and a parallel universe linked by a wormhole ( ! ) but that’s a discussion for another day . for our purposes the key feature of this diagram is that light rays follow straight lines with gradient $\pm 1$ . ingoing light rays travel from lower right to upper left ( gradient $-1$ ) while outgoing light rays travel from lower right to upper left ( gradient +1 ) . the worldlines of massive objects have a gradient closer to the $v$ axis than light rays , and the faster the object is travelling the closer its worldline gets to a gradient of $\pm 1$ . now we’re in a position to answer rijul’s question , but let’s zoom into the top right bit of the diagram so we can see what happens : at some point after you’ve crossed the event horizon you shine two light rays , one inwards and one outwards , and these are shown by the magenta lines . remember that light rays always travel at 45° on this diagram , so it’s easy to draw the worldlines of the light rays because they are just straight lines . your worldline is approximate in the sense that i didn’t sit down and calculate it , but it must everywhere be at an angle greater than 45° , and as you accelerate the gradient approaches 45° . so your worldline will look something like the blue line i’ve drawn , and in any case the exact shape of your worldline doesn’t matter for this proof . and with that we’re done ! the briefest glance at the diagram shows that your worldline and the worldlines of the light rays can only intersect at one point , i.e. the point you shine the light rays inwards and outwards . so the black hole can’t act as a mirror . note also that both light rays end up intersecting the worldline of the singularity so even the light ray directed outwards ends up falling into the singularity .
it is largely a matter of definition . here are some quotes . from springer reference : definition there is no universally recognized definition of chaotic flows . flows with properties that are neither constant in time nor presenting any regular periodicity are normally referred as chaotic . fluid turbulence is generally found to be chaotic . it is also random , dissipative , and multiple scaled in time and space . it is a complex system of infinite degrees of freedom . a paper full of state diagrams ( but rather heavy on the math ) for transitions to chaotic flow : https://tspace.library.utoronto.ca/bitstream/1807/25484/1/transition%20to%20chaos%20in%20converging-diverging%20channel%20flows.pdf and finally , a nice discussion at quora . com ( registration apparently required ) as piyush grover pointed out , a chaotic flow has to have mixing by definition . the following answer assumes ' chaotic ' to mean ' random ' which is technically incorrect . but i have decided to leave it here anyway , because i think that is what the op meant . however , i still think that you will not observe a k^ ( -5/3 ) spectrum in the case of chaotic advection . i am also adding piyush 's comment as a part of this answer . chaotic but non-turbulent flows can have exponential mixing . there is a whole field of chaotic advection based on this fact . in fact , you can have exponential mixing of mass in stokes flow , which is as far away from turbulence as possible . this is often used to mix fluid efficiently at micro devices ( low re ) , where turbulence is simply not feasible due to energy considerations . a chaotic flow is one in which there seems to be a high irregularity in the behavior of one/all flow variables with time/space . while a turbulent flow certainly exhibits this behavior , there are also other properties that should be present for a flow to be called turbulent , one of which is high levels of mixing , i.e. , mass/momentum/heat transfer . this is a distinct ( and perhaps the most useful ) property of turbulent flows which is frequently exploited . when you try to mix the sugar in a cup of coffee by stirring it , you are essentially making use of this property . this effect can be clearly seen by looking at the velocity profiles of laminar and turbulent flows through a pipe . ( taken from page on flowcontrolnetwork ) the lines show the magnitude of horizontal/streamwise velocities with respect to height along a pipe . you can see that the turbulent flow has a much flatter velocity profile than a laminar flow , i.e. , there are higher velocities close to the wall for a turbulent flow compared to a laminar flow , while there are lower velocities close to the centerline for a turbulent flow compared to a laminar flow . this shows that velocity ( momentum for an incompressible/constant density flow ) is transferred to a greater extent from the fluid elements close to the centerline to the fluid elements close to the walls in case of a turbulent flow . a good example of a flow which is chaotic but is not turbulent is the trail behind an aircraft . though the flow inside the jet trail is highly chaotic , it is not turbulent because it maintains the shape ( diameter ) for very large distances behind the aircraft , which means that there is very low/negligible mixing with the surrounding atmosphere .
it is possible to write a relationship between flow and pressure drop in a tube . if the flow is laminar , this will be given by the hagen–poiseuille equation ( http://en.wikipedia.org/wiki/hagen%e2%80%93poiseuille_equation ) . for turbulent flow , phenomenological correlations can be used ( http://en.wikipedia.org/wiki/darcy%e2%80%93weisbach_equation ) . with this , it is possible to calculate the flowrate between the two bubbles and how it varies with time .
infinite potential barrier – which is not of dirac delta form – will not allow tunneling is not quite right . a barrier where there is a finite region of infinite potential will not allow tunneling , nor will potentials with singularities going suffiently fast $\to \infty$ . but it is easy to construct a non-dirac potential with singularity that still permits tunneling ; in particular the one-dimensional singularities of the $\tfrac{1}{|r|}$ peaks as which you might model the nuclei 's coulomb potential are not much of a problem . you can basically model the ball 's cm amplitude as a bloch wave there . so , yes , a macroscopic ball can in fact tunnel through a brick wall . of course , the probability is exponentially small in the thickness , so indeed an enourmously large number of throws is required . more problematically , it is far more likely for the ball to , say , spontaneously disintegrate into two identically-shaped halfs , to develop a tight chemical connection to the wall , or perhaps catch fire .
use cgs units : grams and cm , as you asked .
you are right in that you do not need recombination to decouple and that hubble expansion is an important part of decoupling . decoupling basically happens when the interaction rate drops to the hubble expansion : $ \gamma = h$ in terms of the cross section : $\gamma = n_x &lt ; \sigma v&gt ; $ where $n_x$ is the number density , $\sigma$ is the interaction cross section , and $v$ is the speed of the particle . so , if there is expansion the cross section need not vanish for decoupling . also , even if the free electron number density did not drop precipitously due to recombination , the photons would decouple eventually . see modern cosmology by dodelson pg . 73 for example ( available in google books ) . dodelson states the free electron fraction drops to about 2% to get decoupling , so in some sense recombination had to proceed partially ( though not to completion ) to get decoupling . hope this answers your question .
one may always choose any coordinates on a spacetime manifold or any other manifold , for that matter . that is not only a simple mathematical insight but also a cornerstone of the general theory of relativity . in fact , gr starts with the postulate that all ( non-singular etc . ) coordinate systems are as good as any other coordinate systems and the basic laws of physics should have the same ( and equally simple ) form in all of them . this democracy is known as the " general covariance " and the fact that gr respects this principle is why it is called the general theory of relativity . in cosmology , we may want one of the coordinates to be a cosmic time $t$ whose constant value may specify the same moment in the whole universe – the same time since the big bang , as reflected e.g. in the local temperature which is a function of $t$ . when we add the assumption that $t$ , one of the coordinates is a cosmic time , we are adding some extra information about the space . anti de sitter space or de sitter space or other spacetimes may admit various slicings to " cosmic time " , even with different signs of the spatial curvature of the slices . what allows us to create an embedding and invoke arbitrary coordinates on it within it is ' larger ' minkowski space ? this question seems to implicitly assume that the larger minkowski space into which you embedded the anti de sitter space plays a physical role . but it does not play any role . it is nothing more than one of the many ways to visualize the anti de sitter space and its shape ( although arguably a particularly simple one ) . it is not a " real " space . the spacetime or the universe is everything there is ; so if the spacetime is an anti de sitter space , it means that there is no " larger space outside it " . and coordinate systems on anti de sitter space are not " obliged " to resemble the coordinates $x_0 , x_1 , \dots$ on your larger minkowski space . in fact , the physically natural and useful coordinate systems almost never do .
dear paulo , the space will be empty in the sense that the density of particles will be tiny ( the particles will include diluted gas as well as the hawking radiation from the black holes that will have evaporated ) . however , the universe will also be huge because its expansion will continue - and it will become the exponential expansion of the de sitter space ( because of the dark energy , also known as the cosmological constant , that has a constant positive energy density ) . it is only the total entropy of the system that is increasing and the growth of the total volume of the universe beats the decrease of the entropy density . there is another method to look at the entropy in the future . one may only look inside the " cosmic horizon " of the de sitter space that our universe is already becoming . this cosmic horizon - the maximally distant place where you can still see , if you wish - has a fixed radius and indeed , the entropy inside the cosmic horizon will go zero . in this description , however , the cosmic horizon itself carries a huge entropy equal to $$ s = a / 4a_0$$ where $a_0=\hbar g / c^3$ is the planck area , about $10^{-70}$ squared meters . the formula is identical to the bekenstein-hawking formula for a black hole . you may imagine that this entropy $s$ of the event horizon remembers the entropy of everything that may exist behind the cosmic horizon . for the value of the radius of the cosmic horizon which is determined by the cosmological constant in our universe , $s$ is approximately $10^{120}$ in natural units ( essentially bits of information , also equal to $10^{97}$ j/k or so ) that is about 20 orders of magnitude larger than the current entropy $10^{100}$ of the visible universe - the latter figure is dominated by the entropy of the large black holes in the middle of galaxies . ( before people knew about the large black holes and their entropy , they thought that most of the entropy of the current universe was carried by the cosmic microwave background - but the cmb entropy is roughly 10-20 orders of magnitude lower than the entropy of the black holes . ) the entropy of the universe will continue to increase but in some proper language , it will converge to $10^{120}$ which is the maximum allowed entropy of our universe . the latter statement - that the entropy of a de sitter can not exceed this bound equal to the inverse cosmological constant in the planck units - is a bit of the lore of quantum gravity . i suppose that brian cox did not want to explain this point because the tv program would become really confusing for his typical audiences , much like this comment inevitably will . whoever is confused by this bound should keep on assuming that $10^{120}$ is infinity and the entropy will continue to grow indefinitely .
a " quadripole " is a " four-terminal network " , if the currents through pairs of terminals are equal , it becomes a " two-port network " . if you post a schematic containing a quadripole , i could probably tell you if it is a two-port network or not . the equation for a two-port network is : $$ \begin{bmatrix}v_1\\v_2\end{bmatrix}= \begin{bmatrix}z_{11} z_{12}\\z_{21} z_{22}\end{bmatrix}\begin{bmatrix}i_1i_2\end{bmatrix} $$
to say the same thing david zaslavsky said in slightly different words , the second law implies that entropy cannot be destroyed , but it does not prevent you from moving it around from place to place . when we write the equation $\delta s = \int_a^b \frac{dq}{t}$ , we are assuming that this $dq$ represents a flow of heat into or out of the system from somewhere else . therefore $s$ ( which , by convention , represents only the entropy of some particular system ) can either increase or decrease . since we are talking about a reversible process , the entropy of some other system must change by an equal and opposite amount , in order to keep the total constant . that is , $\delta s + \delta s_\text{surroundings} = 0$ . one other thing : in thermodynamics , " closed " and " isolated " mean different things . " isolated " means neither heat nor matter can be exchanged with the environment , whereas " closed " means that matter cannot be exchanged , but heat can . in your question you say the second law " prohibits a decrease in the entropy of a closed system , " but actually this only applies to isolated systems , not closed ones . when we apply the equations above , we are not talking about an isolated system , which is why its entropy is allowed to change . i mention this because you said you are teaching yourself , and in that case it will be important to make sure you do not get confused by subtleties of terminology .
moment of inertia the definition of ( mass ) moment of inertia of a point mass is $$ i=r^2m $$ however in the real world you do not encounter point masses , but objects with non-zero volume ( finite density ) . and leads to an integral to determine moment of inertia $$ i=\int_m{r^2dm}=\int_v{\rho ( r ) r^2dv}=\int_x{\int_y{\int_z{\rho ( x , y , z ) ( x^2+y^2+z^2 ) dz}dy}dx} $$ the solutions of this integral of a few bodies , with constant non-zero density within geometric volume and zero density outside of it , can be found here . for example the moment of inertia of thin rod rotating around its center of mass is equal to $i=\frac{ml^2}{12}$ and for a solid cylinder $i=\frac{ml^2}{2}$ . experimental setup in your experimental a string is on one end connected to the hanging mass , lead over the pulley and then wounded around a drum ( the other end is also connected to the drum ) . this drum is of the object from which you would like to determine its moment of inertia and it is assumed that it can rotate freely ( without slip ) around its axis . according to your documentation you measure how far the pulley has rotated , i will call this angle $\theta$ , and its first and second time derivative $\omega=\dot{\theta}$ and $\alpha=\ddot{\theta}$ . the displacement of the hanging mass is related to the angular displacement of the pulley and its radius , $r_p$ , assuming that the string does not slip , so $$ s=r_p\theta $$ where $s$ is the vertical downward displacement of the hanging mass . this displacement is equal to the amount of string unrolled from the drum ( assuming that the string is not elastic ) , which means that the angular displacement of the object from which you would like to determine the moment of inertia , i will call this $\theta_i$ , can be calculated from this the other way around using the radius of the drum $r_d$ $$ s=r_p\theta=r_d\theta_i\rightarrow\theta_i=\frac{s}{r_d}=\frac{r_p}{r_d}\theta $$ this linear correlation also applies to the $\omega$ and $\alpha$ . the only force applied on this system ( which can perform work ) is gravity on the hanging mass . using all this you can derive the equation of motion ( possibly using free body diagrams and tension in the string ) .
as far as i can tell , despite the recent achievements the experimental toolbox in this field is quite limited . the two main techniques are photon-subtraction from squeezed vaquum and generation from fock states by conditional homodyne detection . first technique is based on the fact , that an odd cat state may be expressed as : $$\left ( \left|\alpha\right&gt ; -\left|-\alpha\right&gt ; \right ) \propto \alpha\left|1\right&gt ; +\frac{\alpha^3}{\sqrt{6}}\left|3\right&gt ; +\ldots , $$ which for small $\alpha$ resembles squeezed vacuum state with one photon removed . experimentally photon subtraction is realized with a low reflectivity beam splitter and single photon detection in the reflected port . detecting a single photon in this port heralds the preparation of the desired state . this is only applicable for states with small $\alpha$ - so called " schrödinger kittens " . these kittens may be later " breeded " on a beamsplitter to increase $\alpha$ . second technique uses homodyne detection of $p$ quadrature of a fock state $\left|n\right&gt ; $ splitted on a 50/50 beamsplitter to conditionally prepare cats . a detection of $p\sim0$ heralds the preparation of a cat state with $\alpha=\sqrt{n}$ . the closest to arbitrary superposition preparation is described here . this is not exactly a cat state , but a superposition of a squeezed vacuum and squeezed single-photon states of the form : $$\left|\psi\right&gt ; =\cos\theta \hat{s} ( r ) \left|0\right&gt ; +e^{i\varphi}\sin\theta \hat{s} ( r ) \left|1\right&gt ; , $$ with $\hat{s} ( r ) $ being the squeezing operator . it is probably the most general continious-variable superposition experimentally generated so far . i have to say that i do not specifically keep track of experiments in this field , that is just those which i heard of . as a good reference on experiemtal techniques i can recommend this review by lvovsky and raymer . this paper also contains a lot of references .
an electric field is said to be static if it does not change with time , i.e. the the charges that produced that field are stationary . this does not imply any constriction on its spatial dependence . in particular , no spherical symmetry is implicit in the definition of electrostatic field , and that field may not depend only on $r$ , as your example shows . this is common when you consider examples of field produced by more than one point charge , e.g. an electric dipole : $$\mathbf{e} ( \mathbf{r} ) ={3\mathbf{p}\cdot\hat{\mathbf{r}}\over 4\pi\varepsilon_0 r^3}\hat{\mathbf{r}}-{\mathbf{p}\over 4\pi\varepsilon_0 r^3}$$ depends on $\mathbf{r}$ and $\mathbf{p}$ .
ondřej 's answer is partially correct . in reality , you actually do have to worry about how far your eye is from the " eye lens " as the secondary lens system is called in a telescope . this is due to the need to match or over fill the eye 's entrance pupil with the telescopes exit pupil . if you do not do this , you will see a small circle where the telescope is actually working , and it can limit your field of view . you can also think about the telescope in the following way : the objective lens ( the first lens or lens system ) is forming an aerial image ( a really tiny one ) near the shared focal point . the eye lens ( the second lens or lens system ) is magnifying that image , in exactly the same way that you would use a simply magnifier to observe beetles or something . so , to select an eye lens first you want the most magnification which you can achieve without distortion or blurriness . you can do this by taking your eye lens system and looking at normal objects as if it were a magnifying glass . the calculation for the magnification of the eye lens is : $$ m_{\text{eye}} = \frac{250mm}{f_{\text{eye}}} $$ where $f_{\text{eye}}$ is the focal length of the lens ( or lens system ) and $250mm$ is the near point of the average relaxed human eye . the telescope magnification is then given by $$ m_{\text{telescope}} = -\frac{f_{\text{objective}}}{f_{\text{eye}}} $$ the second thing that you have to worry about is the exit pupil matching as i stated above . a galilean telescope will always have a narrower field of view and a little circle that you can see . for a reflective or keplerian telescope , you can add field lenses to increase the field of view . this is basically what huygens or ramsden eyepieces do . since you want your eye 's field of view to be maximized , you can do this by making sure that your exit pupil is located a reasonable distance outside of the telescope system . you will have to do calculations to find this distance , i suggest buying grievenkamp 's field guide to geometrical optics for a complete description of first order optics and a decent account of aberrations .
in general , invariance of the action under some transformations , such as lorenz rotations , is an extra condition that we impose on a theory . this is not related to the extreme condition that just gives trajectories along which the classical evolution goes . in your case , i guess , there is some confusion in formulation of the idea . according to the 1st postulate of special relativity , all physical laws are the same in any inertial frame ( related by a lorenz transformation ) . to realise this theoretically , we need to make our lagrangian invariant under such transformations . then it will give the same physics in all frames . hence , one should impose some extra condition to satisfy the 1st postulate .
i do not have an answer to the question " why would one want to consider such crazy stuff in physics ? " since i do not know much physics , but as a mathematics student i do have an answer to the question " why would one want to consider such crazy stuff in mathematics ? " what physicists call grassmann numbers are what mathematicians call elements of the exterior algebra $\lambda ( v ) $ over a vector space $v$ . the exterior algebra naturally arises as the solution to the following geometric problem . say that $v$ has dimension $n$ and let $v_1 , . . . v_n$ be a basis of it . we would like a nice natural definition of the $n$-dimensional volume of the paralleletope defined by the vectors $\epsilon_1 v_1 + . . . + \epsilon_n v_n , e_i \in \{ 0 , 1 \}$ . when $n = 2$ this is the standard parallelogram defined by two linearly independent vectors , and when $n = 3$ this is the standard paralellepiped defined by three linearly independent vectors . the thing about the naive definition of volume is that it is very close to having really nice mathematical properties : it is almost multilinear . that is , if we denote the volume we are looking at by $\text{vol} ( v_1 , . . . v_n ) $ , then it is almost true that $\text{vol} ( v_1 , . . . v_i + cw , . . . v_n ) = \text{vol} ( v_1 , . . . v_n ) + c \text{vol} ( v_1 , . . . v_{i-1} , w , v_{i+1} , . . . v_n ) $ . you can draw nice diagrams to see this readily . however , it is not actually completely multilinear : depending on how you vary $w$ you will find that sometimes the volume shrinks to zero and then goes back up in a non-smooth way when really it ought to keep getting more negative . ( you can see this even in two dimensions , by varying one of the vectors until it goes past the other . ) to fix that , we need to look instead at oriented volume , which can be negative , but which has the enormous advantage of being completely multilinear and smooth . the other major property it satisfies is that if any of the two vectors $v_i$ agree ( that is , the vectors are linearly dependent ) then the oriented volume is zero , which makes sense . it turns out ( and this is a nice exercise ) that this is equivalent to oriented volume coming from a " product " operation , the exterior product , which is anticommutative . formally , these two conditions define an element of the top exterior power $\lambda^n ( v ) $ defined by the exterior product $v_1 \wedge v_2 . . . \wedge v_n$ , and choosing an element of this top exterior power ( a volume form ) allows us to associate an actual number to an $n$-tuple of vectors which we can call its oriented volume in the more naive sense . if $v$ is equipped with an inner product , then there are two distinguished elements of $\lambda^n ( v ) $ given by a wedge product of an orthonormal basis in some order , and it is natural to pick one of these as a volume form . alright , so what about the rest of the exterior powers $\lambda^p ( v ) $ that make up the exterior algebra ? the point of these is that if $v_1 , . . . v_p , p &lt ; n$ is a tuple of vectors in $v$ , we can consider the subspace they span and talk about the $p$-dimensional oriented volume of the paralleletope given by the $v_i$ in this subspace . but the result of this computation should not just be a number : we need a way to do this that keeps track of what subspace we are in . it turns out that mathematically the most natural way to do this is to keep in mind the requirements we really want out of this computation ( multilinearity and the fact that if the $v_i$ are not linearly independent then the answer should be zero ) , and then just define the result of the computation to be the universal thing that we get by imposing these requirements and nothing else , and this is nothing more than the exterior power $\lambda^p ( v ) $ . this discussion hopefully motivated for you why the exterior algebra is a natural object from the perspective of geometry . since einstein , physicists have been aware that geometry has a lot to say about physics , so hopefully the concept makes a little more sense now . let me also say something about how modern mathematicians think about " space " in the abstract sense . the inspiration for the modern point of view actually derives at least partially from physics : the only thing you can really know about a space are observables defined on it . in classical physics , observables form a commutative ring , so one might say roughly speaking that the study of commutative rings is the study of " classical spaces . " in mathematics this study , in the abstract , is called algebraic geometry . it is a very sophisticated theory that encompasses classical algebraic geometry , arithmetic geometry , and much more , and it is in large part because of the success of this theory and related commutative ring approaches to geometry ( topological spaces , manifolds , measure spaces ) that mathematicians have gotten used to the slogan that " commutative rings are rings of observables on some space . " of course , quantum mechanics tells us that the actual universe around us does not work this way . the observables we care about do not commute , and this is a big issue . so mathematically what is needed is a way to think about noncommutative rings as " quantum spaces " in some sense . this subject is very broad , but roughly it goes by the name of noncommutative geometry . the idea is simple : if we want to take quantum mechanics completely seriously , our spaces should not have " points " at all because points are classical phenomena that implicitly require a commutative ring of observables , which we know is not what we actually have . so our spaces should be more complicated things coming from noncommutative rings in some way . grassmann numbers satisfy one of the most tractable forms of noncommutativity ( actually they are commutative if one alters the definition of " commutative " very slightly , but never mind that . . . ) , and even better it is a form of noncommutativity that is clearly related to something physicists care about ( the properties of fermions ) , so anticommuting observables are a natural step up from commuting observables in order to get our mathematics to align more closely with reality while still being able to think in an approximately classical way .
if the air in the bubble is expanding , the gas should be doing work against the ocean , and the temperature should drop ( in the case of adiabatic expansion ) . since the temperature is said to remain constant , heat must be added to the bubble .
there are two immediate problems with this idea . first , the acceleration due to the dark energy appears to be the same in all directions . in general relativity ( and newtonian gravity for that matter ) the influence of distant matter can only cause a tidal acceleration that expands matter in some directions and compresses it in others . a uniform expansion has to be due to something present locally . second , gravitational influences travel at the same speed as light , which means we can see the most distant matter that could gravitationally influence any of the other matter we can see . what we actually see ( in the cosmic microwave background ) is remarkable uniformity in all directions , to within about 0.01% . so there is no density fluctuation close enough to have an effect . dark energy is not as strange as you think ; it is actually quite natural in quantum field theory , and the biggest mystery is why the dark energy is so small , not that it exists . and modern big bang cosmology starts with inflation , not with a singularity .
honestly , i think this is one of those cases where you should just accept it and push on . this ' derivation ' is really nothing more than a pedagogical device to make field theory seem somewhat natural to students with a background in classical mechanics . what we are trying to do is to take the continuum i.e. $n\to \infty$ limit of the following lagrangian : $$l_n=\frac{1}{2} \biggl ( \sum_{i=1}^n\delta x \frac{m}{\delta x} \dot{\phi_i}^2-\sum_{i=1}^{n-1}\delta x\ k\delta x \biggl [ \frac{\phi_{i+1}-\phi_i}{\delta x}\biggr ] ^2\biggr ) $$ define $\mu=\frac{m}{\delta x}$ and $y=k\delta x$ clearly , for a continuum limit , we get infinitely many particles , so the total kinetic energy of the system should diverge . . . unless we impose ( or put in by hand , as they call it ) , that $\mu$ remains constant , not $m$ . similarly , it is obvious that the equilibrium force of each spring $f=k \delta x$ should vanish . . . unless we impose that $k\delta x$ is constant when we take our limit . with these ad-hoc assumptions , and replacing the discrete index $i$ with a continuous spatial coordinate , we get $$l\equiv \lim_{n\to \infty}l_n=\frac{1}{2}\int_0^l \mathrm{d}x \biggl ( \mu\dot \phi^2 -y ( \nabla\phi ) ^2\biggr ) $$ this gives us the right action for a free , massless , scalar field \begin{align*}s [ \phi ] and =-\frac{y}{2}\int_0^l \mathrm{d}x\ \mathrm{d}t \biggl ( -\frac{\mu}{y}\dot \phi^2+ ( \nabla \phi ) ^2\biggr ) \\ and =-\frac{\mu c^2}{2} \int_0^l \mathrm{d}x\ \mathrm{d}t \biggl ( -\frac{1}{c^2} ( \partial_t\phi ) ^2+ ( \nabla\phi ) ^2\biggr ) \hspace{2cm}c=\sqrt{\frac{y}{\mu}}\\ and =-\mu c^2\int_0^l\mathrm{d}^2x\ \frac{1}{2}\eta^{\mu\nu}\partial_\mu\phi\partial_\nu\phi\end{align*} the definition of $c$ is the standard one for the speed of longitudinal waves , and as one can see this lagrangian is also reminiscent of the action for a relativistic point particle ( especially the prefactor ) . this is , of course , a very nice result , so we can be happy about the way we took our limit , even if we had to make some ad-hoc assumptions .
first , note that this comes from the heisenberg uncertainty principle , $$ \delta x\delta p\geq \frac\hbar2 $$ where $\hbar\approx10^{-34}$ j$\cdot$s ( i.e. . , a very small number ) . this is a constraint on the simultaneous measurements of momentum and position . if you know the position of the coin , then it can not actually be anywhere else because it is measured to be there . next , i quote sean carroll : quantum mechanics features a " classical limit " in which objects behave just as they would had newton been right all along , and that limit includes all of our everyday experiences . for objects such as cats that are macroscopic in size , we never find them in superpositions of the form "75 percent here , 25 percent there" ; it is always "99.9999999 percent ( or much more ) here , 0.0000001 percent ( or much less ) there . " classical mechanics is an approximation to how the macroscopic world operates , but a very good one . the real world runs by the rules of quantum mechanics , but classical mechanics is more than good enough to get us through everyday life . it is only when we start to consider atoms and elementary particles that the full consequences of quantum mechanics simply can not be avoided . for your coin , we can describe it correctly by classical mechanics ( meaning we can measure its position and momentum simultaneously ) , so there is no need to invoke quantum mechanics in regards to this thought experiment .
in the normal usage , real and virtual are not properties of feynman diagrams themselves , but of the particles depicted in them . the particles corresponding to external lines ( attached to at most one vertex only ) are real , the others ( attached to two vertices ) are virtual . a feynman diagram may be considered as a repetitive part of a bigger diagram . this requires that the external lines of the diagram are kept off-shell , so that the corresponding integrals depend on off-shell momenta ( rather than only on-shell momenta , which would suffice for s-matrix elements ) . their computation is more complex as one has to account for a bigger parameter space . the use of off-shell feynman diagrams in this sense is that they can be used in resummation techniques as building blocks of infinite families of on-shell feynman diagrams . indeed , such a recursive usage necessitates treating the external lines as virtual . as with virtual particles , such off-shell feynman diagrams have no measurable counterpart but are just intermediate expressions in the resummation calculation . however , in papers such as http://www.sciencedirect.com/science/article/pii/0550321379901160 , a virtual feynman graph is simply an ordinary feynman graph involving a loop , in contrast to a real feynman graph = lowest order tree graph . see the explanation after eqs . ( 42 ) and ( 81 ) . this paper is quoted in http://arxiv.org/abs/1012.0507 after eq . ( 3 ) for details about virtual diagrams . in http://arxiv.org/abs/1112.1061 , this terminology applies to the graphs cut at the lines in fig . 1 , corresponding to a factorization . http://arxiv.org/abs/hep-ph/9701284 also uses this terminology ; see the titles of sections 3.1 and 3.2 , and the corresponding figures .
i ) well , gaussian integrals $$\tag{1} \int_{\mathbb{r}^n} \ ! d^n x ~e^{-\frac{1}{2} x^t a x} ~=~ \sqrt{\frac{ ( 2\pi ) ^n}{\det a}}$$ are easy to calculate exactly , where the matrix ${\rm re} ( a ) $ is positive definite . ii ) but if op just wants to confirm that the power $p$ of the determinant $\det a$ on the rhs . of eq . ( 1 ) is $p=-1/2$ ( as opposed to some other power $p$ ) , then indeed one may use dimensional analysis . if the integration variables $x^i$ have dimension of length $ [ x^i ] =l$ , then the matrix elements $a_{ij}$ have dimension $ [ a_{ij} ] =l^{-2}$ to keep the argument of the exponential dimensionless . therefore $\det a$ has dimension $ [ \det a ] =l^{-2n}$ . moreover both sides of eq . ( 1 ) must have dimension $l^n$ . hence the power $p=-1/2$ of the determinant $\det ( a ) $ .
if you know the rotational kinetic energy then cut the power and time how long it takes for the motor to come to a stop . divide the kinetic energy by this time and you have the power dissipation as friction . the difference between this and the input power is then the power dissipated though non-frictional mechanisms . whether this is related to efficiency is debatable as the motor is not doing any useful work so the efficiency is 0% . all your doing is splitting up the energy dissipation into frictional and non-frictional parts . it is still an interesting experiment though .
if you have a smart phone with a flash , you probably have a strobe feature at your disposal . you can put a drinking straw into it and record the sound it makes , and measure the frequency in sound-editing software . imagine the fun explaining this to paramedics . similarly , a laser-pointer , a mirror , and a photo-diode cleverly hooked up to your computer 's sound card can be coaxed into doing science . finally , a laser pointer , two mirrors and knowledge of the speed of light in your room can be used to to determine the rotational speed of the fan , provided your room is big enough , and the fan is moving fast enough . if this is going to work , it will probably be the least accurate but most fun .
here is the proof that i think you are looking for . as ali remarks in his answer , the results holds true for a rigid body undergoing rotation with constant angular velocity . let $\vec r_i$ denote the position of some particle in a rigid body . suppose this rigid body is undergoing rotation with angular velocity $\vec \omega$ , then $$ \dot {\vec r}_i = \vec \omega\times\vec r_i $$ see the appendix for a proof of this . by taking the derivative of both sides with respect to time and multiplying both sides by $m_i$ , the mass of particle $i$ , we obtain $$ \dot {\vec p}_i = \omega\times \vec p_i $$ now we simply note that if $\vec f_i$ denotes the net force on particle $i$ , then newton 's second law gives $\vec f_i = \dot{\vec p_i}$ so that \begin{align} \vec\tau_i and = \vec r_i\times \vec f_i \\ and = \vec r_i\times\dot{\vec p_i} \\ and = \vec r_i\times ( \vec\omega\times\vec p_i ) \\ and = -\vec p_i\times ( \vec r_i\times \vec\omega ) - \vec\omega\times ( \vec p_i\times\vec r_i ) \\ and = \vec p_i\times ( \vec\omega\times\vec r_i ) + \vec\omega\times ( \vec r_i\times\vec p_i ) \\ and = \vec p_i\times \dot{\vec r}_i + \vec \omega\times \vec l_i \\ and = \vec\omega\times \vec l_i \end{align} this is basically the identity you were looking for . in the fourth equality , i used the so-called jacobi identity . now , by taking the sum over $i$ , the result can readily be seen to also hold for the net torque $\tau$ on the body and the total angular momentum $\vec l$ of the body ; $$ \vec \tau = \vec\omega\times\vec l $$ appendix . the motion of a rigid body undergoing rotation is generated by rotations . in other words , there is some time-dependent rotation $r ( t ) $ for which $$ \vec r ( t ) = r ( t ) \vec r ( 0 ) $$ it follows that $$ \dot{\vec r} ( t ) = \dot r ( t ) \vec r ( 0 ) = \dot r ( t ) r ( t ) ^t\vec r ( t ) = \vec\omega ( t ) \times \vec r ( t ) $$ in the last step , i used the fact that $r ( t ) $ is an orthogonal matrix for each $t$ which implies that $\dot r r^t$ is antisymmetric . it follows that there exists some vector $\vec \omega$ , which we call the angular velocity of the body , for which $\dot r r^t \vec a = \vec \omega\times\vec a$ for any $\vec a$ .
you are right , in this case , scalar means lorentz invariant field . but it is not invariant under the transformations of su ( 2 ) xu ( 1 ) of the electroweak model . and it is a scalar under the su ( 3 ) of qcd . so the four real components of the higgs are indeed invariant under space-time transformations . physicists are usually not very clear in these distinctions , and you have to guess under which transformation the field is a scalar .
in particle physics the core tool for this purpose these days is root ( a few years back there were still a significant number of people using paw ( part of cernlib ) ) . both of these choices suffer somewhat from being big , heavy tools to install just to get some graphing done---we use them because they are primarily the environments in which we did analysis . a lighter and less specific ( but still surprisingly capable ) tool is gnuplot . i have also seen a lot of activity on stack overflow from people using python based tools like scipy for plotting . root also provides python bindings .
$e^{i\theta} + e^{-i\theta}$ is just $2\cos \theta$ . the superposed wavefunction is $$\psi ( x , t ) = 2n\cos ( ax ) e^{i ( f ( x ) + \omega t ) }$$ then $$\psi^*\psi = 4n^2\cos^2 ( ax ) $$ the average height is $2n^2$ if $x_0a = n\pi/2$ , in which case $n = \frac{1}{2}\sqrt{1/x_0}$ . otherwise you can do this integral .
the proof is most easiest if we use the vector notation . we have $$\vec \tau = \int {d\vec \tau } = \int { ( \vec r \times dm\vec g ) } = \left ( {\int {\vec r dm} } \right ) \times \vec g$$ where i have used the assumption that near the earth $\vec g$ is constant . now according to the definition of center of mass we have , $${{\vec r}_{cm}} = \frac{1}{m}\int {\vec rdm}$$ therefore , we find $$\vec \tau = m{{\vec r}_{cm}} \times \vec g = {{\vec r}_{cm}} \times m\vec g$$ which is the desired result .
not sure why the word " phase " is in the name of this function . the intensity of unpolarized light scattered by rayleigh particles as a function of detector angle $\theta$ is proportional to this function . colloidal scientists call this measurement " static light scattering , " but atmospheric scientists and others may have different names for it . you can get some physical intuition for this function by picturing an em wave interacting with a point scatterer . imagine sweeping a detector in a circle of a radius $r$ centered on the scatterer , such that the incident light is in the plane of this circle . then place an analyzer ( polarizer ) in front of the detector so you are detecting only either vertically or horizontally polarized light . if the incident light is vertically polarized , i.e. polarization normal to the plane of the circle , the amplitude ( and intensity ) of scattered light is isotropic . if the light is horizontally polarized , i.e. ( polarization in the plane of the circle ) , the intensity detected depends on detection angle . convention is to call $\theta=0$ degrees the angle of forward scattered ( also unscattered ) light . that way , $\theta=180$ is the angle where we detect backscattered light , and $\theta=90$ and $\theta=270$ are each right in between those two " poles " . imagine horizontally polarized radiation interacting with our point scatterer . this polarization leads to maximum scattered intensity at $\theta=0$ and $\theta=180$ , and minimum of zero scattered intensity at $\theta=90$ ( and $\theta=270$ ) . you can use the right-hand rule and dot products to convince yourself of this . let your fingers point in the direction of the polarization , and your thumb point in $\hat{k}$ , the direction in which the scattered photon is propagating . you can let your thumb point in any direction around the circle ; imagine that your thumb points toward the detector , so it determines which $\theta$ you are analyzing . if your fingers are in the plane of the circle you are looking at horizontally polarized light . try to put your fingers in the plane and point your thumb at $\theta=90$ . you should find that your fingers are pointing in a direction that is orthogonal to the horizontal polarization of the incident light , which means none of such light will scatter in this direction . the $cos^2 ( \theta ) $ term characterizes this angular dependence of horizontal polarization , and the $1$ term states the isotropy of the vertically polarized radiation . we have to add these for unpolarized incident light . this general dependence is true for dilute solutions of rayleigh scatterers , or gases . the intensity measurement comes from counting a large number of photons at each angle . the function comes from classical optics/electromagnetism . i suspect it could also be derived from a quantum mechanical ( qm ) treatment of photons . qm may be better suited for determining the " angles at which the scattering is more likely to happen " as you put it , since we think of the classical picture of as purely deterministic .
extra-dimensional scenarios may be described as " inspired " by string theory but they are independent hypotheses and they may be true even if string theory is not . however , one has to reduce the ambitions and standards of consistency . sociologically , it is surely true that the research of models with extra dimensions has been adopted and pursued by many people who have never take studied proper string theory or taken a course in it . despite the academic independence , a confirmation of experimentally accessible extra dimensions - which is extremely unlikely to occur , due to their likely tiny size - would be a huge evidence supporting string theory because it is the only framework in which the extra dimensions actually have a justification ( many ) .
1 ) everything op writes ( v1 ) above his last equation is correct . the $bc$ ope reads $$ {\cal r}c ( z ) b ( w ) ~\sim~ \frac 1{z-w} , $$ where ${\cal r}$ denotes radial ordering . 2 ) to calculate the two-point function $$\langle c ( z ) b ( w ) \rangle $$ ( which as op writes must vanish if the conformal dimensions for $b$ and $c$ are different ) is more subtle due to the presence of the ghost number anomaly , i.e. the vacuum should be prepared with certain modes of the $bc$ system , see e.g. polchinski , string theory , vol . 1 , sections 2.5-2.7 .
negative resistance is not uncommon . you see it in arc lamps , too . for your motor ( used as a generator ) , i would guess that it is most efficient at higher current , possibly having field coils and not permanent magnets . the voltage from a generator comes from moving a wire through a magnetic field . with field coils in series , the magnetic field is generated by the current flowing through the ( generator ) . there is probably a small residual permanent field in the iron , so you get some voltage even when there is no current . i bet if you could run up to higher currents , eventually you would get max power from the generator , and start seeing voltage go down again . i am going on this being a ' universal ' motor you have salvaged from somewhere . read about universal or series wound motors at the motor wiki : http://en.wikipedia.org/wiki/electric_motor i found a discussion group with one sensible ( to my mind ) answer amongst the cruft ( see engineertony 's reply ) here : http://cr4.globalspec.com/thread/77573/how-to-make-a-generator-from-a-universal-motor my own take is , go for it ! it kinda works , it looks like you can get real power from it , though the voltage is all over the map , you will want to regulate it .
the main point is that goldstein is not saying we must exclude friction forces in our treatment , but we must place them in the tally of applied forces ( that we keep track of in d'alembert 's principle ) and not in the other bin of the remaining forces , see this and this phys . se posts . of course , there does not exist a generalized potential $u$ for the friction forces ${\bf f}=-k {\bf v}$ , only the rayleigh dissipation function , see this phys . se post and this mathoverflow post .
radiation comes from the verb " radiate " and , as the wiki article linked says is used for describing two different physical phenomena . both have in common that a " source " is radiating energy ( radiation ) whose effects appear at a distance from the source . 1 ) electromagnetic radiation . this includes from radar and radio waves , to visible light , to x-rays and gamma rays . it is emitted from a source according to t he electromagnetic equations where there must be a varying electric or magnetic field at the source , with the corresponding frequency of the wave so as to generate it . in the case of radio and radar waves the source is a metallic rod or system ( antenna ) where the electric field is varied across it , and the antenna radiates a wave of energy as seen in the answer given by @mhodel above . to see this radiation one needs a receiver which will absorb the electromagnetic wave and decipher the signals it may be carrying . visible light appears from incandescent material as the sun , or various lamps . the antennas/sources are the tiny releases of energy of a zillion microscopic electric charges moving in the electric and magnetic field of each other or released by transitions from bound energy states due to the motions of the atoms and molecules at the high temperatures necessary for incandescence . all masses of matter radiate electromagnetic radiation according to their temperature , black body radiation , but most of the radiation is in the invisible part for our eyes : infrared ( we sense it as heat on our skin ) and ultra violet ( turns our skin black by reacting with melanin in it ) radiation . x-rays are of even higher frequency than ultraviolet , generated by charges moving in very strong electric fields . they are invisible except when reacting on matter , film , etc , and long exposures are dangerous for living things . gamma rays come from even higher energy sources , nuclear sources in transitions of energy states of nuclear matter . electromagnetic radiation is invisible in air and when falling on matter it is either absorbed or reflected . 2 ) massive particles as radiation . when first the radioactive sources were discovered the " action at a distance from the source " seemed to the observers similar to the radiation given off by electromagnetic sources and the description " radiation " was given to describe the " action through air'of the energy emitted by radium and other sources . it was later found out that in addition to gamma rays , energy was also emitted as small charged particles which did not interact with air enough as to be visible but could leave a track in a denser medium and thus reveal their mass and charge . these are described by the answer of @wayfarer . thus radiation is an inclusive attribute of two different types of energy transfers through space , observed in nature and used by us in various situations .
since radioactivity is a random process , you had expect some fluctuation in the number of decays , i.e. if you wait for the half life , there is no guarantee that exactly half of the nuclei will have decayed ! based on your 3 estimates of the half life , you could just take the mean and go with that . and how to find the isotope ? there is lists for that . i just googled " half lives of radioactive isotopes " and , voila : http://en.wikipedia.org/wiki/list_of_radioactive_isotopes_by_half-life
the invariant process is the speed of light $c$ which should be ( measured ) the same for every observer in relative motion . and indeed einstein 's 1905 paper derives from first principles , the lorentz transformations ( i.e. the transformations of special relativity ) using ( what some would derogatorily say ) ' high-school mathematics ' . yet the concepts behind these simple relations is the whole point . if one wants to characterise the geometry of minkowski space in terms of f . klein 's erlangen program of groups and invariants , then the geometry of special relativity is exactly this geometry which leaves the lorentz metric invariant ( using a signature of $ ( + , - , - , - ) $ ) : $$ds^2 = ( cdt ) ^2-dx^2-dy^2-dz^2$$ this is exactly analogous to the euclidean geometry as the geometry which leaves the quadratic metric invariant ( e . g in 4 dimensions , $x , y , z , w$ ) : $$ds^2 = dx^2 + dy^2 + dz^2 + dw^2$$
the most important physics with respect to cloud formation happens in what is called the atmospheric boundary layer ( abl ) . a lot of research is done in this field , since the effects of clouds is the major source of incertainty in all climate prediction models . to get some sort of cloud formation , in my opinion you would need to have some kind of abl inside of your dome . the typical height of this abl is ~800m ( in the morning ) up till 2km ( in the evening ) . so if your dome is that large , you will be able to form clouds . but it is not that easy : heat fluxes ( representing radiation ) , temperature gradient , humidity , pressure , condensation , everything should be controlled in the dome to spontaneously form these clouds . another aspect to be aware of is the size of the base of the dome . this has to be larger than the largest turbulent structures in this domain , so to be sure to let these structures develop , it should be at least a few times the height of the dome . if this dome is too large for you , it is probably possible to make clouds in a smaller dome , but then you still need to capture the abl . to achieve this , you need to change the conditions : increase temperatures and temperature gradients , replace the air by some other fluid , etc etc . it will in no case be pleasant for people . actually , some experimental physicists have been able to model this abl in a water tank ( ~1m³ ) . however , cloud formation was off course impossible to achieve in such a experimental set-up .
what follows is a very rough adaption of chapter 25 in gravitation by misner , thorne , and wheeler . begin with the schwarzschild metric with polar angle $\theta$ fixed at $\pi/2$: $$ ds^2 = -\left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \mathrm{d}t^2 + \frac{1}{1-r_\mathrm{s}/r} \mathrm{d}r^2 + r^2 \mathrm{d}\phi^2 . $$ for a test particle of rest mass $m$ , 1 we know by definition $$ g_{\mu\nu} p^\mu p^\nu + m^2 = 0 , $$ where $\vec{p}$ is the 4-momentum of the particle . for an affine parameter $\lambda$ parametrizing the worldline of the particle , these two equations can be combined to give $$ -\left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{\mathrm{d}t}{\mathrm{d}\lambda}\right ) ^2 + \frac{1}{1-r_\mathrm{s}/r} \left ( \frac{\mathrm{d}r}{\mathrm{d}\lambda}\right ) ^2 + r^2 \left ( \frac{\mathrm{d}\phi}{\mathrm{d}\lambda}\right ) ^2 + m^2 = 0 $$ now the derivative in the first term is simply the energy $e$ , which is related to the conserved energy at infinity $e_\infty$ by $e = e_\infty/ ( 1-r_\mathrm{s}/r ) $ . furthermore , the definition of angular momentum is $l = r^2 ( \mathrm{d}\phi/\mathrm{d}\lambda ) $ , which is also conserved . inserting these definitions gives $$ -\frac{e_\infty^2}{1-r_\mathrm{s}/r} + \frac{1}{1-r_\mathrm{s}/r} \left ( \frac{\mathrm{d}r}{\mathrm{d}\lambda}\right ) ^2 + \frac{l^2}{r^2} + m^2 = 0 , $$ or $$ \left ( \frac{\mathrm{d}r}{\mathrm{d}\lambda}\right ) ^2 = e_\infty^2 - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{l^2}{r^2} + m^2\right ) . $$ using again the definition of $l$ , we can write $$ \left ( \frac{\mathrm{d}r}{\mathrm{d}\phi}\right ) ^2 = \frac{r^4}{l^2} \left ( e_\infty^2 - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{l^2}{r^2} + m^2\right ) \right ) , $$ which we can rewrite to be \begin{align} \left ( \frac{\mathrm{d}r}{\mathrm{d}\phi}\right ) ^2 and = r^4 \frac{e_\infty^2-m^2}{l^2} \left ( \frac{e_\infty^2}{e_\infty^2-m^2} - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{1}{r^2} \left ( \frac{l^2}{e_\infty^2-m^2}\right ) + \frac{m^2}{e_\infty^2-m^2}\right ) \right ) \\ and = \frac{r^4}{b^2} \left ( \frac{e_\infty^2}{e_\infty^2-m^2} - \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \left ( \frac{b^2}{r^2} + \frac{m^2}{e_\infty^2-m^2}\right ) \right ) , \end{align} where $$ b = \frac{l}{\sqrt{e_\infty^2-m^2}} $$ is the impact parameter , defined to be the ratio of angular to linear momentum . at this point , we have a nice general result , but to apply it to photons we take the limit $m \to 0$ , which gives $$ \left ( \frac{\mathrm{d}r}{\mathrm{d}\phi}\right ) ^2 = \frac{r^4}{b^2} \left ( 1 - \frac{b^2}{r^2} \left ( 1 - \frac{r_\mathrm{s}}{r}\right ) \right ) . $$ the radius of closest approach will be the value $r = r_\text{min}$ for which $\mathrm{d}r/\mathrm{d}\phi$ vanishes : \begin{gather} \frac{r_\text{min}^4}{b^2} \left ( 1 - \frac{b^2}{r_\text{min}^2} \left ( 1 - \frac{r_\mathrm{s}}{r_\text{min}}\right ) \right ) = 0 ; \\ \frac{b^2}{r_\text{min}^2} \left ( 1 - \frac{r_\mathrm{s}}{r_\text{min}}\right ) = 1 ; \\ b^2 ( r_\text{min} - r_\mathrm{s} ) = r_\text{min}^3 . \end{gather} the only thing left is to decide what $r_\text{min}$ is allowed to be if the light is to escape . since at this point in the trajectory the photon 's velocity is entirely in the tangential direction by construction , the question becomes how close in can a photon have a circular orbit ? the answer is $r_\text{min} = 3r_\mathrm{s}/2$ . thus $b_\text{max}$ , the maximum impact parameter for a photon to be captured , obeys $$ b_\text{max}^2 \left ( \frac{3}{2} r_\mathrm{s} - r_\mathrm{s}\right ) = \left ( \frac{3}{2} r_\mathrm{s}\right ) ^3 , $$ or $$ b_\text{max}^2 = \frac{27}{4} r_\mathrm{s}^2 . $$ this is exactly what we sought , since it immediately tells us $$ \sigma = \pi b_\text{max}^2 = \frac{27}{4} \pi r_\mathrm{s}^2 . $$ 1 we could set $m = 0$ here , but by holding off we get slightly more general intermediate results .
in the original formula we have that $$\omega_1t-k_1x = a$$say , and $$\omega_2t-k_2x = b$$say , by hypothesis for a specific point at time $t$ and position $x$ . this is a point of constant phase ( for the $a$ wave and $b$ wave respectively . ) to determine the velocity of a ( sine or cosine ) wave from first principles one wants to know the velocity of that point : how far does a point of constant phase move in time t ? this gives the answer of the phase velocity $$ v_p=\frac{\omega}{k} $$ so the component waves are moving with phase velocities : $v_{p1}$ and $v_{p2}$ respectively . using the above values for $a$ and $b$ the middle derivation is an application of the trigonometric identity : $$ sin a + sin b = 2 sin ( 1/2 ( a+b ) ) \cdot cos ( 1/2 ( a-b ) ) . $$ this gives your expression for $s = s ( x , t ) $ . so how can one talk about a point of constant phase here to obtain the group velocity as it is a product of sin and cos ? well the trick indeed is to recognise the two separate wave components and treat these ( for now ) as two separate waves and calculate their ( phase ) velocity - ie the rate of movement of points of constant phase in each " wave " . for the sine wave ie the envelope we would get $\frac{\overline{\omega}}{\overline{k}}$ . now for the cosine wave . the short answer is that we are looking for its phase velocity also , namely $\frac{\delta{\omega}}{\delta k}$ . however what your professor has done here , is to calculate from first principles the velocity of that cosine wave . that is to ask for the definition of a point of constant phase , viz : $\frac{\delta{\omega}}{2}t - \frac{\delta k}{2} x = const$ and then to determine the velocity ( by differentiation , etc ) of this point , again resulting in $$ v_g=\frac{\delta\omega}{\delta k} $$
a push up is a form of lever . the athlete must exert roughly half her body weight ( under some assumptions i will clarify at the end of the post . ) we can solve this problem using the principle of virtual work . assume the athlete raises her body through a small angle $\textrm{d}\theta$ . then her center of mass rises by $l \cos\theta \ \textrm{d}\theta$ , with $l$ the distance from her feet to her center of mass . the work done is $$\textrm{d}w = mgl\cos\theta \ \textrm{d}\theta$$ this work is equal to the force multiplied by the distance over which the force is exerted . if we call the distance from her feet to her shoulders $l$ , then $$mgl\cos\theta \ \textrm{d}\theta = f l \cos\theta \ \textrm{d}\theta$$ $$f = \frac{mgl}{l}$$ your center of mass is roughly half way up your body , so a push up requires you to lift about half your weight . this answer assumes the pushup is infinitely slow ( i.e. . no acceleration ) . this is actually not so bad an approximation as it sounds , but people doing fast push ups will probably exert a higher force at the bottom of the push up , then let gravity do negative work to slow them down as they near the top . this is the basis for " clapping push ups " ( which i am too weak to perform ) . we have only calculated the component of force in the direction of motion , so , we are assuming the athlete pushes directly in the direction of motion of her shoulders . this means she does not push along the red line in your picture , but diagonally ahead of it . in reality , she might push more straight down , increasing the force and decreasing the angle through with her elbows flex . the force may actually change throughout the push up due to this effect . if you lean into the arm of a couch and do a push up off that , you will find it is much easier at this high angle . the reason is you are not pushing straight down any more , so it is a little bit like going up a ramp ( in that the distance over which the force is exerted is lengthened to accomplish the same amount of work ) . i also assumed the athlete 's center of mass does not move , which means i am pretty much thinking of her arms as massless , and the rest of her is a rigid board . finally , i am assuming her hands and feet do not slip , the floor is too big to move , etc . an empirical way to determine the force would simply be to put scales underneath the hands , but the reading could be a little off depending on whether the scales are constructed to measure total force on them , or only the vertical component of the force . finally , an actual human is not simply one force pushing in one direction . there are muscles , bones , ligaments , etc . all sorts of different forces are going on in different places . this calculation gives the force that the arms exert on the main body . the force that the triceps exert on the elbow , for instance could be much higher .
assuming cooling is mainly by convection , the cooling will be described by newton 's law of cooling . this states that the rate of temperature change is proportional to the temperature difference , and result is that the difference between the temperature of your object and the room decays exponentially : $$ t_{room} - t_{object} = ( t_{room} - t_{0} ) e^{-kt} $$ where $t_{0}$ is the initial temperature of your object and $k$ is some constant . you can take the temperature of the object as a function of time and then fit the expression above , but the problem is that this type of fit gives rather large errors in the final temperature $t_{room}$ unless you measure for long enough that you have almost reached the final temperature . you may also find newton 's law of cooling breaks down when the temperature difference is very small , because there is no longer effective convection .
in the many-worlds interpretation of quantum mechanics , there is at all times just one state vector for the entire universe . it is a vector in a certain ( infinite-dimensional ) vector space , and that vector space is always the same . so there is nothing in the theory whose dimension changes when the number of particles in the universe changes . to be a bit more precise , the space in which the state vector of the universe lives is ( something like ) a fock space . vectors in that space include states with all possible different numbers of particles , as well as superpositions containing different numbers of particles . so if a particle-antiparticle pair is created , the state vector simply " wanders " from one part of that space to another ; the space itself need not get any bigger .
the answers of martin and edward are quite right , but theý lack a componet which plays a role in the case of that " garage opener battery " . if current is not limited in the circuit outside the battery , first internal resistance can be the limiting factor . but , often in such small ( the measure for " small " is the area/volume of active material at the battery poles ) batteries the current is limited by the speed of chemical reaction at the electrodes . this is not a linear function of current , thus it cannot viewed as a part of internal resistance . does it have anything to do with contact area ? if it does , then how would you model it ? yes , contact area is important , but this is a problem at high current densities mostly . depending on voltage in such a circuit , small contact area ( = high current density ) can result in contact welding , sparking , or ignition of an arc , or in some strange semiconductor effects of the contact point . ( metal surfaces are nearly always coated by some oxide ) did you ever watch when an electric arc is started by a welder ? this is main business when designing contacts in relays , swiches in home , or the switches in 230 kv lines .
there is hardly a book covering all physics , but for particular subjects there is some . for example : jammer : the conceptual development of quantum mechanics . whittaker : a history of the theories of aether and electricity .
i think that some confusion is here . wind speed is determined by differences in the air pression between two points . the max . limits that we observe must set the max wind speed . wind is the movement of a mass of air . the sound speed determines the sound barrier . sound speed is determined by considerations about density , temperature . . . and is a property about the relative speed of the wavefront of a sound event ( a perturbation of te medium - air , that propagates ) in relation to te the air ( considered at rest ) . the speed of sound in relation to the ground is the vector addition/subtraction of the sound speed ( +-340m/s ) with the speed of wind in relation to the ground . the max wind speed is observed in the jet streams in altitude and in the tornados and hurricanes at surface . somewhere we can find the max speed of the exhausted air in the jets and determine if air moves trhu the air with a speed superior to the speed barrier . nothing prevent this from happening with enough thrust . i have no time to search now , sorry . edit add : there is is need to search . the exhausted mixture of jet engines , not beeing wind but mostly a mass of air , can travel thru the atmosphere at speeds much superior of sound barrier . if it is was no so the jet planes could not cross the sound barrier . so , inspite of the confusion in the question , i the short answer is a yes ( a mass of air can travel faster than max sound speed ) .
this is more of a psychology question . after you start seeing things , you notice that a certain side of your vision is the part that will see your finger if you touch your forehead , and the other side will see your finger if you touch your lips . we designate the first as " up " and the second as " down " . the brain just gets a bunch of signals . " up " and " down " are artificial tags we attach , where " up " is the side of our forehead and " down " is the side of our lips . besides , if one was to wear glasses that inverted vision , the mind would indeed slowly adjust .
the correct equation for position at time $t$ : $\vec{r ( t ) } = \vec{p} + \vec{v} . t + {1\over2}\vec{a} . t^2$ where $\vec{v}$ accounts for both initial speed and wind speed , thus : $\vec{v} = 30 . \hat{\vec{d}} + 15 . \hat{\vec{v}} = ( 18,15,24 ) $ and $\vec{a} = ( 0,0 , -9.81 ) $ is gravity acceleration assuming it is pointed toward -z axis . it hits the ground when z equates the radius , so : $2 + 24t - {9.81\over2}t^2 = 0.015$ solving for the above equation give you the time . mass does not play a role because we know the acceleration from the start , also the same for wind velocity as long as it has no z component . edit : we could calculate the drag force for a sphere : $f_d = {1\over2}c_d \rho v^2 a$ in which $c_d$ is a geometrical constant that is equal to 0.47 for sphere , $\rho$ is the density of the medium ( air in this case ) , $v$ speed relative to air , $a$ is the cross section area ( $\pi r^2$ ) . from that you could calculate acceleration due to drag ( that is where mass comes in ) and you need to integrate manually or ( numerically ) ( look at jj fleck solution for this part ) acceleration equation to get position equation as the above equation for $\vec{r ( t ) }$ is valid only for the case of constant acceleration .
i supposed you are in a context of bound states , with normalized eigenfunctions $\psi_n ( x , t ) = \phi_n ( x ) e ^{ie_nt}$ . of course , if you calculate $\langle x ( t ) \rangle_{\psi_n} = \int dx \bar \psi_n x \psi_n$ , you will find a position expectation value which does not depend on time . now , this is not the general case , if you take a linear combination of the $\psi_n$ : $\psi = \sum a_n \psi_n$ , and calculate $\langle x ( t ) \rangle_{\psi} = \int dx \bar \psi x \psi$ , you will find a positition expectation value which depends on time .
fermionic spin 3/2 fields , much like bosonic fields with spin 1 and higher , contain negative-norm polarizations . roughly speaking , a spin 3/2 field is $r_{\mu a}$ where $\mu$ is a vector index and $a$ is a spinor index . if $\mu$ is chosen to be 0 , the timelike direction , one gets components of the spintensor $r$ that creates negative-norm excitations . this is not allowed to be a part of the physical spectrum because probabilities can not be negative . it follows that there must be a gauge symmetry that removes the $r_{0a}$ components - a spinor of them . the generator of this symmetry clearly has to transform as a spinor , too . there must be a spinor worth of gauge symmetry generators . the generators are fermionic because the original field $r$ is also fermionic , by the spin-statistics relation . it follows that the conserved spinor generators are local supersymmetry generators and their anticommutator inevitably includes a vector-like bosonic symmetry which has to be the energy-momentum density . this completes the proof that in any consistent theory , spin 3/2 fields have to be gravitinos . the number of " minimal spinors " - the size of the gravitinos - has to be equal to the number of supercharges spinors which counts how much the local supersymmetry algebra is extended . in particular , it can not be linked to another quantity such as the dimension of a yang-mills group . so while both 3/2 and 1/2 differ by 1/2 from $j=1$ , more detailed physical considerations show that it is inevitable for the superpartner of a gauge boson , a gaugino , to have spin equal to 1/2 and not 3/2 . similarly , one can show that the superpartner of the graviton can not have spin 5/2 because that would require too many conserved spin-3/2 fermionic generators which would make the s-matrix essentially trivial , in analogy with the coleman-mandula theorem . gravitinos can only have spin 3/2 , not 5/2 .
i do not just mean reactions that require heat to proceed , storing surplus energy in chemical bonds . i wonder about strongly endothermic reactions that suck heat out of environment a reaction that requires heat to proceed , a reaction that sucks heat out of the environment , and an endothermic reaction are all the same . these are all just descriptions of reactions that occur because entropy ( s ) increases , despite that fact that enthalpy ( h ) also increases . $g = h -ts$ a reaction is favorable if gibbs free energy decreases . the reaction can still be favorable , despite enthalpy ( h ) increasing , if entropy ( s ) increases enough . you take some substance a ( e . g . ammonium nitrate ) , and some substance b ( e . g . water ) . . . i wonder what happens on the microscopic scale you have to separate molecules/ions/atoms of a from others of a . this could involve breaking apart ionic interactions in a crystal , or intermolecular interactions for example . this takes energy . you have to seperate molecules/ions/atoms of b from others of b . for a solvent , this would involve breaking apart intermolecular interactions such as dipole-dipole interactions . this takes energy . new interactions between a and b form . this releases energy . if the energy released in step 3 is less than the energy required for steps 1 and 2 , the process is endothermic .
maybe that was just window frost ( http://www.its.caltech.edu/~atomic/snowcrystals/frost/frost.htm - " forms when a pane of glass is exposed to below-freezing temperatures on the outside and moist air on the inside" ) ?
the question as posed in the title ( "heating a volume of water ) has a clear answer : with thermal expansion the mass of the water becomes less and so the specific heat goes down . but in the body of the question you ask about a constant quantity ( "a pot of" ) of water and several effects start to play : water evaporates - this means it loses heat . this heat loss rapidly gets worse as you get closer to the boiling point of the water . thus unless you have an isolated system , it will definitely take longer to heat from 80 to 90 than from 50 to 60 - because heat input is not the only heat term . heat transfer : if you have a constant flame ( for example ) the heat transfer to the pot will be a function of the temperature of the pot - the hotter the pot , the less efficient the transfer . when you want to boil water efficiently , you do two things : cover the pot ( limit loss due to evaporation ) and put the heat inside if you can : for example the submerged heater element in electric kettles . other forms of boilers also put the heat in the middle of the water ( think water heaters for homes ) so most of the hot gas gets to give off its energy to the water . but if you have a flame , the best you can hope to to is transfer all it is internal energy to the water - so when the water is hotter a flame is always less efficient . very efficient systems use counter flow - the hot air moves left to right , and the water to be heated right to left : in that way the colder gas meets even colder water so when the gas finally is exhausted it has no heat left . same principle is used in efficient gas furnace for homes , etc .
displacemtns are vector quantities which are determined on the basis of the final and initial position of the object . when you have a cube let 's say the insect traveled form one of the blue balls to another blue ball . let 's represent each of them by vectors from a reference origin $o$ . then the initial position vector $r_0$ and final position vector $r_1$ . hence your displacements would be $r_1 - r_0$ . which in magnitude will be $|r_1-r_0|$ i.e. the distance between the two blue balls . one can use pythagoras theorem to get the desired displacement in magnitude and direction of the vector would be from initial positron to final position .
recall that the potential difference between two points $a$ and $b$ is given by $$\delta v=-\int_{a}^{b} \vec{e}\cdot d\vec{\ell} . $$ consider evaluating this integral for two paralell plates , i.e. the point $a$ is in one plate and the point $b$ is in the other plate . then , we know that the electric field between paralell plates ( assuming they are very close together ) is of the form $$\vec{e}=e\hat{x} , $$ where $\hat{x}$ is a unit vector perpendicular to any of the plates . now , because the path integral that i quoted for the potential difference is path independent , i can take $d\vec{\ell}=d\vec{x}=dx\hat{x}$ . then : $$\delta v=-\int_{a}^{b} e\hat{x}\cdot dx\hat{x} . =-\int_{a}^{b}edx=e ( a-b ) . $$ in your notation , $\delta v=v$ and $ ( a-b ) =d$ ( the sign is just a matter of the use ) , so , translating the above result we have $$v=ed \longrightarrow e=\frac{v}{d} . $$
seems like an awfully complicated way to go about it . for starters , radiant intensity , measured in watts per steradian , is only defined for " point " sources , so no surface areas , or distances are involved . but even a " point " source would generally have some angular distribution pattern , that seems undefined in your citation . the simplest concept , would be an isotropic point source , but no such thing can physically exist . small area plane sources , will often have " lambertian " radiation patterns , where the off axis intensity is given by:- i = i ( 0 ) cos ( theta ) this is only accurate when observed at distances greater than ten times the source diameter , where errors in measured intensity , will be less than 1/2% . the irradiance on some distant surface normal to the source axis , at some surface point off at an angle theta , is given by i ( 0 ) . cos^4 ( theta ) / d^2 , where d is the axial distance from source to surface . the cos^4 arises , from i ( theta ) = i ( 0 ) cos ( theta ) then two more cos ( theta ) terms for the oblique distance squared , and the fourth cos ( theta ) from the incidence angle of the receiving surface , which spreads the radiation further . for a lambertian source , the total radiated power ( watts ) is pi . i ( 0 )
i worked this up the chain to martin bojowald , and he sent me this response : the short answer is : both . the number of edges and vertices as well as their properties ( the geometrical excitation level ) change . details have not been analyzed much because the numbers and properties of edges and vertices can be seen only when a full , inhomogeneous graph is used . however , all the explicit models of cosmological expansion in loop quantum gravity rely on a complete reduction to homogeneity , which is so radical that it smears out all edges and vertices and their properties into just one quantum number ( or maybe three in anisotropic models ) . but we do know that the full hamiltonian generates changes of both the numbers and properties of edges and vertices . the only reference i can think of is http://arxiv.org/abs/0705.4398 ; see especially fig . 1 . basically , lqc models are framed in terms of large-scale properties of quantum geometry , so nobody knows what happens to the spin networks in very much detail .
the circuit likely was closed by his body , or by a grounding wire he was holding . at least that is one way the demonstration has been done . i assume the other end was in contact with the generator . another way is to suspend the tube so it is not in electrical contact with anything , and swing it around , so that you observe a momentary discharge when the tube is orthogonal to the field . in this case the discharge shuts itself off because there is no closed circuit path as you observe . yes . think of v as like the height of a hill for a positive charge . e wants to push the charge down the hill , in the direction of lower v . hence the minus sign .
i would like to add a bit of mathematical detail the ( correct ) statements by djbunk . let a scalar function $f$ be given ( let 's not restrict ourselves to the electric potential ) . for any unit vector $\mathbf n$ , we can define the directional derivative $d_\mathbf{n}$ of the function $f$ in the direction $\mathbf n$ as follows : $$ d_\mathbf{n}f ( \mathbf x ) = \mathbf n\cdot\nabla f ( x ) . $$ the directional derivative gives the rate of change of the scalar function $f$ in the direction of the unit vector $\mathbf n$ . notice that $$ \mathbf n\cdot \nabla f ( \mathbf x ) = |\nabla f ( \mathbf x ) |\cos\theta $$ where $\theta$ is the angle between $\mathbf n$ and $\nabla f ( \mathbf x ) $ , so the directional derivative is maximized when $\theta = 0$ , and is minimized when $\theta = -\pi$ . in other words ; the the rate of change of a scalar function $f$ at a point $\mathbf x$ is positive and greatest in magnitude in the direction of the gradient of $f$ at $\mathbf x$ . this confirms bjbunk 's statements .
the question is : does the air+alcohol-gas burn creating a big pressure or does the container need of a little hole to burn ? strange " need of a little hole " , what does that mean ? the question was : do we need of hole to start the ratio of oxygen+alcohol another time strange " need of hole " . and on the whole two rather different questions . one thing i can say : in a container with alcohol vapor ( i assume ethyl alcohol is meant ) and air one will have 40 mmhg partial pressure of ethanol at 19 °c 720 mmhg partial pressure of air this makes 40/760 = 5.3 vol-% of ethanol . a view in tables says that ethanol has lower explosive limit ( lel ) 3.3 vol% upper explosive limit ( uel ) 19 vol% answer is : your container will explode , the liquid ethanol will be sprayed around , burning . i would prefer to be several dozens of meters away from such " experiments " , at least .
a very natural and interesting question . if the paper stays nearly flat , we may use the linearized approximation . i will neglect the gravitational potential energy because it is probably much smaller than the bending energy ( note that the shape of the bent paper is almost the same when the desk is vertical as when it is horizontal ) . the paper does not want to bend much , so the energy contains a term that punishes the second derivatives of $y$ ( the curvature ) $$e = k \int_0^l {\rm d}x\ , ( y&#39 ; &#39 ; ) ^2$$ where $y&#39 ; \equiv dy/dx$ , and so on . we want to keep the length of the paper fixed - it is prescribed by the way how you attach it at the end points . in the linear approximation , the length is a linear function of $$d = \int_0^l {\rm d} x\ , ( y&#39 ; ) ^2 $$ which may be seen , if you need it , by a taylor expansion of the exact expression for the length of the graph , $\int ( 1+y^{\prime 2} ) ^{1/2}{\rm d}x$ . add it with a lagrange multiplier , requiring $$\delta ( e+\lambda d ) = 0 . $$ i think that the resulting equations say $$ y&#39 ; &#39 ; &#39 ; &#39 ; = \frac{\lambda}{k} y&#39 ; &#39 ; . $$ note that in the euler-lagrange equations , all the primes " clump " . if you write $y=y&#39 ; &#39 ; $ , it says that the second derivative of $y$ is proportional to $y$ itself . the physically relevant solution is $$ y = y_0 \cos ( 2\pi x n / l ) $$ where only $n=1$ is possible if there is a table underneath the paper . consequently , $$ y = y_0 [ 1-\cos ( 2\pi x / l ) ] , $$ too . i added the term $1$ as an integration constant ( while the other is zero ) to guarantee that $y=0$ and $y&#39 ; =0$ at the boundaries . so what you get in practice is one wave of a cosine , from one minimum to the next one . a more detailed discussion would be needed to eliminate the other cosine-like function with the same frequency , the sine , as well as linear functions , and to discuss whether the exponentially increasing/decreasing solutions may ever be relevant . well , it would not be too difficult . a more general solution for the same ( cosine-like ) sign of $\lambda$ would be $$ y = y_0 [ 1-\cos ( \phi_0+ 2\pi x n / l ) ] + ax^2+b , $$ and the four conditions $y=0$ , $y&#39 ; =0$ at $x=0 , l$ as well as the fixed length of the paper would imply $n=1$ , $\phi_0=0$ , $a=0$ , $b=0$ , as well as the right normalization $y_0$ . as suggested above , the conditions would have other solutions where $n=2,3,4\dots$ but these solutions would not satisfy $y\geq 0$ for $0\leq x \leq l$ , so the paper could not arrange itself above the table . if there were a hole in the table , these higher harmonics ( several periods of the wave containing ) solutions would be possible - but i guess that they would be unstable . i am not sure whether i would be able to solve it analytically if the angle $y&#39 ; $ were not infinitesimal . it seems clear that the exact solution for significant angles is not just a cosine : the bent paper tends to resemble a balloon - for which $y ( x ) $ is not even single-valued - when there is too much redundant paper in the middle . on the other hand , if you sharply bend the paper at $x=0 , l$ so that $y&#39 ; $ may be arbitrary at these two points , the paper in between will be bent as an arc of a circle - see another answer below - and this result is accurate even if $y&#39 ; $ is of order one . note that in the linearized approximation , the arc gives $y$ being a quadratic function of $x$ so that $y&#39 ; &#39 ; =0$ still solves the fourth-order equation above .
the hamiltonian of this system lives in a 4-dimensional hilbert space since you have two spin $1/2$ . therefore , you should represent the spin matrix in this four dimensional space like this : $s_1^z=\begin{pmatrix} -0.5 and 0 and 0 and 0 \\ 0 and -0.5 and 0 and 0 \\ 0 and 0 and 0.5 and 0 \\ 0 and 0 and 0 and 0.5 \end{pmatrix}$ , $s_2^z=\begin{pmatrix} -0.5 and 0 and 0 and 0 \\ 0 and 0.5 and 0 and 0 \\ 0 and 0 and -0.5 and 0 \\ 0 and 0 and 0 and 0.5 \end{pmatrix}$ the order of the four states along the rows and columns is $|dd\rangle , |du\rangle , |ud\rangle , |uu\rangle$ where $u$ stands for spin up and $d$ stands for spin down . in this case $s_1^z . s_2^z=\begin{pmatrix} 0.25 and 0 and 0 and 0 \\ 0 and -0.25 and 0 and 0 \\ 0 and 0 and -0.25 and 0 \\ 0 and 0 and 0 and 0.25 \end{pmatrix}$
all the following is simply a simple application of math . if i am wrong anywhere ( and i am bound to be ) , please feel free to point it out to me or just edit the post , if you like . the largest planetoid is ceres , at almost 1000 km in diameter . if we assume that your planetoid was a bit larger than this , and that ceres was the asteroid you are trying to slingshot around , we can do some maths and estimate a limit to see if it is truly possible to slingshot the rocket back into the spacecraft , using the largest gravity fields that we can . at a mass of 940*10^18 kg , your asteroid will exert a gravitational force on your ( perhaps ) 100kg missile that is f = ( 6.27262 *10^12 ) /$r$^2 . this will be for any value of $r$ . if you slingshot a body by a planet , the speed of the missile will then be twice the speed of the planet plus the missile 's initial speed . so you should probably be orbiting against the revolution of the asteroid belt . anyway , the final speed of the missile will be $2u + v$ , where $u$ is the speed of the planet and $v$ is the speed of the missile . if we let this quantity be $w$ , then you only have 2 variables now determining if you can slingshot back to the spacecraft : $w$ and $r$ . the critical condition here is that given a w ( determining your kinetic energy ) and an r ( determining your potential energy ) , can we reach an energy quantity that : a ) enters the orbit of the asteroid such that it is captured in the orbit b ) escapes the gravity of the planetoid so the missile does not actually end up orbiting the asteroid c ) gains enough speed that it can slingshot back to the spacecraft in a few minutes . the escape speed of the asteroid is : $$v_{escape} = \sqrt{{\frac{2gm}{r}}}$$ so entering the orbit , your $v &lt ; v_{escape}$ . but since you are exiting the planet , your $w &gt ; v_{escape}$ . since $2u + v = w$ , then we want everything expressed in terms of $v$: so $$v &lt ; v_{escape} &lt ; w $$ $$v &lt ; v_{escape} &lt ; 2u + v $$ we are going to assume that the missile will be whizzing just above the surface of the planetoid ( for maximum energy ) , so $v_{escape} = 0.51$ km/s and $r = 490$ km . plus , we will assume the missile is going at the fastest speed of satellite we have ever produced , which is $v = 70$ km/s . substituting : $$70 &lt ; 0.51 &lt ; 2u + 70 $$ so , this can not happen . your missile should be going slower than this . what we need is a speed that is less than 0.51 km/s when we enter , so let 's go for 0.50 km/s . $$0.50 &lt ; 0.51 &lt ; 2u + 0.50 $$ to determine $u$ , we need to estimate the distance your missile is going to be travelling . if we let $x$ be the distance of the spacecraft from the planetoid , and the planetoid is not moving , then if the missile is whizzing just above the surface of the planetoid , your missile will be travelling a distance of $2x + \pi r$ . the missile will be travelling at different speeds throughout this course , so let 's sum up the times required . let 's let the acceleration of the missile experiences be $a$ . the acceleration is the square root of the sum of the squares if the tangential and radial acceleration . $$a = \sqrt{a_t^2 + a_r^2}$$ we know the acceleration at the surface of ceres , which is our $a_r$ . we also know $a_t$ because that is just the acceleration to get from $v$ to $w$ . $$a = \sqrt{ ( \frac{w - v}{t} ) ^2 + ( 0.028g ) ^2}$$ playing around with this equation , we will get $$t = \frac{w - v}{\sqrt{a^2 - a_r^2}}$$ and this is the time it will take the missile to whiz by half the surface of ceres . so summing up all the times : $$t_{tot} = \frac{x}{v} + \frac{w - v}{\sqrt{a^2 - a_r^2}} + \frac{x}{w}$$ $$t_{tot} = \frac{x}{v} + \frac{2u + v - v}{\sqrt{a^2 - a_r^2}} + \frac{x}{2u + v}$$ $$t_{tot} = \frac{x}{v} + \frac{2u}{\sqrt{a^2 - ( 0.028g ) ^2}} + \frac{x}{2u + v}$$ since we just added a new variable , let 's define a new constraint . let $t = 300$ seconds , for example . so it takes 5 minutes for the missile to whizz just above the surface of ceres . then we have : $$300 = \frac{2u}{\sqrt{a^2 - ( 0.028g ) ^2}}$$ let 's also say that we do not want the entire trip to take more than ten minutes in total . so we let $t_{tot} = 600$ . $$600 = \frac{x}{0.50} + 300 + \frac{x}{2u + 0.50}$$ we can turn even more " knobs " with these variables and say that the spacecraft is 500 km away , so $x = 500$ now having these equations , we can " solve " for $u$ and $a$ . because there is 1 equation and 2 unknowns , we are not really going to get just one answer . $u = -0.607$ $a$ = 4.05*10^-3 so , that looks like a very slow acceleration . and ceres seems to be going the other way around , away from you . this probably means that there is an error in the formulas . or , your scenario is impossible . i graphed the equation above ( though i dont know how to do that here ) , with x = 500 and t = 300 , and it showed me that you can not have a positive u unless your $t_{tot}$ is above 1300 . and the graph looks the same if i change any of the constants i set . i believe that this should be enough to give you something to play with and determine for yourself if it is possible or not . besides , you get to decide where the ship is and how fast the missile is going . and anyway , the final equation that i got was way too complex for me to solve completely ( and it has a lot of possible values , besides ) . i do not know any software that can help . but the good news is , it is probably possible to slingshot the missile , but you have to allow a larger time for it to whizz by the asteroid . feel free to change the dimensions of the asteroid in the formula . some good data can be found on wikipedia . hope this is something though .
" free energy " in the sense of " gibbs free energy " or " helmholtz free energy " is a rigorously defined concept . by contrast , the word " energy " is ill defined and means different things in different contexts . i think you will have to state your question more precisely for us to help . from lagerbaer in a comment : one way to think about energy vs free energy is that the former is a measure of all the work you had have to do to assemble the system , whereas the free energy , which you get from the energy itself by subtracting $t\cdot s$ where $t$ is temperature and $s$ is entropy , tells you how much work you could get out of a system as " useful " work ( in contrast to the " random " energy in heat ) i did a quick search for " what is energy " and what is energy ? where did it come from ? looks an informative thread . have a look at this discussion and see if it casts any light on your question .
from special relativity we know that a lorentz transformation : \begin{equation} x'^\mu = \lambda^\mu {}_\nu x^\nu \end{equation} preserves the distance : \begin{equation} g^{\mu \nu} \delta x_\mu \delta x_\nu = g^{\mu \nu} \delta x_\mu ' \delta x_\nu ' \end{equation} the above two equations imply : \begin{equation} g^{\mu \nu} = g^{\rho \sigma}\lambda_\rho {}^\mu \lambda_\sigma {}^\nu \end{equation} now , let us consider an infinitesimal transformation : \begin{equation} \lambda_\nu {}^\mu = \delta_\nu{}^\mu + \omega_\nu{}^\mu + o ( \omega^2 ) \end{equation} such that we can write : \begin{equation} \begin{aligned} g^{\mu \nu} and = g^{\rho \sigma}\lambda_\rho {}^\mu \lambda_\sigma {}^\nu \\ and = g^{\rho \sigma} \left ( \delta_\rho{}^\mu + \omega_\rho{}^\mu + \cdots \right ) \left ( \delta_\sigma{}^\nu + \omega_\sigma{}^\nu + \cdots \right ) \\ and = g^{\mu \nu} + g^{\mu \sigma} \omega_\sigma{}^\nu + g^{\rho \nu} \omega_\rho{}^\mu + o ( \omega^2 ) \\ and = g^{\mu \nu} + \omega^{\mu\nu} + \omega^{\nu \mu} + o ( \omega^2 ) \end{aligned} \end{equation} and so : \begin{equation} \omega^{\mu\nu} = - \omega^{\nu \mu} \end{equation} thus , the matrix $\omega$ is a $4 \times 4$ antisymmetric matrix , which corresponds to $6$ independent parameters ( i.e. . the $3$ parameters corresponding to boosts and the $3$ parameters corresponding to rotations ) .
i ) two square matrices $a$ and $b$ are similar matrices if they are connected via a relation $$\tag{1}ap~=~pb$$ for some invertible matrix $p$ . ii ) two square matrices $a$ and $b$ are unitarily similar matrices if $p$ in eq . ( 1 ) is a unitary matrix .
you have a 2d point source , so the energy of the wave is spread out along a circle of circumference $2\pi r$ where r is the distance from the point source . this means the intensity of the wave varies as 1/r . it is a 2d version of the inverse square law . obviously the amplitude can not be constant everywhere on the second wall or the energy of the wave would be infinite . you get the curve drawn for the amplitude on the second wall because the distance from the point source to the wall increases as you move along the away from the point immediately opposite the slit .
a plasma sheath is the interaction of a plasma with a boundary . as long as you let enough time for the plasma to be created , you let enough time for the plasma to create a self-consistent electric field in the sheath . in stationary plasmas , the only case when there is no sheath , is when the surface that the plasma is in contact to , is at the plasma potential ( i.e. . same potential as the plasma ) , which means you have to supply current to this surface ( in langmuir probes in dc glow discharges , for example ) in order to compensate for the fact that electrons arrive more quickly - and hence , at a higher rate - than positive ions , to the boundary , producing a net negative charge flux and thus a current . and in this case , there is no electric field either . in all other cases , the sheath is necessarily accompanied by an electric field of its own , because the electric field is what allows the sheath to exist , as an entity with different properties and structure than the plasma bulk . if you increase or decrease the electric field applied to your discharge , the sheath will change , and adapt to that field . basically , if you try to increase the voltage in a dc glow discharge , the cathode sheath will first cover all of the cathode area , then the current across the sheath will increase to allow the potential drop across the sheath to increase . then the cathode will heat up during the transition to arc , and the sheath will change to take into account the increase ion and electron emissivity of the cathode . all these processes are rather complex and depend highly on the situation . so i think one might say : sheath and electric field are bound together by many relations , so it is difficult to answer without knowing a little more about your particular case !
there are various layers here . first , the higgs boson is a ew doublet with a certain hypercharge $y$ . defining as usual the electric charge as $q=t^3+y$ you see that $\phi^0$ is indeed neutral for the choice $y=1/2$ . but you see also that for the very same choice , for which a vev of $\phi_3$ does not break the charge $q$ , the other component $\phi^+$ must be charged . in fact , must have charge exactly $+1$ , as given by $q=t^3+y=\mathrm{diag} ( 1,0 ) $ . second , even without knowing what the charges are , as long as the charge $q$ is not broken , the 3 polarizations for every $w$ must have the same electric charge . finally , you can see that $\phi_{1,2,4}$ are indeed ``eaten'' by changing variables and using instead $$\phi=e^{i\pi ( x ) /v \hat{t}}\left ( \begin{array}{cc}0\\ \frac{v+h}{\sqrt{2}}\end{array}\right ) $$ where $\hat{t}$ are the broken generators . expanding in the $\pi ( x ) $ field the expression above for $\phi ( x ) $ you see that they are , at leading order , the $\phi_{1,2,4}$ ( that is , the $\pi ( x ) $ generate on the vacuum states with the same quantum numbers as the $\phi_{1,2,4}$ do ) . by doing a gauge transformation that schematically looks like $$w_\mu\rightarrow e^{i\pi ( x ) /v \hat{t}}w_\mu e^{-i\pi ( x ) /v \hat{t}}-e^{i\pi ( x ) /v \hat{t}}\partial_\mu e^{-i\pi ( x ) /v \hat{t}}$$ ( hoping that i dind't mess up the signs and other factors ) you can remove the $\pi ( x ) $ fields , that is removing the $\phi_{1,2,4}$ . as you can see from the gauge transformation , they give rise to non-vanishing longitudinal terms , such as $\partial_\mu\pi ( x ) /v$ , in the expression for the $w$ . in other words , a massive $w$ emerges via the so-called higgs mechanism .
for small velocities and displacements around a circular kepler orbit , the equations of motion are the hill-clohessy-wiltshire equations . they can be exactly solved to give ( the above switches your convention for y and z . ) your approach of finding the forces in a rotating frame works ; there is a derivation here . the motion is in general a sort of looping that is unstable . a pure-z initial velocity or a pure-y initial velocity or displacement leads to periodic solutions ( using your notation ) , but any other initial displacement or velocity leads to run-away . ( however , the run-away is linear in time , not exponential . ) i made some plots of the trajectories here .
when they show a decay with a certian amount of energy , this energy is net of the masses of the particles . so you get co = $\beta$ + ni + 0.31 mev , the energy is attached to the ejected beta particle .
there is one formula relating the speeds of any two " platforms " ( say $p$ and $q$ ) between each other : $$v_{p} [ q ] = v_{q} [ p ] . $$ and there is of course the well known symbol for " speed of light ( in vacuum ) " , as determined of light signals exchanged by members of any one platform between each other : $c$ . the speed of any one platform ( $q$ ) as determined by members of any other platform ( $p$ ) can thereby be expressed ( and this will be found convenient below ) as $$v_{p} [ q ] = c \ , \ , \beta_{p} [ q ] $$ with the appropriate real number value $\beta_{p} [ q ] $ . correspondingly of course : $$\beta_{p} [ q ] = \beta_{q} [ p ] . $$ and then there is one formula relating the ( pairwise ) speeds , or for simplicity rather the corresponding $\beta$-numbers , of any three " platforms " ( say $g$ , $h$ , and $j$ ) between each other : $$1 - \left ( \ , \beta_{h} [ g ] \ , \ , \ , \beta_{h} [ j ] \ , \ , \ , \text{cos} [ \angle_{h} [ g , j ] ] \ , \right ) = \sqrt{ \frac{ ( 1 - \beta^2_{h} [ g ] ) \ , ( 1 - \beta^2_{h} [ j ] ) }{ ( 1 - \beta^2_{g} [ j ] ) } } . $$ in case $\text{cos} [ \angle_{h} [ g , j ] ] = -1$ which should be applicable to the " stack of platforms " described in the question this simplifies to the surely familiar formula $$\beta_{g} [ j ] = \frac{ ( \beta_{h} [ g ] + \beta_{h} [ j ] ) }{ ( 1 + \beta_{h} [ g ] \ , \ , \beta_{h} [ j ] ) } , $$ or likewise : $$\beta_{g} [ j ] = \frac{ ( \beta_{g} [ h ] + \beta_{h} [ j ] ) }{ ( 1 + \beta_{g} [ h ] \ , \ , \beta_{h} [ j ] ) } . $$ now , this formula can be applied to the speed values given in the question $v_1 := \beta_1 \ , c$ , $v_2 := \beta_2 \ , c$ , . . . $v_k := \beta_k \ , c$ and so on ; where $v_1$ is the speed of the bullet wrt . the first platform , $v_2$ the speed of the second platform wrt . the first , and so on ; successively in the proposed " russian doll " setup . of particular interest are surely the resulting speed value $v_{ [ 0 , k ] }$ ( or the corresponding real number $\beta_{ [ 0 , k ] }$ ) of the bullet wrt . the $k$ th platform . with the above formula follows $$ \beta_{ [ 0 , ( k + 1 ) ] } = \frac{ ( \beta_{ [ 0 , k ] } + \beta_k ) }{ ( 1 + \beta_{ [ 0 , k ] } \ , \ , \beta_k ) } $$ noting the similarity of this formula to the addition theorem of the " hyperbolic tangent " function $\text{tanh}$ , $$\text{tanh} [ x + y ] = \frac{ ( \text{tanh} [ x ] + \text{tanh} [ y ] ) }{ ( 1 + \text{tanh} [ x ] \ , \ , \text{tanh} [ y ] ) } , $$ we obtain $$ \beta_{ [ 0 , ( k + 1 ) ] } = \text{tanh} [ \text{arctanh} [ \beta_{ [ 0 , k ] } ] + \text{arctanh} [ \beta_k ] ] $$ . applying this to all ( "$n$" ) given speed values ( or corresponding $\beta$ values ) then $$ \beta_{ [ 0 , n ] } = \text{tanh} [ \sum^n_{j = 1} \text{arctanh} [ \beta_j ] ] , $$ or correspondingly $$ v_{ [ 0 , n ] } = c \ , \ , \text{tanh} [ \sum^n_{j = 1} \text{arctanh} [ \frac{v_j}{c} ] ] . $$ the values of the $\text{tanh}$ function are or course approching $1$ , but do not reach the value $1$ for any argument value ; cmp . http://commons.wikimedia.org/wiki/file:sinh%2bcosh%2btanh.svg therefore the total bullet speed ( wrt . the " final , $n$ th platform" ) that may be reached in the described " russian doll " setup with $n$ successive platforms cannot reach ( or even exceed ) the speed of light , $c$ .
it turns out that asher peres ' original theoretical paper on delayed choice entanglement swapping is short and quite readable . it sets out the idea behind the experimental setup without getting distracted by practicalities . basically the idea is that , as the ars technica post says , alice and bob each make their choice of measurement on one of the two photons the each have , and send the other to eve . ( it seems that ma et al . renamed eve to victor - do not ask me why . ) eve then makes her choice of measurement on the two particles she receives . this is then repeated many times . alice , bob and eve all record which measurement they choose to make on each trial , as well as its result . alice and bob will each have a list of completely random measurement outcomes ( each measurement produces a 0 or a 1 output with equal probability ) , which are not correlated in any way . however , eve also has a list of which measurements she made and what the outcomes were . what she does next is to sort the data from alice and bob 's trials into four subsets , according to both which measurement she decided to make on that trial and what the result was . it then turns out that , according to the formalism of quantum mechanics , each of these four subsets will be correlated in exactly the same way as if alice and bob had been measuring entangled particles . this is what ma et al . have confirmed experimentally . the important thing is that the results of eve 's measurements are needed in order to sort alice and bob 's results into subsets . this means that you do not have any information about whether the results from any given trial are correlated until after eve has made her measurement , so eve cannot cause a paradox by making a different decision based on information about the correlations .
so is it correct to say that magnetic field are ultimately caused by currents ? no , think of a magnetic field as a field that permeates all of space and time , existing independent of anything else . ( however an empty field does not do anything so changes in $\vec b$ field are what matter . ) the ( change in ) magnetic field can be created by currents but also by other stuff too . using the gauss 's law : $$ \nabla \cdot \vec b = 0 $$ we can see that the magnetic field does not have any divergences i.e. no sources or sinks ( mono-poles ) . you can imagine it as an incompressible fluid ( like a tank of water , the water can move but you can not create high density water or vacuum volumes ) . so we have established there are no divergences however there can be curls in the field , i.e. the field can flow , but since there are no divergences these flows must be closed loops . the equation you describe , ampère 's circuital law ( with maxwell 's correction ) , is given by : $$ \nabla \times \vec b = \mu \vec j + \epsilon \mu \frac {\partial \vec e}{\partial t} $$ this states that there are two ways to create a curl in the $\vec b$ field . the first is with a current density i.e. movement of charge which necessarily requires electric charges . so yes ( curls in ) magnetic fields can be created by electric charges . however there is a second part which states that curls in magnetic fields can also be created by $\vec e$ fields changing in time . this usually requires a charge particle but it can also be caused by a changing magnetic field . this is how light propagates , changing $\vec b$ creates a changing $\vec e$ field which creates a changing $\vec b$ etc . if magnetic mono-poles exist then they can be used to create the first changing $\vec e$ field . which eliminates the need for charges ( well they are magnetic charges ) .
multiply both sides of your equation \begin{equation} \det ( i-u ( t ) e^{ite} ) =0 \end{equation} by $e^{-inte}$ where $n$ is the number of dimensions of the state vector space . we obtain \begin{equation} \det ( e^{-ite}i-u ( t ) ) =0 \end{equation} ( see below for how this works . ) this is a special case of the following equation \begin{equation} \det ( \lambda i - a ) = 0 \end{equation} whose solutions $λ_k$ are precisely the eigenvalues of operator $a$ . hence , the meaning of your equation is : each $e^{-ite}$ satisfying your equation is an eigenvalue of the unitary operator $u ( t ) $ . note that all eigenvalues of unitary operators are complex numbers with absolute value 1 . above we used the following property of determinant : for any scalar $b$ \begin{equation} b^n\det ( a ) = \det ( ba ) \end{equation} determinant of operator $a$ is defined by leibniz formula as \begin{equation} \det ( a ) = \sum_{\sigma \in s_n} \operatorname{sgn} ( \sigma ) \prod_{k=1}^{n}a_{\sigma ( k ) , k} \end{equation} which implies that for scalar $b$ \begin{equation} b^n\det ( a ) = \sum_{\sigma \in s_n} \operatorname{sgn} ( \sigma ) \prod_{k=1}^{n}ba_{\sigma ( k ) , k}=\det ( ba ) \end{equation}
there are no forbidden trajectories in feynman ' path integral approach to quantum mechanics . that is really the main point of the approach – we have to sum over all histories of the system . every trajectory whose degrees of freedom ( coordinates ) obey the local constraints that should be obeyed has to be summed over . we certainly do not exclude non-differentiable trajectories – the differentiable trajectories are measure-zero and their contribution to the path integral may , in fact , be completely neglected ! the dominance of the non-differentiable trajectories is how feynman 's path integral obeys the uncertainty principle . this also means that in the language of the history of point-like particles , we do not forbid superluminal trajectories . in fact , the trajectories that contribute the bulk ( effectively all ) to the path integral are superluminal almost everywhere . we do not exclude trajectories in which particles-that-will-become-fermions are overlapping , either . the special relativity postulates including the speed-of-light limit on the velocity , differentiability of the expectation values as a function of time , and the pauli exclusion principle all arise from the interference of many trajectories that contribute to feynman 's path integral . physics resembles classical physics in the classical limit because the trajectories near the classical solution constructively interfere . physics restores the limit on the velocity because of a cancellation between the trajectories of particles and antiparticles , if we choose their coordinates as the primary degrees of freedom . and the pauli exclusion principle arises from the interference of a trajectory and another one in which the particles are permuted . there could be confusion : we are talking about " classically forbidden trajectories " . they are trajectories that would be forbidden in classical physics . but in quantum physics – and feynman 's approach defines quantum physics , not classical physics – they are allowed although they may end up having an exponentially small probability . that is also why e.g. the tunneling effect exists in quantum mechanics . in this effect , a particle emerges on the opposite side of a wall that would be impenetrable in classical physics . quantum mechanically , the probability amplitude for penetration contains contributions from classically forbidden but quantum allowed trajectories going through the wall .
no , the elements of the periodic table do not form any representation of a group or , more precisely , any irreducible representation . even more precisely , the real insights by mendeleev – that the reactivity etc . is a repeating function of the atomic number – does not follow from any property of a representation that could be derived by group theory . the periodic table boils down to the electron 's filling the shells in the atom , quantum states that are close to the energy eigenstates of a rescaled hydrogen atom . the closest thing to your project that actually can be done is to solve the full hydrogen by the $so ( 4 ) $ symmetry – the rotational symmetry enhanced by the runge-lenz vector : http://motls.blogspot.com/2011/11/hydrogen-atom-and-so4-symmetry.html this solution dictates not only degeneracies but even the energies because the hamiltonian is a function of a casimir . and these energies are important to determine which $z$ produce more reactive elements . more complicated atoms do not have any $so ( 4 ) $ symmetry , only $so ( 3 ) $ , and they can not be solved purely by symmetries . the eightfold symmetry is useful because the elementary building blocks are numerous and they carry various labels – like quarks come in different flavors . but that ain't the case of atoms in the approximation of chemistry or atomic physics for which the nucleus only matters when it comes to its charge , i.e. $z$ , and electrons are the only other particles that matter , without any flavor indices . so there is simply no room for eightfold symmetries etc . the fundamental symmetry between elementary particles is $u ( 1 ) $ , not $su ( 3 ) _f$ as it is for light quarks . if we neglect the electron-electron interactions in the atoms , we get another solvable problem – one in which we literally fill shells of the hydrogen atom . this system is a second-quantized hydrogen atom of some sort and it is solvable . we could say it is solvable by group theory . of course , this approximation ultimately leads to a wrong ordering of shells and the predicted periodic table would be wrong for high $z$ , too . to conclude , physical systems that may be fully solved just by group theory – and even properties of physical systems that may be determined by group theory – are rare enough , a small enough minority of the questions we may ask . atoms are complicated enough so that their properties mostly boil down to more complicated dynamics than just symmetries .
how can we measure meson decay constants ? i am not an experimental physicst , but i think that the best way to obtain the decay constant is to study processes like $\pi^+\to \mu^+ \nu$ and extract them from the branching ratio : $$\rm{br} ( \pi^+\to \mu^+\nu ) =\dfrac{g_f^2 m_{\pi^+} m_\mu^2}{8 \pi}\left ( 1-\dfrac{m_\mu^2}{m_{\pi^+}^2} \right ) ^2 f_{\pi^+}^2 |v_{ud}|^2 \tau_{\pi^+} , $$ which is measured nowadays with great precision . @dmckee answer 's suggests that we can also extract the decay constant from the pion form factor , but this method seems less precise , because it is more difficult to measure form factors than decay constants ( but maybe i am wrong . . . ) . if you take a look at pdg , you will see that the process $\pi^+\to \mu^+\nu$ is measured with an incredible precision . one last comment about decay constants : actually , these quantities can be computed for pions using lattice qcd methods and the theoretical error bars are comparable to the experimental ones ! you can even find very precise computations for more exotic mesons , like $d$ , $b$ and $b_s$ . for your theoretical question : it depends on the process you are considering ! for example , if you have $\pi^+\to \mu^+\nu$ , then you must take a second order term . in this term , you need a current $j^\mu_{q}$ related to the annihilation $u \bar{d}\to w^+$ and a leptonic current $j^\mu_\ell$ related to the creation $w^+\to\mu^+\nu$ . then , the time ordered product will only apply to the $w^+w^-$ term and it will give you simply the $w^+$ boson propagator . from my experience , i would suggest you to integrate-out the vector bosons , because the corrections to the fermi theory are negligible . in this case , you can write an effective hamiltonian : $$\mathcal{h}_{\text{eff}}=-\sqrt{2} g_f v_{ub} [ \overline{u}\gamma_\mu ( 1-\gamma_5 ) d ] [ \bar{\mu}_l \gamma^\mu {\nu_\mu}_l ] +\text{h . c . } , $$ and it is much simpler to read the amplitude and to relate it with the decay constant , because the hadronic part factorizes : $$\mathcal{a}=-i\langle \mu^+ , \nu | {h}_{\text{eff}} |\pi^+\rangle =i\sqrt{2} g_f v_{ub} \langle 0 |\overline{u}\gamma_\mu \gamma^5 d|\pi^+\rangle\cdot \bar{u} ( p_\nu ) [ \gamma^\mu ( 1-\gamma_5 ) /2 ] v ( p_\mu ) , $$ where $$\langle 0 |\overline{u}\gamma_\mu \gamma^5 d|\pi^+\rangle=-i p_\mu f_{\pi^+} . $$
i reproduce a blog post i wrote some time ago : we tend to not use higher derivative theories . it turns out that there is a very good reason for this , but that reason is rarely discussed in textbooks . we will take , for concreteness , $l\left ( q , \dot q , \ddot q\right ) $ , a lagrangian which depends on the 2nd derivative in an essential manner . inessential dependences are terms such as $q\ddot q$ which may be partially integrated to give ${\dot q}^2$ . mathematically , this is expressed through the necessity of being able to invert the expression $$p_2 = \frac{\partial l\left ( q , \dot q , \ddot q\right ) }{\partial \ddot q} , $$ and get a closed form for $\ddot q \left ( q , \dot q , p_2 \right ) $ . note that usually we also require a similar statement for $\dot q \left ( q , p\right ) $ , and failure in this respect is a sign of having a constrained system , possibly with gauge degrees of freedom . in any case , the non-degeneracy leads to the euler-lagrange equations in the usual manner : $$\frac{\partial l}{\partial q} - \frac{d}{dt}\frac{\partial l}{\partial \dot q} + \frac{d^2}{dt^2}\frac{\partial l}{\partial \ddot q} = 0 . $$ this is then fourth order in $t$ , and so require four initial conditions , such as $q$ , $\dot q$ , $\ddot q$ , $q^{ ( 3 ) }$ . this is twice as many as usual , and so we can get a new pair of conjugate variables when we move into a hamiltonian formalism . we follow the steps of ostrogradski , and choose our canonical variables as $q_1 = q$ , $q_2 = \dot q$ , which leads to \begin{align} p_1 and = \frac{\partial l}{\partial \dot q} - \frac{d}{dt}\frac{\partial l}{\partial \ddot q} , \ p_2 and = \frac{\partial l}{\partial \ddot q} . \end{align} note that the non-degeneracy allows $\ddot q$ to be expressed in terms of $q_1$ , $q_2$ and $p_2$ through the second equation , and the first one is only necessary to define $q^{ ( 3 ) }$ . we can then proceed in the usual fashion , and find the hamiltonian through a legendre transform : \begin{align} h and = \sum_i p_i \dot{q}_i - l \ and = p_1 q_2 + p_2 \ddot{q}\left ( q_1 , q_2 , p_2\right ) - l\left ( q_1 , q_2 , \ddot{q}\right ) . \end{align} again , as usual , we can take time derivative of the hamiltonian to find that it is time independent if the lagrangian does not depend on time explicitly , and thus can be identified as the energy of the system . however , we now have a problem : $h$ has only a linear dependence on $p_1$ , and so can be arbitrarily negative . in an interacting system this means that we can excite positive energy modes by transferring energy from the negative energy modes , and in doing so we would increase the entropy &mdash ; there would simply be more particles , and so a need to put them somewhere . thus such a system could never reach equilibrium , exploding instantly in an orgy of particle creation . this problem is in fact completely general , and applies to even higher derivatives in a similar fashion .
the quadratic case is much more involved , but for a simple linear term describing drag the height of a projectile shot vertically is given by $m \ddot{y} + \frac{g}{v_{t}}\dot{y}+g=0$ for graviational acceleration $g$ , teminal velocity $v_t$ and mass $m$ . subject to the boundary conditions $y ( t=0 ) $ and $\dot{y} ( t=0 ) =v_0$ this differential equation has the solution $y = \frac{m v_t}{g} ( v_0 + v_t ) ( 1-\exp ( {-\frac{g t}{m v_t}} ) ) -v_t t$ the particle reaches the apex at $\dot{y}=0$ , which after solving for $t$ is given by $t_{max}= \frac{m v_t}{g} \log ( 1+\frac{v_0}{v_t} ) $ thus we find at time $t=2 t_{max}$ that $y ( t=2 t_{max} ) = \frac{m v}{g} [ v_0 ( v_0+2 v_t ) -2 v_t ( v_0+v_t ) \log ( 1+\frac{v_0}{v_t} ) ] $ if we find that at this time that $y$ is negative , we conclude the particle fell to ground from the apex quicker than it climbed to it . in term of the ratio $r=\frac{v_0}{v_t}$ this condition is $\log ( 1+r ) &gt ; \frac{r ( r+2 ) }{2 ( r+1 ) }$ this is never satisfied and so we conclude that your intuition was correct . in fact these two quantities reach equality only for $r=0$ which is the pathological case of no motion . i hope this walkthrough is able to offer you insight on how to tackle the more general problem .
the metal is detuning both the tag 's antenna and depending on how close the phone is , the phone 's rfid antenna too . when a piece of metal is placed in the near field area of an antenna it becomes coupled to the antenna and it is resonance frequency drops , the impedance decreases ( causing a large signal loss ) and the bandwidth widens ( q decreases ) . in an rfid tag , these little radios probably have very little transmitter power and a small reference ground plane . both of these things make detuning the system very hard to have enough signal to communicate . this is a very common problem in antenna design as many people around the world witnessed with iphone4 as demonstrated in this video . anything conductive can do this . so your hand with water in it , detunes your cell phone . designers have to be aware of these cases and make sure the design is robust enough to overcome some of these conditions .
wolfram 's early work on cellular automata ( cas ) has been useful in some didactical ways . the 1d cas defined by wolfram can be seen as minimalistic models for systems with many degrees of freedom and a thermodynamic limit . insofar these cas are based on a mixing discrete local dynamics , deterministic chaos results . apart from these didactical achievements , wolfram 's work on cas has not resulted in anything tangible . this statement can be extended to a much broader group of cas , and even holds for lattice gas automata ( lgas ) , dedicated cas for hydrodynamic simulations . lgas have never delivered on their initial promise of providing a method to simulate turbulence . a derivative system ( lattice boltzmann - not a ca - has some applications in flow simulation ) . it is against this background that nks was released with much fanfare . not surprisingly , reception by the scientific community has been negative . the book contains no new results ( the result that the ' rule 110 ca ' is turing complete was proven years earlier by wolfram 's research assistant matthew cook ) , and has had zero impact on other fields of physics . i recently saw a pile of nks copies for sale for less than $ 10 in my local half price books store .
you are asking so many questions that the only way to answer satisfactorily would be to completely rejustify special relativity . i suggest you take a look at a book like special relativity which does such a thing ! i will try to answer your first question : how does this happen ? if a photon is at position $ ( 0,0 , ct ) $ at time $t$ , that is , it has a spacetime event at time t of $e= ( ct , 0,0 , ct ) $ , one has to apply the lorentz transform , which would yield $e'= ( ct ' , x ' , y ' , z' ) = ( \gamma c t , -\beta\gamma ct , 0 , ct ) $ . so $ct=ct'/\gamma$ , and therefore : $$e'= ( c t ' , -\beta c t ' , 0 , ct'/\gamma ) $$ now , if we want to find the speed of the photon in this frame , that will be the magnitude of the spatial portion of the event $e'$ , differentiated by $t'$ ( and in this case , since everything is linear , that has the same effect as dividing by $t'$ ) : $$\| ( -\beta c , 0 , c/\gamma ) \|=\sqrt{c^2 ( \beta^2+ ( 1-\beta^2 ) ) }=c$$ it moves at the speed of light , still . the reduction in the y-direction , in this frame of reference , is a consequence of the lorentz transformation , which can be viewed as a consequence of the constancy of the speed of light . of course you can notice that if the motion in the y direction did not decrease , motion in the $x$ and $z$ directions would have to stay the same for the speed of light to stay constant , but this does not make sense , because it implies the beam of light is being dragged along with your reference frame . also , in lorentz transformation , suppose two observers measure a rod , can one of the observers see that the other person who is moving w.r.t. him , measure the rod to be less than his own measurement ? in other words , can the metre sticks of a moving observer get bigger than the observer who views the moving observer and sees himself to be at rest ? i will limit my reply to this . try to work it out from the definition of a lorentz transformation . the answer to " can one of the observers see that the other person who is moving w.r.t. him , measure the rod to be less than his own measurement " is yes . in your next sentence , if you mean to imply that some observer measures a meter stick at longer than a meter , the answer is no . a meter stick at rest is a meter , and a meter stick flying by ( along its axis ) in either direction is slightly shorter than a meter , by a factor of $1/\gamma$ .
i will assume you have heard something about proton decay . proton decay is a hypothetical form of radioactive decay in which the proton decays into lighter subatomic particles , such as a neutral pion and a positron . 1 there is currently no experimental evidence that proton decay occurs . there are a number of experiments which test whether such decays occur . for example a current one is here . there exist limits and in this review they are given depending on the theory that would predict proton decay . neutrons decay when free anyway into a proton an electron and an e-antineutrino . reappearance would require something like a new big bang , i.e. all matter/energy become a plasma and then by expansion the reappearance of nucleons .
the no-go results from algebraic and constructive qft you mention deal with related but slightly different matters . ( edit : the previous version of the following paragraph was slightly misleading - haag 's theorem is actually stronger than i stated before ; see below for details ) haag 's theorem ( which actually slightly predates the inception of algebraic qft ) tells us that we cannot write interaction picture dynamics within hilbert spaces which are free field representations of the ccr 's . this is not the same as to say that interacting dynamics does not exist at all - it simply says that we cannot implement it as unitary operators in the interaction picture . this is done by showing that the possibility to do so in some hilbert space does imply that we are dealing with a free field representation of the ccr 's . the argument is closed by a " soft triviality " result by jost , schroer and pohlmeyer arguing that the latter implies that all truncated $n$-point functions vanish for $n&gt ; 2$ , hence the field is really free - in particular , the " interaction hamiltonian " is zero . this has consequences for both scattering theory and attempts to rigorously construct field theoretical models starting from free fields . in the first case , haag 's theorem is circumvented by either the lsz of haag-ruelle scattering formalisms , which obtain the s-matrix by respectively taking infinite time limits in the weak ( matrix elements ) and strong ( hilbert space vectors ) sense . recall that both setups require the assumption of a mass gap in the joint energy-momentum spectrum ( i.e. . an isolated , non-zero mass shell ) , otherwise we run into the notorious " infrared catastrophe " , which is dealt with using " non-recoil " ( i.e. . bloch-nordsieck ) approximation methods in formal perturbation theory but remains a challenge in a more rigorous setting , save in some non-relativistic models . in the second case , one is led to consider representations of the ccr 's which are inequivalent to free field ones . since field theories living in the whole space-time have infinite degrees of freedom , the stone-von neumann uniqueness theorem no longer holds ( actually , haag 's theorem can be seen as a manifestation of this particular failure mechanism ) , and hence such representations should exist in abundance . motivated by these results , algebraic qft was devised with a focus on structural ( i.e. . " model-independent" ) aspects of qft in a way that does not depend on a particular representation ; on other front , one may also try to explore this abundance of representations to construct models rigorously , which brings us to the realm of constructive qft . the " existence " ( a . k . a " non-triviality" ) and " non-existence " ( a . k.a. " triviality" ) results in constructive qft tell us which interactions survive after non-perturbative renormalization . more precisely , you construct field theoretical models in a mathematically rigorous way by first considering " truncated " interacting theories ( i.e. . with uv and ir cutoffs ) , and then carefully removing the cutoffs in a sequence of controlled operations . the resulting model may be interacting ( i.e. . " non-trivial" ) or not ( i.e. . " trivial" ) , in the sense that its truncated $n$-point correlation functions for $n&gt ; 2$ may be respectively non-vanishing or not . in the first case , any representation of the ccr 's in the hilbert space where the interacting vacuum state vector lives is necessarily inequivalent to a free field one - in particular , one cannot write the interacting dynamics as unitary operators in the interaction picture , in accordance with haag 's theorem . in the second case , you really obtain a free field representation of the ccr 's , but here because renormalization has completely killed the interaction . finally , it is important to notice that triviality of a model may stem from reasons unrelated to the underlying mechanism of haag 's theorem . the latter , once more , is a consequence of having an infinite number of degrees of freedom in infinite volumes ( this theorem does not hold " in a box " , for instance ) , whereas the former usually derives from an interaction which has too singular a short-distance behavior , as argued in the previous paragraph . this can be intuitively be understood by the ( local ) singularity and ( global ) integrability of the free field 's green functions : the lower the space-time dimension , the better the singular ( uv ) behaviour and the worse the integrability ( ir ) behaviour , and vice-versa . that is the underlying reason why $\lambda\phi^4$ scalar models are super-renormalizable in 2 and 3 dimensions ( having only tadpole feynman graphs as divergent in 2 dimensions ) and non-perturbatively trivial in $&gt ; 4$ dimensions . ah , i have almost forgotten about the references : in my opinion , the best discussion of triviality results in qft from a rigorous viewpoint is the book by r . fernández , j . fröhlich and a . d . sokal , " random walks , critical phenomena , and triviality in quantum field theory " ( springer-verlag , 1992 ) , specially chapter 13 . there both the above " hard triviality " results for $\lambda\phi^4$ models and " soft triviality results " such as the jost-schroer-pohlmeyer theorem ( which underlies haag 's theorem , as mentioned at the beginning of my answer ) are discussed . the book is not exactly for the faint of the heart , but the first sections of this chapter provide a good discussion of the statements of the theorems , before proceeding to the proofs of the above " hard triviality " results . for a detailed discussion of jost-schroer-pohlmeyer 's and haag 's theorems , as well as their proofs , i recommend the book of j . t . lopuszanski , " an introduction to symmetry and supersymmetry in quantum field theory " ( world scientific , 1991 ) . the classic book of r . f . streater and a . s . wightman , " pct , spin and statistics , and all that " ( princeton univ . press ) also discusses these two results .
there is no significance in the choice between upper- and lower-case $\psi$ ( or $\psi$ ) to denote a system 's wavefunction . the two are used interchangeably and it is the author 's discretion to use either symbol . ( on the other hand , of course , one should not use the two symbols interchangeably within the same text ; if both are used they would refer to different objects . ) there is also little to say , in general , about whether the position $x$ is included as an argument to the wavefunction or not . some authors may , for example , choose to denote by $\psi ( t ) \in\mathcal h$ the hilbert space state vector , which in the position representation is given by the function $x\mapsto\psi ( x , t ) $ , but this is relatively rare . in general this is a case-by-case matter , but most of the modern literature uses dirac notation , in which the state vector $|\psi\rangle\in\mathcal h$ and its wavefunction $\langle x|\psi\rangle=\psi ( x , t ) \in\mathbb c$ are distinct objects , shown emphatically distinct by the notation . finally , these uses of notation are usually a source of confusion when the wavefunction is split into spatial and temporal parts , such as when setting up a separation of variables scheme for solving the tdse via the tise . in such cases , one postulates that the wavefunction $\psi ( x , t ) $ of a particle takes the form $$\psi ( x , t ) =\psi ( x ) e^{-i e t/\hbar} , $$ in which the temporal and spatial dependences are separated ; here if $\psi$ satisfies the tise then $\psi$ will satisfy the tdse . in such situations it is important to keep in mind that $\psi$ , whilst a convenient calculational tool , does not represent the state of the system : $\psi$ does . beware of notation in such situations , and always check what definition each symbol has been given in the particular text you are reading .
i am on record of having the opinion that there is no real argument against us being a simulation in a general sense , however we frequently find people jumping to quick into the simulation pool and stating there new what-ever-it-is proves the universe is a simulation . the example given above sounds like one of them . first off , quantum error correcting code ( qecc ) are mathematical approaches to allow for stable transfer of quantum information by correcting for decoherence effects . if some version of qecc is apparent in any formulation of quantum mechanics , it is interesting but probably not very meaningful in proving we exists in some sort of emulation . second , just because it shows up in one theory , unless that particular version is shown to have the ability to predict physical effects then it is hard to make the claim about its relevance . whether these things are testable is a matter of debate . however , there are people who are proposing to look for " glitches " in the universe . some would hypothesize that if we lived in a simulation based on lattice quantum chromodynamics ( lqcd ) we should be able to find places where the lattice work becomes apparent . this is clearly far-fetched but who am i to judge ? for the world of warcraft , although i do not play that game , the first evidence of a simulation is along the same lines as the theory to test for lqcd latticework . the pixelation of the characters would be the first indication of potential emulation . the universe as we know it has a continuous spacetime ( versus discrete ) , so any sign of blockiness is a good indicator . generally , anything that is an inconsistency with basic laws of physics ( e . g . perpetual motion , decreasing entropy , etc ) would be the first indicator something was wrong . now , in wow one can assume that they might operate under a slightly different set of physics than our real world . so ultimately inconsistencies in some portion of the world relative to the laws of physics would be a flag . something you should look into is the equivalence principle . in a nutshell it is a statement that the laws of physics should be the same regardless of you location in spacetime . it is very critical to our notion of the world around us , but a similar rule should be applicable wow , and significant inconsistencies would be cause for exploration .
if you think of the two parallel glass sides as canceling each other out you are pretty close to it . the first impact ( low to high indices ) does in fact disperse frequencies if the light is coming in at an angle , but the exits ( high to low ) mostly cancels that effect . there actually can be some small residual effects leading to small colored fringes . the prism works better because the oppositely angled sides enhance rather than cancel the dispersion effects . here are the results of some further analysis and experimentation . part of the answer for why color effects are so hard to find when light passes through flat glass plates appears to be in the eye of the beholder . . . literally ! here 's the scoop : angled light entering a flat plate should at first fan out its angles by color while within the plate . by symmetry , however , those slightly fanned out rays of colored light return to their original paths when they reach the second surface an re-emerge . so , the new rays will show essentially no difference in direction from their paths in the original beam , but will no be spaced very slightly apart from each other in rainbow order . for a typical plate of glass this separation would seldom be more than a millimeter , and for most glasses would be a lot less than that . now picture a point of light on one side of the glass and a human eye on the other side . arrange both so that the line between then is at a sharp angle to the surface of the glass . let 's look at the ray going from the point to the center of you pupil . your eye focuses that parallel white light as a single point on the retina , as expected . but when the angled glass plate is inserted , the same ray of light gets spread out along a tiny distance , usually much less than a millimeter . however , each colored ray in this bundle remains parallel to the original path . this little sub-millimeter bundle then enters the pupil of the eye , carrying pretty much the same light as before , all traveling in parallel . what does your eye do with it ? it forms the same white colored point image as before , since the light is all traveling in parallel . think for example of red and blue light entering opposite sides of a magnifying glass : both will end up near the center . there will be some chromatic aberration , sure , but it turns out that vertebrate eyes are very , very good at eliminating that form of chromatic aberration at the image level . the bottom line becomes this : as long as the plate is not too thick , the physical separation of chromatic components will fall within the size of the human eye pupil , and the image will appear to be white - color free and pretty much just like the original , with just a bit more blurring . that also leads to an experimental prediction that i have not yet tried : if you hold a pinhole in front of your eye while observing a pinhole light on the other side of an angled piece of glass , you may be able to see a short colored line instead of a white dot . i can not guarantee it , but it is likely enough that it would be interesting to try . now to the final part of the analysis : what if the glass is so hugely thick that there is no way the separated components can be captured by the human eye all at once ? should not that lead to some visible color effects , such as blue and red fringes on either side of a point of white light ? specifically , for a white dot or light , a blue fringe should appear towards the side angled away from the viewer , and a red fringe on the side of the glass that is closer to the viewer . for a black line on a light background this would be reversed , with the red on the glass-is-farther edge of the black line ( since that is the nearer edge of the lighter part ) , and blue on the glass-is-nearer edge of the black line . ( you can work out why that is with simple dispersion diagrams . ) but since the space-form chromatic separation effect is going to be small even for a quite thick piece of glass , where can you find something thick enough to show such fringes ? fish lovers have conveniently provided a solution : they are called aquariums ! the combination of glass and water makes a quite good approximation of a very thick piece of glass with decent chromatic dispersion . but does it really work ? if like me you do not currently have an aquarium , here is a convenient online image of a see-through aquarium that is angled sharply away on the right . on the other side of the tank are both vertical bright lights from curtain folds ( near the right side ) , and vertical dark lines from a picture frame ( on the left ) . if you magnify the image , you will see blue fringes on the right sides of the curtain folds . neither effect is intense , but both are definitely in this image . if you do happen to have an aquarium , you should of course try it for yourself , since good direct experiment always trumps theory if they disagree ! do not look directly into a light , since modern led lights are very bright and should never be gazed at directly . instead , place a thin vertical stripe of white paper on a black background and illuminate that with a bright light pointing away from the observer . you can also try holding a small pinhole in aluminum foil in front of you eye to enhance any color fringing effect you may see . and with that . . . i think i will give this one a rest . further discussion , especially actual results from experimenters with real aquariums , would be great though !
op is formally correct that $$\frac{\partial}{\partial x^{\mu}}~\in~ \gamma ( tm|_{u} ) $$ is a vector field ( defined in a local coordinate neighborhood $u$ ) , and not a one-form . what weinberg simply means by casually saying that the partial derivative operator $\partial/\partial x^\mu$ is a covariant vector , or in other words a 1-form , [ . . . ] is just that the local basis of vector fields $$\tag{1} \frac{\partial}{\partial x^{\mu}} ~=~\frac{\partial y^{\nu}}{\partial x^{\mu}} \frac{\partial}{\partial y^{\nu}}$$ transforms in the same way as the components $$\tag{2} \eta^{ ( x ) }_{\mu}~=~\frac{\partial y^{\nu}}{\partial x^{\mu}}\eta^{ ( y ) }_{\nu} $$ of a 1-form/covector [ or a covariant ( 0,1 ) tensor ] $$\eta~=~\eta^{ ( x ) }_{\mu} dx^{\mu} ~=~\eta^{ ( y ) }_{\nu} dy^{\nu}$$ under a local coordinate transformation $x^{\mu} ~\to~ y^{\nu}=y^{\nu} ( x ) $ . the point is that the ( traditional ) physicist often thinks of a $ ( r , s ) $ tensor $$t~=~ \frac{\partial}{\partial x^{\mu_1}}\otimes \cdots \otimes \frac{\partial}{\partial x^{\mu_r}} ~t^{\mu_1\ldots\mu_r}{}_{\nu_1\ldots\nu_s} ~ dx^{\nu_1}\otimes \cdots \otimes dx^{\nu_s}$$ merely in terms of its components $t^{\mu_1\ldots\mu_r}{}_{\nu_1\ldots\nu_s}$ , and in particular , the transformation property thereof under local coordinate transformations . local basis elements , such as , e.g. , $\frac{\partial}{\partial x^{\mu}}$ and $dx^{\nu}$ are often viewed as merely bookkeeping devices . in conclusion , ref . 1 is probably not the best textbook to learn differential geometry from . for instance , already in eq . ( 4.11.12 ) on the very next page 116 weinberg claims that the fact that the exterior derivative squares to zero , $d^2=0$ , is known as poincare 's lemma . this is definitely not correct , cf . wikipedia : the identity $d^2=0$ means that exact forms are closed , while poincare 's lemma states that the opposite holds locally : closed forms are locally exact ( except for zero-forms ) . references : s . weinberg , gravitation and cosmology , 1972 .
edited : indeed , the flat moon would provide 50% more illumination than the round moon . i assumed that it is full moon now , so the sun illuminates the moon , and we are on axis $z$ connecting the moon and the sun ( the origin is at the center of the moon ) . if $\theta$ is the angle between axis $z$ and some direction , and illumination ( total light energy per area - both incident and reflected , as there is no absorption ) on the surface of the ( round or flat ) moon equals $a$ for $\theta=0$ . then illumination on the surface of the round moon for some $\theta$ equals $a\cos ( \theta ) $ , as the same sun light energy falls on a larger area . therefore , the lambertian light intensity in the direction $z$ will be proportional to $a\cos^2 ( \theta ) $ . therefore , we need to compare an integral of $a$ over a unit circle with an integral of $a\cos^2 ( \theta ) $ over the surface of the unit hemisphere . ( initially , i integrated $a\cos^2 ( \theta ) $ over the unit circle , rather than the unit hemisphere , and offered a wrong result here ) .
you are correct that you can not really use a gaussian of length 10 meters . instead , consider using a very short gaussian cylinder near your area of interest . in this limit , you should still be able to approximate the electric field as " nice and symmetric " with respect to the area vectors $d\vec{a}$ . by the way , the same thing is done for finite sheets of charge . if you use a small enough gaussian surface that is located near the center of the sheet , the procedure and results become the same as that due to an infinite sheet . the only difference is the limited region of applicability .
well , you have basically answered you question yourself . rescaling of the action is the same thing as rescaling of the planck 's constant . obviously this can not have any effect classically . but on the quantum level $\hbar$ measures the non-commutativity of observables and in the extreme limit $\hbar \to 0$ you recover classical mechanics . as for the sign , this does not matter either classically or quantum mechanically . we are not interested in only minimizing the action but in all extrema . reverting the sign only means that we swap the meaning of maximum and minimum but the solutions will not change at all .
because if you have zero mass you can not use the newtonian $f=m a$ to find force , you will have a hard time making a physical argument for having zero mass . likewise , you break all physical barriers when you go to an infinite mass . the best one can do is say that the mass approaches zero or approaches infinity . as $m\to \infty$ , $\frac{m}{k}\to \infty$ , and so the period goes to infinity , which can be physically interpreted as the spring 's $k$ constant is just not strong enough to accelerate the mass at all . as $m\to 0$ , $\frac{m}{k}\to 0$ , and so the period goes to zero , which can be physically interpreted as very very fast vibrations . the trick used here is the same one used in $\varepsilon$ - $\delta$ limit/derivative proofs in pure math . to avoid the problem of calculating $\infty-\infty$ or $\infty/\infty$ , we just calculate behavior " arbitrarily large " or " arbitrarily small " , but not actually infinite nor zero .
there is actually at least one very big clue that is been accessible to skygazers since the earliest times : the first quarter moon at dusk . every child in the northern hemisphere going back to 30,000 bce likely would have been familiar with how 1st-quarter moons always tend to rise at noon , reach its highest point at sunset ( with an azimuth directly south ) , and set at midnight . form a triangle out the observer , the sun and the moon : $\triangle osm . $ the only angle the observer can measure directly is of course the angle between the sun and moon , the observer forming the vertex . the sun is in the direction of the horizon , and the 1st-quarter moon is near zenith , hence $\angle som \approx 90° . $ the angle with vertex at the moon , $\angle oms$ , could not be measured in general , but it does not take too much imagination to infer that the shape of the sunlit portion of a 1st-quarter moon results whenever $\angle oms \approx 90°$ . hence , $\triangle osm$ is an acute , nearly isosceles right triangle , whose legs are practically parallel and much , much greater in length than the base . this small base length is the earth-moon distance $|om|$ is itself much greater than any terrestrial distances we measure on the earth 's surface . thus , with extremely little effort we can be reasonably confident that eratosthenes ' condition of parallel sunlight rays holds to good enough approximation for the purpose of his measurements ( uncertainties in the measurements of distances between cities would have been the limiting factor towards overall precision anyway ) .
the wind is certainly doing work , because it applies a force and the point where the force is applied is displaced . however it is not doing any work on the boat , it is doing the work on the water . the key point is that the net force on the boat is zero . we know the net force on the boat is zero because the boat is moving at constant velocity - if the net force were non-zero the boat would be accelerating . since the net force on the boat is zero no work is being done on the boat . the velocity of the boat is constant because the drag of the water is balancing out the force applied by the wind . so overall the wind is applying a force to the water - the boat is just the instrument through which the force from the wind is communicated to the water . so the wind is doing work on the water , but not on the boat .
imagine the anchor is hanging from the bottom of the boat , dangling mid-water like this : ( there is no difference between the anchor hanging in the water and sitting in the boat , since the system boat + anchor weighs the same either way . ) the water level depends on how heavy the anchor is . if you make the anchor heavier , it pulls the boat down further , pushing the water out of the way . this water goes out to the sides and raises the water level a bit . if you make the anchor lighter , the boat rises up some , leaving a gap . water rushes in underneath the boat , and the water level goes down . imagine making the chain longer until at last the anchor starts to rest on the bottom of the tank . since the bottom of the tank is supporting the anchor , it does not pull down on the boat as much . from the boat 's perspective , it is as if the anchor got lighter . thus , the boat rises and the water level falls . we must assume that the anchor is more dense than water , but that is all . if you wanted to calculate how much the water level falls , you would need to know the density and weight of the anchor .
the particles of light waves - the photons - have the rest mass $m_0$ equal to zero . however , at the speed of light , $v=c$ , the total mass $$ m= \frac{m_0}{\sqrt{1-v^2/c^2}} $$ is increased to an indeterminate form , $0/0$ , which should be evaluated as a finite number . the photons - and everything else - carry the total mass that is proportional to the total energy via the famous $e=mc^2$ relation . yes , this mass may be measured . for example , uranium nuclear power plants burn the uranium and reduce its mass by 0.1 percent or so because the waste products ( the nuclei ) are actually a little bit lighter . this energy may be completely transformed to the radiation coming from light bulbs - and the light from these light bulbs carry 0.1 percent of the uranium mass away . this mass is a source of gravitational field and adds inertia to boxes with this light etc . sound is different . the speed of sound is much smaller than the speed of light . while " phonons " in low-temperature condensed matter physics - particles of sound - are analogous to photons in many respects , and $e=mc^2$ still applies , the same is not true for sound waves in the air etc . because the temperature of the air is nonzero , the " ground state " - the lowest-energy state at fixed conditions , with the minimum number of " sound quanta " or " phonons " - is not really unique . instead , there are many states of the air " without any sound " which correspond to chaotic configurations of the air molecules . so one can not consistently divide the energy of the air to the energy of its ground state and the energy of the phonons . but of course , if you produce some loud sounds , they will carry lots of energy in the air and the mass of the air will inevitably increase by $m=e/c^2$ which is , well , not too high because $c^2$ is a large number .
the energy conditions are just various possible generalizations of the statement that " energy density cannot be negative " to the whole stress-energy tensor . the reason why energy density cannot be allowed to be negative - at least in some sense - is that if arbitrary positive- and negative-energy regions were allowed , the empty vacuum would become unstable : it could spontaneously change to regions with positive energy and regions with negative energy , the the " energy density gap " would keep on growing . we know that a condition of this kind has to hold and prevent the universe from developing regions of a large negative energy density but we are somewhat uncertain what the precise condition satisfied by the universe is . as i mentioned , there are many ways how to generalize the condition $t_{00}\geq 0$ to the whole tensor - one that also includes the momentum density and the fluxes of energy and momentum ( the latter is the very " stress" ) . do we require the energy density to be greater than zero or even the absolute value of the pressure ? and so on . . . the most important conditions are null energy condition dominant energy condition weak energy condition strong energy condition see their mathematical conditions at : http://en.wikipedia.org/wiki/energy_condition#mathematical_statement it is somewhat likely that the null energy condition is indeed satisfied in the whole universe - any macroscopic region of it - and the same may be true for the weak and dominant one ( dominant one is strictly stronger than the weak one ) . however , one may write down models of somewhat exotic matter where the strong energy condition is violated . although lots of " data " are known , it remains somewhat uncertain what is the precise condition that follows from the fundamental theory that knows all about the allowed types of matter - string theory . i forgot to say that all these conditions may be modified by a special treatment of the vacuum energy density coming from the cosmological constant .
have these super-dense states been replicated by third parties ? no , i do not think there has been anything published in any reputable journal claiming to have reproduced holmlid 's supposed experimental discovery of ultradense deuterium . if there had been , it would have been big news . he is active in the cold fusion community , so it would not be surprising if other cold-fusion kooks did similar experiments and presented them at true-believer conferences , etc . holmlid is basically a one-man echo chamber who tirelessly pushes his crackpottery in online venues such as wikipedia and physics . se . although he has managed to get his articles published in journals , a literature search showed that out of 2154 references to his papers , 1863 were self-citations . extraordinary claims require extraordinary evidence . holmlid 's claims , about both " rydberg matter " and " ultradense deuterium , " are extraordinary , and there is no evidence for them from any reputable experimentalist . by the way , he has another , more recent paper claiming laser-induced fusion in ultradense deuterium : http://arxiv.org/abs/1302.2781
the first formula indeed follows from the second formula if we let $\omega\to0$ . to see that , expand the fractions as $$ \frac1{\pm\hbar\omega + e^a - e^b} = \frac1{e^a-e^b}\left ( 1 \mp \frac{\hbar\omega}{e^a-e^b}\right ) + \mathcal o ( \omega^2 ) $$ to obtain $\sigma_{xy} = \sigma^1 + \sigma^2$ as the sum of a potentially divergent term $$ \sigma^1 = \frac{-ie^2}{v\omega} \sum_{a , b} f ( e^a ) \frac{\langle a|v_x|b \rangle \langle b|v_y|a \rangle + \langle a|v_y|b \rangle \langle b|v_x|a \rangle}{e^a - e^b} $$ and a term that looks like the first formula $$ \sigma^2 = \frac{-ie^2\hbar}{v} \sum_{a , b} f ( e^a ) \frac{- \langle a|v_x|b \rangle \langle b|v_y|a \rangle + \langle a|v_y|b \rangle \langle b|v_x|a \rangle}{ ( e^a - e^b ) ^2} . $$ to see that the first term vanishes instead of diverging , we have to use the heisenberg equation of motion $v_x = \frac{d}{dt}x = [ h_0 , x ] $ which gives $$ \langle a | v_x | b \rangle = \langle a | h_0 x - x h_0 | b \rangle = ( e^a-e^b ) \langle a | x | b \rangle $$ and thus $$ \langle a|v_x|b \rangle \langle b|v_y|a \rangle + \langle a|v_y|b \rangle \langle b|v_x|a \rangle = ( e^a-e^b ) ( \langle a|x|b \rangle \langle b|v_y|a \rangle - \langle a|v_y|b \rangle \langle b|x|a \rangle ) . $$ the factors $ ( e^b-e^b ) $ cancel and the remaining sum over $b$ becomes a sum over the identity $\sum_b |b\rangle\langle b| = 1$ . thus , we arrive at $$ \sigma^1 = \frac{-ie^2}{v\omega} \sum_{a , b} f ( e^a ) \left ( \langle a|xv_y - v_yx |a \rangle \right ) = 0 . $$ since the commutator $ [ x , v_y ] $ vanishes . to see that the second term is correct , we have to get the summation indices right . to do that , we have to rearrange the summation to obtain $$ \sigma^2 = \frac{ie^2\hbar}{v} \sum_{a , b} ( f ( e^a ) -f ( e^b ) ) \frac{\langle a|v_x|b \rangle \langle b|v_y|a \rangle}{ ( e^a - e^b ) ^2} . $$ in the limit $t\to0$ , the difference of fermi-dirac distributions $f ( e^a ) -f ( e^b ) $ will be equal to $1$ if $e^a &lt ; e_f &lt ; e^b$ $-1$ if $e^b &lt ; e_f &lt ; e^a$ $0$ otherwise using this and rearranging the summation again gives the kubo formula in the first form .
you start from this $ [ p , f ( x ) ] \psi= ( pf ( x ) -f ( x ) p ) \psi$ knowing that $p=-i\hbar\frac{\partial}{\partial x}$ you will get $ [ p , f ( x ) ] \psi=-i\hbar\frac{\partial}{\partial x} ( f ( x ) \psi ) +i\hbar f ( x ) \frac{\partial }{\partial x}\psi=-i\hbar\psi\frac{\partial}{\partial x}f ( x ) -i\hbar f ( x ) \frac{\partial}{\partial x}\psi+i\hbar f ( x ) \frac{\partial }{\partial x}\psi$ from where you find that $ [ p , f ( x ) ] =-i\hbar\frac{\partial}{\partial x}f ( x ) $
i will assume that " enhance the vibrancy of colour " means something like turning up the saturation in a photo editing application . i think it is highly unlikely that this could be accomplished using analogue optics , because it is a highly nonlinear operation . this is because we need to block or let through light in three different frequency ranges ( red , green and blue ) by an amount that depends on the amount of light of other frequencies that is also hitting the filter . for example , suppose there is a small amount of blue light hitting one part of the filter . if this is the only light hitting it then it will be perceived as a dark but vibrant blue and we should let as much through as possible . however , if there is also a larger amount of red and green light hitting the filter then the blue light is merely diluting the saturation of a yellow shade , making it appear greyish , and we should block some of the blue light while letting the other frequencies through . this sort of thing would need to happen reliably across a very wide range of light intensities . while materials that have non-linear effects on the frequency of light do exist , creating something with properties this specific is probably impossible . of course , it is possible to enhance the saturation of an image digitally , so could we do that in something the size of a contact lens ? i strongly suspect this is also impossible , for a few different reasons . it is certainly far beyond the reach of modern technology . the first reason is the miniaturisation of the screen . it would have to have an enormous resolution , a dynamic range ( difference between the lowest and highest intensities it can emit ) many orders of magnitude higher than any currently available display , and draw an incredibly tiny amount of power . the second reason is the camera . not just because of the resolution , sensitivity and size of the sensor , but also because the distance between this camera 's lens and its sensor would have to be less than a millimetre , and i do not think there is any trick that would allow a sharp image to be produced under such circumstances . finally , the eye can not focus on something that is attached to its surface , so the screen would somehow have to emit parallel rays of light , rather than emitting light in all directions like a normal screen . i do not know whether this is possible for such a small device , but i suspect not . so unfortunately the answer to your question is , this certainly will not be possible in the near future , and it probably never will . on the other hand , if you do not mind wearing goggles then it is possible with current technology : you just need a virtual reality helmet with cameras on the front . i have tried such a device , and it is surprising how much it feels like you are not looking through a screen .
for the first question , you normally do not have a " real " short circuit , but a very low load ( say a milliohm ) . in this case there is a very low ( non measurable with normal instruments ) difference of potential and this will cause the current to flow through your load . the amount of current flowing in the load will depend on the internal resistance of the generator . if the internal resistance is $r_i$ and the load resistance is $r_l$ , with a open circuit potential difference of $v$ , the current will be $i=\frac{v}{r_i + r_l}$ . if you had a proper zero resistance load ( a real short circuit ) , the difference of potential across the load would be exactly zero but , being the resistance also zero , you would not need a potential difference to support a current ( think of electrons flowing frictionless in your load , so you do not need to provide energy to allow the electrons to win the friction , therefore current without difference of potential ) . in this case the current would be $i=\frac{v}{r_i}$ . this should help understanding the second question as well . if your load is constituted by two resistors in series ( say of resistances $r_1$ and $r_2$ ) , the total current in the circuit would be $i=\frac{v}{r_i + r_1 + r_2}$ , the difference of potential across the load would be $v_l=i ( r_1+r_2 ) $ and the drops across the resistors would be $v_1=i r_1$ and $v_2=i r_2$ .
the short answer to your conundrum is that you have two contradictory equations : ( 1 ) $\delta t ' = \gamma \delta t$ ( 2 ) $\delta t ' = \gamma \left ( \delta t - \delta x \dfrac{u}{c^2} \right ) $ now , the 2nd equation is generally true . the 1st equation is true only if $\delta x = 0$ ( well , and also if $u = 0$ but that is not the case here ) . but $\delta x$ is not zero ! i think that the root of your confusion is the failure to identify what the symbols in these equations actually represent . step by step : ( 1 ) there are two coordinate systems , primed and unprimed , in uniform relative motion . in the unprimed frame , the primed frame has a velocity $u = 0.6c$ . ( 2 ) in the primed coordinate system , twin a is at rest at the spatial origin . ( 3 ) in either system , we are considering the coordinates of twin a . thus , $\delta x$ is the displacement of twin a in the unprimed system and $\delta x'$ is the displacement of twin a in the primed system . so , for the outbound leg , $\delta x = 12ly , \delta t = 20y$ since the speed of twin a , in the unprimed system is $0.6c$ . now , in the primed system , $\delta x ' = 0$ ( again , twin a is at rest in the primed system ) . by the invariance of the interval , we have : $ ( c\delta t ) ^2 - \delta x^2 = ( c\delta t' ) ^2 - \delta x'^2$ with the values we have , we can solve for $\delta t'$: $\delta t'^2 = \delta t^2 - ( \frac{\delta x}{c} ) ^2 = ( 20y ) ^2 - ( \frac{12ly}{c} ) ^2$ $\delta t ' = 16y$ but , there is another , equivalent , way to find this $\delta t'$ . since twin a is at rest in the primed frame , twin a 's proper time is just $\tau_a = \delta t'$ so , we have : $\gamma_u \tau_a = \delta t = \gamma_u \delta t'$ so , where you went wrong was writing $\delta t ' = \gamma \delta t$ without checking to see if it made sense in this problem . this is why that equation gives the wrong answer . your conceptual issue seems to be with correctly identifying proper time . the proper time you are interested in is the proper time of twin a ( we already know the proper time for twin b ) and , since twin a is at rest in the primed frame , the correct identification of twin a 's proper time is : $\tau_a = \delta t'$ .
conservation laws typically hold only for closed systems . in this case the inner drum is losing mass so it can not be considered a closed system ; instead , you must also include the angular momentum of the outer drum . on the other hand , you are right that there is no torque on the inner drum , and that therefore its angular momentum - that of the drum itself , ignoring the sand - is also constant . together , these two constraints are enough to determine the two final velocities .
you only need to assume the schrödinger equation ( yes , the same old linear schrödinger equation , so the proof does not work for weird nonlinear quantum-mechanics-like theories ) the standard assumptions about projective measurements ( i.e. . the born rule and the assumption that after you measure a system it gets projected into the eigenspace corresponding to the eigenvalue you measured ) then it is easy to show that the evolution of a quantum system depends only on its density matrix , so " different " ensembles with the same density matrix are not actually distinguishable . first , you can derive from the schrödinger equation a time evolution equation for the density matrix . this shows that if two ensembles have the same density matrix and they are just evolving unitarily , not being measured , then they will continue to have the same density matrix at all future times . the equation is $$\frac{d\rho}{dt} = \frac{1}{i\hbar} \left [ h , \rho \right ] $$ second , when you perform a measurement on an ensemble , the probability distribution of the measurment results depends only on the density matrix , and the density matrix after the measurement ( of the whole ensemble , or of any sub-ensemble for which the measurement result was some specific value ) only depends on the density matrix before the measurement . specifically , consider a general observable ( assumed to have discrete spectrum for simplicity ) represented by a hermitian operator $a$ . let the diagonalization of $a$ be $$a = \sum_i a_i p_i$$ where $p_i$ is the projection operator in to the eigenspace corresponding to eigenvalue ( measurement outcome ) $a_i$ . then the probability that the measurement outcome is $a_i$ is $$p ( a_i ) = \operatorname{tr} ( \rho p_i ) $$ this gives the complete probability distribution of $a$ . the density matrix of the full ensemble after the measurment is $$\rho&#39 ; = \sum_i p_i \rho p_i$$ and the density matrix of the sub-ensemble for which the measurment value turned out to be $a_i$ is $$\rho&#39 ; _i = \frac{p_i \rho p_i}{\operatorname{tr} ( \rho p_i ) }$$ since none of these equations depend on any property of the ensemble other than its density matrix ( e . g . the pure states and probabilities of which the mixed state is " composed" ) , the density matrix is a full and complete description of the quantum state of the ensemble .
even though i tend to be of the same mind as the comments , i would like to answer , before the question is closed , since comments are limited in character counts . 1st ) validation . validation means that somebody , , in this case the investigator proposing a new theory comes up with hard numbers from the proposed theory and validates them by checking with the accurate data available from elementary particle tables to astronomy ones . i.e. checks predictions against data . if , for example , the standard model of particle physics does not come out naturally from the new theory , it is invalidated as a theory of everything . 2 ) if the first stage is passed , and the theory of your friend by some miraculous manner is validated by the existing data up to now , she can publish with her name and address , her theory and its validation in the archives so there is no way somebody will take the credit from her .
i would leave special relativity out of the picture . i mean : magnetism is a relativistic effect , in the sense that it pops out from the application of ( special ) relativity to electrostatics , but their relationship is more conceptual and may deserve a new question entirely devoted to the matter . long story short , you do not need special relativity to understand magnetic forces . all you need to know is that when a charged particle $q$ moves at a velocity $\mathbf{v}$ through an electric field $\mathbf{e}$ and a magnetic field $\mathbf{b}$ , it experiences the lorentz force $$ \mathbf{f_l} = q ( \mathbf{e} + \mathbf{v}\times\mathbf{b} ) $$ this really is the fundamental equation for electrodynamics , together with maxwell 's equations . now , a current-carrying wire is usually considered to be neutral overall ( i.e. . the number of electrons and ions are the same so that the net charge is zero ) , so you can take the electric field $\mathbf{e} = 0$ . then , you are dealing with a current , not with a single charge . how to get the above equation in terms of the current $i$ ? the electric current $i$ is defined to be $\frac{dq}{dt}$ where $q$ is the electric charge . the velocity is defined as $\frac{d\mathbf{l}}{dt}$ where $d\mathbf{l}$ is the path element in the direction of the current ( or of the charge 's trajectory ) . so , the infinitesimal force on an infinitesimal charge $dq$ is $d\mathbf{f_l} = dq ( \mathbf{v}\times\mathbf{b} ) $ , where $$ dq\cdot\mathbf{v} = dq\cdot\frac{d\mathbf{l}}{dt} = \frac{dq}{dt} d\mathbf{l} = id\mathbf{l}$$ . to get the total force , you just integrate along the path defined by $d\mathbf{l}$: $$\mathbf{f_l} = \int_l i d\mathbf{l}\times\mathbf{b} $$ ok . but you have two wires . and you know that a current carrying wire generates a magnetic field , given by ( assuming the wire is in a straigh line ) : $$ b_{\phi} = \frac{\mu_0}{2\pi}\frac{i}{r} $$ meaning that it is only in the tangential ( $\neq$ radial ) direction . the direction of the field is given by the right hand rule , line up your right thumb with the current direction and your fingers will tell you the direction of the magnetic field . this formula is obtained via ampère 's law . if the two wires are in a straight line and are separated by a distance $d$ , the value of the magnetic field due to one at the position of the other is $b = \frac{\mu_0}{2\pi}\frac{i}{d}$ . assume the wires carry equal , constant currents $i$ . using this as the magnetic field in the lorentz force , you get a force of $$ |\mathbf{f}| = i\cdot ( \frac{\mu_0}{2\pi}\frac{i}{d} ) \cdot \int_l dl = \frac{\mu_0}{2\pi}\frac{i^2}{d}\cdot l $$ where $l$ is the length of the wires , or a force per unit length of $$ |\mathbf{f}| = \frac{\mu_0}{2\pi}\frac{i^2}{d} $$ . $d\mathbf{l}$ points along the current : use this and the cross product in the lorentz force equation to get the direction of the resulting force . if the currents are parallel , the force is attractive , while if they are opposite , the force is repulsive . the magnitudes of the forces are the same , as they only depend on the wire separation $d$ and on the current $i$ . notice , the other wire will exert the exact same force on the fist , due to newton 's iii law of motion .
no , the entire weight will not directly rest on the base of the slanted container ( although it does indirectly ) . there are a number of ways to approach this , but the easiest way is to observe that the total force acting on the bottom of the container is equal to the sum of the hydrostatic pressure force ( the pressure at the bottom of the container multiplied by its area ) and the shear force around the edge of the base ( draw a free-body diagram to convince yourself of this ) . the hydrostatic pressure depends only on the height of the column of fluid above the given location ( $p=\rho gh$ ) , and the shear force is equal to the weight of the fluid outside of the base ( supported by the walls ) . since both vessels contain the same volume , but the slanted container has a wider cross section above the base , the total height of the body of fluid will be less for the container with slanted walls than the other container . thus , the hydrostatic pressure at the base will be less . but remember , any reduction in the pressure force will be compensated by an increase in the shear force around the edge of the base . the vertical walls support no vertical force , and so they exert no shear force on the base . the slanted walls do bear some of the weight , and so they transfer this force to the base via shear . in both cases , the sum of the hydrostatic pressure force plus the shear force ( if there be any ) is equal to the weight of the fluid , and this is what the scale measures .
say you have a weight tied to each side a a rope which is strung over a pulley with friction . here 's a really easy way to see why the tensions on each side of the rope can not be equal . imagine a really stiff pulley - in other words , ${\bf f}_{friction}$ is high . if that is the case , it'll be possible to balance unequal loads on this pulley system - i.e. a heavy wieght on the right side and a lighter weight on th left - without the system moving . if the weights do not move , then we can say that the forces acting on each weight add up to zero : for the heavy weight , there is the weight downward , ${\bf w}_{heavy}$ and there is the tension of the right side of the rope upward , ${\bf t}_{right}$ . the tension pulls up and the weight down , and the system does not move , so $$ {\bf t}_{right} - {\bf w}_{heavy} = 0 $$ or $$ {\bf t}_{right} = {\bf w}_{heavy} $$ similarly for the left ( light ) side , $$ {\bf t}_{left} - {\bf w}_{light} = 0 \quad \rightarrow \quad{\bf t}_{left} = {\bf w}_{light} $$ as you can see , the tension on the right , ${\bf t}_{right}$ is equal in magnitude to the heavy weight , while the tension on the left , ${\bf t}_{left}$ is equal to that of the lighter weight . the friction is introducing an extra force which changes the tensions on each side . as far as your question about rope stretching goes , if you anchor a rope on one side and pull , the rope will pull back , creating a tension . this is indeed because of stretching in the rope . this is not really what newton 's 3rd law is referring to . newton 's third law , in this case , tells us that the force that we feel from the rope , tension , is exactly the force the rope feels from us pulling . the two are equal and opposite . you can change the tension by changing the stiffness of the rope , but whatever the tension , newton 's 3rd law will still be true - the rope will feel us pulling it as much as we feel it pulling us .
we know that we can describe a spin $1/2$ massless particle using only a single weyl field ( lets say left-handed $\psi_{l}$ ) . to introduce a mass term we have to use two spinor fields ( one left-handed and one right-handed ) and this gives the dirac mass term . the question is now that if we can describe a massive particle with a single weyl field . well yes , due to the fact that given a left-handed weyl spinor , it is possible to construct a right-handed spinor $\psi_{r}=i\sigma^{2}\psi_{l}^{*}$ . thus , we can write the dirac equation using $i\sigma^{2}\psi_{l}^{*}$ $$\hspace{43mm} \bar{\sigma}^{\mu}i\partial_{\mu}\psi_{l}=im\sigma^{2}\psi_{l}^{*} \hspace{30mm} ( 1 ) $$ the known algebraic methods performed for the dirac equation to prove that it implies a massive klein-gordon equation can be performed here without any problems . thus , the above equation implies $ ( \box+m^{2} ) \psi_{l}=0$ . here we have constructed a mass term using only $\psi_{l}$ and this is known as majorana mass . the similarity with the dirac mass can be seen by writting $ ( 1 ) $ in terms of the four component majorana spinor $\psi_{m}$ in the chiral representation $$\psi_{m}=\begin{pmatrix} \psi_{l}\\i\sigma^{2}\psi_{l}^{*} \end{pmatrix}$$ now , equation $ ( 1 ) $ becomes $$ ( i\gamma_{\mu}\partial^{\mu}-m ) \psi_{m}=0$$ the majorana mass has a very important physical difference when compared to the dirac mass . we know that the dirac action with a mass term is invariant under a global $u ( 1 ) $ transformations of $\psi_{l}$ and $\psi_{r}$ ( i.e. . $\psi_{l}\rightarrow e^{i\alpha}\psi_{l} , \hspace{2mm}\psi_{r}\rightarrow e^{i\alpha}\psi_{r}$ ) . but for majorana spinors , $\psi_{l}$ and $\psi_{r}$ are not independent , they are related by complex conjugation . so , if $\psi_{l}$ transforms as $\psi_{l}\rightarrow e^{i\alpha}\psi_{l}$then $\psi_{r}$ transforms like $\psi_{r}\rightarrow e^{-i\alpha}\psi_{r}$ . the majorana equation $ ( 1 ) $ is not invariant under global $u ( 1 ) $ symmetries . this fact implies that a spin $1/2$ particle with a $u ( 1 ) $ conserved charge cannot have a majorana mass . all spin $1/2$ particles with an electric charge cannot have a majorana mass . also leptons that have a majorana mass violate the lepton number ( because this is a $u ( 1 ) $ symmetry ) . one possible particle that could have a majorana mass is the neutrino . but this is yet to be determined . ( i did not answer your questions point by point but i hope this clarifies some of them ) .
you have misunderstood the statement . when a exerts $\vec{f}_{a \to b}$ on b , b exerts an equal and opposite force $\vec{f}_{b \to a} = - \vec{f}_{a \to b}$ on a . the only forces acting on the ball a gravity and the normal force , and the floor experiences a force from the ball which is equal in magnitude and opposite in direction from the normal force on the ball .
the premise of the question is not correct , but there is a general shape to rivers . from leopold and langbein , writing in scientific american : a sample of 50 typical meanders on many different rivers and streams has yielded an average value for this ratio of ahout 4.7 to one . the ratio they use in that article is different from the definition you ( and wikipedia , and comments ) are using . i am sure it is simple algebra to convert one ratio to the other . but that article also notes that : for the large majority of meandering rivcrs the value of this ratio ranges between 1.3 to one and four to one . whatever the conversion comes out to , there appears to be quite a range of real-life meander ratios . that scientific american article is a summary of a longer , more technical article by the same authors . the method they use is to fix beginning and end points a and b , and allow the river to random walk from a to b . the most probable shape for such a path is what they call a " sine-generated " curve . at a given point , the angle between the tangent to the river and the mean direction of the river the sine of the distance along the channel . the resulting curve is not quite a semi-circular curve , so the meander ratio is not predicted to be $\pi$ . a more recent study by garret williams confirms leopold and langbein 's results , and reports that the most common value for the ratio of the radius of curvature to the channel width is between 2 and 3 . the other major effect driving river shape is how easily the river can erode the soil that it passes through . the river will tend to flow more directly downhill if the surrounding soil is difficult to erode . in areas where the soils erodes quite easily , the river will assume this sine-generated curvature . as an example of that , you can look at the mississippi river in the united states . it has the classic sine-generated shape all the way from about cairo , il south to new orleans , la . but it is much straighter up along the illinois/iowa border .
kinematically , yes . in terms of describing the positions of objects , it is equivalent to say " a is accelerating away from b " and " b is accelerating away from a " . however , it is an observed fact that the universe treats these two situations differently . a and b can check whether they feel artificial gravity in their reference frame . if so , it is accelerating . as far as i know , the " way the universe decides " to break this symmetry is a topic of continuing speculation . check out some related questions : inertia in an empty universe is acceleration an absolute quantity ? is rotational motion relative to space ? acceleration in special relativity newton&#39 ; s bucket what if the universe is rotating as a whole ?
if the universe is compact then its scale is at least 100 times greater than the observable universe ( because according to experiment the observable universe is flat to within 1% ) . no experiment of a length scale much shorter than this scale would be able to detect the compact topology , so the michelson morley experiment is not incompatible with a compact universe . it just means the universe is locally non-compact , which of course we already know just by looking around . even if the universe is compact its scale is increasing as the universe expands , and indeed the expansion is probably exponential due to dark energy . if so , no experiment we can ever do will show the universe to be compact .
given a vector space with a basis , one way to find the coordinates of a given vector with respect to that basis would be to define an inner product which makes the basis orthogonal . then if the vector in question is $v$ and the basis is $e_i$ , then the $i$th coordinate of $v$ is $\frac{v \cdot e_i }{|e_i|^2}$ . this is the strategy i will use here , but the question is to find the appropriate inner product . since all i seem to remember from qft is doing traces of these matrices , the natural inner product seems to be multiplying the two matrices together and taking the trace . i should say that to stick with this strategy i will view the indices as enumeration indices instead of lorentz indices . thus i will not raise or lower them although summation will be implied over repeated indices . at the very end i will go back to usual notation . the first step then is to verify that this basis is indeed orthogonal with respect to this inner product . this is easily checked once a few identies are recalled . first , the trace of $\gamma^5$ with fewer than four $\gamma$s ( of the non-five variety ) is zero ; $\gamma^5$ anticommutes with $\gamma^\mu$ ; $\mathrm{tr} \gamma^5 = 0$ ; the trace of an odd number of $\gamma$ 's ( of the non-five variety ) is zero ; $ ( \gamma^5 ) ^2=1$ ; and $\mathrm{tr} ( \gamma^\mu \gamma^\nu ) = 4 g^{\mu \nu}$ . now we need to compute two things : the square norm of each basis element , and compute the inner product of our vector with each basis element . let 's start with the first . we have $\mathrm{tr} ( 1*1 ) =4$ , $\mathrm{tr} ( \gamma^5*\gamma^5 ) =\mathrm{tr} ( 1 ) =4$ , $\mathrm{tr} ( \gamma^\mu *\gamma^\nu ) =\mathrm{tr} ( \gamma^\mu \gamma^\nu ) = 4g^{\mu\nu}$ . this is zero for $\mu \ne \nu$ and it is $4 ( -1 ) ^\mu$ for $\mu = \nu$ , where $ ( -1 ) ^\mu$ is just the $\mu$th diagonal entry of $g^{\mu\nu}$ . $\mathrm{tr} ( \gamma^\mu \gamma^5*\gamma^\nu \gamma^5 ) =\mathrm{tr} ( -\gamma^\mu \gamma^\nu \gamma^5\gamma^5 ) = -4g^{\mu\nu} = -4 ( -1 ) ^\mu$ for $\mu = \nu$ and zero otherwise . $\mathrm{tr} ( [ \gamma^\mu , \gamma^\nu ] * [ \gamma^\lambda \gamma^\eta ] ) $ is just $4 \mathrm{tr} ( \gamma^\mu \gamma^\nu \gamma^\lambda \gamma^\eta ) $ antisymmetrized over $ ( \mu \nu ) $ and $ ( \lambda \eta ) $ , so it is the same as $16 ( g^{\mu \nu} g^{\lambda \eta} - g^{\mu \lambda} g^{\mu \eta} + g^{\mu \eta} g^{\lambda \nu} ) $ antisymmetrized over $ ( \mu \nu ) $ and $ ( \lambda \eta ) $ . thus it is the same as $-16 ( g^{\mu \eta} g^{\lambda \nu} -g^{\mu \lambda} g^{\mu \eta} ) $ antisymmetrized over $ ( \mu \nu ) $ and $ ( \lambda \eta ) $ . since we do not consider $\mu = \nu$ or $\lambda = \eta$ here , this is only non-zero if $\{\mu , \nu \} = \{\lambda , \eta\}$ as a set . without loss of generality we may assume $\mu = \lambda$ and $\nu = \eta$ . then the expression becomes $-16 ( ( -1 ) ^\mu ( -1 ) ^\nu ) $ now that that is done we can compute our inner products with the matrix in question . the inner products of our vector with $1$ , $\gamma^\lambda$ , and $\gamma^\lambda \gamma^5$ are zero . however , the inner product with $\gamma^5$ is $4 g^{\mu \nu}$ , and the inner product with $ [ \gamma^\lambda , \gamma^\eta ] $ is $-8i \epsilon^{\lambda \eta \mu \nu}$ . putting the two pieces from the last two paragraphs together , we get that our matrix is $g^{\mu \nu} \gamma^5 + \frac{i \epsilon ^{\mu \nu \lambda \eta}}{2 ( -1 ) ^\lambda ( -1 ) ^\eta}\ \ [ \gamma^\lambda , \gamma^\eta ] = g^{\mu \nu} \gamma^5 + \frac{i}{2} \epsilon ^{\mu \nu} {}_{\lambda \eta} [ \gamma^\lambda , \gamma^\eta ] $ . note this expression differs from one given in the comments to the question . this formula can be checked with mathematica code :
the radiation density has two components : the present-day photon density $\rho_\gamma$ and the neutrino density $\rho_\nu$ . the photon density as a function of frequency can be derived directly from the cmb : the photon number density follows the planck law $$ n ( \nu ) \ , \text{d}\nu = \frac{8\pi\nu^2\ , \text{d}\nu}{e^{h\nu/k_b t_0}-1} , $$ with $k_b$ the stefan-boltzmann constant , and $t_0$ the current cmb temperature . the photon energy density is then $$ \rho_\gamma\ , c^2 = \int_0^{\infty}h\nu\ , n ( \nu ) \ , \text{d}\nu = a_b\ , t_0^4 , $$ where $$ a_b = \frac{8\pi^5 k_b^4}{15h^3c^3} = 7.56577\times 10^{-16}\ ; \text{j}\ , \text{m}^{-3}\ , \text{k}^{-4} $$ is the radiation energy constant . with $t_0=2.7255\ , \text{k}$ , we get $$ \rho_\gamma = \frac{a_b\ , t_0^4}{c^2} = 4.64511\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ the neutrino density is related to the photon density : in eq . ( 1 ) on page 5 in the paper , you see that $$ \rho_\nu = 3.046\frac{7}{8}\left ( \frac{4}{11}\right ) ^{4/3}\rho_\gamma . $$ this relation can be derived from physics in the early universe , when neutrinos and photons were in thermal equilibrium . so $$ \rho_\nu = 3.21334\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} , $$ and the total present-day radiation density is $$ \rho_{r , 0} = \rho_\gamma + \rho_\nu = 7.85846\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ we can also express this relative to the present-day critical density $$ \rho_{c , 0} = \frac{3h_0}{8\pi g} = 1.87847\ , h^2\times 10^{-26}\ ; \text{kg}\ , \text{m}^{-3} , $$ where the hubble constant is expressed in terms of the dimensionless parameter $h$ , as $$ h_0 = 100\ , h\ ; \text{km}\ , \text{s}^{-1}\ , \text{mpc}^{-1} , $$ so we get $$ \begin{align} \omega_{\gamma}\ , h^2 and = \dfrac{\rho_\gamma}{\rho_{c , 0}}h^2 = 1.71061\times 10^{-5} , \\ \omega_{\nu}\ , h^2 and = \dfrac{\rho_\nu}{\rho_{c , 0}}h^2 = 2.47282\times 10^{-5} , \\ \omega_{r , 0}\ , h^2 and = \omega_{\gamma}\ , h^2 + \omega_{\nu}\ , h^2 = 4.18343\times 10^{-5} . \end{align} $$ for a hubble value $h=0.673$ , one finds $\omega_{r , 0} = 9.23640\times 10^{-5}$ .
in an ideal situation ( no air resistance ) there will be absolutely no difference in the place where the coin lands ! whether you toss the coin up from inside the train or while standing on the roof , the coin will land back in your hand ( provided you have tossed it perfectly vertically ) . however , in practice , while standing on a fairly fast train 's roof , there is a lot of wind because you are moving at high speed . the moment you toss the coin , the wind force acts on it , and creates an acceleration in the backward direction , making it go slower than the train ( and you ) . however , it will not land in exactly the same place from where you tossed it every time ! this purely depends on the retarding force acting on the coin - in this case , the wind .
let me give you an intuition of what components are and then i will answer your question : i pull a dog on a leash with a tension of 5n at an angle of 53.1 degrees . this is equivalent to pulling the dog simultaneously horizontally with a rope with a force of 3n ( 5cosθ ) and vertically with a rope with a tension force of 4n ( 5sinθ ) in both scenarios , the dog " feels " the same . in other words , the two components are equivalent to the single force of 5n at an angle of 53.1 degrees . in your question , the weight only acts downwards . we can " break " the force down using a coordinate system and find two components which are equivalent to the weight . the vertical weight vector can be divided into components along the slope and perpendicular to it . by replacing the weight vector by these components , and dividing the other forces into the same coordinate system , we can determine the motion of the object along and perpendicular to the plane .
i have found the solution to my problem . first of all , i had a factor 2 discrepancy due to the pixel dimension and , more important from an optical point of view , the laser beam was underfilling the entrance pupil of the objective . this means the focusing was worse !
let $ [ a , a^{\dagger} ] ~=~1 , $ and let $|0\rangle$ be the vacuum state : $a|0\rangle=0$ . define $$|n\rangle~:=~ \frac{1}{\sqrt{n ! }} ( a^{\dagger} ) ^n|0\rangle . $$ then $$ a |n\rangle ~=~ \sqrt{n} |n-1\rangle , \qquad a^{\dagger} |n\rangle ~=~ \sqrt{n+1} |n+1\rangle , \qquad\langle n |m\rangle ~=~ \delta_{n , m} . $$ an arbitrary linear operator is of the form $$t= \sum_{n , m\in\mathbb{n}_0} |n\rangle t_{nm} \langle m| , \qquad t_{nm}~:=~\langle n|t |m\rangle~\in~ \mathbb{c} , $$ so it is enough to study operators of the form $|n\rangle \langle m|$ . it is straightforward to see that $$ |n\rangle \langle m|~=~\sum_{k\in\mathbb{n}_0} c^{nm}_k ( a^{\dagger} ) ^{n+k} a^{m+k} , $$ where there exist unique coefficients $c^{nm}_k\in \mathbb{c}$ , which can be recursively obtained from the relations $$\delta_k^0~=~\sum_{r=0}^k c^{nm}_{k-r} \sqrt{ \frac{ ( n+k ) ! }{r ! } \frac{ ( m+k ) ! }{r ! } } . $$
it is simply a matter of notation . the $p_1$ ( and hence $e_1$ and $e_2$ ) in $$\int d\pi_2=\int d\omega\frac{p_1^2}{16\pi^2e_1e_2} ( \frac{p_1}{e_1}+\frac{p_1}{e_2} ) ^{-1}$$ is no longer an integration variable ; it has the fixed value that satisfies the delta function $\delta ( e_{cm}-e_1-e_2 ) $ in the previous integral . the factor $ ( \frac{p_1}{e_1}+\frac{p_1}{e_2} ) ^{-1}$ comes from applying an identity of the delta function : $$\delta ( g ( x ) ) = \frac{\delta ( x-x_0 ) }{|g' ( x_0 ) |} . $$
the statement " a , b , c " are cyclic rotations ( more often : cyclic permutations ) of " e , f , g " means that " a , b , c " is either " e , f , g " or " f , g , e " or " g , e , f " , in one of these three orders ( but not in the remaining orders " e , g , f " , " f , e , g " , " g , f , e" ) . relatively to " e , f , g " , these three letters were ordered by one of the three elements of the so-called cyclic group ${\mathbb z}_3$ . similarly for cyclic rotations of $k$ elements and the group ${\mathbb z}_k$ . in your example , if you used a non-cyclic permutation of the pauli matrices , you would get $j*k-k*j = -i*l$ with the minus sign on the right hand side because the pauli matrices anticommute . in particular , let me stress that no operation is applied " inside " the matrices . they are treated as wholes , as elements of a set , that are being permuted with other elements .
the z-projection of the angular momentum of a completely unpolarized system is distributed uniformly both classically and quantum mechanically . in quantum mechanics , the density matrix of a three state system is given by : $\rho = \frac{1}{3} ( i_3+\overrightarrow{n} . \overrightarrow{\lambda} ) $ , where $i_3$ is the three dimensional unit matrix . $\overrightarrow{n}$ is the polarization vector and $\overrightarrow{\lambda}$ is a vector of gell-mann matrices . in a fully polarized system $\overrightarrow{n} . \overrightarrow{n}= 1$ , while in the case of a completely unpolarized system : $\overrightarrow{n} = 0$ . in this case : $\rho = \frac{1}{3} ( i_3 ) $ . therefore , the three states are equally distributed . in classical mechanics , the phase space of a spin system is the unit sphere . the z-component of a unit angular momentum system is given by : $ j_z = cos ( \theta ) $ . where $\theta$ is the inclination angle . a completely unpolarized classical angular momentum should be distributed uniformly with respect to the surface area of the sphere which is the classical phase space . the area of a spherical zone of thickness $h$ is $s = 2 \pi r h$ , where $r=1$ is the sphere radius ( this result was already known to archimedes ) . therefore the z-coordinate is uniformly distributed on the sphere and the z-component of the angular momentum expressed as : $ j_z = \frac{z}{r}$ is linear in $z$ , therefore also uniformly distributed .
i do this sort of work in uhecr anisotropy . there are numerous techniques ( which i am currently working on advancing ) . expansion in spherical harmonics is popular ( and what i am working on ) . for that we use the fact that the spherical harmonics are complete to write $$f=\sum_{\ell , m}a_{\ell m}y_{\ell m}$$ and then from orthogonality we can write ( assuming a given normalization ) $$a_{\ell m}=\int d\omega fy_{\ell m}$$ once we have the coefficients $a_{\ell m}$ we have a number of approaches . one obvious one is to follow the approach of the cmb people and write the power spectrum as $$c_\ell=\frac1{2\ell+1}\sum_ma_{\ell m}^2$$ which is axis independent . then you can compare this to that from isotropy . in the pure case you can write for monopoles that $c_0&gt ; 0 , c_\ell=0\forall\ell&gt ; 0$ where the value depends on the normalization . the remaining terms all show deviations from anisotropy . for the sum of discrete events a mc simulation is useful here . another common standard definition that is quite simple is to define $$\alpha=\frac{\max f-\min f}{\max f+\min f}$$ isotropy gives $\alpha=0$ as desired . in addition , for the simple dipole case , we can write $f=1+a\cos\theta$ where $a$ measures the strength of the dipole . $a=0$ is obviously isotropy . $a=1$ gives $f=2$ at $\theta=0$ ( the " direction " of the dipole ) and $f=0$ at $\theta=\pi$ . note that the 1 ( and the implicit requirement $a\le1$ ) is to ensure that $f$ is positive-definite ( which may or may not be necessary for your situation ) . then , at $a=1$ , we get $\alpha=1$ ( and in fact it is easy to show that $a=\alpha$ ) . note that the three $ ( \ell , m ) = ( 1 , m ) $ spherical harmonics are all degenerate . a similar thing can be done for the quadrupole case , at least for the $m=0$ case , with $f=1-b\cos^2\theta$ where $b$ and $\alpha$ have a more complicated ( but still simple ) relationship . for more complicated shapes you can try to match your geometry with spherical harmonics with what is known as the $k$-matrix approach presented here ( arxiv abs ) in section 3 . in that example they consider a sphere with non-uniform exposure . i suspect that the same ( or a similar ) approach could be used for your case . i should warn though that the reconstruction power falls off very quickly with $\ell_{\rm{max}}$ in the $k$-matrix , so do not go any higher than you have to .
the standard way is to use generating functions ( in this case a la coherent states ) . usually one would like the resulting formula to be normal-ordered . recall the following version $$\tag{1} e^ae^b~=~e^{ [ a , b ] }e^be^a$$ of the baker-campbell-hausdorff formula . the formula ( 1 ) holds if the commutator $ [ a , b ] $ commutes with both the operators $a$ and $b$ . put $a=\alpha a $ and $b=\beta a^{\dagger}$ , where $\alpha , \beta\in\mathbb{c}$ . let $ [ a , a^{\dagger} ] =\hbar {\bf 1}$ , so that the commutator $ [ a , b ] =\alpha\beta\hbar {\bf 1}$ is a $c$-number . now taylor-expand the exponential factors in eq . ( 1 ) . for fixed orders $n , m\in \mathbb{n}_0$ , consider terms in eq . ( 1 ) proportional to $\alpha^n\beta^m$ . deduce that the the antinormal-ordered operator $a^n ( a^{\dagger} ) ^m$ can be normal-ordered as $$\tag{2} a^n ( a^{\dagger} ) ^m~=~\sum_{k=0}^{\min ( n , m ) } \frac{n ! m ! \hbar^k}{ ( n-k ) ! ( m-k ) ! k ! } ( a^{\dagger} ) ^{m-k}a^{n-k} . $$ finally , deduce that the normal-ordered commutator is $$\tag{3} [ a^n , ( a^{\dagger} ) ^m ] ~=~\sum_{k=1}^{\min ( n , m ) } \frac{n ! m ! \hbar^k}{ ( n-k ) ! ( m-k ) ! k ! } ( a^{\dagger} ) ^{m-k}a^{n-k} . $$
the state for generator a can be written more formally as , $$ | a \rangle = \sqrt{\frac{1}{4}} | 1 \rangle_{a} + \sqrt{\frac{1}{4}} | 2 \rangle_{a} +\sqrt{\frac{1}{4}} | 3 \rangle_{a} +\sqrt{\frac{1}{4}} | 4 \rangle_{a} \ , , $$ where $ | 1 \rangle_{a}$ represents the generator $a$ in " state " number 1 . the probablity of getting , for example , number 3 from generator $a$ is derived as , $$ |\langle 3| a \rangle|^{2} = \left| \sqrt{\frac{1}{4}} \langle 3 | 1 \rangle_{a} + \sqrt{\frac{1}{4}} \langle 3| 2 \rangle_{a} +\sqrt{\frac{1}{4}} \langle 3| 3 \rangle_{a} +\sqrt{\frac{1}{4}} \langle 3| 4 \rangle_{a} \right|^{2} = \frac{1}{4} $$ analogously $b$ is more formally given by $$ | b \rangle = \sqrt{\frac{1}{3}} | 2 \rangle_{b} +\sqrt{\frac{1}{3}} | 3 \rangle_{b} +\sqrt{\frac{1}{3}} | 4 \rangle_{b} \ , . $$ a " superposition " of these generators could be their direct sum , if we are considering the states or numbers $a$ and $b$ generate to be " different " . $$ | a \rangle \oplus | b \rangle = \sqrt{\frac{1}{4}} | 1 \rangle_{a} + \sqrt{\frac{1}{4}} | 2 \rangle_{a} +\sqrt{\frac{1}{4}} | 3 \rangle_{a} +\sqrt{\frac{1}{4}} | 4 \rangle_{a} + \sqrt{\frac{1}{3}} | 2 \rangle_{b} +\sqrt{\frac{1}{3}} | 3 \rangle_{b} +\sqrt{\frac{1}{3}} | 4 \rangle_{b} $$ where it is now understood that the kets $ | \ldots \rangle_{a}$ and $| \ldots \rangle_{b}$ live in the direct sum space $ a \oplus b $ . now the probability that $a$ generates number "2" is given by , $$ | \langle 2_a | a \oplus b \rangle |^{2} = |\langle 2| a \rangle|^{2} = \frac{1}{4} = p ( 2|a ) $$ and the probability that $b$ generates number "2" is given by , $$ | \langle 2_b | a \oplus b \rangle |^{2} = |\langle 2| b \rangle|^{2} = \frac{1}{3} =p ( 2|b ) \ , . $$ just as we had before . if we want the probabilities of each generator given the number "2" we have , $$ p ( a|2 ) = \frac{ p ( 2|a ) p ( a ) }{ p ( 2 ) } \ ; , \ ; p ( b|2 ) = \frac{ p ( 2|b ) p ( b ) }{ p ( 2 ) } $$ now we need $p ( a ) $ , $p ( b ) $ and $p ( 2 ) $ . if we regard the generators to be chosen equally as likely $p ( a ) = p ( b ) $ , we get for the likelihood ratio of the generators , $$ \frac{p ( a|2 ) }{p ( b|2 ) } = \frac{p ( 2|a ) }{p ( 2|b ) } = \frac{3}{4} $$
you pretty much know it already . " random " is a broad word that we use to mean that we can not predict behavior . each of the three cases of randomness that you cite is unpredictable for a different reason , though - that is the difference . dice are random because they are complicated , chaotic pendulums are random because we are not good enough to measure their initial position perfectly , and quantum systems are random because they are not deterministic . expanding on that : the randomness of a coin toss or a dice roll is based on an imprecise model . in principle , if we knew everything about the coin ( its initial position , the forces applied , the density of air that slows it down , etc ) then you could predict whether it winds up heads or tails with certainty . in the real world , nobody bothers , since constructing this model is very difficult . it depends sensitively on the height you are flipping the coin from , its initial spot on your thumb , and so on . chaotic randomness is due to imprecision in initial measurements alone . different initial conditions do not cause smooth changes in the final outcome . a good example is the chaotic motion of the planets - if we try to predict the position of saturn in 500,000,000 years , we get a certain position based on where it is now and all of the forces acting on saturn . but if we choose a slightly different initial condition - say , 10cm further along in its orbit to start - then we get a totally different answer potentially hundreds of thousands of km off . then we look at an initial condition in between , 5cm further along - and the deviation is even worse - it is now millions of kilometers off ! in other words , it chaotic randomness arises in systems where improving your accuracy of measurement does not help . the only way to get the " true " answer is to have the exact initial value . quantum randomness is due to fundamental laws of nature . quantum particles behave randomly on their own because that is just what they do , axiomatically . there is no initial measurement which could even be exact . the outcome is fundamentally nondeterministic , not a limit based on our models or our measurements . in some sense , quantum randomness guarantees that we can never " beat " chaos by getting a perfect initial measurement . but it arises from a fundamentally different origin .
yes . if you define $f=-\partial_\mu a^\mu$ then you can write the equation in the form $$ \partial_\mu\partial^\mu\psi = f$$ this is the klein-gordon equation with a nonzero source ( $f$ ) and can be solved via green 's function methods . once you have the klein-gordon propagator* $g ( x ) $ ( this is derived in any e.g. quantum field theory textbook ) appropriate to the boundary conditions the solution can be written as $$ \psi ( x ) =\int d^4 x ' g ( x-x' ) f ( x' ) $$ since green 's functions by definition satisfy $$\partial_\mu\partial^\mu g ( x-x' ) = \delta ( x-x' ) $$ where we take all differentiations to be with respect to x . *you need the propagator in the position space representation to write this down . it is usually more convenient to write it in momentum space ; you can go back and forth using ( inverse ) fourier transforms .
when two or more sources of light combine incoherently ( not in any fixed phase relation ) , you can only " add " , and that is intensity ( power ) - electromagnetic field " squared " and averaged , in the appropriate sense . when you have control over phase relations between two beams , yeah , sure you " add " the two at some point . to " subtract " all you need to do is shift one beam by 180degrees . ( you are dealing with monochromatic light ? ) just add an optically transparent material sufficient to delay the beam by half a wavelength , and there you go . note that " subtract " and " add " may be meaningful only in a limited sense , in the change of phase relationship of two beams in a stable optical apparatus . that is , it is not in the phase relationship itself , but in how it changes by 180 degrees . this is practical and sufficient in many experiments . you do not really have control over things such as spacing of holes in an optical table down to 100nm accuracy , but whatever the distances are , you can be sure they will not change by much during the course of the experiment ( &lt ; 100nm ) . if you have absolute geometric control to several-nanometer scale accuracy all the way from beam splitter to beam combiner ( or detector ) then you could compare the optical paths and say that the beams add , or subtract , or combine in an in-between phase relation , depending on the difference of optical paths being integer , half-integer or other number of wavelengths .
" restoring " forces refer primarily to forces that try to return a system to equilibrium . so a spring has a restoring force of $f = -k\delta x$ . this means that if you choose the origin as being $x = 0$ , then compressing the spring would correspond to a negative $x$ ( displacing the spring to the left ) , and stretching the spring would correspond to a positive $x$ ( stretching the spring to the right ) . in that sense , by extending the spring , a positive $\delta x$ creates a negative force ( $-1 \times \delta x$ ) that acts to restore the spring to equilibrium ( pulling back on the spring extension ) and by compressing the spring , you would have a negative $\delta x$ ( $-1 \times \delta x$ ) , which creates a positive force that restores the spring equilibrium . so hooke 's law is actually $f=-k \delta x$ hope that helps .
excellent question ! in short , there are two logical possibilities to explain the data : there is dark matter and a cosmological constant ( standard model ) gravity needs to be modified interestingly , both possibilities have historical precedent : the discovery of neptune ( by johann gottfried galle and heinrich louis d’arrest ) one year after its prediction by urbain le verrier was a success-story for the dark matter idea . ( of course , after its discovery by astronomers it was no longer dark . . . ) the non-discovery of vulcan was the a failure of the dark matter idea - instead , gravity had to be modified from newton to einstein . ( funnily , vulcan actually was observed by lescarbault a year after its prediction by urbain le verrier , but this observation was never confirmed by anyone else . ) so basically you are asking : are we in a neptune or a vulcan scenario ? and could not the vulcan scenario be more credible ? the likely answer appears to be no . modifications of gravity that seem to explain galactic rotation curves are usually either in conflict with solar system precision tests ( where einstein 's theory works extraordinarily well ) or they are complicated and less predictive than einstein 's theory ( like teves ) or they are not theories to begin with ( like mond ) . besides the gravitational evidence for dark matter , there is also indirect evidence from particle physics . for instance , if you believe in grand unification then you must also accept supersymmetry so that the coupling constants merge in one point at the gut scale . then you have a natural dark matter candidate , the lightest supersymmetric particle . there are also other particle physics predictions that lead to dark matter candidates , like axions . so the point is , there are no lack of dark matter candidates ( rather , there an abundance of them ) that may be responsible for the galactic rotation curves , the dynamics of clusters , the structure formation etc . note also that the standard model of cosmology is a rather precise model ( at the percent level ) , and it requires around 23% of dark matter . there are a lot of independent measurements that have scrutinized this model ( cmb anisotropies , supernovae data , clusters etc . ) , so we do have reasonable confidence in its validity . in some sense , the best evidence for dark matter is perhaps the lack of good alternatives . still , as long as dark matter is not detected directly through some particle/astro-particle physics experiment it is scientifically sound to try to look for alternatives ( i plead guilty in this regard ) . it just seems doubtful that some ad-hoc alternative passes all the observational tests .
congratulations to your cute and solid paper and your new loophole that is morally on par with a loophole circumventing the coleman-mandula theorem itself – almost . ; - ) i am confident you did the algebra correctly so let me offer you the form of the lore that i usually present and the way how you circumvented it . the lore says that the local fermionic transformations are generated by the density of a locally conserved quantity – the supercharge : and you may call it this way in your case , too . the anticommutator of such supercharges must be a " spacetime vector " of a sort and this statement must hold at the level of the densities , too . however , one must be careful what is the " spacetime vector " and how many components of it are participating in the algebra . normally , the lore assumes that the " real large spacetime dimensions " i.e. the momenta and energies must be included on the right-hand side of the fermionic anticommutator algebra . when localized , this would inevitably lead to a gravitating theory . nevertheless , it is not strictly necessary that all components of a spacetime vector are included on the right hand side in this way and it is not true that the energy-momentum is the only conserved quantity that transforms as a vector . as your example shows , one may split the 11 dimensions of m-theory and the " necessary appearance of a vector-like generator " on the right hand side does not have to include the regular momentum along the m5-brane directions at all . instead , your example has different things that transform as a vector , but they are not the energy-momentum . instead , they are the " winding charge of the boundary of an m2-brane " or the density of the " self-dual strings " dissolved within your m5-brane . this " winding charge " is a heuristic way how to describe the generator of the $$\delta b_{\mu\nu} ( x ) = \partial_\mu \lambda_\nu ( x ) - \partial_\nu \lambda_\mu ( x ) $$ local gauge transformation for the two-form potential . much like the regular electromagnetic gauge transformations are generated by a density of the electric charge , these extended $p$-form transformations are generated by densities of various strings and branes ' winding and wrapping numbers . so the algebraic requirement of having the density of a vector on the right hand side is protected and this part of the lore is preserved ; however , you debunk the assumption of the lore that the conserved vector has to be energy-momentum . instead , your conserved vector is a sort of the winding number of the self-dual string and its density enters the right hand side . let me note that having at least 2 of the minimal spinors of supercharges is a necessary condition for you to be able to avoid the energy-momentum : with extended ( chiral ) supersymmetry like the ( 2,0 ) supersymmetry , you may get purely " central charges " on the right hand side and send the coefficient of the normal energy-momentum to zero . the minimal ${\mathcal n} = 1$ supersymmetry does not have any central charges so the energy-momentum ( and therefore gravity , in the local case ) has to appear on the right hand side . the lore has therefore incorrectly generalized the experience from the minimal supersymmetry , not acknowledging that the energy-momentum and central-charge terms in supersymmetry algebras may " decouple " and beat each other in different ways .
my first naive guess ( in line with andy 's comment but not terribly well thought out ) is that no , morphisms of voas are not physically terribly natural - rather the more natural thing to consider is an appropriate notion of bimodule for two voas , which would be domain walls between the corresponding chiral cfts ( or 3d tfts , in the rational case ) . to make an analogy one dimension down , let 's think that we have an associative ( or if you prefer $a_\infty$ ) algebra , and we use it to try to define a 2d tft --- i.e. we can always integrate it over the circle to get a vector space ( take hochschild homology or center , depending on how you think of circles ) and if it is fully dualizable ( f . d . semisimple in the abelian case or " smooth proper " in the dg case ) we can integrate it over 2-manifolds to get numbers . however from the tft point of view what is important is the category of modules over the algebra rather than the algebra itself -- i.e. , relations between two algebras are given by bimodules ( ie morita morphisms ) , not necessarily by morphisms of algebras . these are exactly domain walls between 2d field theories . ( this is also natural from thinking of 2d tfts as noncommutative varieties --- only in the commutative case does it really make sense to focus on maps of algebras , since in that case you can recover the algebra from the corresponding category or tft ) . [ if you want honest 2d cfts rather than modular functors then you want a modification of the story above . . ] likewise i think ( following costello and lurie ) of a voa as what you attach to a point in a 2d chiral cft or modular functor ( ie we are attaching vector spaces to riemann surfaces , obtained by integrating the voa over the surface --- conformal blocks , aka chiral homology ) . the coarse topological analog is an e_2 algebra . in any case what seems physically meaningful is domain walls between these modular functors ( or the corresponding 3d tft if it makes sense ) , and these are " chiral bimodules " for two vertex algebras : something you can put on a wall , so that on either side you have bulk operators given by your two voas ( and in particular there is also a " boundary ope " structure on this chiral bimodule --- topologically this would be an associative algebra object in bimodules over two e_2 algebras ) . anyway to summarize usual morphisms of vertex algebras are special cases of something more natural , which are domain walls of chiral cfts , which give monoidal functors between the monoidal categories of left modules ( "boundary conditions" ) for the two voas . .
the relevant "100%" from which you should calculate the percentage is not the depth of the ocean but the radius of the earth $$ r\sim 6,378,000\ , {\rm m} $$ multiply this $r$ by $10^{-7}$ and you will get $0.6$ meters , a reasonable estimate for average tides . you must understand that the surface of the ocean always tries to create an " equipotential surface " – connect all points that have the same gravitational potential . the earth 's gravity ( plus the centrifugal potential ) adds the major contribution to the potential and , as you said , the moon modifies this function by corrections that are 7 orders of magnitude smaller and that are anisotropic ( different in different directions ) . that is why the ellipse we get because of the moon will differ from the previous one by corrections of order $10^{-7}$ , too . for example , if you imagine the moon-less earth to be a sphere , its ocean is spherical , i.e. ellipsoid with semi-axes $a=b=c$ . a correction to the original potential that is 7 orders of magnitude smaller will create $|a-b|/a$ of order $10^{-7}$ . all these calculations may be done much more accurately although the precise shape of continents etc . is needed for learning the precise shape of tides at various points of the real globe . whether there is 100 meters of water , 11 km of water , or ( unrealistically ) 3,000 km of water beneath a point on the ocean surface plays no role in the fact that the ocean will be elevated or suppressed by a few meters or so .
there is an additional loss of energy when driving through puddles on a wet road , because the tire treads have to exert work in order to eject water . one way to look at it is that the keeps trying to glide on top of the water , but is continuously sinking into it to meet the pavement , which is equivalent to driving slightly uphill .
the news release from nasa does mention this . apparently in this transitional phase the field is not well represented as a dipolar field . right now it may be a quadra polar or higher ( even numbered ) field . think of the total field being the superposition of multiple non-axial non-centered dipoles . the field is weakening in intensity too and may become weaker still before it strengthens into a reversed dipole field .
the order of magnitude given by gugg is correct . the molar volume for the succinic acid is $v_m=\frac{m}{\rho}=\frac{118.09}{1.59}\frac{cm^3}{mol}=74.27cm^{3}/mol$ where $m$ is the molar mass and $\rho$ the density . from this , you find the volume of the molecule to be $v_{molec}=\frac{v_m}{n_a}=1.23\cdot10^{-22}cm^3 $
the partial pressure of ethanol in the air space above the beer would be approximately given by raoult 's law . the vapour pressure of ethanol at 20°c is about 5.4kpa and water at the same temperature is 2.3kpa . the percentage by volume of ethanol in budweiser is 5% , which is about 4% by weight or a mole fraction of 0.016 . so the partial pressure of the ethanol vapour is about 87pa . this corresponds to about 1 milligram of ethanol per litre of air . you would have to inhale a very large amount of the vapour to get drunk . note that this calculation does not ( at least to a first approximation ) depend on the partial pressure of other gases like nitrogen and oxygen ( i.e. . air ) so pumping air into the bottle will not affect the amount of ethanol in the vapor above it and will not increase the amount of ethanol you can inhale . the gentleman in the video appears to be having a good time , but it is unlikely to be due to inhalation of alcohol . if his intoxication is anything other than psychosomatic it is probably due to dizziness brought on by hyperventilation . i would just drink the beer .
mmc gave the answer in his comment to the original question above . generally on such big airliners , between 10% and 20% of the lift is generated by the body rather than the wings . see the paper he linked to , figure 7 .
the confusion arises because there are two different versions of what earth 's surface looks like , and two different models of how gravity works between the case where the ball goes into orbit and the case where both balls hit the ground at the same time . we often approximate gravity near the earth 's surface by saying that it is constant everywhere . this approximation is pretty good if we are only interested in exploring distances much smaller than earth 's radius . in this approximation , the two balls will always hit the ground at the same time , no matter what speed they leave the surface with . but , in reality , the earth is round , and if you push the ball so fast that it travels a distance which is comparable to the radius before it lands , you must take a more sophisticated picture of gravity and the earth into account . in this picture , it is possible that the fast moving ball will remain in orbit forever , or even fly away from the earth never to return .
both are right . any moving clock is slower than a clock at rest , from the perspective of the frame at rest . maybe this simplified freehand graphic ( apologies for its lack of precision ) helps to see that both a and b feel the same about each other 's time dilation : let 's say that the red axis represents a and its proper time measured in minutes ( first eight minutes are showed ) . green axis and its numbers represents b observer . light or radio signals from a to b , represented in red oblique lines , are fired on a minute basis . six of them are showed , that took six minutes of a proper time . however , these six signals from a to b take some eight minutes in b proper time . b concludes that a clock is slower . the same holds if we invert the situation ( green lines from b to a ) . well , almost the same ( the last green line is intended to go from green 6 to red 8 , blame my trembling fingers ) .
the quickest way would be to use the right hand grip rule . from symmetry you may conclude that the magnetic field around each wire forms concentric circular loops around the wire . so now it remains to determine whether clockwise or anticlockwise . to do this , just imagine that you are gripping the wire with your right hand in such a way that your thumb points in the direction of current . then the remaining fingers point in the direction of the magnetic field . this follows directly from the convention used for doing path integrals - if the closed line integral is performed in the anti-clockwise direction , then the area vector for doing the corresponding surface integral is taken to point towards you . of course , having thus found the contributions from each individual wire , you would then have to take their vector sum . on a side note , similar conventions are adopted in mechanics for finding the direction of angular velocity corresponding to clockwise or anticlockwise rotation . thankfully , the convention is more or less uniform through all areas of physics !
the total incident energy per second , $w_{tot}$ , is the energy per unit area multiplied by the area , so : $$ w_{tot} = 2 w/m^2 \times 10^{-4} m^2 $$ the total number of photons is $w_{tot}$ divided by the energy per photon ( in joules ) , and the energy per photon is : $$ e = 10.6ev \times 1.602 \times 10^{-19} $$ where $1.602 \times 10^{-19}$ is the conversion factor from ev to joules . so the total number of photons per second is : $$ n = \frac{w_{tot}}{e} = \frac{2 w/m^2 \times 10^{-4} m^2}{10.6ev \times 1.602 \times 10^{-19}} $$ multiply by 0.53/100 because only 0.53% of photons expel a photoelectron and you get your formula .
so this will take a really simplistic look at it , ignoring things like flexibility in the tires/wheels/bike and assuming that you do not go too fast to slide out . the work done to turn is the force to turn times the distance of the turn . the force is $mv^2/r$ where $m$ is the mass of the system , $v$ is the speed of the turn , and $r$ is the radius of the turn . so the force gets higher as either the speed increases or the turn gets smaller , as expected . the distance of the turn is the arc-length of a circle ( assumption of course ) of the radius , which is $d = \theta r$ . this means the work done is $$w = \frac{mv^2}{r} \theta r$$ or , in other words , if you are not going to slide out and you can generate the forces you need to make the turn , it does not actually matter what radius you choose , the work done is the same either way . now this is where the assumptions become obviously violated . we can not turn instantly ( $r = 0$ ) because there is not enough friction to keep us upright . and the amount of friction increases as the turn gets sharper . the turning friction dominates the friction along the arc-length ( unless your tires are slipping riding in a straight line ) , so all of this comes together to imply that you save energy by taking a bigger turn faster than a narrower turn slower . but you obviously want to take the tightest turn you can at the fastest speed you can without crashing to maximize everything .
edit with respect to correction/edit in question : first of all lets assume that $m_a$ is caused by force $f$ applied at distance $x$ from a , and distance from a to b is $l$ . to calculate $m_a$ $xf = m_a$ $m_a = \sqrt [ 2 ] {x^2 + l^2} f \sin\theta$ $m_a = \sqrt [ 2 ] {x^2 + l^2} f x / \sqrt [ 2 ] {x^2 + l^2}$ $m_a = m_a $ $m_a$ is moment of the force which provides moment $m_a$ about point a , about point b . rest of your equations look correct !
your physics is correct : all colors ( wavelengths ) that make up the " blue " sky are " darkened " with altitude in ( approximately ) the same proportion because there is lass air to scatter . however , the brain does not directly perceive how much red , green , and blue there is . the brain transforms this information into something like the " hcl " color space . " h " means " hue" ; different colors on the rainbow are different hues . " c " means " chroma " , low chroma colors are washed out , zero chroma means shades of grey . " l " means luminosity , on a scale of dark vs light . if the red , green , and blue components are all reduced we notice less " l " but the same " h " and " c " ( as long as it stays bright enough for cones to work ) . see : http://tools.medialab.sciences-po.fr/iwanthue/theory.php for more information .
well you are right . the other teacher is right only if he is a considering a massive rod . furthermore , if he is considering a massive rod , he is right only if he is considering the infinitesimal time interval during collision , not after , and even furthermore only for a very specific combination of parameters - it is actually quite subtle ! here 's why . because the rod is massless in your scenario , during the collision the two masses can be simply thought of as being free . your have worked out this scenario in your post . then starting from just after the collision , what the rod serves to do is to simply constrain the motion to that of a circle . hence we need the tension force to provide the centripetal force as you have analyzed . ok , now let 's see why the other teacher might be right . if the rod is massive , then ( it plus the mass at the end ) 's center of mass is not at the end . say that it is some distance $x$ away from the end where the mass is attached ( or equivalently , $l-x$ away from the hinge ) . furthermore let 's assume that the rod has a uniform density . removing this assumption makes the problem much harder , but it is still tractable . what the collision does is it provides an infinite force $f$ , but with finite impulse ( think delta function ) . let 's assume that the hinge also provides an infinite force $f'$with finite impulse , in the opposite direction . we will see if this force $f'$ is $0$ . the impulse delivered is \begin{align} j = \int f - f ' dt . \end{align} but we know that if the mass that strikes the rod+mass carries along its merry way with $v/2$ ( although the situation is a bit unphysical as it would have to ' teleport ' to the other side ) , then by conservation of momentum the rod+mass setup must acquire a velocity \begin{align} v ' = \frac{m}{m ( x ) +m}\frac{v}{2} , \end{align} where $m ( x ) $ is the mass of the rod . ( $m$ is actually a function of $x$ , because once you specify where the center of mass is , you have specified the mass of the rod , and vice versa . ) because the impulse causes a change in momentum of the mass+rod system $j = ( m+m ) \delta v$ , we have \begin{align} j = m \frac{v}{2} . \end{align} besides a linear impulse , these two forces provide a rotational impulse too , that causes the mass+rod to spin . that is , \begin{align} ( l-x ) \int f ' dt + x \int f dt = i ( x ) \omega , \end{align} where $i ( x ) $ is the moment of inertia of the mass+rod system about its common center of mass , given by the parallel axis theorem : $i ( x ) = \frac{ml^2}{12} + m ( l/2 - x ) ^2 + mx^2$ . ok , this is kind of messy . but let 's push on . let 's figure out what $\omega$ is . multiplying $j$ by $-x$ and adding it to the last equation , we get \begin{align} and l\int f ' dt = i ( x ) \omega -xm \frac{v}{2} \end{align} now if $\omega ( l-x ) = v'$ , i.e. that the velocity of the point of the rod+mass at the hinge , which is the sum of the translational velocity plus the rotational velocity is $0$ , then $f ' = 0$ . now , all these analyses was done in the infinitesimal time interval during collision . just after the collision , once everything has settled . then regardless of whatever situation we have , we have $\omega ( l-x ) = v$ . then the force of the hinge on the rod will be tangential to the motion because of circular motion requirements . ok , long story short : if you are talking about just after the collision , then there will be no horizontal force . if you are talking about during the collision , if the rod is massless , there will be no horizontal force . if you are talking about during the collision , if the rod is massive , then in general there will be a horizontal force .
yes , it will affect angular velocity since different mass distribution have different moment of inertia $i$ in general . the effect of torque $\tau$ on the angular velocity $\omega$ of the object is given by $$\tau=\frac{d}{dt} ( i\omega ) $$ the moment of inertia of a point mass is given by $i=mr^2$ , so in your case , the radius differ by 10 time so moment of inertia differ by 100 times and so does the angular velocity . note that when you mention torque , you dont necessary need to specify which point it acts on .
the ontological phrasing you use to describe quantum teleportation should be a red flag . there is no sense it which the quantum state " appears " . it may be the case that you are confounding the physical system itself ( real , ontological ) with the quantum state assigned to it by the experimenter ( subjective , epistemic ) . in other words , think of the quantum state as the information one has about a physical system and not the physical system itself . if the fidelity between the final state and the input is not 1 , then some operation in the protocol was not perfect . of course , this will always be the case . so , you should understand " quantum teleportation " as the protocol itself and not the act of transferring exactly the quantum state of one physical system to another . for any practical application of the protocol , it need not be perfect to be useful .
as requested , but please do not mark this as the answer because it is a rehash of manishearth 's post : when physicists say for example " time is proportional to $\sqrt{h}$" what they mean is that : $$t = c \times \sqrt{h}$$ where c is just a number , called the constant of proportionality , and it does not change . the only way to find out the value of the constant $c$ is to measure $t$ for a variety of values of $\sqrt{h}$ . typically you would draw a graph of $t$ against $\sqrt{h}$ and you should get a straight line . the gradient of the line gives you the value of $c$ . if you do not get a straight line then you have made a mistake somewhere and $c$ is not constant . so the lecturer in the video starts by guessing that $t$ is proportional to $h^\alpha m^\beta g^\gamma$ , and what he means by this is : $$t = c \times h^\alpha m^\beta g^\gamma$$ where $c$ is the constant of proportionality . dimensional analysis can not tell you what the constant of proportionality is . the only way to find the value of $c$ is to do the experiment .
your term is allowed , and in fact present in qed . it gives the anomalous magnetic form factor contribution to the current . in qed , the number $2f ( 0 ) $ is the anomalous magnetic moment of the electron . see peskin/schroeder p . 186-188 .
we could say : $$ \frac{\partial \mathbf{b}}{\partial t} \neq 0 \implies \mathbf{\nabla} \times \mathbf{e} \neq 0 \implies \mathbf e \neq 0 $$ where the first implication follows from the transitivity of inequality and faraday 's law : $$ \mathbf \nabla \times \mathbf e = \frac{\partial \mathbf b }{\partial t } $$ and the second implication is the contrapositive of $$ \mathbf e = 0 \implies \mathbf \nabla \times \mathbf e = 0 $$ you are right to point out that we could have just as well said : $$ \nabla \times \mathbf e \neq 0 \implies \frac{\partial \mathbf b}{\partial t} \neq 0 \implies \mathbf b \neq 0 $$ this does not present a problem . we tend to interpret any implication derived from physical law as causal , and while you will regularly find passages in books with things like : " the changing magnetic field causes an electric field " , you will equally likely find other passages of the same book that say something like " the curl in the electric field causes a change in the magnetic field " . and i think calling these causal is justified , as we can set up an experiment where we modify a magnetic field and measure an electric field , or for the second one , you could perform an experiment where you rotate a capacitor and measure a magnetic field , for instance . as for jefimenko 's equations , as far as i know they are completely equivalent to maxwell 's equations in their predictions . in fact , you can derive them from maxwell 's equations and vice versa . † similarly describing electromagnetism by use of the electromagnetic tensor $f_{\mu\nu}$ and four current is completely equivalent to the other two in its predictions . ( in this way , we were both lucky and blind , to have developed a classical theory by 1865 that was completely relativistic , but not develop relativity until 1905 . ) †: for instance , see section 6.5 of jackson 's classical electrodynamics , third edition at that point , physics can no longer distinguish which is true , and you are free to believe whichever you like , or to switch beliefs between the three systems as you please , or as proves convenient . they are , in effect , three different , completely equivalent formulations of electromagnetism . this is standard . for instance , newtonian mechanics , lagrangian mechanics , hamiltonian mechanics , or hamilton-jacobi mechanics all give the same predictions , but all seem to tell a very different story about how the world works . each is useful for solving particular problems , and each gives you a different way to view the world , and each is , as far as science can ascertain , completely equivalent and valid ways to view the world . some people like to believe in one interpretation over another , but i think its healthy to keep in mind that they are all as valid as the other . but you are more than welcome to keep one as your favorite .
the measure is not arbitrary . in classical mechanics , the symplectic structure of phase space defines the liouville measure . in quantum mechanics , the hilbert space norm plays this role .
there is no simple equation for how a paper airplane flies like there is for a simple projectile because the airplane can interact with the air in complicated ways . the physics of a paper airplane is described by newton 's laws of motion . these laws apply to both the airplane and the air it travels through . the plane is acted on by a constant gravitational force and by contact forces with the air , especially drag and lift . the nature of the force between the air and the plane can be quite complicated , and requires an extremely detailed analysis for accurate simulation . for example , by constructing the plane slightly differently , you can make it fly faster , slower , further , curve left or right , or bob up and down . the basic physical ideas are those of fluid dynamics and the basic equation involved is the navier-stokes equation . modeling something like an airplane accurately is mostly the domain of expertise of aeronautical engineers . to make a simple model for a game , you might want to start with a simple constant gravity force , a drag force proportional to the square of the velocity , and a lift force also proportional to the square of velocity ( which comes from here ) , and then play around with the parameters until you find something pleasing to your eye .
one recent result which provided certain definite constraints came not from particle physics but from molecular physics . using cold molecules , it is possible to achieve very high precision measurements on properties like the electric dipole moment of the electron , which are on a par or better than the constraints provided by particle physics experiments , and beginning to bite into the territory of certain supersymmetric theories . this is nicely explained in this physics world article about the latest result , an experiment on thorium monoxide molecules performed by the acme collaboration and reported in order of magnitude smaller limit on the electric dipole moment of the electron . the acme collaboration . science 1248213 , 2013-12-19 . arxiv:1310.7534 . to borrow an image from physics world , different supersymmetric theories predict different ranges of values for $d_e$: the acme result constrains it to $|d_e|&lt ; 8.7\times 10^{−29}\ e\ \text{cm}$ . it is therefore my impression that it does rule out , or heavily constrain , several susy candidates .
it is only a question of definition . there is the operator of interaction of particle with an externally-produced magnetic field : $\hat{h}_{int}=-\hat{\boldsymbol{\mu}}\cdot\mathbf{h}$ , where $\mathbf{h}$ is a magnetic field and $\hat{\boldsymbol{\mu}}$ is an operator : $\hat{\boldsymbol{\mu}}=\displaystyle \frac{g e}{2 m} \hat{\mathbf{s}}$ by the «value» of the magnetic moment of particle , people usually imply the maximum of the following diagonal matrix element : $\mu=\langle\psi\vert \hat{\boldsymbol{\mu}}_{z} \vert\psi\rangle , $ which is of course $g \mu_n/2$
the xy pattern is used with directional microphones ( usually cardioid ) pointed in different directions . cardioid microphones reject signal coming from different directions . therefore , even if the mics somehow occupied the same space , they would still be hearing different things ( as long as they are pointed in different directions ) . your statement that " the two signals obtained are equal in amplitude " is therefore incorrect : when instruments are mic'ed using the xy pattern , there are differences in amplitude if the sound source ( including reverberant reflections of sound from surfaces in the room ) are coming " off axis " of the stereo pair .
yes . for photons in vacuum , the energy per photon is proportional to the photon 's classical , electromagnetic frequency , as $e = \hbar \omega = h f$ . here , we see a connection between two classical properties of light : the energy and frequency . what is surprising is that the relation holds for matter , where there is no classical equivalent of the frequency . nevertheless , in an interferometry experiment , an relative energy shift of $\delta e$ can lead to an observable frequency difference $\delta f$ , so that the phase of an interferometer operated for a time $t$ is $\phi = \delta e\ , t/\hbar$ . this was originally observed in neutrons and has more recently been seen in electrons and atoms . even the rest mass energy $mc^2$ has a equivalent frequency , which is known as the compton frequency $\omega_c = mc^2/\hbar$ . while we can not ( currently ) experimentally measure it , it can be inferred from atom interferometry experiments . the general idea of a matter-wave frequency occurs where it is possible to make and readout a superposition state , which does not occur classically .
you might say , of course , that in some sense the fermions do interact ( and it is even called exchange interaction ) . however , it is physical forces , like coulomb ones , that are understood to be absent . a relevant discussion has taken place here : degeneracy pressure , what is it ? concerning the second question about single atoms , the answer is no . firstly , fermi statistics , as any statistics , can only be applied to macroscopic objects . secondly , even if you were able to create a giant nucleus of a large charge and cover it with a macrosopic number of electrons , the electrons would be interacting with each other through coulomb forces , hence will not represent a degenerate gas and hence will neither follow the statistical distribution of degenerate gases nor possess fermi energy .
you just made some math mistakes . you made a mistake when you did $q = h\int_a kr$ . you got $q = h\pi k r^2$ , but you should have gotten $q = \frac{2}{3} h\pi k r^3$ . notice how this second expression has units of charge while the first one does not . another mistake you make is that you say $\frac{1}{r} \frac{\partial re ( r ) }{\partial r} = \frac{1}{r} ( \frac{\partial e ( r ) }{\partial r} + r\frac{\partial e ( r ) }{\partial r} ) $ . this is not proper application of the product rule . if correct these mistake you will get the right answer unless there are other mistakes i did not find .
i believe that in that text , $i$ refers to the magnitude of the current ( a scalar ) , which is assumed to be in the same direction as the length vector $\vec{l}$ ( a vector ) . there is no need for both $i$ and $\vec{l}$ to be vectors . think of current flowing through a wire—if $i$ were a vector ( $\vec{i}$ ) , then the direction of $\vec{i}$ would always be the same as the direction of the wire , because current always flows along a wire . the direction of the wire is already captured by $\vec{l}$ , so it is not necessary to make $i$ a vector quantity also .
no . the standard metric for cosmology is given by : $$ds^{2} = - dt^{2} + a ( t ) ^{2}\left ( d^{3}{\vec x}^{2}\right ) $$ where the term inside the parenthees represents the 3-metric of a homogenous three space . as you can see , there is no difficulty with evaluating the age of the universe : $$ t = \int\sqrt{-g}\ , \ , x^{a}y^{a}z^{a}\epsilon_{abcd} = \int dt$$ where the integral is evaluated from the time when $a = 0$ to now .
they accrete gas from a disk , fed by either a wind or roche lobe overflow from their companion . almost all known millisecond pulsars are in binary systems , but i think some in globular clusters may have been disrupted by three-body encounters , so appear to be isolated .
this question has been modified into a more specific form concerning two structures : haemoglobin and the ppn formalism for gravitation . so i shall make some general comments about how to consider this : a linked supplementary question might then be the best means to progress . haemoglobin is a large ( class of ) molecules , which have high complexity . approx molecular weight 68000 ( where hydrogen=1 ) . they are in many terrestrial life forms and they are a key component of blood . there are undoubtedly many outstanding scientific questions concerning the origin and dynamics of such a large molecule . one aspect in particular is the whole issue of " protein folding " which would help form them efficiently and successfully . in terms of physics there are two aspects : fundamental physics aspects and local environment aspects . the latter are the most important in practice with e . g temperature undoubtedly being important ; although pressure ( ie blood pressure ) is likely a function of local gravitation too . so there is the whole question of the biomechanics of a body in differing gravitational fields ( surface of earth , surface of moon , in vacuum , on jupiter , etc ) to consider . as the body is the " manufacturing unit " for this molecule its well being is important too - and vacuum conditions and jupiter conditions are generally considered hostile to life as we understand it . in terms of the fundamental physics and laws , the most important here are those of chemistry which come from quantum mechanics . in a hypothetical different universe with a different electric charge for example undoubtedly different molecules would form ; perhaps no molecules can form , only atoms . in terms of our universe ( whatever gravitational laws really apply : newton , einstein , etc ) the local strength of the gravitational field will be an important parameter in the body formation . in terms of biochemistry it probably only has an indirect effect from its contribution to environment ( air , sea ) pressure and other thermodynamic variables . all these issues from the persective of life history and presence on other planets and environments is important research in the field of astrobiology . the ppn formalism was invented to compare einstein 's general relativity with competing theories via delicate astronomical measurements : but gr remains the best theory in terms of these tests . the best way for a gravity theory parameter to affect quantum parameters would be if , in a theory of quantum gravity , it was found that electric charge , say , was a function of the gravitation constant ( which then might not be constant ) . so that theory might have additional solutions with unusual parameters which might or might not allow the formation of complex molecules .
hawking radiation can occur in conditions influenced by more than just gravity . . temperature can also influence this - ( see bogoliubov theory of acoustic hawking radiation in bose-einstein condensates , a . recati , n . pavloff , i . carusotto and hawking radiation in a two-component bose-einstein condensate , p . -é . larré and n . pavloff ) this means that gravity is but a single factor to consider when trying to establish the initial conditions for hawking radiation . it is not clear this answers your question , so lets assume we are only looking at gravity . the event horizon typically takes place at the boundary within which the gravitational object 's escape velocity is equal to or greater than the speed of light . there are other definitions involving the pathways of lights , but lets go with this one for now . this means that at the event-horizon nothing with mass can escape since nothing with mass can go the speed of light , and particles with 0 rest mass , such as photons , can , at best , remain in orbit , or some other such odd behaviour such as red-shifting itself to match or exceed the circumference of event-horizon itself . in any event , the effectively disappear to an outside observer . notwithstanding , suggestions such as light having fractal properties , or debates about the fractal-dimension of event-horizons and hairy black-holes , as you move away from the event-horizon - lets assume , for the sake of illustration , a simple decaying exponential gradient function describes how the black-hole 's influence is felt , with it being greatest near the event-horizon , dying off exponentially the further you travel away ( meaning as space-time bends less ) . let 's also assume the black-hole is not spinning ( which is not likely but it simplifies the illustration ) . the particle pairs responsible for hawking radiation , are thought to happen anywhere in space , but for this radiation to actually occur ( meaning be detectable ) , the black-hole 's influence must be felt on the pair . so the likelihood of this radiation occurring will be diminishing according to some function as you move further away from the event-horizon but absolutely guaranteed on the event-horizon boundary itself . this means you could , in theory , have hawking radiation occurring infinitely far away except the likelihood of the black-hole being able to wield influence on the particle pairs is so unlikely at that range that the probability of this occurring or being detectable is effectively 0 . nevertheless , even from some great distance as you approach your black-hole the probability will slowly raise enough that at some point and given sufficient time , you should be able to measure an occurrence . so , to answer your question , the strength of gravity sufficient for hawking radiation to occur , given a particular black-hole , is inversely proportional to the degree of patience you posses . the greater your patience , the weaker the gravity needs to be . the less your patience , the stronger it needs to be .
this interference is unfortunately quite typical as david pointed out in his comment . a typical household microwave oven operates at 2.45 ghz , the 802.11g wireless spectrum lies in the range of 2.412 to 2.472 ghz . this by itself is not a big problem , as the wifi algorithms use sophisticated algorithms to operate even with noise at the same frequencies . the problem is the leaked power that can be much higher than any nearby wifi signal . the shielding is never perfect and usually from the ~800w only a tiny amount will leak out through the seals , the metallic grid in the front window , etc . we analysed the amount of leakage with a calibrated microwave sender/receiver pair during my undergraduate time and found attenuation in the range of 99.75% to 99.98% for different types of metallic grids in the front doors . this means that still 160mw to 2w can leak out of the oven ( i am not sure if 2w of leakage are even allowed today as it was an old microwave oven ) . compared to the allowed output power of wifi of 100mw to 300mw depending on the region/country it is easy to see why a not well shielded oven can drown out the signal completely . you can avoid the wifi interruptions by trying out different channels , using a 5ghz setup or using a better microwave . the biological harm caused my microwave radiation is still under debate to phrase it mildly . there a lots of different effects and long-term influences are very hard to characterize . so if we take the worst case scenario of the 2w leaking microwave it is safe to assume that any thermal effects are tiny as you will not put your head against the oven and from the distance of a couple of centimeters the deposited power per volume is too small to have any effect . the same cannot be said so easily for a cellphone which you have for a long time very close to your head , but after hundreds of studies it is clear that immediate negative effects could not be found and now big long term studies try to characterize effects that only happen after years of exposure .
method the method is based on measuring variations in perceived revolution time of io around jupiter . io is the innermost of the four galilean moons of jupiter and it takes around 42.5 hours to orbit jupiter . the revolution time can be measured by calculating the time interval between the moments io enters or leaves jupiter 's shadow . depending on the relative position of earth and jupiter , you will either be able to see io entering the shadow but not leaving it or you will be able to see it leaving the shadow , but not entering . this is because jupiter will obstruct the view in one of the cases . you might expect that if you keep looking at io for a few weeks or months you will see it enter/leave jupiter 's shadow at roughly regular intervals matching io 's revolution around jupiter . however , even after introducing corrections for earth 's and jupiter 's orbit eccentricity , you still notice that for a few weeks as earth moves away from jupiter the time between observations becomes longer ( eventually by a few minutes ) . at other time of year , you notice that for a few weeks as earth moves towards jupiter the time between observations becomes shorter ( again , eventually by a few minutes ) . this few minutes difference comes from the fact that when earth is further away from jupiter it takes light more time to reach you than when earth is closer to jupiter . say you have made two consecutive observations of io entering jupiter 's shadow at t 0 and t 1 separated by n io 's revolutions about jupiter t . if the speed of light was infinite , one would expect \begin{equation} t_1 = t_0 + nt \end{equation} this is however not the case and the difference \begin{equation} \delta t = t_1 - t_0 - nt \end{equation} can be used to measure the speed of light since it is the extra time that light needs to travel the distance equal to the difference in the separation of earth and jupiter at t 1 and t 0 : \begin{equation} c = \frac{\delta d}{\delta t} = \frac{d_{ej} ( t_1 ) -d_{ej} ( t_0 ) }{\delta t} \end{equation} ( both numerator and denominator can be negative representing earth approaching or receding from jupiter ) in reality more than two observations are needed since t is not known . it can be approximated by averaging observations equally distributed around earth 's orbit accounting for eccentricity or simply solved for as another variable . practical considerations note that you will not manage to see io enter/leave jupiter 's shadow every io 's orbit ( i.e. . roughly every 42.5 hours ) since some of your observation times will fall on a day or will be made impossible by weather conditions . this is of no concern however . you should simply number all io 's revolutions around jupiter ( timed by io entering/leaving jupiter 's shadow ) and note which ones you managed to observe . for successful observations you should record precise time . it might be good to use utc to avoid problems with daylight saving time changes . after a few weeks you will notice cumulative effect of the speed of light in that the average intervals between io entering/leaving jupiter 's shadow will become longer or shorter . cumulative effect is easier to notice . at minimum you should try to make two observations relatively close to each other ( separated by just a few io revolutions ) and then at least one more observation a few weeks or months later ( a few dozens of io revolutions ) . this will let you calculate the average time interval between observations within a short and long time period by dividing the length of the time period by the number of revolutions io has made around jupiter in that period . the average computed over the long time period will exhibit cumulative effect of the speed of light by being noticeably longer or shorter than the average computed over the short time period . more observations will help you make a more accurate determination of the speed of light . you must plan all of the observations ahead since you can not make the observations when earth and jupiter are close to conjunction or opposition . calculations once you collected the observations you should determine the position of earth and jupiter at the times of the observations ( for example using jpl 's horizons system ) . you can then use the positions to determine the distance between the planets at the time the observations were made . finally , you can use the distance and the variation in io 's perceived revolution period to compute the speed of light . you will notice that roughly every 18 millions kms change in the distance of earth and jupiter makes an observation happen 1 minute earlier or later . cost the cost of the experiment is largely the cost of buying a telescope that allows you to see io . note that the experiment takes a few months and requires measuring time of the observations with the accuracy of seconds . history see this wikipedia article for historical account of the determination of the speed of light by rømer using io .
the do not disappear with zero energy . their energy ( both that originating from rest mass and any kinetic energy ) appears somehow . as photons , a spray of other ( lighter ) particles , etc . for instance , when an electron meets a positron ( that is , a anti-electron ) the most common result is a pair of gamma rays each of 511 kev ( in the center of momentum ( com ) frame of the $e$--$e^+$ pair ) . it has to be at least two because you have to conserve both energy and momentum ( and angular momentum , too , but that is a complication we will ignore for the moment ) , and that also tells us that there have opposite momenta in the com frame .
here we will not answer all the subquestions , but just mention that there are at least two theoretical reasons why susy qm is important : witten 's derivation of atiyah-singer index theorem using susy qm . there is a big overlap between susy qm systems , and qm systems that can be analytically solved .
energetic cosmic rays are rare , as @john rennie states in his answer , and the detectors to measure their effect cover kilometers . one high energy entrant creates what is called an air shower and it is measured as @dmckee describes . it is not possible to use them as an incoming beam because we do not know what they are ( except in the case of gamma rays and neutrinos , because they need special detectors ) , so we do not know the incoming beam , and do not know where the interaction happened . they are interesting events but new physics can not be garnered from the observations . the observations are useful for astrophysics though and cosmological theories .
it is mainly an electrostatic interaction through a dipole-dipole interaction . however , the dipole moment can be permanent or induced . depending on its nature the force has a different name : force between two permanent dipoles ( keesom force ) force between a permanent dipole and a corresponding induced dipole ( debye force ) force between two instantaneously induced dipoles ( london dispersion force ) you can also refer vanderwaals force an atom or molecule is usually globaly neutral : ie there is exactly the same number of positive and negative charges . however the center of charge ( barycenter of the qi ) for the positive and negative charges does not always coincide . this gives rise to an electrostatic dipole that can interact with an external electric field . to answer to your first bullet , the electrostaic dipole is not linked to electrons changing orbital . the cohesive forces in solids have two different origin : orbital coupling ( not of electromagnetic nature ) or ionic bonds in ionic crystals ( na+ , cl- ) for instance . in the latter the cohesion of the crystal is due to electrostaic interaction ( not dipolar ) . for the liquids , the electrostatic forces are responsible for the observed properties . polar solutions are made of molecules with a dipolar moment and are able to dissolve ionic crystals .
a goldstone boson is a generic type of particle formed when symmetries are spontaneously broken . if you want to suggest that dark matter is a goldstone boson then that says very little unless you suggest a specific model with a symmetry to be broken . when exact symmetries are broken you get a massless goldstone boson ( except in a few special circustances , e.g. in gauge theory the extra mode gives mass to the gauge bosons instead of forming a goldstone boson ) dark matter cannot be formed from massless particles since they would not be gravitationally bound to galaxies and we know that dark matter is . massless particles would fly past on the same trajectories as photons in the microwave background . if the broken symmetry is not perfect you get pseudo-golstone bosons which are light on the scale of the model , but not massless . the pion is an example from flavour chiral symmetry breaking , but it is not stable . any theory that predicted such a particle would predict other new particles that could just as easily be part of dark matter if they are stable . without a specific proposal for such a theory not much has been said . note that it is actually very easy to dream up particle models of dark matter , e.g. you just need a new quantum number to explain stability . the difficulty is to find a theory that is well motivated from other considerations . e . g supersymmetry solves the hierarchy problem etc . , axions solve the strong cp problem . however there is no clear reason why dark matter needs to solve other problems in this way . until we can detect a signature for dark matter interactions it is going to be very hard to settle what it is .
if $\theta$ is the angle between the arms , displaced from the equilibrium $\theta_0$ by $\delta \theta$ and the torque applied is $\tau =-\kappa \delta \theta$ , assuming equal masses of $m$ with initially motionless parts . the first step is the kinematics , whereas the acceleration of 2 and 3 is related to the acceleration of 1 and the common angle . for simplification we have that 1 is not accelerating in the horizontal direction $\ddot{x}_1=0$ ( as seen in figure below ) . $$ \begin{aligned} \ddot{x}_2 and = \ddot{x}_1 - \ell \cos \left ( \frac{\theta}{2} \right ) \frac{ \ddot{\theta}}{2} and \ddot{x}_3 and = \ddot{x}_1 + \ell \cos \left ( \frac{\theta}{2} \right ) \frac{ \ddot{\theta}}{2} \\ \ddot{y}_2 and = \ddot{y}_1 + \ell \sin \left ( \frac{\theta}{2} \right ) \frac{\ddot{\theta}}{2} and \ddot{y}_3 and = \ddot{y}_1 + \ell \sin \left ( \frac{\theta}{2} \right ) \frac{\ddot{\theta}}{2} \end{aligned} $$ now for the equations of motion of each part . we start with free body diagrams in order to sum up the forces on each part . $$\begin{aligned} -fr_2 \sin \left ( \frac{\theta}{2} \right ) + fr_3 \sin \left ( \frac{\theta}{2} \right ) + fn_2 \cos \left ( \frac{\theta}{2} \right ) + fn_3 \cos \left ( \frac{\theta}{2} \right ) and = m \ddot{x}_1 = 0 \\ -fr_2 \cos \left ( \frac{\theta}{2} \right ) - fr_3 \cos \left ( \frac{\theta}{2} \right ) + fn_2 \sin \left ( \frac{\theta}{2} \right ) + fn_3 \sin \left ( \frac{\theta}{2} \right ) and = m \ddot{y}_1 \end{aligned} $$ the eom are done in a direction along the arm $$\begin{aligned} m \ddot{x}_2 \cos \left ( \frac{\theta}{2} \right ) - m \ddot{y}_2 \sin \left ( \frac{\theta}{2} \right ) and = -fn_2 \\ m \ddot{y}_2 \cos \left ( \frac{\theta}{2} \right ) + m \ddot{x}_2 \sin \left ( \frac{\theta}{2} \right ) and = fr_2 \\ 0 and =\ell fn_2 + \tau \end{aligned} $$ with $\rightarrow fn_2 =-\frac{\tau}{\ell}$ $$\begin{aligned} m \ddot{x}_3 \cos \left ( \frac{\theta}{2} \right ) + m \ddot{y}_3 \sin \left ( \frac{\theta}{2} \right ) and = -fn_3 \\ m \ddot{y}_3 \cos \left ( \frac{\theta}{2} \right ) - m \ddot{x}_3 \sin \left ( \frac{\theta}{2} \right ) and = fr_3 \\ 0 and =\ell fn_3 - \tau \end{aligned} $$ with $\rightarrow fn_3 =\frac{\tau}{\ell}$ combined the all of the above equations substituted into the kinematics are $$ \begin{aligned} -fn_2 \cos \left ( \theta \right ) + fr_2 \sin \left ( \theta \right ) - 2 fn3 and = m \ell \frac{\ddot{\theta}}{2} \\ fr_2 \cos \left ( \theta \right ) + fn_2 \sin \left ( \theta \right ) + 2 fr_3 and = 0 \\ - fn_3 \cos \left ( \theta\right ) - fr_3 \sin \left ( \theta \right ) + 2 fn_2 and = - m \ell \frac{\ddot{\theta}}{2} \\ fr_3 \cos \left ( \theta \right ) - fn_3 \sin \left ( \theta \right ) + 2 fr_2 and = 0 \end{aligned} $$ the above is solved with $$\boxed{\frac{3 \tau ( \cos\theta-2 ) }{\ell ( \sin^2\theta+3 ) } = m \ell \frac{\ddot{\theta}}{2}}$$ and $$\begin{aligned} fr_2 and = \frac{\tau \sin\theta ( 2-\cos\theta ) }{\ell ( \sin^2\theta+3 ) } \\ fn_2 and = -\frac{\tau}{\ell} \\ fr_3 and = \frac{\tau \sin\theta ( 2-\cos\theta ) }{\ell ( \sin^2\theta+3 ) } \\ fn_3 and = \frac{\tau}{\ell} \end{aligned}$$
for simple harmonic motion , the restoring force is proportional to the displacement . so if you can compute the force $f$ on the drum when you displace it by a small distance $d$ , then your " spring constant " $k = \frac{f}{x}$ . i am sure you can compute that force ( from the weight of the displaced fluid - archimedes ' principle ) . but warning - before you can compute the frequency of oscillation , you need to account for the additional inertia of the system ( which is actually very hard to compute for a drum - i know that for a fully submerged sphere it is equal to the mass of half the displaced fluid ) . if you have trouble with the first part of this , i expect you are not expected to understand this second paragraph at all… but if you wanted to compute the motion of the barrel , it would really matter . . .
if i understand correctly the intention of your question , the answer you are looking for is not the planck time . if the question is at what time scales a systems behaves as quantum rather than classical , the answer would be equivalent to at what spatial scales a measurement is quantum instead of classical . the answer depends on the system itself . there are macroscopic systems ( for instance , bose-einstein condensates ) that behave in quantum manner , but most classical examples involve small particles or molecules . thus , there is no single answer , the range can vary orders of magnitude if you can prepare carefully the system . the same happens with the time scale . a system behaves in a quantum manner if the measurement is smaller than the decoherence time . as with the case of spatial scale , this length of time varies with the system .
i think you are missing that the source is actually referring to he observable universe explicitly - just somewhat indirect : it is not obvious as it is talking about " visible/observable universe " , but about " total mass of the visible matter " and " mass density of visible matter " . as far as i can see , any references to mass etc in the universe are covered by this . so it is basically talking about the visible matter in the whole universe ; but that is just the same as the whole matter in the visible universe , and all we care about here is the matter .
the spin referred in condensed matter is the spin of the electrons least bound to the atoms ( usually valance electrons ) . the atoms reside on the lattice sites . a spin half problem means the atoms have only one valance electron . but there are other possibilities like spin 1 , 3/2 and all . as qeb has already mentioned it can also be used for nuclear spins also . i just want mention that any two level quantum system can be represented as a effective spin 1/2 problem .
the definition for point particles is : $t = \frac{p^2}{2m}$ , where $t$ denotes my kinetic energy . if you are dealing with classical physics , the momentum for a point-particle equals $\vec{p} = m\vec{v}$ , which would give you $t = \frac{mv^2}{2}$ . in the theory of special relativity we notice that objects become heavier when they move , the momentum becomes : $\vec{p} = {m\vec{v}\gamma}$ , where $\gamma = \frac{1}{\sqrt{1- ( v/c ) ^2}}$ . hence the kinetic energy becomes : $t={mc^2\gamma}$ , this is derived below . if you try to make a non-relativistic electromagnetic lagrangian , it is not possible to incorporate the magnetic field in terms of a potential energy . so the solution is to define the momentum equal to $\vec{p} = m\vec{v}-\frac{q\vec{a}}{c}$ , where a is the vector potential so that $\vec{b} = \nabla\times\vec{a}$ . this would give an kinetic energy ( or kinetic term in your hamiltonian ) equal to : $t = \frac{ ( m\vec{v}-\frac{q\vec{a}}{c} ) ^2}{2m}$ . one often calls $m\vec{v}$ the kinematic momentum and $\vec{p}$ the canonical momentum ( since this is the momentum that follows from a lagrangian ) . it are the canonical momenta that are called " thé momenta " . and last but not least , in quantum mechanics you work on a different kind of mathematical structure ( in hilbert-space in stead of the classical phase-space ) . by equivalence with wave-theory the momentum is derived as $\hat{p} = \frac{\hbar}{i}\frac{\partial}{\partial x}$ , where we have an operator $\hat{p}$ which works on that hilbertspace . note : from dj_mummy note that the given relations for kinetic energy all assume the particle was originally at rest at a given time $t=t_0$ . the kinetic energy is derived by using the work-energy theorem ( difference in kinetic energy = work done on the point particle ) . using the laws of classical physics ( newton 's second law ) we get : $t = \int\limits_{x_0}^{x_1}\vec{f}\cdot d\vec{x} = \int\limits_{t_0}^{t_1}\vec{f}\cdot \vec{v}\ , dt = \int\limits_{t_0}^{t_1}\left ( m\frac{d\vec{v}}{dt}\right ) \cdot \vec{v}\ , dt= m\int\limits_{t_0}^{t_1} \vec{v}\cdot d\vec{v} = \frac{m}{2}v^2$ . where $\vec{v}$ is the velocity at time $t_1$ . this gives us the kinetic energy in the case of classical physics ! for the relativistic particle we define a 4-momentum $p^\alpha = m\frac{du^\alpha}{d\tau}$ . where $u^\alpha$ is the four-velocity and $\tau$ is the proper time . the derivation is actually done here and we get the formula above !
entropy is an extensive property . if you double the size of your system but keep the conditions within it the same then you will double the entropy . you can not do this without changing other properties of the system because lots of things , e.g. free energy , are also extensive . if you keep the size of your system constant but pack in twice as much gas then the entropy will change for two reasons , firstly because you have more gas , but secondly because you have changed the pressure and/or temperature . you need to specify the state of your system after addition of the gas to work out the change in the entropy .
temperature is proportional to the average kinetic energy , not velocity , of the particles . kinetic energy is unbounded ; it goes to infinity as velocity approaches light-speed , proportional to $ ( 1 - v^2/c^2 ) ^{-1/2}$ .
to establish a frame of reference , you need a reference object and reference direction . which one is inertial is actually a tough question . newton 's first law defines a inertial frame , in which all free particles stay stationary or move at constant velocity . but " free " , or " subject to no force " is ill-defined and impossible : gravitation is universal and you can never know whether the gravitational force of the whole universe combined balance out or not . in practice , we regard some frames as inertial with enough precision . the earth is a rough inertial frame ( more accurately , with the direction pointing to distant stars ) , the sun is better and the milky way is even better . in modern view ( that is , in general relativity 's view ) , all frames are equivalent , and any inertial frame is infinitesimally small . this view solves many logic difficulties with the classical idea of privileged sets of frames of reference .
i did something similar to that in a labs course once , where the flux was temperature-dependent and the temperature was time-dependent . however , ‘my’ rate of change was very slow ( a few hours for a temperature change from 2 k to 300 k with an associated flux change of maybe 500% ) , so i don’t know how well this applies to your experiment . once i get home i can have a look at the data we collected back then to tell the exact times and rates of flux change .
the vacuum dirac equation automatically implies the klein-gordon equation . it means that every solution to the vacuum dirac equation is automatically a solution to the klein-gordon equation . the converse of course does not hold . the most basic reason is that the klein-gordon equation should really act on scalars , a single bosonic field , while the minimum number of components for the $d=4$ dirac equation is four ( and they should be fermionic fields ) . so a general ( or generic ) valid solution to the klein-gordon equation is a valid solution to the klein-gordon equation ( this much is a tautology , but you were asking about it ) , but it is not a solution to the dirac equation . even if you combine 4 solutions to the klein-gordon equation , declare that they are 4 components of a dirac spinor , and ask whether they solve the dirac equation , the answer is no . it is because the dirac equation is really " stronger " than the klein-gordon equations for its components . effectively , the dirac equation is first-order while the klein-gordon equation is second-order . the dirac equation implies certain correlations between the spin ( up/down ) of the particle and the sign of the energy ( positive/negative ) . the quadruplet of klein-gordon equations allows all combinations of spin up/down and the sign of the energy . however , the most general quadruplet of solutions to the klein-gordon equation may be written as a solution of the dirac equation with a positive mass and a solution to the dirac equation with a negative ( opposite ) mass . the dirac equation describes spin-1/2 ( and therefore " fermionic" ) particles such as electrons , other leptons , and quarks , while the klein-gordon equation describes spin-0 " scalar " ( and bosonic ) particles such as the higgs boson . however , before they do the proper job , the " wave functions " have to be promoted to full fields and these fields have to be quantized .
part a a . this " simple would-be relativistic quantum theory " indeed violates locality , but not for the reason you write . you wrote that the energy i.e. the hamiltonian is defined as $$ h =\sqrt{p^2+m^2} $$ so it cannot be " set as we want to " , independently of $k$ . in fact , the problem is that $h$ is too constrained , not that it is too free . by your rules , it ( and even its sign that is demanded to be positive ) is a function of $\vec k$ . instead , the right reason why this dynamics is non-local is that the displayed equation above – which is in the momentum representation – may be translated to the position representation . the momentum is always $$ \vec p = -i\hbar \nabla $$ which is uniquely determined by the canonical commutators and/or the fact that the momentum generates translations in space ( the space-translational symmetry is equivalent to the momentum conservation by noether 's theorem ) . so the hamiltonian in the position representation is $$ h = \sqrt{m^2 - \hbar^2 \nabla^2} $$ i shifted the squared mass to the first place in the square root because it makes it more obvious that we may try to taylor-expand the formula as $$ h = \dots = m\sqrt{1 - \hbar^2/m^2 \cdot \nabla^2} = m ( 1-\dots ) $$ the point here is that the taylor expansion contains arbitrary powers of the $\nabla$ operators , including the 100th power . if something depends on derivatives of arbitrarily high orders , it is nonlocal . ( for a well-known example , consider taylor expansions of $f ( x ) $ . the value $f ( 1 ) $ may be calculated from $f ( 0 ) $ and all other derivatives at $x=0$ . ) there are also other ways to show that $\psi ( x , t+dt ) $ would depend on $\psi ( y , t ) $ for $y$ being arbitrarily far from $x$ . part b b . the underlying " unnatural aspect " of the construction above is that it takes one of the values of the square root and neglects the other one . more precisely , it " violently " tries to put the coefficients of $\exp ( -iet ) $ for all negative values of $e$ to zero . that is brutally constraining and it is the ultimate cause of the nonlocality because ordinary enough functions of space and time have both positive-frequency and negative-frequency components . in particular , for example , real functions $\psi ( x , t ) $ have " equally strong " positive- and negative-energy contributions , $\tilde\psi ( -k , e ) =\tilde\psi^* ( k , e ) $ . so all the right solutions how to write down quantum mechanical equations for particles compatible with relativity must work with objects such as wave functions that contain both positive- and negative-energy components . this doubles the number of degrees of freedom and such a doubling is only possible if we double the number of initial conditions ( one has to add $\dot\psi ( t ) $ next to $\psi ( t ) $ as initial conditions ) . your equation was " unnaturally " first-order in time , but it was infinite-order in spatial derivatives ( this asymmetry between space and time is another way to see it was not really compatible with relativity despite superficial attempts to pretend it was ) . instead , the right equation is 2nd order in both space and time , it is the klein-gordon equation $$ ( \partial^2/\partial t^2 -\nabla^2 +m^2 ) \phi = 0 . $$ it is almost the same thing but it is important that we do not try to take the square root manually . ( the dirac equation and even maxwell equations in the vacuum imply that the components obey the klein-gordon equations above , too : they just constrain the polarizations in various ways . ) such klein-gordon equation and all of its generalizations therefore has to have both positive- and negative-energy components . however , in the real world , the energy must be bounded from below – there can not be states with energy lower than the energy of the vacuum – so it must be impossible to create particles with these negative energies . one has to treat them differently . the only possible " different treatment " means that the negative-frequency solutions are actually linked to creation operators for the particle while the positive-frequency ones are linked to some annihilation operators . it means that the klein-gordon field $\phi ( x , t ) $ is no longer a combination of " just creation operators " , describing " one added particle " or its wave function . it is a combination of operators adding $n=+1$ and those with $n=-1$ . it is able to reduce the number of particles at the same moment . consequently , everything that depends on such objects is inevitably able to raise and lower the number of particles and there is no way to study processes that have a fixed number of particles in isolation . this has many precise manifestations in quantum field theory . for example , if one studies the scattering of two particles , it is always possible that we create a particle-antiparticle pair out of the excess energy . the probability of such processes changing the number of particles is nonzero and in fact , it may be related to the probability of other processes that preserve the number of particles . part c c . if the answer got lost , allowing the negative energy modes meant that we wrote the second-order differential equation in time with $\partial^2/\partial t^2$ in it ( we had to add the first derivatives $\partial/\partial t ( \phi ) $ to the list of initial conditions whose number was therefore doubled , by the way ) and this allowed us to avoid the awkward square root above the $k^2+m^2$ , and this square root was what produced the nonlocality ( via spatial derivatives of arbitrarily high orders ) . instead , the equation got symmetrically dependent on 2nd derivatives with respect to both space and time , which is really nicely relativistic , and because only finite-order spatial derivatives are used , the value of $\phi ( x , t+dt ) $ only depends on $\phi ( y , t ) $ for $y$ belonging to the infinitesimal neighborhood of $x$: the evolution is local .
muonic atoms should be stable in electron-degenerate matter ( white dwarf material ) as long as the fermi energy is more than $m_\mu - m_e$ . this is more or less exactly a analogy with neutron stability in the nucleus where the the protons are effectively in a degenerate state . any answer has to forbid electrons ( which is not going to be possible as they share almost all quantum numbers with the muon ) or have electron be degenerate with the stated fermi energy .
physicists collide particles to study their behavior under extreme conditions . new unknown particles can be created in high energy collisions or new unknown processes may be observed . our equations describing the particles are predicting certain behavior and physicists are testing if the particles really behave like that . and they are hoping to find a discrepancy to locate a problem in our theories and improve them . the short lived particles are short lived not only in our laboratories , but also in space . they are created also in collisions , but not in accelerators . certain processes in the space are powerful enough to accelerate and collide particles at much higher energies than our most powerful accelerator . if a short lived particle is created in such process , it decays quickly , like in the earth laboratories . long lived high energy particles ( protons , neutrions , electrons ) are flying through the space everywhere and they are even hitting the earth . every day there are lots of particles arriving to the earth from the space that have much higher energy than protons in lhc .
dear humble , good questions . i can not answer all your questions , but : first , the name of the author is feigel , not fiegel , and the paper is also a preprint here : http://arxiv.org/abs/physics/0304100 the controversies about the stress-energy tensor were inevitable . only the total energy and momentum are conserved as a consequence of noether 's theorem , and how they are distributed in space may often be a matter of conventions . in particular , the poynting vector may be defined in several different ways - and various automatically conserved pieces may be added , too . the integrals will not change . but for example , if there are crossing electric and magnetic fields in the vacuum , the usual poynting vector $e\times b$ shows that the energy is flowing somewhere . this flow ends up " circular " if you look globally . feigel 's paper seems to be just a generalization of the casimir effect , with a slightly more complicated arrangement of things . instead of the casimir energy , he wants to play with the energy of the vacuum , and instead of the metallic boundaries , he plays with dielectric liquids . none of these things changes that the total energy and the total momentum are exactly conserved , because of the symmetries . whether the momentum is being extracted from the vacuum is a matter of interpretation . you may also say that it is extracted from a low-frequency electromagnetic wave that was emitted by another object . feigel suggests that there is a relevance of his setup for the abraham-minkowski arguments about the density of electromagnetic momentum in dielectric materials . but one must be careful . the energy carried by the casimir effect is distributed nonlocally - it comes from modes that look like standing waves in between the metallic plates . so the casimir energy is not " naturally " written as an integral of a density , i think . please correct me if i am wrong . i suspect this may be the case of any one-loop constructions of this sort , and feigel 's construction seems to be just a more complicated example . so i suspect the effect of the feigel 's phenomena are not just about changing some local densities either in the abraham or in the minkowski way . finally , i would say that in principle , quantum electrodynamics or the standard model have standardized prescriptions for the stress-energy tensor that should be calculable in any context , including dielectric liquids in crossed electric and magnetic fields . all the best , lm
i believe that no such spacetime exists , if the matter is assumed to satisfy an inequality known as the dominant energy condition . the dominant energy condition says that , if $\xi$ is a future-directed timelike vector , then $-{t^a}_b\xi^b$ is a future-directed timelike or null vector ( sign conventions , etc . , following wald 's book general relativity ) . heuristically , this condition means that an observer at any location will always measure an energy-momentum 4-current in his vicinity that is flowing at less than or equal to the speed of light . with this condition , one can show that , if space is empty at one time ( i.e. . , if there is a spacelike cauchy hypersurface along which $t=0$ ) , then it vanishes at all times ( wald , p . 219 ) . if in addition spacetime is minkowski along some cauchy surface , then the initial value theorems say that it is minkowski at all times ( wald , chapter 10 ) . heuristically , the above argument says that , if there is no matter at one time , but there is at a later time , then matter must have popped out of nowhere . the dominant energy condition does not allow that . i do not know whether the result holds if you assume something weaker than the dominant energy condition ( e . g . , the aptly-named weak energy condition ) . certainly you need some sort of restriction on the allowed properties of the matter , as lubos motl says : if any $t_{ab}$ is allowed , then any smooth metric is allowed .
this must be impossible , even for lady castafiore with her earthquake voice . for a glass to break by sheer sound you need to produce a tone equal to the glass 's natural frequency - the frequency at which a body vibrates with the least amount of energy . in other words : there you get the most vibration with a minimum of effort . this is also called resonance . however , it is much harder to create resonance in mixed materials because each component has a different fundamental frequency . so two layers of different types of glass will effectively prevent resonance . of course , apart from that the mere thickness of the material will be a problem for our nightingale .
calculating the effect of acceleration in special relativity is straightforward , but i suspect the algebra is a bit much at high school level . see john baez 's article on the relativistic rocket for a summary , or see chapter 6 of gravitation by misner , thorne and wheeler for a more detailed analysis . when you are first introduced to sr you tend to be told about time dilation and length contraction and given formulae to calculate them . however this is at best an oversimplification and at worst actively misleading . when you are looking at some object moving relative to you you do indeed measure the object 's length to be contracted , but what actually happens is that the two end points in the object 's rest frame transform into points at slightly different times in your rest frame . you measure the object to be contracted because you are measuring the end points at slightly different times . there is no sense in which the object is squeezed by it is high velocity . any object has a proper length , which is equal to its length in its rest frame . the proper length is an invariant and all observers will measure the same proper length regardless of their relative velocity . if you consider proper length then the object is not contracted . anyhow , the answer to your question is that when the object comes to a stop relative to you its length has not changed . this is because it never did change - the change you measured was due to the coordinates you were using not matching the coordinates the object was using . when the object comes to rest in your frame you and the object are using the same coordinates ( at worst differing in the position of the origin ) so both of you measure the length to be the proper length .
i do not think that local weyl ( conformal ) invariance implies global invariance . the fields are only defined over local open sets $u_i$ , so if the action is invariant under a constant conformal transformation ( which does not depend on $x\in u_i$ ) $\phi\to \omega_i^2 \phi$ this does not imply anything about the global properties of $\omega:m\to \mathbb{r}$ , only that $\omega|u_i$ is constant . there is something called a " conformal connection " which i do not know very much about , but i think the short answer is that if you restrict the fields to satisfy some specific symmetry group $g$ ( i.e. . they are part of a $g$-bundle and $g$ is some group of conformal transformations ) then the fields will always be conformally invariant , locally and globally .
you are definitely taking this picture too seriously . hybridization of atomic orbitals is just an approximation based on an independent particle model created by pauling to rationalize some structural trends in chemistry with quantum mechanics . we can only assign orbitals unambiguously for one-electron systems , though of course independent particle models were widely employed and still are to explain reactivity trends , etc . anyway , if you prepare an electron in an sp3 state , which is an equally weighted linear combination of the px , py pz and s orbitals , the probability to find and electron at a certain angular momentum might be calculated by taking the square of the projection of this angular momentum on the wave function , as qm tells us to do . do not worry too much if you find weak spots in general theories developed to explain molecules , reactivity , etc , since they are highly approximated in most cases ( unless you do an expensive computation for a system , but then it is not general anymore ) , and the guys that developed those such as pauling already knew that . as you know these highly approximated theories ( e . g . , molecular orbital and valence bond theory ) were and still are very successful , even if they are not very rigorous . it gets really hard to be rigorous in chemistry beyond a certain point . . . @tedbunn of course the basic formalism of qm would predict properties of this hypothetical electronic state , i did not say anything that would imply the opposite . still , to think about bonding in terms of sp3 hybrid orbitals such as one does in general chemistry is really a very crude picture , unless you are dealing with molecules that have very special properties such as td symmetry ( methane , for example ) , and still in those cases if you perform a calculation by using vb or mo ( hartree-fock ) theory ( and it does not even need to be with a computer , since symmetry is going to make life very easy here ) with only the sp3 orbitals of carbon and s of hydrogen in your set of basis functions , you will see a very big quantitative error in predicted ionization energies when you compare to photoelectron spectroscopy measurements of ionization energies . another good test would be to perform high level quantum chemistry calculation of methane using a software and employing two different basis sets , one containing only s and p functions centered on carbon and s functions centered on hydrogen atoms , and another in which the basis set has functions of several angular momenta centered on each atom . if you compare your results with experiments you will see that the first method will have a much lower accuracy compared to the second one . s and p functions might still be the ones that contribute mostly to the bonding molecular orbitals in the different orbital configurations ( set of occupied molecular orbitals ) that contribute to a good description of methane , but we can only say that for sure for very special cases . when you go to molecules with very distinct geometries , it is vital to have d functions centered on carbon , for example , in order to determine even qualitatively right chemistry trends . quantum chemistry semi-empirical methods widely used in the past often had this problem of only employing valence basis functions centered on an atom in molecule calculations , not giving enough flexibility for the description of molecular geometries , and therefore providing bad results whenever the geometry deviated too much from what the qualitative models ( such as the hybridization model ) predicted . in fact , i would really be surprised ( but not too much ) if any modern paper that discusses chemical bonding , or that qualitatively discusses bonding orbitals to explain computed trends , do that by employing hybridization theory arguments . those that i have seen almost always do that by looking at molecular orbitals or some refined valence bond treatments . therefore , although hybridization is a deep and important topic that every chemist should dominate , in the way it was developed by pauling it has severe limitations that people are , or should be , aware of .
there are two considerations : a ) if you follow a bit the current theories , the manifestation of the higgs decays can vary , depending on the parameters of the models , and there are many models . b ) hadron colliders , in contrast to e+e- ones are " dirty " , there are enormous backgrounds that have to be understood , and the few clear decay channels proposed by theories have small crossections and have to be fished out of these backgrounds . so more data than the ones existing are necessary to be able to answer definitively the higgs question . lhc is now building up the statistics necessary . so what i foresee will happen , if we are lucky , is first a new resonance in some of the channels checked will be declared with at least 4 sigma . then everybody will fall on it , experimentalists to find other decay channels and theorists to fit specific models to decide whether it is the higgs or not .
the speed of sound in air determines how fast wave phenomena propagate in the air through collisions between molecules , it does not determine how fast other objects immersed in the air travel . if an object in a fluid like air begins at rest , then the speed of that object after some time is determined by the net external force on that object . say , for example , that an airplane 's engines exert a force $f$ on the plane , then the net force on the plane will be \begin{align} f - f_\mathrm{air} \end{align} where $f_\mathrm{air}$ is the force of the air on the plane , namely air resistance . as long as this quantity is positive , namely as long is the force of the plane 's engines exceeds the resistive force of air , the plane can go faster and faster provided it is made of a material that can withstand the air resistance and heat that will result . note , this is a super-simplified model , but it suffices to get the main point across ; the speed of sound in the air does not limit the speed of other objects traveling within it , only the speed of sound waves that propagate within it .
the device short-circuits internally ( that means entirely over either both poles of the energy supply having contact with water , or the pole under electric tension together with the earth conductor - that way , the current should not leave the devices chassis ) . think what the short will do : it will melt the hair drier parts and the live side of the incoming circuit will connect with the water and the water through the taps and supports of the tub to the ground . you will be holding something sparking and in contact with the water , all wet . so unless the fuse goes off there is great danger the sparks will include you too because they will be erratic . i.e. your second scenario the device short-circuits over the pole under electric tension and the bathtub itself ( earth eventually , but a different route ) . comes fast on the tracks of the first . 5 amperes is a lot of current . not to forget the surprise element for the person being stupid enough to use a hairdrier in the bath . people are over 50% or so water after all , in good paths like blood vessels . btw i have seen sparks from a 220 volt meter for the house ( provided by the electricity company ) in which rain water had run through , and the whole meter gauge was sparking and melting everything resembling in sparks those melting clocks of salvatore dali , flowing down the wall .
faraday 's law fails here . let 's go back to basics . we use the lorentz force . and what is happening is as the rod rotates , the charges in it rotate too . however , the rod is neutral so there is no net current flowing . now field of the bar magnet is towards left in the wire , the lorentz force applies on the protons and electrons inside the wire , causes the electrons only to move in the wire circularly as the force on the electron is towards centre of the winding ( radially inwards ) and the force on the protons is radially outwards which gets balanced by constraint forces of the wire . thus only , electrons flow . this causes , a net current to flow and thus we see the effect as an emf . after a certain instant , there is an accumulation of negative charges at one end after which no more accumulation will take place . now regarding the direction of the current flow , it could have flown both ways by this logic . to find direction , now use the numerical values which are give as only one direction will give the matching value of the current in the question as the $\vec{v}$ for the electrons , you need to take it from the frame of the magnet .
there is such a theorem about product spaces , it is called the künneth formula . for de rham cohomology , $$h^n ( y \times x ) \cong \bigoplus_{i+j=n} h^j ( x ) \otimes h^i ( y ) . $$ since the cohomology of $s^n$ and $\mathbb r^n$ are both well known , the calculation is now simple . you can find this theorem in the book by bott and tu , differential forms in algebraic topology . you could of course also use a mayer-vietoris sequence with $u$ as a spherical cap slightly south of the equator , and $v$ as a spherical cap slightly north of the equator , both extended for all times . then both $u$ and $v$ have the topology of $\mathbb r^4$ ( because a hemisphere has the topology of $\mathbb r^2$ ) and $u \cap v$ has the topology of $s^1 \times \mathbb r^3$ ( because a band around sphere has the topology of $s^1 \times \mathbb r$ ) . then you need to find the cohomology of $s^1 \times \mathbb r^3$ . you can do this also with the mayer-vietoris sequence . bott and tu use the m-v sequence on $s^1$ as an example , this calculation will be essentially the same . there is a good reason that the calculation will be essentially the same , namely that $s^1$ is a deformation retract of $s^1 \times \mathbb r^3$ . cohomology is invariant under deformation retracts , so we really just need to know the cohomology of $s^1$ . in fact for the original problem of $s^2 \times \mathbb r^2$ has $s^2$ has a deformation retract , so we could have used this from the beginning . however calculating the cohomology of $s^n$ is rather simple with the m-v sequence . you just need to know it for $s^1$ and then you can proceed by induction .
yes , spring elongation is $\delta= \frac{f}{k}$ regardless of where $f$ comes from .
this is almost a duplicate to can you magnetize iron with a hammer . have a look at it . the only difference is that the rail lines are fixed in their north south direction for years . the iron in the lines themselves become magnetized and so the argument with the hammer should also hold , i.e. the small magnetic domains , momentarily freed by the impact , reorient to the magnetic field direction of north south of the earth 's magnetic field . if the coin partially melts under the weight of the train , even better . it should of course be made from a ferromagnetic metal .
both approaches are equally correct in this case . $f = mv^2/r $ is just a consequence of the law for rotational motion , which says $ \tau = i\alpha$ ( torque = moment of inertia * angular acceleration ) . the former formula may be used in case the objects in consideration are point masses . but the latter , more general version of the formula is applicable for any rotating body .
that is exactly the case . if you look at the trajectory of any given spacecraft , you will see that it has a few burns of the rocket engines punctuating very long periods just coasting along in orbit around some other body . for example , the flight path of apollo 8 has something like eight different rocket burns : launch , translunar and transearth injection ( to get out of orbit and go towards the other body ) , three course correction burns , lunar orbit insertion to catch up with the moon , and one orbit correction burn on the moon . image source : wikipedia the rocket engines spend most of their time turned off , and carry just enough fuel for all of this plus a little extra for safety . this still means that the initial rocket needs to be huge , because the translunar injection requires quite a bit of fuel and that fuel needs a huge other load of fuel to get into orbit .
neglecting air resistance ( whether this is a good idea or not is besides the point ) , suppose you drop the egg from height 10.0m , and it fell to a height of 0.10m , and then rebounded to a height of 9.0m . what can you deduce from this fact ? edit : since you want something a bit more along the modelling side , if you know how the tension changes with temperature , you can deduce the change in entropy with change in length at fixed temperature using maxwell relations . you can assume the rubber band obeys $du = \delta q + t dl$ where $t$ is the tension and $l$ is it is length . you can then use more thermodynamic cleverness to determine the change in temperature with length for adiabatic stretching , which allows you to figure out the irreversible component ( lost as heat ) . google for thermodynamics+rubber+elasticity should get you started . more links : http://physics.oregonstate.edu/~roundyd/courses/ph423/lab-2.pdf ftp://ftp . ccmr . cornell . edu/tmp/mse-4020/4020-notes-7-text-book-rubber-elasticity . pdf
it is true that by noether 's theorem energy is conserved when the action is depednent on position only and not time . however , i think they want you to write down the equation of motion ( use $f=ma$ ) and then use that to show that the total energy is a constant . write down the expression for total energy and take its derivative w.r.t. time .
with a potential $v ( x ) = - \frac{\alpha}{|x|}$ , with the notation $a = \large \frac{\hbar^2}{m \alpha}$ , solutions are : $$u^+_n ( x , t ) \sim x e^{ - \large \frac{x}{na}} ~l_{n -1}^1 ( \frac{2x }{na} ) e^{ -\frac{1}{\hbar} \large e_nt}~~for~~ x&gt ; 0$$ $$u^+_n ( x , t ) = 0~for~~ x\le0$$ and : $$u^-_n ( x , t ) \sim x e^{ + \large \frac{x}{na}} ~l_{n -1}^1 ( \frac{2x }{na} ) e^{ -\frac{1}{\hbar} \large e_nt}~~for~~ x&lt ; 0$$ $$u^-_n ( x , t ) = 0~for~~ x\ge0$$ whose energy is : $$e_n = - \frac{1}{n^2} ( \frac{m \alpha^2}{2 \hbar^2} ) $$ $l_n^\gamma$ is the generalized laguerre polynomial [ edit ] there are 2 different set of basis functions , see this reference page $192$ formulae $20a$ and $20b$
mass manifests itself as 1 ) resistance to changes in velocity ( inertial mass ) , and 2 ) the source of gravitational changes in velocity ( gravitational mass ) . galileo galilei was the first person to formulate coherent thoughts on inertial and gravitational mass and their equivalence . his insights were based on experiments with rolling balls on inclined planes . this all happened early 17th century . a quantitative theory for mass and motion had to await the arrival of isaac newton . his principia got published at he end of the 17th century .
both definitions are fine as long as you are careful with signs . here 's a derivation of your gravitational potential energy using the idea of the negative work done " by the field . " ( note the initial negative sign . ) $$u ( r ) =-w_\text{by field}=-\int\vec{f}_\text{field}\cdot d\vec{s}=-\int_{r=\infty}^{r=x}\underbrace{\frac{-gmm\ , \hat{r}}{r^2}}_\text{toward origin}\cdot d\vec{s}=\int_{r=\infty}^{r=x}\frac{gmm}{r^2}dr=-\frac{gmm}{x}$$ the negative inside the integral indicates that the force by the field is toward the origin , in the $-\hat{r}$ direction . now , divide by $m$ and you have your expression . here 's a derivation using the work done by the external agent . if the object does not accelerate , the net force on the object is zero . thus , the force exerted by the external agent is equal in magnitude and opposite in direction to that by the field . we indicate this by explicitly writing that this force is away from the origin : $$u ( r ) =+w_\text{ext agent}=+\int\vec{f}_\text{ext}\cdot d\vec{s}=+\int_{r=\infty}^{r=x}\underbrace{\frac{+gmm\ , \hat{r}}{r^2}}_\text{away from orig . }\cdot d\vec{s}$$ we can stop there since we already have the same expression as our first derivation .
so it turns out that the answer to my question can be found explained in great detail in the following answer to another question : stackexchange-url ( taken directly from the linked answer above : ) the main message is that in the euler equation we are considering langrangian ( material ) derivatives of tensor fields $s ( t , x ) $ in the eulerian picture of the form : $$\frac{\mathrm{d}s}{\mathrm{d}t} := \frac{\partial s}{\partial t} + v ( t , x ) \cdot \nabla_x s ( t , x ) \text{ , }$$ where it turns out that $$\left . \frac{\mathrm{d}s}{\mathrm{d}t} ( t , x ) \right|_{x=x ( t , y ) }= \frac{\partial}{\partial t} s_l ( t , y ) \text{ , }$$ which simply is the same time derivative but now in the lagrangian picture . this is just what i was looking for . ( have a look at the very good derivation in the link above . )
the " other requirements " are the real requirements , the string business is just a gloss , which omits the most important s-matrix assumptions . . there is another question regarding this here : are there strings that aren&#39 ; t chew-ish ? ( and the linked discussion explains some of the issues ) . string theory is not just a theory of strings . in its simplest formulation , it is a theory of strings which can only interact by exchanging other strings , not by exchanging point particles ( or anything else ) . this excludes things like atomic polymers , or strings made out of points , or strings that interact by self-intersection , except to the extent that you can view the special string-theory strings as made out of string bits , like in matrix theory . the historical marginalization of s-matrix theory and its practitioners is the main reason that the string assumption is played up , and the bootstrap assumptions are played down . bootstrap was politically unpopular , and any theory that said " bootstrap " would be ignored in the 1980s and 1990s . a good list of requirements on string theory is this : there are strings , so the spectrum of the theory is the oscillation spectrum of a string worldsheet action . the exchange of strings is the source of all the ( perturbative ) forces in string theory . this means that once you know the string spectrum , you know the interactions are by summing over all intermediate states of the string alone , with nothing else . the scattering is regge-soft scattering in regge limits . this requirement is technical , and hard to state for a general audience , so it is left out . what it says is that the sum over all intermediate string states gives cancelling amplitude from particles of different spins , and that these cancellations lead to an amplitude that falls off faster than a power at very high energies . although each spin-n state gives an amplitude that blows up ever harder at high energies . this is sometimes called the bootstrap assumption , the assumption that everything in the theory is a bound state which is part of a family of related bound states which together give softer scattering than each one individually . the exchange of strings in the t-channel is the same as exchange in the s-channel , and does not require counting the particles separately . this gives the world-sheet picture , from the symmetry properties of tree diagrams in such a theory . that the interactions are by worldsheet just is not derivable from the spectrum alone , without the assumption that the spectrum bootstraps , and does not resolve to something else at short distances . these requirements partially overlap , nobody has made orthogonal axioms . together with the following rule : the string must have a fermionic excitation they should be enough to uniquely determine the perturbative superstring theories . it is clear that there is at least one string theory that people missed completely , this is simeon hellerman 's m-theory-on-a-klein-bottle strings , and there are tons of different vacua which could be thought of as new theories in this formulation , because each s-matrix is a different theory . these perturbative string theories link up non-perturbatively into an ubertheory called m-theory . a theory today would be called part of string theory if it is a description of some configuration of m-theory . there is no full enumeration of these , so it is impossible to say exactly what a configuration is , although there is a partial list , and there is no real arbitrariness .
if that operator provides a basis for the hilbert space of your problem , and you actually can is do the linear combination , the wave function collapse is the following statement $$ |\psi\rangle = n \sum_{i}\alpha_{i}|i\rangle \rightarrow |\psi\rangle_{&gt ; }=m \alpha_{j}|j\rangle$$ where $\rightarrow$ means measurement of the observable linked to $\hat{o}$ and the subscript $&gt ; $ means the wave function after that measure . you can see that in the state $$|\psi\rangle=m \alpha_{j}|j\rangle $$ the probability of measuring any eigenvalue different from $\alpha_{j}$ is zero , this is , the probability of measuring $\alpha_{j}$ is one . however , due to the nature of the schrödinger equation , the wave function will evolve into other linear combination after a certain amount of time
perhaps my ensuing answer will be a little too simple to be satisfying to you , but i only have a firm grasp of things when i keep it simple . let 's just consider statistical mechanics and look at the canonical ensemble ( where there is a constant number of particles [ n ] , volume [ v ] , and temperature [ t ] ) . the components that make up this system want to reach equilibrium because that is how thermodynamic systems behave . the equilibrium point in the canonical ensemble is defined as the point at which the helmholtz free energy , a , is minimized , and it is defined as : a = u - ts where u is potential energy ( a negative value ) and s is entropy . entropy is also defined in stat mech as s = -k ln ( w ) where k is the boltzmann constant and w is the number of microstates in a system . what does this mean ? a system in equilibrium does not always increase its entropy , rather it minimizes its free energy and one way to minimize free energy is to maximize entropy . let 's consider the three different states of h2o : at low temperatures , hydrogen bonds ( i.e. . potential energy ) dominate free energy and the system exists as a low entropy solid . at intermediate temperatures , neither potential energy or entropy dominate and its a liquid . at high temperatures , the entropy term dominates and the water molecules exist far away from each other so as to have an extremely high number of microstates , and very little potential energy .
there are 3 observations that support the big bang theory , i.e. origin of the universe in a singularity : the redshift of galaxies , as you already mentioned . the cosmic background radiation . the amounts of different nuclei in the universe , notably the preponderance of light elements like hydrogen and helium . each of these alone would probably not be sufficient to support the big bang theory . the redshift of galaxies could be explained by some other theory , some have been suggested by hoyle and narlikar in the past . probably the other two phenomena could be explained independently as well , but it is the conjunction that fits so well with the big bang hypothesis . does that settle the matter once and for all ? short answer is no . since these 3 observations have been made and confirmed , more detailed observations have been added to the mix and this has complicated the story for the big bang model . but that would take us into a longer post . the current model which is the most widely accepted is the so-called lambda-cdm model . as for the problem of the universe starting in a real singularity , instead of a very dense state , this is still an open problem related to a yet to be invented ( or completed ) theory of quantum gravity . our current understanding of singularities in general relativity is going back to the penrose-hawking singularity theorems . they are of the kind " here be dragons ! " in that they delineate the conditions for singularities to form and point where our knowledge ends . more can not be done , because a singularity is basically a failure of the theory .
first of all , i think you are correct that mercury 's precession is only useful as a check on classical , large-scale theories of gravity , like mond , which would actually govern astronomical interactions . as pointed out in the comments , quantum gravity mainly concerns itself with an entirely different domain , namely planck-scale distances , times , and densities , since those are the scenarios in which classical gr seems to break down . accordingly , the typical checks on theories of quantum gravity are things like black hole entropy , and predicting some sort of non-singular behavior for the universe at the big bang . using lqg to calculate a property of a complex real-world system like the solar system is at worst impossible and at best absurdly complicated , if it in fact reduces to classical gr ( which i have also heard is yet to be proven ) . on the other hand , a theory of quantum gravity could perhaps handle the " interesting " part of the calculation of planetary precession without incorporating mass directly . all it really needs is a connection , which in lqg is provided by the su ( 2 ) gauge field $a^i_a$ defined on the edges of a graph . given a graph and a connection , one might be able to compute something akin to a geodesic through the graph , which in the continuum limit would correspond to the orbit - though my knowledge of lqg falls short here , i am not sure whether that is how you would actually get from quantum geometry to an orbit . and anyway , as far as i know , in order to get the connection for a given mass distribution ( i.e. . the sun ) in the first place , you would have to use classical gr to find the christoffel symbols .
i will only give a general idea of why this should be so . the concrete derivation for non-ideal gas is a little tricky and not very illuminating in my opinion . so , let us write the grand canonical partition function $$\omega \equiv \sum_n z_n {\lambda}^n$$ where $\lambda = e^{\mu \over k_b t}$ and $z_n$ is $n$-particle canonical partition function . now , the virial expansion is nothing else than cluster expansion of $\omega$ . this is just a combinatorial theorem , so that you can write $$\log ( \omega ) = \sum_n b_n {\lambda}^n$$ with $b_n = v^{n-1} f_n ( z_1 , \cdots , z_n ) $ and $f_n$ determined by the cluster expansion theorem . because none of the $z_n$ depends on the pressure , $b_n$ can not either . actually , the pressure enters these formulas only through the thermodynamic identity $\log ( \omega ) = {pv \over k_b t}$ .
the central point of the question is somewhat ambiguous , but here is an effort to answer it . i am sorry in advance if i have misunderstood it . does light/photons travel ? the question whether light travels from place a to place b or not , can be answered mainly by experience and experiment/observation . when you hold a torch in the dark and you aim it at some point in the background where it is dark , you can see its effects almost immediately . from having being dark , now it is bright and you can see the objects that exist there . that means that light not only travelled there and illuminated the area , it also came back to your eye to give you the information about the objects . this means that light has not always been there , suspended in the air , waiting for you to turn the torch on and make it become reality . i don’t think this is how you envision it . does light “feel” the existence of space ? this type of questions touch on the borders of ontology , somewhat . it is not very easy to formulate answers because one has to talk in terms of metaphysical notions and concepts which , unfortunately , fall outside the scientific method of thinking . but let us take a look at it from this point of view : imagine we send a laser beam from one side of our room to the other . watching it without an apparatus it looks as if light did not have to travel at all , it looks as if the event evolved instantly . a very sensitive apparatus , however , can sense that light has actually taken some time to go there and back . the situation can become more obvious if we try to send the laser beam to the moon and back ( this has been done . ) even we , without any apparatus , can tell that the distance involved must be huge . so space becomes important and even light “feels” the vastness of it . in the experiments you mentioned , the extremely sensitive detectors can distinguish photons arriving with a time difference just a few nanoseconds or less , due to the slightly different paths they take ( space becomes very important in less obvious ways ) light can even “feel” the geometry of space-time , as is demonstrated by the deflection of light-rays passing near the surface of the sun , during a total solar eclipse . light can “feel” the immense density of a bose-einstein condensate by slowing down to incredibly low speed . you can run fast enough and catch up with it ! ! the question whether or not light takes a well defined path to go from a to b involves quantum mechanics , and from your comment i read that it does not interest you at the moment ( ? )
in special relativity , hertz per dioptre is an excellent unit for showing the joint invariance of electromagnetic phenomena in the behavior of all types of lenses , reflective or refractive , under the effects of the lorentz transformation along the axis of motion . i am not aware of any other unit that links those two domains in quite that way . in the case of refractive lenses with chromatic dispersion , the invariance turns out to be non-trivial and a bit surprising , since it asserts that the atomic materials in a lorentz compressed lens must maintain a very specific relationship in how they interact with a spectrum of gamma-shifted light frequencies . here 's how it works for the easier reflective-lens case . first , imagine a sphere 4 meters across with an $f=280$ thz resonant infrared light wave inside . why 4 meters ? well , i am trying to use the correct definition of dioptre . that is the focal length of a refractive or reflective lens , which means the distance it requires to converge parallel light down to a single focal point . in this case , the lens is reflective and has spherical curvature . looking only at a region small enough ( e . g . 2 cm across ) to avoid spherical aberration , the focal length of the $d=4$ m sphere is $l=\frac{1}{2}r=\frac{1}{4}d=\frac{1}{4}4=1$ m . so , a 4 m diameter sphere thus correctly gives a dioptre ( curvature ) of $\delta=1/l=1/1=1$ d , where d $=m^{-1}$ . next , accelerate the sphere along it x axis to a velocity of $v=\sqrt{\frac{3}{4}}$ c , which gives a lorentz factor of $\gamma=2$ . that means that both the sphere and the resonant light pattern within it will be compressed to $\frac{1}{2}$ their original lengths along the x axis , from the perspective of a viewer " at rest " relative to the moving sphere . for the small reflective lens regions around either end of where the x axis crosses the sphere , the pre-acceleration curvature was $\delta_0=1d$ ( the zero subscript indicates the rest frame ) . after acceleration to $\gamma=2$ the sphere becomes an oblate spheroid , and the curvatures of the two reflective lens areas have been reduced to $\delta_1=2d$ , where higher dioptre numbers indicate flatter curves . ( the proof of that is left as an exercise for the reader , but it is not difficult . ) now let 's examine what happens to the frequency of the light within the sphere . the neat thing about special relativity is that physics must remain invariant for both the observer and the observed system . so , if there were n wavelengths of resonant light crossing the sphere along the x axis prior to it being accelerated , there must also be n wavelengths along that same length after the compression . in other words , the wavelengths of the radiation must also be cut in half along x ( only ) , resulting in twice the frequency as before . that transforms the original x-axis $f_0=280$ thz light of the at-rest sphere into $f_1=560$ thz light in the moving sphere . an observer in the rest frame would see this as bright green . observant readers may now be saying " hey , that can not be right ! the lorentz factor also slows time . . . so should not the light in the moving sphere be slower and thus less energetic ? " while it is true that time will pass more slowly within the moving sphere , it is not correct to think that this same light will be slower when viewed from the rest frame . for that situation the geometry of the wavelengths wins , and the light looks green . however , a simpler way to think of it is that since the light is being emitted and reflected by an object traveling at $\gamma=2$ ( or equivalently $v=\sqrt{\frac{3}{4}}$ c ) , the ordinary doppler effect will double its frequency . ( @colink has correctly noted that the above explanation glosses over some important complications . please see his excellent comment for more info . i may try to address that soon . ) now it is time to put this all together . the original light and sphere had an eta factor of : $\eta_0=f_0/\delta_0 = ( 280 thz ) / ( 1 d ) = 280\times{10}^{12}$ hpd where 1 hpd = 1 hz/d ( hertz per dioptre ) . the moving light and sphere has an eta factor of : $\eta_1=f_1/\delta_1 = ( 560 thz ) / ( 2 d ) = 280\times{10}^{12}$ hpd . in other words , the eta factor $\eta$ , which relates the lorentz-transformed electromagnetic waves to the lorentz-contracted physical mirrors from which they reflect , has remained invariant for this example of $\gamma=2$ . it is not an isolated case . it is easy to show that $\eta$ is a universal invariant of special relativity : $\forall{v_i} ( \eta_i=\frac{f_i}{\delta_i}= c ) $ where c is a constant in units of hpd = hz/d = hertz per dioptre . now the remarkable generalization of all of this is that by the same kinds of geometric arguments and application of the " physics must be preserved in both frames " principle , refractive lenses must also fall under the above argument . if a refractive lens has chromatic dispersion ( the colored fringes seen in cheap lenses ) , then the constant c in the above equation will become a frequency-dependent value $c ( f ) $ . yet the eta invariance remains intact ! that is surprising because light dispersion is a pretty complicated phenomenon , yet from the rest frame these messy compressed atoms must nonetheless maintain eta invariance . that is . . . unexpected . thus hpd units not only have real physical meaning , but a meaning that relates directly to the original intent of both the hertz and dioptre units ( versus just being $m/s$ in disguise ) . this meaning in turn provides an easy way to express an invariant relationship in special relativity that links together the electromagnetic and mechanical lorentz transformations in an unexpected and non-intuitive fashion . and finally , despite all the above unexpectedly interesting ( to me at least ! ) sr relationships involved , the hpd unit really did originate as a bit of humor in ( as best i could uncover ) this xkcd discussion posting back in 2007 . so , shrodingersduck from the people 's democratic republic of leodensia , wherever you are six years later , i thank you for inadvertently creating an interesting and quite fun opportunity to explore special relativity in a rather unusual context . addendum 2013-01-31 the generality of the hpd unit in special relativity can i think be stated even more broadly . so , here goes : light frequency , geometric forms , and frequency-dependent refractive indices all change when systems undergo lorentz transformation , so they are not individually lorentz invariant . theorem : if the optical characteristics of an optical system are instead described using hpd ( hertz per dioptre ) and/or its inverse unit dph ( dioptres per hertz ) , the resulting description of its optical properties will remain constant ( "eta invariance" ) regardless of relativistic frame or orientation from which the optical system is analyzed . that is a theorem only . @colink 's excellent observation that the doppler argument i made could be bogus because the shift works differently depending on whether the light is moving with or against the velocity still concerns me . so , i want to look at that a lot more closely and see if i can disprove my own theorem . still , would not it be delightful if a unit defined as a joke turned out to be relativistically invariant when the common units for the same phenomena are not ? the other obvious generalization question is this : does eta invariance ( if it exists ) apply to other wave phenomena ? and finally , @joezeng , i think i misunderstood your question about whether the eta factors ( descriptions of optical components using hpd units ) are related to the velocity of light . well , hpd does have dimensional equivalence to a velocity ( $m/s$ ) , but if there is a meaningful way to re-interpret an hpd value as a velocity , i sure do not see it . intriguing question , though . . .
let 's split up the forces , it gives an easier view on what should happen . horizontal forces what we see is a centrifugal force . centrifugal force ( from latin centrum , meaning " center " , and fugere , meaning " to flee" ) represents the effects of inertia that arise in connection with rotation which are experienced as an outward force away from the center of rotation so , outward force away from the center of rotation . this means that the water would only be able to appear in place if there is a force in the opposite direction , which is known as friction . where is the friction if you take the glass away ? there is no friction anymore , thus that video shows special effects . ; - ) vertical forces there is also the gravitational force . what would keep the water up so long ? nothing . furthermore , if you look at his other videos , you can confirm it is cgi . . . :- )
we can calculate charge on baloon by assuming it a perfect sphere with uniform charge distribution . $$v=\frac{q}{4\pi\epsilon_0\times r}$$ where $r$ is radius of ballon .
this question can be answered , but only with the addition of some extra assumptions . what we know for sure is that the entropy of the universe as a whole cannot decrease , and hence fluids can not be unmixed without increasing the entropy of some other system by an amount equal to $\delta s_\text{mixing}$ . this does not immediately tell us anything about work . however , let us now assume we have to hand a source of mechanical work , and a large heat reservoir at temperature $t$ . i will assume that we will use the work ( somehow ) to unmix the fluids , and that any excess heat that gets generated will be dumped in the heat reservoir . in general the mixed and unmixed states of the fluid system will have different energies . i will call the difference $\delta h$ , with positive $\delta h$ meaning that the unmixed state has a higher energy than the mixed one . ( i am guessing that for real miscible fluids this is usually a small positive value , but it need not necessarily be . ) we will assume it takes us an amount $w$ of work to unmix the fluids . $w$ has been lost from the work source , and $\delta h$ has been gained from the fluid system , so the first law says that the energy of the heat bath must increase by $w-\delta h$ . we can now calculate the total change in entropy . the fluids ' entropy has decreased by $\delta s_\text{mixing}$ , and the heat bath 's entropy has increased by $ ( w - \delta h ) /t$ , so the total is $\delta s_\text{total} = \frac{w - \delta h}{t} - \delta s_\text{mixing} \ge 0 , $ or $w \ge t\delta s_\text{mixing} + \delta h . $ changing this to an equality gives you the minimum amount of work required to unmix the fluids . but note that the expression involves $t$ , which is the temperature of the heat bath that i assumed to exist . without the heat bath there would be nowhere for the energy from the work source to go once it is used up . also note that $t$ is not necessarily the temperature of the mixed fluids , which could be different from that of the heat bath . i did not need to make any assumptions about the fluids ' temperature in order to work out the above . in practice you had usually assume the fluids to be in contact with the heat bath , so that the two temperatures are in fact equal . in this case ( and only in this case ) , the minimum work required is equal to the difference in helmholtz free energy between the mixed and unmixed states . it is possible for this minimum bound to be negative , meaning that you can actually get work out of the system by unmixing the fluids - but in this case the fluids would be immiscible , so the mixed state would be the unstable one . finally , let 's make the additional assumption that the fluids are gases and that we have two very special semi-permeable pistons to hand , one at either end of the container . one is permeable to the first gas but completely blocks the second , and the other is permeable to the second gas but blocks the first . we can now very slowly push the two pistons in opposite directions until they meet in the middle , while holding the system in contact with a heat bath . if we do it slowly enough we will have reversibly unmixed the two gases by something similar to reverse osmosis , using an amount of work equal to the minimum $w$ calculated above . ( i got that last idea from this brilliant paper on the entropy of mixing by edwin jaynes : http://bayes.wustl.edu/etj/articles/gibbs.paradox.pdf )
i put an extra answer , since i believe the first jeremy 's question is still unanswered . the previous answer is clear , pedagogical and correct . the discussion is really interesting , too . thanks to nanophys and heidar for this . to answer directly jeremy 's question : you can always construct a representation of your favorite fermions modes in term of majorana 's modes . i am using the convention " modes " since i am a condensed matter physicist . i never work with particles , only with quasi-particles . perhaps better to talk about mode . so the unitary transformation from fermion modes created by $c^{\dagger}$ and destroyed by the operator $c$ to majorana modes is $$ c=\dfrac{\gamma_{1}+\mathbf{i}\gamma_{2}}{\sqrt{2}}\ ; \text{and}\ ; c{}^{\dagger}=\dfrac{\gamma_{1}-\mathbf{i}\gamma_{2}}{\sqrt{2}} $$ or equivalently $$ \gamma_{1}=\dfrac{c+c{}^{\dagger}}{\sqrt{2}}\ ; \text{and}\ ; \gamma_{2}=\dfrac{c-c{}^{\dagger}}{\mathbf{i}\sqrt{2}} $$ and this transformation is always allowed , being unitary . having doing this , you just changed the basis of your hamiltonian . the quasi-particles associated with the $\gamma_{i}$ 's modes verify $\gamma{}_{i}^{\dagger}=\gamma_{i}$ , a fermionic anticommutation relation $\left\{ \gamma_{i} , \gamma_{j}\right\} =\delta_{ij}$ , but they are not particle at all . a simple way to see this is to try to construct a number operator with them ( if we can not count the particles , are they particles ? i guess no . ) . we would guess $\gamma{}^{\dagger}\gamma$ is a good one . this is not true , since $\gamma{}^{\dagger}\gamma=\gamma^{2}=1$ is always $1$ . . . the only correct number operator is $c{}^{\dagger}c=\left ( 1-\mathbf{i}\gamma_{1}\gamma_{2}\right ) $ . to verify that the majorana modes are anyons , you should braid them ( know their exchange statistic ) -- i do not want to say much about that , heidar made all the interesting remarks about this point . i will come back later to the fact that there are always $2$ majorana modes associated to $1$ fermionic ( $c{}^{\dagger}c$ ) one . most has been already said by nanophys , except an important point i will discuss later , when discussing the delocalization of the majorana mode . i would like to finnish this paragraph saying that the majorana construction is no more than the usual construction for boson : $x=\left ( a+a{}^{\dagger}\right ) /\sqrt{2}$ and $p=\left ( a-a{}^{\dagger}\right ) /\mathbf{i}\sqrt{2}$: only $x^{2}+p^{2} \propto a^{\dagger} a$ ( with proper dimension constants ) is an excitation number . majorana modes share a lot of properties with the $p$ and $x$ representation of quantum mechanics ( simplectic structure among other ) . the next question is the following : are there some situations when the $\gamma_{1}$ and $\gamma_{2}$ are the natural excitations of the system ? well , the answer is complicated , both yes and no . yes , because majorana operators describe the correct excitations of some topological condensed matter realisation , like the $p$-wave superconductivity ( among a lot of others , but let me concentrate on this specific one , that i know better ) . no , because these modes are not excitation at all ! they are zero energy modes , which is not the definition of an excitation . indeed , they describe the different possible vacuum realisations of an emergent vacuum ( emergent in the sense that superconductivity is not a natural situation , it is a condensate of interacting electrons ( say ) ) . as pointed out in the discussion associated to the previous answer , the normal terminology for these pseudo-excitations are zero-energy-mode . that is what their are : energy mode at zero-energy , in the middle of the ( superconducting ) gap . note also that in condensed matter , the gap provides the entire protection of the majorana-mode , there is no other protection in a sense . some people believe there is a kind of delocalization of the majorana , which is true ( i will come to that in a moment ) . but the delocalization comes along with the gap in fact : there is not allowed propagation below the gap energy . so the majorana mode are necessarilly localized because they lie at zero energy , in the middle of the gap . more words about the delocalization now -- as i promised . because one needs two majorana modes $\gamma_{1}$ and $\gamma_{2}$ to each regular fermionic $c{}^{\dagger}c$ one , any two associated majorana modes combine to create a regular fermion . so the most important challenge is to find delocalized majorana modes ! that is the famous kitaev proposal arxiv:cond-mat/0010440 -- he said unpaired majorana instead of delocalised , since delocalization comes for free once again . at the end of a topological wire ( for me , a $p$-wave superconducting wire ) there will be two zero-energy modes , exponentially decaying in space since they lie at the middle of the gap . these zero-energy modes can be written as $\gamma_{1}$ and $\gamma_{2}$ and they verify $\gamma{}_{i}^{\dagger}=\gamma_{i}$ each ! to conclude , an actual vivid question , still open : there are a lot of pseudo-excitations at zero-energy ( in the middle of the gap ) . the only difference between majorana modes and the other pseudo-excitations is the definition of the majorana $\gamma^{\dagger}=\gamma$ , the other ones are regular fermions . how to detect for sure the majorana pseudo-excitation ( zero-energy mode ) in the jungle of the other ones ?
if the universe is spatially infinite , it always had to be spatially infinite , even though the distances were shortened by an arbitrary factor right after the big bang . in the case of a spatially infinite universe , one has to be careful that the singularity does not necessarily mean a single point in space . it is a place - the whole universe - where quantities such as the density of matter diverge . in general relativity , people use the so-called penrose ( causal ) diagrams of spacetime in which the light rays always propagate along diagonal lines tilted by 45 degrees . if you draw the penrose diagram for an old-fashioned big bang cosmology , the big bang itself is a horizontal line - suggesting that the big bang was a " whole space worth of points " and not just a point . this is true whether or not the space is spatially infinite . at the popular level - and slightly beyond - these issues are nicely explained in brian greene 's new book , the hidden reality .
there are a lot of comments . the end result is that your first proposition is correct as far as the physics models we have validated with experimental evidence go . even classically , when one has an extended body whose trajectory is assumed by the center of mass , the paradox becomes trivial , even if the center of mass never reaches the goal , half of the solid body minus an infinitesimal essentially does . the heisenberg uncertainty principle is involved at the elementary particle state , and yes the classical motion is no longer relevant and does not describe the behavior of nature at the microscopic scale . as for your second proposition , what disallows discreteness of space and time is not lack of imagination . it is at the moment incompatibility of known and validated physics and mathematical models proposed with such discreteness incorporated . at the moment as far as i know , locality for lorenz invariance , which has been validated an innumerable number of times , is the main obstacle for such theories , but also i am not aware if there exist proposals that can also incorporate the standard model of particle physics , which is an encapsulation of all the experimental measurements we have up to now . thus discreteness in time exists in some models but is not validated by any data and is not a proposition accepted by the mainstream physics community . in any case even if you take time as a variable the argument with the heisenberg uncertainty principle is sufficient as there also exist a delta ( e ) delta ( t ) > h_bar form of it .
this types of problems are solved by observing projectile movements in $x$ and $y$ direction separately . in $x$ direction you have constant velocity movement $$v_x = v_{x0} = v_0 \cos ( \theta ) , \ ; ( 1 ) $$ $$x = v_{x0} t +x_0 = v_0 \cos ( \theta ) \ ; t +x_0 , \ ; ( 2 ) $$ and in $y$ direction you have constant acceleration movement with negative acceleration $-g$ $$v_y = - g t + v_{y0} = - g t + v_0 \sin ( \theta ) , \ ; ( 3 ) $$ $$y = - \frac{1}{2} g t^2 + v_{y0} t + y_0 = - \frac{1}{2} g t^2 + v_0 \sin ( \theta ) \ ; t + y_0 . \ ; ( 4 ) $$ your initial conditions are $$x_0 = 0 , \ ; y_0 \ne 0 , $$ and final conditions ( at moment $t=t$ projectile falls back on the ground ) are $$t = t , \ ; x = d , \ ; y = 0 . $$ if you put initial and final conditions into equations ( 2 ) and ( 4 ) you end up with two equations and two unknowns $v_0 , t$ . by eliminating $t$ you get expression for $v_0$ . my calculations show that $$v_0 = \frac{1}{\cos ( \theta ) }\sqrt{\frac{\frac{1}{2} g d^2}{d \tan ( \theta ) +y_0}}$$ which is i believe equal to your equation . maybe your problem is that $d$ means displacement in direction $x$ , while the total displacement is $\sqrt{d^2+y_0^2}$ ?
yeah , neils bohr and john von neumann were skeptics : many prominent physicists thought it could not even work , based on their knowledge of physical principles . in quantum mechanics , the uncertainty principle developed by einstein , says that the energy ( and therefore the frequency , by e=hv ) of a photon can not be known to great precision in a short time . in masers , photons last for a very short time . therefore , no less than neils bohr and john von neumann thought it could not work , even after it had been created . the solution to this apparent paradox is that , though the photons all have the same frequency and direction , which atoms do the emitting and when remains unknown . the emitting atoms maintain an anonymity that avoids uncertainty > violation . read more : why was the laser light invented ? | ehow . com http://www.ehow.com/about_5250763_laser-light-invented_.html#ixzz1uic0yare
you need to careful about treating photons as particles like electrons . a photon is best thought of the unit of interaction of the photon field . the light from distant galaxies is not like a hail of little bullets coming towards us . the photons are delocalised and do not have a particle like position until they interect with your eye/ccd/whatever . anyhow , the reason light from distant galaxies is red shifted is that the spacetime in between us and the distant galaxy has expanded since the light was emitted , and that means the energy of the light has been spread out over a larger area .
it is difficult to answer this question . an em wave is generated by vibrating charges and nuclear reactions . sun is full of vibrating charges and nuclear fusions . because of this full range of frequencies are emitted . at distances close to sun we observe the directions of waves to be random . but at far away distances the direction of waves seem parallel . since only parallel waves can have constant separation between them . converging and diverging waves become distant at longer distances .
i did some research and calculations : to summarize : the relativistic rocket will not break apart , uniform acceleration along it is possible . but the observers will measure different accelerations due to the gravitational time dilation . in more detail : let 's assume the observer at the bottom measures $\alpha$ acceleration . so for an inertial observer outside ( who draws the minkowski chart ) this accelerating observer 's motion will be hyperbolic . the semi mayor axis of this hyperbola will be $c^2/\alpha$ . let 's say the length of the relativistic rocket is $h$ this is measured before the launch , and the mechanical stresses during the travel try to keep this value constant in the reference frame of the rocket , otherwise the rocket would break apart ( we assume it do not break apart ) . as the rocket accelerates the plane of simultaneity rotates from the viewpoint of the inertial observer . so the two ends of the rocket will not trace two identical hyperbolas . but the two ends always connect two points on the two hyperbolas whose slope is the same ( you can see this on the rindler chart ) . so all parts of the rocket travel with the same speed at the local frame simultaneously , so the rocket 's acceleration will be uniform and it will not break apart . but the two hyperbolas are different . the bottom traces a hyperbola whose semi-mayor axis is $c^2/\alpha$ , the top traces a hyperbola whose semi-mayor axis is $c^2/\alpha + h$ . the acceleration that corresponds to the second hyperbola is $1 + \alpha h / c^2$ times smaller than $\alpha$ . this a bit paradoxical situation , because i stated that the acceleration is uniform along the rocket , now i state it is different due to the different hyperbolas . this paradox can be resolved if i introduce gravitational time dilation , so i assume that the clock of the observer at the top ages faster with the rate i mentioned above . so the top observer measures less acceleration this way . there is an event horizon at the rindler-horizon where $h = -c^2/\alpha$ , there the time stands still . this is somewhat analogous with the black hole 's event ho+rizon . the gravitational time dilation formula mentioned in the wikipedia and the one mentioned in the comment is the same , but that exponentional formula never reaches zero , that would mean the rindler-horizon does not exist . . . which would be a bit odd . so i still need some research . update : the wikipedia article has been fixed since last update . so the general formula to the gravitational time dilation is $e^{\int^h_0 g ( h ) dh / c^2}$ , where $g ( h ) $ is the measured gravitational acceleration at the given level . for rindler observers $g ( h ) = c^2/ ( h+h ) $ where $h = c^2/\alpha$ . doing the integral gives $e^{ln ( h+h ) - ln ( h ) } = ( h+h ) /h$ . substituting $h$ back i will get the original $1 + \alpha h / c^2$ i mentioned earlier .
in reality , the definition of incompressible fluid is not what you listed . physically speaking , incompressible means : $$ \frac{\partial \rho}{\partial p} = 0 $$ or the change in density with pressure is zero . this in turn implies the speed of sound is infinite . this also , technically , allows for changes in density with time if those changes are only due to temperature changes and not pressure changes , ie . $$ \frac{\partial \rho}{\partial t} \neq 0$$
( i can not quite comment on the previous post , so i will have to write a new answer ) . if we set the curvature of the earth to be non-negligible in our problem , yes , gravity would slow the baseball down by an extremely tiny amount , but , if we exclude this case ( which , again , i stress to be many orders of magnitude below anything considerable ) , then no , gravity itself does not slow the ball down since the force of attraction ( the direction of the vector of acceleration ) points exactly downwards and contributes nothing to the horizontal component .
here 's an example . let 's consider the case of a free particle in a force field given by a potential $u$ . if $$l=t-u , $$ then the newton 's equations of motion $$\dot {\mathbf p}=-\dfrac{\partial u}{\partial \mathbf r}$$ are equivalent to lagrange 's equation in cartesian coordinates : $$\dot {\mathbf p} = \frac{\partial l}{{\partial \mathbf q}} , $$ with $\mathbf q = \mathbf r$ . the principle of least action states that motion $\gamma$ beetween two points of the generalized configurations space , $ ( \mathbf r _1 , t_1 ) $ and $ ( \mathbf r _2 , t_2 ) $ , is the one that makes the action $s ( \gamma ) =\int _{t_1}^{t_2} l ( \gamma ( t ) , \dot\gamma ( t ) ) \text d t$ stationary , meaning that the linear part ( the so called differential ) of the variation $$\delta s ( \gamma , h ) =s ( \gamma+h ) -s ( \gamma ) $$ is zero in $\gamma $ . this two things are linked by the fact that ( for a sufficiently nice $l$ ) this condition is equivalent to the lagrange 's equation above mentioned . an interesting feature of this principle is that it does not depend on the system of coordinates chosen , which gives a lot of freedom in choosing the more appropriate set of coordinates for the problems . to give an application , suppose that $\mathbf r= ( x , y ) $ are the cartesian coordinates of a point in the plane and let $\pi= ( r , \theta ) :\mathbb r ^2 \to \mathbb r ^2$ be the polar coordinates . suppose that $$\mathbf r ( t ) = ( x ( t ) , y ( t ) ) $$ is a solution of the equations of motion . then we can show easily that $$\mathbf q ( t ) =\pi ( \mathbf r ( t ) ) , $$ which is the projection onto the polar plane of this solution , solves the equations of motion $$\dfrac{\text d }{\text d t} \dfrac{\partial \tilde l}{\partial \dot {\mathbf q}}=\dfrac{ \partial \tilde l}{\partial \mathbf q} , $$ that is , lagrange 's equation are still valid in the new system of coordinates , with a new lagrangian given by $$\tilde l ( \mathbf q , \dot {\mathbf q} ) =l ( \mathbf r , \dot {\mathbf r} ) . $$ in fact by definition of $\tilde l$ , we have $$\int _{t_1} ^{t_2} \tilde l ( \mathbf q ( t ) , \dot { \mathbf q} ( t ) ) \text d t=\int _{t_1} ^{t_2} l ( \mathbf r ( t ) , \dot { \mathbf r} ( t ) ) \text d t . $$ since $\mathbf r$ satisfies lagrange 's equations , it minimizes the action associated to $l$ . so $\mathbf q$ minimizes the action associated to $\tilde l . $ so $\mathbf q$ satisfies lagrange 's equations with the new lagrangian .
your mistake here is to assume that the multiplication $\vec v\cdot \vec \nabla$ is commutative . it is not ; the dot product here is just a convenient mathematical notation . this part of the wikipedia article on navier-stokes equations explains how to interpret this term .
your intuitive picture is basically correct . if you perturb a black hole it will respond by " ringing " . however , due to the emission of gravitational waves and because you have to impose ingoing boundary conditions at the black hole horizon , the black hole will not ring with normal-modes , but with quasi-normal modes ( qnms ) , i.e. , with damped oscillations . these oscillations depend on the black hole parameters ( mass , charge , angular momentum ) , and are therefore a characteristic feature for a given black hole . historically , the field of black hole perturbations was pioneered by regge and wheeler in the 1950ies . for a review article see gr-qc/9909058 for the specific case of the schwarzschild black hole there is a very nice analytical calculation of the asymptotic qnm spectrum in the limit of high damping by lubos motl , see here . see also his paper with andy neitzke for a generalization . otherwise usually you have to rely on numerical calculations to extract the qnms .
the answer is the there is some reduction in mass whenever energy is released , whether in nuclear fission or burning of coal or whatever . however , the amount of mass lost is very small , even compared to the masses of the constituent particles . a good overview is given in the wikipedia article on mass excess . basically , the mass of a nucleus will in general be a little bit off from the sum of the masses of the protons and neutrons inside it . this is because there is a binding energy holding the nucleus together , and your standard $e = mc^2$ gives the equivalent mass for this energy . in the fission of uranium-235 , $$ {}^{235}_{\phantom{0}92}\mathrm{u} + {}^1_0\mathrm{n} \to {}^{236}_{\phantom{0}92}\mathrm{u} \to {}^{141}_{\phantom{0}56}\mathrm{ba} + {}^{92}_{36}\mathrm{kr} + 3\ {}^1_0\mathrm{n} , $$ the total rest mass of the products is slightly less than of the reactants . this is true even though there are the same number of protons ( 92 ) and neutrons ( 144 ) before and after . so it is not as though an entire nucleus disappears , or even an entire proton or neutron . the lost mass comes from the binding energy . the take-away message is that we are not destroying particles to create energy . even nuclear fusion conserves the total number of protons and neutrons . instead , you should think about the mass-energy equivalence the other way around . the fact that there is potential energy capable of being released in nuclear fission implies that the reactants must be heavier than the products . in the same fashion , a typical battery weighs less after being discharged ( though by an immeasurably small amount ) , even though the nuclei are unchanged and the number of electrons is the same . that is , potential energy in any form adds to the mass of the system as a whole , and is not attributable to any one component .
i would not say that fusors are a waste , but so far they have turned out to not be very efficient . there are several factors working against them , compared to linear accelerator-based devices . basically what it boils down to is the neutron production cross section . in a fusor , the ions are in a plasma , and the mean free path is huge . so the effective cross section is small ( in other words , ions have a relatively small probability of hitting each other and fusing ) , and you are spending a lot of energy accelerating ions ( and electrons ) that fuse very rarely . perhaps efficiency can be improved , but the results speak for themselves -- the best efficiency i could find was 10^4 neutrons/w [ 2 ] . despite a lot of work over the years , we do not have a good qualitative understanding of what goes on inside a fusor . linear neutron generators have a huge advantage because they use solid tritiated metal hydride targets [ 1 ] . the ratio of hydride atoms to metal atoms is typically ~2 , which presents a very large effective cross section compared to a plasma . in addition , the generated beam ( s ) of deuterons can be optimized for the target geometry . furthermore , we have a better understanding of what goes on : it amounts to a fixed-target setup , which has a long history in nuclear physics . there is the side issue of d-d vs d-t . since the fusor is fed by gases and typically purges to atmosphere , hobbyists ( and most scientists ) operating fusors are restricted to the use of deuterium due to safety concerns . for a given energy in the typical range , the microscopic cross section for d-d is lower by an order of magnitude or more . but when the efficiency deficit is 10^4 , 10^5 , there is still a big gap even if d-t was used in both cases . [ 1 ] see chapter 7 of phd thesis by j.m. verbeke , uc berkeley , 2000 [ 2 ] miley group , uiuc , 2004
the harsh answer is , " by solving einstein 's field equations " . that is an extremely more difficult problem than using newton 's law -- the equations take up at least a page when you write them out . however for the case of light near a black hole , you can treat the black hole as unaffected by the light , and instead solve the null geodesic equation . this is still harder than newton 's law , but it is at least tractable , compared to einstein 's field equations . for schwarschild black holes , the wikipedia article has much information .
the rope on the left pulls the weight and the pulley towards each other . the rope on the right pulls the floor and the pulley towards each other . each of these pull downwards on the pulley with force $t$ . the total downward force on the pulley is $2t$ . the pulley does not move because the ceiling pulls upward on it hard enough to keep it motionless . the pulley can only be motionless if the total force on the pulley is $0$ . so the upward force from the ceiling is $2t$ . perhaps the tension in the rope on the right is counter intuitive . the weight is motionless , so the total force on it is $0$ . the two forces are gravity and the tension of the rope on the left . both of these have magnitude $t$ . if the rope on the left is pulling upward on the weight , something must pull downward with force $t$ on the rope on the right . that is the floor , which pulls just hard enough to keep the rope from moving . another way you could arrange for a force $t$ to pull downward on the rope on the left is to hang another weight on the rope on the right . this would create the same tension in the rope on the right as the floor does .
a pendulum of ( long ) length l will tick with a period of $2\pi\sqrt{l\over g}$ , and air resistance can be made negligible for a mm-sized oscillation of a heavy object on a several-meter long rigid arm . you need to determine the location of the center of mass accurately to know the effective value of $l$ , but this can be done arbitrarily accurately by balancing the arm and weight on a fulcrum ( or by accurately finding the cm of each of the parts and measuring the configuration of the parts accurately ) . then you just have to count the number of oscillations over a long enough period of time . this is a practical method that allows the determination of g to 5 significant figures ( assuming the error on l is the signifcant one , the lever is 10m , and the position measurements are at the . 1mm scale ) with no technology .
the next thing you will need is time period of a simple pendulum $$t=2\pi\sqrt{\dfrac{l}{g}}$$
have derived it myself : $${\tau}=\dfrac{\underbrace {\large { i}}_{\text{through inductor at steady state}}}{\underbrace {{di_{\small }}/{dt}}_{\text{initial}}}$$ also $$v_{\text{inductor}}=l\dfrac{di}{dt}$$ where $i$ is current through inductor .
in terms of measurements of the field , we can regard the vacuum state as the state that has the least correlations between measurements of the field in different places . at an elementary level , we can call the vacuum state the zero-quantum field state . the statistics of field measurements in the vacuum state are also highly symmetric , being isotropic , homogeneous , and invariant under lorentz boosts . as we add more quanta to the quantum field state , the correlations between field measurements in different places become more complex and less symmetrical . there are many significant differences between a classical field and a quantum field , but the most basic is that instead of talking just about the field at a particular place , we talk about the probability that the field will take one value or another , at many different places all at once . as a result , instead of describing modulations of a classical field --how a particular configuration of the field is different from the zero field-- , we have to describe modulations of all the correlations of the field --how a particular configuration of the quantum field is different from the vacuum state . a classical field is a function of just one point in space time , something like $\phi ( x ) $ , but a quantum field can best be understood ( imo ) in terms of a function of many points , $w ( x_1 , x_2 , . . . , x_n ) $ , that describes the correlations between the field at $n$ different places . for the vacuum state , these functions are called the vacuum expectation values ( vevs ) , but we can equally well construct a similar function in any state . a quantum field operator $\hat\phi ( x ) $ describes how to construct correlation functions for all different $n$ , in any state that we can construct in the hilbert space of quantum field states . the exact changes that are made to the vevs by the action of a single quantum field operator are in a mathematical sense elementary operations on the discrete structure that ultimately derives from the fact that we do not talk about 2½-point correlation functions , for example , we only talk about 2-point , 3-point , . . . , $n$-point correlation functions . a single quantum of the field is often taken to be associated with a given frequency , but it can in general be associated with an arbitrarily complicated modulation of the correlation functions , at many different frequencies . the significance of this is that a quantum field operator is not just " add one quantum " ( though it is that ) , it also says where in space-time to add the quantum , $\hat\phi ( x ) $ . a single quantum , however , may be described as a superposition of different frequencies , or it may equally well be described as a superposition of different positions . in particular , $\hat\phi ( x_1 ) +\hat\phi ( x_2 ) $ is in many ways as good as a single quantum field operator as is $\hat\phi ( x ) $ . the mathematics of creation and annihilation operators is given in answers to the question you linked to , in very bare form , so if that is not what you want then you must want something different . to put the above into two bullet points , $\bullet$ quantum field operators modulate the correlations that are present in the vacuum state , and $\bullet$ correlations are an intrinsically discrete concept because they are constructed as correlations between 2 , 3 , or any integer number of measurements . there is quite a lot of detail missing from the above , of course ; i would particularly single out the issue of how to understand the consequences of measurement incompatibility at time-like separation . i have attempted not to speak too much beyond your question . if you remember your qm well enough , btw , the quantized simple harmonic oscillator ( sho ) can be regarded as the mathematical foundation of qft ; the quantum field can be constructed as an infinite number of interacting shos ( albeit non-rigorously ) . but you can get that from many of the textbooks .
the quantity that determines day-to-day gravity we feel is the difference is clock-rate at different places , but yes , it is all in the time coordinate , in the sense that the space we see is flat , and only the time coordinate is mismatched from point to point . gravity describes spaces that curve too , distances that change from place to place , but this effect does not matter since it is very small . the reason we see time-component of metric and nothing else is simply because we are so much longer in time than in any other direction--- when you sit still for a few seconds , you extend over a meter or so , but your time extent is one light-second , or 300,000 meters . so on our scales , thing are very very long in time , and short in space , and we can see a slight curvature between different places in the time coordinate , because we go by so much time coordinate .
a fresnell lens is essentially a " collapsed " plano-convex lens . if you were to raise each ring of the fresnell lens so the outer diameter of the ring were at the height of the inner diameter of the next larger ring , you had have reconstructed the original plano-convex lens . obviously , there are some diffraction effects and various other higher-order aberrations , but at the simple level i think you are asking , the fresnell lens produces a full image of the object . that is another way of saying yes , it would produce an image of the sun even if the sun were not on the optic axis .
for your first question : obtaining equation ( 2 ) from ( 1 ) is basically just a matter of taking the derivative . consider this expression that relates an infinitesimal change in the value of a function $f ( x , b ) $ to the infinitesimal changes in the values of its arguments : $$\delta f ( x , b ) = \frac{\partial f ( x , b ) }{\partial x}\delta x + \frac{\partial f ( x , b ) }{\partial b}\delta b$$ if you have ever done anything with uncertainty analysis , in particular error propagation , you should be familiar with this sort of thing , but if not , it is still not too complicated - it is just the chain rule of multivariable calculus . try plugging in the left side of equation ( 1 ) for $f ( x , b ) $ and see that it works out . a different way to do the same derivation is to consider the right triangle whose legs are formed by the rod and the " ceiling " and whose hypotenuse is formed by the string . originally , the triangle has side lengths $a$ , $b$ , and $l - x$ ; after an infinitesimal displacement of the system , it has side lengths $a$ , $b + \delta b$ , and $l - x - \delta x$ . using the pythagorean theorem on the first set of lengths , you get $$a^2 + b^2 = ( l - x ) ^2$$ and on the second set of lengths , you get $$a^2 + ( b + \delta b ) ^2 = ( l - x + \delta x ) ^2$$ try combining these to see that you get equation ( 2 ) out of it . remember that because the $\delta$ quantities are infinitesimal , you can ignore terms involving $\delta x^2$ and $\delta b^2$ . as for the second question : probably the easiest way to think about it is that for a static system , the principle of virtual work is equivalent to potential energy minimization . to find the state of the system , you find what values of the coordinates minimize the potential energy , which in this case is $-mgx - mgb$ . ordinarily , when you want to minimize a function , you take the derivative and set it equal to zero , but you can just as well calculate an infinitesimal change using the formula above and set that equal to zero . try setting $f ( x , b ) $ in that formula equal to the potential energy . if you are still concerned about tension , though , you could make the following argument , staying true to the spirit of virtual work . as the ring moves downward by a small amount $\delta b$ , the work done on it is $$\delta w_m = mg\delta b - t\sin\theta\ , \delta b$$ where the factor $\sin\theta = \frac{b}{\sqrt{a^2 + b^2}}$ picks out the vertical component of the tension . as this happens , the string gets pulled through the pulley , thus raising the larger mass by a displacement $\delta x$ . the work done on the larger mass is going to be $$\delta w_m = -mg\delta x + t\delta x$$ when the system is in equilibrium , it will be in a state such that the no work gets done if it shifts a tiny amount in either direction . that means that the total work for an infinitesimal displacement should be zero , $$\delta w_m + \delta w_m = 0$$ using this and equation ( 2 ) , namely $\delta x = -\frac{b}{\sqrt{a^2 + b^2}}\delta b$ , you can show that the two tension terms , one from $\delta w_m$ and one from $\delta w_m$ , cancel out . any work done by the tension on one mass is canceled out by work done by tension on the other mass . so even if you do not actually take the tension into account , you still wind up getting the right result .
if we restrict ourselves to newtonian gravity , then it is indeed temporally symmetric . orbits require the orbiting body to be gravitational bound to the central object---i . e . they must have negative energy ( i.e. . the magnitude of the potential energy is greater than the kinetic ) relative to the central body . one way gravity can do this is by exchanging energy with a third body . this is still time-symmetric . ( thanks @michaelbrown for reminding me ) . in general , for something to ' enter ' an orbit without such a three-body type interaction , there needs to be a mechanism of dissipating its initial ( non-negative ) energy . in practice , how this would happen depends on the context . for stellar systems , it could happen from tidal dissipation ; for satellites , i guess it could be atmospheric drag ; or for a spaceship it could be its engines .
0 ) your guess about the hilbert space is on the right track , but not correct . the space of gauge invariant operators is much too big ; you have to mod out by the equations of motion in an appropriate sense . ( think about the case of 1d quantum mechanics , where the gauge symmetry is trivial . the hilbert space is $l^2 ( \mathbb{r} ) $ , generated by time zero position observables , not $l^2 ( \mathbb{r}^{\mathbb{r}} ) $ , which is what you had get if you used all observables . ) 1 ) in pure yang-mills , the wilson loops are a complete set of observables . ( credit , iirc , goes to migdal . ) 2 ) you can recover the local observables by taking the limit of small loops . 3 ) it is not always true that observables are gauge invariant polynomials in the fields . wilson loops are not polynomials ! 4 ) if there is enough matter fields , the gauge theory may not confine , in which case , baryons and mesons are not the only observables . how many matter fields are required depends on the gauge group . 5 ) not sure what you are asking here . why do you think they should be baryons or mesons ?
you want to find the eigenfrequences of this system . first , note the existence of zero mode : $$ q_l=vt+\phi_l , $$ it is the ' equilibrium ' rotating around with arbitrary velocity . next , we have the equations $$ \ddot{q}_l=-\omega^2 ( 2q_l-q_{l-1}-q_{l+1} ) $$ and the periodicity condition $q_{l+n}=q_l$ . let us use the ansatz for the eigenvectors $$ q_l=\re a\exp ( ikl-i\omega t ) . $$ substituition reads $$ \omega^2=\omega^2 ( 2-e^{-ik}-e^{ik} ) =2\omega^2 ( 1-\cos k ) , $$ while the periodicity condition is $nk=2\pi m , \ , m\in\mathbb{z}$ . let us restrict $k$ to $ ( 0,2\pi ) $ , in order to exclude double-counting the eigenvectors and the zero mode . then $m=1 , \dots , n-1$ , and the eigenfrequency reads : $$ \omega^2=2\omega^2 ( 1-\cos \frac{2\pi m}{n} ) , \ , m\in\{1\dots n-1\} . $$ thus we have $n-1$ vectors of the form $q_l=\re a\exp ( ikl-i\omega t ) $ with the above eigenfrequency , and one zero mode given by $q_l=vt+l\delta$ , $\delta$ being the equilibrium distance , with zero frequency . so , the full spectrum reads as $$ \omega^2=2\omega^2 ( 1-\cos \frac{2\pi m}{n} ) , \ , m\in\{0\dots n-1\} . $$ ( note that this is the full spectrum since we have found all $n$ eigenvectors ) . edit : note that while $n-m$ corresponds to the same eigenvalue as $m$ , we have two different eigenvectors for each eigenvalue , because $a$ can be complex . e.g. we can take $q_l=\cos ( kl-\omega t ) $ and $q_l=\sin ( kl-\omega t ) $ , $k=\frac{2\pi m}{n}$ .
you would like to state the källen-lehman representation through the fermonic field commutator , that is $\{\bar\psi ( x ) , \psi ( y ) \}$ but , as far as the proof goes , you can only use this in a time-ordered way . i mean , you should use $\theta ( x_0-y_0 ) \psi ( x ) \bar\psi ( y ) -\theta ( y_0-x_0 ) \bar\psi ( y ) \psi ( x ) $ . this will grant the due appearance of the klein-gordon propagator in the final formula . when you will do that , the standard view , seen through states and bound states , holds true . about adiabatic continuity , you will always get a weighted sum of free propagators with all the spectrum of the theory , free and bound states , that is in agreement with such a hypothesis . the effect of the interaction will be coded in the weights and the spectrum itself . finally , positivity of the spectral function can only be granted , and a proof holds , when the states behave in a proper way . this is not exactly the case for a gauge theory and some of the difficulties arising in proving the existence of a mass gap can be tracked back to a problem like this . e.g. see this book by franco strocchi . further clarification : when you insert the operator generating a translation in the bosonic field , the same is somewhat different for the spinorial case . you will get $$u^\dagger\psi u=s\psi $$ with $s$ the one i think you studied in the proof of lorentz invariance of the dirac equation . now , you are almost done . this will give for you matrix element $$\langle 0|\psi ( 0 ) |\alpha\rangle=\sqrt{z}u ( \alpha ) $$ being $\alpha$ running both on momenta and spin . you are practically done as , using the known relations $\sum_s u\bar u= \gamma\cdot p+m$ and $\sum_s v\bar v= \gamma\cdot p-m$ , you will get back källen-lehman representation .
the ordinary bessel functions are perfectly well defined for complex arguments . for example , here is a plot of $\re [ j_2 ( x + i y ) ] $: the difference between the ordinary and modified bessel functions is that they satisfy different equations : $$ z^2 y'' + z y ' + ( z^2 - n^2 ) y = 0 , $$ for the ordinary bessel functions and $$ z^2 y'' + z y ' - ( z^2 + n^2 ) y = 0 , $$ for the modified bessel functions . note that there is a relationship between them : $$ j_{\nu } ( z ) =\frac{z^{\nu } i_{\nu } ( i z ) }{ ( i z ) ^{\nu }} $$ with similar identities going the other way . it is all very similar to the relationship between the trig functions $\sin ( z ) , \cos ( z ) $ with the hyperbolic functions $\sinh ( z ) , \cosh ( z ) $ .
yes , a color is a point in lms space . at least , that is the signal that the eye tells to the brain starting signal which is post-processed by neurons in the eye and brain . for example , the brain does some inferences on what the lighting condition is etc . , so that an object looks like it has one color even if half of it is in sunlight and half of it is in shadow . there is a linear transformation between lms and the xyy of the cie color-space . it is some 3x3 matrix . the horseshoe / sharkfin / shoe-sole is a slice of xyy space at constant y surface in xyy space projected onto the xy plane . below left is the slice at y ~ 1 , below right is the slide at y ~ 0.5 below left is the surface showing the highest possible y for each xy , below right is the surface showing a somewhat lower y at each xy . the slice at y = 0 would be a completely black horseshoe . if there was a frequency of light that only stimulated the l cells , and a different frequency that only stimulated m , and a different frequency that only stimulated s , then it would be possible to create a kind of rgb space that fills the entire visual gamut . unfortunately , that is not the case . if you are making a projector with three color lights , the rules are ( 1 ) each of those three colors has to be composed of a non-negative amount of each frequency , and ( 2 ) each pixel on the screen has to have a non-negative amount of each color . these two requirements are mathematically incompatible with recreating every possible sml color stimulus . ( the lack of negative numbers means that your linear algebra intuition does not apply here . ) the " old painters belief " is certainly not true , and i doubt that any painters really believe that . ask a painter to grab red , yellow , and blue tubes of paint , and then mix them to get black , and then mix them a different way to get white . they will surely acknowledge that it is impossible . basically , i agree with what you said .
heuristically i would say yes , it always must be curved if you are starting from rest . think of it this way ; suppose the graph is not curved , that is , it is a straight line , then either the straight line is horizontal , or not horizontal . if the graph ( straight line ) is horizontal , then the distance is not changing and you remain at rest . if the graph is a straight line and not horizontal then you would be moving at a constant velocity , but you started from rest , which is a contradiction . i will admit this argument does not really account for pathological examples . maybe one could construct some graph , which in some sort of piece-wise consideration , is made up of straight lines . but i feel like so long as the graph is smooth , then the above argument holds up . again , at least with the above logic ( i will admit , quite hand-wavey ) , it seems like there should be some curvature . regarding your example , specifically , if the car has constant velocity then the distance/time graph must be a straight line .
i would guess that the article is referring to solitons . i am not sure if every non-linear system gives soliton solutions , but many do . the wikipedia article i have linked gives lots of examples of classical solitons , but i am not sure to what extent ( if at all ) they are important in the standard model . perhaps one of the qft specialists hereabouts could comment .
work done by the electric force is positive and not negative . the change in the potential energy of the electric field $\delta u$ is equal to the negative of the work done $w$ by the electric force . you have $$\delta u = -w$$ $$u_1=u_0-w&lt ; u_0$$ $$\therefore w&gt ; 0$$ and as $$w=\int f\cdot dl&gt ; 0$$ , we can say that $f$ acts in the same direction as $dl$ , i.e. it is attractive .
$\partial_i \frac{f}{1-f^2}=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1-f^2}{f} ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1}{f}-f ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( \partial_i ( \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( - \frac{1}{f}\partial_i f \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $= ( \frac{1}{1-f^2} ) \partial_i f ( \frac{1}{1-f^2} ) + ( \frac{f}{1-f^2} ) \partial_i f ( \frac{f}{1-f^2} ) $
a hydrogen atom can be made to show chaotic behaviour if it is excited to very near it is ionisation energy . the maths is somewhat beyond me , but this paper discusses calculations of the hydrogen atom showing the onset of chaotic behaviour . you can find more by googling for " rydberg atom " combined with " chaos " or " chaotic behaviour " .
there seem to be a lot of human body mechanical models , such as this one : as for applications , i have heard that sub-audio frequency vibrations have been considered as nonlethal weapons for riot control .
grassman $d\theta$ has opposite mass dimension to $\theta$ , which is why the notation is not 100% optimal , it confuses on this issue . but if you know how to evaluate the integral , that it goes like the derivative , then you know how change of scale works , and it is the opposite of normal change of scale : $$\int d ( k\theta ) f ( k\theta ) = {1\over k} \int d\theta f ( \theta ) $$ and this is why the volume determinant for the integration ends up being the reciprocal of the bose case .
some of the bianchi metrics have off-diagonal terms . examples are the taub metric and the mixmaster metric . for more information on the bianchi models , see collins and hawking ( 1973 ) . another example is the gödel metric . you may also find this paper useful : http://arxiv.org/abs/1003.0043
is it possible to produce gamma radiaton using radio emitter ? unlikely . a ' radio emitter ' consists of , at least , some type of antenna and a transmitter to drive that antenna . the size of the antenna is related to the wavelength of the transmitted radio wave , e.g. , half-wave dipole , quarter-wave monopole . but the wavelength of gamma rays is less than the diameter of an atom .
i will give you the derivation from my book which includes a nice way to see how the delta functions arise : . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . we can derive the potential field $\vec{a}$ and the electromagnetic fields $\vec{e}$ and $\vec{b}$ of a vector point dipole and an axial point dipole in the same way as we derive these in the case of a the point monopole . we start with a static point charge $\delta ( \vec{r} ) $ and derive the dipole charge/current densities with the help of differential operators . we apply the same differential operators for $\vec{a}$ , $\vec{e}$ and $\vec{b}$ to obtain the dipole fields from the monopole fields . first we recall the fields of the monopole . the point charge obtains ( over time ) a potential field given by . $ \mbox{field}\big\{\ , \delta ( \vec{r} ) \ , \big\} ~~=~~ \frac{1}{4\pi r} $ the reversed operator which derives the source from the field is just the laplacian operator . $ -\nabla^2\big\{\ , \frac{1}{4\pi r}\ , \big\} ~~=~~ \delta ( \vec{r} ) $ integrating the delta function over space shows equal contributions from the three spatial components . $ \int \delta ( \vec{r} ) ~d\vec{r} ~=~ -\int\left [ \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}\right ] \frac{d\vec{r}}{4\pi r} ~=~ \frac13+\frac13+\frac13 ~=~ 1 $ we can define vector dipole and axial dipole sources by using differential operators on the monopole $\delta ( \vec{r} ) $ and derive their potential and electromagnetic fields . the vector dipole is obtained by differentiating the monopole along the direction that we want the dipole to have . the result is a combination of a positive and a negative delta function . the axial dipole is defined by the curl of the monopole so that it gives a circular point current in the direction of the dipole . charge/current densities , potentials and fields of dipoles : $ \begin{array}{|lcll|} \hline and and and \\ j^o and = and ~~~\mbox{div}\ , \left ( ~\vec{\mu} \ , \delta ( \vec{r} ) ~\right ) and \mbox{vector dipole charge density} \\ \vec{j} and = and ~~~\mbox{curl}\left ( ~\vec{\mu}\ , \delta ( \vec{r} ) ~\right ) and \mbox{axial dipole current density} \\ and and and \\ a^o and = and ~~~\mbox{div}\ , \left ( ~\vec{\mu} \ , \frac{1}{4\pi r}\ , \right ) and \mbox{vector dipole electric potential} \\ \vec{a} and = and ~~~\mbox{curl}\left ( ~\vec{\mu} \ , \frac{1}{4\pi r}\ , \right ) and \mbox{axial dipole magnetic potential} \\ and and and \\ \mathsf{e} and = and -~\mbox{grad}\left ( \mbox{div}\ , \left ( ~\vec{\mu} \ , \frac{1}{4\pi r}\ , \right ) \right ) and \mbox{vector dipole electric field} \\ \mathsf{b} and = and +~\ , \mbox{curl}\left ( \mbox{curl} \left ( ~\vec{\mu}\ , \frac{1}{4\pi r}\ , \right ) \right ) and \mbox{axial dipole magnetic field} \\ and and and \\ \hline \end{array} $ the potential fields are obtained by applying the same differential operators on the field $1/4\pi r$ of the monopole rather than on the charge distribution . for a dipole moment in the $z$-direction we obtain in si-units . potential fields of the electric and magnetic dipoles : $\phi ~=~ \frac{z}{4\pi\epsilon_o r^3}\ , \mu~~~~~~~~ \vec{a} ~=~ \big ( -y ~ , ~ x~ ~ , ~ 0 \big ) ~ \frac{\mu_o}{4\pi r^3}\ , \mu $ the expressions for the electromagnetic fields do implicitly contain delta functions at the center with the right magnitude . these delta functions are easily lost if the derivation is not careful enough . the e and b fields are related to each other by the standard vector identity . $\mbox{curl} ( \mbox{curl}\vec{x} ) ~=~ \mbox{grad} ( \mbox{div}\vec{x} ) -\nabla^2\vec{x} $ we have seen that the last term ( the laplacian ) yields $\delta ( \vec{r} ) $ . the two therefor differ only at the center by a delta function in the $\vec{\mu}$ direction . we can see this explicitly if we align the dipoles with the z-axis , so that $\vec{\mu}$ has only a z-component , and write out the fields . the only difference is in the z-components . the total difference between the two is the laplacian and thus $\delta ( \vec{r} ) $ . the vector dipole gets -1/3 while the axial dipole gets +2/3 . $\begin{aligned} \mathsf{e} and = and \frac{1}{\epsilon_o}\bigg [ ~ \mathbf{\hat{x}}\ , \frac{\partial}{\partial x}\frac{\partial}{\partial z} ~+~ \mathbf{\hat{y}}\ , \frac{\partial}{\partial y}\frac{\partial}{\partial z} ~+~~~ \mathbf{\hat{z}}\ , \frac{\partial^2}{\partial z^2} ~~~~~~~~~~~~~ and \bigg ] ~ \frac{\mu}{4\pi r}\\ \\ \mathsf{b} and = and \mu_o\bigg [ ~ \mathbf{\hat{x}}\ , \frac{\partial}{\partial x}\frac{\partial}{\partial z} ~+~ \mathbf{\hat{y}}\ , \frac{\partial}{\partial y}\frac{\partial}{\partial z} ~-~ \mathbf{\hat{z}}\left ( \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}\right ) ~ and \bigg ] ~ \frac{\mu}{4\pi r}\\ \end{aligned} $ which we can see from the third equation with the three 1/3 parts . in the general case , with arbitrary dipole direction $\vec{\mu}$ we get for the electromagnetic fields in vector form . electromagnetic dipole fields : $ \begin{aligned} \mathsf{e} and = and \frac{1}{\epsilon_o}\left ( ~\frac{3\left ( \vec{\mu}\cdot\hat{r}\right ) \hat{r}-\vec{\mu}}{4\pi r^3} ~-~\frac13\vec{\mu}\ , \delta ( \vec{r} ) ~\right ) \\ \\ \mathsf{b} and = and \mu_o\left ( ~\frac{3\left ( \vec{\mu}\cdot\hat{r}\right ) \hat{r}-\vec{\mu}}{4\pi r^3} ~+~\frac23\vec{\mu}\ , \delta ( \vec{r} ) ~\right ) \end{aligned} $ alternatively we can look at the fields explicitly expressed in the individual $x$ , $y$ and $z$ components which gives . ( with the dipole moment in the $z$-direction ) $\begin{aligned} \mathsf{e} and = and \frac{\mu}{4\pi\epsilon_o}\bigg [ ~ \mathbf{\hat{x}}\ , \frac{3xz}{r^5} ~+~ \mathbf{\hat{y}}\ , \frac{3yz}{r^5} ~+~~~ \mathbf{\hat{z}}\ , \left ( \frac{3zz}{r^5}-\frac{1}{r^3} - \frac{4\pi}{3}\delta ( r ) \right ) ~ and \bigg ] \\ \\ \mathsf{b} and = and \frac{\mu_o\mu}{~4\pi~}\bigg [ ~ \mathbf{\hat{x}}\ , \frac{3xz}{r^5} ~+~ \mathbf{\hat{y}}\ , \frac{3yz}{r^5} ~+~~~ \mathbf{\hat{z}}\ , \left ( \frac{3zz}{r^5}-\frac{1}{r^3} + \frac{8\pi}{3}\delta ( r ) \right ) ~ and \bigg ] \\ \end{aligned} $ in general the fields decrease with the third order of $r$ compared to $1/r^2$ for the charge which makes it possible not many effects at larger scale . at the other hand , when we go to smaller scales , the magnetic dipole fields become more powerful relative to the charge .
it is a little tricky giving a formal answer to this , but here is a sketch : . the electromagnetic potential of the kerr-newman hole is given by : $$a_{a}dx^{a}=-\frac{qr}{r^{2}+a^{2}\cos^{2}\theta}\left ( dt-a \sin^{2}\theta d\phi\right ) $$ this field will acquire a magnetic field from the fact that $\frac{\partial a_{\phi}}{\partial r}$ and $\frac{\partial a_{\phi}}{\partial \theta}$ are both nonzero . the problem is that the magnetic field , when re-phrased in terms of vectors and not one-forms , will fall off as $\frac{1}{r^{3}}$ . at that point , if we are keeping terms that fall off that quickly , then we need to have a discussion about that asymptotic form of the metric you imply above , because there are terms in the metric that we need to keep , arising from frame-dragging effects of the black hole . if you want , i can go into more detail . edit : ok , so once we have the vector potential , we can calculate the magnetic field according to the rule : $b^{i}=\frac{1}{\sqrt{\left|g\right|}}\epsilon^{ijk}\left ( \frac{\partial a_{j}}{dx^{k}}-\frac{\partial a_{k}}{dx^{j}}\right ) $ , where $\epsilon^{r\theta\phi}=1$ , $\epsilon^{ijk}=-\epsilon^{jik}=-\epsilon^{ikj}$ and $\epsilon^{ijk}=0$ if $i=j$ , $i=k$ or $j=k$ . so , now , we just plug in the above expression for the spatial components of $a_{a}$ , the metric tensor , and turn the crank . after doing this , and taking the limit that $r$ is larger than everything else , we find that $${\vec b}=\frac{2qa\cos\theta}{r^{3}}{\hat e}_{r} + \frac{qa \sin \theta}{r^{3}}{\hat e}_{\theta}$$ sensibly , this is zero if either $q=0$ or $a=0$ . and i will once again assert that there are $\frac{1}{r^{3}}$ corrections to the gravitational force that must be taken into account in your high $r$ limit if you are going to keep this magnetic field .
it is called the landau-zener formula : wittig , curt . the landau-zener formula . j . phys . chem . b 109 ( 2005 ) pp . 8428-8430 . doi:10.1021/jp040627u clarence zener found it correctly , landau had a factor of $2\pi$ error , they were independent . there are two papers by landau on this : landau , lev d . a theory of energy transfer on collisions . phys . z . sowjet . ( physikalische zeitschrift der sowjetunion ) 1 88 ( 1932 ) and landau , lev d . a theory of energy transfer ii . physik . z . sowjet . 2 46 ( 1932 ) . they can both be found in landau 's collected papers , which may be easier to get hold of than physik z sowjet . see also [ 1 ] in the first paper above for the zener reference .
daylight does not contain equal amounts of all colours . the spectrum of daylight peaks around yellow/green , and the amount of energy falls off quite sharply as you move to the blue end of the spectrum . that means even if the width of the blue filter is the same as a green filter , you will measure less light coming through the blue filter . if you are using artificial light the effect will be even more pronounced as artificial light generally has a lower colour temperature than sunlight . incidentally , the response of your solar cell will also vary with the colour of the light , and this may well also be affecting your results . i can not lay my hands on a typical frequency response for a silicon solar cell , but i think the response falls off at the blue end of the spectrum .
you ask if the region where the field is defined has holes in it , what happens ? well , in that case you can define the vector potential on simply-connected sub-regions $r_i$ whose intersection is the whole non-simply connected region $r$ and such that they differ by only by a gauge transformation on the regions of overlap . this is a physically well-motivated thing to do , because it means that up to gauge transformation , the vector potential can be defined on $r$ . here 's a simple example . let $\ell=\{ ( x , y , z ) \ , |\ , x=0 , y=0\}$ denote the $z$-axis , then the region $r=\mathbb r^2\setminus\ell$ is not simply connected . to see this , simply consider a closed loop enclosing the axis ; there is no way to continuously shrink it down to a point while staying in $r$ . because of this , the there is no $\mathbf a$ defined on all of $r$ . however , let $\ell_+$ denote the positive $z$-axis , and let $\ell_-$ denote the negative $z$-axis , then the regions $r_- = \mathbb r^3\setminus \ell_+$ and $r_+ = \mathbb r^3\setminus \ell_-$ have the property that they are each simply connected and $r = r_+\cap r_-$ . moreover , we can define a vector potential $\mathbf a_+$ on $r_+$ and $\mathbf a_-$ on $r_-$ such that there exists a scalar function $\lambda$ for which \begin{align} \mathbf a_+ ( \mathbf x ) - \mathbf a_- ( \mathbf x ) = \nabla\lambda ( \mathbf x ) , \qquad \text{for all $\mathbf x\in r$} \end{align} in fact , here are the explicit expressions in spherical coordinates $ ( r , \theta , \phi ) $: \begin{align} \mathbf a_{\pm} and = -g\frac{\cos\theta\mp 1}{r\sin\theta}\hat{\boldsymbol \phi} \end{align} i will leave it to you to determine $\lambda$ ; it is a fun exercise . what are the physical consequences of that ? well , in the context of quantum mechanics , these sorts of topological issues are physically relevant ( i am unsure if there are examples in which they are relevant at the classical level , but i do not think so ) . i will not go into the details here ( unless perhaps there is some demand ) , but the very vector potentials i wrote down in the example above come up when discussing magnetic monopoles and the quantization of electric charge ( see dirac quantization ) . these topological issues also become significant in discussing the famous aharonov-bohm effect .
initially , due to acceleration of the magnet the rate of changing of linked flux ( $-d\phi /dt$ is itself changing and hence the induced emf , and the force due to it , is increasing ( changing ) . ultimately at a sufficiently high velocity , the force due to induced current will be equal to the weight . when $f=mg$ , then the net force and hence acceleration is zero , but the flux is still changing if it has a non zero velocity . after this , since velocity is constant and no acceleration exists , the induced emf will be constant because $-d\phi /dt$ is constant due to constant velocity . this means the forces will no longer vary and the magnet will continue with same speed . your analysis assumed the force due to induced emf to still change after $f=mg$ but that is incorrect as constant velocity will cause constant induced force , in the perfectly long tube 's case .
the answer is $d/2$ . since this is such a nice answer , there might be a really simple way to obtain it that does not involve any differential equations . i do not know one , though . here 's my way of solving it . let 's adjust the problem so that the turtle at the origin swims in the $-y$ direction , and the other turtle swims straight towards him ( i did this because i thought it would make the signs easier for me ) . now , let 's switch the rest frame so that there is a current flowing in the $+y$ direction , the turtle at the origin sits on a little island , and the other turtle always swims towards the origin , but is pulled off-course by the current . we can now use geometry to figure out which direction the turtle moves at any point in the plane . at point $ ( x , y ) $ , the turtle swims with velocity $$\left ( \frac{-x}{\sqrt{x^2+y^2}} , \frac{-y}{\sqrt{x^2+y^2}} \right ) . $$ . taking the current into account , the turtle 's velocity is $$\left ( \frac{-x}{\sqrt{x^2+y^2}} , 1-\frac{y}{\sqrt{x^2+y^2}} \right ) . $$ this gives us the differential equation for the turtle 's path $$\frac{dy}{dx} = \frac{y - \sqrt{x^2+y^2}}{x} . $$ maple gives us the solution to this : $$ y = \frac{1}{2} \left ( c-\frac{x^2}{c} \right ) , $$ which can easily be checked to be a solution for $c&gt ; 0$ ( although i would like to see how to solve it without a computer algebra system ) . now , we know that when $y=0$ , $x=d$ . this gives $c=d$ . when $x=0$ , we have $y=d/2$ .
with no resistance , the full voltage is applied to the fan , and you get mechanical work done , at whatever efficiency the fan itself is capable of . never minding the fan itself , so far as the electrical aspect goes , you could say it is 100% efficient . with resistance in the system , for example about equal to the resistance of the fan , you have less current flowing through the system . half as much . same voltage . so the system is using half the power . the fan , now one part of a voltage divider circuit , is getting half the voltage , and getting half the current . it runs slower , doing less , doing about 1/4 the work . 1/4 of the fan action divided by 1/2 power going into the system means that the system is 50% efficient . imagine a lot of resistance in the system , megohms . you will have only a trickle of current , say a microamp . the fan barely moves . the system will be using very little power , but most of that little power is just making the resistor hot . or rather , a small fraction of a degree warmer than ambient temperature . the system efficiency is close to zero .
$-i\hbar\vec{\nabla}|\psi\rangle$ is not a valid notation . the nabla operator is defined in three-dimensional euclidean space , not the in the hilbert space of quantum states . when the author says $\hat{\boldsymbol{p}}=-i\hbar\vec{\nabla}$ he does not mean the momentum operator defined in the state space , but the space of wavefunctions . then $\hat{\boldsymbol{p}}\psi ( \boldsymbol{r} ) =-i\hbar\vec{\nabla}\psi ( \boldsymbol{r} ) $ .
the answer to your question is very wide and include many phenomena . there is no single mechanism that converts absorbed energy into heat . i will give you a general overview on this topic . heat is transferred by radiation which is in general an electromagnetic wave ( not light only ) . for example , ir radiation alone provides 49% of the heat provided to earth from sun radiation . have a look at heat section of this page . in electromagnetic spectrum , the wavelength varies significantly , which means that materials respond differently to different ranges of wavelengths . for example ( listing from long wavelength to short wavelength ) : radio waves : the wavelength here is very long such that the materials constituents ( atoms and molecules ) do not sense the waves . thus most solids are transparent to radio waves , which means there is no wave-material interaction . there is no heat generated . the transparency is clear as you can get cellphone reception in your home as the wave simply travel through walls and our bodies . microwaves : the wavelength here is shorter , the photons energy in this level are comparable to the energy levels of the molecular rotational levels , which means that a molecule could absorb a microwave photon and start to rotate . this rotation is a form of kinetic energy of molecules which is translated as heat when you look at the whole ensemble of molecules . this mechanism is the mechanism known for microwave heating in your kitchen . ir : the photon energy in this range is comparable to molecular vibrational levels . so if a molecule absorbed an ir photon it will vibrate , which is translated into heat when you look at the whole ensemble of molecules . visible light : things get complicated here , the photon energy here is comparable to electronic levels within an atom . there is no simple direct explanation of how photon energy is transformed into heat . one can argue for visible light that an electron can absorb a photon and radiate it without increasing kinetic energy of atoms ( no increase oh heat ) . that is true but that is a very simplistic picture of what happens in reality . first because the visible light is continuous spectrum , while the photons that can be absorbed by electrons are discrete . so there are many visible light photons with wavelengths that electrons can not absorb , which could be absorbed by ions or affect polar bonds somehow if they existed in the material . second , some materials like solids ( where heat by radiation is efficient ) have energy bands rather than the simplistic electron levels concept . so the reaction of materials to visible light in this case is complex . but one of the mechanism of transforming photon energy to heat that i can think of here is the absorption of visible light by ion lattice in some crystals . uv : the photons in this range have enough energy to ionize the atoms they hit ( by liberating an electron from its orbital ) . again , there is no simple direct explanation of how photon energy is transformed into heat . one possible route is the inelastic scattering of electrons with materials atoms . for example , have a look at page 5 of this report where it is mentioned that inelastic electron scattering can induce phonons ( lattice vibration , which is a form of kinetic energy of atoms ) x rays : the photon energy in this range is much larger than the energy required to move electron from one level to other . x rays photons can ionize an atom and become lower energy photon through compton scattering . there is no simple direct explanation of how photon energy is transformed into heat . briefly , heating by radiation in general can not be attributed to a simple explanation . the mechanisms through which photons energy is transformed into heat depend on the energy of the photon and the properties of the material . the simplistic picture of a photon being absorbed by electron is narrow to explain conversion to heat because it only describes a single electron in a free atom . that is a narrow view of material properties . have a look here and the nice figure of non-ionizing radiation section of this page hopefully that was helpful
i had not realized this had been happening , very interesting . hand waving : when the intensity of the laser is high enough electrons can be accelerated to relativistic energies and create e+e- pairs interacting with the collective electric and magnetic fields of the laser pulse . here is a link with some semiclassical calculations . electrons have to be supplied from the atoms of a gas on which the laser is shining . the interaction is a collective field one with the electron , in these calculations , not with the nucleus . and another using quantum electrodynamics : production of electron-positron pairs from vacuum in the combined electromagnetic fields of a high-intensity laser pulse and an atomic nucleus is studied within the framework of laser-dressed quantum electrodynamics . the focus lies on the influence exerted by a finite laser pulse length on the energy spectra of created electrons and positrons , which is examined in a broad range of field frequencies and intensities . the results for an isolated short laser pulse are also compared with corresponding calculations for an infinite train of laser pulses . it is shown that the laser pulse length and its carrier-envelope phase have a substantial effect on the pair creation process , leading to both quantitative and qualitative differences in the particle spectra . it is behind a pay wall so i cannot see what " atomic nucleus " means in this case . certainly the diagrams will be different than the gamma-nucleus diagrams .
what you are describing is the difference between brittle and ductile behaviour . most materials show both properties under the appropriate conditions . for example glass becomes ductile as the temperature rises towards the glass transition , while metals become ductile at low temperatures . a brittle material may be superficially unaffected by a blow , but it is likely that there will be some effect at the atomic scales . if you study the surface with an electron microscope you will probably find the blow has caused small defects on the surface , and these could nucleate a crack under stress . alternatively the blow could have caused very small cracks , that again could lead to failure under stress . to what extent this happens depends on the force of the blow . in metals repeated small deformations can lead to the well known phenomenon of metal fatigue . analogous processes do happen with brittle materials , though i have to confess this is outside my area of expertise . a quick google for something like " fatigue in brittle materials " will find lots of related articles like this one .
there are a number of mathematical imprecisions in your question and your answer . some advice : you will be less confused if you take more care to avoid sloppy language . first , the term spinor either refers to the fundamental representation of $su ( 2 ) $ or one of the several spinor representations of the lorentz group . this is an abuse of language , but not a bad one . a particularly fussy point : what you have described in your first paragraph is a spinor field , i.e. , a function on minkowski space which takes values in the vector space of spinors . now to your main question , with maximal pedantry : let $l$ denote the connected component of the identity of the lorentz group $so ( 3,1 ) $ , aka the proper orthochronous subgroup . projective representations of $l$ are representations of its universal cover , the spin group $spin ( 3,1 ) $ . this group has two different irreducible representations on complex vector spaces of dimension 2 , conventionally known as the left- and right- handed weyl representations . this is best understood as a consequence of some general representation theory machinery . the finite-dimensional irreps of $spin ( 3,1 ) $ on complex vector spaces are in one-to-one correspondence with the f.d. complex irreps of the complexification $\mathfrak{l}_{\mathbb{c}} = \mathfrak{spin} ( 3,1 ) \otimes \mathbb{c}$ of the lie algebra $\mathfrak{spin} ( 3,1 ) $ of $spin ( 3,1 ) $ . this lie algebra $\mathfrak{l}_{\mathbb{c}}$ is isomorphic to the complexification $\mathfrak{k} \otimes \mathbb{c}$ of the lie algebra $\mathfrak{k} = \mathfrak{su} ( 2 ) \oplus \mathfrak{su} ( 2 ) $ . here $\mathfrak{su} ( 2 ) $ is the lie algebra of the real group $su ( 2 ) $ ; it is a real vector space with a bracket . i am being a bit fussy about the fact that $\mathfrak{su} ( 2 ) $ is a real vector space , because i want to make the following point : if someone gives you generators $j_i$ ( $i=1,2,3$ ) for a representation of $\mathfrak{su} ( 2 ) $ , you can construct a representation of the compact group $su ( 2 ) $ by taking real linear combinations and exponentiating . but if they give you two sets of generators $a_i$ and $b_i$ , then you by taking certain linear combinations with complex coefficients and exponentiating , you get a representation of $spin ( 3,1 ) $ , aka , a projective representation of $l$ . if memory serves , the 6 generators are $a_i + b_i$ ( rotations ) and $-i ( a_i - b_i ) $ ( boosts ) . see weinberg volume i , ch 5.6 for details . the upshot of all this is that complex projective irreps of $l$ are labelled by pairs of half-integers $ ( a , b ) \in \frac{1}{2}\mathbb{z} \times \frac{1}{2}\mathbb{z}$ . the compex dimension of the representation labelled by $a$ , $b$ is $ ( 2a + 1 ) ( 2b+1 ) $ . the left-handed weyl-representation is $ ( 1/2,0 ) $ . the right-handed weyl representation is $ ( 0,1/2 ) $ . the dirac representation is $ ( 1/2,0 ) \oplus ( 0,1/2 ) $ . the defining vector representation of $l$ is $ ( 1/2,1/2 ) $ . the dirac representation is on a complex vector space , but it has a subrepresentation which is real , the majorana representation . the majorana representation is a real irrep , but in 4d it is not a subrepresentation of either of the weyl representations . this whole story generalizes beautifully to higher and lower dimensions . see appendix b of vol 2 of polchinski . figuring out how to extend these representations to full lorentz group ( by adding parity and time reversal ) is left as an exercise for the reader . one caution however : parity reversal will interchange the weyl representations . sorry for the long rant , but it raises my hackles when people use notation that implies that some vector spaces are spheres . ( if it is any consolation , i know mathematicians who get very excited about the difference between a representation $\rho : g \to aut ( v ) $ and the " module " $v$ on which the group acts . )
the solution is to realize that the steady-state solution of a harmonically driven system must also oscillate harmonically . ( as regards your solution , this means that spectrally the $c_i ( \omega ) $ are delta functions , which resolves your contradiction . ) thus one usually begins by postulating the oscillatory ansatz $$c_a ( t ) =c_a e^{-i\omega_a t} , \ , \ , c_b ( t ) =c_b e^{-i\omega_b t} , $$ where the $c_a$ and $c_b$ are now constants . as an ansatz this is harmless and if it turns out to not be a solution you can drop it ( but as it happens it will ) . your schrödinger equation thus reads $$ \begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v e^{i t \omega _d} \\ \frac{1}{2} v e^{-i t \omega _d} and \frac{\omega _0}{2} \end{pmatrix} \begin{pmatrix} c_a ( t ) \\ c_b ( t ) \end{pmatrix} = i\frac d{dt} \begin{pmatrix} c_a ( t ) \\ c_b ( t ) \end{pmatrix} $$ so $$ \begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v e^{i t \omega _d} \\ \frac{1}{2} v e^{-i t \omega _d} and \frac{\omega _0}{2} \end{pmatrix} \begin{pmatrix} c_a e^{-i\omega_a t}\\ c_b e^{-i\omega_b t} \end{pmatrix} = \begin{pmatrix} \omega_a c_a e^{-i\omega_a t} \\ \omega_b c_b e^{-i\omega_b t} \end{pmatrix} $$ or $$ \left\{ \begin{array}{ccc} -\frac{\omega _0}{2} c_a e^{-i\omega_a t} and +\frac{1}{2} v e^{i t \omega _d}c_b e^{-i\omega_b t} and = \omega_a c_a e^{-i\omega_a t} , \\ \frac{1}{2} v e^{-i t \omega _d} c_a e^{-i\omega_a t} and + \frac{\omega _0}{2} c_b e^{-i\omega_b t} and = \omega_b c_b e^{-i\omega_b t} . \end{array} \right . $$ this needs you to set $\omega_a+\omega_d=\omega_b$ , after which you can eliminate the time dependence . that leaves you with the simple linear system $$ \left\{ \begin{array}{ccc} -\frac{\omega _0}{2} c_a +\frac{1}{2} v c_b = \omega_a c_a , \\ \phantom+ \frac{1}{2} v c_a + \frac{\omega _0}{2} c_b = ( \omega_a+\omega_d ) c_b , \end{array} \right . $$ which is an eigenvalue system for the hamiltonian $h=\begin{pmatrix} -\frac{\omega _0}{2} and \frac{1}{2} v \\ \frac{1}{2} v and \frac{\omega _0}{2} -\omega_d \end{pmatrix}$ . since you tagged this as homework , i will leave the calculation here , as i am sure you are better off calculating eigenvectors and eigenvalues on your own .
the question was motivated because i have a suspicion that the electron does not participate to the source of a gravitational field , and eventually not even responds to such a field . in 1908 milikan measured the charge on a single electron . the charge-to-electron mass ratio $q/m_{e}$ was calculated by thomson in 1897 using the angular momentum and the deflection due to a perpendicular magnetic field . i think that this value reflects the inertial mass . i found this old ( 1964 ) doc gravitational and resonance experiments on very low-energy free electrons by fairbank , and this entry experimental comparison of the gravitational force on freely falling electrons and metallic electrons by wittborn and fairbank , 1967 , with the abstract : a free-fall technique has been used to measure the net vertical component of force on electrons in a vacuum enclosed by a copper tube . this force was shown to be less than 0.09mg , where m is the inertial mass of the electron and g is 980 cm/sec2 . this supports the contention that gravity induces an electric field outside a metal surface , of magnitude and direction such that the gravitational force on electrons is cancelled . it seems that the issue remains unsettled . - docs of 1992 and 2007 - ( tests of the weak equivalence principle for charged particles in space ) fairbank , as everyone else back then , and even now , believed that the electron must participate in gravity , contrary to my suspicion , preferring to imagine the existence of an imaginary induced electric force , possibly because in the 50s and 60s existed some hype about possible effects relating electricity and gravity . experimentation is central to advancement of physics , and the solution of this unsettled issue may prove important in the genesis of a sucessful theory encompassing both the so called particles and the gravitation field . i can understand that performing the test of fairbank under microgravity conditions can discriminate if the electron responds to the gravity field . to test if the electron has an active gravitational mass a possible test that may work ( 'i do not know the tecnological limits . . . ' ) could be done using a collimated beam of slow neutrons with a path traversing between the plates of a very large high-voltage capacitor . the possible deflection , or not , of the beam may prove very interesting .
there are earlier references - i have a copy of " the fate of the false vacuum " by sid coleman in a collection of reprints . i do not know if this is the earliest reference though .
per wikipedia , the electromagnetic tensor $f^{\mu \nu}$ contributes to the stress energy tensor $t^{\mu \nu}$ by $$t^{\mu \nu} = \frac{1}{\mu_0} \left ( f^{\mu \alpha} g_{\alpha \beta} f^{\nu \beta} - \frac{1}{4} g^{\mu \nu} f^{\gamma \delta} f_{\gamma \delta} \right ) $$ the einstein equations govern how the stress-energy tensor is coupled to spacetime curvature . since the magnetic field is entirely captured by the electromagnetic tensor , the answer is yes , magnetic fields contribute to gravitation .
there is quite a big controversy these days about the correct definition of the entropy in the microcanonical ensemble ( the debate between the gibbs and boltzmann entropy ) , which is closely related to the question . everyone agrees that the correct definition of the density matrix is given by $$\rho ( e ) =\frac{\delta ( e-h ) }{\omega ( e ) } , $$ where $h$ is the hamiltonian and $$ \omega ( e ) =tr\ , \delta ( e-h ) . $$ then the question is the correct definition of the entropy . boltzmann says $s_b=\ln \omega ( e ) $ , whereas gibbs argued $s_g=\ln \omega ( e ) $ where $$ \omega ( e ) =\int_0^e\omega ( e ) de . $$ in the text quoted by the op , the partition function corresponds to $\omega ( e ) $ . note that in most cases , in the thermodynamics limit , both entropies gives the same result . the question arises in the case of small systems and special cases with bounded from above spectra . hilbert et al . ( arxiv:1408.5382 and arxiv:1304.2066 ) argue that only the gibbs entropy is thermodynamically consistent . i must say that i find their arguments compelling , and that of their opponents , given in at least two comments of their papers , not at all .
the fundamental quantity in thermodynamics is entropy , which is a function of $n$-variables $s=s ( x_1 , x_2 , . . . , x_n ) $ . for instance , for a simple mono-component system $s=s ( u , v , n ) $ where $u$ is internal energy , $v$ is volume , and $n$ composition . taking the differential $$\mathrm{d}s = \sum_i \left ( \frac{\partial s}{\partial x_i}\right ) _{j \neq i} \mathrm{d}x_i = \sum_i f_i \mathrm{d}x_i$$ the quantities $f_i \equiv ( {\partial s}/{\partial x_i} ) _{j \neq i} $ are intensive entropic parameters and measure the change in entropy when variables change . for instance the intensive entropic parameter $ ( 1/t ) $ gives the change on entropy due to a change in the energy $u$ . using the thermodynamic theory of fluctuations it can be shown that $$f_i = k \left ( \frac{\partial \ln p}{\partial x_i}\right ) $$ where $p$ is the probability of a fluctuation in the variables near an equilibrium state . using the definition of average $$\langle a \rangle = \int a p \mathrm{d}x_1 \mathrm{d}x_2 \cdots \mathrm{d}x_n$$ the demonstration of the central result of linear nonequilibrium thermodynamics $$\langle f_i \cdot x_j \rangle = -k \delta_{ij} $$ is direct although it needs first the use of $ ( \partial \ln p / \partial x_i ) p = \partial p / \partial x_i$ in the integrand and next integration by parts . what wikipedia makes is to rewrite this central result using the new quantities $x_i \equiv - f_i / k$ but the physically important quantities are the $f_i$ often also called in this context thermodynamic forces or affinities .
the reason is because it is the lightest baryon with strangeness 3 , the mass energy of lightest strange object of s=1 ( the kaons ) is greater than 1/3 it is mass , so it can not decay into these . it is a general principle of energy conservation and strangeness conservation : the lightest example of any conserved quantum numbers can not decay without changing this number . in this case you need the strangeness to go down , and this requires weak decay , because , as dmckee says in his comment , the strong interaction respects flavor .
maybe a way for you to be not confused is to imagine a time dependence . for instance , let suppose three times $t_i , t , t_f$ with $t_i &lt ; t &lt ; t_f$ . one may suppose that the particle is in the initial state $|a\rangle$ at time $t_i$ , is in the final state $|z\rangle$ at time $t_f$ , and , at the intermediary time $t$ is in one of the $2$ states $|s_1\rangle$ or $|s_2\rangle$ . the law of composition of amplitudes say that : $\langle z , t_f|a , t_i\rangle =\langle z , t_f|s_1 , t\rangle\langle s_1 , t|a , t_i\rangle + \langle z , t_f|s_2 , t\rangle\langle s_2 , t|a , t_i\rangle$ this is true for all $z$ , so we have : $|a , t_i\rangle = |s_1 , t\rangle\langle s_1 , t|a , t_i\rangle + |s_2 , t\rangle\langle s_2 , t|a , t_i\rangle$ now , you may interpret this equation as follows : given that the particle is in the state $|a\rangle$ at time $t_i$ , the probability amplitude to find the particle in the state $|s_1\rangle$ , at time $t$ , is : $\langle s_1 , t|a , t_i\rangle$
if the photon has more energy than the ionization energy , it can ( and often does ) ionize the atom . one nice way to think about this is to put the various energy levels of the atom in an energy level diagram like this : ( taken from this web site ) . all of the bound-state energy levels lie below the line marked $e_\infty=0$ ev . above this line is the region called " free electrons " in the diagram . this corresponds to an ionized atom . in this range , the energy levels are not quantized -- that is , any energy is possible . so any photon with enough energy to put the total energy somewhere in that shaded region is capable of ionizing the atom .
this may be a bit too weighted on the mathematical side , rather than the physics side , but if you look up references on geometric quantization , you will see how phase space has geometric structure which hilbert space does not have and vice versa . gq attempts to give explicit instructions for constructing the hilbert space from a classical phase space , so the characteristics of the spaces involved are brought out very explicitly .
for this process , the interaction hamiltonian is given by : $$\mathcal{h}_{\rm int}=-\frac{g}{\sqrt 2}\left ( v_{cb}\bar{b}_l\gamma^\mu c_l w^-_\mu+\bar{\nu}_l\gamma^\mu\ell_l w^+_\mu\right ) . $$ after integrating-out the heavy bosons , we obtain the following hamiltonian $$\mathcal{h}_{\rm eff}=-\dfrac{g_f}{\sqrt{2}}v_{cb} [ \bar{b}\gamma^\mu ( 1-\gamma_5 ) c ] [ \bar{\nu}\gamma^\mu ( 1-\gamma_5 ) \ell ] , $$ where $g_f/\sqrt{2}=g^2/ ( 8 m_w^2 ) $ is the fermi constant . to obtain the tree-level amplitude for the process $b_c\to j/\psi \ell^+ \nu$ , we consider the following matrix element $$\mathcal{a} ( b_c\to j/\psi \ell^+ \nu ) =-i\langle j/\psi \ , \ell^+\ , \nu_\ell |\mathcal{h}_{\rm eff} | b_c\rangle . $$ if you write explicitly the leptonic fields in terms of creation and annihilation operators , then you will notice that $$\mathcal{a} ( b_c\to j/\psi \ell^+ \nu ) =i \dfrac{g_f}{\sqrt{2}}v_{cb}\bar{u}_\nu \gamma^\mu ( 1-\gamma_5 ) v_\ell \langle j/\psi| \bar{b}\gamma^\mu ( 1-\gamma_5 ) c| b_c\rangle . $$ note that we have isolated the hadronix matrix element from the rest . now , if we are able to find this element by using lattice qcd methods or experimental results , then we will be able to compute the decay rate and other observables . [ however , i do not think this is possible for this particular transition at present . ] for your second question , if you consider only valence quarks in the mesons , then you are using a tree-level approximation to describe hadronic states . this is a crude approximation , because qcd is non-perturbative at low energies . you can improve it by computing high-order qcd corrections , but you will never have a reliable result .
actually , most research suggests that interactions of two galaxies ( i.e. . , " mergers " or collisions ) is what determines the shapes of galaxies , and not necessarily gravity alone . this astronomy now article from a few years ago goes a bit more into the details . it is true that there is a strong force pulling us inwards ( on the order of a billion newtons ) , so you indeed could say that a galaxy is collapsing , but it would take something like $10^{100}$ years before the stars would fall into the supermassive black hole in the center . on that time-scale , it really is not a statement worth making , imo .
are you worried that the cables that go to the fukushima reactors will carry radioactivity out ? the answer is no . you should read up a bit on radioactivity and educate yourself , since it is one of the facts of life . in the article you will see that it is atoms that are responsible for radioactivity whereas the current in the cables is due to electrons . the parts of the cables that are in a radiation environment will become radioactive , but that activity will remain in the locality , and the length of cable that was exposed . it in no way can be transmitted away from the region the way the current is transmitted .
the heat capacity of an einstein solid is given by \begin{equation} c = nk \left ( \frac{\epsilon}{kt}\right ) ^{2} \frac{e^{\epsilon/kt}}{ ( e^{\epsilon/kt}-1 ) ^{2}} , \end{equation} where $n$ is the number of degrees of freedom . so the value of the energy quantum $\epsilon$ , or more precisely the ratio $x\equiv\epsilon/kt$ matters ! the above equation tends to the equipartition value $nk$ as $x \rightarrow 0$ , and the half maximum corresponds the value of $x$ satisfying \begin{equation} x^{2}\frac{e^{x}}{ ( e^{x}-1 ) ^{2}} =\frac{1}{2} . \end{equation} it turns out that the solution is $x_{\rm{half}} = 2.98287\ldots \approx 3$ , and this is the origin of the relation $kt/\epsilon\approx 1/3$ given to you .
the energy flux of an acoustic wave is $$ \vec j = \vec v p \ ; \ ; \ ; \ ; \ ; \ ; \ ; \ ; \ ; \ ; \ ; \ ; \ ; ( 1 ) $$ the relevant energy density to be used in these calculation is actually $p+1/2 \rho v^2$ , but since we are discussing a small amplitude wave ( = no shock wave ) , $v$ is an infinitesimal quantity ; thus $1/2\rho v^2$ is lower order than $p$ ( second vs . first ) , thus it can be neglected . you should also keep in mind that eq . ( 1 ) applies instantaneously , while you will most likely want a value averaged over a period . if you write $p = p_0 +\delta\ ! p$ , you find $$ &lt ; \vec j&gt ; = &lt ; \vec v p &gt ; = &lt ; \vec v p_0&gt ; + &lt ; \vec v \delta\ ! p&gt ; $$ where $&lt ; &gt ; $ symbolize averages over a period . the first term on the rhs obviously vanishes , so you are left with $$ &lt ; \vec j&gt ; = &lt ; \vec v \delta\ ! p&gt ; $$ the rhs can easily be average using expressions from elementary textbooks . a reasonable comment is that an expansion to first order in the wave amplitude has given rise to second-order energy flux and density .
typically one is introduced to the spectral theorem for hermitian operators . recall : if $a$ is hermitian then $$a = \sum_k a_k | k\rangle\langle k| , $$ where each $a_k$ is real and $\{| k \rangle\}$ is an orthonormal basis . if we have a function $f:\mathbb r\to \mathbb r$ ( i.e. . a real valued function ) , then we define ( overloading the definition ) $f:$hermitian operators $\to$ hermitian operators as $$f ( a ) :=\sum_k f ( a_k ) | k\rangle\langle k| . $$ but we need not restrict ourselves to real valued functions . we could have a complex valued function $f:\mathbb r\to\mathbb c$ . now , however , the function defined on hermitian operators will have a more general range , i.e. $f:$hermitian operators $\to$ linear operators . consider the specific function $f ( a ) =e^{i a}$ applied to $a$ . by definition $$f ( a ) = e^{ia} = \sum_k e^{i a_k} | k\rangle\langle k| , $$ which you can prove to yourself is unitary . it turns out that every unitary $u$ can be obtained by applying this function to a ( non-unique ) hermitian operator ( the canonical one being $-i\log u ) $ . ( stone 's theorem generalizes this a bit to parameterized groups of unitaries with the upshot that the hermitian operator is uniquely determined . )
any siphon in your house will be affected in this way . the inside surface ( that you can see ) is affected by the air pressure in your house , the other level is connected to the sewer piping system , wich gets his pressure from a vent of the sewer system . both pressures can be affected in many ways by the stormy wind , depending where some passages ( like windows not really tight ) for the air are located , eg at the windward or leeward side of your house . the vent of the sewer system can develop pressure variations depending on wind speed by some water-aspirator-like action . so the siphon in your toilet works as a pressure differential indicator .
well to be hones both of your questions are related . let me start by rewriting your equations of $y_1$ and $y_2$ ( this will make the discussion easier ) , your version of $y_1$ and $y_2$ can be rewritten as : $$y_1=a\sin\left ( \frac{2\pi}{\lambda} ( x+vt ) \right ) , $$$$y_2=-a\sin\left ( \frac{2\pi}{\lambda} ( x-vt ) \right ) , $$where i put a minus outside of $y_2$ by using $\sin ( -x ) =\sin ( x ) $ . let 's now first look at question 2 . question 2 : the answer to this question can be found by looking at the wavefronts of $y_1$ and $y_2$ , which in its turn can be done by looking at a constant value for the arguments of $y_1$ and $y_2$ ( since a constant argument yields a constant value of $y_1$ and $y_2$ and hence a wavefront ) . let 's call this constant value of the argument $x_0$ , then the arguments of $y_1$ and $y_2$ become : $$x_0=x+vt \text{ for the argument of $y_1$} , $$$$x_0=x-vt \text{ for the argument of $y_2$} . $$ both arguments can be rewritten as:$$x=x_0-vt \text{ for the argument of $y_1$} , $$$$x=x_0+vt \text{ for the argument of $y_2$} , $$ where we see that the introduced constant $x_0$ denotes the position of the wavefront at $t=0$ . what this tells us is that $y_1$ represents a wave travelling in the negative $x$-direction ( when the time increases the value of $x$ decreases ) and $y_2$ represents a wave travelling in the positive $x$-direction ( when time increases the value of $x$ increases ) . in general . you can do this analysis for every kind of wave , and you will find that waves with an argument of the form $ ( x+vt ) $ are waves travelling in the negative x-direction ( als called ''left travalling waves'' ) and waves with arguments of the form $ ( x-vt ) $ are wave travelling in the positive x-direction ( also called ''right travelling waves'' ) . on hyperphysics a few more drawings and discussions are available ( should you be interested ) . $$$$ question 1 : the answer to question 1 can be given by the fact that te amplitudes of waves can be summed . this is because of the fact that each wave tells you what displacement $y$ is causes at a given point $x$ on a time $t$ , when you have two waves which interact , the displacements should be summed . now when you look at reflection ( of sound our light or whatever wave you are looking at ) , there are 3 things that can happen : ( first case on the figure ) : when you reflect the reflected wave picks up a phase $\phi=\pi/2$ ( this happens when you reflect on a dense medium ) . in that case your reflected wave picks up a minus sign since $\sin ( x+\pi/2 ) =-\sin ( x ) $ , this is probably the case you are looking at ) . ( second case on the figure ) : when you reflect the reflected and indecent wave are in anti-phase , so they cancel eachother out . ( third case on the figure ) : when you reflect , the reflected wave does not pick up a sign ( and they are in phase ) , the amplitude of the wave doubles .
capacitors , as used in electric circuits , do not store electric charge . when we say a capacitor is charged , we mean energy is stored in the capacitor and , in fact , energy storage is one application of capacitors . now , for an ideal capacitor in a circuit context , the current through is proportional to the rate of change of the voltage across : $$i_c = c \frac{dv_c}{dt}$$ where $c$ is the capacitance of the capacitor . note that when the voltage across the capacitor is constant , the current through the capacitor is zero . this property means that a series capacitor can be used to couple signals between two circuit nodes with different dc voltages . this same property also means that a parallel capacitor can be used to decouple signals between two circuits that share a common dc voltage . one of the simplest circuits with a capacitor is the rc circuit which has numerous uses . when combined with a resistor and inductor , we have an rlc circuit which also has numerous uses . the fact is that the uses of capacitors are so numerous that there is no simple answer . you can find much more here : applications of capacitors
your situation is unfortunate , but this will become very typical as you get further in school . i will do my best to explain this problem fully : to begin with , in this problem , two interference patterns are formed , each pattern unique to one of the wavelengths provided to you . it is important to understand here that the fringe patterns might overlap , but do $\textbf{not}$ $\textit{interfere}$ with one another ( in terms of wave front interference ) . consequently , it can be concluded that the bright fringes of one wavelength will eventually share locations with the dark fringes of the other wavelength . when this occurs , no fringes will be visible , as there will be no dark bands to differentiate a between a single fringe , and the fringes adjacent to it . in order to make the transition between $\textbf{periods of fringe absence / appearance}$ the mirror 's change in position must produce an $\textbf{integer number}$ of fringe shifts for each wavelength , and the number of shifts for the shorter wavelength must be one more than the number for the longer wavelength . $\textbf{this next bit is imperative for understanding the michelson interferometer:}$ the light travels the length of the apparatus $\textbf{twice}$ , so the change in the position of the mirror must also be accounted for $\textbf{twice}$ . thus , even though a fringe shift possesses a wavelength value of $\lambda$ , we will denote it as $\cfrac{\lambda}{2}$ and the change in position of the mirror $ ( \bigtriangleup x ) $ will become $2 ( \bigtriangleup x ) $ to wrap this problem up , let 's make $\epsilon_n$ the number of fringes produced by a given wavelength . $\epsilon_1$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_a}$ and $\epsilon_2$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_b}$ , therefore $\epsilon_2$ = ( $\epsilon_1 + 1$ ) from here we can see that $\cfrac{2 ( \bigtriangleup x ) }{\lambda_b}$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_a} + 1$ , meaning that $\bigtriangleup x = \cfrac{\lambda_a\lambda_b}{2 ( \lambda_a-\lambda_b ) }$ i will let you plug in your values , you should get a number much less than a meter , but much greater than a nanometer . this was not an easy problem for what i am assuming is an a level physics course . let me know if you have questions . good luck .
we do not really understand why charge is quantized . nor we do know if there ought to be magnetic monopoles . these two things seem linked . dirac gave an argument for charge quantization in the early days , but this presupposed the existence of a magnetic monopole . in maxwell 's equations , it would be completely natural to imagine the existence of magnetic monopoles , and some high energy theories predict their existence , but we have seen no direct evidence of them yet . and to my knowledge , though it is not my area of expertise , i do not think there have been any other really compelling arguments for charge quantization to compete with dirac 's original proposal . we also do not understand high temperature superconductivity yet . we also have a hard time computing electronic energy bands for complicated structures . current density functional theory techniques have errors on the percent level .
the main difference between qm and qft w.r. t measurements etc . is the kind of question you ask . usually in qft you simply start your calculation assuming a plane wave of incident particles and determine the probablility of scattering in different directions . you do this to learn something about the interactions of the particles , not about the state the particle is in . you infer the interactions that took place from classical measurements of momentum and energy , not from any idealized quantum measurement . in order to describe the evolution of one specific particle , you would need to convolute your results for plane wave states with the wave function of your particle . still , once you have determined the state your particle ends with , the wave function collapses and this process is not described dynamically by the theory , but put in by hand . tl/dr : in qft the measurement process is still not described , but the state of the system is also not the object of interest .
texts on qcd do not divide the generators of $su ( 3 ) $ – and therefore " bicolors of gluons " – into two groups because this separation is completely unphysical and mathematically artificial ( basis-dependent ) . moreover , the number of " bicolors of gluons " i.e. generators of $su ( 3 ) $ , the gauge group of qcd , is not nine as you seem to think but only eight . the group $u ( 3 ) $ has nine generators but $su ( 3 ) $ is the subgroup of matrices with the unit determinant so one generator is removed . at the level of gluons or lie algebra generators , the special condition $s$ means that the trace is zero . so the combinations $$ a ( r\bar r ) + b ( g\bar g ) + c ( b \bar b ) $$ are only allowed " bicolors of gluons " if $a+b+c=0$ . now , in this 8-dimensional space of " bicolors of gluons " , there are no directions ( "bicolors" ) that are better than others . for any direction in this space , there exists an $su ( 3 ) $ transformation that transforms this direction into a direction non-orthogonal to any chosen direction you choose . this is true because the 8-dimensional representation is an irreducible one ( the adjective " irreducible " means that one should not try to split it to two or several separated collections ! ) . and there does not exist any consistent yang-mills theory that would only contain the six off-diagonal " bicolors " because the corresponding six generators are not closed under the commutator . the actual calculations of the processes with virtual gluons ( "forces " between quarks etc . ) therefore never divide terms to your two types because this separation is just an artifact of your not having learned group theory . instead , all the expressions are summing over three colors of quarks , $i=1,2,3$ indices of some kind , and there is never any condition $i\neq j$ in the sums because such a condition would break the $su ( 3 ) $ symmetry . now , the $r\bar r , g\bar g , b\bar b$ " bicolors of gluons " ( only two combinations of the three are allowed ) are actually closer to the photons than the mixed colors . so it is these bicolors that produce an attractive force of a very similar kind as photons – they are generators of the $u ( 1 ) ^2$ " cartan subalgebra " of the $su ( 3 ) $ group and each $u ( 1 ) $ behaves like electromagnetism . that is why these components of gluons cause attraction between opposite-sign charges and repulsion between the like charges . the six off-diagonal " bicolors of gluons " ( and let me repeat that the actual formulae for the interactions never separate them from the rest – they are included in the same color-agreement-blind sum over color indices ) cause neither attraction nor repulsion : they change the colors of the interacting quarks so the color labels of the initial and final states are different . it makes no sense to compare them , with the idea that only momentum changes , because that would be comparing apples and oranges ( whether the force looks attractive or not depends on the relative phases of the amplitudes for the different color arrangements of the quarks ) . at any rate , particles like protons contain quarks of colors that are " different from each other " so they are closer to the opposite-sign charges and one mostly gets attraction . however , the situation is more complicated than it is for the photons and electromagnetism because of the six off-diagonal components of the gluons ; and because gluons are charged themselves so the theory including just them is nonlinear i.e. interacting .
i think this is related to elastic fatigue . here 's the wikipedia page .
in a comment you write space time symmetries do not fit into the framework of the action since the action is a functional on the fields only not also on space time ( space time here appears merely as a dummy variable this is not quite right . a given spacetime transformation often induces a transformation on fields themselves , and in this way , spacetime transformations fit into the framework of the action . this is most easily and explicitly illustrated by way of a simple example . example . consider a theory of a single real scalar field on $\mathbb r^{3,1}$ ( minkowski space ) . let $\mathcal f$ denote the space of fields considered in the theory ( which usually consists of e.g. smoothness assumptions and assumptions about the behavior of the fields at infinity ) . the action functional will be a function $s:\mathcal f\to \mathbb r$ . now , on the one hand , the lorentz group $\mathrm{so} ( 3,1 ) $ acts in a natural way on $\mathbb r^{3,1}$ , namely through the group action $\rho:\mathrm{so} ( 3,1 ) \to \mathrm{sym} ( \mathbb r^{3,1} ) $ defined as follows : \begin{align} \rho ( \lambda ) ( x ) = \lambda x , \end{align} where $\mathrm{sym} ( s ) $ denote the set of bijections on a set $s$ . on the other hand , this group action induces an action $\rho_\mathcal f$ of $\mathrm{so} ( 3,1 ) $ on $\mathcal f$ , the space of field configurations , as follows : \begin{align} \rho_\mathcal f ( \lambda ) ( \phi ) ( x ) = \phi ( \lambda^{-1} x ) , \end{align} which is sometimes written as $\phi' ( x ) = \phi ( \lambda^{-1} x ) $ for brevity . it is this action of $\mathrm{so} ( 3,1 ) $ on the fields that one would use to fit spacetime symmetries into the action framework . in particular , in this case we could say for example that $s$ is lorentz-invariant provided \begin{align} s [ \rho_\mathcal f ( \lambda ) ( \phi ) ] = s [ \phi ] \end{align} for all $\lambda\in\mathrm{so} ( 3,1 ) $ and for all $\phi\in\mathcal f$ . all of this can also be easily extended to theories of fields of more complicated types , like vector and tensor fields . in such cases , the action $\rho_\mathcal f$ will in general be more complicated because it will contain a target space transformation .
the traveller can travel 10 billion light years arbitrarily fast in his/her own experienced time . however , to the observer who stays at home on earth , the traveller 's speed will simply get asymptotically closer to the speed of light . this does , if you think about it , line up very fine with the twin " paradox " ( which is not really a paradox at all ) . as the traveller keeps accelerating the engine , when $v \rightarrow c$ , the time his travel takes will not be shortened to the twin who stays at home . a journey of , say , 40 light years will mean that the twin who stays home will always have aged at least 80 years when the traveller returns home . the traveller , however , can experience an arbitrarily short travel time , tending asymptotically to zero as $v$ tends to $c$ .
comets are some of the material left over from the formation of the planets . our entire solar system , including comets , was created by the collapse of a giant , diffuse cloud of gas and dust about 4.6 billion years ago . much of the matter merged into planets , but some remained to form small lumps of frozen gas and dust in the outer region of the solar system , where temperatures were cold enough to produce ice . a comet is generally considered to consist of a small nucleus embedded in a nebulous disk called the coma . the nucleus , containing practically all the mass of the comet , is a “dirty snowball” conglomerate of ices and dust . for one , of the observed gases and meteoric particles that are ejected to provide the coma and tails of comets , most of the gases are fragmentary molecules , or radicals , of the most common elements in space : hydrogen , carbon , nitrogen , and oxygen . the radicals , for example , of ch , nh , and oh may be broken away from the stable molecules ch4 ( methane ) , nh3 ( ammonia ) , and h2o ( water ) , which may exist as ices or more complex , very cold compounds in the nucleus . 3 . many astronomers believe that these small objects never became planets or other large objects because of the gravity of the large planets . for example , the pull of jupiter 's kept ' stirring the pot ' of the asteroid belt , so that the gravitational pull of the asteroids on each other was constantly being disturbed . for the kuiper belt and oort cloud , there is a popular theory called ' planetary migration . ' the main idea behind this theory is that the large outer planets of our solar system started out much closer to the sun when the solar system was formed . as they migrated outward through the cloud of small objects still there , the gravity of these large planets pulled a lot of the small objectsout of their orbits . some were pulled into the planets , and some were flung far into the outer reaches of the solar system . the objects that were flung very far out by jupiter became the oort cloud . the object that were not flung out quite as far by the movement of neptune became the kuiper belt . source
nothing as easy from basic principles as for conduction or radiation . you can multiple the mean heat carried by the convecting liquid by it is mass rate of flow $$ \delta t = c_p * ( t_{in} - t_{out} ) * \frac{\delta v}{\delta t} \rho \delta t . $$ where $t_{in}$ is the mean temperature of the liquid moving toward the sink ( cool side ) and $t_{out}$ is the temperature of the liquid moving the other way . the problem is that you can not get the temperature ( s ) of the convecting liquid or the flow rate without detailed calculations or measurements . for the generaly case you pretty much have to go to cfd .
well , to be really pedantic , almost anything is data . any measurement or observation is data . i suppose i would define data as any piece of information , regardless of how it was obtained or whether it is valid . of course , i think your question was really getting at what would be considered evidence , in the context of providing support or opposition to a theoretical model . i would say that the answer is very situation dependent . using simulation results as an example since it was in your question , i would certainty take simulation results in support of some theory , but only on the condition that the simulation has been very well validated against experiment . the details of how and why the simulation results are favorable to the theory is also certainly important . ultimately , all science must be validated by experiment or observation , but that can sometimes be a long journey , and there is definitely a place for other sorts of data along the way .
at the specified point , there is a certain tangent line to the path . also at that point , the vector field $\mathbf f$ has certain components . we want to know the component of $\mathbf f$ in the direction of the tangent line to the path at the point given . here 's how i would approach this . let 's restrict to the $x$-$y$ plane . the curve on which the object is traveling can be written as a parameterized curve in 2d as $$ \mathbf x ( s ) = ( x ( s ) , y ( s ) ) = ( s , s^2 ) $$ the unit tangent vector to this path at parameter value $s$ is $$ \mathbf t ( s ) = \frac{\mathbf x' ( s ) }{|\mathbf x ( s ) |} = \frac{ ( 1 , 2s ) }{\sqrt{1+4s^2}} $$ the tangential component of a vector field $\mathbf f ( \mathbf x ) $ at a given point along the curve corresponding to parameter value $s$ is therefore $$ f_t ( s ) = \mathbf f ( \mathbf x ( s ) ) \cdot\mathbf t ( s ) $$ i will let you try to figure out the rest . cheers !
" site " is synonymous with " vertex " . they are called " sites " because they are the places where the objects of interest ( particles with a spin or whatever ) are located . so it is a lattice with $n$ vertices .
pipes are damaged when ice forms a complete blockage , and the expansion of water trapped by it puts too much pressure on them . now , ice is a pretty good thermal insulator , so once a little ice forms on the inside of the pipe further freezing proceeds slowly . if the water is flowing there will not be enough time for it to freeze between leaving the ( relatively warm and protected ) underground run and exiting the pipe at the dripping faucet . thus the pipe never freezes all the way through , no blockage forms and excess pressure is never applied .
total momentum is always conserved , in both elastic and inelastic collisions , but total kinetic energy is only conserved in elastic collisions . this example seems to be a completely inelastic collision , because at the end the objects merge . there is a formula to calculate the final velocity $v$ of two object with speed $u_1$ and $u_2$ and mass $m_1$ and $m_2$ in a completely inelastic collision , which is : $$v=\frac{m_1u_1+m_2u_2}{m_1+m_2}$$ here 's a simple derivation : since momentum is always conserved , the sum of momenta at the beginning is the same as the end : $$p_{i1}+p_{i2}=p_{f1}+p_{f2}$$ however , since this is a completely inelastic collision , at the end the two objects will merge , and so there will be only one final momentum . the final momentum is simply the sum of initial momenta , like final mass is the sum of initial masses : $$p_{1}+p_{2}=p_f\qquad m_1+m_2=m_f$$ then : $$v=\frac{p_f}{m_f}\qquad v=\frac{p_1+p_2}{m_1+m_2}\qquad v=\frac{m_1u_1+m_2u_2}{m_1+m_2}$$ total kinetic energy however is not conserved , as you can see summing initial kinetic energies and comparing with the final kinetic energy .
if we consider two spin $1/2$ particles with spin up and spin down states , then there are four possibilities in total : \begin{equation} \begin{array}{cccc} |++\rangle \ ; , and |+-\rangle \ ; , and |-+\rangle \ ; , and |--\rangle \end{array} \end{equation} where this notation means for instance : \begin{equation} |+-\rangle = |s_1=1/2 , m_1=1/2 ; s_2=1/2 , m_2=-1/2\rangle \end{equation} we will suppose that the system consisting of both particle has zero orbital angular momentum and let $s_z$ denote that operator acting on the system . then it is very easy to evaluate the following eigenvalue equations : \begin{align} s_z|++\rangle and =\hbar|++\rangle \\ s_z|+-\rangle and =0|+-\rangle\\ s_z|-+\rangle and =0|-+\rangle \\ s_z|--\rangle and =-\hbar|--\rangle \\ \end{align} and so $s_z$ can be written as : \begin{equation} s_z = \hbar \begin{pmatrix} 1 and 0 and 0 and 0 \\ 0 and 0 and 0 and 0 \\ 0 and 0 and 0 and 0 \\ 0 and 0 and 0 and -1 \end{pmatrix} \end{equation} now , if $s=1$ ( remember $s$ denotes the quantum number for the total system ) , then the three states of the system are : \begin{align} |s=1 , m=1\rangle and = |++\rangle \\ |s=1 , m=0\rangle and = \sqrt{\frac{1}{2}} ( |+-\rangle+|-+\rangle ) \\ |s=1 , m=-1\rangle and = |--\rangle \end{align} with eigenvalues : \begin{align} s_z |1,1\rangle and = \hbar |1,1\rangle \\ s_z |1,0\rangle and = 0|1,0\rangle \\ s_z |1 , -1\rangle and = -\hbar |1 , -1\rangle \end{align} where i have switched to a more compact notation : \begin{equation} |s=1 , m=1\rangle \equiv |1,1\rangle \end{equation} however , we can also get an eigenstate with $s=0$: \begin{equation} |0,0\rangle = \sqrt{\frac{1}{2}} ( |+-\rangle-|-+\rangle ) \end{equation} with eigenvalue : \begin{equation} s_z |0,0\rangle = 0|0,0\rangle \end{equation} now , we can also verify that : \begin{equation} s^2 \equiv s_x^2 + s_y^2 + s_z^2 \end{equation} satisfies : \begin{align} s^2 |1,1\rangle and = 2 \hbar^2 |1,1\rangle \\ s^2 |1,0\rangle and = 2 \hbar^2 |1,0\rangle \\ s^2 |1 , -1\rangle and = 2 \hbar^2 |1 , -1\rangle \\ s^2 |0,0\rangle and =0|0,0\rangle \end{align} therefore , we see that the following two important equations are satisfied : \begin{equation} \begin{array}{cc} s^2 |s , m \rangle = \hbar^2 s ( s+1 ) |s , m \rangle \ ; , and s_z |s , m \rangle = \hbar m |s , m \rangle \end{array} \end{equation} to sum up , we have found the possible eigenvalues for the magnitude and $z$-component of the system and the eigenstates corresponding to these values : the allowed values for total spin are $s=1$ and $s=0$ , while the allowed value of $s_z$ are $\hbar$ , $0$ , and $-\hbar$ and the corresponding eigenstates in the product basis $|1,1\rangle$ , $|1,0\rangle$ , $|1 , -1\rangle$ and $|0,0\rangle$ . thus , the meaning of these triplet and singlet states is that they are the possible states of the system consisting of the two aforementioned particles . this is often written as : \begin{equation} \frac{1}{2} \otimes \frac{1}{2} = 1 \oplus 0 \end{equation} which means that the tensor product of two spin-$1/2$ hilbert spaces is a direct sum of a spin-$1$ space and a spin-$0$ space .
in the weak-field case , $$\mathrm{d}s^2 = -\left ( 1+2\frac{\phi}{c^2}\right ) c^2\mathrm{d}t^2 - \frac{4}{c}a_i\mathrm{d}t\mathrm{d}x^i + \left ( 1-2\frac{\phi}{c^2}\right ) \mathrm{d}s^2\text{ , }$$ where $\phi$ is the newtonian potential and $\mathrm{d}s^2 = \mathrm{d}x^2 + \mathrm{d}y^2 + \mathrm{d}z^2$ is the euclidean metric . in the static case , $a_i = 0$ , which is the form used for gps calculations , but in general it is more interesting as being a direct analogue to classical electromagnetism , first formulated for gravity by heaviside in 1893: $$\begin{eqnarray*}\mathbf{e}_\text{g} = -\nabla\phi - \frac{1}{2c}\frac{\partial\mathbf{a}}{\partial t} and \quad\quad and \mathbf{b}_\text{g} = \nabla\times\mathbf{a}\end{eqnarray*}$$ $$\begin{eqnarray*} \nabla\cdot\mathbf{e}_\text{g} = -4 \pi g \rho_\text{g} and \quad\quad and \nabla \times \mathbf{e}_\text{g} = -\frac{1}{2c}\frac{\partial\mathbf{b}_\text{g}}{\partial t} \\ \nabla\cdot\mathbf{b}_\text{g} = 0 and \quad\quad and \nabla\times\frac{1}{2}\mathbf{b}_\text{g} = -\frac{4\pi g}{c}\mathbf{j}_\text{g} + \frac{1}{c}\frac{\partial\mathbf{e}_\text{g}} {\partial t} \end{eqnarray*}$$ this particular version was taken from einstein 's general theory of relativity by grøn øyvind and sigbjørn hervik ; a few variations in defining these fields exist in the literature . but probably more importantly , the post-newtonian formalism gives a more general approximation scheme , the first few terms of which are : $$\mathrm{d}s^2 = - ( 1+2\phi+2\beta\phi^2+\ldots ) \mathrm{d}t^2 + ( 1-2\gamma\phi+\ldots ) \mathrm{d}s^2 + ( \ldots ) \mathrm{d}t\mathrm{d}x^i\text{ , }$$ with many other potentials that i am omitting here . this is very useful for understanding the general predictions of gtr and comparing them to alternative theories of gravity ( e . g . , gtr predicts $\beta = \gamma = 1$ , other theories might not ) .
i think this is a typo in morse and feshbach methods of theoretical physics . the correct expression is $-\frac{1}{6}r^2\nabla ^2\psi$ or $-\frac{1}{6} ( dx^2+dy^2+dz^2 ) \nabla^2 \psi$ .
if i were thinking about it from my own perspective , i would see it as the need to balance energy . we already knew from relativity that mass carried an intrinsic energy with it , and so if i measure something and energy is missing , either i ( 1 ) did not account for some lossy mechanism or ( 2 ) did not measure all the energy coming out . from here , you are stuck with either a particle that carries energy but has no mass ( the photon , which i can measure ) or something that has mass but has kinetic energy . as you observed , this particle would have to carry no charge because of charge conservation but also because charged massive particles are easy to detect if they exist and do not decay rapidly . assuming you are hunting for photons at a known wavelength and did not detect them , the only choice left to you is a massive chargeless particle . energy conservation and charge conservation are the two most fundamental laws of physics , and we have never observed them violated in fundamental processes .
the explicit eigenfunctions of the harmonic oscillator hamiltonian are given here , but i would highly discourage you from explicitly doing an integral using these expressions to determine $a$ . it is significantly easier to use the fact that the eigenfunctions are orthogonal ; $$ \int_{-\infty}^\infty dx\ , \phi_m^* ( x ) \phi_n ( x ) = \delta_{mn} $$ if you use this fact , then the integral on the left hand side of the $t=0$ normalization condition you wrote down will be very easy . try this out , and if you still have trouble we can give you more guidance since this is a homework question .
in the context of quantum mechanics , the entropy of a system whose initial state is given by a density matrix $\rho ( 0 ) $ is given by the so-called von neumann entropy ; $$ s_\mathrm{vn} ( \rho ) = -k\ , \mathrm{tr} ( \rho\ln\rho ) $$ for an isolated system , quantum mechanical time evolution is unitary ; for each time $t$ , there is a unitary operator $u ( t ) $ such that the state of the system at time $t$ is given by $$ \rho ( t ) = u ( t ) \rho ( 0 ) \ , u^\dagger ( t ) $$ it can be shown that the von neumann entropy is invariant under unitary similarity transformation of $\rho$ ; in other words $$ s_\mathrm{vn} ( u\rho u^\dagger ) = s_\mathrm{vn} ( \rho ) $$ and it immediately follows that $$ s_\mathrm{vn} ( \rho ( 0 ) ) = s_\mathrm{vn} ( \rho ( t ) ) $$ in other words , the entropy of an isolated quantum system does not change with time in accordance with the second law of thermodynamics . author 's admission . i have always been somewhat bothered by the argument i just gave you , not because i think it is incorrect , but rather because in light of the conclusion we draw from it regarding isolated systems , why do not people say that the stronger statement $ds=0$ for isolated systems as opposed to $ds\geq 0$ . it is not that these are inconsistent statements ; one is just stronger than than the other , so i would think one should simply assert the stronger one in the context of isolated systems . addendum . in response to my " admission , " i should note that there is a cute argument i have seen for the non-negativity of a change in total ( von-neumann ) entropy of an isolated system provided one defines total entropy properly . here it is . suppose that we have an isolated system , let 's call it the universe , described by a hilbert space $\mathcal h$ . suppose that this system can be divided into two subsystems $a$ and $b$ so that the combined hilbert space can be written $\mathcal h = \mathcal h_a\otimes\mathcal h_b$ . if the density matrix of the universe is $\rho$ , then the density matrices of the subsystems $a$ and $b$ are defined as partial traces over $\rho$ ; $$ \rho_a = \mathrm{tr}_{\mathcal h_a}\rho , \qquad \rho_b = \mathrm{tr}_{\mathcal h_b}\rho $$ now we can prove the following : if systems $a$ and $b$ are initially uncorrelated , then then the total entropy $s ( \rho_a ) + s ( \rho_b ) $ will never be lower than at the initial time . proof . if the systems are initially uncorrelated , then by definition the total density operator at the initial time is a tensor product $\rho ( 0 ) = \rho_a^0\otimes \rho_b^0$ . it follows from taking partial traces and using the fact that the density operator is unit trace that the density matrices of the subsystems $a$ and $b$ at the initial time are $$ \rho_a ( 0 ) = \rho_a^0 , \qquad \rho_b ( 0 ) = \rho_b^0 $$ now , at any later time , the total density matrix evolves unitarily , so that $$ s ( \rho ( 0 ) ) = s ( \rho ( t ) ) $$ on the other hand , entropy is subadditive which means that $$ s ( \rho ( t ) ) \leq s ( \rho_a ( t ) ) +s ( \rho_b ( t ) ) $$ and is additive for uncorrelated systems which gives $$ s ( \rho ( 0 ) ) = s ( \rho_a ( 0 ) ) + s ( \rho_b ( 0 ) ) $$ putting this all together yields $$ s ( \rho_a ( 0 ) ) + s ( \rho_b ( 0 ) ) \leq s ( \rho_a ( t ) ) +s ( \rho_b ( t ) ) $$ i have always been somewhat unsatisfied with this argument , however , because ( i ) it assumes that the subsystems are originally uncorrelated and ( ii ) it is not clear to me that the definition of total entropy as the sum of the entropies of the reduced density operators of the subsystems is what we should be calling $s$ when we write $\delta s \geq 0$ . by the way , this argument was stolen from lectures i took : eric d'hoker 's quantum lecture notes .
the $\phi$ meson does indeed have angular momentum of $1$ and so the kaons must have a total angular momentum of $1$ . since the kaons are both spin zero that means that they must come out in a state with non-zero orbital angular momentum state , i.e. , an excited state . in general any orbital angular momentum is allowed , however i believe that the general trend is that the higher the necessary orbital angular momentum , then more the process is suppressed .
no . there is no theory of open , oriented strings . any string theory must contain closed strings , while the open strings are optional . if there is a string theory which contains oriented open strings , then it has the problem that the oriented open strings cannot couple to the oriented closed strings . why ? this is my understanding of the explanation given by the by thomas mohaupt in lecture notes " introduction to string theory " : in the closed string spectrum , there is an $\mathcal n = 2a$ algebra and and an $\mathcal n = 2b$ algebra which lead to different string theories . both have 32 supercharges . in each of these , , there are 2 gravitinoes , dilationoes , 1 in the ramond neveu-schwarz sector and 1 more in the neveu-schwarz ramond sector . these 2 gravitinos need 2 different supercurrents to couple to . but the $\mathcal n=1$ supersymmetric algebra with only 16 supercharges clearly cannot allow this ! thus , the open oriented strings would not couple with the closed oriented strings . the solution is to have open unoriented strings instead . this along with the unoriented closed strings is the type i string theory . the " only unoriented closed strings " theory is also inconsistent because of other reasons .
the spit horizon in a rindler wedge occurs at a distance $d~=~c^2/g$ for the acceleration $g$ . in spatial coordinates this particle horizon occurs at the distance $d$ behind the accelerated frame . clearly if $d~=~0$ the acceleration is infinite , or better put indefinite or divergent . however , we can think of this as approximating the near horizon frame of an accelerated observer above a black hole . the closest one can get without hitting the horizon is within a planck unit of length . so the acceleration required for $d~=~\ell_p$ $=~\sqrt{g\hbar/c^2}$ is $g~=~c^2/\ell_p$ which gives $g~=~5.6\times 10^{53}cm/s^2$ . that is absolutely enormous . the general rule is that unruh radiation has about $1k$ for each $10^{21}cm/s^2$ of acceleration . so this accelerated frame would detect an unruh radiation at $\sim~10^{31}k$ . this is about an order of magnitude larger than the hagedorn temperature . we should then use the string length instead of the planck length $4\pi\sqrt{\alpha’}$ and the maximum acceleration will correspond to the hagedorn temperature .
no . the elementary idea of grounding is that excess current finds an easier way to the ground than through your body . if you are wet , you ' may ' in minimal probability get a minor shock , but unless your body offers the current a path easier or at least comparable to the metallic ground wire , the answer is no . edit : current gets divided in inverse ratio to resistance offered by the paths , hence very less or in this case negligible current flows through a path which has a very high resistance compared to the other .
assuming there is more flux coming into the open end of the tube than the thermal radiation in the closed part of the piston then yes . radiation pressure doing work is perfectly feasible eg https://en.wikipedia.org/wiki/solar_sail
no . the law of the conservation of mass-energy is a purely local law ( i.e. . the divergence of the stress-energy tensor must be zero at any point in spacetime ) . you will find that , as long as your wormhole is a solution to einstein’s equations , this law will hold anywhere in the vicinity of the wormhole or within it . in other words , there is no point at which the mass suddenly disappears . it enters the mouth of the wormhole , pass through its throat , and emerges from its other mouth .
assuming you are using c code : this implements a simple integration of the equations of motion : $$v = \frac{dx}{dt}\\ a = \frac{dv}{dt}$$ i made the time step very small : you can get away with bigger steps ( takes less time ) . also i keep the loop going until the velocity has reached a certain downward value : you can use any other criteria ( time , position , etc ) . update as johannes pointed out , you get slightly more accurate results ( even when you use larger time steps ) when you use the average velocity during a time step to compute the next position . this leads to a small change in code : . below , i show both the old and the new calculation side by side - as you can see , when the time step is coarser , the new method continues to give good results : here are the results at time = 1.80 seconds for different time steps : finally , the " correct " result from integrating the equations of motion gives velocity = 8 - 1.8*9.8 = -9.64 position = 0 + 8 * 1.8 - 0.5 * 9.8 * 1.8^2 = -1.4760  as you can see , smaller steps get better results , and using the " new method " ( as suggested by johannes ) gets you closer to the true position than the " lazy method " i first proposed . last update to implement other forces , you can do something like this : i hope you can see how you could implement lots of different things that might change the total force on the object and that will change the motion . in this case , the last term ( onsupport ) sets both velocity and acceleration to zero , meaning that after a small time step it will still be zero - but if you set onSupport = 0 it will once again be able to move . and like this you could have multiple objects doing different things under the influence of gravity , rocket packs , etc . you obviously want to experiment with the drag=200 factor - i just picked a number from thin air , and with a value like 200 you will end up with a terminal velocity of about 3.5 m/s ( you would survive that no problem ) . as you make drag lower , the terminal velocity will be higher ( also depends on the mass of the object ) .
i was wondering if there is an intuitive way to understand the motion of a body influenced by two other massive bodies ( say the moon being influenced by the earth and the sun . no , intuition is not of real use in a three body gravitational problem , more so in many body . in 1887 , mathematicians ernst bruns [ 4 ] and henri poincaré showed that there is no general analytical solution for the three-body problem given by algebraic expressions and integrals . the motion of three bodies is generally non-repeating , except in special cases . [ 5 ] actually the motions of the planets are studied as possible dynamic chaos . planetaria solve the many body problem with numerical approximations .
i would add a humidity sensor , as water vapor is the strongest green house gas . this graph may suggest other gases > breakdown of the anthropic greenhouse gas emissions by gas . source : ipcc , 2007 here is an article on halocarbons .
for practical reasons , physicists like to label the states of the system by a set of " quantum numbers " . technically this means that you are looking for a set of mutually commuting hermitian operators such that : ( i ) every vector from the basis of common eigenvectors of these operators is uniquely characterized by the set of eigenvalues ( i.e. . the above mentioned quantum numbers ) ; ( ii ) the set of operators is minimal in the sense that by removing any of the operators , you lose the property ( i ) . the first condition implies that your set of operators is complete while the second one that there are no redundancies . since you typically want to use the quantum numbers to label the stationary states , that is , the eigenstates of the hamiltonian , you in addition require that your operators commute with the hamiltonian ( the hamiltonian itself need not be included in the set though ) . now it is obvious that for any given system and its hilbert space , there are many sets of operators that satisfy the above two conditions . in principle , you can find your own set of operators using the following algorithm . pick any ( hermitian ) operator $a$ and determine its spectrum . if there is no degeneracy , you are done . otherwise , add another operator $b$ that commutes with $a$ such that there is a pair of common eigenvectors with the same eigenvalues of $a$ but different eigenvalues of $b$ . ( this guarantees that you improve your " resolution " in the specification of the eigenvectors by the set of eigenvalues . ) if there are no two eigenvectors for which both $a$ and $b$ have the same eigenvalue , you are done . otherwise , repeat the previous step . it should be emphasized that the notion of a complete set of commuting operators introduced in this way is a bit vague . for example , it does not say how many operators you need to form a complete set in a given hilbert space . in fact , as pointed out by willie wong in his comment , one operator is in principle enough in a separable hilbert space . here is maybe a slightly less abstract example . you know that the states of a spin system can be characterized by specifying simultaneously the values of $\vec l^2$ and $l_z$ . ( i disregard the orbital degrees of freedom so that these two operators are enough . ) however , one can as well take the operator $\vec l^2+\alpha l_z$ where $\alpha$ is an arbitrary irrational number . you can easily convince yourself that the eigenvalue of this single operator is sufficient to label uniquely all the states $|l , m\rangle$ . as to your second question , i would simply refer you to any textbook on quantum mechanics . the fact that the eigenvalues of $\vec l^2$ and $l_z$ take the form $\hbar l ( l+1 ) $ ( where $l$ is a non-negative integer or half-integer ) and $\hbar m$ where $m\in\{-l , -l+1 , \dots , l-1 , l\}$ , is not merely a matter of a definition , but follows from the commutation relation for the angular momentum operator , $$ [ l_i , l_j ] =i\hbar\varepsilon_{ijk}l_k . $$
in principle , the operator $h$ is not a matrix . however , you can write down a matrix representation . for that , you need a basis , which should be complete ( =very large matrix , probably infinitely large ) - for many illustrating cases in quantum mechanics , it is not complete . if your basis consists of $|a\rangle$ and $|b\rangle$ , it is not complete , but you can still describe some effects nicely . you gave the example $h=|a\rangle\langle b| + |b\rangle\langle a|$ , which means that $h$ is ( it is just another way to write it ) $$h=\begin{pmatrix} 0 and 1\\ 1 and 0 \end{pmatrix} , $$ and you can calculate its eigenvalues . for a general 2x2 hamiltonian matrix , the formula is $$h=\sum_{i , j}c_{i , j}|i\rangle\langle j|=\begin{pmatrix} c_{1,1} and c_{1,2}\\ c_{2,1} and c_{2,2} \end{pmatrix}$$ $i$ and $j$ can take the value $a$ and $b$ . the matrix is a 2x2 matrix because the hamiltonian only contains two vectors , $a$ and $b$ . how to expand a hamiltonian operator into a matrix is explained nicely on wikipedia ( first four formulas of the section ) . if it is already in the abstract form $h=|a\rangle\langle b| + |b\rangle\langle a|$ , you can just read it off : the prefactor of $|a\rangle\langle a|$ ( $=0$ ) is the top left entry , the prefactor of $|a\rangle\langle b|$ ( $=1$ ) is the top left entry etc . you do not need to know the explicit form of basis in which the matrix is written . remember that the eigenvalues of a matrix are invariant to unitary transformations .
( i try to answer to my own question , after some reflections made with the help of luboš . ) for an incompressible and irrotational flow , the conditions $\nabla\times \boldsymbol u=\boldsymbol 0$ and $\nabla\cdot \boldsymbol u = 0$ imply , $\nabla^2\boldsymbol u =\boldsymbol 0$ . indeed : $$\nabla^2\boldsymbol u = \nabla ( \nabla\cdot\boldsymbol u ) -\nabla\times ( \nabla\times \boldsymbol u ) = \boldsymbol 0$$ this forces us to write down the navier-stokes equation for the motion of the fluid without the viscous term $\mu\nabla^2\boldsymbol u$ , no matter the viscosity : $$ \rho ( \partial_t \boldsymbol u + u\cdot\nabla \boldsymbol u ) = -\nabla p \ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ now , it could seem that this implies the flow is automatically a high-reynolds number flow ( for which we could have wrote down the same equation , but for a different reason : $\mu=\rho\nu\simeq 0$ , and this would have been an approximation ) . but , even if the viscosity is far from neglectable , we can make another kind of approximation , saying that the inertia , represented by the left-hand-side terms in n-s equation , can be neglected because of $re=ul\rho/\mu\ll 1$ ( this can happen in a lot of situations : microobjects , extra-slow flows , and - of course - high viscosity . in this case , the equations of motion become : $$-\nabla p = \boldsymbol 0\ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ which are , in fact , the equations we would arrive at if we started by the stokes equation ( for inertia-less flows ) for irrotational flows . then , an irrotational flow is not necessarily governed by the euler equation , i.e. , it is not necessarily inviscid .
i can not really know why your professor used c++ , but there are several reasons why you would : performance : scientific computations might require top-notch performance . c++ allows for very low level control over the hardware and has many possibilities for micro-optimization while still providing high-level abstraction . of course , this is also the reason that c++ is quite complicated . historical reasons : c++ has been the de facto general programming language for quite a while . your prof 's code might need to be compatible with other projects . plus , i think there are more libraries for c/c++ than for any other language , your professor might have to make use of such a library . it is also possible that c++ is the standard language for this kind of thing in your professor 's university or field , simply because it used to be the best choice . maybe your professor only knows c++ . . .
there are several ways to solve this . the simplest is to google for the formula for the focal length of two lenses in contact , but let 's instead do the problem step by step . this shows the situation with just one lens . we know th magnification is -3 , so we know that $v = -3u$ so the lens equation tells us ( i am taking +ve to be to the left i.e. $u$ is positive and $v$ is negative ) : $$ \frac{1}{u} - \frac{1}{v} = \frac{1}{f} $$ and since $v = -3u = -3d$ we have $f = 3d/4$ . now use the image from the first lens as the object for the second i.e. $u_2 = -3d$ and we know $f = 3d/4$ , then we have : $$ -\frac{1}{3d} - \frac{1}{v_2} = \frac{4}{3d} $$ and a quick rearrangement gives $v_2 = -3d/5$ and the magnification is $m = -3/5$ . the alternative is to google for the focal length of two lenses in contact , and amongst the hits will be this page telling you that the combined focal length is given by : $$ f = \frac{f_1 f_2}{f_1 + f_2} $$ or when the two lenses are identical : $$ f = \frac{f^2}{2f} = \frac{f}{2} $$ we have already worked out that $f = 3d/4$ , so $f = 3d/8$ , and using the lens equation we get : $$ \frac{1}{d} - \frac{1}{v} = \frac{8}{3d} $$ and solving for $v$ gives $v = -3d/5$ , just as we got before .
the givens : $\delta t$ , $d$ , $v_1$ , $v_2$ . and you demand that the acceleration is exponential . let $$ a ( t ) =a_0 e^{\omega t} $$ then if we say that $v_1=0$ we have after integrating $$ v ( t ) =\frac{a_0}{\omega} ( e^{\omega t}-1 ) $$ now if we say that the initial coordinate is zero ( $x_1=0$ ) , then $$ x ( t ) =\frac{a_0}{\omega^2} ( e^{\omega t}-1 ) -\frac{a_0 t}{\omega} $$ now you say that at $t_2$ , $x ( t_2 ) =d$ , and at $v ( t_2 ) =v_2$ , so $$ \frac{\omega^2 d}{a_0}+\omega t_2=e^{\omega t_2}-1 $$ and $$ \frac{\omega v_2}{a_0}=e^{\omega t_2}-1\implies \frac{\omega^2 d}{a_0}+ ( t_{2}-\frac{v_2}{a_0} ) \omega=0 $$ then $$ \omega=0 , \quad \frac{v_2 -a_0 t_2}{d} \equiv \omega_0 $$ then $$ x ( t ) =\frac{a_0}{\omega_{0}^2} ( e^{\omega_0 t}-1 ) -\frac{a_0 t}{\omega_0} $$ this gives you distance in terms of time . for time , set up $$ \frac{\omega_{0}^2 x}{a_0}+1+\omega_0 t=e^{\omega_0 t}=b+at=e^{at} $$ with solution $$t ( x ) =\frac{-\mathcal{w}\left ( -e^{-b}\right ) -b}{a}=\frac{-\mathcal{w}\left ( -\exp\left [ -\left ( \frac{\omega_{0}^2 x}{a_0}+1\right ) \right ] \right ) -\left ( \frac{\omega_{0}^2 x}{a_0}+1\right ) }{\omega_0} $$ with $\mathcal{w}$ the product log function . hopefully this one is helpful : )
first , note that in physics , we consider unitary representations $u$ of the poincare group acting on the hilbert space $\mathcal h$ of the theory because we are interested in a precise formulation of the concept of poincare transformations acting on the quantum mechanical states of the theory as symmetries ( since the laws of physics should be inertial frame-invariant ) ; and by wigner 's theorem , we choose these symmetries to be realized by unitary operators . these observations are related to your #1 and #3 and i think they should be kept conceptually distinct from the notion of a state that represents a single particle state . second , since such quantum field theories are supposed to allow for the emergence of states of particles , and in particular should account for states in which there is a single elementary particle , we expect that there is some subset $\mathcal h_1$ of the hilbert space of the theory corresponding to states " containing " a single elementary particle . given these observations , let 's rephrase your question as follows : what properties do we expect that the action of the representation $u$ will have when its domain is restricted to the subspace $\mathcal h_1$ ? in particular , we would like to justify the following statement the restriction of the unitary representation $u$ acting on $\mathcal h$ to the single-particle subspace $\mathcal h_1$ is an irreducible representation of the poincare group acting on $\mathcal h_1$ . this requires justifying two things : the restriction maps $\mathcal h_1$ into itself . the restriction is irreducible . i think that the justification of the first property is pretty intuitive . if all we are doing is applying a poincare transformation to the state of the system , namely we are just changing frames , then the number of particles in the state should not change . it would be pretty strange if you were to , for example , boost or rotate from one inertial frame into another and find that there are suddenly more particles in our system . the irredicibility requirement means that the only invariant subspace of the single particle subspace $\mathcal h_1$ is itself and $\{0\}$ . the physical intuition here is that since we are considering a subspace of the hilbert space in which there is a single elementary particle , expect that there is no non-trivial subspace of $\mathcal h_1$ in which vectors of this subspace are simply " rotated " into one another . if there were , then the particle would not be " elementary " in the sense that the non-trivial invariant subspace would represent the states of some " more elementary " particle . when it really comes down to it , however , i am not sure if there is some more fundamental justification for why the restriction of $u$ to $\mathcal h_1$ is irreducible aside from the decades of experience we have now had with particle physics and quantum field theory .
as the comments say , you have to be precise about your reference point when you talk about time dilation . time dilation is always relative to something else . but there is an obvious interpretation to your question . suppose you have an observer well outside the solar system and stationary with respect to the sun . for that observer your clock on earth is ticking slowly for two reasons : you are in a gravitional well so there is gravitational time dilation . you are on the earth which is hurtling round the sun at about ( it varies with position in the orbit ) 30 km/sec . the earth 's surface is also moving as the earth rotates , but the maximum velocity ( at the equator ) is only 0.46 km/sec so it is small compared to the orbital velocity and we will ignore it . as it happens the problem of combined gravitational and lotentz time dilation has been treated in the question how does time dilate in a gravitational field having a relative velocity of v with the field ? , but this has some heavy maths so let 's do a simplified calculation here . the gravitational time dilation , i.e. the factor that time slows relative to the observer outside the solar system is : $$ \frac{t}{t_0} = \sqrt{1 - \frac{2gm}{rc^2}} $$ where $m$ is the mass of the object and $r$ is the distance from it . for the sun $m = 1.9891 \times 10^{30}$ kilograms and $r$ ( the orbital radius of the earth ) $\approx 1.5 \times 10^{11}$ so the time dilation factor is $0.99999999017$ . for the earth $m = 5.97219 \times 10^{24}$ kilograms and $r$ ( the radius of the earth ) $\approx 6.4 \times 10^{6}$ so the time dilation factor is $0.999999999305$ . the lorentz factor due to the earth 's orbital motion is : $$\begin{align} \frac{1}{\gamma} and = \sqrt{1 - v^2/c^2} \\ and = 0.999999995 \end{align}$$ and to a first approximation we can simply multiply all these factors together to get the total time dilation factor : $$ \frac{t}{t_0} = 0.999999984 $$ to put this into context , in a lifetime of three score and ten you on earth would age about 34 seconds less than the observer watching from outside .
the length of the arc pq is $r\delta\theta$ , as feynman says , but the difference from $\tan\delta\theta$ or $2\tan ( \delta \theta/2 ) $ or something else is negligible because $\delta \theta$ is assumed to be infinitesimal ( infinitely small ) in the argument , anyway . for that reason , both angles opq and oqp should be taken to be 90 degrees .
the reason your calculation is not right is because the mean energy of the molecules hitting the wall is not the mean number times the mean energy per molecule , because the fast molecules hit the walls more frequently than the slow ones .
no , it does not propagate . a low frequency electric field is a near field - it may depend on time but its distance-dependence is such that this field decays as $1/r^2$ at best . the propagating part of a low-frequency field ( $\propto 1/r$ ) has a magnetic component too . edit : the time-dependent field $e ( t ) $ has two terms : one is a near field and another is a propagating field $$e ( t ) =a_1/r^2+a_2\omega/r . $$ the propagating field strength is proportional to the frequency $\omega$ and is accompanied with a propagating magnetic field of the same strength . they can be small . a near magnetic field is smaller than a near electric field due to low frequency . so it is possible to have a variable electric field without variable magnetic field , but it does not propagate in reality . light is a propagating emf .
two books to get you started in general computational radiation transport : computational methods of neutron transport , written by e.e. lewis , edited by w.f. , jr . miller , isbn 0-89448-452-4 monte carlo particle transport methods : neutron and photon calculations , written by ivan lux and laszlo koblinger , isbn 0-8493-6074-9 the supporting materials for the mcnp ( monte carlo n-particle ) transport code maintained by los alamos national laboratory may also be useful : http://mcnp.lanl.gov/ these references are not targeted at nuclear rockets , instead they focus on the basic principles of computational radiation transport . application to nuclear rockets , or any other particular system , is just a matter of constructing the right set of assumptions and constraints to answer your specific question .
my problem appears to be with the initial state of the system , which i have written as $$\left| \psi\right\rangle=\left|\psi_1\right\rangle\left|\psi_2\right\rangle+\left|\psi_2\right\rangle\left|\psi_1\right\rangle , $$ where $\left|\psi_1\right\rangle$ is packet from one source , and $\left|\psi_2\right\rangle$ is packet from another one . this state says that the system is in a superposition of states , in each of which one of the particles comes from one source , and another necessarily from another source . i.e. the system is highly entangled . such system could be created e.g. by some generator of pairs of particles with opposite momenta . but two independent sources are clearly not such a source of entangled pairs . as the particles are indistinguishable , and there is no symmetry which would allow us to determine that the particles come from different sources , we can not say which source the particle has come from . if we watch single particle emitting from the sources , they might come one after another from different sources , or they could repeatedly come from single source , then several times from another one . i.e. there is no rule that if one particle is from source a , next detected one is from source b . so , the initial state must be in the following form : $$\left|\psi\right\rangle=\left ( \left|\psi_1\right\rangle+e^{i\phi}\left|\psi_2\right\rangle\right ) \otimes\left ( e^{i\psi}\left|\psi_1\right\rangle+e^{i\chi}\left|\psi_2\right\rangle\right ) +\\ +\left ( e^{i\psi}\left|\psi_1\right\rangle+e^{i\chi}\left|\psi_2\right\rangle\right ) \otimes\left ( \left|\psi_1\right\rangle+e^{i\phi}\left|\psi_2\right\rangle\right ) , $$ where $\phi , \psi , \chi$ are constants , which depend on the experimental setup . now from the form of the initial state it is obvious that the interference pattern will be present , and it is confirmed by numerical simulation . thus , it appears that even in this multiparticle experiment particles interfere with themselves , rather than with each other , to produce visible pattern on the screen .
first a comment about the following statements made by kitchi and wouter : lines of force always have to be smooth , there can not be a sharp bend in them like in your second diagram and the comment by @kitchi basically says it all : lines of force should be continuous and differentiable and they should never intersect . these statements are incorrect without qualification , and here are two reasons why : the electric field lines of a point charge consist of rays all of whom intersect at the location of the point charge . moreover , the field is not smooth at the location of the point charge ( it is not even continuous there since there is a singularity ) . there is a discontinuity in the electric field in passing through a surface charge with charge density $\sigma$ , in fact the discontinuity is proportional to the surface charge density ; $$ ( \mathbf{e}_2 - \mathbf e_1 ) \cdot \mathbf n = \frac{\sigma}{\epsilon_0} $$ if you want to make some statement about smoothness of electric field lines , ( which you should try to avoid calling " lines of force" ) , then you really need to make some other qualifications that exclude such cases . second , these smoothness justifications are , to some extent , missing the forest for the trees so to speak . the crux of the issue is really addressed by richard . the way one obtains the field due to a charge distribution is by invoking the principle of superposition . this immediately rules out the second diagram you drew because if you simply add the fields of the two point charges vectorially , then you will see that the field lines look like you draw them in the first diagram .
welcome to the community wizzphiz . you do not state your age in your profile , but i would think &lt ; 20 ? there are no spontaneously physical electron positron pairs created from the vacuum . the reason is called " conservation of energy " it would take energy to create such a pair . the vacuum sea consists of " virtual particles " and the electrons going around the accelerator cannot " see " them as in addition to being virtual they are created and annihilated in a small delta ( time ) . in this link , which is a festschrift for a scientist , in paragraphs 2 and later there are a lot of explorations of electrons scattering off radiation , even very low black body radiation that exists in a vacuum , but these are not the dirac sea pairs you are asking about . the vacuum pairs do have experimental signatures in the casimir effect , and in the widening of the lamb shift . when one goes to general relativity the vacuum particles theoretically may become physical in accelerated systems but there is no solid experimental evidence of this . the beam energies in the accelerators we have are not in that ball park of acceleration .
there is absolutely no danger from the electromagnetic waves coming from the phone . they are nowhere near powerful enough to heat up the gasoline , much less cause it to explode . they also do not produce more static electricity than anything else . if there is any part of the phone that is hypothetically " dangerous " , it might be the battery , as you say . if the battery happens to fall out , and land precisely ( terminals-first ) onto a conductive surface , it can create a tiny spark which can ignite the vapors . there is much more danger from the static electricity from your own body when handling the gas nozzle . the best practice should be to lay one hand onto your car , then pick up the nozzle with the other hand and insert it into your tank . ( the same when removing the nozzle )
standard deviation adds uncertainties to the measured value : $23.3\pm 0.4\ , {\rm m}$ . one can quickly look at the error ( which has units of ${\rm m}$ in my case ) and think , the value could be as low as $22.9\ , {\rm m}$ or as high as $23.7\ , {\rm m}$ without much thinking . modifying this to being a percentage of the value would be confusing . plus it would be arbitrary . as martin beckett points out , since the celsius and kelvin scales have identical temperature spacing but differing 0-points , making the error ( std . dev . ) scale to the value would be ridiculous : $$20.0\pm0.1\ , {\rm c} \to 20.0\ , {\rm c}\pm0.5\%$$ $$293.0\pm0.1\ , {\rm k}\to293.0\ , {\rm k}\pm0.034\%$$ same value in units systems would lead to differing uncertainties ; this would bring about more confusion .
the following is a passage from diffusion in the standard map ( pdf ) by itzhack dana and shmuel fishman : a major difficulty in the analysis of chaotic behavior of hamiltonian systems is the proximity of chaotic and regular orbits on various scales [ 2 ] . thus , for $k \approx 1$ , the phase plane of ( 1 ) is an intricate mixture of regular and chaotic orbits [ 3 ] . for $k \gg 1$ chaotic orbits fill almost the entire phase plane , but " islets of stability " , in particular accelerator modes [ 2 , 7 ] , are known to exist for $k$ arbitrarily large . when a chaotic orbit approaches such an islet , it may wander near it in a " regular " fashion . the area of each islet generally decreases when $k$ increases . distinctive features of a stochastic or random motion , which allow a statistical description of it , are the rapid decay of correlations and diffusion . the decay of correlations in the chaotic region is related to the local instability , measure lyapunov characteristic exponent [ 1 , 2 , 8 ] . for $k \gg 1$ , the decay of correlations is actually exponential , with the decay exponent proportional to the lyapunov exponent [ 8 ] . the existence of a definite characteristic time for the decay of correlations and the resulting statistical independence generally imply diffusion in the unbounded direction of the action $i$ for $k &gt ; k_\mathrm{c}$ [ . . . ] . therefore a definite diffusion coefficient $d$ can be associated with the chaotic motion $$ d = \lim_{n\to\infty} \frac{\big\langle ( i_n-i_0 ) ^2\big\rangle}{n} , $$ where the average is taken over some ensemble of initial positions $ ( i_0 , \theta_0 ) $ within the chaotic region . for large $k$ , where the lyapunov exponent is large , diffusion was verified unambiguously [ 2 ] .
to answer your second question : yes , you can write any arbitrary function $f ( x ) $ as a linear combination of eigenfunctions of $\mathcal{p}$: $$f ( x ) =\underset{=f_+ ( x ) }{\underbrace{\frac{f ( x ) +f ( -x ) }{2}}}+\underset{=f_- ( x ) }{\underbrace{\frac{f ( x ) -f ( -x ) }{2}}}$$ with $\mathcal{p}f_+ ( x ) =f_+ ( x ) $ and $\mathcal{p}f_- ( x ) =-f_- ( x ) $ .
for 1d potentials , the sequence of bound state energy eigenvalues $e_n$ cannot grow faster than what happens in the case of an infinite well , i.e. $e_n$ cannot grow faster than $n^2$ .
what we call " laws of physics " have an evolutionary path . it really started with newton and the falling apple and slowly it evolved into complete mathematical models of experimental observations , called theories . from the observations and the theories conservation laws emerged . these laws are strictly obeyed within the framework that they have been validated . take the thermodynamic law " entropy remains the same or increases in closed systems " . the region of validity of the law was transformed when the atomic nature of matter became understood , and statistical mechanics became the underlying framework . there , from an absolute law it became an estimate of probability outcomes , which to all intents and purposes recreates the law for macroscopic systems . if the universe could emerge from nothing , what about physical laws ? as our observations and experiments advance , new mathematical frameworks appear which transmute the laws of the overlying frameworks : conservation of energy became the relativistic four vector energy which blended mass in the mixture . conservation of relativistically defined energy and momentum also became fuzzy instead of absolute due to the heisenberg uncertainty principle of quantum mechanics . so when we come to cosmology where there exist theoretical models of solutions of general relativity it is not surprising that apparent inconsistencies with conservation laws developed for different frameworks . at the moment there does not exist a theory of everything which quantizes gravity and includes the other three forces , weak , strong , electromagnetic that has been validated through all relevant observations , even though string theory offers such possibilities . it is therefore premature to be definitive of how the known conservation laws validated by our laboratory experiments will evolve in a cosmological setting . something and nothing have to be mathematically defined within the appropriate theoretical models .
note that it is the residue of the combination $j ( z ) \mathcal a ( z_0 , \bar{z_0} ) $ that we need : the poles come from the ope where the operators come together . now it could be that $j ( z ) \mathcal a ( z_0 , \bar{z_0} ) $ is more singular than $\frac{1}{z-z_0}$ at $z=z_0$ . only the simple poles contribute by the residue theorem . and if there is no simple pole ( maybe $\mathcal a$ is $1$ , and/or the combination is regular ) , then the answer will be zero .
there is no fallacy , you are just not being particularly careful . you need to include both the electric and magnetic forces of the right magnitude and a covariant result drops out . ( of course historically it went the other way around : people noticed that frame changes were messed up unless the transformation laws were different and this led to the development of special relativity . ) for simplicity let both beams be very thin and have equal uniform charge density $\rho$ in the rest frame and suppose they run exactly parallel separated by a distance $l$ . let the velocity of the beams be $v$ in the lab frame . rest frame : taking the usual gaussian pillbox gives the electric field of one beam at the location of the other as $$ \vec{e}_\text{rest} = \frac{\rho}{2\pi\epsilon_0 l} \hat{r} , $$ where $\hat{r}$ is the unit vector directed away from the source beam . thus the force on a single particle in the second beam ( charge $q$ ) is : $$ \vec{f}_\text{rest} = \frac{\mathrm{d}p}{\mathrm{d}t_\text{rest}} = \frac{\rho q}{2\pi\epsilon_0 l} \hat{r} . $$ lab frame : the charge density of the beam is enhanced by the relativistic $\gamma=1/\sqrt{1-v^2/c^2}$ factor . thus the electric field is : $$ \vec{e}_\text{lab} = \frac{\gamma \rho}{2\pi\epsilon_0 l} \hat{r} . $$ there is also a magnetic field of magnitude $$ b = \frac{\mu_0 \gamma\rho v}{2\pi l} $$ and directed so as to produce an attractive force . plugging these in the lorentz force formula $$ \vec{f}_\text{lab} = q ( \vec{e}_\text{lab} + \vec{v}\times\vec{b} ) = q\left ( \frac{\gamma \rho}{2\pi\epsilon_0 l} - \frac{\mu_0 \gamma\rho v^2}{2\pi l}\right ) \hat{r} = \frac{\rho q}{2\pi\epsilon_0 l}\gamma\left ( 1 - \epsilon_0\mu_0 v^2\right ) \hat{r} . $$ using $\epsilon_0 \mu_0 = c^{-2}$ this reduces to $\vec{f}_\text{lab} = \gamma^{-1} \vec{f}_\text{rest}$ which , on noting the relativistic time dilation $\mathrm{d}t_\text{lab} = \gamma \mathrm{d}t_\text{rest}$ , is exactly right ! note that i have used the fact that the force is orthogonal to the velocity implicity when writing the lorentz transformation law for the force . you can prove the covariance for general motions using the covariant formulation of em . lesson : relativity and electromagnetism go together like hand and glove !
the interpretation of the metric as anisotropy may or may not be physical , depending on your point of view . the main problem with this interpretation is that it requires two distinct metrics . one of the lasting influences of general relativity in other fields of physics is the idea of treating objects by thinking about their intrinsic geometry . so when you are dealing with the propagation of the wave in a medium , and you have a riemannian metric $g$ with respect to which the wave propagates exactly like $\partial_t^2 - \triangle$ , you want to use that riemannian metric $g$ as the preferred one when you are studying the equations . you are of course free to impose a background coordinate system and attach to it a secondary metric , but that secondary metric can be argued to be unphysical in the situation , if it does not actually affect the physics of the situation , but rather only the way your present your measurements . example : consider the case where your metric is $4 dx^2 + dy^2 + dz^2$ . if you are only given this metric , you would not say that " one unit " in the $x$ direction is the same length as " one unit " in the $y$ direction . in fact , you would be tempted to redefine your coordinate system with $x&#39 ; = \frac{1}{2}x$ such that the metric " look " more symmetric . in general , given any symmetric , positive definite bilinear form on a vector space , you can always find an orthonormal basis . so if the relevant geometry for the physics is described by a single riemannian metric , then there is automatically a notion of infinitesimal isotropy built-in to the physics . ( this is related to the notion of local lorentz covariance in relativity , as well as to the notion of frame bundles in riemannian geometry . ) the important thing to keep in mind is that the spectral theorem for positive definite bilinear forms can be stated in the following way : given any finite dimensional real vector space $v$ , and any two positive definite symmetric bilinear forms $a$ and $b$ on $v$ . we can find a basis $\{ e_1 , e_2 , e_3 , e_4\}$ in which $a$ and $b$ are simultaneously diagonalized . in other words , if you have a physical problem in which there are two distinct natural riemannian metrics used in the equations , what you have is that the two parts of the physics corresponding to the use of the two riemannian metrics are anisotropic relative to each other . another way of saying the same thing is that the wave propagation in a medium that is governed by $\partial_t^2 - \triangle$ is still uniform in all directions , it is just that now the correct notion of directions should be the one given by the riemannian metric . another example : imagine you are driving around on a plane . because of friction ( and other dissipative forces ) , to maintain constant velocity you need to keep the accelerator pedal depressed . now above you there is a pigeon . because of drag , for it to maintain constant air velocity it also needs to keep putting in energy . now there is a wind blowing at the height where the pigeon flies . from your point of view , the pigeon 's motion is anisotropic : because to maintain the same ground velocity with wind and against wind requires different amount of energy input . but from the pigeon 's point of view , your motion is anisotropic : the pigeon 's " stationary " coordinate system would be one that is being blown around by the wind ! with that said , from the point of view of studying the wave equation in a medium , the notion of ambient space isotropy is not all that important . so while the interpretation is perfectly valid , it is not something that you necessarily need to keep in mind . when one studies wave propagation in a non-linear medium ( keywords : crystal optics , elastic waves ) , the notion of anisotropy more often refers to the fact that the longitudinal components of a wave and the various polarisations of the traverse components of a wave can have anisotropic propagation behaviour relative to each other . like whoplisp wrote in his comments , the general case is governed by a system of equations that cannot be reduced to just the simple scalar wave equation .
as your calculations show , yes , it does . the reason for this is that the work performed by a force $f$ on an object is proportional to the displacement through which it is applied , $$w=f \delta x . $$ if an object is going faster , then for an given time interval $\delta t$ ( and thus a given impulse $i=f\delta t$ ) the displacement $\delta x=v\delta t$ is bigger , and you must perform more work .
there are a two levels for the answer : first , perturbative string theory actually has two different scales : the string scale in which the extended nature of the string comes into play ( even classically strings are different than particles ) , and the planck scale in which the quantum mechanics of the string ( including quantum gravity ) becomes important . the string energy scale is lower than the planck energy scale , their ratio is the string coupling constant ( which is proportional to $\hbar$ ) , so it is meaningful to talk about stringy physics in the sub-planckian regime . your mental picture then depends on the size of the compact geometry . when the cy manifolds are larger the planck length you can use perturbative string theory , in which case the notion of a manifold still kind of makes sense . when reaching the planck scale the string theory is no longer perturbative and large fluctuations in the geometry are part of the story . since we know quite a bit about non-perturbative string theory ( through dualities and non-renormalization theorems ) these expected phenomena can be observed and analyzed precisely . for example , topology change of the cy manifold is a smooth process that can be discussed very precisely in this framework . ( incidentally , in this context we see that in the transition region between two well-defined geometries there is no geometrical description at all - contrary to some people 's intuition about quantum geometry and spacetime foam and all the rest of that . the geometrical description fails and there is some well-defined procedure to do any well-defined calculation without any reference to any type of geometry whatsoever ) . the second level of answer is that even when discussing perturbative strings on sub-planckian cy manifolds , you have to keep in mind that this is a shorthand for something more precise and technical - ( 2,2 ) scft which has a limit in which it becomes a sigma model with a cy manifold as a target space . in english this means that even classical strings in some target space probe the geometry in different ways than point particles , and to describe their physics you need more complicated machinery than just differential geometry . as you suspected the fact that the string is extended means that it is more forgiving to non-smooth features of the manifold , even classically . this is encoded in the fact that the scft is better behaved than differential geometry on that space , and stays well defined even in the presence of some types of singularities ( orbifolds of space are a famous example ) . so , in summary " string theory on cy " really means string theory whose classical point particle limit ( valid when the manifold is very large and smooth ) reduces to cy manifold . good to remember in general that there is some poetic license taken sometimes when physicists describe their work .
there is a simple reason why we can consider variations on the whole $a$ rather than the quotient $c=a/g$ and the reason is following : all configurations that are $g$-equivalent have the same value of the action $s$ . that is what we mean by the statement that the theory has the symmetry $g$ . so the variation of the action $s$ in the directions that are equivalent to the action of a gauge transformation in $g$ vanish automatically , by the gauge invariance of the action ! the variation of the action $\delta s$ is therefore a combination of the variations $\delta a_\mu$ only of those kinds that are independent of the directions along $g$ . that is also why the equations of motion that we derive from $\delta s=0$ do not determine the evolution of the fields such as $a_\mu$ unambiguously out of the initial conditions : the equations of motion only constrain $f_{\mu\nu}$ and they always allow us to change $a_\mu$ in the future , by a gauge transformation . this ambiguity arises from the " flat directions of the action " . you could be studying $\delta s =0$ on the quotient $c=a/g$ only but it would be cumbersome and $\delta s$ would be physically and literally the same thing as it is on $a$ . it is the very point of introducing degrees of freedom which include one redundant one ( because of the gauge symmetry ) to simplify the picture . in fact , the equations of motion from $\delta s= 0$ on $a$ are manifestly lorentz-covariant etc . if you were trying to parameterize the space $c=a/g$ by some fields , you would probably have to impose some lorentz-breaking or otherwise unnatural conditions , e.g. $a_0=0$ , and the whole formalism would lose the manifest lorentz symmetry even though the actual phenomena , when looked at properly , would still obey the laws of relativity . so yes , the methods on $a$ and on $c=a/g$ are equivalent , and it is the calculus on $a$ that is the smarter one . if the formalism using $c=a/g$ were more convenient from all points of view , we would never talk about the gauge symmetry because it would be a totally counterproductive concept ! be sure that it is a very useful concept . i can not make sense out of the second part of the question . when we discuss infinitesimal variations of quantities such as energy , they should be linear combinations of the infinitesimal variations of the fields , like $du=\vec e\cdot d\vec d$ . but your expression is " doubly infinitesimal " , it is bilinear in $\delta x$ where $x$ is something , and terms this small can be neglected in the usual infinitesimal calculus in which $\delta a$ is sent to zero because they are of higher order . the energy ( and stress-energy tensor ) is gauge-invariant in electrodynamics , too , so its ( linear , first-order ) variation induced by gauge transformations is equal to zero . talking about some second-order " variations " seems completely misguided to me . also , less seriously , i feel uneasy about your usage of the word " orthogonal " . in general , one does not have an inner product ( needed to determine orthogonality ) on the full configuration space and it is really not needed for most questions of this sort . the directions in $a$ away from a slice that may be used as representatives of $c=a/g$ come in two types : those that are pure gauge transformations , in the direction of the $g$ " fibers " , and those that are not . most of them " are not " but any combination of those that are and those that are not " is not " again and no combination is really " fundamentally different " than others . so it is a bit meaningless to look for " orthogonal " directions to the directions along $g$ . finally , you ask whether " a " gauss law is related to the standard gauss law taught at school but you have not really explained what you mean by " a " gauss law . there is only one gauss ' law . it is the equation of motion obtained by varying the action with respect to $a_0$ , and it gives us something like ${\rm div}\ , \vec d = \rho$ . it is always the same law – which may be generalized to more complicated theories than electrodynamics , e.g. yang-mills theory . this equation ${\rm div}\ , \vec d =\rho$ is interesting because it does not contain any time derivatives . so it really does not dictate the time evolution of anything : it already constrains the initial state . one may prove that if the equation holds at $t=0$ , it will hold at any time : the time-derivative of the gauss ' law may be derived as a spatial derivative of other maxwell 's equations involving $\vec d$ . this non-dynamical = constraint character of the gauss ' law ( the fact it does not contain time derivative ) is related to the fact that this equation of motion is derived from the variation of a field that may be interpreted as a completely redundant one in a particular convention how to fix the gauge symmetry . that is why we can identify it with the statement that the states related by gauge transformations are treated as equivalent states by the theory . this is particularly clear in the quantum theory where we may a priori have states not annihilated by ${\rm div}\ , \vec d - \rho$ but only states that are annihilated by this operator , i.e. states that respect the equivalence of the states related by gauge transformations , may be considered physical .
imagine $\theta$ to be quite large , about 80$^\circ$ . any normal car would fall down ( rather slip down against the friction force from tires . ) but if the speed of the car is very high , the centrifugal force would prevent it from slipping ( imagine a horizontal roller coaster loop-the-loop ) . the frictional force would have to act up the incline in this case to counter a component of the weight down the incline . also , consider static friction , not kinetic . the friction is the sideways dragging of the tires and not related to their kinetic/rolling motion .
it is well-known that x-rays are blocked by metal . [ ref : kid 's science ] obviously the doctor wants to look at your internal organs , unobscured by a fuzzy outline of your house keys and pendant . so , the sign is requesting that you removing metal items from the external parts of your body , to allow visibility to the internal parts . ( mri is a totally different story . )
you have $2$ atomic ensembles . for each atomic ensemble , the ground state is $|\psi_0\rangle$ , and the excited state is $|\psi_1\rangle$ . the excited state is signaled by an idler photon . without beam splitter , if you detect only one photon ( for the whole set ) , you know which atomic ensemble is excited , so you have the state $|\psi_0\rangle |\psi_1\rangle$ or the state $|\psi_1\rangle |\psi_0\rangle$ however , if you merge the optical path of the two idlers photons ( with a beam splitter ) , and if you detect a photon , you are not able to say which atomic ensemble is in the excited state . the global state is entangled : $$|\psi\rangle = \frac{1}{\sqrt{2}} ( |\psi_0\rangle |\psi_1\rangle + e^{i\phi} |\psi_1\rangle |\psi_0\rangle ) \tag{4}$$ where $\phi$ is a phase factor due to the difference of optical path .
given that leftaroundabout and vonjd have addressed the fundamental place of the fourier transform in the formalism , let me talk a little about an experimental application . what is the shape and size of a atomic nucleus ? from rutherford we learned that the nucleus is rather a lot smaller than the atom as a whole . now , electron microscopy can just about provide vague picture of a medium or large atom as a out-of-focus ball , but there is no hope of employing that technique to something orders of magnitude smaller . what we do is scatter things off of the component parts of the nucleus . a nice reaction here is $$ e + a \to e + p + b $$ where $a$ represents that target nucleus and $b$ the remnant after we bounce a proton out . ( this is what nuclear physicists call " quasi-elastic scattering " . ) now , if ( 1 ) we are shooting a beam of electrons at a stationary target , ( 2 ) we have a precision measurement of the momenta of the incident and scattered electrons and the ejected proton , ( 3 ) we are willing to neglect excitation energy of the remnant nucleus , and ( 4 ) we assume that $p$ mostly did not interact with the remnant after being scattered , we know the momentum of the proton inside the nucleus at the time it was struck . collect enough statistics on this and we have sampled the proton momentum distribution of the nucleus . now , here 's the fun part : you can show that the spacial distribution of protons in the nucleus is the fourier transform of the momentum distribution . and bingo , a measurement of the size of the nucleus . do it with a polarized target and you can get info on the shape as well .
it will depend on a lot of factors . summer typically , otherwise-identical flats near the top , will be hotter than those near the bottom , for two reasons : 1 ) heat rises - so heat will rise from lower flats to upper flats . more accurately , the density of air decreases as temperature rises , so hotter air will tend to rise up through buildings , where convection is possible . 2 ) overshading is likely to be less , higher up : in summer , most of the heat in a typical flat will come from solar gain ( rather than , say , from internal gains from cooking , people , appliances ) . the more that windows are overshaded , the lower this solar gain is . flats low down will have their windows overshaded by neighbouring buildings , trees , and so on . flats higher up will see more sky from their windows ; so will have higher solar gain . winter much of the stuff above , particularly about heat rising , but also about solar gain , still applies in winter : although heating systems may now be the single largest source of heat , solar gains can still be relevant , if there are large south-facing windows . basements obviously , there is little or no solar gain - there may be some small windows at the footway ground level , but not much . however , as @anna-v says , there is the moderating effect of the ground itself , which acts as a large thermal store . this large thermal mass will act as a seasonal buffer , heating very slowly through spring and summer , and cooling slowly through autumn and winter , thus typically moderating both the hottest summer temperatures and the coldest winter temperatures . there are software packages , such as energyplus , and phpp that can model solar gain and the effects of thermal mass , at any time of year , for any location ; but note that will need a lot of input parameters to do a decent job of it .
the mistake you are making is in " daggering " the object $\omega_{\mu\nu}$ . for each $\mu , \nu = 0 , \dots 3$ , the symbol $\omega_{\mu\nu}$ is a real number , so its dagger ( which is really just complex conjugation in this case ) does nothing ; $ ( \omega_{\mu\nu} ) ^\dagger = \omega_{\mu\nu}$ . when we say that $\omega_{\mu\nu}$ is an antisymmetric real matrix , we really mean that the matrix with these numbers as components is such a matrix , not that $\omega_{\mu\nu}$ is a matrix for each $\mu$ and $\nu$ .
a tesla coil is in many ways the same as a transformer and hence if you know the working of one you can understand the working of the other . now in common transformers , the coils are couple tightly , so that a large amount of energy transfers from the primary to secondary . this works well at low voltages , but at high voltages the insulating air gap may suffer a dielectric breakdown , thereby becoming the reason for heavy losses . the tesla coil avoids this and hence can be used at very high voltages . a tesla coil consists of two lc oscillators very loosely coupled to each other . when a charged capacitor is connected to an inductor an electric current will flow from the capacitor through the inductor creating a magnetic field . when the electric field in the capacitor is exhausted the current stops and the magnetic field collapses . as the magnetic field collapses , it induces a current to flow in the inductor in the opposite direction to the original current . this new current charges the capacitor , creating a new electric field , equal but opposite to the original field . as long as the inductor and capacitor are connected the energy in the system will oscillate between the magnetic field and the electric field as the current constantly reverses . the figure below shows a common schematic of the tesla coil . when the switch is open , the cap is charged . now when the switch is closed , the action discussed earlier will cause a magnetic field to be built up in the primary inductor and this in turn will set up a field in the secondary . since the secondary has a large amount of turns , this will cause an extremely high e field to be built up thereby resulting in large voltages . the new coils have some additional components , but this is the very basic working of the tesla coil with minimal components .
first , $u$ is surely not " any non-singular matrix " . for a given basis , $u$ is almost completely determined i.e. unique . it contains $\gamma_2$ because it is derived from the only imaginary pauli matrix . because of the basic dirac algebra $$ \{ \gamma_\mu , \gamma_\nu \} = 2\cdot 1_{2\times 2} \cdot g_{\mu\nu} $$ one may see that $\gamma_0$ is hermitian , $\gamma_0=\gamma_0^\dagger$ , while the spatial ones are anti-hermitian , $\gamma_i=-\gamma_i^\dagger$ . in your identity , you want to relate $\gamma^\mu$ to its transposition $\gamma^{\mu t}$ . up to the sign that depends on the spatial or temporal character of $\mu$ , the transposition is the same thing as complex conjugation . so a related problem is whether the complex conjugate matrices $\gamma^{\mu*}$ can be related to $\gamma^\mu$ by something like a conjugation . and the answer is yes . the main fact behind the exercise is that $\sigma^2$ is the only imaginary pauli matrix , so complex conjugation of pauli matrices is equivalent to the conjugation by $\sigma^2$ with an extra sign . this may be easily generalized if you also include the temporal 0th component and if you use the normal basis . you should check the identity you want to verify in a particular convenient basis , i.e. with an explicit form of the gamma matrices . the verification is most convenient if you write the gamma matrices in block form , with $2\times 2$ blocks being either multiples of pauli matrices or the unit matrix . in a more general representation , the dirac gamma matrices differ from those in the particular basis you will have verified by a conjugation only , and this may only mean that $u$ is changed in the formula , but the essence of the conjugation is unchanged . these equations are important because $c$ is related to the charge conjugation – the replacement of particles by antiparticles ( e . g . exchange of electrons and positrons ) . mathematically , the most important part of the charge conjugation is complex conjugation which is why we needed to express the " complex conjugate gamma matrices as some conjugations of the normal ones " . theories with a symmetry between matter and antimatter are symmetric under c - the charge conjugation symmetry . spinors are mapped to $\psi\to c\psi$ etc . and the only hard part of the symmetry of the lagrangian is a step that requires you to conjugate the gamma matrices by $c$ which is why it is good that we have a way to simplify $c^{-1t} \gamma^\mu c^t$ .
here tension is zero for very sort span of time ( infinitesimally sort time ) , or for an instant only . when ever it moves away from the vertically top position it will fill tension again . so it will move in circular trajectory . and if we consider instantaneous velocity it is tangential . i think this clarify your doubt .
well , after symmetry breaking , all that remains is electromagnetic $u ( 1 ) $ , so the only generator that is truly a symmetry generator is $q$ . the fermions couple to the " higgs " via the yukawa coupling : $\mathcal{l}_y = -y_e^{ij} \bar l_{l , i} \phi e_{r , j} - y_u^{ij} \bar q_{l , i} \tilde{\phi} u_{r , j} - y_d^{ij} \bar q_{l , i} \phi d_{r , j} + h.c. \ , $ which mixes left and right handed fermions . here $l$ is the left-handed doublet $ ( e_l , \nu_l ) $ , and $e_r$ is the right-handed singlet . because both $l$ and $\phi$ transform under $su ( 2 ) _l$ , there is a symmetry . after symmetry breaking , $\mathcal{l}_m = -\frac{y_e^{ij} v}{\sqrt{2}} \bar e_{l , i} e_{r , j} -\frac{y_u^{ij} v}{\sqrt{2}} \bar u_{l , i} u_{r , j} -\frac{y_d^{ij} v}{\sqrt{2}} \bar d_{l , i} d_{r , j} + h.c. $ where $v$ is the higg 's vev . this is not invariant under $su ( 2 ) _l$ . the same thing happens with the gauge bosons that become massive , although there the interaction term comes from the covariant derivative acting on $\phi$ . finally , the potential for $\phi$ , ( the mexican hat ) is symmetric under su ( 2 ) , but the vacuum is not , because for the vacuum state , $\langle 0 | \phi | 0\rangle = ( 0 , v/\sqrt{2} ) $ , which is not invariant .
we have conservation of energy to contend with . the magnetic energy being released in the upper layers of the suns atmosphere originated in turbulent convection lower down . so in effect the upper layers of the sun ( upper here meaning roughly from the photosphere and downward ) are acting as a heat engine putting some energy into the suns magnetic field . so we can not " twist " magnetic field lines without providing the energy to do so . it would be possible to generate electricity from changes in the earth 's magnetic field caused by the impingement of solar wind born magnetic energy on the earths magnetic field . any change in the total flux of magnetic field going through a current loop , generates a voltage . in fact during geomagnetic storms there is a danger that currents induced in power transmission lines could cause serious damage to th electrical grid system . but this is not a practical way to generate energy , but rather a hazard that our infrastructure has to contend with .
to model a diffuse surface , imagine a house that is on fire inside ( ! ) so that everything inside is emitting light equally in all directions . you can also imagine a very hot oven or kiln in which the interior walls are aglow . now , if you look through the door of the house , the flux of light entering your eye is obviously proportional to the area of the door . in other words , proportional to the cross-sectional area of the column of light passing through the door on its way towards you . if you view the door from an angle , then the effective area has been reduced by a factor of $\cos\theta$ where $\theta$ is the incident angle . the door you see has gotten narrower by a factor of $\cos\theta$ , and the column of light similarly . to see this , draw a right triangle where the hypotenuse is the door , and one side lies parallel to the line between you and the door , while one side is perpendicular to that line . the angle between the hypotenuse and the second side is complimentary to the angle between the normal of the door and the line connecting you and the door . hence the third side has length $hypotenuse \times \cos\theta$ , you can see the geometry of the situation in this image : http://www.idav.ucdavis.edu/education/graphicsnotes/shading/img86.gif now the door was letting out light in all directions equally , so if we instead imagine the door emitting light in all directions equally ( * ) , then this is the same as a diffuse surface . ( * ) this equivalence , i believe , depends on the inverse-square fall-off of intensity .
those things are surely not enough to find the inner product $\langle q|p\rangle$ uniquely . for example , starting with the conventional $q , p$ , you may redefine them by a canonical transformation , for example by $$ q\to q'=q , \quad p\to p'= p + q^3 $$ then $p ' , q'$ obey all the four conditions in the same way as $p , q$ . they also have eigenstates and $|p'\rangle$ states are something else than $|p\rangle$ . in fact , eigenstates with a large eigenvalue $p'$ are close to $q$ eigenstates because $q^3$ easily dominates . the inner product – which is nothing else than the wave function of the $p'$ eigenstate written in the $q$ representation – will end up being different . it will be a complicated solution of the equation for $\psi$ saying that it is a $p+q^3$ eigenstate . the conditions you wrote can only tell you that $$\langle q | pq | p \rangle = \langle q | qp | p \rangle - i\hbar \langle q | p \rangle = ( qp - i\hbar ) \langle q | p \rangle $$ so they are only enough to say how $q$ acts on the $p$ eigenstates and vice versa , in this combination . but the inner product itself is not related to itself in any sense , so it can not be determined . let me mention that even if you imposed additional conditions that would say that $p$ and $q$ are physically what they should be – e.g. that they scale properly under the scaling of $q , p$ – the inner product would still be undetermined because at least the phase of the $|p\rangle$ and $|q\rangle$ eigenstates is arbitrary . in fact , even the " absolute value part of the " normalization is a matter of conventions that could be modified . more generally , and i would say that this point is often overlooked , many properties of " wave functions " similar to your inner products are intermediate , convention- and representation-dependent quantities that do not have a direct physical impact ( i.e. . direct link with observable quantities ) . it is really the properties of observables such as the four conditions that you described that may be considered objective facts .
define the operators $\hat a ( f ) =\sum f_j\hat a_j$ and $ |f_1 , . . . , f_n\rangle:=\hat a ( f_1 ) . . . \hat a ( f_n ) |vac\rangle$ . then $\langle g_1 , . . . , g_m|f_1 , . . . , f_n\rangle$ vanishes for $m\ne n$ and is a sum of the products $\langle g_1|f_{j_1}\rangle . . . \langle g_n|f_{j_n}\rangle$ for all possible permutations $ ( j_1 , . . . , j_n ) $ of $1 , . . . , n$ . note that the $\langle g|f\rangle$ are easy to compute . the formula you requested is a special case of this .
for relativistic time dilation to pull 13.8 billion years down to 6500 years , the object would have to have a lorentz factor of $\gamma = 2 \times 10^6$ , or be traveling at 0.9999999999998891 of lightspeed . at this speed , a collision with an interstellar hydrogen atom would yield about a pev , or 100x the energy released per collision at the lhc . it would produce a lot of higgs bosons in its flight around the cosmos , and would of course have to be made of unobtanium to survive . ; )
that rather depends on the accelerator and the intended purpose of the beam . but to choose one particular example the main electron accelerator at jefferson lab is the continuous electron beam accelerator facility ( cebaf ) which ( duh ! ) functions in a continuous fashion ( thought the electrons still comes in " bunches " because at any given time a rf klystron is only adding energy to particles found in part of it is length . the bunch spacing is ~2 ns in each of the three experimental halls and can be clearly reconstructed in some experiments . that said , the ability of cebaf to recirculate electrons in a continuous mode like that is a very special feature of the machine ( which to my knowledge is not duplicated anywhere else at this time ) . other than that , linear machines and synchrotrons can run in continuous mode , but accelerator rings machines must fill-n-spill .
the pressure coefficient at a certain point ( at which the value of the pressure is $p$ ) is defined as $c_p=\frac{p-p_\infty}{\frac12\rho_\infty u_\infty^2} , $ where the $\infty$-symbol denotes freestream quantities . for an incompressible and steady fluid and assuming zero viscosity , bernoulli 's equation is given by $p+\frac12\rho u^2=p_\infty+\frac12\rho u_\infty^2 , $ which we can rearrange as $\frac{p-p_\infty}{\frac12\rho u_\infty^2}=1-\frac{u^2}{u_\infty^2} . $ in order for this expression to be valid for given fluid , we have to show that it is incompressible and steady . incompressibility means that the laplacian of the flow potential $\phi$ vanishes , this can be shown to be true for the problem at hand . furthermore , a fluid is steady if its flow does not depend explicitely on time , which is also the case .
you are incorrect to suppose that this spacetime is curved . in fact , up to some conditions on the coordinate ranges , this is simply a piece of minkowski spacetime . let me put it in this form : $$ds^2 = dt^2 - t^2 ( d\psi^2 + \sinh^2\psi\ , d\omega^2 ) \text{ , }$$ where $d\omega^2 = d\theta^2 + \sin^2\theta\ , d\phi^2$ is the metric for a unit $2$-sphere , and we can go from spherical minkowski form $dt^2 - dr^2 - r^2\ , d\omega^2$ to yours by the substitution $$\begin{eqnarray*}r = t\sinh\psi and \quad\quad and t = t\cosh\psi\end{eqnarray*}$$ simply , the ricci tensor is zero because $r_{\mu\nu} = 0$ is a coordinate-invariant condition : if a tensor zero in some coordinate chart ( e . g . , minkowski ) , then it is zero in every coordinate chart ( e . g . , yours ) . naturally , the riemann tensor is also zero . interestingly , this is the also the hyperbolic flrw ( "big bang" ) metric in the limit of zero density , where a galaxies do not interact gravitationally but every galaxy sees itself a center of cosmological expansion . for details , see the milne universe model . . . . the article at the link below says that this interval corresponds to an open flat ( space curvature = -1/r^2 &lt ; 0 ) expanding universe ( look at the picture in the question ) . can you explain that ? sure : spacetime curvature is not the same thing as space curvature , or more generally the curvature of a manifold is not the same thing as the curvature of a submanifold of lower dimensionality . take euclidean space $e^3$ in spherical coordinates , $ds^2 = dr^2 + r^2d\omega^2$ , and look at submanifolds of constant radius : $$d\sigma^2 = r^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) \text{ , }\quad r = \text{constant}$$ one can verify by direct computation that the scalar curvature is positive , as one might expect from them being spheres . obviously , if you ' slice up ' $e^3$ by planes , each will have zero curvature instead . in your case , what is going on is just the same thing , except we are ' slicing up'/'foliating ' the minkowski spacetime $e^{1,3}$ by spacelike hypersurfaces of the form $$d\sigma^2 = t^2\left [ d\psi^2 + \sinh^2\psi\ , d\omega^2\right ] \text{ , }\quad t = \text{constant}$$ in this form , it is not hard to guess that the curvature for each hypersurface is same as above except negative . ( let me know if you need help calculating it explicitly , but for the moment i think intuitively motivating the fact that this makes sense would be more fruitful . ) since coordinate changes do not change the scalar curvature , we can let $\rho = t\sinh\psi$ to get $d\rho = t\cosh\psi\ , d\psi$ and $$t^2\ , d\psi^2 = \frac{d\rho^2}{\cosh^2\psi} = \frac{d\rho^2}{1 + \sinh^2\psi} = \frac{d\rho^2}{1+\rho^2/t^2}\text{ , }$$ putting it in the same form as ( 80 ) except for variable names ; the other part of the metric should be more obvious .
temperature is not a concept that has a lot of utility at the level of single atoms because it represents the mean kinetic energy of a group of particles ( to within a coefficient ) . you can define it , it just does not help much . at the level of two atoms you revert to a more fundamental model such as the forces between them . one atom transfers energy to another through electromagnetic forces between them . when that energy manifests as randomized kinetic energy at the microscopic scale we refer to it as " heat " at the macroscopic scale . in a solid it is usually reasonable to treat the forces between individual pairs of atoms as being spring-like ( i.e. . they obey $f_{i , j} = -k ( r_{i , j} - r_0 ) $ ) . starting from there you can build various models of solid behavior . for instance the einstein model of a crystal .
fusion is plasma physics , and in plasma physics the temperature is defined by the average kinetic energy of the ions and electrons in the plasma . therefore the kinetic energy distributions have to be measured and they have developed ingenious methods of doing so . fitting with the black body radiations curves allows an estimate of the temperature . there are more ingenious methods developed , using thomson scattering , which allow by scattering light on different ions accurate measurements of temperature . very high temperatures can be measured for the stars from their black body radiation and the spectral distribution of the light reaching us also .
faraday 's law of induction - one of maxwell 's equations - says $$\nabla \times e = - \frac{\partial b}{\partial t} . $$ so the time derivative of $b$ induces an electric field . however , what you incorrectly assumed was that the time derivative of $b$ will directly be proportional to the induced electric field . in reality , it is the " curl " of the electric field that is equal to the time derivative of $b$ . in other words , if $b$ and its time derivative is uniform in space , the field $e$ will not be uniform . for example , if $-\partial b / \partial t = ( 0,0 , \beta ) $ in the $z$-direction , the most natural $e$ will depend linearly on spatial coordinates , and may be e.g. $$ e = ( -y\beta/2 , x\beta/2 , 0 ) $$ this electric field is rotating around the $z$-axis - around the direction of the magnetic field ( its change in time ) . note that it is constructed so that its curl is equal to $ ( 0,0 , \beta ) $ . however , this configuration is also nicely rotational symmetric because up to the extra factor of $\beta$ , the vector $e$ is nothing else than the vector $r$ projected to the $xy$-plane and rotated by 90 degrees around the $z$-axis - and this definition does not depend on the choice of axes in the $xy$-plane , so the configuration is rotationally symmetric . it is not translationally symmetric in the $x$ and $y$ directions , however . there exists a specific axis - in my case , the $z$-axis - where the induced $e$ vanishes . the solution to maxwell 's equations is not unique . i may add any other uniform $e$ field ( or , more generally , any electromagnetic wave solution of maxwell 's equations with no right hand side ) without spoiling the validity of the maxwell 's equations - this is a kind of trivial artifact of the linearity of these equations .
afer the discussions in the comments this would be my solution : the l.h. s in the equations of motion is always simply $m\ddot{x}$ , the r.h. s contains the sum of all acting forces . so you have gravity and the force of the spring . thats it . $m\ddot{x}=mg-k ( x-x_{0} ) $
hooke 's law is frequently used to model multi-dimensional materials because the stress tensor is simple ( linear ) . the full expression can be found on wikipedia . the simplification for 2d is straight forward ( drop any terms with a 3 in the subscript ) . note that whether deformation in one dimension affects the others is a property of the material and shows up through poisson 's ratio ( $\nu$ ) . independence between deformations in x and y imply $\nu = 0$ if you imagine two perpendicular springs only , then the terms with $\gamma$ ( or different subscripts like 12 , 23 , 31 , depending on the form of the equation ) drop out of the expression as those are shear terms . the shear terms can be thought of as a spring across the diagonal . the stress tensor $\sigma$ is defined as the force per unit area .
i think you may be misguided by the concept that we associate $\textbf{observables}$ to self-adjoint operators . they do operate on the hilbert space , but to see them as entities that transform states or prepare them is a little bit tricky . i will describe here self-adjoint operators and preparation of states . 1 ) the true ( physical ) power of self-adjoint operators for describing observables lies in the spectral theorem , and not in its $\psi \mapsto a\psi$ action . physically , what it means ? there is a set called spectrum of an observable , and it is the set of possible outcomes on its measure for given states . for example , a spin observable $s$ on a 1/2-spin system has spectrum $\sigma ( s ) = \{-1/2 , +1/2\}$ , and decomposes as a sum of its spectral projections , $s = +1/2 p_{+} -1/2 p_{-}$ . in general , there is a spectral resolution $e$ , that is , a bunch of projection related to the spectrum , such that the operator can be written as $a = \int_{\sigma ( a ) }\lambda de ( \lambda ) $ . and what are the spectral projections ? those are again ( self-adjoint ) operators , but the whole collection of spectral projections will give you a probability measure when coupled with a state . in the spin system example , if you take a state $\psi$ , then $\langle\psi , p_+\psi\rangle$ would give you the probability of measuring a +1/2 spin , and likewise for -1/2 . now suppose you had a 1/2 spin system with prepared state $\psi$ , and you measure the spin , and get +1/2 . after the measurement , your state collapses to a $|+1/2\rangle$ state . in a more detailed formalism , suppose you have prepared a state $\psi$ and you are going to make a measurement of an observable expressed as $a = \int_{\sigma ( a ) } \lambda de ( \lambda ) $ ( where the $e$ is the spectral resolution of your operator , just think of the 1/2-spin example intuitively ) . then suppose your measurement is on a subset $\lambda \subset \sigma ( a ) $ ( you may think of the set $\{+1/2\} \subset \{-1/2 , +1/2\}$ . your state $\psi$ then collapses to the following state $\phi$: $\psi \rightarrow \phi = \frac{e ( \lambda ) \psi}{\|e ( \lambda ) \psi\|} . $ ( notice that $\phi$ is normalized and well defined , since $e ( \lambda ) \psi=0$ then the probability of the outcome being in $\lambda$ would be zero to start ) . summing up , you do not simply apply a self-adjoint operator on a state , since , as you have seen , it does not have much meaning . this is a point most introductory qm books do not stress as much as i would want . what happens with measurements and collapses and whatsoever uses , as i tried to point out , the spectral projections more than the operator itself . so , as you said about your hamiltonian operator , it does not act like your syrup machine , which we will try to cover up next . 2 ) now what you describe as " tools " , in your example , the putting syrup , is not a measurement per se , it is a preparation of states , which would grab a state without syrup and put syrup in it . the modeling of such procedure is usually ignored , at least to my knowledge . one choice would be just saying " my state now is syrup" , end of discussion . other option is using unitary operators ( $u$ such that $uu^* = u^*u = 1$ ) . those transform state vectors in state vectors . if you would like more sophisticated examples , it starts to get tricky , and i will shut up before i say something very wrong about it . but rest assured this is not easy at all , and your question is really nice . hope to see some other inspiring aswers .
my answer is more of a summary of insight that others have presented on a number of questions on physics . se . it is true that all astronomical bodies larger than a certain mass will take on a nearly spherical shape . the logical process to arrive at this conclusion involves several steps . i will try to enumerate these with the smallest number of non-trivial steps . material strength matters less and less as the scale of your system increases . a material yield strength ( or other definable limit ) has units of pressure ( like mpa ) . when material structure is acting to hold the shape of something against an external force , it offers some some anisotropy in the stress tensor . as we increase the size of a self-gravitating system , the scale on which this anisotropic component can matter becomes less and less . basically on larger scales , even rocky bodies behave more and more like a liquid than a solid . this does not preclude the existence of complex solid structures and mountains on the surface even though the vast majority the vast majority of the planet behaves mostly like a liquid ( see earth ) . when the material strength matters very little , then a single self-gravitating body will either : attain a shape that is consistent with hydrostatic equilibrium , or it will break up into pieces . in the presence of angular momentum , the hydrostatic equilibrium shape is non-spherical . most astronomical bodies ( but not all ) have sufficiently small angular momentum such that they are sphere-like . if the angular momentum is very high , it will break up into pieces , although practically it probably never forms in such a way to begin with . there are relatively few bodies spinning below the break-apart threshold but fast enough to be highly non-spherical for reasons that i do not fully understand . the kepler space telescope is offering new insights into the variety of planets including those with super-fast rotation and will likely shed new light on the subject . i have a hard time understanding what the star wars pictures are even trying to depict , but i will focus on lola sayu since i think i can make out what the picture is showing . the depiction is akin to an apple with a bite taken out of it . ( image license cc-by-sa-3.0 , wikimedia commons ) specifically , here are the various reasons such a shape is unphysical for a planet : the core of the planet is molten , therefore it behaves as a liquid , therefore it assumes a hydrostatic equilibrium shape , and the above shape is not included , qed . now , it would be over-generalizing without some qualifiers . the inner of most rocky planets is molten because of heat left over since its creation and internal heat production . we can potentially think of a sci-fi scenario where both of these will not be present ( just set it for a trillion years in the future ) . we defer to the next reason . even with the entire volume being fully solid , there is a separate , distinct , reason that mountains can not be higher than what the material properties will permit , and a planet with 1/4th of the matter blown away as in the case of lola sayu , the edges of that crater appear like a mountain to gravity . limited material strength cannot , against the gravity force , uphold shapes so severely deformed from the hydrostatic condition . as a final note , the pieces blow off from the planet in the picture are either in orbit , or they will be cleared away within a fairly small amount of time . most objects will probably not remain in orbit since they are ejected from the surface , and it will more than likely return to the surface at some point in the future , per : ( image license cc-by-sa-3.0 , wikimedia commons ) now , obviously you can not tell if something is in orbit from the picture ( since a still picture does not show movement ) , but it leaves plenty of unanswered questions . where did that mass of the planet go ? ! perhaps it blew away faster than escape velocity . either way , i am pretty sure none of the concerns mentioned here were given consideration in the creation of the artwork .
the rms ( root-mean square ) value of an ac voltage , which is what is represented as "110 v " or "120 v " or "240 v " is lower than the electricity 's peak voltage . alternating current has a sinusoidal voltage , that is how it alternates . so yes , it is more than it appears , but not by a terrific amount . 120 v rms turns out to be about 170 v peak-to-ground . i remember hearing once that it is current , not voltage , that is dangerous to the human body . this page describes it well . according to them , if more than 100 ma makes it through your body , ac or dc , you are probably dead . one of the reasons that ac might be considered more dangerous is that it arguably has more ways of getting into your body . since the voltage alternates , it can cause current to enter and exit your body even without a closed loop , since your body ( and what ground it is attached to ) has capacitance . dc cannot do that . also , ac is quite easily stepped up to higher voltages using transformers , while with dc that requires some relatively elaborate electronics . finally , while your skin has a fairly high resistance to protect you , and the air is also a terrific insulator as long as you are not touching any wires , sometimes the inductance of ac transformers can cause high-voltage sparks that break down the air and i imagine can get through your skin a bit as well . also , like you mentioned , the heart is controlled by electric pulses and repeated pulses of electricity can throw this off quite a bit and cause a heart attack . however , i do not think that this is unique to alternating current . i read once about an unfortunate young man that was learning about electricity and wanted to measure the resistance of his own body . he took a multimeter and set a lead to each thumb . by accident or by stupidity , he punctured both thumbs with the leads , and the small ( i imagine it to be 9 v ) battery in the multimeter caused a current in his bloodstream , and he died on the spot . so maybe ignorance is more dangerous than either ac or dc .
i have an answer to establish expectations from first principles . i have not looked up real values , nor am i hopeful of finding such values . all i am doing here is setting a general expectation for the difference in cross sections of ion-ion fusion and ion-atom or atom-atom fusion . i should note that the one glaring piece of information missing from the question is the energy of the reaction . as such , my answer will be somewhat non-specific on that point , but my position is that it does not affect my overall conclusion that no major reaction rate benefit could be obtained in fusion machines by this proposed mechanism . i used a number of simplifications to craft a simple algebraic form for the difference in cross sections . i should first specify though , i used a " radius " for the cross section defined as $a=\sqrt{\sigma / \pi}$ . yes , it is true that cross-sections are not really areas , but for the relative scales here it can be treated as such . now , here are the assumptions i used : fusion cross section &lt ; &lt ; atomic radius physics are identical to ion-ion interaction after atomic radii meet only the impact of cross section broadening considered , the increase in energy due to atom charge interaction not considered target nuclei does not move , you could try to eliminate this with reduced mass or a adjustment to reaction energy , i will leave that for someone else to try . this assumption is not really necessary but i just did not see a quick way around it . the basic proposition we have is that the ion-ion cross sections of the reactions are already known . this , naturally , depends on the energy . my proposition is that , whatever the ion-ion reaction trajectory is , it will be different from the ion-atom interaction by the electrostatic force operating at distances greater than the atomic radius . i hope my method id already starting to seem clear , but i will use an illustration . the proposition is that we know $a_{++}$ we know the profile of the $y-y_{++}$ with simple electrostatic physics . using the assumption that the atomic radius is small compared to the fusion cross section radius , i can claim $\sin{ \theta} = a/s$ . the electrostatic force from the net charges on the atom/ion will operate along the line between the moving atom/ion and the target atom/ion . the distance between these can be approximated to be equal to $s$ . if we do not bother with the speedup from this force , then we will just consider the component inward . $$f_{in} = \sin{\theta} \frac{ k q_1 q_2 }{ s^2 } = \frac{ k e^2 z_1 z_2 a }{ s^3 } $$ the speed of the incoming atom/ion is roughly constant , so i can give this approximate expression to get the above math in terms of time . this applies from $-\infty$ to $-r/v$ . the $v$ follows from the energy of the reaction . $$s ( t ) = - v t$$ then the basic kinematics follow logically . $$v_{in} ( t ) = \int_{-\infty}^{t} \frac{f_{in} ( t' ) }{m} dt ' = \frac{ k e^2 z_1 z_2 a }{2 v^3 m t^3}$$ $$ y ( r ) - y_{++} ( r ) = \int_{-\infty}^{-r/v} v_{in} ( t ) dt $$ $$ a - a_{++} = v_{in} ( r ) \frac{r}{v} + ( y ( r ) - y_{++} ( r ) ) = \frac{ k e^2 z_1 z_2 /r }{m v^2} a$$ this gives the difference in cross section radius due to atom-ion or atom-atom interaction beyond that atomic radius . this is a particularly useful form if we introduce $e_c=k e^2 / r$ and the kinetic energy of the incoming atom/ion . the difference in cross sections will be about twice this difference in radii . $$\sigma - \sigma_{++} = \pi ( a^2 - a_{++}^2 ) \approx - 2 \pi a^2 z_1 z_2 \frac{e_c}{e_k}$$ $$\frac{ \sigma - \sigma_{++} }{ \sigma_{++} } \approx 2 z_1 z_2 \frac{e_c}{e_k}$$ $$e_c = 0.027 kev$$ this makes intuitive sense . the cross section will be affected by an amount proportional to the electrostatic energy of the two touching atoms and inversely proportional to the kinetic energy of the interaction . the interaction energy in iter will be about $8 kev$ , and the optimal energy for dt fusion is $80 kev$ ( previous graph ) . if you had two neutral atoms interacting , the cross sections will be greater than the ion-ion interaction by 0.6 % at iter energies . it would be an improvement of 0.06 % at optimal dt energies . the best you could ever hope for would be the pb interaction with the b fully ionized ( +5 ) and the hydrogen with ( -1 ) . even this is likely unattainable as some comments have pointed out . if we look at this at iter energies ( impossible but this is best-case ) , that would increase the cross section by about 5% . pretty much all other scenarios would have less of an improvement than this . the bottom line is that the coulomb attraction at distances beyond the atomic radius is very very small compared with the coulomb repulsion that the nuclei have to overcome as they get closer than that . any adjustments to the cross section from this atom-ion or atom-atom interaction will really be &lt ; 1% for most conceivable fusion reactions .
the usual approximation for arithmatic of quantities with uncorrelated uncertainties that for small ( ish ) uncertainties $\delta x_i$ or measurements $x_i$ let 's us write for multiplicative operation $$ \begin{array}~ y and = and \left ( \frac{x_1}{x_2}\right ) \ , \text{ or }\ , \left ( x_1 x_2\right ) \\ \frac{\delta y}{y} and = and \sqrt{ \left ( \frac{\delta x_1}{x_1}\right ) ^2 + \left ( \frac{\delta x_2}{x_2}\right ) ^2 } \end{array} $$ ( i.e. . add relative uncertainties in quadrature ) . you can get this kind of result from a bastardization of the chain the rule . given $y = f ( x_1 , x_2 , \dots ) $ or each input measurement $x_i$ , compute $\left ( \frac{\partial f ( x_1 , x_2 , \dots ) }{\partial x_i}\right ) \delta x_i$ and add all the resulting terms in quadrature . you have also been a little free with your nomenclature here . call $s$ the underling signal and $n$ the random noise ( this can be counting statistics or any other random process such as shot noise in the detector , but not a constant bias which must be subtracted off--our noise is assumed to have a mean of zero ) with a distribution whose width is characterized by $\sigma$ . a single measurement is then $m_i = s_i + n_i$ , and the population has a signal to noise ration of $\frac{s}{\sigma}$ . the win from addition is that the sum of $n$ such measurements is $$ m = \sum_{i=1}^n m_i = ns + \sqrt{n}\sigma $$ meaning that the signal to noise ratio of the sum is $\frac{ns}{\sqrt{n}\sigma} = \sqrt{n}\left ( \frac{s}{\sigma}\right ) $ , an improvement .
it would be helpful to include what the x/y axes represent -- the physics of the problem may suggest an answer . for a generic approach you could just ( explicitly ) construct $y'=\ln y$ , $x'=\ln x$ and then do polynomial fits to this data $x ' , y'$ . a functional form that might be worth considering is $y = c x^{a +b x}$ which could give you the curvature you are seeing in the log-log plot , but i would only go there if there was some physical motivation for the $x^{bx}$ behaviour .
in my view , the important question to answer here is a special case of the more general question given a space $m$ , what are the physically allowable wavefunctions for a particle moving on $m$ ? aside from issues of smoothness wavefunctions ( which can be tricky ; consider the dirac delta potential well on the real line for example ) , as far as i can tell there are precisely two other conditions that one needs to consider : does the wavefunction in question satisfy the desired boundary conditions ? is the wavefunction in question square integrable ? if a wavefunction satisfies these properties , then i would be inclined to assert that it is physically allowable . in your case where $m$ is the circle $s^1$ , the constant solution is smooth , satisfies the appropriate conditions to be a function on the circle ( periodicity ) , and is square integrable , so it is a physically allowed state . it also happens to be an eigenvector of the hamiltonian operator with zero eigenvalue ; there is nothing wrong with a state having zero energy .
dear john , note that 23.85 å is equal to 2.385 nm , while the observed 4.3 nm is approximately two times larger . there is a simple error in your calculation that exactly fixes the factor of two . note that the actual calculation you should have done has a radius proportional to $1/m$ and the correct $m$ that you should substitute is the reduced mass of the two-body problem governing the relative position of the two particles . http://en.wikipedia.org/wiki/reduced_mass the reduced mass is $m_1 m_2 / ( m_1+m_2 ) $ . now , the important point is that an exciton is not a bound state of the effective electron and a superheavy nucleus : instead , it is a bound state of an effective electron and an effective hole - a larger counterpart of the positronium ( an electron-positron bound state ) . http://en.wikipedia.org/wiki/exciton assuming that both the electron and hole masses are equal , 0.26 $m_0$ , the reduced ( and still also effective ) mass is 0.26/2 $m_0$ = 0.13 $m_0$ , and the resulting $a$ is twice as big as your result , 4.77 nm - assuming that your arithmetics is right . the deviation from 4.3 nm is not too large but i can only handwave if i were trying to pinpoint the most important source of the discrepancy . it could be a different effective mass of the hole ; finite-size effects caused by the fact that the silicon atoms were not quite uniformly distributed inside the exciton , and so on . update oh , in fact , i noticed that your properties table does include a special figure of the effective hole 's mass and it differs from the electron mass : 0.38 $m_0$ . so the reduced mass is $$\frac{0.38\times 0.26}{0.38+0.26} m_0 = \frac{0.0988}{0.64} m_0 = 0.154 m_0 $$ and the calculated radius is $$ \frac{11.7}{0.154} \times 0.53\ &#197 ; = 40.3\ &#197 ; . $$ well , this is 7 percent too small , much like the previous one was 7 percent too big . ; - ) hydrogen atoms with composite heavy fermions concerning your second question , as you clearly realize , the calculated radius of the " atom " with such " heavy electrons " would be much smaller than the ordinary atom . this also proves that the assumptions of such a calculation fail : the heavy fermions ( in condensed matter physics ) are the result of the collective action of many atoms on the electron and its mass . so the large mass of the heavy fermions is only appropriate for questions about physics at long distances - much longer than the ordinary atom . if you look at very short distances - a would-be tiny atom with the heavy fermion - you cannot use the long-distance or low-energy effective approximations of condensed matter physics . you have to return to the more fundamental , short-distance or high-energy description which sees electrons again . at any rate , you will find out that there can be no supertiny atoms created out of the effective particles such as heavy fermions . the validity of all such phenomenological effective theories - such as those with heavy fermions - is limited to phenomena at distances longer than a certain specific cutoff and highly sub-atomic distances surely violate this condition , so one must use a more accurate theory than this effective theory , and in those more effective theories , most of the fancy emergent condensed matter objects disappear . non-relativistic effective theories just a disclaimer for particle physicists : in this condensed matter setup , we are talking about non-relativistic theories so the maximum allowed energy $e$ of quasiparticles does not have to be $pc$ where $p$ is the maximum allowed momentum in the effective theory . in other words , we can not assume $v/c=o ( 1 ) $ . quite on the contrary , the validity of such effective theories in condensed matter physics typically depends on the velocities ' being much smaller than the speed of light , too . so the mass of the heavy fermions is much greater than $m_0$ which would make $m_e c^2$ much greater than $m_0 c^2$ ; however , the latter is not a relevant formula for energy in non-relativistic theories . instead , $p^2/2m_e$ , which is ( for heavy fermions ) much smaller than the kinetic energy of electrons , is relevant . the maximum allowed $p$ of these quasiparticles is much larger than $\hbar/r_{\rm bohr}$ - the de broglie wavelength must be longer than the bohr radius . that makes $p^2/2m_e$ really tiny relatively to the hydrogen ionization energy .
see the wiki article on polarized 3d glasses . most likely , you have a pair of circularly polarized glasses . the mirror reverses the circular polarization . the article on circular polarization does it better than i would be likely to achieve in less than an hour or two . or hyperphysics , or google .
their race is ill-defined . you can not declare a winner if you can not agree on the ordering of events . if they failed to pick a reference frame for the race before starting it , then of course an argument over the winner may ensue . no laws of physics have been violated . if you try and extend this to " malfunctioning machines " , then yes , a machine that was not designed with special relativity taken into account ( like the race was designed without sr ) will give unexpected results . but again , no laws of physics will be broken . as to shooting people . . . that is murder , whether the bullet left the gun before you pulled the trigger or after .
in addition to what you have listed : use the buckingham pi theorem to create as many non-dimensional numbers as possible from combinations of dimensional numbers . this simplifies the expression but also allows you to reduce the number of variations on variables that need studied . non-dimensionalize all of your variables using suitable reference measures for the problem you are studying . use this to do an order-of-magnitude assessment of terms to decide if some terms are insignificant under certain conditions . perform a perturbation analysis . for example , if you have an equation for a wave , $\psi$ , substitute in $\psi = \overline{\psi}+\psi'$ where $\overline{\psi}$ is the average wave value ( in time , space or any of your other independent variables ) and $\psi'$ is a disturbance on that mean . then expand all terms and collect the means together and the perturbations together . do a lot of manipulation and you will get an expression for the mean behavior and the response to disturbances . this is not always " simpler " but gives tremendous insight . you could then substitute in simple functions , like trigonometric functions , for that disturbance and study how it grows or shrinks and under what conditions . learn what types of terms do what . which terms transport or convect your variable ? which terms produce or dissipate your variable ? these types of terms usually have typical forms in terms of derivatives . you can group terms together to come up with a simple conservation equation ( time change = transport + production - dissipation ) where all those terms can be lumped together . you can then attempt models for each individual group of terms . of course , all of these can be combined with one another to do all sorts of complex study .
the doppler shift causes a shift in wavelength at the origin of the wave ( the frequency of the source never changes ) . this results in a shift in frequency for the observer . in the link below you can see the emission of the wave for a moving source causes the wavelength to be shorter in front and longer behind . the actual source is not changing in frequency . so it is a matter of relativity . to the traveling observer ( in the source ) , only the wavelength is changing , to the stationary observer ( experiencing the doppler shift ) both frequency and wavelength have changed . lookang , wikimedia commons . more simulations and applets here .
all matter is made of waves —at least it can be represented that way , and it behaves that way . of course , matter also behaves like particles . this is one of the odd-but-true conclusions of quantum mechanics . the de broglie wavelength gives the wavelength of any " matter wave . " these waves are not waves in the classical sense with amplitude and the like ; they are wave functions , which express the probable location of a particle as something that looks like a wave . you can think of it in a sense that , looking very closely , the location and motion of a particle becomes blurry and starts to look like a wave instead of just one point . we usually think of matter as a wave only when making observations on a scale comparable to the de broglie wavelength , which is very small for most things . using the de broglie relation $\lambda=\frac{h}{p}$ , you can calculate the wavelength of a particle with momentum $p$ . as an example , a electron with a kinetic energy of 10 ev has a wavelength of 0.39 nm . that is the wavelength of the electron 's wavefunction . if you perform an experiment that would highlight that wave behavior , such as pass a beam of electrons of that energy through a diffraction grating with a spacing comparable to that wavelength , you would see an interference pattern , just as you do with light , because electrons behave like waves ( when we want them to ) .
the book says " if at a moment δt later the angle of the whole object has turned through δθ . . . " a " moment " in this context is meant to convey a vanishingly short period of time , not an indefinitely long one , which means that δθ is taken to be a vanishingly small angle . furthermore , feynman ( and anyone else ) is free to use whatever notations they like ; there is no " law " that dictates dθ has to be used in this context .
firstly the link you have provided for the paper is only accessible for those who have a subscription to the aps journal . so maybe instead you could just give a preview of the equation you are referring to . but i can give you a general overview on projections in quantum mechanics and bell states . i will use dirac 's bra-ket notation for the rest of this post : mathematically speaking , a projection of e.g. one state $\left|v\right&gt ; $ onto another state $\left|u\right&gt ; $ , is just written as follows : $$p_{u} := \left\langle u\right| v\rangle \left|u\right&gt ; $$ where $p_u$ stands for projection onto $\left|u\right&gt ; $ and not a probability . the $\left\langle u\right| v\rangle$ term is just the inner product between the two kets ( states ) in dirac 's notation . now in a physical sense , projection of states in quantum mechanics results from simple measurement of a system 's observables , e.g. a photon 's polarization , in a superposition of states $\left|\psi\right&gt ; = \alpha \left|h\right&gt ; + \beta \left|v\right&gt ; $ once measured , can collapse ( be projected ) onto one of its eigenstates , being horizontal or vertical polarization in this case . as for bell states , each bell state describes a unique maximally entangled state of two qubits ( photons e.g. ) . meaning all 4 bell states together : ( a and b to distinguish between the photons ) $$\left|\phi_{\pm}^{ab}\right&gt ; = \frac{1}{\sqrt{2}}\left ( \left|h^{a} , h^{b}\right&gt ; \pm \left|v^{a} , v^{b}\right&gt ; \right ) $$ $$\left|\psi_{\pm}^{ab}\right&gt ; = \frac{1}{\sqrt{2}}\left ( \left|h^{a} , v^{b}\right&gt ; \pm \left|v^{a} , h^{b}\right&gt ; \right ) $$ form a basis in the hilbert space $h_4^{ab}$ of 2-qubit entangled states . so as shown before , here one can also talk about the projection of two qubit states onto a given bell state , using the same formula . an example to showcase : if the bell states are said to form a complete basis for 2-qubit states , then for any given 2-qubit state , one should be able to decompose and express it in terms of superpositon of bell states . simply meaning that you project the 2-qubit state on each one of the bell states . for example lets see whether the state $\left|v\right&gt ; =\left|h^{a} , v^{b}\right&gt ; $ has a non-vanishing probability of collapsing ( being projected after measurement ) onto the first bell state : \begin{align*} \left\langle v\right| \phi_+^{ab}\rangle and = \frac{1}{\sqrt{2}} \left\langle v^{b} , h^{a}\right| \left ( \left|h^{a} , h^{b}\right&gt ; + \left|v^{a} , v^{b}\right&gt ; \right ) \\ and =\frac{1}{\sqrt{2}}\left ( \left\langle v^{b} , h^{a}\right| \left|h^{a} , h^{b}\right&gt ; + \left\langle v^{b} , h^{a}\right| \left|v^{a} , v^{b}\right&gt ; \right ) \\ and =\frac{1}{\sqrt{2}}\left ( \left\langle h^{a}\right| h^{a}\rangle \left\langle v^{b}\right| h^{b}\rangle + \left\langle h^{a}\right| v^{a}\rangle \left\langle v^{b}\right| v^{b}\rangle\right ) \\ and = 0 \end{align*} so we see that such projection is impossible , a result which is expected as $\left|h^{a} , v^{b}\right&gt ; $ is not an entangled state since it can be written down as a single outer product of the individual kets . i hope this overview gives you enough insight to be able to follow the mathematical steps involved in the paper you are reading .
as your read suggestion suggested , you can approximate physical reality with " no collisions are simultaneous " . the reason is that the physical world is full of indeterminacies ( aka errors ) due to thermal fluctuations and many other sources . what this means is that , even if the strict mathematical solutions that you will find by assuming simultaneous collisions versus assuming random collisions ( treating all collisions that your algorithm thinks are simultaneous as happening in random order ) could differ , the " physical " behavior of the system will not change in any meaningful way . no human could tell the difference between the two . in conclusion , you are safe in treating the system of collisions as non-simultaneous , and the consequences will be undetectable for the human eye ( including any real physicist 's real world measurements )
entanglement is a quantum correlation between two ( or many ) objects - a correlation means that these two objects ' properties are not independent of each other - which was created in the objects ' common past when they were close to one another i.e. when they were two parts of the same physical system . quantum mechanics changes the character of possible " properties " that objects may have ( the quantities describing objects 's properties are usually called " observables " and they are represented by hermitian operators on the hilbert space ) , as well as the way how these properties are measured and predicted ( just probabilistically ) , so it also changes the character and magnitude of correlations that the objects may exhibit . in particular , quantum correlations may often be stronger - and affecting a large fraction of measurable properties of the objects - than what would be possible according to classical ( i.e. . non-quantum ) physics . in classical physics , correlations have to satisfy e.g. the so-called bell 's inequalities in various situations but quantum mechanics - and the real world - can easily surpass these bounds . technically , objects and their properties in quantum mechanics are described by wave functions . to describe the state of two mostly independent objects , one has to take a wave function from the tensor product $h_1\otimes h_2$ of the hilbert spaces describing the individual objects . the wave function in the tensor product implies probabilistic predictions for any pairs of properties of the first and second object ; in general , they are not independent , and for each combination of the objects ' properties , quantum mechanics ( and the wave function ) may remember an independent probability . any vector in the tensor product that can not be written as a tensor product of vectors from $h_1$ and $h_2$ ( instead , it can only be written as a linear combination of such tensor products of vectors ) is called entangled . in other words , it is non-entangled if it is a simple tensor product of two simpler vectors . if it is a simple tensor product , all probabilities of " coupled properties " of the pair of objects simply factorize to the probability of the first object , and probability of the second object , as you know from probabilities of independent phenomena . the simplest pedagogical example of an entangled state ( as well as the entangled state that is most often found in literature ) is $$\frac{x_1\otimes y_1 + x_2\otimes y_2}{\sqrt{2}}$$ because there are two terms with four different factors , you can not use the distributive law in any way that would allow you to rewrite it as a simple product . the letters $x , y$ refer to the two objects and the labels $1,2$ refer to two different states of each of the two objects . in this state , if the property "1 or 2" is measured on $x$ , one obtains the answers 1 or 2 with 50% probability for each : the coefficient of the wave function is $1/\sqrt{2}$ because these complex coefficients have to be squared to obtain the probability . however , because $x_1$ is " coupled " to $y_1$ and $x_2$ is coupled to $y_2$ , the state and the machinery of quantum mechanics predict that the object $y$ will be measured to have the same property : if $x$ is in 1 , $y$ is in 1 , and the same for the state 2 . linear algebra - which is crucially important for quantum mechanics - allows one to reinterpret the state above as an " identity operator " so the correlation will exist regardless of the type of measurement that we perform both on $x$ and $y$ . for example , if the two states represent spins , the two particles will be correlated so that you will find out that they are polarized with respect to the same axis , if you measure both particles ' polarizations with respect to the same , particular , but arbitrary axis . this would be kind of impossible for two separated particles in classical physics that could only be perfectly correlated for one choice of the axis - but not another axis rotated by 45 degrees , for example - without a communication in between them . however , quantum mechanics predicts that such a 100% correlation " regardless of the axis " is not only possible but guaranteed by the state above and it requires no communication . indeed , one can prove that relativistic theories in quantum mechanics - especially quantum field theory - do not allow one to transmit a single bit of information faster than light even though this would be needed in classical physics to guarantee the perfect correlation that quantum mechanics predicts for these experiments ( and that the experiments confirm ) .
your last sentence answers your question . we observe lorentz symmetry in the laws of nature . therefore , we demand that the building blocks of our theory transform in definite representations of the lorentz ( or rather poincaré ) group . would you allow fields that are not representations of the lorentz group , it would become extremely hard to construct a theory that looks lorentz invariant .
first , check this reference on wikipedia . now , it is generally true that the " speed " ( or , more accurately , the dispersion relation ) of any particle is affected by a medium , where it travels . well , of course , if the particle interacts with the medium . for neutrinos the " slowing down " itself is absolutely negligible even in very dense media . what is important is that interaction of the electron neutrino with ordinary matter is much stronger , so it affects the patterns of neutrino oscillations -- the effect is known as msw effect . finally , this is not particularly related to supernovae . the idea behind interest in supernovae is that during an explosion there are a lot of neutrinos so one must account for the " neutrino matter " and its effect on oscillations as well .
you have different questions in the title and in the body of your post and i will answer both . electronic excitation happens very fast compared to nuclear motion so the molecule remains " frozen " when it is brought from an equilibrium to an excited state . if the equilibrium geometry of the excited state is very similar to the ground state then the molecule will remain still , but if this is not the case , the molecule will start to oscillate around the equilibrium point . this is known as frank-condon principle and wikipedia article has nice figures to illustrate this point . exciting different electrons leads to different electronic states that have different equilibrium geometries so you cannot expect same vibrational distributions . in your title you also ask about relaxation of excited molecule . since the energy of the x-ray photon is much higher than the ionization energy , the excited molecule will be ionized before it can relax vibrationally .
the average of any quantity $s$ is $\frac{\sum\limits_{r=0}^ns_r}{n}$ . if the distribution is continuous , lets say as a function of x , then it becomes $\lim\limits_{n\to\infty}\frac{\sum\limits_{r=0}^ns_r}{n}$ . this can be rewritten as $\frac{\int s ( x ) dx}{\int dx}$ , taking limits as the length of the wire . in your formula , i do not see any $x$ term in the rhs , nor anything that could depend on x , so i do not see how we can proceed . please specify what is constant and what is a function of x . so the final formula is $$\frac{\int t ( x ) dx}{\int dx}$$ if your wire is infinite , you may need to take limits 0 to y , and then limit the expression for average as $y\to\infty$ . update : with the updated formula , assuming the wire spans from x=0 to x=l , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\tanh ( ml ) }{ml}-1\right ) $$ if the wire spans from 0 to y , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\sinh ( my ) }{my\cosh ( my ) }-1\right ) $$ . limiting y to infinity gives us an infinite answer . so i am assuming that i have interpreted it correctly in my previous answer .
to the last comment - i disagree . wind turbines operate in a fairly different flow regime , and compare the table fan to the prop of a ship , they are about the same shape . the key difference is moving fluid volume . the larger blades push against the fluid more strongly , which is desirable in the fan case and in the ship case . wind turbines are different as the builder cares nothing about the exhaust wind . anyway . . . 1 . i do not think it is ' functional ' in any sense other than the fact that the motor is behind it . 2 . i do not believe so and i agree with other commentators that the fan would be more simple and less noisy without them . i will withhold comment on the obvious safety considerations . 3 . everything dealing with pumping or pushing a fluid makes noise , and it is quite considerable . you could remove the blades and listen to the electric motor itself and you would find it constitutes only a small fraction of the noise . noise minimization for fans is a major engineering topic and it is more or less impossible to reduce it . obviously , the faster you get the bigger of a problem it is . i believe we are talking about turbulent flow in general , i do not think many pumps are actually laminar . the metal bars will have an influence , but i could not say how much .
see e.g. page 3 of http://arxiv.org/abs/0707.3400 it is nonsensical to attribute this simple particular insight to a " discoverer" ; all these considerations should be associated with ludwig boltzmann who knew the answer even though the information in physics was considered continuous at that time . one may easily derive the result . for example , put one molecule of an ideal gas in a vessel , learn in which half of the vessel the molecule is ( one bit of information ) , and put a barrier in the middle . you will be able to allow the molecule to do the work and expand from $v/2$ to the original volume $v$ . the molecule will do the work $$ w = \int_{v/2}^v p\ , dv = \int_{v/2}^v \frac{kt}{v}dv = kt \ln \frac{v}{v/2} = kt\ln 2 $$ where i used $pv = nkt$ for $n=1$ molecule of an ideal gas . more generally , you do not have to consider ideal gas . just recall how work is related to the free energy , $e-ts$ . to reduce the entropy of a subsystem by one bit , i.e. by $k\ln 2$ ( look at boltzmann 's tomb formula to know why it is this value ) , we have to do change $e-ts$ by $kt\ln 2$ .
presumably each axle is rigidly attached to its gear ( no bending or breaking ) . this means that the torque on the gear is the same as the torque on its axle . so you can ignore the axles and just think about the gears themselves . and in that case , you can just use the torque ratio of the gears themselves .
these things do happen . the phenomena is called multipath interference . you do not notice it much in ordinary life becase there is rarely a situation where sound is not being scattered from lots of different surfaces and that the frequency is pure enough so that there exists a single null point for all the sound . note that the nodes and anti-nodes caused by the multipath interference move around as a function of frequency . ordinary sounds contains a wide range of frequencies such that even if one specific tone is partly nulled out , we usually do not notice . you can create situations without a fancy lab where you can observe interference effects yourself . for example , in college we put a speaker at one end of a long hallway and drove it with a frequency generator . at the right frequency , which was 43 hz for that hallway if i remember right , there were nodes and anti-nodes dues to standing waves . you could of course still hear the tone at the anti-nodes , but it was significantly softer than at the nodes . the effect was strong enough that if you were at one of the nodes and started walking toward the speaker , the sound would get softer and you had think it was coming from the other direction . it was great fun to see peoples ' reactions that could not figure out where the sound was coming from . with radio waves you can experience this more easily without a deliberate setup because any one radio transmission is in a very narrow frequency band . particularly with commerical fm ( about 3 meter wavelength ) you can notice this driving around in a car . there will be spots where the reception is very poor , even though a few meters in any direction it will be better .
i assume op means an even potential $v ( x ) ~=~v ( -x ) $ , e.g. , a finite square well potential $v ( x ) ~\propto~ \theta ( |x|-a ) $ . then the answer to the question ( v1 ) is no . sketched proof : under the assumption that $v$ is even , the hamiltonian $$h= \frac{p^2}{2m}+v ( x ) $$ then commutes with the parity operator $p$ . so the operators $h$ and $p$ can be diagonalized simultaneously . so there exists a complete set of energy eigenstates that are either even or odd . let 's call them $e_i ( x ) =e_i ( -x ) $ and $o_j ( x ) =-o_j ( -x ) $ , respectively . an initially even state $$\psi ( x , t\ ! =\ ! 0 ) ~=~\psi ( -x , t\ ! =\ ! 0 ) $$ is hence a linear combination of even energy eigenstate only $$\psi ( x , t\ ! =\ ! 0 ) ~=~\sum_i c_i e_i ( x ) . $$ the wave function $$ \psi ( x , t ) ~=~\sum_i c_i e_i ( x ) \exp\left [ -\frac{\mathrm{i}t e_i}{\hbar}\right ] ~=~\psi ( -x , t ) $$ remains an even function also at a future time $t$ , and can hence not become odd .
since hamiltonian vector fields generate symplectomorphisms in $\mathbb{r}^{2n}$ ( with the canonical symplectic form ) , one can pick any hamiltonian , solve the hamilton equations of motion to get a symplectomorphism . since , a nonlinear symplectomorphism is seeked , free particle and harmonic oscillator hamiltonians will not be good examples , as they give linear dependence on the initial conditions . but the hydrogen atom hamiltonian is a good example ( for $\mathbb{r}^{12}$ ( two bodies in three dimensions subject to an inverse square attractive cental force ) ) . the explicitely known solutions of its equations of motion are nonlinear symplectomorphisms .
electrons in the $n$-type conduction band diffuse across the boundary into the empty conduction band of the $p$-type semiconductor . once there they recombine with holes in the $p$-type valence band . so the net motion of electrons is from the $n$-type conduction band to the $p$-type valence band . this means that near the boundary there are no electrons in the conduction band and no holes in the valence band i.e. the material becomes insulating . the transfer of electrons causes the $p$ side to become negatively charged and the $n$ side positively charged . as electrons move , the charge separation creates an electric field that opposes the electron diffusion .
a possible answer to the last part of the question : the article six easy roads to the planck scale , adler , am . j . phys . , 78 , 925 ( 2010 ) contains multiple " derivations " that you might ( or might not ) find more satisfactory than the one you mention . as far as the rest of the question is concerned , others have made the most relevant points . i think a fair summary of what magueijo is getting at is something like the following : one frequently hears that " interesting new physics " happens when some length $l$ is less than the planck length . the planck length is manifestly lorentz invariant . the other length $l$ , if it is the physical length of some object , is manifestly not lorentz invariant . what meaning , then , can one assign to such statements ? it seems to me that reasonable people can differ over whether this is an interesting question . i do not find it manifestly insane , myself .
i think your problem is assuming that $a = b$ . your triangle looks isosceles , but it will not usually be . using your terminology : $\cos \alpha = \overrightarrow{ae} / \overrightarrow{ad}$ . if we let $\mathbf{f} = \overrightarrow{ad}$ , then $|\overrightarrow{ae}| = f \cos \alpha$ , and $|\overrightarrow{ae}|$ is the component of the force in the direction of movement . the reason you can not just fix $a=b$ is that it makes $\alpha = \frac{\pi}{4}$ automatically , while in reality it can be anything , and it will depend on the relationship between the force and the direcion of movement .
without going into the details of it , a conventional bwr gives a power density of $50 kw/liter$ inside the core , which is about half that of pwr . mind you , this includes the coolant inside the core and not the fissile material only . ref
in axiomatic approaches to quantum field theory , the basic field operators are usually realized as operator-valued distributions . that is what wightman fields are : operator-valued distributions satisfying the wightman axioms . wightman functions are the correlation functions of wightman fields , nothing more . there is a nice theorem that says if you have a bunch of functions that look like they are the wightman functions of qft , then you can actually reconstruct the hilbert space and the algebra of wightman fields from it . neither of these concepts is anything new physically . they are just more precise ways of speaking about things physicists already know . ( thinking of fields as operator-valued distributions instead of operator-valued functions lets you make precise what goes wrong when you multiply two fields at the same point . ) you can talk about opes or scattering theory in this language , but it will not gain you anything , unless you are trying to publish in math journals . if you are interested in the wightman axioms , they are explained nicely in the first of kazhdan 's lectures in the ias qft year .
it is a non-perturbative effect because it is 1-loop exact . the triangle diagram is actually the least insightful method to think about this , in my opinion . the core of the matter is the anomaly of the chiral symmetry , which you can also , for example , calculate by the fujikawa method examining the change of the path integral measure under the chiral transformation . you can obtain quite directly that the anomaly is proportional to $$\int \mathrm{tr} ( f \wedge f ) $$ which is manifestly a global , topological term , ( modulo some intricacies ) it is the so-called second chern class and takes only values of $8\pi^2k$ for integer $k$ . it is , by the atiyah-singer index theorem ( this can also be seen by fujikawa ) , essentially the difference between positive and negative chiral zero modes of the dirac operator . this is obviously a discontinuous function of $a$ ( or $f$ ) , which is already bad for something which , if it were perturbative , should be a smooth correction to something , and it is also the number describing which instanton vacuum sector we are in , see my answer here . since perturbation theory takes place around a fixed vacuum , this is not a perturbative effect , since it is effectively describing a tunneling between two different vacuum sectors .
it is very important to distinguish whether the symmetry is broken explicitly or spontaneously . i think that the sentence " now when i break this symmetry spontaneously ( or explicitly ) " indicates that its author is not quite distinguishing these things . an explicit symmetry breaking generally lifts the degeneracy because the different parts of the multiplets no longer have the same energy . however , spontaneous symmetry breaking increases the degeneracy , particularly of the ground state . it is really how spontaneous symmmetry is defined . it is a fate of the symmetry that remains the symmetry of the laws of physics , but in practice , the environment we encounter , starting from the ground state , is no longer invariant under the symmetry . its not being invariant means nothing else than the fact that if we act with a generator $g$ of the continuous symmetry on the ground state , we get $$ g|0\rangle \neq 0 $$ we would get zero if the symmetry were not spontaneously broken . if it is broken , we get a nonzero vector that is independent from the original $|0\rangle$ , so we get another " copy " of the ground state . here , $g$ still commutes with the hamiltonian $h$ so these copies have the same energy – we have degeneracy . for example , if the electroweak symmetry were global , just to simplify things , the ground state of the higgs field could have vev $ ( 0,246 ) $ in the units of gev , but it could have any other vev with the same magnitude , too . so there are infinitely many vacua . ( in gauge theory , they are made equivalent , but if we spontaneously break a global symmetry , they are states related by the symmetry and therefore having the same energy , but distinct elements of the hilbert space . ) so the claim that the op is dissatisfied with really holds completely generally , by definition of the spontaneous symmetry breaking ! misunderstanding that the ground state of a spontaneously broken symmetric theory is degenerate is the misunderstanding of the basic idea of the spontaneous symmetry breaking . note that spontaneous symmetry breaking still effectively means that the " symmetry is broken for most practical purposes " because the " copies " mentioned above may be imagined to be physically identified and we split the hilbert space into " superselection sectors " each of which is built upon one copy of the ground state . the action of the symmetry generator on any excited state gives us a vector from another superselection sector which can not be identified with zero so from the perspective of a single superselection sector , the symmetry is simply broken . none of these questions and discussions about explicit vs spontaneous symmetry breaking has anything to do with the time-reversal symmetry ( or the absence of it ) which is just another symmetry ( one given by an antiunitary transformation , so some of the comments above would not apply to this symmetry ) . both systems with and without time-reversal symmetry satisfy the claim that the ground state is degenerate if a symmetry is spontaneously broken .
the short answer is no ! more about imagination how space of our universe looks ( and how expanding ) today and in past check here : wmap
if you assume perfect efficiency , then the energy of dissociation of a liter of water is computed as follows : 1 liter of water , molar mass 18g , => 55.6 moles the energy needed is 237 kj per mole ( from your link - see under " thermodynamics" ) . 237 * 55.6 => 31.7 mj of energy for a liter of water . in terms of power , this is 3.67 kw for one hour . this shows you that the energy density of hydrogen ( in terms of energy stored per gram ) is very very high : it is why people have from time to time had such high hopes for a " hydrogen economy " which could leverage this energy density ( much more than the energy per gram of gasoline ) . but storing the stuff safely and cheaply is remarkably difficult . . .
a mixed state is mathematically represented by a bounded , positive trace-class operator with unit trace $\rho : \cal h \to \cal h$ . here $\cal h$ denotes the complex hilbert space of the system ( it may be nonseparable ) . the set of mixed states $s ( \cal h ) $ is a convex body in the complex linear space of trace class operators $b_1 ( \cal h ) $ which is a two-side $*$-ideal of the $c^*$-algebra of bounded operators $b ( \cal h ) $ . convex means that if $\rho_1 , \rho_2 \in s ( \cal h ) $ then a convex combination of them , i.e. $p\rho_1 + q\rho_2$ if $p , q\in [ 0,1 ] $ with $p+q=1$ , satisfies $p\rho_1 + q\rho_2 \in s ( \cal h ) $ . two-side $*$-ideal means that linear combinations of elements of $b_1 ( \cal h ) $ belong to that space ( the set is a subspace ) , the adjoint of an element of $b_1 ( \cal h ) $ stays in that space as well and $ab , ba \in b_1 ( \cal h ) $ if $a\in b_1 ( \cal h ) $ and $b \in b ( \cal h ) $ . i stress that , instead , the subset of states $s ( \cal h ) \subset b_1 ( \cal h ) $ is not a vector space since only convex combinations are allowed therein . the extremal elements of $s ( \cal h ) $ , namely the elements which cannot be decomposed as a nontrivial convex combinations of other elements , are all of the pure states . they are of the form $|\psi \rangle \langle \psi|$ for some unit vector of $\cal h$ . ( notice that , since phases are physically irrelevant the operators $|\psi \rangle \langle \psi|$ biunivocally determine the pure states , i.e. $|\psi\rangle$ up to a phase . ) the space $b_1 ( \cal h ) $ and thus the set $s ( \cal h ) $ admits at least three relevant normed topologies induced by corresponding norms . one is the standard operator norm $||t||= \sup_{||x||=1}||tx||$ and the remaining ones are : $$||t||_1 = || \sqrt{t^*t} ||\qquad \mbox{the trace norm}$$ $$||t||_2 = \sqrt{||t^*t||} \qquad \mbox{the hilbert-schmidt norm}\: . $$ it is possible to prove that : $$||t|| \leq ||t||_2 \leq ||t||_1 \quad \mbox{if $t\in b_1 ( \cal h ) $ . }$$ moreover , it turns out that $b_1 ( \cal h ) $ is a banach space with respect to $||\cdot||_1$ ( it is not closed with respect the other two topologies , in particular , the closure with respect to $||\cdot||$ coincides to the ideal of compact operators $b_\infty ( \cal h ) $ ) . as $s ( \cal h ) $ is closed with respect to $||\cdot ||_1$ , it is a complete metric space with respect to the distance $d_1 ( \rho , \rho' ) := ||\rho-\rho'||_1$ . when $dim ( \cal h ) $ is finite the three topologies coincide ( though the norms do not ) , as a general result on finite dimensional banach spaces . concerning your last question , there are many viewpoints . my opinion is that a density matrix is physical exactly as pure states are . it is disputable whether or not a mixed state encompasses a sort of physical ignorance , since there is no way to distinguish between " classical probability " and " quantum probability " in a quantum mixture as soon as the mixture is created . see my question classical and quantum probabilities in density matrices and , in particular luboš motl 's answer . see also my answer to why is the application of probability in qm fundamentally different from application of probability in other areas ? addendum . in finite dimension , barring the trivial case $dim ( {\cal h} ) =2$ where the structure of the space of the states is pictured by the poincaré-bloch ball as a manifold with boundary , $s ( \cal h ) $ has a structure which generalizes that of a manifold with boundary . a stratified space . roughly speaking , it is not a manifold but is the union of ( riemannian ) manifolds with different dimension ( depending on the range of the operators ) and the intersections are not smooth . when the dimension of $\cal h$ is infinite , one should deal with the notion of infinite dimensional manifold and things become much more complicated .
one can get an order of magnitude estimate of the maximum speed attainable by gravitational slingshots without doing any real calculation . the ' rough physics ' reasoning goes as follows : the gravitational field of the planets used for slingshots needs to be strong enough to " grab " the speeding spaceship . as a planet cannot " grab " a spaceships moving faster than the planet 's escape velocity , it is impossible to slingshot a spaceship to speeds beyond the planetary escape velocities . so no matter how often our solar 's system planets line up and no matter how often you manage to pull off a perfect gravitational slingshot , you are practically limited to speeds not exceeding roughly the maximum escape velocity in the solar system ( i.e. . 80 km/s or 0.027 % of the speed of light , the escape velocity of jupiter ) . ( note : by working with well-defined trajectories one can refine the above argument and get all the numerical factors correct . )
well . . . the mass of the sun is $2 \times 10^{30} kg$ . if it loses $4 \times 10^9 kg$ per second , it would take 160 billion years for it to lose 1% of its mass . the dark matter content of the universe is theorized to be 26.8% . so , the total mass contribution from photons cannot possibly account for the missing dark matter . also , if light from stars really made such an enormous contribution to the mass-energy of the universe , then we would see this light . ( i.e. . it would not be dark ! )
the standard model yukawa interactions must be $su ( 3 ) \times su ( 2 ) \times u ( 1 ) _y$ gauge invariant . the down-type yukawa interaction is $$ \mathcal{l} \supset -y_d \bar q \phi d_r + \text{h . c . } . $$ this is indeed gauge invariant . the $\bar q d_r$ form a colour singlet ( $3^* \times 3$ ) , the $\bar q \phi$ form an $su ( 2 ) $ singlet ( $2^*\times2 ) $ , and the whole thing is neutral under $u ( 1 ) _y$ , because the quantum numbers are $-\frac13$ , $1$ and $-\frac23$ , for $\bar q$ , $\phi$ and $d_r$ , which sum to zero . let 's try writing a similar up-type yukawa $$ \mathcal{l} \supset^ ? _ ? -y_u \bar q \phi u_r + \text{h . c . } $$ is it gauge invariant ? no - it breaks $u ( 1 ) _y$ , because the quantum numbers are $-\frac13$ , $1$ and $\frac43$ , for $\bar q$ , $\phi$ and $d_r$ , such that $y=2$ . to fix this problem , we might try $$ \mathcal{l} \supset^ ? _ ? -y_u \bar q \phi^* u_r + \text{h . c . } $$ this is $u ( 1 ) $ invariant because the hypercharge of $\phi^*$ is $-1$ , so we have $-\frac13-1+\frac43=0$ , but now it is no longer $su ( 2 ) $ invariant ( $2^*\times2^*$ ) . now we use the property that $i\tau_2\phi^*$ trasforms in the same way under $su ( 2 ) $ as $\phi$ finally , we can write $$ \mathcal{l} \supset -y_u \bar q i\tau_2 \phi^* u_r + \text{h . c . } $$ which is indeed fully gauge invariant .
one of the standard texts for this kind of thing ( the quantum mechanics of lasers , without all the technical details you had need to know to design a real one ) is loudon 's quantum theory of light . i have got the 2nd edition , i think he is up to the 3rd . in my edition he does the " pre-quantum " explanation of lasers ( i.e. . in terms of einstein a and b coefficients ) in about 40 pages . over the next 200 pages or so he does the fully quantum treatment of light ( quantization of the field , creation and annihilation operators , etc ) and then revisits the laser . the first 40 pages should cover the " clearly written description " criteria , and - as far as it goes - it is relatively rigorous as well .
neither of the interaction terms would appear were the atom not present , and you cannot simply set $\hat{p}=0$ as it is a dynamically fluctuating quantum variable . therefore , both of the terms must describe scattering of the light from the atom . roughly speaking , the term $\sim \hat{p}\hat{a}$ encodes inelastic scattering , while the term $\sim \hat{a}^2$ encodes elastic scattering . in first order perturbation theory , transitions due to the first term are controlled by the matrix elements $$ \mathcal{m}_1 = \langle m , e_f| \hat{p}\hat{a} |n , e_i\rangle = \langle m|\hat{a}|n\rangle\langle e_f|\hat{p}|e_i\rangle . $$ here , $n , m$ are the initial and final number of photons in the light field ( assume for simplicity there is only one mode , obviously in a real calculation you will also have to consider different wavevectors and polarisation states of the field ) , while $e_{i , f}$ are the initial and final atomic eigenstates . since the operator $\hat{a}$ is linear in annihilation/creation operators , the matrix element is zero unless $m = n\pm 1$ . meanwhile , since atomic eigenstates have definite parity , the matrix element is zero unless $|e_f\rangle \neq |e_i\rangle$ ( or more precisely , it is zero unless the two states have opposite parity ) . so to first order , the interaction vertex $\hat{p}\hat{a}$ describes processes in which a photon is absorbed or emitted by the atom , changing its internal state . at higher orders in perturbation theory you will see processes where the atom 's internal state changes multiple times . this includes , for example , an inelastic scattering process where the atom absorbs a photon at one frequency and an electron becomes excited , then the electron drops in energy by emitting a photon at another frequency . ( you could also have elastic scattering processes where the atom ends up in the same state , hence the " roughly speaking " disclaimer above . ) matrix elements of the second interaction vertex look like $$\mathcal{m}_2 = \langle m , e_f|\hat{a}^2 |n , e_i\rangle = \langle m| \hat{a}^2 |n\rangle \langle e_f| e_i\rangle . $$ therefore , these matrix elements vanish unless the initial and final state of the atom is the same . however , the vertex is now quadratic in annihilation/creation operators , so you will see , for example , elastic scattering processes where a photon is absorbed and then re-emitted at the same frequency ( but different direction , in general ) . at higher orders you will start to see really interesting optical non-linearities where , for example , two photons are absorbed , and then two photons are re-emitted at different frequencies . hopefully this explains your idea about " perturbing the hamiltonian of the free em field " . the strong interaction between light and electrons can produce an effective interaction in the presence of matter ( aka optical non-linearity ) between otherwise non-interacting photons . finally , do not forget that the full interaction operator contains both contributions so at higher orders in perturbation theory you will also get cross terms between $\hat{p}\hat{a}$ and $\hat{a}^2$ .
the earth receives a certain amount of radiation from the sun , and also generates a certain amount of heat from radioactivity in the core . it must radiate this amount of energy because otherwise it would heat up until the thermal emission matched the rate of energy input . it is this heat flux that you had use when calculating how much your plate was heated by the earth . the sums seem simple enough and i am sure google would be able to retrieve the values of heat received from the sun and generated internally . given the heat flux from the earth you can calculate a temperature using the stefan boltzmann law . this temperature would not match the temperature of the earth 's surface , but then why should it ? as you say , between the surface and space is a layer of insulating gas . the stefan boltzmann temperature would be some kind of average for the various parts of the earth from which radiation is received . increasing co$_2$ or other greenhouse gases would change the temperature profiles between the surface and space , but not the overall amount of energy being emitted . you can define an emissivity for the earth , but i am not sure how helpful this is . if you compare the temperature at the surface with the stefan boltzmann temperature you had get an emissivity less than one , but then this is not an especially useful comparison .
according to the rsb : migrating ( european ) swallows cover 200 miles a day , mainly during daylight , at speeds of 17-22 miles per hour . the maximum flight speed is 35 mph i think that calculating the flight speed of a bird from first principles would be impossibly difficult as there are just too many variables . the web site you link does get an answer that agrees with the rspb , but one has to wonder if they knew the answer in advance , and in any case they do a lot of the work by comparing with other birds rather than by ab initio calculation .
there is a small error in your math by the way - the factor of $4$ should be a factor of $1$ - check the denominator of the quadratic equation . why is this so unreasonable ? at a glance , it looks like the momentum of the sail is a bit larger than what it started out with . if i make the reasonable assumptions that : the sail is non-relativistic $\frac{p_0}{mc} = \frac{v}{c} \ll 1$ the energy of the photon is much less than $mc^2$ and use the approximation $\sqrt{1-x}\approx 1-\frac{x}{2}\ ; {\rm for}\ ; x\ll1$ , your answer reduces to : $$p_1 \approx p_0+2p$$ this should look familiar - it is like a ball of mass $m$ colliding elastically with a ball of mass $m$ with $m\ll m$ .
neglecting friction , the force experienced is the centrifugal force $f=\frac{mv^2}{r}$ ( it would be less if you included friction since the car actually slips ) vectorially added to the orthogonal gravitational force $f_g=mg$ , i.e. $f = m\sqrt{\left ( \frac{v^2}r\right ) ^2 + g^2}$ where $g = 9.81 \frac{m}{s^2}$ 1 . divide this by $f_g$ to obtain a result in gs . remember to use si-units , i.e. divide a km/h speed by 3.6 ( 1000 km/m / 3600 s/h ) to obtain m/s . for the 350 m curve and 285 km/h that yields about 2.08 gs only , to obtain the 5.5 gs mentioned a radius of about 118 m is required , or some higher velocity ( 490 km/h for the 350 m curve ) . 1 ) thanks j.h. for this important correction !
the projection operator $\delta_{\varepsilon\ , a}$ is $1$ for some particular $|a\rangle$ and is $0$ on any orthogonal state . the definition of the expectation in classical probability theory for this operator is given by \begin{equation} \left\langle \delta_{\varepsilon\ , a}\right\rangle = 1\pr ( |x\rangle = |a\rangle ) + 0\pr ( |x\rangle \perp |a\rangle ) = \pr ( |x\rangle = |a\rangle ) \end{equation}i . e the expectation of the projection operator is the probability that $|x\rangle$ is found in state $|a\rangle$ when a measurement is made . however we can also write \begin{align} \left\langle \delta_{\varepsilon\ , a}\right\rangle and = \langle x|\delta_{\varepsilon\ , a}|x\rangle\\ and = \langle x|a\rangle\langle a|x\rangle\\ and = |\langle a|x\rangle|^2\end{align} so the rule for finding probabilities in qm can be derived from the rule for finding expectations of operators .
mad props for a cool question . i am going to justify essentially the converse of the statement because it does not make much sense to talk of the temperature of a system that is in a pure state . let 's assume that we are talking about a quantum system with disrete energy spectrum ( with no accumuation points ) in thermal equilibrium . let $\beta = 1/kt$ be the inverse temperature . then recall that the boltzmann distribution tells us that the population fraction of systems in the ensemble corresponding to energy $e_i$ is given by $$ p_i = \frac{g_ie^{-\beta e_i}}{z} $$ where $g_i$ is the degeneracy of the energy level . in particular , note that the relative frequency with which energies $e_i$ and $e_j$ will be found in the ensemble is $$ p_{ij} ( \beta ) = \frac{g_ie^{-\beta e_i}}{g_je^{-\beta e_j}} = \frac{g_i}{g_j}e^{-\beta ( e_i - e_j ) } $$ in particular , let $i=0$ correspond to the ground level , then the frequency of any level relative to the ground level is $$ p_{i0} ( \beta ) = \frac{g_i}{g_0}e^{-\beta ( e_i - e_0 ) } $$ notice that since the ground level has the lowest energy by definition , we have $e_i - e_0 \leq 0$ , but zero temperature corresponds to the limit $\beta \to \infty$ , and we have $$ \lim_{\beta\to 0 } p_{i0} ( \beta ) = \delta_{i0} $$ in other words , at zero temperature , every member of the ensemble must be in the ground energy level ; the probability that a system in the ensemble will have any other energy becomes vanishingly small compared to the probability that a member of the ensemble has the lowest energy .
this comes from the pioneering work of amateur physicist , michael jackson : you can not win , you can not break even , and you can not get out of the game . people keep sayin ' , things are gonna change . but they just look like they are stayin ' the same . -michael jackson , " you can not win " okay , okay . i think this is one of those jokes that starts between graduate students or something by the water cooler , so it does not have an " exact " form . wikiquotes gives the original author as c.p. snow in the following form : zeroth : " you must play the game . " first : " you can not win . " second : " you can not break even . " third : " you can not quit the game . " but that seems somewhat dubious . no one can provide a cite of c.p. snow actually saying or writing it anywhere ; i suspect that this history is a little mythical and that the joke has no real precise statement .
the last integral is a surface integral . it is the fluid ( current ) flux crossing the volume surface integrated over the whole surface . it is how much charge is leaving the volume per second . ( $\partial v$ means boundary of v and you sum all local $j_\bot ds$ . )
the reason this works is because the field operators obey linear equations of motion . the heisenberg equations of motion for the canonically quantized field are the same linear equations as the classical equations of motion , and they are translationally invariant . so you take the fourier transform of the field , and the equation of motion guarantees that you only get a superposition of those plane waves which solve the linear equation . the fourier transform coefficients of these plane waves are operators which manifestly have a definite frequency , and therefore are creation/annihilation operators . it is a consequence of the commutation relation with the hamiltonian that a definite frequency operator acting on an energy state produces a state with energy incremented by the frequency of the operator . digression on normalization when doing the field expansion , field theory books usually do not put things in manifestly covariant form , so the formulas are very ugly . the proper relativistic conventions are as follows : the k integration measure on a relativistic mass shell is given by a delta function concentrated on the mass shell . $$ \int {d^4k\over ( 2\pi ) ^4} 2\pi \delta ( k^2-m^2 ) = \int {d^3k\over ( 2\pi ) ^3 2\omega_k}$$ all k delta-functions carry $2\pi$ factors , all integrations over dk have a $2\pi$ in the denominator , this is the physicist conventions for fourier transforms . these factors should be implicit when you write down the things , so the above should be written : $$ \int d^4 k \delta ( k^2 - m^2 ) = \int {d^3k\over 2\omega_k} = \int d\vec{k}$$ where the last equality is a definition of notation . the k-states of a particle should be normalized so that their inner product is orthogonal when integrating with the measure above ( the $2\pi$ factors in the delta function are supressed , as immediately above ) $$ \langle k|k&#39 ; \rangle = 2\omega_k \delta^3 ( k-k&#39 ; ) $$ this means that the state $|k\rangle$ is really $\sqrt{2\omega_k}$ bigger than the nonrelativistically normalized state , where the right hand side of the above is just a delta-function . it is also $ ( 2\pi ) ^{3/2}$ times bigger than the state normalized with a non-2pi-absorbing delta function on the right hand side . the relativistic creation operators need to create these bigger-normalized states , so they are bigger than the naive creation operators $$ \alpha^\dagger ( k ) = ( 2\pi ) ^{3\over 2} \sqrt{2\omega_k} a ( k ) $$ and likewise for the annihilation operators . with these conventions , the fourier expansion for the scalar field $\phi$ reads $$\phi ( x ) = \int \alpha ( k ) e^{i ( k\cdot x + \omega_k t ) } + \alpha^\dagger ( k ) e^{-i ( k\cdot x + \omega_k t ) } d\vec{k} $$ and the naturalness of the expansion is manifest .
yes , there are rigorous ways of defining locality in such contexts , but the precise terminology used unfortunately depends on both the context , and who is making the definition . let me give an example context and definition . example context/definition . for conceptual simplicity , let $\mathcal f$ denote a set of smooth , rapidly decaying functions $f:\mathbb r\to \mathbb r$ . a functional $\phi$ on $\mathcal f$ is a function $\phi:\mathcal f\to \mathbb r$ . a function ( not yet a functional on $\mathcal f$ ) $\phi:\mathcal f\to\mathcal f$ is called local provided there exists a positive integer $n$ , and a function $\bar\phi:\mathbb r^{n+1}\to\mathbb r$ for which \begin{align} \phi [ f ] ( x ) = \bar\phi\big ( x , f ( x ) , f' ( x ) , f'' ( x ) , \dots , f^{ ( n ) } ( x ) \big ) \tag{1} \end{align} for all $f\in \mathcal f$ and for all $x\in\mathbb r$ . in other words , such a function is local provided it depends only on $x$ , the value of the function $f$ at $x$ , and the value of any finite number of derivatives of $f$ at $x$ . a functional $\phi$ is called an integral functional provided there exists a function $\phi:\mathcal f\to\mathcal f$ such that \begin{align} \phi [ f ] = \int_{\mathbb r} dx \ , \phi [ f ] ( x ) . \tag{2} \end{align} an integral functional $\phi$ is called local provided there exists some local function $\phi:\mathcal f\to\mathcal f$ for which $ ( 2 ) $ holds . what could we have defined differently ? some authors might not allow for derivatives in the definition $ ( 1 ) $ , or might call something with derivatives semi-local . this makes intuitive sense because if you think of taylor expanding a function , say , in single-variable calculus , you get \begin{align} f ( x+a ) = f ( x ) + f' ( x ) a + f'' ( x ) \frac{a^2}{2} + \cdots , \end{align} and if you want $a$ to be large , namely if you want information about what the function is doing far from $x$ ( non-local behavior ) , then you need more an more derivative terms to sense that . the more derivatives you consider , the more you sense the " non-local " behavior of the function . one can also generalize to situations in which the functions involved are on manifolds , or are not smooth but perhaps only differentiable a finite number of times etc . , but these are just details and i do not think illuminate the concept . example 1 - a local functional . suppose that we define a function $\phi_0:\mathcal f\to \mathcal f$ as follows : \begin{align} \phi_0 [ f ] ( x ) = f ( x ) , \end{align} then $\phi_0$ is a local function $\mathcal f\to\mathcal f$ , and it yields a local integral functional $\phi_0$ given by \begin{align} \phi_0 [ f ] = \int_{\mathbb r} dx\ , \phi_0 [ f ] ( x ) = \int_{\mathbb r} dx\ , f ( x ) , \end{align} which simply integrates the function over the real line . example 2 - another local functional . consider the function $\phi_a:\mathcal f\to\mathcal f$ defined as follows : \begin{align} \phi_a [ f ] ( x ) = f ( x+a ) . \end{align} is this $\phi_a$ local ? well , for $a=0$ it certainly is since it agrees with $\phi_0$ . what about for $a\neq 0$ ? well for such a case $\phi_a$ certainly is not because $f ( x+a ) $ depends both on $f ( x ) $ and on an infinite number of derivatives of $f$ at $x$ . what about the functional $\phi_a$ obtained by integrating $\phi_a$ ? notice that \begin{align} \phi_a [ f ] and = \int_{\mathbb r} dx\ , \phi_a [ f ] ( x ) \\ and = \int_{\mathbb r} dx\ , f ( x+a ) \\ and = \int_{\mathbb r} dx\ , f ( x ) \\ and = \int_{\mathbb r} dx\ , \phi_0 [ f ] ( x ) \\ and = \phi_0 [ f ] . \end{align} so $\phi_a [ f ] $ is local even though $\phi_a$ is not for $a\neq 0$ . the lesson of this example is this : you may encounter an integral functional $\phi:\mathcal f\to\mathbb r$ that is defined by integrating over a non-local function $\phi:\mathcal f\to\mathcal f$ . however , there might still be a way of writing the functional $\phi$ as the integral over a different function , say $\phi'$ , that is local , in which case we can assert that $\phi$ is local as well because to verify that a functional is local , you just need to find one way of writing it as the integral of a local function .
you are correct : there is no free charge so $\vec{d}=0$ which means $$ \vec{e}=-\frac{1}{\epsilon_0}\vec{p}=-\frac{k}{\epsilon_0r}\hat{r} $$ but this is for $r_1\leq r\leq r_2$ . inside the shell , $r&lt ; r_1$ , there are no enclosed charges , so $\vec{e}=0$ there . outside the shell , there is also no charge . recall that the total charge for dielectrics can be expressed as $$ q_{tot}=\oint_\mathcal{s}\sigma_b da-\int_\mathcal{v}\rho_b dv = \oint_\mathcal{s}\vec{p}\cdot d\vec{a}-\int_\mathcal{v}\vec{\nabla}\cdot\vec{p} dv $$ where $\rho_b$ and $\sigma_b$ are the volume and surface charge densities of the bound charges . from the divergence theorem , the two terms involving $\vec{p}$ are identical , so $q_{tot}=0$ . thus , your answer should be $$ \vec{e}\left ( r\right ) =\begin{cases}-\frac{k}{\epsilon_0r}\hat{r} and r_1\leq r\leq r_2 \\ 0 and {\rm otherwise}\end{cases} $$
a very sensitive device would be required to measure the minuscule change in the water depth along the glass walls , because the differences in the strength of the gravitational field between each side of the glass are essentially zero . because of this the force exerted on the glass is the same , but due to the small volume of water in a glass as opposed to an ocean it would be very hard to measure any change . however , the difference in the strength of the gravitational field between the side of the earth closest to the moon and furthest to the moon is enough to pull water more towards the side close to the moon . it is also worth nothing that the surface tension , at these small sizes , will have a much more significant effect than tidal forces , further hindering the measurements .
dodo , i am not really sure why you would want to do this . i do not think it will work when unrolled . this is also dangerous and not advisable . if you will go through with it anyway , wear gloves and do it in a well ventilated area . if you remove the case and unroll it , it is just a flat piece of foil . the foil does not make the capacitor as much as it is the foil and the geometry . if the anode foil and cathode foil were still ordered properly after removal , it would still work obviously , but i do not think it would work to the manufactories specifications . if the rod were in direct contact with the layer you put on the rod , yes it would work . to what end you would want that for , i can only guess . cheers .
while i think lubos answer leaves nothing conceptualy to add , it is on too high level , if you have problem to understand nonconservative fields . your question is actually freshman topic so i think at first you should understand this in freshman fashion and then move to lubos answer . therefore you might use a very nice resource from ocw mit by walter lewin it basically tells you to always use faraday 's law . in this framework , as you said , when we have not-changing magnetic field , we get that electric potential drop on any loop is 0 ( which textbooks call kirchhoff ii law ) . when we have changing magnetic field , than going in the direction of the current ( so that you will not have to think about sign in inductance part ) , you write that the right-hand-side of faraday 's law ( that is negative derivative of magnetic flux with respect to time ) is simply =$-l\ , di/dt$ than you do left-hand-side , remembering that ideal conductor has no resistance - therefore there is no electric field in it . if you still do not get it , here is another resource , or the full lecture
it is the result of a dependence of the pressure with growing depth , due to the gravitational field ( i.e. . the weight of the water ) . you may do an easy calculation with some simple geometrical form , e.g. a cylinder totally submerged in water , to quickly understand how it works . the force due to pressure in each surface element of the curved wall of the cylinder is proportional to the depth of that element , and has the normal direction to the wall , i.e. towards the axis of the cylinder . after an easy integration in polar coordinates , you can see that the resultant force points upwards . that is because the forces in the upper parts are smaller that the ones near the more deeply submerged part of the cylinder . a surprising conclusion is that a golf ball submerged in a tank of water in the space station , would not go upwards . . . or that the bubbles in a coke in the hands of an astronaut remain where they origin . . . i would love to see that .
dear rajesh , the goal as well as main achievement of string theory is not to " achieve spacetime quantization " whatever this phrase is supposed to mean but to allow calculations about spacetime that are compatible with the postulates of quantum mechanics . it is not the same thing . the statement that " spacetime is quantized " , whatever it means , is just a working hypothesis , not a holy grail that should be " achieved " . instead , what is needed is to have a theory that is a quantum theory as a whole , i.e. one that agrees with the uncertainty principle , probabilistic character of predictions , observables ' being expressed by linear operators on the hilbert space , and so on . on the other hand , the spacetime is an approximate concept in string theory . at distances much longer than the characteristic distance scale of string theory , the string scale ( or the related gravitational planck scale ) , classical general relativity is a good approximation - and some of its aspects remain exact at all distance scales . at distances comparable to the string scale ( or planck scale ) , entirely new set of physical phenomena take over . it is not true that the only difference of these new phenomena from classical general relativity is that " spacetime is quantized " . and in fact , it is not true in any sense and it cannot be true in a consistent theory of quantum gravity that spacetime becomes discrete . and in fact , geometric quantities - while remaining continuous whenever it makes sense to use them - may be shown in string theory to be invalid variables to describe physics at very short distances . to say the least , they are incomplete . so if your question is how string theory confirms the prejudices and beliefs that the spacetime quantities survive as good variables up to arbitrarily short distance scales and/or that they become discrete , the answer is that string theory is a way to prove that both of these prejudices are dead wrong . if your question is which quantum phenomena affecting spacetime are predicted by string theory , it is a valid question but it is too broad and a proper answer would require to review all of string theory - because all important insights of string theory , in some sense , show the consequences of the co-existence of quantum mechanics with a dynamical spacetime .
we note $$ \left [ \alpha_m^0 , \alpha_n^0 \right ] = \eta^{00} \delta_{m+n , 0} = - \delta_{m , -n} $$ a timelike excitation is $\alpha_{-n}^0 \left| 0 ; k \right&gt ; $ . the norm of this state is \begin{equation} \begin{split} \left&lt ; 0 ; k'\right| \alpha_{m}^0 \alpha_{-n}^0 \left|0 ; k\right&gt ; and = \left&lt ; 0 ; k'\right| \left ( \left [ \alpha_{m}^0 , \alpha_{-n}^0 \right ] + \alpha_{-n}^0 \alpha_{m}^0 \right ) \left|0 ; k\right&gt ; \\ and = - \delta_{m , n} \left&lt ; 0 ; k'\right . \left|0 ; k\right&gt ; \\ and = - \delta_{m , n} ( 2\pi ) ^d \delta^d ( k - k ' ) &lt ; 0 \end{split} \end{equation} thus a timelike excitation has negative norm .
nothing - that is the correct definition . one little caveat is that small systems are usually in contact with a larger system with a temperature that is more easily controlled or measured ( the " heat bath" ) , and $t$ usually stands for the temperature of the heat bath rather than the small system itself . however , this does not make a lot of difference in practice , because the small system will very rapidly attain the same temperature as the heat bath anyway .
the molecules $o_2$ and $n_2$ are symmetric and have no dipole momentum . that is why they can not interact with emw ( at least within dipole approximation ) . one can say that transitions between the oscillator levels in these molecules are forbidden by symmetry in electrodipole approximation . the molecule $co$ consists of two different atoms . the average positions of positive and negative charges are not the same . this molecule is polar .
the elements of this particular metric tensor do not depend on time . $dx^j$ and $dt$ are treated as constants when you take the time derivative . you should also have $\gamma^{\lambda}_{00} = -\frac{1}{2} g^{\nu \lambda} \frac{\partial g_{00}}{\partial x^\nu}$ .
well the real question should be why is there a °c ( celsius ) . the celsius scale is a " centigrade " scale in that it uniformly divides the temperature range between the boiling point of water , and the freezing point of water into 100 equal parts , and then it arbitrarily calls the freezing point zero °c , and the boiling point becomes 100°c . the kelvin scale is referenced to the triple point of water , not the freezing point , and that temperature is about 0.1°c ( it might be 0.098°c but i am not sure about that ) . quite arbitrarily , it was decided that degrees on the kelvin scale , should be identical in size to celsius degrees , and experimentally the zero on the kelvin scale ( zero kelvins ) is 273.16 celsius degrees below the triple point of water , which makes it also -273.15°c
your intuition is correct : the top loop inductor prevents its current from changing instantaneously , so it starts at zero . qualitatively , the top loop current : starts at zero ( here the inductors block current flow in both loops . ) rises to a max as the top loop uncoupled inductor allows current to flow through it while its source voltage ( the coupled inductor voltage ) simultaneously falls as the bottom loop current rises . decays to zero as the coupled inductor voltage falls to zero . an equivalent circuit model does not contain all 3 inductors in series , but rather two parallel branches ( corresponding to the primary and secondary of the coupled inductor ) , with the " secondary " branch ( corresponding to the top loop ) containing an inductance which blocks initial current flow .
generally both formulations ( largangian and hamiltonian ) are equivalent , but in your case , if $\theta$ is small , you have a simplified equation for $p$ and you can use a solution ansatz like $e^{i\omega t}$ for both $p$ and $\theta$ . to draw a path in the phase space , you have to solve the equations and/or manage to express $p ( \theta ) $ or $\theta ( p ) $ .
it wont be unboundedly fast e.g. due to the viscosity of the water . in addition , i would like to point out that you will not have the cylinder of water . water will accelerate with gravity as it falls . the density of water is constant . the total flow of the water at different height is constant as well . so the area of the water 's cross-section will decline . for thin stream of water from a kitchen tap , surface tension force is enough to keep the water flowing in one stream , but the thickness of the scream decreases , take a look here . however , as you increase the initial radius of your stream , the water mass ( ~r^2 ) grows while the pressure gradient created by surface tension forces ( ~1/r ) declines . that is why the cylinder will pretty quickly separates into multiple streams , and eventually into droplets . .
the amount of force generated by a stirling engine for a particular temperature gradient is different for each engine and depends on things like the particular engine configuration , materials used , volume of the pistons , stroke length , . . . as for the speed or frequency since its cyclic , this will also vary by just as many parameters . in general the larger the heat difference available the ' better ' an engine will run , by how much is something for the individual engine product manuals . for a given temperature gradient ( say between an underground volcanic heat source and the open air ) a trade off needs to be made between speed and force . an engine could be designed to be more powerful by making the pistons and cylinders really large to maximize the expanded gases pressing on the piston , or really fast by making pistons and cylinders much smaller so that the smaller amount of gas inside the chamber finishes expanding much faster for a new cycle to begin .
well at the most fundamental level , the index of refraction of a material is defined as $ n = \sqrt {\epsilon \mu} $ where $\epsilon $ is the electric permittivity and $\mu$ is the magnetic permeability of the material . this arises from the solution of maxwell 's equations in a medium . also arising from the wave equation , which can be derived from maxwell 's equations , is that the index of refraction is the speed of light in vacuum , $c$ divided by the speed of light in the material $c_m$ . $ n ={ c \over c_m} $ worth noting is that since $c_m$ is always less than $c$ , the index of refraction is always greater than 1 . now the phase velocity for an electromagnetic wave of angular frequency $\omega$ is given by $v_p = {\omega \over k}$ where $k= {2\pi \over\lambda_m}$ is the magnitude of the wave vector and $\lambda_m$ is the wavelength in the medium . so after a little algebra , we find that the wave vector and index of refraction are related by $k ={ n\omega \over c}$ where does all this come into play in refraction and snell 's law ? well , it is the wave vector that comes into play in satisfying the boundary conditions on the electric ( and magnetic ) fields at the interface between two media . to see this , let 's look at the simple case of a plane wave of monochromatic light incident on the interface between two media with indices of refraction $n_1$ and $n_2$: in this simple case , considering the boundary conditions on the electric field at the interface is sufficient to derive snell 's law . the boundary condition is given by equation ( 1 ) in the diagram , namely that the incident and reflected electric fields minus the transmitted electric field must be zero , or equivalently that the total electric field at the interface must be continuous . since we defined y=0 as the plane of the interface , this boundary condition must hold for any value of x . this leads after a little algebra to equation ( 2 ) in the diagram . this equation depends only on the angles of incidence $\phi_{inc}$ and refraction $\phi_{tr}$ , the wave vector magnitudes in both media $k_1$ and $k_2$ , and the transmission and reflection coefficients $t$ and $r$ ( the fraction of energy that is transmitted into the new medium and reflected into the old medium respectively ) . by symmetry , $\phi_{inc} = \phi_{refl}$ . because the left side of equation 2 is independent of angle , so must the right side be . this leads to the term in the exponential being zero , which leads directly to snell 's law , using the relation between wave vector and index of refraction shown above . the group velocity never comes into play in the boundary conditions of refraction . it does come into play in the propagation of energy in the media ( as opposed to the fields ) , but that is another question .
the speed of the ions is slower than that of the electrons by a factor of the ratio of masses ( to first order ) which is of order 40,000 , and in a alternating field they do not go far enough to bother ourselves about before the field reverses . so , in principle yes , they drift , but they do not move far enough to accumulate near the cathode and for practical purposes you can ignore it . more over , if you did get a large separation it would surpress the field .
1 ) since u5 and i5 are given we can use ohm 's law to calcualte r5=90ω . good first step . 2 ) knowing u6 we can use the mesh-current method to determine u7=1v . knowing u6 , you have u7 by inspection - u6 and u7 are identical as these voltage variables are across the same two nodes . 3 ) again i used ohm 's law to calculate i7=80ma with u7=1v from 2 ) i7 is given as 80ma . no calculation required . what you should do here instead is calculate r7 = 1v / 80ma = 12.5 ohms if you need it . 4 ) ohm 's law to determine i6=10ma since we have u6 , r6 given . correct . now , you know i8 since , by kcl , i8 = i5 + i6 + i7 = 100ma . and now the dominoes topple . by kvl , u0 = u6 + u8 -> u8 = 1v . the value of r8 is given by ohm 's law : r8 = 1v / 100ma = 10 ohms . again , by kvl , u0 = u1 + u5 + u8 -> u1 = 0.1v now , since u1 = u2 = u3 = u4 , by inspection , we know i1 , i2 , and i3 using ohm 's law . by kcl , i5 = i1 + i2 + i3 + i4 . solve for i4 to get i4 = 2ma -> r4 = 50 ohms .
there are , in general , no closed form solutions ( aka formulas ) for the spectra of multi-electron atoms . there are reasonably precise formulas for special cases , like approximate values of x-ray transitions from inner shell electrons , though . unlike in case of hydrogen and rydberg atoms , which can be treated as a non-relativistic one-body problems ( i.e. . for which the schroedinger equation is a good approximation ) , no such simplification exists for atoms with z> 2 and more than one electron . precise calculation of spectral properties of heavy atoms requires a fully relativistic treatment of a quantum mechanical many-body problem . the correct theoretical framework for that is quantum electrodynamics ( qed ) , which is very complex and can only produce numerical results .
if you write $$f_{\mu\nu} = \partial_{ [ \mu}a_{\nu ] } $$ then your lagrange density is $${ \mathcal{l}} = \epsilon^{\mu\nu\alpha\beta}\partial_{ [ \mu}a_{\nu ] }\partial_{ [ \alpha}a_{\beta ] }$$ now we want a vector whose divergence is ${ \mathcal{l}}$ . the $\partial_{\mu}$ looks promising , so we ask if we can bring that outside so it acts on the remaining vector , i.e. $${ \mathcal{l}} = \partial_{\mu} ( \epsilon^{\mu\nu\alpha\beta}a_{\nu }\partial_{ [ \alpha}a_{\beta ] } ) \ \ ( 1 ) $$ we do not need to worry that we have lost the antisymmetrization brackets on $\mu$ and $\nu$ because the epsilon symbol forces this . as you pointed out , this is not quite what we started with because we have an additional term of the form $$ ( \epsilon^{\mu\nu\alpha\beta}a_{\nu }\partial_{\mu}\partial_{ [ \alpha}a_{\beta ] } ) $$ however the total antisymmetry of the epsilon symbol means we can treat the $\mu$ $\alpha$ $\beta$ contribution as $$\partial_{ [ \mu}f_{\alpha\beta ] } $$ which vanishes due to the maxwell equations . hence the anzatz ( 1 ) holds . ( the vector whose divergence we take looks like the abelian chern simons current ) .
it is indeed the case that due to gravitational time dilation a person on top of a hill would age slightly faster than a person at sea level . you do not need to climb hills to measure this effect . gravitational time dilations due to height differences of a few feet have been measured in the laboratory . the effect is tiny though . in the neighborhood of a large spherically-symmetric massive object such as earth , and compared to being infinitely far away from the object , your aging slows down by a factor $$\sqrt{1\ -\ \frac{v_{esc}^2}{c^2}}$$ here $v_{esc}$ is the velocity needed at your particular position to escape from the gravitational pull of the object , and $c$ the speed of light . when comparing two positions near to each other and close to the object , we can derive a simple equation that describes the relative time dilation . for two positions close to earth with height differences much smaller than the radius of earth , the fractional time dilation is given by $g h / c^2$ , where $g$ denotes the local gravitational acceleration and $h$ the height difference . for a hill on earth that peaks 90 m ( 300 ft ) high , and using $g=10\ m/s^2$ and $c=3\ 10^8 m/s$ , we find $g h / c^2 \approx 10^{-14}$ . in other words , over a period of 3 years , a person on top of the hill would age one millionth of a second ( one microsecond ) more compared to a person at the foot of the hill .
water waves are rather complicated , and the differential equations which describe them are call boussenesq equations . a tsunami is not a transverse wave . it is a pressure wave with a longitudinal mode . it also travels very fast at about 700km/hr . what happens is that this travels as a pressure wave in the open ocean , but when it reaches a continental shelf the wave is reflected partially upwards . this has the effect of converting it into a transverse wave as water moving along is now pushed upwards . this is a very nonlinear process and nontrivial to model . this pushing up of the water does initially cause water at the shore to recede outwards . the wave which seconds later reaches shore is much more slow moving , and a lot of that wave energy is converted into the towering wave front that sweeps in .
when we measure a red shift , what we are actually measuring is the absorption spectra of elements in the stars and dust clouds in the target galaxy . these absorption spectra have well known patterns of lines , and when red shifted the entire pattern moves to lower energy/longer wavelength . so you are correct that you can not measure the red shift from just one line , because you do not know where that line was originally , but you can do it when you measure a known pattern of many lines .
well , there is the kawai-lewellen-tye ( klt ) relations , which says that a closed string amplitude is roughly speaking a product of two open string amplitudes . see e.g. ref . 1 . references : z . bern , perturbative quantum gravity and its relation to gauge theory , living rev . relativity 5 ( 2002 ) 5 ; section 3.1 .
this is the distinction between continuous and discrete spectrum , but only considering the low energy excitations . for an hamiltonian with gapped spectrum , the lowest eigenvalue is separated by a gap from $e=0$ . for example dispersion relation of the form $e=|k|$ is an example of gapless spectrum , and $e=|k+m|$ is an example of gapped one , where $k$ is the wave vector ( which can be any real number ) and $m$ is the mass ( = gap ) . this distinction leads to qualitative difference in the physics - for example the difference between a material being a conductor or an insulator . many times also , the gap is generated by interesting physics ( like the mass gap in yang-mills theory , or the gap in bcs superconductivity ) .
dbrane , aside from " beauty " , the electroweak unification is actually needed for a finite theory of weak interactions . the need for all the fields found in the electroweak theory may be explained step by step , requiring the " tree unitarity " . this is explained e.g. in this book by jiří hořejší: http://www.amazon.com/dp/9810218575/ google books : http://books.google.com/books?id=mnnagd7otlicprintsec=frontcoverhl=cs#v=onepageqf=false the sketch of the algorithm is as follows : beta-decay changes the neutron to a proton , electron , and an antineutrino ; or a down-quark to an up-quark , electron , and an anti-neutrino . this requires a direct four-fermion interaction , originally sketched by fermi in the 1930s , and improved - including the right vector indices and gamma matrices - by gell-mann and feynman in the 1960s . however , this 4-fermion interaction is immediately in trouble . it is non-renormalizable . you may see the problem by noticing that the tree-level probability instantly exceeds 100% when the energies of the four interacting fermions go above hundreds of gev or so . the only way to fix it is to regulate the theory at higher energies , and the only consistent way to regulate a contact interaction is to explain it as an exchange of another particle . the only right particle that can be exchanged to match basic experimental tests is a vector boson . well , they could also exchange a massive scalar but that is not what nature chose for the weak interactions . so there has to be a massive gauge boson , the w boson . one finds out inconsistency in other processes , and has to include the z-bosons as well . one also has to add the partner quarks and leptons - to complete the doublets - otherwise there are problems with other processes ( probabilities of interactions , calculated at the tree level , exceed 100 percent ) . it goes on and on . at the end , one studies the scattering of two longitudinally polarized w-bosons at high energies , and again , it surpasses 100 percent . the only way to subtract the unwanted term is to add new diagrams where the w-bosons exchange a higgs boson . that is how one completes the standard model , including the higgs sector . of course , the final result is physically equivalent to one that assumes the " beautiful " electroweak gauge symmetry to start with . it is a matter of taste which approach is more fundamental and more logical . but it is certainly true that the form of the standard model is not justified just by aesthetic criteria ; it can be justified by the need for it to be consistent , too . by the way , 3 generations of quarks are needed for cp-violation - if this were needed . there is not much other explanation why there are 3 generations . however , the form of the generations is tightly constrained , too - by anomalies . for example , a standard model with quarks and no leptons , or vice versa , would also be inconsistent ( it would suffer from gauge anomalies ) .
the coulomb energy goes like $z ( z-1 ) $ , so it is typically a very small effect for light nuclei . the coulomb energy would be responsible for the difference in binding energy between 3he and 3h . in general , even-even nuclei are always more bound than odd nuclei . this is due to pairing . apart from pairing , one can usually predict nuclear binding energies quite well at a quantitative level by considering them to be the sum of a liquid-drop energy and a shell correction ( strutinsky 1968 ) . the shell correction for 4 he is large , because it is doubly magic . ( i do not actually know if the strutinsky works well for such light nuclei , but it is definitely good enough to give a correct qualitative explanation . ) “shells” in deformed nuclei . v . m . strutinsky , nucl . phys . a 122 no . 1 ( 1968 ) pp . 1-33 . see also : curvature correction in the strutinsky 's method . p . salamon , a . t . kruppa . j . phys . g : nucl . part . phys . 37 no . 10 ( 2010 ) 105106 . arxiv:1004.0079 . ( a variation on the technique , describes the technique itself . )
there are numerous distance indicators used for within the galaxy . the most common way is by using intrinsic magnitude . by knowing how bright an object would be if we were close , we can determine how far away it is by how dim it is . there are many types of stars where we have a rough idea of how bright they should be due to characteristics of the star : cephied variables : the original type of variable star that was used by hubble to determine the distance to the andromeda galaxy . rr lyrae variable : like the cephied variable , but usually dimmer . type 1a supernova : these guys , unlike the first two , are cataclismic variables . essentially a binary white dwarf slowly accretes matter from its binary till it reaches the chandrashankar limit , after which point it explodes in a very characteristic way ( since the mass at the time of explosion is roughly constant ) . main sequence stars : generally less accurate than the first 3 , there are some types of main sequence stars which are used to find distances in a similar way . there are a few other ways we can measure distances : perpendicular movement : for example there is a " light echo " from sn 1987a which is essentially light from the supernova interacting with dust around the old star . since this echo should be expanding at the speed of light , we can tell how far away the nova is by the angular velocity of the light . relative velocity in a moving cluster : ( see dmckee 's answer ) tulley-fisher relation : a relationship between the luminosity of the galaxy and it is apparent width . can be used as a decent distance calculator . faber-jackson relation : similar to tulley-fisher , relates luminosity with radial velocity dispersion rate . edit : some more information about redshifts . the whole relationship between redshift and distance was in fact established by hubble by relating distance to cephied variables ( i believe ) with redshift . later on it was made more precise using supernova , which are brighter and can be seen from much father away ( i think recent supernova can be occasionally seen around z=2 , while cephieds are all z&lt ; 1 ) . within a galaxy , redshift cannot be used directs since the " peculiar velocity , " the velocity within the galaxy , completely overshadows the effects of universe expansion on which hubble 's law is based . redshift within the galaxy is useful for certain other techniques . edit : corrected a few minor errors .
if you want to generalize a potential to a class that is broader than the simple $\frac12 k_2 x^2$ , it is tempting as a first step to include a small perturbation of the form $\frac13k_3x^3$ . unfortunately , this drastically changes the structure of the potential , because it becomes unbounded from below . thus , you might get a slightly perturbed behaviour from a harmonic oscillator at low amplitudes , but if you drive it hard enough , then the system will cross some hill and then just run away . this behaviour is simply not what one is trying to model , which is the back-and-forth motion about a potential minimum . it is indeed possible ( or i do not see why it would not ) to do a proper analysis of this case in the limit of small enough oscillations that the system never sees the hill , while the harmonic oscillation is still perturbed . however , the existence of the hill and its other side makes the general solution very complicated and quite different to what you really want . for a real potential , of course , there will be a term in $x^4$ that kicks in way before the system sees the hill . thus , after the harmonic oscillator , the next class of models that really capture the type of back-and-forth behaviour one is trying to model , for all amplitudes and all driving forces , is of the form $$v ( x ) =\frac12 k_2 x^2+\frac13 k_3 x^3+\frac14 k_4 x^4 . $$ this succeeds in fixing the problems with a pure $x^3$ perturbation , but in solving those problems we have made the model quite a bit more complicated than we were really hoping for . ( for instance , we now have two dimensional constants , the lengths $k_2/k_3$ and $k_3/k_4$ , instead of just one , and the interplay between those two will affect the behaviour . for example , if $k_3^2&gt ; \frac92k_2k_4$ the potential develops a second dip , which you do not really want . ) thus , i think the useful classification is the duffing potential , $v ( x ) =\frac12 k_2 x^2+\frac14 k_4 x^4$ , is the simplest potential which generalizes the harmonic oscillator to the anharmonic case while avoiding runaway solutions for all starting positions . that said , if you start generalizing this model , your first step should be to put the cubic term back in . even then , the requirement that $k_3^2$ be bounded by $k_4$ means that the cubic term must be a perturbation on top of the quartic behaviour , instead of the other way around . there is an additional physical reason why odd-order perturbations to the potential are not very common , and it is due to the type of symmetry - or lack thereof - your system has . specifically , even-order terms in the potential are symmetric around the equilibrium point , but including odd-order terms will make for a lopsided potential that is not symmetric any more . for all oscillating systems , if you drive them hard enough they will become anharmonic . however , for odd-order nonlinearities to come into play , the system itself needs to be asymmetric , and that need not be the case . indeed , depending on the setting , it can be quite hard to make a system that is asymmetric enough for those effects to be noticeable . a good example of this is in nonlinear optics , where the relevant perturbation expansion is that of the polarization 's response to the driving electric field : $$p=\chi e+\chi^{ ( 2 ) }e^2+\chi^{ ( 3 ) }e^3+\cdots , $$ ignoring the vector/tensor character of these quantities . it generally requires pretty high intensities to drive materials into those regimes , so using $\chi^{ ( 2 ) }$ nonlinearities seems a lot better than using $\chi^{ ( 3 ) }$ ones . however , for most materials , the $\chi^{ ( 2 ) }$ susceptibility is zero because of symmetry considerations : the medium needs to be asymmetric on the scale of a few atoms . this is possible , for example , in non-centrosymmetric crystals , but without such a material you can not do any three-wave mixing process , which rules out second-harmonic generation , sum- and difference-frequency generation , optical parametric amplification , and a host of other toolkit essentials . so , the bottom line on this is : the symmetry of your system matters . you can only get odd-order perturbations to the potential if your system itself is asymmetric . even orders , on the other hand , you get for free by simply driving it hard enough . regarding the last part of your question , " mixing " does have a specific meaning in this context . if you have a harmonic driving that is also being driven harmonically , $$m\ddot x+m\gamma\dot x+m\omega_0^2 x=f\cos ( \omega t ) , $$ then the solution will also oscillate at the driving frequency $\omega$ , with possibly a phase delay which is not really important . however , when you include a cubic or quartic term perturbatively , the first step is to treat those terms as external forces with the position given by the unperturbed solution , say $x ( t ) =x_0\sin ( \omega t ) $ . this means that you have a harmonic system that is got an additional driving of the form $$k_3 x_0^3 \sin^3 ( \omega t ) \quad\text{or}\quad k_4 x_0^4 \sin^4 ( \omega t ) . $$ these are hard to deal with in that specific form , but they become quite a bit easier to handle if you use the appropriate trigonometric identities to reduce them to a finite sum of harmonic drivings . thus , the forces above reduce to $$ \begin{cases} k_3 x_0^3 \sin^3 ( \omega t ) = \frac{3\sin ( \omega t ) -\sin ( 3\omega t ) }{4}\\ k_4 x_0^4 \sin^4 ( \omega t ) =\frac{3-4\cos ( 2\omega t ) +\cos ( 4\omega t ) }{8} . \end{cases} $$ this is then a harmonic driving on a harmonic system , and that we can solve . ( it is only the first term in a perturbation series , but that is another story . ) the mixing you read about is the fact that the final solution will contain terms that oscillate at the original frequency , but it may also contain other harmonics . as you can see from the above , even harmonics lead to a dc perturbation plus a $2\omega$ contribution . there will also be additional terms at higher harmonics , but those are typically harder to detect . an odd-power perturbation , on the other hand , will result only in odd harmonics , including a component at the driving frequency ( "operating frequency band" ) . in terms of the time dependence of the solution $x ( t ) $ , this is probably not that important . however , there are large classes of systems which are easier to interact with on the frequency domain , and then these harmonics are the best way to physically understand the solutions . for example , if you are driving some slightly nonlinear electric oscillator , you are probably doing so at rf or microwave frequencies , and then it is very easy to study the output of the system via the fourier transform capacity of an oscilloscope . once you are there , detecting peaks at $2\omega$ or $3\omega$ can be relatively easy , and it speaks directly as to the type of nonlinearity that is present in the system .
the boiling point of liquid oxygen is 90k , so it is easily condensed by liquid nitrogen . i have personally made lox by pumping air through a glass u tube immersed in liquid nitrogen , so i can confirm it works . later : as discussed in the comments , what condenses is a mixture of liquid oxygen and nitrogen rather than pure liquid oxygen . the dew point for air is about 82k , far enough above the boiling point of liquid nitrogen for a condensate to form , and the condensate is about 50% liquid oxygen . this article includes the relevant phase diagram . from personal experience i can say the condensate is blue , though i did not test its magnetic properties or the violence of its reaction with organic materials .
if you have ever swum to the bottom of a swimming pool you will know that in water the pressure increases as you go deeper . at a depth of about 10 metres the pressure is twice what it is at the surface , but the water 10 metres down does not burst up to the surface because it is held down by the weight of water above it . in fact the increase of pressure with depth is exactly the weight of water above . exactly the same is true of the atmosphere . the pressure at ground level is 101,325 pa because each square metre of the ground has about 10,329 kg of air above it ( 10329 kg times the acceleration due to gravity 9.81 m/sec$^2$ = 101325 pa ) . if you could magically remove the 100 km or so of atmosphere that is above some patch of air at ground level that air would indeed immediately expand upwards . incidentally , bernoulli 's principle is unrelated to the problem .
if a glass of wine ( for example ) only has meaning to a human , what makes a shattered spilled glass any less orderly than a full whole glass ? symmetries . symmetrical systems have smaller entropy as defined by delta ( s ) =delta ( q ) /t ) because to break the symmetry energy/heat has to be expended . or if a particle is observed , and it is probability wave collapses , is not this moving from a ' chaotic ' ( uncertain and in many places ) , to an ' orderly ' ( certain , definite position ) state ? the crux in this sentence is " observed " . to observe there must be interactions and the interactions of observing add energy and disorder to the system . it is only in closed hermetically systems that the entropy is 0 or increasing statement can be evaluated . think of biological systems . they are par excellence reducing entropy in their bodies , but they are not in a closed system , they die if the y cannot continually exchange energy and chemicals with the larger surroundings increasing entropy in the exchanges . also , is not the formation of stars , planets , galaxies etc . . . a move from a more chaotic state to a more orderly one ? in an individual star , no , the heat goes up within when the star is formed from the big bang debris , much faster than the temperature which starts at near zero kelvin ; also a lot of radiation is created and escapes , increasing the entropy of the universe .
the clockwise direction is normally defined by the right hand grip rule . when your thumb is pointing away from you , your fingers are curled clockwise . so when you look at a clock the axis of rotation is away from you through the clock . i would guess the downvotes are because people believe your question is not physics related , but in fact this rule is how you determine the direction of the angular momentum vector , so there is a connection with physics .
given that there has not been any acceleration to cause these velocities , [ . . . ] as a side issue , even in newtonian mechanics , accelerations do not cause velocities . accelerations are just a measure of how rapidly velocities are changing . what you are running into here is the fact that general relativity does not have any notion of how to measure the motion of object a relative to a distant object b . it is neither true nor false that a and b gain relative velocity due to cosmological expansion . it is neither true nor false that a and b have nonzero accelerations relative to one another . frames of reference in gr are local , not global . it is valid to say that distant galaxies are moving away from us at some velocity . it is also valid to say that everything is standing still , but the space between us and the distant galaxy is expanding . [ . . . ] are there still relativistic effects in play ? that is , is there time dilation between the two frames ? kinematic time dilation is well defined in sr , which means that in gr it is only defined locally . gravitational time dilation is only well defined in gr in the case of a static spacetime , but cosmological spacetimes are not static . so it is neither true nor false that there is time dilation between us and a distant galaxy . concretely , you could measure doppler shifts . if you feel like interpreting these shifts in purely kinematic terms , you can assign a velocity to the distant galaxy relative to us . but this is not mandatory and actually does not really work very well , in the sense that the velocity you get is usually several times smaller than the rate at which the proper distance between the galaxies is increasing . ( proper distance is defined as the distance you would measure with a chain of rulers , each at rest relative to the cmb , at a moment in time defined according to a notion of simultaneity defined by cosmological conditions such as the temperature of the cmb . ) in particular , there are galaxies that we can observe that are now and always have been receding from us at $v&gt ; c$ , if you define $v$ as the rate of change of proper distance . the fact that we can observe them tells us that their doppler shifts are finite and correspond to $v&lt ; c$ . here is a nice popular-level article that explains a lot of this kind of stuff : davis and lineweaver , " misconceptions about the big bang , " http://www.scientificamerican.com/article.cfm?id=misconceptions-about-the-2005-03 it is paywalled , but there are lots of copyright-violating copies floating around on the web . the following is a presentation of the same material at a higher level : davis and lineweaver , " expanding confusion : common misconceptions of cosmological horizons and the superluminal expansion of the universe , " http://arxiv.org/abs/astro-ph/0310808
that is a very interesting question . the short of it is that gravity is always attractive . when you imagine a gravitational wave ( gw ) going by , its not like the gravity is pushing and pulling on the object , its like it pulling-less , and pulling-more . by analogy its a lot like the tides . there is a tidal bulge on the side of the earth opposite the moon , not because the moon is pushing that part away from it , but because its just being pulled less than the earth itself .
the induced metric on a sphere $s^2$ of radius $r$ is given by , $$\mathrm{d}s^2 = g_{\mu\nu}\mathrm{d}x^\mu \mathrm{d}x^\nu = r^2\mathrm{d}\theta^2 + r^2 \sin^2 \theta \ , \mathrm{d}\phi^2$$ we find $\sqrt{g}=r^2 \sin \theta$ , and the surface area must be given by the integral , $$a = \int_{s^2} \mathrm{d}^2 x \ , \sqrt{g}= \int_{0}^{2\pi} \ ! \ ! \mathrm{d}\phi \int_0^\pi \ ! \mathrm{d}\theta \ , ( r^2 \sin \theta ) =4\pi r^2$$ if a net charge $q$ is distributed evenly on the surface of a sphere , the charge density is , $$\sigma = \frac{q}{a} = \frac{q}{4\pi r^2}$$ the integral provided in the question is also an integration over a sphere of radius $a$ , but in more generality for any angle $\theta$ ; we chose $\theta=\pi$ to integrate over all of $s^2$ . otherwise , the integral is nothing more than that . addendum : induced metric how did we find $g_{\mu\nu}$ ? the embedding of a sphere $s^2$ in $\mathbb{r}^3$ is given by , $$x^\mu =\left ( r\cos\theta\sin\phi , r\sin\theta\sin\phi , r\cos\phi\right ) $$ the induced metric is given by the pullback of $\mathbb{r}^3$ onto the sphere , $$g_{ab}=\frac{\partial x^\mu}{\partial \sigma^a} \frac{\partial x^\nu}{\partial \sigma^b}\delta_{\mu\nu} $$ as $\delta_{\mu\nu} = \mathrm{diag} ( 1,1,1 ) $ is the metric of $\mathbb{r}^3$ . the embedding itself is a solution to , $$x^2 +y^2 +z^2 = r^2$$ which is the standard equation of a sphere in $\mathbb{r}^3$ centered at the origin .
the stationary action principle and the euler-lagrange ( el ) equations are very broad and general constructions . the field variables in the variational principle could in principle map into some generic manifold $m$ . on the other hand , euler-poincare ( ep ) equations appear in the special situation where the manifold is a lie group $m=g$ , and the action is left-$g$- invariant . one next uses the exponential map to make the variables lie algebra-valued ( rather than lie group-valued ) . the ep equations reads $$ \left ( \frac{d}{dt}+ {\rm ad}^{\ast}_{\xi}\right ) \frac{\delta \ell}{\delta\xi}~=~0 , $$ where the variables $\xi$ are lie algebra-valued . the lie algebra-valued ep equations are equivalent to the lie group-valued el equations for the same problem . see ref . 1 for further details . the euler ( e ) equations for a rigid body are a special case of the ep equations . references : j.e. marsden and t.s. ratiu , intro to mechanics and symmetry , 2nd eds , 1998 ; section 13.5 .
your interpretation is partially correct . complex numbers are superimposition of two independent variables . coming to electromagnetic ( em ) waves we consider two components of em wave which are perpendicular to each other . this two components do not interact with each other but they are both part of a single wave . so the vector some of these components is the magnitude of em wave . complex component ( i ) of the em wave is not a dimension , it is used to avoid confusion . i could have written it in terms of simple vectors but these em components do not interact with each other . i can introduce numerous components of an em wave . f ( x ) =a ( x ) + i . b ( x ) + j . c ( x ) + k . d ( x ) + l . e ( x ) the move equation is a higher order complex equation . i have number of complex planes and all are independent of each other . fourth dimension exists in em wave and it is time but not polarization .
the main difference between hamilton and lagrange/newton mechanics is , that it happens directly on the phase space , i.e. any point on your manifold already fully determines the state of your system . intuitively , you realize this by specifying position and momentum coordinates . on a mathematical level , the world we see is some smooth manifold ( a priori not even necessarily riemann ) , this is the place where you specify your position coordinates . to specify momentum coordinates , you have to consider points from the cotangent space , therefore the structure where you can fully specify your state is the full cotangent bundle of your original manifold . however , the cotangent bundle may be regarded as manifold on its own right ( with twice the dimension of the original manifold ) . furthermore , you can construct a canonical ( ! ) symplectic structure quite easily by using the derivative of the natural projection . the reason , why it makes sense to put the " original manifold " aside and operate only on symplectic manifolds resp . cotangent spaces , is darboux ' theorem , which basically says that every symplectic manifold is actually locally equivalent to the cotangent space of some manifold . polemically simplified , one could say " the cotangent bundle is the symplectic manifold " . lets now take a closer look at the symplectic structure : locally , it looks like $$ \omega=d\vec{q}\wedge d \vec{p}=dq_i\wedge dp^i $$using this , you can construct a volume form ( up to some constant ) as $\omega^n$ where $2n$ is the dimension of your manifold . especially , you see that this volume is even oriented - locally , you can actually imagine some region as a region in euclidean space . the famous liouville theorem states , that the hamiltonian phase flux ( i.e. . the one-parametric group of the corresponding autonomic system ) leaves the symplectic structure intact , therefore it also preserves the volume . this is an important consequence especially in statistical mechanics . however , this is not main the point about the symplectic structure . the important thing is , that the symplectic form together with the hamiltonian determines the trajectory : trajectories are defined as integral curves of the hamiltonian vector field $x_h$ , which is uniquely determined by the hamilton function and the symplectic structure by $$ \omega ( \cdot , x_h ) =dh $$ vividly spoken : if you throw some particle onto a symplectic manifold , it will move along the hamiltonian vector field . the point now about canonical transformations is , that it has the symplectic form as an integral invariant , or , mathematically speaking , it is a special symplectomorphism . of course , this yields as consequence , that the local equations governing the trajectories are not affected . you can therefore think of a canonical transformation just as a reparametrization of some region of the symplectic manifold , just like a change from cartesian to spherical coordinates . in this sense , the major advantage over lagrange is , that you can simultaneously transform both $p$ and $q$ , which of course leads to much easier equations in the end , as you can use canonical transformations to make all coordinates cyclic . this is the basic idea of hamilton-jacobi theory .
i ) yes , e.g. all three mandelstam variables $$ s~:=~ ( p_1+p_2 ) ^2~=~m_1^2+m_2^2+2 p_1\cdot p_2 ~\approx~ ( m_1+m_2 ) ^2 + m_1m_2 ( {\bf v}_1-{\bf v}_2 ) ^2 ~&gt ; ~0 , $$ $$ t~:=~ ( p_1-p_3 ) ^2~=~m_1^2+m_3^2-2 p_1\cdot p_3~\approx~ ( m_1-m_3 ) ^2 - m_1m_3 ( {\bf v}_1-{\bf v}_3 ) ^2 ~&gt ; ~0 , $$ $$ u~:=~ ( p_1-p_4 ) ^2~=~m_1^2+m_4^2-2 p_1\cdot p_4~\approx~ ( m_1-m_4 ) ^2 - m_1m_4 ( {\bf v}_1-{\bf v}_4 ) ^2 ~&gt ; ~0 , $$ are strictly positive in the non-relativistic limit $$|{\bf v}_i|~\ll~ c , \qquad i~\in~\{1,2,3,4\} , $$ of massive particles $$m_i~&gt ; ~ 0 , \qquad i~\in~\{1,2,3,4\} , $$ with unequal ( rest ) masses $$i~\neq~j~~\rightarrow~~ m_i~\neq~ m_j , \qquad i , j~\in~\{1,2,3,4\} . $$ here we have used units where $c=1$ , and the non-relativistic formulas $$ {\bf p}_i~\approx~m_i{\bf v}_i , \qquad e_i~=~\sqrt{m_i^2+{\bf p}_i^2}~\approx~m_i\left ( 1+ \frac{{\bf v}_i^2 }{2}\right ) , \qquad i~\in~\{1,2,3,4\} , $$ and $$ p_i\cdot p_j~=~e_i e_j - {\bf p}_i\cdot {\bf p}_j~\approx~~m_i m_j \left ( 1+ \frac{1}{2}\left ( {\bf v}_i-{\bf v}_j\right ) ^2 \right ) , \qquad i , j~\in~\{1,2,3,4\} . $$ ii ) by the way $s+t+u=\sum_{i=1}^4 m_i^2 \geq 0$ implies that it is impossible to have all mandelstam variables $s , t , u&lt ; 0$ negative . so at least one of the three sectors are physical .
what does it mean to integrate $\frac{d\mathbf p}{dt}dt$ ? first , and in scalar form , recall from elementary calculus that $$\int_{x_1}^{x_2} dx = x_2 - x_1 $$ second , recall that $$f ( x + dx ) = f ( x ) + f' ( x ) dx$$ where $$f' ( x ) = \frac{df ( x ) }{dx} $$ denoting the differential of $f$ as $$df = f ( x + dx ) - f ( x ) $$ we have $$df = f' ( x ) dx$$ since $$\int_{x_1}^{x_2} f' ( x ) dx = f ( x_2 ) - f ( x_1 ) = f_2 - f_1$$ it follows that $$\int_{f_1}^{f_2} df = f_2 - f_1 = f ( x_2 ) - f ( x_1 ) = \int_{x_1}^{x_2} f' ( x ) dx$$
in principle of course you try something like that . but there are three issues that will kill you : $q$ . every resonance has a quality factor which represents how quickly the energy in the mode drains away by assorted dissipative processes . i do not know what it is for the schumann resonances , but i will give you long odds that it is not good : much of the energy you put into the field will just dribble away into space . power density . whatever energy you pump into these modes will spread out over the whole cavity , and you will only be able to draw as much as there is in the region covered by your antenna , which will be effective nothing even with gigawatts driven into the resonance . not only could not you power a iphone , you could not power the little shoplifting-prevention tag that retailers put onto bits of mobile merchandise . antenna dimensions . the naive way to design an antennas to use at frequency $f$ requires conductors of length on order of $c f$ . bit of a problem for frequencies of a few or few tens of hertz .
do not forget the context you are working in . you are solving for the phasor voltage across the resistor . when you measure the actual time domain voltage with , say , an oscilloscope , you will see a sinusoid with an amplitude and a phase ( referenced to the source $u_e$ ) . the magnitude of $u_a$ is the amplitude you will measure . the angle ( phase ) of $u_a$ is the phase you will measure . update : in response to the last question : first , we limit $\omega$ to being a real number . now , if you work out $\frac{u_a}{u_e}$ by hand ( something i highly recommend if you have not ) , you should get ( assuming i have not made an error ) : $\frac{u_a}{u_e} = \dfrac{j \omega rc}{1 - ( \omega rc ) ^2 + j3 \omega rc}$ now , you can " see " where this is maximum without calculation . as $\omega$ increases from zero , the real part of the denominator is decreasing and becomes zero when $\omega = \frac{1}{rc}$ . from that point on , the magnitude of the denominator increases faster than the magnitude of the numerator .
there is a lot of ambiguity in the definition of the stress-energy tensor . the stress-energy tensor is a conserved current , and like all conserved currents it is only defined up to a total divergence . i assume this $t_{\mu \nu}$ was calculated using the canonical prescription $ t^\mu_\nu=\frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi^i ) }\partial_\nu \phi^i-\mathcal{l}\delta^\mu_\nu $ ( you seem to be missing the second piece , or you are dealing with a massless field ) . the canonical tensor is not symmetric for fields with spin . essentially , the intrinsic angular momentum is also contributing to t . so you find a term $s^\lambda_{\mu \nu}$ satisfying $\partial_\lambda s^\lambda_{\mu \nu}\approx t_{ [ \mu \nu ] }$ ( s is antisymmetric in its first two indices , and thus has vanishing divergence ) and add it to the canonical tensor . see this worked out in detail here http://en.wikipedia.org/wiki/belinfante%e2%80%93rosenfeld_stress%e2%80%93energy_tensor this procedure might seem a little random , but of course what you really should be doing is obtaining t from $t^{\mu \nu}=\frac{\delta s}{\delta g_{\mu \nu}}$ as in general relativity . this $t$ will always be symmetric , and is in fact the same as the belinfante tensor . however , there is still ambiguity in this procedure . in order to obtain t this way , you have to " covariantize " the theory , promoting the metric to a dynamical field . this covariantization is ambiguous : you may couple the metric to the curvature non-minimally . these couplings vanish in the flat space limit , but can still affect the expression for t . but at least this expression will always be symmetric . hope this helps !
electric field lines are a visualization of the electrical vector field . at each point , the direction ( tangent ) of the field line is in the direction of the electric field . at each point in space ( in the absence of any charge ) , the electric field has a single direction , whereas crossing field lines would somehow indicate the electric field pointing in two directions at once in the same location . field lines do cross , or at least intersect , in the sense that they converge on charge . if there is a location with charge , the field lines will converge on that point . however we typically say the field lines terminate on the charge rather than crossing there .
the article you refer to is about the electrolytic splitting of water . a 100% efficient electrolytic cell would require a voltage of about 1.23v to split water , but for various reasons a simple electrolytic cell requires about 1.48v . the difference between the voltages is called the overpotential , and it increases the amount of power needed to split the water because the power required per unit of hydrogen produced is proportional to the cell voltage . the excess power goes into heating the hydrogen and oxygen produced , and in this case it means that simple cells are about 83% efficient at converting electricity into hydrogen . catalysts can be used to increase the efficiency , and indeed platinum based catalysts can be used to reduce the overpotential and make cells with near 100% efficiency . the problem is that platinum is expensive . the result from the stanford team is that a much cheaper nickel based catalyst can achieve the same efficiency as platinum . the paper is here , but note that it is behind a paywall . if the catalyst proves to be stable enough then it will be useful for electrolytic production of hydrogen , but the improvement in efficiency is not going to change the world overnight . it still takes a lot of power to electrolyse water so it is only feasible when cheap electricity is available .
momentum is conserved in magnitude and direction . so in order to analyze any situation of momentum conservation , you should always start with $$ \sum \mathbf p_{i}=\sum\mathbf p_f $$ where the subscripts denote the initial and final momenta . as to the ball and wall , you are correct that momentum is not conserved if you are only looking at the ball . if you consider that the system includes the wall , then the momentum conservation holds . this does mean that the wall contains a momentum of $2mv$ ( for mass $m$ and velocity $v$ ) . but note that since the mass of the wall is incredible compared to the ball , the velocity is notably imperceptible !
olber 's paradox assume infinite and static universe ( infinite life of universe and stars too ) . cosmological redshift due to universe 's expansion shift visible light to infrared or microwave region of electromagnetic spectrum . the cmb radiation is the most clear effect . observable universe is finite ( more or less 93 billion light years ) . this limit the number of galaxies from which we receive radiation . stars have finite life . in addition also some part of universe with recession velocity greater than c could enter ( in the future ) in the event horizon , but they will red shifted at z more than 1.8 , away from visible region . reference : arxiv:astro-ph/0310808v2 13 nov 2003 peacock cosmological physics cambridge press 2010 ( section 12.1 ) davis linewear misconceptions about big bang scientific american march 2005
i am not sure how useful this " back of the envelope " calculation of reliability of numerical weather prediction is going to be . several of the assumptions in the question are not correct , and there are other factors to consider . here are some correcting points : the weather is 3 dimensional and resides on the surface of the planet up to a height of at least 10km . furthermore the density decreases exponentially upwards . many atmospheric phenomena involve the third dimension such as rising and falling air circulation effects ; jet streams ( 7-16km ) . the equations are fluid dynamics plus thermodynamics . the navier-stokes equations are not only too complex to solve , but in a sense inappropriate as well for the larger scales . one problem is that they might introduce " high frequency " effects ( akin to every individual gust of wind or lapping of waves ) , which should be ignored . the earliest weather prediction models were seriously wrong because the high frequency fluctuations of pressure needed to be averaged rather than directly extrapolated . here is a possible equation for one point of the atmosphere : tchange/time = solar + ir ( input ) + ir ( output ) + conduction + convection + evaporation + condensation + advection the regionality of the model is important too . in a global model there will be larger grid sizes and sources of error from initial conditions and surface and atmosphere top boundary conditions . in a mesoscopic prediction there will be smaller grid sizes but sources of error from the input edges as well . the smallest scale predictions of airflow around buildings and so on might be a true cfd problem using the navier-stokes equations however . i dont know that any calculation is done to predict the inaccuracies , although the different types including the numerical analysis ( chaos ) error sources can be studied separately . models are tested against historical data for accuracy overall with predictions made 6-10 days out . to assume that the atmosphere " goes turbulent " after 3 days seems to conflate several issues together .
the link does not work for me either , but it is not obvious that this is a typo ( http://cartech.ides.com/imagedisplay.aspx?e=111imgurl=%2fcarpenterimages%2fa-toolanddie%2f23-ts23-micromelta11%2f05_ts23_3point.gifimgtitle=3-point+bend+test , http://www.matweb.com/search/datasheet.aspx?matguid=638937fc52ca4683bc0c3f18f54f5a24 ) . and it looks like this strength is indeed the highest for an alloy ( http://gofygure.hubpages.com/hub/what-is-the-strongest-metal-the-hardest-metals-known-to-man )
the best you will get is ‘middle/small’ , assuming you treat humans as a whole . here’s why : usually , ‘large scale’ physics as given by gr ( or even sr ) does not apply to us : the gravitational force between two humans is small and the curvature in spacetime caused by a human being is absolutely negligible . at the same time , ‘small scale’ physics , described by quantum mechanics , does not apply to us either : you can run against a wall as often as you like , but you won’t get through it . however , this point relies on the description of humans as a whole , rather than made up of ‘small’ atoms . this is only partly true for us , as many biological systems rely - at least to some extent - on quantum mechanical ( non-classical ) effects . were minor parametres ( magnitude of the van der waals force , for example ) to change suddenly , we certainly would notice ( and die ) . planetary objects , stars and black holes nearly do not rely on such small-scale effects . nuclear fusion will probably even take place if some forces double or triple and a black hole won’t stop existing just because quantum fluctuations change slightly . hence , as we rely ‘more’ on small-scale physics than on large-scale physics , we are certainly more on the ‘small’ side of stuff , but since we are much larger than actually ‘small’ things ( atoms ) , ‘middle/small’ is probably the best you can get . but really , ‘large’ and ‘small’ only make sense in relation to some other object : we are about as large as trees , smaller than the moon , much smaller than the sun and much larger than atoms . i guess you knew that already ☺ .
you want the time . simply put , for that the minimum requirement is position ( and hence the distance ) and velocity . to know the position you need to detect it . once you detect it , you can calculate the trajectory and thus the time you have to settle your issues ( assuming it is on a collision course ) . i got this from cnn : the b612 foundation is building the sentinel space telescope , the world 's most powerful asteroid detection and tracking system , to see the millions of asteroids we can not see today and could pose threats to our planet . also , neossat , the near earth object surveillance satellite , is a micro-satellite launched in february 2013 by the canadian space agency ( csa ) that will hunt for neos in space . tracking systems are recording asteroids even as large as 140 meters . any asteroid with a radius more than 300 meters means an assured global catastrophe . check this out . the size that you are asking about is so big that it will create noticeable gravitational effects ( like perturbation in orbit ) and so we will know about it .
there is a critical current density for every superconductor where the superconductor acts as an ordinary conductor and a voltage difference can be measured between its ends .
for an object following a track with speed $v$ , you can decompose the velocity vector and acceleration vector as follows : $$ \vec{v} = v \hat{e} \\ \vec{a} = \dot{v} \hat{e} + \frac{v^2}{\rho} \hat{n} $$ where $\hat{e}$ is the tangent vector , and $\vec{n}$ the normal vector , but you already know that ( i hope ) . in 2nd case $\rho=\infty$ and in the first case the path curvature $\rho$ is finite . to tie all this with the fbd you need to decompose the applied forces along $\hat{e}$ and $\hat{n}$ to see what contributes to speed change $\dot{v}$ and what to reaction force .
the whole point of feynman 's paragraph is to show that what we might believe is not what must happen by physical law . the full electromagnetic force on a particle is the lorentz force , which is $$\vec{f} = q ( \vec{e} + \vec{v} \times \vec{b} ) $$ since the particle is stationary , the second summand is necessarily $0$ in the frame considered , and so , in this frame , there must be an electric field , since we know the total force is not zero . i have just paraphrased feynman above , this is exactly what he wants to tell you : as counterintuitive as it may seem , moving magnetic fields seem to be electric , and vice versa .
unpredictability and special relativity can come from the fact that objects that are at a space like separation from us can influence our future like cone . for example , if alpha centauri exploded in a supernova right " now " in our reference frame , we would not know about it for 4 years since that star is 4 light years away from us . so we can not now predict that 4 years from now we will be hit with the supernova blast wave . as @arnoques succintcly put in his comment , to predict an event in our future light cone , the entire past light cone of that event would need to be known and that would include events that are outside our present light cone .
$\mathbf{f}_m ( \mathbf{r}_m ) =\mathbf{f}_{ext} ( \mathbf{r}_m ) +q_m \sum_{i\ne m}q_i\mathbf{n}_{im}/ ( \mathbf{r}_m-\mathbf{r}_i ) ^2$ , is not it ? or you want magnetic forces too ? edit : for magnetic part , you can use the lagrangian from landau-lifshitz textbook ( §65 ) :
the expectation value $$\tag{1} \langle \psi | v| \psi \rangle~=~0$$ of the potential energy operator $v$ is indeed zero , but the expectation value $$\tag{2} \langle \psi |k| \psi \rangle~=\frac{\hbar^2}{2m} \int_{\mathbb{r}}\ ! dx~ |\psi^{\prime} ( x ) |^2 ~=~+\infty$$ of the kinetic energy operator $k$ is actually infinite for the wave function $$\tag{3} \psi ( x ) ~=~ \sqrt{\frac{2}{l}}\left ( \theta ( x-\frac{l}{2} ) -\theta ( x-l ) \right ) , \qquad x\in \mathbb{r} . $$ here $\theta$ is the heaviside step function . the kinetic energy operator $k:=-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}$ is an example of an unbounded operator , which only make sense on its domain ${\cal d}_k\subsetneq {\cal h}$ inside the hilbert space ${\cal h}:=l^{2} ( \mathbb{r} ) $ of square lebesgue integrable functions . in particular , it is a non-trivial mathematical problem how to apply the differential operator $k$ to the non-differentiable wave function ( 3 ) . the infinite result ( 2 ) can be seen ( at the physical level of rigor ) in at least three ways ( ordered with the computationally simplest calculation first ) : plug the heaviside step function into eq . ( 2 ) to get an integral over the square of a pair of dirac delta function situated at $x=\frac{l}{2}$ and $x=l$ . this is strictly speaking mathematically ill-defined . physically , it makes sense to assign the integral the value infinite , cf . this phys . se post . calculate the overlaps $c_n=\langle \phi_n | \psi \rangle$ , and show than the sum $$\tag{4} \langle \psi |h| \psi \rangle=\sum_{n=1}^{\infty}|c_n|^2 e_n ~=~+\infty $$ diverges . this infinite conclusion seems physically robust , since all terms in the series ( 4 ) are non-negative . by regularization , as emilio pisanty suggests in a comment . define a regularized wavefunction $\psi_{\varepsilon} \in c^1 ( \mathbb{r} ) $ in such a way that ( i ) it converges $\psi_{\varepsilon}\to \psi$ for $\varepsilon\to 0^{+}$ , ( ii ) the expectation value $\langle \psi_{\varepsilon} |k| \psi_{\varepsilon} \rangle$ is easy to compute and finite for $\varepsilon&gt ; 0$ . show that $\langle \psi_{\varepsilon} |k|\psi_{\varepsilon} \rangle\to +\infty$ diverges for $\varepsilon\to 0^{+}$ .
1 ) if there is an error $e_j$ , the new states $e_j|0\rangle_l$ and $e_j|1\rangle_l$ are eigenvectors , with eigenvalue $-1$ , of all the stabilizers $s_j$ belonging to some set subset $s_j$ of $s$ . ( the elements of $s_j$ anticommute with $e_j$ ) . this subset $s_j$ identifies uniquely the error $e_j$ . 2 ) $|0\rangle_l$ and $|1\rangle_l$ are eigenvectors , with eigenvalue $1$ , of all the stabilizers $s$ belonging to $s$ ( this is not true for the " components " of $|0\rangle_l$ and $|1\rangle_l$ like , for instance , $|1010101\rangle$ ) . for a stabilizer $s$ , you just calculate $s|0\rangle_l$ and $s|1\rangle_l$ , and you check that the result is $|0\rangle_l$ or $|1\rangle_l$ . for instance : $k^1\left|0\right\rangle_l = ( iiixxxx ) \\\frac{1}{\sqrt{8}} ( \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle + \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle ) = \\ \frac{1}{\sqrt{8}} ( \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle + \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle ) \\ =\left|0\right\rangle_l$ 3 ) $k^4 \left|1010101\right\rangle = iiizzzz |1010101\rangle$ . with $z |0\rangle = |0\rangle$ , and $z |1\rangle = -|1\rangle$ , you get : $k^4 \left|1010101\right\rangle = |1010101\rangle$
the actual value of the potential is not meaningful , what is meaningful is the gradient : the force on a particle is given by $\vec{f} = -\vec{\nabla} v$ . equivalently , what is meaningful is only the potential difference between 2 points . notice that one can define a new potential differing from the old by an additive constant : $v ' = v + c$ and the force will still be the same . so we have a redundancy in the system when describing potential energy , this is a gauge degree of freedom . but nevermind the jargon for now . in your case , the test charge ( positive ) 's derivative of its potential energy will give rise to a force towards the negative charge , and so it moves towards the negative charge . conservation of energy holds , and the gain in ke must be compensated by a loss ( i.e. . difference ) of pe . the potential being $0$ at the midpoint is a consequence of choosing $c$ such that the potential at the point $r\to \infty$ has $v \to 0 $ , but one can equally well define the potential at $\infty$ to be $1$ gev , for example , and nothing physical would change .
the problem with the phase space flow in hamiltonian mechanics is that the flow itself is non-dynamical , that is , the flow is immediately defined for a given hamiltonian , so there is no independent equation governing its evolution . thus , liouville equation is simply a transport of a scalar variable in a given flow . so , dimensional analysis of the flow would be simply subset of dimensional analysis of underlying hamiltonian structure . similarly , i do not think there is any sense of attempting to find turbulence in the phase space flows . sure , time dependence of hamiltonian can introduce changes in the phase space , including the type of changes associated with transitions to chaos : such as bifurcations , tori destruction . . . but again , the flow itself is not the fundamental object in such transitions . if we are talking about the phase space of kinetic equations , the same arguments apply . even though the flow is ' more dynamical ' especially if taken in the context of self-interacting system of equations such as vlasov-maxwell , in these equations the flow itself again is not a fundamental object , so rarely it is analyzed independently . however , most methods of ( numerical ) solutions for such equations like particle-in-cell method and its many variations do use approaches quite similar to that of hydrodynamics .
the expression for $\delta s_m$ that you are expecting holds provided the variation you are performing is the variation with respect to the inverse metric only ; there should be no $\delta\varphi$ terms . in other words ; set $\delta\varphi = 0$ , and you obtain the desired expression . see , for example , carroll spacetime and geometry p . 164 , he does the same computation and explicitly remarks " now vary this action with respect , not to $\phi$ , but to the inverse metric . . . " generally speaking , in fact , the stress tensor is defined to be proportional to the functional derivative of the action with respect to the inverse metric ; \begin{align} t_{\mu\nu} = -2\frac{1}{\sqrt{-g}}\frac{\delta s}{\delta g^{\mu\nu}} \end{align}
according to wikipedia : in particle physics , the history of quantum field theory starts with its creation by paul dirac , when he attempted to quantize the electromagnetic field in the late 1920s . [ my emphasis . ] and according to wikipedia 's entry on dirac : he was the first to develop quantum field theory , which underlies all theoretical work on sub-atomic or " elementary " particles today , work that is fundamental to our understanding of the forces of nature . [ my emphasis . ] so , it seems definitely wrong to credit feynman with the " invention " of qft . if one would have to give credit for that to anyone in particular , the above suggests that that should be ( and is ) dirac . feynman 's and other contributions to quantum electrodynamics are summarised here . it seems that there is possibly another contender ( than dirac ) for the title of " father of qft": pascual jordan . see arxiv:0709.3812 [ physics . hist-ph ] , pp . 12-15 , e.g. , since this was a conference honoring dirac , other speakers can be forgiven for declaring dirac to be the founding father of quantum field theory . [ . . . ] [ i ] t is important to keep in mind that jordan was virtually alone at first in recognizing the need for the extension of quantum theory to fields .
disclaimer : since this is a homework style problem , i am not going to write all the details . writing down the projectile 's equation , and replacing $\theta$ we arrive at the equation : $$-\frac{x^2}{\frac{4 g ( h-y ) \left ( l^2- ( y-h ) ^2\right ) }{l^2}}-\frac{x ( y-h ) }{\sqrt{l^2- ( y-h ) ^2}}+h=0$$ solving this for $x$ , and simplifying the huge expressions ( perhaps by something like mathematica ) ; we find : $$x\to \frac{ \left ( 2 \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }-2 g ( h-y ) ^2 ( h-l-y ) ( h+l-y ) \right ) }{l^2 \sqrt{- ( h-l-y ) ( h+l-y ) }}$$ squaring : $$\rightarrow x^2=\frac{1}{l^4}4 g ( y-h ) \left ( -2 h \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }+2 y \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }+2 g ( h-y ) ^3 ( h-l-y ) ( h+l-y ) +h l^2 ( h-l-y ) ( h+l-y ) \right ) $$ in general if we have a function of several independent parameters ( i.e. . $f ( x_1 , x_2 , \cdots , x_n ) $ ) , and we want to calculate the error in $f$ , we would write : $$\delta f = \sqrt{\sum_i \left ( \frac{\partial f}{\partial x_i}\delta x_i \right ) ^2}$$ in this case $x^2$ only depens on $y$ , and its error comes from the error in $y$: $$\delta ( x^2 ) = \left| \frac{\partial ( x^2 ) }{\partial y} \delta y\right| = x^2 \left|\frac{ \left ( -g ( h-y ) ^3 \left ( 3 ( h-y ) ^2-l^2\right ) +3 h \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }-3 y \sqrt{g ( h-y ) ( h+l-y ) ^2 ( -h+l+y ) ^2 \left ( g ( h-y ) ^3+h l^2\right ) }+h l^2 \left ( l^2-3 ( h-y ) ^2\right ) \right ) }{ ( h-y ) ( h-l-y ) ( h+l-y ) \left ( g ( h-y ) ^3+h l^2\right ) } \delta y\right|$$ it does not even fit properly ! anyway , putting back all the corresponding numbers , we find : $$\frac{\delta ( x^2 ) }{x^2} \approx 0.52 \delta y$$ which looks like a decent result to me . note , this final equation is only true for values given in this link , namely $h=9.7 \text{cm} , y=17.5 \text{cm} , l=60\text{cm} \ \text{and}\ g=980 \text{cm}\text{ . s}^{-2}$ .
by using an arbitrary test function you have not included any information about the state of the electron . this means you are considering all possible states of an electron in a coulomb potential , so that formula you derived is true for all bound states of the hydrogen atom and all the unbound states of an electron scattering off the proton . it is true in the classical limit , where it should give you back the kepler orbits , but it also true for states which are behaving distinctly quantum mechanically , notably the eigenstates for the hydrogen atom , which are stationary . given that the expression is so general , its not that surprising that you get a result that does not immediately resemble the classical result . normally to take the classical limit of a system you have to think a bit about what is physically happening as you take that limit . in this case you would probably need to consider a superposition of a large number of very high $n$ states , i.e. states with a large energy and a large uncertainty in the energy . ( you would probably need similar conditions on $l$ as well )
gravity is viewed as a force because it is a force . a force $f$ is something that makes objects of mass $m$ accelerate according to $f=ma$ . the moon or the iss orbiting the earth or a falling apple are accelerated by a particular force that is linked to the existence of the earth and we have reserved the technical term " gravity " for it for 3+ centuries . einstein explained this gravitational force , $f=gmm/r^2$ , as a consequence of the curved spacetime around the massive objects . but it is still true that : gravity is an interaction mediated by a field and the field also has an associated particle , exactly like the electromagnetic field . the field that communicates gravity is the metric tensor field $g_{\mu\nu} ( x , y , z , t ) $ . it also defines/disturbs the relationships for distances and geometry in the spacetime but this additional " pretty " interpretation does not matter . it is a field in the very same sense as the electric vector $\vec e ( x , y , z , t ) $ is a field . the metric tensor has a higher number of components but that is just a technical difference . much like the electromagnetic fields may support wave-like solutions , the electromagnetic waves , the metric tensor allows wave-like solutions , the gravitational waves . according to quantum theory , the energy carried by frequency $f$ waves is not continuous . the energy of electromagnetic waves is carried in units , photons , of energy $e=hf$ . the energy of gravitational waves is carried in the units , gravitons , that have energy $e=hf$ . this relationship $e=hf$ is completely universal . in fact , not only " beams " of waves may be interpreted in terms of these particles . even static situations with a force in between may be explained by the action of these particles – photons and gravitons – but they must be virtual , not real , photons and gravitons . again , the situations of electromagnetism and gravity are totally analogous . you ask whether the spacetime is the force field . to some extent yes , but it is more accurate to say that the spacetime geometry , the metric tensor , is the field . concerning your last question , indeed , one may describe the free motion of a probe in the gravitational field by saying that the probe follows the straightest possible trajectories . but where these straightest trajectories lead – and , for example , whether they are periodic in space ( orbits ) – depends on what the gravitational field ( spacetime geometry ) actually is . so instead of thinking about the trajectories as " straight lines " ( which is not good as a universal attitude because the spacetime itself is not " flat " i.e. made of mutually orthogonal straight uniform grids ) , it is more appropriate to think about the trajectories in a coordinate space and they are not straight in general . they are curved and the degree of curvature of these trajectories depends on the metric tensor – the spacetime geometry – the gravitational force field . to summarize , gravity is a fundamental interaction just like the other three . the only differences between gravity and the other three forces are an additional " pretty " interpretation of the gravitational force field and some technicalities such as the higher spin of the messenger particle and non-renormalizability of the effective theory describing this particle .
your polarized sphere is spherically symmetric . therefore outside the sphere , in the vaccuum , the solution must be spherically symmetric . however , the only spherically symmetric solution of maxwell 's equations in vacuum is the electric field of a point charge . since your sphere has zero net charge , it must be the electric field of a point with zero charge . thus $\vec{e} = 0$ .
45 degrees is , in fact , the angle for maximum range for a projectile with no air resistance . in the absence of air resistance , the only force acting is gravity , which causes a constant acceleration of g downwards . this determines the amount of time the particle spends in the air , via the formula for the position of a particle with constant acceleration : $y ( t ) = y ( 0 ) + v_y t + \frac{1}{2}a_y t^2$ putting in the relevant parameters ( start and end positions both 0 , acceleration -g ( negative because it is downward ) ) this becomes : $0 = v_y t - \frac{1}{2}g t^2$ which we solve to get : $t = \frac{2v_y}{g}$ this time then goes into the equation for the horizontal position : $x ( t ) = x ( 0 ) + v_x t + \frac{1}{2}a_x t^2$ as there is no horizontal force acting , this reduces to just $x ( t ) = v_x t = \frac{2v_x v_y}{g}$ to get this in terms of the angle , we use the fact from trigonometry that for a velocity $v$ at an angle $\theta$ from the horizontal , the vertical velocity is $v \sin \theta$ and the horizontal velocity is $v \cos \theta$ , giving us : $x = \frac{2v \cos \theta v\sin \theta}{g} =\frac{v^2}{g}\sin 2\theta$ this has its maximum value for $\theta = \frac{\pi}{4}$ , namely , 45 degrees from the horizontal .
there is a " common " chart in most chemistry labs that covers exactly this topic . one version of it can be found here : http://chemistry.kenyon.edu/getzler/research%20files/miscibility_elutropicity.pdf for reference 's sake , this was found by a google image search using the term " miscibility . " a clear case of having to know something exists in order to find it .
in an inertial reference frame , there most certainly is not zero net force on a planet . there is a force pulling the planet towards the sun , and that is it . there are no other forces on the planet . you are 100% correct : if there really were no net force on the planet , then the planet would be stationary or travel in a straight line at constant velocity . it would not travel in a circle or ellipse . this is newton 's first law . if your tutor disagrees with newton 's first law , i suggest you find a better tutor !
an isothermal process is a rectangular hyperbola on a pv diagram only for an ideal gas . it follows from the equation of state $pv = n k t$ which shows that at fixed $n$ and $t$ we have $p \propto 1/v$ . assuming you are asking about the shape of an adiabatic process also for an ideal gas , this is given by the equation $$ p \propto v^{-\gamma} $$ where $\gamma = c_p / c_v = ( c_v + r ) /c_v$ ( here $c_v$ and $c_p$ are the molar heat capacities and $r$ is the molar gas constant ) . this can be proved using the equations of state for the ideal gas and the fact that the change in internal energy is equal to the work done for an adiabatic process .
the resistor will heat up the air ( if assume 100% energy convert rate ) , then the total heat transferred from resistor to air will be given by $q = p\cdot t=i^2rt=iut$ then from the first law of the thermodynamics , we know the total inner energy of air $e$ is $e=q-w$ here $q$ is the heat transferred into the system ( if heat is transferred from out , then $q&gt ; 0$ , if the system transfer heat out , then $q&lt ; 0$ ) , and the $w$ is the work done by the air ( if air do work i.e. air pushes the piston out , then $w&gt ; 0$ , if outside does work to the air , i.e. someone pushes the piston to compress the air , then $w&lt ; 0$ ) . so from this , you know the power from the resistor generate heat , and this heat is transferred from resistor to air . if the piston cannot move , then no work will be done , so the increase of inner energy of air is just the heat transferred or generated by the resistor , that is $\delta e = q$ if the piston can move , the increase in energy will result in the increase of air temperature , and therefore , the air pressure will increase and the air will push the piston out . in this process , air do work to the piston . so the inner energy of air will drop , but this work is done by air .
degenerate usually means that the gas is in a quantum regime , that is the thermal de broglie wave length $\lambda_{\rm db}\propto t^{-1/2}$ is much larger that the interparticle distance $l=n^{-1/d}$ , where $n$ is the density and $d$ is the dimension of space . one then has $l\propto k_f^{-1}$ where $k_f$ is the fermi momentum . this regime is the opposite of that of the classical ( dilute ) gas . a degenerate fermi gas is thus such that $t\ll e_f=\frac{k_f^2}{2m}$ . this corresponds to the limit where the fermions form a well defined fermi sphere , etc described in text books ( usually in a chapter about the fermi gas ) . note that electrons in metals also form a degenerate fermi gas .
$x=x ( y , z ) $ , $y=y ( x , z ) $ , $z= ( x , y ) $ $$dx= ( \frac{\partial x}{\partial y} ) _z dy + ( \frac{\partial x}{\partial z} ) _y dz$$ $$dy= ( \frac{\partial y}{\partial x} ) _z dx + ( \frac{\partial y}{\partial z} ) _x dz$$ $$\therefore dx= ( \frac{\partial x}{\partial y} ) _z [ ( \frac{\partial y}{\partial x} ) _z dx + ( \frac{\partial y}{\partial z} ) _x dz ] + ( \frac{\partial x}{\partial z} ) _y dz$$ $$dx= ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial x} ) _z dx + [ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial z} ) _x + ( \frac{\partial x}{\partial z} ) _y ] dz$$ $$ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial x} ) _z dx + [ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial z} ) _x + ( \frac{\partial x}{\partial z} ) _y ] dz=1dx+0dz$$ $$ ( \frac{\partial x}{\partial y} ) _z ( \frac{\partial y}{\partial z} ) _x + ( \frac{\partial x}{\partial z} ) _y=0 $$ using reciprocal relation : $$ ( \frac{\partial x}{\partial y} ) _z = \frac{1}{ ( \frac{\partial y}{\partial x} ) _z} $$ $$\left ( \frac{\partial x}{\partial y} \right ) _{z}\left ( \frac{\partial y}{\partial z} \right ) _{x}\left ( \frac{\partial z}{\partial x} \right ) _{y}=-1$$
a quasi-one-dimensional fermi surface is a fermi surface whose topology is the same as the topology of a surface defined by an equation that only depends on one dimension . in the picture above , taken from this paper , you see that the surface is effectively given by $$|k_x|\leq 1 $$ for the first picture . ( well , it should really be an equality if we talk about the surface itself but i wanted to discuss where the states are located , too . ) it looks like $k_y$ does not really qualitatively matter . that is different from ordinary two- or three-dimensional fermi surfaces given by inequalities such as $$\sqrt{k_x^2+k_y^2+k_z^2}\leq 1$$ because $k_y$ and perhaps $k_z$ play no qualitative role in the quasi-one-dimensional fermi surfaces , one may use various parts of intuition and formalism that are relevant for one-dimensional problems and just add the $y , z$ directions as dummy extra variables that do not affect anything qualitative . i believe – but i may be wrong – that it is confused to talk about " characters of fermi surfaces " . in the context of orbital hybridization , " character " refers to the bonds themselves ( the shape at the level of a single molecule ) , for example $sp^3$ has 25% s-character and 75% p-character . but when one talks about " character " of fermi surfaces , the word " character " just means some more general properties – any properties , unspecified properties , not something particular .
the action $s$ is not a well-known object for the laymen ; however , when one seriously works as a physicist , it becomes as important and natural as the energy $h$ . so the action is probably unintuitive for the inexperienced users - and there is no reason to hide it - but it is important for professional physicists , especially in particle and theoretical physics . the op 's statement that the hamiltonian corresponds to energy is a vacuous tautology because the hamiltonian is a technical synonym for energy . in the same way , one might say that the action intuitively corresponds to wirkung ( a german name ) because it is the same thing , too . because it now has two names , it becomes more natural :- ) and the op could also blame the energy for having " unnatural " units of action per unit time . in other words , the question assumes that energy ( and its unit ) is more fundamental and intuitive than the action ( and its unit ) - so it should not be surprising that using his assumptions , the op may also " deduce " the conclusion that the energy is more fundamental and intuitive than the action . ; - ) but is the assumption = conclusion right ? well , energy is intuitive because it is conserved , and the action is intuitive because it is minimized - so there is no qualitative difference in their importance . of course , the only difference is that non-physicists do not learn to use the action at all . the energy may be imagined as " potatoes " which every can do ; the action is an abstract score on the history that is only useful once we start to derive differential equations out of it - which almost no layman can imagine . if the laymen 's experience with a concept measures whether something is " intuitive " , then the action simply is less intuitive and there is no reason to pretend otherwise . however , physicists learn that it is in some sense more fundamental than the energy . well , the hamiltonian is the key formula defining time evolution in the hamiltonian picture while the action is the key formula to define the evolution in the nicer , covariant , " spacetime " picture , which is why hep physicists use it all the time . what the action is in general otherwise , the main raison d'etre for the action is the principle of least action , http://en.wikipedia.org/wiki/principle_of_least_action which is what everyone should learn if he wants to know anything about the action itself . historically , this principle - and the concept of action - generalized various rules for the light rays that minimize time to get somewhere , and so on . it makes no sense to learn about a quantity without learning about the defining " application " that makes it important in physics . energy is defined so that it is conserved whenever the laws of nature are time-translational symmetric ; and action is defined as whatever is minimized by the history that the system ultimately takes to obey the same laws . the energy is a property of a system at a fixed moment of time - and because it is usually conserved , it has the same values at all moments . on the other hand , the action is not associated with the state of a physical object ; it is associated with a history . there is one point i need to re-emphasize . for particular systems , there may exist particular " defining " formulae for the hamiltonian or the action , such as $e=mv^2/2$ or $s = \int dt ( mv^2/2-kx^2/2 ) $ . however , they are not the most universal and valid definitions of the concepts . these formulae do not explain why they were chosen in this particular way , what they are good for , and how to generalize them in other systems . and one should not be surprised that one may derive the right equations of motion out of these formulae for $h$ or $s$ . instead , the energy is universally defined in such a way that it is conserved as a result of the time-translational symmetry ; and the action is defined in such a way that the condition $\delta s = 0$ ( stationarity of the action ) is equivalent to the equations of motion . these are the general conditions that define the concepts in general and that make them important ; particular formulae for the energy or action are just particular applications of the general rules . in the text above , i was talking about classical i.e. non-quantum physics . in quantum physics , the action does not pick the only allowed history ; instead , one calculates the probability amplitudes as sums over all histories weighted by $\exp ( is/\hbar ) $ which may be easily seen to reduce to the classical predictions in the classical limit . a stationary action of a history means that the nearby histories have a similar phase and they constructively interfere with each other , making the classically allowed history more important than others .
as you note , this is really a question about cosmology . there is a lot to say , but in the interest of brevity i will just address a part of the question . quantum chromodynamics is a strongly-coupled theory with a very complex vacuum state . the universe is not in the qcd vacuum today , but is pretty darn close to it , since the 2.7k background temperature is way below the mass gap of qcd . at high temperature qcd is a lot simpler -- the quarks and gluons are weakly interacting ( asymptotic freedom ) . the state of high temperature qcd is fairly simple ( though not trivial ) ; it is approximately an ideal gas of relativistic particles . as the universe expanded and cooled adiabatically it passed through a ( first order ) phase transition from the " easier " high-temp state to the " harder " low temp state . just below the phase transition the state was still pretty hot , but as the universe continued to expand adiabatically the state approached the qcd vacuum . of course , this would not have worked if preparing the qcd vacuum were qma hard ; the universe would have gotten stuck is some long-lived metastable state . that apparently did not happen , so we have cosmological evidence that preparing the qcd vacuum is easy , even if jlp have not proved it ( yet ) . you can ask further questions about how the hot initial state was prepared , why it was so homogeneous and isotropic , etc . which would lead us into a discussion about how cosmic inflation started and ended , but that is enough for now .
if $\phi$ is a function of $t$ , then $x$ , for example , is written as $$ x ( t ) =r\cos ( \omega t ) +\ell\sin ( \phi ( t ) ) . $$ applying the chain rule gives $$ \begin{align} \frac{d}{dt}x ( t ) and =\frac{d}{dt}r\cos ( \omega t ) +\frac{d}{d\phi}\ell\sin ( \phi ) \frac{d}{dt}\phi ( t ) \\ and =-r\omega\sin ( \omega t ) +\ell\cos ( \phi ) \dot{\phi} . \end{align} $$
the excesses have looked convincing to many people but they do not look convincing anymore . last october , lux in south dakota presented the results of their superior analysis http://motls.blogspot.com/2013/10/fiat-lux.html?m=1 http://motls.blogspot.com/2013/10/dark-matter-wars-are-over-lux-safely.html?m=1 which safely excluded the theories of wimp dark matter directly suggested by cdms ii si and other experiments . the complete and pure absence of a signal in lux shows almost certainly that the excesses in cdms ii si and other experiments were due to some overlooked background ( non-dark-matter-related ) processes . wimp is still plausible and attractive as a model of dark matter . but there is no evidence one way or another which is why physicists keep on studying it and other models as well , including e.g. those of the sterile neutrino dark matter or axions .
can we view the infinite deep well , as a infinite barrier ? only from the point of view of r=0 , but the electron is not there in the classical model ; and even in the schrodinger model r=0 is only one infinitesimal point , you would have to considered the expectation value of the energy . what would the wavefunction intersect , does it pass through ( 0,0 ) ? the s-orbitals do have a non-zero probability density at ( 0,0 ) , but again that is only one point . i am quite confused by the idea of having negative energy , does it mean i need infinite energy to get out of the well ? no , the electron does not need infinite energy to get out of the well because it does not start at the bottom of the well in the classical model , and has zero probability of starting at the bottom of the well in the schrodinger because it is only a single point . you need to consider potential energy , kinetic energy and total energy . if you look at the following question and answer you should have a solution to the problem : hydrogen atom : potential well and orbit radii
no , because the uncertainty principle operates between position and momentum rather than position and velocity . for speeds much less than $c$ , momentum is just proportional to velocity : $p = mv$ . but at relativistic speeds we have to use the relativistic version , $$ p = \gamma mv , $$ where $\gamma = 1/\sqrt{1-v^2/c^2}$ . substituting this in and squaring both sides we get $$ p^2 = \frac{m^2v^2}{1-{v^2}/{c^2}} , $$ which we can rearrange a little to get $$ v^2 = \frac{p^2}{ m^2 + p^2/c^2 } , $$ or $$ v = \frac{p}{\sqrt{ m^2 + p^2/c^2 }} . $$ now , the limit of this as $p \to \infty$ is just $$ v = \frac{p}{\sqrt{p^2/c^2 }} = c . $$ the momentum $p$ can fluctuate due to the uncertainty principle , but now you can see that now matter how big $p$ gets , $v$ will always be less than $c$ .
think about this with an example : the sine and cosine functions . they both average individually to zero over an interval . you can multiply those averages and still obtain zero . but if you multiply sin by itself and then average , you get a very distinct non-zero result . when the functions are arbitrary , the average of the product quantifies statistical correlation between the two functions/variables . this correlation gives a rough measure of how causally connected are both of the variables . this correlation information is completely lost when one takes the averages of the functions individually .
new answer what you have done here is just dimensional analysis . but you have gone a little too far . in particular , just because two things have the same dimensions does not mean that they are equal . if you want to expand $f/a$ a little more , you can choose your favorite from \begin{equation} f = \frac{d p}{dt} = \frac{d}{dt} ( m\ , v ) = m\ , \frac{d}{dt} v = m\ , a \end{equation} here , $p$ is the momentum , $m$ is some amount of mass you are keeping track of , $v$ is its velocity , and $a$ the acceleration . now , pressure is really defined as being the force on a unit of area . so if you do not choose an area , you can not define pressure . there might be situations where there is some momentum change in a volume without the matter directly hitting a surfce . for example , with an electromagnetic force . so there could be a net force on a volume . but again , you can not call anything a pressure unless you select an area to measure it . old answer ( from before reading your clarifications ) pressure is the force per unit area applied perpendicular to the surface of an object . ( or at least the area over which you are imagining measuring the pressure . ) if i understand you correctly , you are interested in the pressure on the surface of the tube . is this correct ? but you are referring to the mass flowing past the tube . that is , it is flowing parallel to the surface . so , in your simplified model where the matter is flowing straight , there would be zero pressure . of course , in real life , the matter would probably having some internal motions as well , which means it would bounce off the walls of the tube , exerting pressure . in short , we would need more information to calculate the pressure . for your particular setup , you still have not told us about any interactions between particles of the matter . from what you have told me , i can still assume that the matter particles all have the same velocity ( but are scattered at different positions ) . now , as you know , force is proportional to acceleration . since the velocity is constant , there is no acceleration , so there must be no force , which means there is no pressure .
any material between two nodes is displaced by the same direction . so the direction of b and c has to be the same as well as the direction of a and d due to symmetry . in addition , the direction of a must be the opposite of b since they are across from a node . similarly the direction of c and d must be opposite . so the two possible configurations are  A--&gt; &lt;--B &lt;--C D--&gt; (figure d) &lt;--A B--&gt; C--&gt; &lt;--D (figure c)  the correct answer is ( 2 ) .
for inflation the potential energy of the field dominates the kinetic energy $\dot{\phi} \ll v ( \phi ) $ this limit is referred as slow roll and under such conditions the universe expands quasi exponentially $a ( t ) \propto \exp \left ( h dt\right ) = e^{-n} $ where we define the number of e-folds $n$ as : $dn = -h dt$ so that $n$ is large in the far past and decreases as we go forward in time and as the scale factor $a$ increases . with this we have : $\epsilon = -\frac{\dot{h}}{h^{2}} = \frac{1}{h}\frac{dh}{dn}$ accelerated expansion will only be sustained for a sufficiently long period of time if the second time derivative of $\phi$ is small enough : $|\ddot{\phi}| \ll |3h\dot{\phi}| , |v' ( \phi ) |$ so that the equation of motion for the scalar field is approximately : $3h\dot{\phi} + v' ( \phi ) \simeq 0$ this condition can be expressed in terms of a second dimensionless parameter , defined as : $\eta \cong -\frac{\ddot{\phi}}{h\dot{\phi}} \cong \epsilon + \frac{1}{2\epsilon}\frac{d\epsilon}{dn}$ then $\eta \simeq \frac{1}{8\pi g} \left ( \frac{v'' ( \phi ) }{v ( \phi ) } \right ) $ in the slow regime $\epsilon , |\eta|\ll 1$ , where the last condition ensures that the change of $\epsilon$ per e-fold is small . notice that $\eta$ need not be small for inflation to take place . inflation takes place when $\epsilon &lt ; 1$ , regardless of the value of $\eta$
the surface $\mathrm{s}$ does indeed contain charge $q_1$ , and so will have nonvanishing electric flux . however , $\mathrm{s}$ does not contain charges $q_2$ and $q_3$ , so it will have zero total flux due to those charges .
in what ways can energy transform into matter and vice versa ? energy and matter are connected according to special relativity and this has been experimentally demonstrated . it is the famous formula : $e=mc^2$ , where $m$ is the relativistic mass and $c$ the velocity of light . or $e^2=m_0^2c^4 +p^2c^2$ , for a particle with rest mass $m_0$ moving with momentum $p$ . the rules of transformation follow quantum mechanical solutions of kinematic and potential problem equations . annihilation is one way to transform matter to energy . yes fission is another ( when splitting and atom , what happens to its two parts ? ) in the quantum mechanical description of nuclei they are represented by potential wells with energy levels , some filled . the number of baryons ( protons and neutrons ) bound in this potential well characterize the nucleus . nucleus a that is struck by a neutron ( for example ) becomes a nucleus b higher up in baryons by absorbing it into an energy level of this potential well . in fission this higher up nucleus is unstable and falls into a lower energy state , giving up part of its mass in energy according to the relativistic formulae , and breaking into smaller nuclei and free neutrons which go on to sustain the fission on another original nucleus . generally a form of fission happens if a nucleus is unstable . there is also fusion , two deuterium nuclei adhering at a lower energy level and giving up energy . the binding energy curve shows whether nucleons can fuse or fission and give up as energy a part of their mass . are quantum fluctuations one way to transform energy to matter ? no , quantum fluctuations are virtual . if you mean tunneling , yes .
the container on earth will be cooled by convection currents i.e. it transfers heat to the air around it , and also by black body radiation . by contrast the container in space can only cool by black body radiation , and obviously it will cool down more slowly . you can calculate the cooling in space using the stefan-boltzmann law assuming you know the emissivity ( if you paint the container black the emissivity will be close to unity ) . calculating the cooling in air is harder ; typically you had use newton 's law with empirically derived constants . the final temperature in air is obviously just the temperature of the air around your container . the final temperature in space depends on where your container is . just as the container can lose heat by emitting radiation it can gain heat by absorbing radiation , and space is full of radiation . for example the moon is just a lump of inert rock with little or no internal generation of heat , however by absorbing sunlight the daytime temperature can rise to over 100ºc . however at night , when there is no sunlight the temperature can fall to -150ºc . so the final temperature of your container would be different during the lunar night and day , even though it is in a vacuum in both cases . if you took your container into intergalactic space , well away from any radiation sources , then it would indeed cool to the 2.7k of the cosmic microwave background .
i think the question is too complex to give a satisfactory answer without deep study of the problem . the biggest problem is that there are several ways how vibrations ( sound waves ) travel from the source to the receiver . the path of the excitation : the source - glass - fluid - glass - the receiver you have described is only one one possible path . some of the excitation shall travell through glass only , e.g. bottom or side of the glass . so if you want to have exact solution you should find standing sound wave ( eigenfunction ) solution of the whole system glass + fluid , which is far from being trivial . of course , in this case you must also consider dissipation , because you have constant excitation and without the dissipation the vibrations would continue to grow to infinity . the second possible , more engineering way to get the answer is to disregard sidepaths of excitations and calculate in terms of traveling sound wave . at one end you have the source , excitations first travel through glass , then difract on the border between glass and liquid , travel throught liquid , again difract on the border between glass and liquid and finally travel through the glass to the receiver , where all of the wave energy is absorbed . in both cases you need speed of sound in medium , which is not hard to obtain : $$v^2 = \frac{e}{\rho} = \frac{k}{\rho} , $$ where $\rho$ is density of the medium , while $e$ is young 's modulus ( solid ) and $k$ is bulk modulus ( liquid ) . in the second case you also need transmitivity , which is of course dependent on the incident angle and you can find the right equations by peaking at fresnel equations . i hope i understood you right and that my answer helps .
if i am not wrong the product is well defined , because the 2 point funtion is boundary value of an analytic function in some tube ( see streater/wightman ) with certain bounds . more genral there is 1-1 correspondence between translation invariant tempered distributions which satisfy the spectrum condition and analytic functions in this mentioned tube and an easy way to define the product of the distribution is the product of the analytic functions which lies in the same class and therefore gives a well defined distribution on the boundary . really easy is the massless case in let 's say 4d . $w ( x-y ) \sim \frac1{ ( x-y+i\epsilon ( x_0-y_0 ) ) ^2}$ $w^2 ( x-y ) \sim \frac1{ ( x-y+i\epsilon ( x_0-y_0 ) ) ^4}$ where the two-point function is just the boundary value of the analytic function $1/z^2$ . in 1 dimension , that means a chiral field on a light ray , it is even more drastic . there the wick ( =normal ordered ) square of the free fermion is a free boson . edit the answer to the question " is a normal-ordered product of free fields at a point a wightman field ? " is : yes ! for convinience i give you a reference , which is " general principles of quantum field theory " by bogolubov , logunov , oksak , todorov ( 1990 ) p . 344 ex . 8.16 prove that the wick monomials satisfy the wightman axioms edit tim : i can not comment on your yet . i did not say you can multiply general distribution which are boundary values by multiplying the function , just for a certain class but maybe i was sloppy because i thought this was standard . the $\delta$ is not in this class , because it does not fullfill a " spectrum contion " , i.e. the fourier transform is supported in some convex cone with non-empty dual cone . the fourier transform of $\delta ( x ) $ is constant so does not fall into this class . but distributions which have this property on their fourier transformation you can multiply : see the standard textbook reed simon volume 2 - page 92 ex 4 . i hope my references clear the confusion .
it is not the four-force that is conservative , but the einstein definition of force , $$ f= {dp\over dt}$$ this force for a particle in an electromagnetic or linearized gravitational field is conservative in the same way as in newton 's model : the force is $$ f = qe$$ and the integral of a static e around a closed loop is zero , still in relativity . the reason is explained in this answer : a priori validity of $w=\int fdx$ in relativity ? . the integral of the force over the distance as einstein defines it is still the work done in the relativistic system .
please do however remember the following line from the link you yourself have cited : in general , if the behaviour of a system of more than two objects cannot be described by the two-body interactions between all possible pairs , as a first approximation , the deviation is mainly due to a three-body force . hence , it can be seen that , initially , when people were thinking of many-body problems , they encountered terms in the mathematical formulation which they later termed as many body forces . incidentally , they were discovered in strong interactions and were a result of gluon mediation . in the celestial scale hence , you would need to have a similar mediating phenomena/theory to explain any physically valid three body force . also related : n-body forces in classical mechanics
i could be wrong , but in my understanding , you are describing and justifying steady state , not detailed balance . in thermal equilibrium , steady state is true always , and detailed balance is true sometimes . detailed balance means that the rate $x \rightarrow y$ is always the same as the rate $y \rightarrow x$ . if a system is both in thermal equilibrium and has time-reversal symmetry , you will have detailed balance . if time-reversal symmetry is broken , for example an electrolyte in an external magnetic field , you can have thermal equilibrium but you will not necessarily have detailed balance . the process $x \rightarrow y$ might be balanced by $y \rightarrow z \rightarrow x$ , instead of being balanced by $y \rightarrow x$ . as an explanation of steady-state , your " continuity equation " explanation is fine . but in my opinion you can say the same thing more clearly without using the words " continuity equation " or writing down any math . if you just say that the number of times per second that the system enters state y equals the number of times per second that the system leaves state y , i think that is intuitively sensible .
this is a notation from differential geometry that is unusual in physics ( as far as i am aware ) . the operators $\frac{\partial}{\partial y}$ and $\frac{\partial}{\partial z}$ are basis vectors for vector fields ; see e.g. this planetmath article for an introduction ( in particular where it says " in some sense " :- ) if you do not want to worry about differential geometry , you could write the field in good old-fashioned vector notation as $$ e=\pmatrix{0\\e_y ( t-x ) \\e_z ( t-x ) }\ ; , \quad b=\pmatrix{0\\b_y ( t-x ) \\b_z ( t-x ) }\ ; . $$
classical chaotic systems can be used to generate random numbers . specifically , if a system is chaotic then it will have a positive lyapunov exponent and so will be unpredictable . although classical mechanics is deterministic , it is not possible to know the initial conditions to infinite precision . therefore it is not possible to predict the future state of a chaotic system . if the future state cannot be predicted by any means , then it is random . you would have to make a study of the particular system in order to determine the rate at which randomness is produced , and the best way to extract it , but it can be done . chaotic systems are all around us ( the weather , turbulence , various electronic circuits , etc . ) however , in a practical sense , it is not possible to do anything without quantum mechanics since you live in a quantum world . in other words , anything you build in this physical world will , underneath it all , be quantum mechanical . now , with quantum mechanics there is something very nice that you can do . it is in fact possible to build a random number generator in which the numbers produced are certifiably random , even if you do not trust the hardware ( say , the hardware was built by your adversary ) . for more information on this , search for " certifiable quantum dice " by umesh vazirani ( which i have not read ) .
firstly , $tan ( \theta ) = \frac{v^2}{rg}$ means that as you increase the centrepital velocity , $v$ , of the pendulum , the angle between the rope and the vertical increases . now as the pendulum gets faster and faster , the angle gets larger and larger but never exceeds 90 degrees . i will not provide supporting data , as the above equation is all the data you need .
temperature certainly affects semiconductors . as someone who is not specialized in semiconductors , i can think of at least two microscopic effects at play here . the first is that at lower temperatures there are less phonons ( quantized vibrations of the atomic lattice ) . the effect of phonons is to scatter the electrons and lower the conductance of the semiconductor . thus at lower temperatures the conductance should be increased ( as in the case of metals ) . the second effect is that as the temperature is lowered , the fraction of electrons on high energy states is lowered ( and on low energy states increased ) thus potentially changing the amount of electrons electrons above/below the band gap ( carrier concentration ) . the effect of this is more complex than the effect of phonons .
the $k$ notation is generally used to describe friedmann robertson walker cosmological models . these are built on the assumptions of homogeneity and isotropy . the spacetime can be described as being foliated by spatial slices of constant curvature . the k value is the sign of this spatial curvature if the {-1 , 0 , +1} convention is adopted . as the curvature is a constant , it makes sense to talk of its sign . further details here .
a backlit sensor is not really a new kind of sensor , it is just a different arrangement of the imaging elements to allow more light into the sensor . from wikipedia : a traditional digital camera sensor consists of a matrix of individual picture elements . each element is constructed in a fashion similar to the human eye , with a lens at the front , sensors at the back , and wiring in between . the front of the detectors require an active matrix to be placed on their front surface . the matrix and its wiring reflects or absorbs some of the incoming light , thereby reducing the signal that is available to be captured . a back-illuminated sensor moves this wiring behind the sensors , similar to a cephalopod eye . they are not without issues though : moving the active matrix transistors to the back of the photosensitive layer normally leads to a host of problems , such as cross-talk , which causes noise , dark current , and color mixing between adjacent pixels . backlit sensors improve the low-light performance of a ccd chip , reducing the iso needed for low-light conditions ( and thus reducing noise ) . this is a clear benefit to astrophotographers , allowing them to use shorter exposures with less noise to get equivalent shots .
short answer . the relation between newtons and kilograms , with respect to work&nbsp ; /&nbsp ; kinetic energy , is actually just through newton 's second law ! here we are not mixing up newtons and kilograms ( forces and masses ) . the mass plays a role in the kinetic energy equation precisely because mass plays a role in how much force it takes to accelerate an object from rest to a speed v . example . consider for instance an object which has a mass of , say , 0.1019&nbsp ; kg . the gravitational force that this body experiences at the surface of the earth is &minus ; 1&nbsp ; n ( that is , a downward force ) , if we take g &nbsp ; =&nbsp ; &minus ; 9.81&nbsp ; m/s&sup2 ; . if we hold this object from a height of one meter , and allow it to drop , gravity performs work on this object , exerting a force of &minus ; 1&nbsp ; n over a ( downward ) displacement of &minus ; 1&nbsp ; m . as a result , just before impacting the ground , the object will have 1 joule of kinetic energy , as ( &minus ; 1&nbsp ; n ) &nbsp ; &middot ; &nbsp ; ( &minus ; 1&nbsp ; m ) = +1&nbsp ; j ; gravity will have done 1 joule worth of work on the object , which causes it to move with 1 joule worth of kinetic energy . in the examples you give , the correspondence that you are looking for is between mass and the gravitational force that it exerts . when you say that you take "1 newton " and lift it one meter , the ' newton ' you are referring is one newton of force exerted downward by an object with some mass &mdash ; or more to the point , the one newton which would be the minimum necessary to raise it steadily by opposing gravity . derivation we can actually show directly how mass comes into the kinetic energy formula , and pinpoint the reason that it is there . suppose that we exert a net force f on an object with mass m , over some displacement d in the same direction as the force , where the object starts at rest . the amount of work done on the object will be $$ w \ ; =\ ; \mathbf f \cdot \mathbf d \ ; =\ ; fd , $$ taking f to be the magnitude of the force f , and d to be the length of the displacement d . because we are doing net work on the object starting from rest , this work will go directly towards the kinertic energy of the object . the acceleration of the object under this force is $$ \mathbf a \ ; =\ ; \mathbf f / m . $$ if t is the time that it takes for the object to be moved by a displacement of d , we have $$ \mathbf d \ ; =\ ; \tfrac{1}{2}\mathbf a \ , t^2 \ , ; \qquad\implies\qquad t \ ; =\ ; \sqrt{2d/a\ ; } \ ; =\ ; \sqrt{2dm/f\ ; } . $$ taking the magnitude a &nbsp ; =&nbsp ; ||&nbsp ; a &nbsp ; || . the speed that the object is travelling after that amount of time is just $$\begin{align*} v \ ; =\ ; a t \ ; and =\ ; \bigl ( f/m\bigr ) \sqrt{2dm/f\ ; } \\ [ 1ex ] and = \sqrt{2fd/m\ ; } , \end{align*}$$ applying newton 's second law again for the acceleration , and cancelling factors of f and m under the square-root . we may then re-express this equation as $$ v^2 \ ; =\ ; 2fd/m \qquad\implies\qquad \tfrac{1}{2}m\ , v^2 = fd = w , $$ where we just applied the formula for the work done on the object at the end . because the net work is the same as the kinetic energy in this case , it follows that $k = \tfrac{1}{2}m\ , v^2$ . in the derivation above , the only ways we used mass was with newton 's law , a &nbsp ; =&nbsp ; f / m . so , the fact that it occurs in the formula for kinetic energy is not because we are confusing mass with force , but because of the relationship between mass and force .
edit : i am not sure what specifically you are after . i will explain a little more why it is difficult to give you a number . it is also quite possible that the number you are seeking might not be what you think it is . . . . the decay would be similar to any plasma type reaction for a mercury-vapor gas . there is a delay between absorption and re-emission of light photons and a typical electrical system driving the energy is not going to stop instantly . in fact the dominate feature in the decay rate will probably be due to the slow discharge of the ballast system driving the gas . the common household circuit lighting up a tube usually has a large transformer with a big magnetic field to step up the voltage and resonate the gas to force the plates to conduct through the ionized gas and light up . it takes a considerable amount of time for this electrical system to discharge and while it does it is going to continue to drive energy to the lamp causing it to light and effecting your " decay " rate significantly . here is a 12v lamp driver just for you to see what is involved electrically : note : the large cap ( 0.047uf ) and the transformer are going to resonate the lamp energy much longer than the actual natural decay rate of the gas . in addition the 47uf cap is going to supply power to the circuit for a non-trivial amount of time after the 12v is removed . for comparison here is an 120v 60hz ballast design and you will see there are similar issues . if you want to look at just the gas decay rate then you might have to excite it with a laser for more precise measurements . the states that the energy moves through is usually measured and plotted on a jablonski diagram like this : in your case $\tau=\frac{1}{k}$ where k is equal to ( in the de-excitation case ) $k = k_f + k_i + k_x + k_{et} + …= k_f + k_{nr}$ where $k_f$ is the rate of fluorescence , $k_i$ the rate of internal conversion and vibrational relaxation , $k_x$ the rate of intersystem crossing , $k_{et}$ the rate of inter-molecular energy transfer and $k_{nr}$ is the sum of rates of radiationless de-excitation pathways . and a more detailed model can be measured : the method for measuring a gas properly looks like : here is an example of what the data looks like . much of this information was taken directly from the research documented here : www.jh-inst.cas.cz/~fluorescence/support/lectures/ufch_fluor03.pps‎
the majority of the energy is dissipated in the travel through the air from the cloud to the ground . the energy goes into heating the air and generating the shockwave that we hear as thunder . i can not give you a single definitive refernce for this , but googling " energy dissipation lightning " will find lots of relevant articles . you can understand why this is because the energy dissipated by a current $i$ travelling through a resistance $r$ is given by $w = i^2r$ . in a lightning strike the current is constant , because the charge flowing in one end has to flow out the other end , so the power dissipated is proportional to the resistance . the resistance of air is a lot higher than the resistance of the ground/tree/person or whatever the lightning hits , so the majority of the energy dissipation is in the air . the electrons flowing from the cloud through the lightning bolt end up in the ground , but with an energy only slightly greater than ambient . they will presumably flow into the surrounding area until the potential difference around the point of strike falls to effectively zero . this is likely to be within a few metres , so they would not get anywhere near the earth 's core .
light has a dual nature , one of photons and the other of waves . but energy does not really travel in waves . so what do the wave represent ? let us be clear in our terminology and the domain to which we apply it . our everyday life is lived with classical mechanics and classical electricity and magnetism ( as long as we do not use the net and transistors and the other paraphernalia of modern life ) . classical theories are well developed mathematically and are applicable in the domain where hbar can be considered practically zero . particularly for light maxwell 's equations have been validated in this domain and describe light as a wave . this wave is a propagating changing electric and magnetic field and is a wave in the four dimensional space time , there are peaks and valleys . it displays the classical wave behavior of interference and dispersion . these carry energy , a universal example is simple sunlight . now you use the word photon . the photon is one of the elementary particles of the standard model . this means that we are no longer in the classical domain but in the quantum mechanical domain when we speak of light as a collection of photons . it means that it behaves sometimes as a particle , ( photoelectric effect ) i.e. it has a specific ( x , y , z , t ) value and sometimes as a probability wave , i.e. according to a quantum mechanical wave function the square of which gives the probability of finding a photon at a specific ( x , y , z , t ) which probability has a wave like variation in space because it is the solution of a wave type differential equation describing the dynamics of the situation . note , it is the probability which shows a wave nature , not the " particle " itself . now the wave nature of maxwell 's equation and the wave nature of the probability function are mathematically reconcilable , so that the frequency of the classical wave is the nu in the energy of the photon in e=h*nu . in addition lubos motl has an interestin article about the collective emergence of classical light from photons in his blog , though it needs a background in physics to understand it .
the well known property of the harmonic coordinates is that the covariant divergence of a vector field and the d'alambertian of a scalar field take a particularly simple form : $$ d_{\mu}a^{\mu} \rightarrow g^{\mu\nu}\partial_{\mu}a_{\nu} , \\ g^{\mu\nu}d_{\nu}d_{\mu}\phi \rightarrow g^{\mu\nu}\partial_{\mu} \partial_{\nu}\phi . $$ the harmonic condition $$ \partial_{\mu}\left ( \sqrt{-g}g^{\mu\nu}\right ) =0\qquad\qquad\left ( 1\right ) $$ is widely used to construct the so called de donder gauge for the quantization of a weak gravitational field . if one uses the deviation $\psi^{\mu\nu}$ of the contravariant metric density from the flat one $\eta^{\mu\nu}=diag\left ( 1 , -1 , -1 , -1\right ) $ as a field variable : $$ \sqrt{-g}g^{\mu\nu}=\eta^{\mu\nu}+\psi^{\mu\nu} , $$ then the gauge condition $\partial_{\mu}\psi^{\mu\nu}=0$ looks very similar to the well known lorentz condition ( feynman gauge ) $\partial_{\mu}a^{\mu}=0$ . if one would like to use the deviation of the covariant metric tensor as a field variable $$ g_{\mu\nu}=\eta_{\mu\nu}+h_{\mu\nu} , \qquad\left ( 2\right ) $$ then the weak field expansion of the condition ( 1 ) take the form : $$ \partial_{\mu}\left ( h^{\mu\nu}-\frac{1}{2}\eta^{\mu\nu}h_{\alpha}^{\alpha }\right ) =0 . $$ the weak-field expansion of the einstein-hilbert action with respect to $h_{\mu\nu}$-field ( 2 ) has the form : $$ s =\frac{1}{16\pi g_{n}}\int\mathrm{d}^{4}x\ , \sqrt{-g}\ , r\\ =\frac{1}{2\kappa^{2}}\int\mathrm{d}^{4}x\ , \left [ \ , \partial_{\alpha} h_{\mu\nu}\partial^{\alpha}h^{\mu\nu}-\ , \partial_{\alpha}h\ , \partial^{\alpha }h-2\ , \partial_{\mu}h^{\mu\nu}\left ( \partial_{\alpha}h_{\nu}^{\alpha }-\partial_{\nu}h\right ) +o\ ! \left ( h^{3}\right ) \right ] , $$ where $\kappa=\sqrt{32\pi g_{n}}$ . the gauge can be fixed by adding the term : $$ \frac{1}{\kappa^{2}}\int\mathrm{d}^{4}x\left ( \partial_{\alpha}h_{\mu }^{\alpha}-\frac{1}{2}\partial_{\mu}h\right ) \left ( \partial_{\beta} h^{\beta\mu}-\frac{1}{2}\partial^{\mu}h\right ) , $$ thus the action takes a particular simple form : $$ s=\frac{1}{2\kappa^{2}}\int\mathrm{d}^{4}x\ , \left [ \ , \partial_{\alpha} h_{\mu\nu}\partial^{\alpha}h^{\mu\nu}-\ , \frac{1}{2}\partial_{\alpha }h\ , \partial^{\alpha}h+o\ ! \left ( h^{3}\right ) \right ] . $$ therefore , in the de donder gauge the graviton propagator has a very simple form : $$ d_{\mu\nu , \alpha\beta}=\left\langle 0\left\vert t\ , h_{\mu\nu}\left ( x\right ) h_{\alpha\beta}\left ( y\right ) \right\vert 0\right\rangle =i\kappa^{2} \int\frac{\mathrm{d}^{4}p}{\left ( 2\pi\right ) ^{4}}\frac{e^{-ip\cdot\left ( x-y\right ) }}{p^{2}+i0}\times\frac{1}{2}\left ( \eta_{\mu\alpha}\eta_{\nu\beta }+\eta_{\mu\beta}\eta_{\nu\alpha}-\eta_{\mu\nu}\eta_{\alpha\beta}\right ) . $$ the de donder gauge is so to speak the gr analogue of the feynman gauge for qcd or qed . using the gauge condition ( 1 ) and vertices extracted from the weak-field expansion of the einstein-hilbert action and utilizing the qft perturbation theory with respect to $h_{\mu\nu}$ , one can find , for example , the gravitational field of a static spinless source . the result will be no more than the $r_{g}/r$-expansion of the schwarzschild metric in the harmonic coordinates ( see , e.g. , s . weinberg , gravitation and cosmology , eq . ( 8.2.15 ) ) : $$ ds^{2}=\frac{1-r_{g}/\left ( 2r\right ) }{1+r_{g}/\left ( 2r\right ) } \ , dt^{2}-\frac{1+r_{g}/\left ( 2r\right ) }{1-r_{g}/\left ( 2r\right ) }\ , \frac{r_{g}^{2}}{4r^{4}}\left ( \mathbf{r}\cdot d\mathbf{r}\right ) ^{2}-\left ( 1+\frac{r_{g}}{2r}\right ) ^{2}d\mathbf{r}^{2} . $$
i did some hunting and followed the paper trail to cook et al . ( 1995 ) . in section 4 , they identify a class of stars that brighten aperiodically . they reckon that these " blue bumpers " are be stars : b-type stars that show strong emission lines . then again , this is off one paper and i am really not sure if this is a widely accepted view , but i imagine there would've been more fanfare if this was an exciting new class of object ? i am not massively learned on the subject of be stars , but the current consensus seems to be that they are rotating very rapidly , to the extent that there is a substantial circumstellar disk of material around the equator . be stars are themselves a type of shell star , all of which are known to show aperiodic variations , presumably because of the unstable nature of the system . in fact , the whole class seems to form an observational problem because of their variable nature and light from the star getting mixed up with the disk .
a square has its diagonals at right angles . so , find the forces along each diag . i.e. , m1 and m3 on m5 for one direction and m2 and m4 on m5 for other . you could actually simpy find difference between , " m1 and m3" and " m2 and m4" and the corresponding charges and use them as resultant mass and charge .
re question 1: you had typically measure the abundance ratio by putting a sample of your element in a mass spectrometer and counting how many atoms of the different masses you get . this is exactly how carbon dating works : it counts the c$^{12}$ and c$^{14}$ atoms and the ratio tells you how much c$^{14}$ has decayed . re question 3: it is difficult to find rocks that have been unchanged since the earth was formed because plate tectonics tends to mangle early rocks . the age of the solar sytem , and therefore the earth , is calculated by doing radiometric dating of meteorites . for example the allende meteorite is dated at 4.567 billion years old . we assume that the earth formed at the same time as the meteorites . the oldest rocks on earth are somewhere around 4 billion years old depending on whose claims you believe . to back to your question 2: i do not know of references for the isotopic composition of the early earth , but i imagine it would be easily googled . it is difficult to be sure about the isotopic ratios because we do not know what the ratios were in the dust cloud that formed the earth , however the ratios can be calculated in some cases . for example uranium decays to lead , but of course there is lots of lead around anyway so we can not tell what lead has come from uranium decay and what was there originally . once again we use meteorites . by finding meteorites that contain lead but no uranium we can infer the original isotopic distribution of the lead . then by comparing the isotopic distribution in earth rocks with the distribution from the meteorites we can work out how much lead has come from uranium decay and therefore what the original uranium ditribution was .
this class of problem is very simple or very complicated , depending on the ratio of the momentum of the bullet to the momentum lost by air drag . you have chosen the unfortunate latter case . basically , if the bullet 's velocity decreases significantly you will need some calculus for it . in the " simple " case where the velocity does not drop much , we will wind up reducing a lot out and using the mass-thickness of the atmosphere , $\mu \approx \rho h$ . $$ f_d = \frac{de}{dh} = \frac{1}{2 } \rho a c_d v^2$$ with a little magic . . . $$ \delta e = \frac{1}{2 } \mu a c_d v^2$$ to get the numbers : lets assume it has a pointy bullet shape ( best know shape ) with 5cm diameter and the orbit is the lower possible out of atmosferic resistance , lets assume the iss ( 150km ) . $$c_d = 0.42$$ $$ a = \pi \left ( 2.5 cm \right ) ^2 $$ $$ \mu = 10 \frac{ tons }{ m^2 } $$ $$ \frac{1}{2 } \mu a c_d = 3.75 kg $$ this is very bad for you , because your bullet weighs $100 g$ . what is the formula for kinetic energy of the bullet at launch ? why that is $\frac{1}{2} m v^2$ of course . let me use a qualitative picture . imagine that the atmosphere is a coherent ceiling with no significant depth . you bullet has to punch through it , but it makes a " perfect hole " when doing this . in short , your bullet 's momentum is decreased because it accelerates the mass in that hole to its same velocity . it picks up that hole and keeps moving with it . you should look at this as basic momentum balance . if the ceiling 's area within that hole is twice the mass of the bullet , the bullet 's velocity will be decreased by half . here , the mass of the hole it has to punch is 37.5 times the mass of the bullet . i hope you see where this is going . with this mental model , the answer is quite obvious . the velocity of the bullet must be 38.5 times orbital velocity . that is because the combined mass of the bullet and the air that it " picks up " in its trip is 38.5x the mass of the bullet . but this would only work if you shot the bullet straight up at orbital velocity . this would be quite useless . in actuality , you want to shoot it at a shallow angle - as shallow as possible so that the boosters that are used to circularize the orbit do not have to be as big . this correction is just dividing by a sin function . if you shoot 90 degrees straight up , then the above ratio holds . if you shoot at an angle lower than this , then divide by sin of the angle ( or multiply cosec ) , and that is the ratio for the extra mass of air you pick up . lets assume the iss orbit 150km above ground , we are at equator latitude this establishes that the apogee should be at the 150 km elevation , if you wanted to do this with only one rocket burn . that actually does not fully specify the problem . the perigee is unknown , but we know that it can not be higher than the surface of earth . still , that does not eliminate the possibility of shooting it straight up and doing the rocket burn for the full orbital velocity . even at a 45 degree shooting angle , the orbit would be extremely elliptical , and thus very far from full orbital velocity . nonetheless , even if you did shoot it straight up , you would require $1.2 km/s$ . then using the air resistance multiplier , that would be $46 km/s$ , which is just plain impossible to produce ( except with nuclear bombs ) . in short , you would send something heavier to get around this problem .
an element of the direct sum $h_1\oplus h_2\oplus . . . $ is a sequence $$\psi = \{\psi_1 , \psi_2 , . . . \}$$ consisting of an element from $h_1$ , and element from $h_2$ . . . etc ( countable ) . this has to have the special property that the sum $$\left\| \psi_1\right\|^2+\left\| \psi_2\right\|^2+ . . . $$converges . this convergence is part of the definition of direct sum . to show that the direct sum is a hilbert space , i need well defined addition operation . if i have a pair of such things : $$ \psi = \{\psi_1 , \psi_2 , . . . \}$$ $$ \phi = \{\phi_1 , \phi_2 , . . . \}$$ i can define their sum to be the sequence $$ \phi+\psi = \{\phi_1+\psi_1 , \phi_2+\psi_2 , . . . \}$$we need to check that $$\left\| \psi_1+\phi_1 \right\|^2+\left\| \psi_2+\phi_2 \right\|^2+ . . . $$converges . to do this , we use the fact that the individual terms satisfy $$\left\| \psi_i+\phi_i \right\|^2 $$ $$=\left\| \psi_i\right\|^2+\left\| \phi_i\right\|^2+ ( \psi_i , \phi_i ) + ( \phi_i , \psi_i ) $$ $$ \leq \left\| \psi_i\right\|^2+\left\| \phi_i\right\|^2+2\left\| \psi_i\right\|\left\| \phi_i\right\| $$ $$ \leq 2\left\| \psi_i\right\|^2+2\left\| \phi_i\right\|^2$$so convergence follows from the convergence properties of the individual spaces being summed . so this shows how we can add elements of the direct sum . scalar products follow in the same way . to define an inner product , we just add the inner products component wise , i.e. $$ ( \psi , \phi ) = ( \psi_1 , \phi_1 ) + ( \psi_2 , \phi_2 ) + . . . \ \ \ ( 1 ) $$to show the rhs converges , note $$| ( \psi_i , \phi_i ) | \leq \left\| \psi_i \right\| \left\| \phi_i \right\| $$but $$2\left\| \psi_i \right\| \left\| \phi_i \right\| \leq \left\| \psi_i \right\|^2+ \left\| \phi_i \right\|^2$$so the rhs of ( 1 ) converges absolutely and we are done - we have a well defined inner product space .
you have several questions . let 's take them one by one : what is teleported ? in continuous variable ( theoretical ) papers , position and momentum are just convenient denominations for any pair of conjugate continuous variables , in the same way than a qubit can be a 2-level atom , a polarized single photon or a spin-1/2 particle . if a quantum object has continuous unbounded degree of freedom , it behaves like the position ( along a given coordinate axis ) , and there exists a complementary observable which behave like momentum . current experimental realizations include the quadrature of the light , the polarization of a bright beam , collective spin of atomic clouds , etc . therefore , i am not sure that focussing on the position of the particle itself helps in understanding these papers . however , since , as told above , everything is equivalent to a position , the paper should apply to the position of a particle . so in the following , i will assume we are dealing with the position $x$ along the $x$ axis , and $p$ will be the momentum along the same axis . what origin ? the origin does not matter , as long as it is fixed . suppose alice and bob have two different labs , and alice wants to teleport the position and momentum of a particle from her lab to bob 's lab . that means that the initial position of particle a , relative to alice 's apparatus is the same as the final position of particle b relative to bob 's apparatus . in that sense , they can have the " same " position , even if they are in different rooms . of course , if one want to add details one should add that : since we are in the quantum world , " the same position " means that every position measurement would lead to the same measurement statistics . thank to galilean relativity , if alice and bob are moving in respect to each other , the " same momentum " is also to be understood as relative to the movement of each lab . actually , teleporting both position and momentum allows to ' completely ' teleport this degree of freedom , and the statistics of any measurement ( e . g fock-state projection ) will be the same . teleportation procedure i tend to think that continuous variable quantum information is often easier to understand in the heisenberg picture , where the observable operators behave almost like in classical physics initial state alice 's initial particle 's position and momentum are described by the operators $\hat x_a$ and $\hat p_a$ . the entangled pair is described by $\hat x'_a , \hat p'_a$ and $\hat x'_b , \hat p'_b$ . for convenience , i will define $\hat x_\pm=\frac1{\sqrt2} ( \hat x'_a\pm x'_b ) $ and $\hat p_\pm=\frac1{\sqrt2} ( \hat p'_a\pm p'_b ) $ . the entanglement of the pair means that , initially $\hat x_-$ and $\hat p_+$ are both small . it is allowed only if $\hat x_-$ and $\hat p_+$ both have big fluctuations . the bell measurement the bell measurement is a simultaneous measure of $\hat x_m=\hat x_a-\hat x_a'$ and $\hat p_m=\hat p_a+\hat p_a'$ . this measurement is possible because both observables commute . but after the measurement induces a back action on the complementary observables ( $\hat x_a + \hat x_a'$ and $\hat p_a + \hat p_a'$ ) , which become very noisy and cannot be measured later . in particular , the state of the particle a after the measurement does not contain information about the initial state anymore the teleportation itself alice tells bob the values of $x_m$ and $p_m$ , and he moves/accelerates his particle accordingly particle . after this step , we have $$ \hat x_b'^{\text{final}}=\hat x'_b + x_m = \hat x'_b + \hat x_a - \hat x'_a=\hat x_a-\sqrt2 \hat x_- $$ $$ \hat p_b'^{\text{final}}=\hat p'_b + \hat p_m = \hat p'_b + \hat p_a + \hat p'_a=p_a+\sqrt2 \hat p_+ $$ since both $\hat x_-$ and $\hat p_+$ are small , the final observables $\hat x'_b$ and $\hat p'_b$ correspond to the initial observables $\hat x_a$ and $\hat p_a$ . the position and momentum of the first particle have been teleported onto another particle . so if the initial particle was moving ( $\hat p_a\neq0$ ) , then the " target " particle is indeed moving at the end of the process .
yes there is a simple explanation . think of a metallic sphere that is not connected to anything ( i.e. . floating ) , move this conducting sphere close to a positive electrode . now what happens is , the negative charges are induced on the surface of the sphere closest to the electrode ; those negative charges leave positive charges behind . just like figure ( a ) here : the charge that accumulated at the surface because of induction by electrode affects the voltage everywhere . in electrostatics , the method of image is used to include the contribution of induced surface charge to the total voltage everywhere . the only parameter that one needs to describe a floating potential conducting sphere is the initial charge . in the figure above the total charge is zero . if the sphere was initially charged by a positive or negative charge , the induced surface charge will be affected depending on the initial charge magnitude and polarity . now speaking of mathematics , the electric flux is related to the volume charge density by gauss law : in the previous equation , d is the electric flux and rho is the volumetric charge density and q0 is the total charge in the volume . the previous equation basically states that the charge enclosed within a volume generates an electric flux that if integrated on the surface of the enclosing volume gives a constant value equal to the charge enclosed . in conductors it is known that there is no volumetric charge , instead there is only surface charge , so for the conducting sphere case gauss law becomes : the difference between the left hand side and the right hand side is that lhs is evaluated anywhere in space where r ( the radial distance ) > r ( the radius of the sphere ) . rhs is only evaluated at the surface of the sphere where surface charge is located . now in your example , first you assumed zero initial charge ( that is why the right hand side of your condition is zero ) ; second you only care about the part of conductor that faces the electrodes , that is why your surface integral is open rather than closed as gauss law dictates . to confirm this explanation try to plot the surface charge along the edge on which you defined a floating potential . i replicated your model , the voltage everywhere is : i plotted the surface charge density along the floating potential boundary : you see , a negative charge was induced close to the positive electrode leaving an identical positive charge behind ( close to grounded electrode ) , which is what happened in figure ( a ) above . if you integrate the surface charge density along the boundary the result is zero ( there is no initial charge ) . if you used non-zero initial charge , the integration result will be equal to that value . keep in mind the accuracy of the solution depends on the resolution of the mesh , so the value in case of zero charge is never equal to zero exactly because of numerical error . i hope that answered your question
you can use a negative charge to test an electric field . you just have to remember that the electric field points antiparallel ( opposite ) to the force on the charge , rather than parallel to it ( in the same direction ) . that is just a convention , though ; we could have defined the electric field to point with the force on a negative charge , and physics would work the same , except for a couple of negative signs in some formulas .
why are you looking for a radial surface . . ? look it as an equipotential surface ( a surface where all points are at same constant electric potential ) as it comes with sphere . hence , you can assume the points a to b as radial to find the potential difference .
confine the motion to a plane , and for convenience make it the complex plane . describe the position of a particle in uniform circular motion by $z = re^{i\omega t}$ . then the jerk ( derivative of acceleration} is $z&#39 ; &#39 ; &#39 ; = -i\omega^3 z$ . compare that to velocity $z&#39 ; = i\omega z$ . the jerk points opposite the velocity . thus , to move in uniform circular motion with $f = ma&#39 ; $ , you must constantly feel a force pushing you backwards , opposite your current direction of motion . additionally , you must have the correct initial conditions for $z ( 0 ) $ , $z&#39 ; ( 0 ) $ , and $z&#39 ; &#39 ; ( 0 ) $ . presumably , a string could not provide this force , because the force would still act in the direction of the string . we would have $z&#39 ; &#39 ; &#39 ; \propto z$ , the force in the direction of the motion , if the string followed hooke 's law . the solutions are $z = e^{\alpha t}$ with $\alpha$ some constant times a cube root of unity . the interesting solutions have both periodic and exponential behavior , so the particle would spiral out or spiral in with this force law . free particles in this universe move the same way particles with constant force move in our universe - on parabolas .
yes . beam neutrino experiments regularly observe events in which a single hadron appears in the detector . such events are attributable to $z$ exchange reactions like $\nu + p \to \nu ' + p ' + \pi ${*} , or $\nu + p \to \nu&#39 ; + p&#39 ; $ ( where $p&#39 ; $ is energetic enough to register in the detector ) . reactions with leptonic products are not obvious because they could also arise from $w$ exchange reaction . yes . contrary to the standard model , neutrinos are known to have mass , and thus have subluminal speeds dependent on their momentum . superkamiokande , sno , kamland , and other experiments have conclusively shown that neutrinos mix . work is proceeding on nailing down the full parameters of the mixing matrix . yes . indeed it is obvious in the above reaction . {*} this event may be elastic at the level where the neutrino interacts with a parton to form on on-shell pion from the nuclear sea . such occurrences are called " quasi-elasitc scattering " by nuclear physicists . alas some confusion arises because neutrino physicists reserve that phase for w exchange reaction likes $\nu_l + a \to l^- + a&#39 ; $ .
the dimensionality of $| \psi \rangle$ does not have any physical significance ( though you can formally set it to be whatever you want ) . the space of states is not the hilbert space of the theory itself , but the projective hilbert space . that is to say that two nonzero vectors in the hilbert space $| \psi \rangle , | \psi ' \rangle$ define the same physical state if $| \psi \rangle = c | \psi ' \rangle$ for some nonzero ( complex ) scalar $c$ . when we talk about dimensionality in physics , it is talking about how quantities scale when we change the units that we use to describe the problem . that is to say , for example , if under a change from kilometers to meters ( keeping all other units the same ) , some quantity $q$ changes numerically as $q \rightarrow 10^9 q$ , then $q$ has dimensions of volume since it scales as length cubed . under the same change , how $| \psi \rangle$ changes is not a measurable quantity . it could change to $| \psi \rangle$ or $10^9 | \psi \rangle$ or even $ i| \psi \rangle$ , but all of these are exactly the same physical state . so in quantum mechanics , the dimensionality of $| \psi \rangle$ is a formally meaningless quantity . it has no physical significance . there is no experiment that can measure it even in principle . you could choose a particular convention , but whatever choice you make has absolutely no impact on physics . if you are going to choose a dimension , the most natural one is dimensionless . with that said , it is often conventional to adopt some normalization condition for $| \psi \rangle$ . this is done to facilitate explicit calculation , though it is not exactly physical . such conditions can lead to some constraints on dimensionalities . one example is $\langle \psi | \psi \rangle = 1$ . this requires $ [ \langle \psi | ] = - [ | \psi \rangle ] = - [ d ] $ where $ [ \cdot ] $ denotes the total dimensionality ( some might argue that this is true by definition of the dual space , but i will not go that far ) and $ [ d ] $ is just some dimensionality . if we require the 1-dimensional wavefunctions $\psi^* ( x ) = \langle \psi | x \rangle$ and $\psi ( x ) = \langle x | \psi \rangle$ to satisfy $\int \psi^* ( x ) \psi ( x ) dx =1$ then this gives us conditions for the position basis and dual basis , namely that $ [ \psi ( x ) ] = [ \psi^* ( x ) ] = [ l^{-1/2} ] $ , where $ [ l ] $ denotes length . this gives $ [ |x \rangle ] =-\frac12 [ l ] + [ d ] $ and $ [ \langle x | ] = -\frac12 [ l ] - [ d ] $ . we still have this $ [ d ] $ floating around though , which is totally arbitrary , and the only way to get rid of it is to just pick some dimensionality for one of these quantities ( or impose some equivalent criterion , such as requiring that $ [ \langle \psi | ] = [ | \psi \rangle ] $ , giving $ [ d ] =0$ ) . no physical argument can give us a dimensionality $ [ d ] $ for the state vector itself , as it has no physical meaning .
the gravitational , massless , bosonic sector of the string effective action contains the metric tensor , and at least one more fundamental field φ , the dilaton . by comparing the einstein’s ( d+1 ) dimensional einstein-hilbert action with the effective tree level action mentioned before a relation between the effective string coupling ( also fixing the g constant ) and the dilaton field becomes manifest . the dilaton in string theory also can be rescaled to absorb trivial volume factors associated with compact spaces , but are also present in the non-compacted string models . the dilaton is a fundamental scalar field in closed string theory . the effective gravity equations in string theory includes the gravi-dilaton part that looks very similar to brans-dicke scalar-tensor theory of gravity ( this is valid only at tree level ) . the dilaton field , as mention before , controls the string coupling constant so the genus expansion in string theory is directly related to the dilaton field and to corrections to general relativity . there is also the possibility of a dilaton potential in noncritical dimensions . this creates the possibility that the dilaton field expectation value be fixed at a local minimum ( probably in a non-perturbative regime ) , fixing the coupling between strings . the dilaton field is then an essential component of all superstrings models , and thus of the cosmological scenarios based on effective string actions . chapter 9 in maurizios gasperini’s string cosmology book is a very nice introduction to dilaton phenomenology and its importance in cosmology .
the simple answer : satellites do feel this force , but obviously do not get ripped apart . the tidal forces are simply too small ( for the satellites ' materials ) to actually rip them apart . the why : tidal forces happen because one side of an object feels such a larger huge difference in force than the other side . the magnitude of the force not only has to deal with the size of things pulling on each other , but the distance . even massive things ( like the sun or jupiter ) have relatively little pull when very far away . if you get close , you feel the effects of them much more strongly and the effects increase more quickly ! due to the fact that io is a moon very close to other large bodies , this makes the difference of the force of gravity on one side is very different than that on the other . ( try plugging in correct values for io , jupiter , and the distance between them ; then try calculating the force of jupiter on io as seen from one side of io to the other . ) most man-made satellites around earth are much , much smaller than moons and they are surrounded by very distant or very small things . this makes the difference between the force of gravity on one side of the satellite is almost the same as the force of gravity on the other . this being said , if a man-made satellite were to be in the same situation as io , either being very large or being very close to other big things , it could get ripped to shreds . in short : it is all about what forces are being applied . distant objects apply small forces , close objects apply bigger forces . tidal forces happen when gravity changes wildly between your head and your feet .
does the stars in clusters rotate ? yes . all stars rotate to a greater or lesser extent whether they are in clusters , galaxies or whatever . broadly , move massive stars rotate more ( and are more often in binary systems ) and there is nothing special , as far as i know , about this trend continuing in clusters . does cluster 's stars have moons . if yes do they rotates/orbits or are they " frozen " in space ? this question does not make sense . moons do not orbit stars : they orbit planets . if you mean " do cluster 's stars have planets ? " i am not sure if we know . given that we are currently finding stacks of planets around stars , i there are probably planets around some cluster stars but they are too far away to see ( and will be for some time ) . and those planets will behave just like planets around other stars . they will rotate and follow an orbit . ( i am not sure what you mean by " frozen " in space ? ) what will happen if one of the stars blows up ? the main effect of supernovae in clusters is to expel interstellar gas from the cluster . that is , there are clouds of gas in the cluster at first and the fast-moving gas from a supernova blasts other gas out too . if other stars are very nearby they might be heated a bit but stars are quite hardy things when it comes to being irradiated by supernova . they would not be vapourized , for example . does that structure attracts or repels space objects ? clusters would attract " space objects " by exerting gravity on them . but actually , clusters repel objects in a way . when a star in a cluster approaches a pair of stars in a binary system , the tendency is for one of the three stars to be thrown out . the cluster effectively " radiates " stars over time . also , the above-mentioned expulsion of gas by supernova gradually depletes the cluster of mass and the stars move apart .
it is a scenario that has heavy scalars and relatively light gauginos , so it is one example of a class of " split susy " or " mini-split susy " scenarios that have survived most of the constraints . in this kind of scenario , collider bounds put the lightest superpartners , namely the winos , above about 270 gev . gluinos are constrained to be somewhere north of a tev , depending on the exact details of the spectrum . the biggest problem with this scenario is that it predicts a large amount of light wino dark matter from moduli decays , which is ruled out by gamma ray data . the constraints push the moduli heavier than desired , and also would tend to push them to a region where the " moduli-induced gravitino problem " becomes another worry . ( full disclosure : that link is a self-citation . ) that constraint can be avoided if there is a significant amount of $r$-parity violation , or perhaps some other modifications to the model .
the key is the coriolis force . the coriolis force is $f_c = -2m\omega \times v $ . here $\omega$ is the rotation of the frame of reference and $v$ is the linear speed of the satellite . if you do the calculations , left as an exercies for the reader , you will get the missing force . in case 2 the coriolis force is 0 , because the velocity $v$ has to be used in the local frame of reference . and there $ v = 0 $ .
the main point is that newtonian gravity fields are conservative . what that means is that it is impossible to have a configuration like the one you drew without there being gravitational fields pointing to the left and to the right in the regions where you want to do the ' horizontal ' transfer . for example , you might try to achieve this on earth by taking the usual uniform gravitational field and locating a very heavy mass just under the foot of the conveyor belt on the left . this will mean , though , that as you move your mass from the foot of that conveyor belt you will be fighting against the attraction of that very same mass , as shown with the red arrows : the net result is that doing both of those horizontal transfers takes work , and in fact it must take exactly the same amount of work as what you have gained from lifting the object in the weaker field . there are , of course , many possible ways to achieve the fields you want , apart from the one in my image , but because all gravitational fields are the sum of attractive forces to a bunch of point masses , and the field of each point mass is conservative , you will always , necessarily , have cross-pointing fields like the one i pictured that will do away with any perpetual motion engine .
no , the halbach array is not related to the monopole at all . it is just an special arrangement of magnet . any arrangement of a " dipole " magnet would not result in any monopole . it can be easily understood as follows : if each individual magnet contains no magnetic monopole , i.e. , $\nabla\cdot\mathbf{b}_i=0$ true for whole space , then the resulting magnetic field is $\mathbf{b} = \mathbf{b}_1 + \mathbf{b}_2$ when you put them together . hence the resulting divergence is also $\nabla\cdot\mathbf{b}=0$ . i do not understand what happens to the poles of magnets , or the direction of magnetic flux ? does it change its direction in each next or third element of array ? you should know that magnetic pole is not a very well defined concept as the " boundary " of the magnetic pole is not fixed . the flux line does change the direction across the boundary . but the most important things ( in the first figure ) is that the magnetic field line is continuous and closed , which means that there is no magnetic monopole . all the field lines go out have to come back at the next nearest neighbor . the effect of halbach array is simply to push the closed magnetic field line on one side , hence the resulting magnetic field was enhanced on one side . the diagram 2 looks like a monopole , because there is the same direction of flux all the time ? is it true , or am i misunderstanding something ? no , the flux direction is not in the same direction over the whole region , they are alternating instead . you can think of it by putting two array in diagram one ( flipped one of them ) together . each of them forming a small field line loop . at the center of two up arrows ( $\uparrow$ ) the magnetic field is pointing up . also , at the center of two down arrows ( $\downarrow$ ) , the magnetic field line is pointing down . hence , it is clear that the field line is alternating after each next nearest neighbor . the wiggler of the electron path is the result of the alternating magnetic field . i think there is a problem for the diagram 2 , the oscillating frequency is double of what it is supposed to be . it should be the same as the period of the array .
what the question refers to as " band intensity " is also referred to a " line strength " $s$ . to calculate an absorption coefficient $k$ from $s$ , a line shape function $f ( \nu - \nu_0 ) $ , where $\nu_0$ is the center of the line . $$k = sf ( \nu - \nu_0 ) $$ then " optical depth " = $ku$ , where $u$ is called " path length " but is really a measure of the absorbing substance in the path . see pages 15 and 16 of this lecture for more information : http://irina.eas.gatech.edu/eas8803_fall2009/lec5.pdf and also : http://nit.colorado.edu/atoc5560/week4.pdf
the work is of course equal to the potential energy of the dipole $\vec m$ in the magnetic field , $$ u = -\vec m \cdot \vec b . $$ now , your wording indicates that the magnetic field $\vec b$ is macroscopic so it may be treated as a classical parameter . however , $\vec m$ from a spinning particle is quantum mechanical . for example , for an electron , $$ \vec m = -g_s \mu_b \frac{\vec s}{\hbar} $$ where the spin $g$-factor is $g_s\sim 2.0023$ , $\mu_b=e\hbar/ ( 2m_e ) $ is the bohr magneton , and $\vec s$ is the operator ( or triplet of operators ) of the electron 's angular momentum – that act as $\hbar/2$ times the pauli matrices on the spin-up and spin-down states . so up to a multiplicative constant $\gamma$ , $$ u = \gamma \vec \sigma\cdot \vec b . $$ the work done on the electron is a $q$-number , an operator , which acts on the spin-up and spin-down states . for the up and down states relatively to the direction of the external magnetic field $\vec b$ , the work done has a sharply defined value , an eigenvalue of $u$ . for general linear superpositions , the work done on the electron has a certain probability amplitude to be positive and a certain probability amplitude to be negative . the squared absolute values of these probability amplitudes determine the probabilities that the work will be seen to be one ( positive ) value or the other ( negative ) value , just like everywhere in quantum mechanics . it is however very interesting to consider superpositions of states in this context , especially for $j=1/2$ . for spin-1/2 particles , each superposition of up and down states is equivalent to " up " with respect to a certain axis in space . if we deal with general superpositions , this axis on which the spin is " up " will generally not be aligned with the direction of the magnetic field . if that is the case , the presence of the magnetic field will have the effect of causing " precession " of the axis defining the spin up . the axis at which the particle is polarized " up " will be " turning around " the direction of $\vec b$ like a ninny . this precession results from the time-dependent change of the phases of the wave function – which is opposite for the up and down states , so the relative phase is changing as well – and the fact that the relative phases of the wave functions always matter ( for something ) in quantum mechanics . the classic experiment measuring this spin-dependent magnetic work and exhibiting lots of the quantum properties is called the stern-gerlach experiment . with some probabilities , the particle behaves in one way or another .
use the product rule : $$ \begin{align*} \partial_\lambda ( g_{\mu\nu} x^\mu x^\nu ) and = g_{\mu\nu , \lambda} x^\mu x^\nu + g_{\mu\nu} x^\mu_{ , \lambda} x^\nu + g_{\mu\nu} x^\mu x^\nu_{ , \lambda} \\ and = g_{\mu\nu , \lambda} x^\mu x^\nu + g_{\mu\nu} \delta^\mu_\lambda x^\nu + g_{\mu\nu} x^\mu \delta^\nu_\lambda \\ and = g_{\mu\nu , \lambda} x^\mu x^\nu + g_{\lambda\nu} x^\nu + g_{\mu\lambda} x^\mu \\ and = g_{\mu\nu , \lambda} x^\mu x^\nu + 2 g_{\lambda\nu} x^\nu \end{align*} $$ where i have used the convention $t^{\mu . . . }_{\nu . . . , \lambda} \equiv \partial_\lambda t^{\mu . . . }_{\nu . . . }$ , the fact that $x^\mu_{ , \lambda}=\delta^\mu_\lambda$ and the symmetry of $g_{\mu\nu}$ .
the canonical metric on $cp^n$ is the fubini-study metric . the distance between two states $\left| x \right\rangle$ and $\left| y \right\rangle$ is $$\gamma ( x , y ) = \arccos \sqrt{\frac{\left| \left\langle x \middle | y \right\rangle \right|^2}{\left\langle x \middle | x \right\rangle \left\langle y \middle | y \right\rangle}} . $$ the infinitesmal metric is thus : $$ds = \frac{\langle dx | dx \rangle}{\langle x | x \rangle} - \frac{\left | \langle dx | x \rangle \right|^2}{\left | \langle x | x \rangle \right|^2} . $$ notice that for $cp^1$ this reduces to the natural metric on the bloch sphere .
if i am only allowed to use one single word to give an oversimplified intuitive reason for the discreteness in quantum mechanics , i would choose the word ' compactness ' . examples : the finite number of states in a compact region of phase space . see e.g. this phys . se post . the discrete spectrum for lie algebra generators of a compact lie group , e.g. angular momentum operators . see also this phys . se post . on the other hand , the position space $\mathbb{r}^3$ in elementary non-relativistic quantum mechanics is not compact , in agreement that we in principle can find the point particle in any continuous position $\vec{r}\in\mathbb{r}^3$ . see also this phys . se post .
you probably know that the mass of the higgs boson is around $125$ gev , which means the energy it takes to create a higgs boson is around $125$ gev and therefore that the temperature at which significant numbers of higgs bosons will be created will be given by $kt = 125$ gev . one gev is $1.602 \times 10^{-10}$j , so the corresponding temperature is around $10^{13}$k - note that this is an order of magnitude estimate . anyhow , the temperature at the centre of the sun is around $10^7$ k , so it is six orders of magnitude too low to create significant numbers of higgs bosons . even a supernova only gets to a temperature of about $10^{11}$k , which is still two orders of magnitude too low .
yes , it does . we do not see it because our brain automagically ' correct it ' because it always see the same aberration from the childhood . our eye focuses on ' green ' wavelength as it is its peak sensitivity , so red and especially violet lines are usually slightly out-of-focus .
you need to define an appropriate " order parameter " for your system , one that takes into account the symmetries in the configuration as well ( rotational , transnational , etc ) . there are many ways you could define such " correlator " as you call it , it depends on the system . for example the nematic order parameter in liquid crystals is taken with respect to a vector denoted " the director $\mathbf{n}$" , which can e.g. be the average orientation of the anisotropic rods in the nematic phase ( to simplify here ) , this way the order parameter for each arbitrary rod can be correlated to the director as : $$s \propto \left&lt ; cos^2\theta^{\alpha}-1 \right&gt ; $$ where $\theta$ would be the angle between the direction of the main axis of the rod denoted $\alpha$ and the director . now this was an example for a uniaxial degree of order , but it can be biaxial as well and so on . . . more rigourosly you had define a distribution function for the orientation of rods in your system , and from which you had calculate the correlation function ( usually an average ) that you look for . finally based on the symmetries in the system ( inversion symmetry etc ) the tensor degree of the order parameter is defined ( order 0 a scalar , 1 a vector etc . ) . but this was for liquid crystals , which give a very straightforward idea to how order parameters are defined . now for crystals , you would be more interested in the local deviation of each atom from its lattice position . so the correlation function you are looking for in this case could be the average displacement needed in each degree of freedom of the system to bring back the atom to its lattice position defined by the crystalline structure . in contrast to the earlier example , here we work with the center of mass vectors of each rod or atom instead of the vector along their main axis ( if they are anisotropic i.e. ) . for more references , look into any textbook of solid state physics if you are studying crystals , and for example de gennes ' book for liquid crystals . finally keep in mind that there can be many different forms of order in a system , so the order parameter is defined accordingly ( system specific ) , but they are all referred to as order parameters .
a process where the energy is kept constant is called isoenergetic ( or , if you prefer , iso-energetic ) . it also seems from the literature that a flow where the energy is constant when following a fluid particle is usually called an isoenergetic flow . similarly , when the enthalpy is kept constant , the process ( or the flow ) is said to be isenthalpic ( or isoenthalpic ) . and so on . notice that if there is some subtlety and you keep a constant internal energy $u=\text{cte}$ but not a constant energy $e=u+e_{\text{m}}$ , by modifying the mechanical energy $e_{\text{m}}$ , you should refrain from using standard names like isoenergetic and explain precisely what happens .
the book derives the equation of continuity , which states that the cross-sectional area times the velocity of a flow is always constant . but nowhere in the derivation does the textbook explicitly assumes that the flow is laminar . so , does the equation hold for turbulent flows too ? that is only a special case of the equation of continuity for situation where density is regarded constant and velocity is constant across the cross-section of the pipe . in general , equation of continuity is $\partial_t \rho + \nabla\cdot ( \rho \mathbf v ) = 0$ and holds true even for turbulent flows if the mass of the fluid is locally conserved .
what i think is being asked here is about the alternatives theoretically ( and experimentally ) consistent with the cosmological solutions in gr . also some history of terminology . first it would be useful to distinguish between the observable universe and the universe . there are so many theories around that this distinction will help clarify communication ( and understanding ) . of course the observable universe is what is observed and deductions about what is true outside of that ( from some theory ) are not going to be astronomically measurable . now to parts of the question : people say that " the universe is expanding " rather than " observable matter is separating " . if the hubble galactic expansion had been discovered in the decades before einstein 's general relativity ( gr ) , then perhaps the second phrase would have been more common initially . however gr also contains cosmological solutions intended therefore to describe the universe ( and not just the observable one ) . at around the time of hubble 's discoveries the lemaitre-friedmann solution was gaining ground in which gr is seen to provide a specific expansion of the universe from a t=0 singularity . so the early and essentially continual match between theory and astronomical observation has resulted in the first phrase being used as well . how do we know that bb was not : t=0 , v ( universe ) > 0 but v ( matter ) =0 , e=very big ? strictly what describes the t=0 solution of gr is outside of gr . however leaving that point aside there are some comments . v ( universe ) =0 or > 0 this really asks what the singularity can be modelled as in gr . in the days when the closed universe model was prominently discussed it was natural to talk about universe radius and origin at a point for the singularity . however , and especially when the flat or open models seem more observationally accurate , those earlier assumptions do need to be re-considered in accounts . for a flat model say , then the singularity could be an e ^3 - much different from the " point " often discussed ( although other geometries may be possible too ) . however the density of the observable universe 's matter at this point would still tend toward infinity at t=0 and thus occupy v ( matter ) =0 as you have written it . so this leaves a big question about what might be true of the universe as a whole . obviously unknown , but a range of theories abound , including some based on inflation ideas . e= very big ? surprisingly perhaps this need not be true . the reason is that gravitational energy ( a kind of binding energy ) counts negatively . so many theorists prefer the idea that e= small , with e=0 also being popular because then there is no net energy in the universe at t=0 .
after you fix the closed string vertex operator , the remaining group of isometries is one dimensional and its volume is finite . for example if the vertex operator is fixed at the center of the disk , the remaining isometries are rotations . the volume of that group in some units is $2\pi$ . figuring out the precise constants involved is a bit messy though . there is another subtlety with this calculation - the on-shell graviton is pure gauge and strictly speaking this amplitude is zero . but with appropriate limiting procedure you can extract the tension as the proportionality constant involved in a slightly off-shell amplitude ( which is slightly ill-defined ) . this is another reason why the annulus calculation is cleaner .
waves on strings combine linearly . this means that you can split up a string 's motion into two ( or more ) superimposed waves . the two superimposed waves behave independently , as if the other one was not there . so if you have a standing wave set up on a string , and then you also introduce a travelling pulse , you get something like the following . ( the arrows represent the direction of movement , and the node is marked with a blue dot . ) now to answer your question . i wish i had a way to make the picture animated , but i think you can see it from still pictures . i am going to draw what happens after a short time , when the pulse reaches the node . the standing wave has also moved , and is now swinging back in the other direction . as you can see , the standing wave component still passes through zero at the node , as it always must , but the combined wave ( pulse + standing wave ) does not . because the pulse and the standing wave do not interact , the pulse just passes straight through the node as if it was not there , and the standing wave just keeps waving as if the pulse was not there . note that not interacting is not the same as not interfering . interference happens when two waves get added together and sum to zero , but neither of the two waves is affected by being added in this way , so even when waves interfere , they do not interact .
your approach is all right but the solution given by the textbook is wrong : ) , at least if no approximation is to be made . let 's go the other way around : start from $$p ( v , t ) = \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2v_0}\left [ \left ( \frac{v_0}{v}\right ) ^5 - \left ( \frac{v_0}{v}\right ) ^3\right ] $$ and then derive the values of $\kappa_t$ and $\beta$ ( by the way , should not it rather be $\chi_t$ ( see here ) and $\alpha$ ( here ) ? ) . as you mentioned , to do this , we need to compute partial derivatives of $p$: $$ \left ( \frac{\partial p}{\partial t}\right ) _v = \frac{\gamma c_v}{v} $$ $$ \left ( \frac{\partial p}{\partial v}\right ) _t = -\left ( \frac{\gamma c_vt}{v^2} + \frac{\varepsilon}{2{v_0}^2}\left [ 5\left ( \frac{v_0}{v}\right ) ^6 - 3\left ( \frac{v_0}{v}\right ) ^4\right ] \right ) $$ which give $$ \frac{1}{\kappa_t} = -v \left ( \frac{\partial p}{\partial v}\right ) _t = \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2{v_0}}\left [ 5\left ( \frac{v_0}{v}\right ) ^5 - 3\left ( \frac{v_0}{v}\right ) ^3\right ] $$ and $$ \beta = \kappa_t \left ( \frac{\partial p}{\partial t}\right ) _v = \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ 5\left ( \frac{v_0}{v}\right ) ^4 - 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) ^{-1} $$ clearly , that is not what the textbook gives in the first place , but it is close enough to understand what they did : a taylor expansion at order 3 in $v_0/v$ in booth cases , which gives back the expressions $$ \frac{1}{\kappa_t} \approx \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2{v_0}}\left [ - 3\left ( \frac{v_0}{v}\right ) ^3\right ] $$ and $$ \beta \approx \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ - 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) ^{-1} \approx \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) $$ it really should have been clearer in the text that you could suppose $v\gg v_0$ ( and not only $v&gt ; v_0$ ) and i do not see how you could derive the term in $\left ( \frac{v_0}{v}\right ) ^5$ from this as it is completely neglected to find back $\kappa_t$ and $\beta$
snell 's law still applies to the curved surface , but you have to measure the angles of incidence and refraction relative to the surface where the light hits . the image is my attempt to show parallel rays of light falling on a curved surface . even though the rays are parallel , the angle of incidence is different for the two rays because it has to be measured relative to the normal at the point the light strikes the surface . hence the angle $i$ is not the same as the angle $i'$ . response to comment : it has become clear from the comments that the problem is that the value of $n$ depends on whether the light is passing from the air to glass or from glass to air . to be precise the two refractive indices are reciprocals of each other i.e. $$ n_{air-glass} = \frac{1}{n_{glass-air}} $$ the refraction of the light ray happens because the speed of light , and therefore the wavelength , changes when the light enters and leaves the glass . the refractive index when a light ray passes from a medium 1 to a medium 2 is : $$ n_{1-2} = \frac{v_1}{v_2} $$ where $v_1$ is the speed of light in medium 1 and $v_2$ is the speed of light in medium 2 . so in our example the refractive index when passing from medium 2 to medium 1 is : $$ n_{2-1} = \frac{v_2}{v_1} $$ i.e. $$ n_{1-2} = \frac{1}{n_{2-1}} $$
your assumption is very wrong . gravity is everywhere . or simply , the box has mass . it exerts its very own gravity . if your 1st location has the field , then the 2nd location is also influenced by it unless it is at $\infty$ . you can not just shield anything from gravitational field . so , energy is still conserved when you are moving the box from one place to another . . .
i do not show any contradiction regarding brant 's answer . but , there is a difference between evaporation and boiling . evaporation is a surface phenomena where molecules gain energy through random collisions . but , boiling takes place throughout the entire volume where the molecules change phase throughout the volume of liquid . ok . . now back to the question : there are two things to put our eyes into . . . boiling point of water is $100^oc$ only at normal temperature ( 298 k ) and pressure ( 1 atm ) . as altitude increases , pressure decreases and eventually attains a near-zero kpa in space . this decreases the boiling point of the liquid ( water ) according to $pv=nrt$ . hence in space ( vacuum ) , water definitely boils . . ! ( but , mentioning " outside the gravitational influence " is non-correlated here . . . ) now , triple point of water comes into play . wiki clearly says that - at zero pressure ( which is below the triple point ) , it is impossible for water to remain as liquid . hence the left-over form - " gas " . at pressures below the triple point ( as in outer space ) , solid ice when heated at constant pressure , is converted directly into water vapor in a process called sublimation . temperature in outer space could be 2-3 k . once a good amount of water has boiled from the liquid phase , there would be an isolated area of water molecules which are at the coldest place of their lifetime . they would suddenly freeze and form ice crystals ( similar to a desublimation ) . see this paper for a brief conversation on the topic . . . astronauts have proved this in space . when astronauts urinate in space and release the contents , the urine rapidly boils into vapor , which immediately desublimates or crystallizes directly from the gas to solid phase into tiny urine crystals . urine is not completely water , but you had expect the same process to occur with a glass of water as with astronaut waste . but , it could be achieved simply by using a home-experiment producing snow by desublimation . note : this kind of rapid boiling and freezing would overcome the adhesive forces drastically under such conditions . . . so , no sticking to glass .
waves on a string are transverse waves not longitudinal waves . they are not variations in pressure , but variations in the displacement of the string . the ( average ) displacement is greatest at the anti-nodes and zero at the nodes .
note : $r_{ab}= ( -i-3j+3k ) m$ . so , $$ v_{ab}=- ( 2i+3j+4k ) \cdot ( -i-3j+3k ) $$ or , $v_{ab}= - ( -2-9+12 ) $ volt ( s ) or , $v_{ab}= -1$ volt .
i assume you used the formulae $f_o = fs\sqrt{\frac{1+v/c}{1-v/c}}$ for the clocks ahead of you and $f_o = fs\sqrt{\frac{1-v/c}{1+v/c}}$ for the clocks behind you . those formulae do imply a singularity for the clock that is closest to you . which equation to use ? the answer is neither . those expressions assume the travel is along the line of sight to the source . there is a singularity because collisions are singularities . your spaceship is plowing through the line of clocks . what you will see in front of you are a series of clocks ticking faster than yours . behind you , you will see a cloud of pulverized clocks . your spaceship had better have some very good forward shields . your spacecraft presumably is not doing that . instead , you are flying parallel to the line of clocks , with some constant , non-zero distance between the spacecraft and the line of clocks . you need to use the more generic expession $$f_o = fs \frac{\sqrt{1-\left ( \frac v c\right ) ^2}} {1-\frac v c \cos \theta_o}$$ where $\theta_o$ is the angle between the clock in question and your line of travel , as observed by you . the sign convention here is that $\theta_o$ is positive for clocks in front of you , negative for clocks behind you . the above expression reduces to the simpler expressions at the start of my answer for clocks very far in front of you and very far behind you . in between , you will get a nice continuous change from faster in front , slower behind . the clock right next to you ? it is a bit redshifted , and hence slower . here $\cos \theta_0=0$ , so in this case $fo=fs\sqrt{1- ( v/c ) ^2}$ . this is called the transverse doppler redshift . this means that there is a clock just slightly ahead of you that is ticking at your own clock 's rate .
your reasoning seems to be that because $\frac{t}{\sqrt{1-v^2}}\to \infty$ when $v\to 1$ , it must be the case that $\frac{t}{\sqrt{1-v^2}}\to 0$ as $v\to 0$ . but in fact when $v=0$ we have $$ \frac{t}{\sqrt{1-v^2}} = \frac{t}{\sqrt{1-0^2}} = \frac{t}{\sqrt{1}} = \frac{t}{{1}} = t . $$ so time does not pass infinitely rapidly but instead passes at exactly its normal rate , as measured by a clock attached to the stationary observer . one should hope that this would be the case !
the depletion region forms due to the equilibrium between drift ( field driven ) and diffusion ( concentration gradient driven ) currents . if you have very low doping , the depletion region will be large because a large volume of depleted semiconductor is needed to generate enough electric field to balance the diffusion current . on the other hand , if you have very high doping a much smaller region is required to balance the diffusion of carriers . i was actually doing this calculation last week , here are some results from my simulation of the poisson-boltzmann equation for a ge/gaas pn-junction for different doping levels . i have indicated the approximate width of the depletion layer with the blue background . the blue and greens lines are the conduction and valance bands , the red and light blue lines are the intrinsic fermi-level and the fermi-level , respectively . the top plot is for light doping $10^{16}\text{cm}^{-3}$ , the bottom is for higher doping $10^{17}\text{cm}^{-3}$ . this width of the depletion with applied bias was discussed here in an avalanche breakdown , where are the electrons that break free from ? , so that might also be interesting .
well , ya gotta be careful . as you noted , there is an acceptance cone angle . now , consider an idealized case where the fiber is perfectly straight and perfectly cylindrical . then , even for skew rays ( per wikipedia , " ray that does not propagate in a plane that contains both the object point and the optical axis . such rays do not cross the optical axis anywhere , and are not parallel to it" ) you can determine the local angle of incidence when they hit the internal $n_1/n_2$ index boundary , and see that the rays may end up with different vector angles $ ( \theta , \phi ) $ ( with respect to the x and y coordinates ) but basically the same range of angle with respect to the optic axis . all this is a long-winded way of pointing out that light paths have to be reversible , so a ray can not exit at an angle greater than the acceptance angle . however , if you feed a beam with angular spread less than the acceptance angle , it is more than likely that multiple skew bounces , and , more important , curvature in the fiber , will lead to exit angles as large as the acceptance angle .
when you look with an " math eye " at the shape in your first link , you will see that the form is more a part of a sphere , as theory predicts . in second link there is a picture of the parabolic mandrel which was used to form the foil . the pressure will not deform the foil much , just keep it inflated .
what i would do is to calculate the effective action from integrating at one loop the propagator in a space with boundaries . the result is quite simple , schematically of the form $\mathrm{tr}\log \delta$ where $\delta ( x_1 , x_2 ) $ is the propagator in position space . indeed , the free action is quadratic in the field , $ s\sim -\frac{1}{2}\phi ( \partial^2-m^2 ) \phi$ , which makes the path integral gaussian and hence explicitly calculable . since $\delta$ will depend on the geometry of your space , say the distance $l$ between two parallel planes , you will get that the vacuum energy depends on such a separation too . taking minus the derivative w.r. t l gives the force . of course , you need to have calculated what the propagator in such a non trivial space is , by solving e.g. the klein- gordon equations with boundary conditions at $y=l$ and $y=0$ , $y$ being the coordinates orthogonal to the plates . this is not the usual feynman propagator because of the non trivial boundary conditions ( far away from the boundary you should recover feynman ) . note also that th effective action will be uv divergent ( therefore you will need to regularize the trace above ) but its derivative w.r. t to $l$ , the force , is finite and calculable .
you are pretty much right and the principle - that many hot bodies are " nearly " black bodies and therefore the colour of their radiation is related to their temperature through the planck law ( or , more succinctly , the wien displacement law ) - is the basis for the optical pyrometer ( see this page on howstuffworks.com ) . there are some approximations though . hot surfaces are always at least slightly differenty from true black bodies and this difference is summarised in the emissivity ( see the wiki page with this name ) which is a scale factor defining the ratio of light emitted from a hot surface to that emitted by an ideal black body at that temperature . the complicated physics giving rise to a nonunity emissivity can be frequency dependent ; if so the spectrum will be distorted and an optical pyrometer , as well as we , can be tricked into thinking the temperature is different from what the body 's temperature really is . however , your principle is a good one , especially as a rule of thumb and your physical reasoning is very sound and admirable .
there are two different questions at work here , that you have kind of mashed together . the first question is " what is the speed at which a change in the electric field propagates ? " the answer to that is the speed of light . in qed terms , the electromagnetic interaction that we see as the electric field is mediated by photons , so any change in an established field ( say , due to shifting the position of the charge creating the field ) will not be felt by a distant object until enough time has passed for a photon from the source to make it to the observation point . the second question is " what is the speed of propagation of electric current ? " this speed is slower than the speed of light , but still on about that order of magnitude-- the exact value depends a little on the arrangement of wires and so on , but you will not be far off if you assume that electrical signals propagate down a cable at the speed of light . this relates to electric field in that the charge moving through a circuit to light a light bulb has to be driven by some electric field , so you can reasonably ask how that field is established , and how much time it takes . qualitatively , the necessary field is established by excess charge on the surface of the wires , with the surface charge being generally positive near the positive terminal of a battery and generally negative near the negative terminal , and dropping off smoothly from one to the other so that the electric field is more or less piecewise constant ( that is , the field is the same everywhere inside a wire , and the field is the same everywhere inside a resistor , but the two field values are not the same ) . when the circuit is first connected , there is a rapid redistribution of the charge on the surface of the wires which establishes the surface charge gradients that drive the steady-state current that will eventually do whatever it is you want it to do . the time required to establish the gradients and settle in to the steady-state condition is very fast , most likely on the order of nanoseconds for a normal circuit . there is a good discussion of the business of how , exactly , charges get moved around to drive a current in the textbook that we use for our introductory classes , matter and interactions , by chabay and sherwood . it does not go into enough detail to let you calculate the relevant times directly , but it lays out the basic science pretty well . ( it is a textbook for a first-year introductory physics class , so it sweeps a lot of condensed matter physics under the metaphorical rug-- there is no discussion of band structure or surface modes , or any of that . it is fairly solid conceptually , though , at least according to colleagues who know more about those fields than i do . )
the usual physics assumption for spontaneous emission of radiation ( which i think is what you are talking about here ) is that of a poisson process ; that is , if one has a large number of possible emitters , the times between consecutive emissions is not correlated . on the other hand , if we have only a single emitter , then the time at which it emits follows the exponential probability distribution . these things seem to have nothing to do with stochastic electrodynamics ( but i admit i would never heard of it and am not inclined to study it ) .
it can be used to measure speed - that is how police radar guns and speed cameras work . radar waves from the gun/ camera are reflected off the moving vehicle , and the wavelength is shifted according to the speed of the vehicle relative to the gun/ camera . in astrophysics , looking at light from distant galaxies , we notice that certain characteristics of the light are shifted in wavelength due to the doppler effect . this is known as red-shift , as we notice the light is mostly shifted to the longer-wavelength ( red ) end of the spectrum . this tells us that distant galaxies are moving away from us , which is the primary piece of information that led to the development of the big bang theory .
you could argue that everything that is used in the mathematical description of a physical problem indeed has physical meaning . for many things that meaning may be hard to find , or requires a completely unintuitive perspective of the situation ( see for example , the gas mileage analysis in this xkcd what-if ) . because of this , not many people will find this physical meaning useful to think about during the advancement of their models or theory . i still think it is a very good habit , if only to practice your physical intuition and theoretical understanding ( but of course , only up to a certain point : ) now , the physical meaning of the quantities you ask about : for circular and elliptical orbits , the semi-major axis $a$ is simply half the distance between pericentre and apocentre . half , because that is often more useful in symmetrical shapes and prevents factors of 2 popping up all over the place ( in some senses it is also strange that we use $2\pi$ everywhere ) . the full distance $2a$ ( the major axis ) , in combination with the mass of the central body $m$ and the " relative strength of gravity " expressed by newton 's gravitational constant $g$ , gives you the total binding energy $\epsilon$ of the orbit : $$ \epsilon = -\frac{gm}{2a} $$ and the orbital period $t$ again relates to only half the length , $$t = 2\pi \sqrt{\frac{a^3}{gm}}$$ which is equivalent to but more convenient than $$t = 2\pi \sqrt{\frac{ ( 2a ) ^3}{8gm}} $$ where the factor of $2\pi$ of course stems from the traversal of a complete circle ( or elliptical angle , in this case ) . there are however still an infinite amount of possible orbits that have equal binding energies and equal orbital periods ; there is more to the orbit than just the semi-major axis . this is often expressed in the eccentricity $e$ , which is simply the scale factor to use on the semi-major axis to get the distance between the ellipse center $c$ and the focus $f$: $$ cf = ae $$ this gives you the meaning of the centre $c$ as well ; the distance between the centre $c$ and the focus $f$ is a measure for how non-circular the orbit is ; it relates to the ratio of the distances at closest and farthest approach . if the distance $cf=0$ , you have a circle . if the distance $cf$ is small , you have an elliptical orbit with the same orbital period and the same binding energy as the circular one . and , applying kepler 's laws blindly , if the distance is a billion lightyears , you have a orbit that is so elliptical you can no longer distinguish it from a straight line , but still with the same orbital period and binding energy as the original circular orbit ( this is completely non-physical , because this would require faster-than-light travel at the close approach , and a black hole with mass $m$ at the centre , but near-zero schwarzschild radius . . . ) . these interpretations only go up to a certain " philosophical level " , so to speak . you could advance your understanding of the problem ( the general problem that is , not restricted to circles or ellipses ) by asking yourself what the physical meaning of the equivalent cone is . in the framework of classical mechanics , the problem is equivalent to the motion of balls thrown on the insides of a cone . so in principle , you can place a huge plastic cone with a specific opening angle and orientation in space , with the central body replaced by an equivalent everywhere-parallel gravitational field , and get the exact same orbit for your " balls " .
$p=iv$ where , $p$ is power , $v$ is voltage , and $i$ is current in amps . $e=pt$ where $e$ is energy and $t$ is time .
the $\theta$-term is also known as the axion term and it is simply the $f\wedge f$ term known to particle physicists . in a more condensed-matter-friendly language , $$\delta {\mathcal l} = \theta\left ( \frac{e^2}{2\pi h} \right ) \vec b \cdot \vec e $$ i do not know the optimum starting point but you may begin with http://arxiv.org/abs/1001.3179 and its followups and references . more generally , the $\theta$-term means the integral of the anomaly polynomial . note that the anomaly polynomial is a nice gauge-invariant expression - but in higher dimensions . the actual anomaly in the original spacetime is related to it by several operations .
habitable by whom ? there are conditions that are uninhabitable by humans , however , many " extremophiles " survive perfectly happy . although , if you are talking about humans , here is a small list ( all the rest are probably more " nice to have " requisites ) : approximately 20% oxygen ( more or less depending on the pressure ) temperatures that allow for liquid water adequate access to water ( and food ) adequate protection from radiation . then a whole host of conditions that would not end human life . such as deadly pathogens on chemicals in the atmosphere . all this said , since we only have a sample size of one currently for planetary life , we really do not know what is possible , or how to bound the problem . anything is just speculation drawn from this one sample . that said , we have found life on our own planet where we never suspected it to be . life has proven to be nearly unstoppable in propagating throughout every niche on this planet . so , the better question i think would be what are the requisites for abiogenesis ? i think once life manages to start on any planet , it will adapt to whatever conditions the planet presents ( to within a reasonable degree ) .
there are two main reasons as temperature is decreased the voltage of a car battery decreases and it is internal resistance increases . this means the battery can supply less current . as temperature is decreased the viscosity of oil in the engine increases so the engine is harder to turn over . in the days before fuel injection there was a third factor because carburettors are less good at vaporising fuel at low temperatures , but this will not be a problem for most modern cars .
you asked about the second equation . see below : $e^{ix}{}= 1 + ix + \frac{ ( ix ) ^2}{2 ! } + \frac{ ( ix ) ^3}{3 ! } + \frac{ ( ix ) ^4}{4 ! } + \frac{ ( ix ) ^5}{5 ! } + \frac{ ( ix ) ^6}{6 ! } + \frac{ ( ix ) ^7}{7 ! } + \frac{ ( ix ) ^8}{8 ! } + \cdots \\ [ 8pt ] {}= 1 + ix - \frac{x^2}{2 ! } - \frac{ix^3}{3 ! } + \frac{x^4}{4 ! } + \frac{ix^5}{5 ! } - \frac{x^6}{6 ! } - \frac{ix^7}{7 ! } + \frac{x^8}{8 ! } + \cdots \\ [ 8pt ] {}= \left ( 1 - \frac{x^2}{2 ! } + \frac{x^4}{4 ! } - \frac{x^6}{6 ! } + \frac{x^8}{8 ! } - \cdots \right ) + i\left ( x - \frac{x^3}{3 ! } + \frac{x^5}{5 ! } - \frac{x^7}{7 ! } + \cdots \right ) \\ [ 8pt ] {}= \cos x + i\sin x \ . $ to calculate the expansions i have used in the above equation , you need to understand the procedure for finding taylor expansions of functions . this youtube video teaches the procedure : http://www.youtube.com/watch?v=gutltrdox3c
if you equate the torque about the center of the cylinder you will find that only two force can produce torque . therefore:- $$f_t*r=f_f*r$$ , and the tension is equal to the friction . then you write the equation in x direction along the slope of the incline :- $$f_t*cos ( \theta ) +f_f-m*g*sin ( \theta ) =o $$ here you replace friction with tension and you will have the answer in required form .
hawking radiation is a very robust prediction . it comes simply from applying quantum field theory in the curved space-time near the event horizon . it is also part of the synthesis called " black hole thermodynamics " , for which string theory provides an explanation in terms of the statistical mechanics of microstates . in the s-matrix of quantum gravity , if black holes did not evaporate , they had show up as asymptotic states , but they do not . ( there are eternal black holes in anti de sitter space , but they still evaporate , they just do not get to evaporate completely ; the particles produced by the evaporation can not escape to infinity because of the peculiarities of ads geometry , and fall in again . ) so denying the existence of hawking radiation would screw up many other things . you could say that hawking radiation is real but that it falls back in , like in ads space , but there is no reason for it to do so . the paper featured at arxivblog is a " what if " paper which ignores all these problems and proceeds to calculate some of the consequences . you could compare it to an engineering study of one of m.c. escher 's impossible structures : if you ignore the contradictions in its design , maybe you can calculate some of its properties , but it only has recreational value to do so . we do not quite know that a nonevaporating quantum black hole is logically impossible , in the way that we know the impossible staircase is impossible , but in the future a genuine proof may be available . but empirical confirmation of black hole evaporation is rather unlikely . if we could produce mini black holes in colliders , then we had see it , but those models are not especially favored ; they are a " what if " of a different sort , one in which there is at least a consistent fundamental picture behind the hypothesis ( particular braneworld models ) , but it is just one of many possibilities about what happens at the next frontier of physics and those models are not significantly favored . ( these models are also the ones which predict a detectable signature in grb data . ) if we could send a probe to the edge of an astrophysical black hole , maybe the radiation could be detected , but that is a job for interstellar civilizations , if they exist . maybe you could find indirect evidence for hawking evaporation of primordial black holes in the cosmic microwave background . but i do not know how likely that is - again , it would be highly model-dependent .
although i have not researched it , you can learn some physics and try to get into computational physics . there is some industry for it out there ( e . g the materials sector , though i would doubt they hire anything but phds ) . however , if you are a really great programmer and bring in your ideas and experience creating software , with the additional knowledge of being able to conduct simulations of physical systems and conduct solid quantitative analysis , then why not . there is also a market for physics specific software . for example , you can learn electrodynamics and create a ( hopefully open source or atleast free ; ) ) counterpart to simion . if developing independent software alternative is too much , you can contribute by creating physics modules , writing patches etc . to products like sage . there are more possible places where you can develop , off the top of my mind the root develeopment team at cern has two non-physicists working for them . the best strategy i would recommend is to start learning basic physics , and simultaneously research what is going on at the interface of physics and computation . one guide would be to look at the conferences and seminars that are held on the subject and find out what currently engages physicists . for example , have a look at : physics and computation 2010 and conference of computational physics . look at the titles of talks and submitted papers , the workshops , tutorials etc . find more on the web ( keyword search on the arxiv ) etc , and you will get a rough idea of the status of the field . start working on something , make some contributions to open source initiatives or get your own results , basically get some credentials so that employers might pay attention to you and you might find yourself working along with phds .
that is a hard experiment . the remnant nucleus generally has a very low ( non-relativistic ) velocity{*} , making it difficult to detect and characterize . should one only consider individual events where all decay products have been detected and their individual momenta obtained ? this is the formal definition of an " exclusive " event , but experimentally we ( that is the nuclear and particle physics " we" ) almost always relax it with respect to the remnant nucleus . how do you make sure that a set of decay product events are associated to the same event ? time correlation ? does this mean that the sample size needs to be small enough so overlapping decay events are percentually few and can be filtered out ? where possible you set your event rate slow compared to the daq latch and report rate . so almost all reported daq events are associated with only one physics event . there is a bit of art and science in this . if i was really going to do it , i would consider measuring the transverse direction only at a radioactive beam accelerator{**} . that will not make it easy , mind you , just less horribly difficult . that simplifies your life in the sense that the remnant is now a ionizing particle with a macroscopic track length , so you can id it , but of course you no longer have good information on exactly where the decay occurs and may not be able to extract a good value for the longitudinal component of the momentum due to the decay {+} . {*} momentum of a few mev , and mass of multiple to hundreds of gev . you might ask the amo people about this , they are more used to working with just barely ionizing energies than particle physicists and may have an answer . perhaps it is a good application for a multi-channel plate . {**} they do exist . {+} lost in the noise of the much larger component due to the beam momentum .
yes , infrared radiation which is invisible to human eyes but still radiates heat . this is how they make thermal cameras , they are using your body 's infrared radiation which is detected by the camera and forms an image based on visible light .
the main problem about a rigorous solution to such a scattering proplem is that computations are extremely demanding . just imagine you have a wavelength $\lambda$ of some $400$nm to $700$nm for visible light ( from here ) : now , to do physically meaningful simulations , you will need a sub-wavelength lattice which makes any computational cell above , say $10\ , \mu m^3$ not accessible since you have in the order of one million grid points . approximative approaches but of course there can be ways out of it if you are willing to make some approximations which will largely depend on the characteristics of the particles you are looking at . it is best to assume that we only have spherical particles since we can apply mie theory in this case . large particles first of all , let us consider particles which are much larger than the wavelength . then , the radius $r$ times the wave vector $k=2\pi/\lambda$ is much bigger than one , $$kr\gg1$$ which basically means that one observes reflection at a plane interface . you can implement these particles using geometrical optics ( mixed with fresnel reflection if you like ) since nothing really wave-like will happen as in this image ( taken from here ) : small particles second , the particles should be much smaller than the wavelength , $$kr\ll1\ , . $$ then , everything what is observed is a sum of dipolar responses of the particles in the so-called rayleigh-scattering . then , the intensity of light scattered by a single small particle from a beam of unpolarized light of wavelength $\lambda$ and intensity $i_0$ is given by : $$i=i_0 ( 1+\cos^2\theta ) \frac{ ( kr ) ^6}{2 ( kr ) ^2}\left ( \frac{n_p^2-1}{n_p^2+2}\right ) $$ where i have chosen the variables to be consistent with the used terminology and $r$ is the distance to the object , $\theta$ is the scattering angle and $n_p$ is the sphere 's refractive index . here is an image of such a situation with some metal particles also having quadrupolar excitation ( from here ) : a mean field approach - effective permittivity if you have a lot of these small objects , you may use the clausius-mossotti relation which gives you an effective permittivity $\epsilon_p=n_p^2$ depending on the concentration of the particle in some volume : $$\epsilon_{eff} = \epsilon_p + \frac{n\alpha}{1-\frac{n\alpha}{3\epsilon_p}}$$ where $\alpha$ is the polarizability of the sphere , for details see e.g. electromagnetic mixing formulas and applications by sihvola . this would be something like a mean-field approach . you can make some very neat effects using this effective approach since it allows you to calculate a continuous refraction around some particle streams under water . however , if the particles size is in the order of the wavelength , $$kr\approx 1$$ then you may have to take higher multipole moments into account which may be a very demanding task . for much more on the subject i would recommend bohren and huffmanns classic absorption and scattering of light by small particles . sincerely
from the article to which you linked : the cloud shines brightly in gamma rays due to a reaction governed by einstein’s famous equation $e=mc^2$ . negatively charged subatomic particles known as electrons collide with their antimatter counterparts , positively charged positrons . so you see , the very reason that we know it is a cloud of antimatter is precisely because it is already annihilating with normal matter that is floating in interstellar space . this will not create a chain reaction because there is not that much more antimatter in our galaxy . ( if there was , it would have annihilated a long time ago . ) and , as others have commented , the products of electron-positron annihilation ( gamma rays ) cannot go on to trigger further annihilations , so the idea of a chain reaction does not apply in this case .
some 3d glasses use narrow-band filters rather than polarization , which is quite clever . see http://en.wikipedia.org/wiki/dolby_3d for details . the reason the light looked different through each eye is that the light spectrum is not uniform across the visible wavelength . thus , when different parts of that spectrum are viewed ( through the left or right lenses ) , you get light that is not quite white , and not quite the same .
there are two problems to deal with which must be disentangled to solve problems like these . both angular momentum operators are vector operators , so in some sense they " take values " in $\mathbb r^3$ ; you are being asked for their dot product , which should be taken within that copy of $\mathbb r^3$ . you would have the same problem if you were asked to calculate the dot product $\mathbf r\cdot\mathbf p$ for a single particle without spin . the orbital and spin angular momentum operators act on the two different factors of a tensor product of hilbet spaces . thus any ( operator ) product of a scalar orbital operator with a scalar spin operator should be interpreted as a tensor product . you would have the same problem if you were asked to calculate the product $l^2s^2$ , which would need to be interpreted as $l^2\otimes s^2$ . thus , in your case , you must read $l\cdot s$ as $$ \mathbf{l}\cdot \mathbf{s}=\sum_{i=1}^3l_is_i=\sum_{i=1}^3l_i\otimes s_i . $$ to compute the matrix representation of this , you should begin with the matrix representation of each $l_i$ and $s_i$ . you then compute the tensor product matrices $l_i\otimes s_i$ . finally , you add all of those matrices together to get the final result . this is all much clearer with an example . the $z$ component , for example , is easy , since each matrix is given by $$ l_z=\hbar\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \quad\text{and}\quad s_z=\frac\hbar 2 \begin{pmatrix}1 and 0\\0 and -1\end{pmatrix} , $$ in the bases $\{|1\rangle , |0\rangle , |-1\rangle\}$ and $\{|\tfrac12\rangle , |-\tfrac12\rangle\}$ respectively . the tensor product matrix , then , in the basis $\{|1\rangle\otimes|\tfrac12\rangle , |0\rangle\otimes|\tfrac12\rangle , |-1\rangle\otimes|\tfrac12\rangle , |1\rangle\otimes|-\tfrac12\rangle , |0\rangle\otimes|-\tfrac12\rangle , |-1\rangle\otimes|-\tfrac12\rangle \}$ , is given by $$ l_z\otimes s_z=\frac{\hbar^2} 2 \begin{pmatrix} 1\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} and 0\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \\ 0\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} and -1\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \end{pmatrix} =\frac{\hbar^2} 2 \begin{pmatrix} 1 and 0 and 0 and 0 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 0\\0 and 0 and -1 and 0 and 0 and 0\\ 0 and 0 and 0 and -1 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 1 \end{pmatrix} . $$ this procedure should be repeated with both the $x$ and the $y$ components . each of those will yield a six-by-six matrix ( in this case ) . to get your final answer you should add all three matrices .
the link you posted in your question contains a link to a wikipedia page on the triboelectric effect , which in turns contains the answer to your question . from the " cause " section : although the word comes from the greek for " rubbing " , τρίβω ( τριβή: friction ) , the two materials only need to come into contact and then separate for electrons to be exchanged . after coming into contact , a chemical bond is formed between some parts of the two surfaces , called adhesion , and charges move from one material to the other to equalize their electrochemical potential . this is what creates the net charge imbalance between the objects . when separated , some of the bonded atoms have a tendency to keep extra electrons , and some a tendency to give them away , though the imbalance will be partially destroyed by tunneling or electrical breakdown ( usually corona discharge ) . in addition , some materials may exchange ions of differing mobility , or exchange charged fragments of larger molecules . the triboelectric effect is related to friction only because they both involve adhesion . however , the effect is greatly enhanced by rubbing the materials together , as they touch and separate many times .
for a linear system , the superposition principle holds since , be definition , a linear system has the following property : ( 1 ) if $y_1$ is the output for input $x_1$ and ( 2 ) if $y_2$ is the output for input $x_2$ then ( 3 ) the output is $ay_1 + by_2$ for input $ax_1 + bx_2$ in other words , the output for a superposition of inputs is the superposition of the associated outputs . so , if the differential equation for your system is linear , e.g. , the harmonic oscillator , the superposition principle holds . what , then , are you trying to prove ? prove the superposition principle for inhomogenous linear equations of motion used in deriving the motion of a driven oscillator . will it still apply if the force on an oscillator was −kx2 instead of −kx ? this is , i think , misworded . for example , for the mass on a ( linear ) spring system , the force on the mass , due to the spring is , by hooke 's law , $-kx$ . a driving force , on the other hand , would be given as a function of time : $f_d = f ( t ) $ . then , the net force on the mass is the sum of the driving force and the spring force , $f = f ( t ) - kx$ , which leads to a linear differential equation : $$m \ddot x +kx = f ( t ) $$ and thus , the superposition principle holds by definition . this is easy to show by assuming $f ( t ) = f_1 ( t ) + f_2 ( t ) $ and $x ( t ) = x_1 ( t ) + x_2 ( t ) $ and inserting into the differential equation . however , the way i read the problem as stated in your edit , it is the restoring force , not the driving force , that is $-kx^2$ . if that is in fact the case , the resulting differential equation is non-linear $$m \ddot x +kx^2 = f ( t ) $$ and thus , the superposition principle will not hold since $$ ( x_1 + x_2 ) ^2 = x_1^2 + x_2^2 + 2x_1x_2 \ne x_1^2 + x_2^2$$
i am going to explain roughly what the born rule , following stan liou 's comment . one of the postulates of quantum mechanics relates a mathematical quantity , the wave function ( or state $\psi$ of a hilbert space , $\mathcal{h}$ ) to a measurable entity , the probability of a given event to happen . the idea goes like this : if you want to measure a quantity $a$ , it might be the energy , angular momentum , position , etc . you will take the related ( linear ) operator to this observable $a$ , lets call it $\hat{a}$ . form the postulates of qm we know that when you measure the observable $a$ , we can only read certain values , the eigenvalues of the operator $\hat{a}$ , we will denote them by $a_i$ . what determines which eigenvalue $a_i$ your measure is going to read ? probability , with the following " protocol": at the beginning of the experiment your system was in a state $\psi\in\mathcal{h}$ , that will be a linear combination of the base states ( functions ) composed by the eigenstates of the operator $\hat{a}$ ( those that if you measure them will yield the eigenvalue associated to that eigenstate always , i.e. with probability 1 ) : $$ \psi = \sum_i\alpha_i\phi_i $$ where $\alpha_i\in\mathbb{c}$ denotes the complex amplitude of the eigenstate $\phi_i$ . then the probability of measuring a given $a_i$ ( eigenvalue associated to the eigenstate $\phi_i$ ) in your experiment will be $|\alpha_i|^2$ .
i am not sure about a convention , but i would normally write it as follows $$k=k ( x ) =k ( l \hat{x} ) = k_0f ( \hat{x} ) , $$ where $k_0$ is the pre-factor carrying the dimension and $f ( \hat{x} ) $ the function determining the spatial variation , which can easily be derived from $k ( x ) $ . this will give you $$\frac{d\hat{t}}{d\hat{t}}=f ( \hat{x} ) \frac{d\hat{u}^2}{d\hat{x}^2} . $$ in practical problems , the terms $x/l$ and $k_0$ naturally appear , e.g. $k ( x ) =k_0 ( 1+b\frac{x}{l} ) $ would give you $f ( x ) =1+b\hat{x}$ .
when you use the reduced mass , what you have first done is to go from the variables $ ( r_1 , p_1 ) $ and $ ( r_2 , p_2 ) $ to $ ( r=r_1-r_2 , p/\mu=p_1/m_1-p_2/m_2 ) $ and $ ( m r=m_1 r_1+m_2 r_2 , p=p_1+p_2 ) $ , where $m=m_1+m_2$ is the total mass and $1/\mu=1/m_1+1/m_2$ is the ( inverse of the ) reduced mass . as you said , this is usually introduced in classical mechanics to simplify the two-body problem , and it is not a priori valid in quantum mechanics . but in fact , it is . you just have to show that $\hat r , \hat r , \hat p , \hat p$ have all the expected commutation relations of independent position and momentum operators . this then allows to separate the two-body hamiltonian in two parts $\hat h ( \hat r_1 , \hat r_2 , \hat p_1 , \hat p_2 ) =\hat h_{red} ( \hat r , \hat p ) +\hat h_{cm} ( \hat r , \hat p ) $ .
the small changes in sunrise and sunset are caused by the tilt of the world , and the changes in light the earth gets causes winter and summer . it is been going on for millions of years , so there is no real harm there . one might also note things like the effect of these on thunderstorms , because the thunderstorms come ultimately from the earth 's magnetic field , and this can be influenced by the solar wind and the moon . nasa keeps an eye out for ' solar flares ' which , while not part of weather , can cause a nasty shock to society chance one comes our way .
all the work was done during the time your foot was in contact with the ball--those few milliseconds of impulse--identical to momentum--was transferred . without gravity that ball would not travel a certain distance but travel forever . the work done is equal to the ke the ball now possesses . if your foot remained in contact with the ball it would continue to accelerate , and even more work would have been done .
there is a identity for the derivative of the cross-product of two vector functions $\mathbf a ( t ) $ and $\mathbf b ( t ) $ ; \begin{align} \frac{d}{dt} ( \mathbf a \times \mathbf b ) = \frac{d\mathbf a}{dt}\times \mathbf b + \mathbf a\times \frac{d\mathbf b}{dt} \end{align} using this rule with the computation you are considering , we obtain \begin{align} \frac{d}{dt}\left ( m_i\mathbf r_i\times \frac{d\mathbf r_i}{dt}\right ) = m_i \frac{d\mathbf r_i}{dt}\times \frac{d\mathbf r_i}{dt} + m_i\mathbf r_i\times \frac{d^2\mathbf r_i}{dt^2} = m_i\mathbf r_i\times \frac{d^2\mathbf r_i}{dt^2} \end{align} where in the last step , we have used the fact that the cross product of any vector with itself is zero .
first of all , sean carroll is a relativist so his treatment of the diffeomorphism symmetry as a gauge symmetry should be applauded because it is the standard modern view preferred by particle physicists – its origin is linked to names such as steven weinberg , it is promoted by physicists like nima arkani-hamed , and naturally incorporated in string theory so seen as " obvious " by all string theorists . in this sense , carroll throws away the obsolete " culture " of the relativists . there are some other " relativists " who irrationally whine that it should not be allowed to call the metric tensor " just another gauge field " and the diffeomorphism group as " just another gauge symmetry " even though this is exactly what these concepts are . second of all , a symmetry expressed by a lie algebra can not be " discrete " , by definition : it is continuous . lie groups are continuous groups ; it is their definition . and only continuous groups are able to make whole polarizations of particles unphysical . it is plausible that a popular book replaces the continuous groups by discrete ones that are easier to imagine by the laymen but this server is not supposed to be " popular " in this sense . third , when you say that if $u$ is unitary , the generator has to be hermitian and traceless , is partly wrong . unitarity of $u$ means the hermiticity of the generators $t^a$ but the tracelessness of these generators is a different condition , namely the property that $u$ is " special " ( having the determinant equal to one ) . the tracelessness is what reduces $u ( n ) $ to $su ( n ) $ , unitary to special unitary . fourth , and it is related to the second point above , " charge conjugation " is not any gauge principle of electromagnetism in any way . electromagnetism is based on the continuous $u ( 1 ) $ gauge group . this group has an outer automorphism – a group of automorphisms is ${\mathbb z}_2$ – but we are never putting these elements of the discrete group into an exponent . fifth , similarly , qcd is not based on the discrete symmetry of permutations of the colors but on the continuous $su ( 3 ) $ group of special unitary transformations of the 3-dimensional space of colors . because none of the things you wrote about the non-gravitational case was quite right , it should not be surprising that you have to encounter lots of apparent contradictions in the case of gravity as well because gravity is indeed more difficult in some sense . sixth , $so ( 3,1 ) $ is not related to the diffeomorphism in any direct way . it is surely not the same thing . this group is the lorentz group and in the gr , you may choose a formalism based on tetrads/vielbeins/vierbeins where it becomes a local symmetry because the orientation of the tetrad may be rotated by a lorentz transformation independently at each point of the space . but this is just an extra gauge symmetry that one must add if he works with tetrads – it is a symmetry that exists on top of the diffeomorphism symmetry and this symmetry is different and " non-local " because it changes the spacetime coordinates of objects or fields while all the yang-mills symmetries above and even the local lorentz group at the beginning of this paragraph are acting locally , inside the field space associated with a fixed point of the spacetime . ( the fact that diffeomorphisms in no way " boil down " to the local lorentz group is a rudimentary insight that is misunderstood by all the people who talk about the " graviweak unification " and similar physically flawed projects . ) i will not use with tetrads in the next paragraph so the gauge symmetry will be just diffeomorphisms and there will not be any local lorentz group as a part of the gauge symmetry . the diffeomorphism symmetry is locally generated by the translations , not lorentz transformations , and the parameters of these 4-translations depend on the position in the 4-dimensional spacetime . this is how a general infinitesimal diffeomorphism may be written down . if there were no gauge symmetries , $g_{\mu\nu}$ would have 10 off-shell degrees of freedom , like 10 scalar fields . however , each generator makes two polarizations unphysical , just like in the case of qed or qcd above ( where the 4 polarizations of a vector were reduced down to 2 ; in qcd , all these numbers were multiplied by 8 , the dimension of the adjoint representation of the gauge group , $su ( 3 ) $ etc . ) . because the general translation per point has 4 parameters , one removes $2\times 4 = 8$ polarizations and he is left with $10-8=2$ physical polarizations of the gravitational wave ( or graviton ) . the usual bases chosen in this 2-dimensional physical space is a right-handed circular plus left-handed circular polarized wave ; or the " linear " polarizations that stretch and shrink the space in the horizontal/vertical direction plus the wave doing the same in directions rotated by 45 degrees : this counting was actually a bit cheating but it does work in the general dimension . to do the counting properly and controllably , one has to distinguish constraints from dynamical equations and see how many of the modes of a plane wave ( gravitational wave ) are affected by a diffeomorphism . in the general dimension of $d$ , it may be seen that the tensor $\delta g_{\mu\nu}$ may be described , after making the right diffeomorphism , by $h_{ij}$ in $d-2$ dimensions and moreover the trace $h_{ii}$ may be set to zero . this gives us $ ( d-2 ) ( d-1 ) /2-1$ physical polarizations of the graviton . in $d=4$ , this yields 2 physical polarizations of the graviton . a gravitational wave moving in the 3rd direction is described by $h_{11}=-h_{22}$ and $h_{12}=h_{21}$ while other components of $h_{\mu\nu}$ may be either made to vanish by a gauge transformation ( diffeomorphism ) , or they are required to vanish by the equations of motion or constraints linked to the same diffeomorphism . morally speaking , it is true that we eliminate two groups of 4 degrees of freedom , as i indicated in the sloppy calculation that happened to lead to the right result . note that $$\frac{d ( d+1 ) }2 -2d = \frac{ ( d-2 ) ( d-1 ) }2-1 $$ i have to emphasize that these is a standard counting of the " linearized gravity " and it is the same procedure to count as the counting of physical polarizations after the diffeomorphism " gauge symmetry " – just the language involving " gauge symmetries " is more particle-physics-oriented .
even in curved spacetime , you can perform a coordinate transformation at any location ( "move to a freely falling frame" ) such that your metric is locally flat , and takes the form \begin{equation} ds^2 = -c^2 dt^2 + dx^2 + dy^2 + dz^2\end{equation} if you consider a null trajectory where $ds^2$ is set to 0 , then the above equation is the statement that " the differential physical distance traveled along the trajectory , as measured by an observer in a freely falling frame at the location in consideration , is equal to the speed of light times the differential time interval measured by that observer . " from einstein 's equivalence principle , this is precisely the way that light must behave .
how does this connect in any way to my equations ? you know the distance travelled and you know the proper time , $t ' = \tau = 10y$ . using the timelike invariant interval equation , solve for t : $t^2 = \tau^2 + ( \frac{r}{c} ) ^2 = ( 10y ) ^2 + ( 4.4y ) ^2 = 119.36y^2 \rightarrow t = 10.93y$ $u = \frac{r}{t} = 0.403c$ since you insist on doing it the hard way , here 's one approach : from your first transformation equation : $\dfrac{\delta x}{\delta t'} = \gamma u = \dfrac{4.4ly}{10y} = 0.44c$ from your 4th transformation equation : $\gamma \delta t = \delta t ' + \delta x \frac{\gamma u}{c^2} = 10y + 4.4y ( 0.44 ) = 11.94y$ combining these results : $ ( \gamma u ) ( \gamma \delta t ) = 5.25ly = \gamma^2 u \delta t = \gamma^2 \delta x = \gamma^2 4.4ly$ from which we get : $\gamma = 1.093$ $\delta t = \gamma \delta t ' = 1.093 \cdot 10yr = 10.93y$ $u = \gamma u / \gamma = \dfrac{0.44c}{1.093} = 0.403c$ as i attempted to point out in comments , the invariance of the interval is fundamental and useful here . the lorentz transformations guarantee that : $ ( c\delta t ) ^2 - ( \delta x ) ^2 = ( c\delta t' ) ^2 - ( \delta x' ) ^2$ you know that $\delta x = 4.4ly , \delta x ' = 0$ , and $\delta t ' = 10y$ so you just put in what you know and you get $\delta t$ immediately and the speed $u = \dfrac{\delta x}{\delta t}$ immediately follows . there is no need to find $\gamma$ in this problem
i was intending to update my answer to your original question to give you the required additional information . it is just a little technical and takes some time to write down all the technicalities . actually , one just has to write down the the functional : $\int_{s_2} \mathrm{tr} ( \phi d\phi \wedge d\phi ) $ , where , $\phi = \phi_x \sigma_x + \phi_y \sigma_y + \phi_z \sigma_z$ , ( $\sigma_x , \sigma_y , \sigma_z$ are the pauli matrices ) explicitely in his favorite coordinate system to understand its meaning : first observation : taking ito account the constraint defining the two sphere:$\phi_x ^2+\phi_y ^2+\phi_z ^2=1$ , ( in matrix notation , this condition is equivalent to $\phi^2 = i$ ) then the integrand is just the area element of the ( higgs vacuum ) two-sphere . second observation : consider the two form : $\omega = \mathrm{tr} ( \phi d\phi \wedge d\phi ) $ . it is easy to verify that it is closed : $d\omega =\mathrm{tr} ( d\phi \wedge d\phi \wedge d\phi ) = 0$ ( by the antisymmetry of the wedge product ) but it cannot be exact , otherwise the area of the two sphere would have been zero by the stokes theorem . third observation : the variation of this form ( with respect to any perturbation ) is exact : $\delta \omega = d \mathrm{tr} ( \delta \phi \phi d\phi ) $ ( please notice that one needs to use the condition : $\phi^2 = i \rightarrow \phi \delta \phi +\delta \phi \phi = 0 $ ) first conclusion : this form does not change under an infinitesimal variation of the fields . since any continuous map can be built of a series of infinitesimal maps , this form does not change for a continuous deformations of the map $\phi$ . thus it is a topological invariant . second conclusion : please look at the surface area element in spherical coordinates : $\omega = sin\theta d\theta d\phi$ and consider the map $\theta \rightarrow \theta $ , $\phi \rightarrow n \phi $ , clearly this map winds the sphere n times and it is not difficult to verify that the integral is equal to n . this is the reason for the name winding number . in my answer to your first question another family of maps with arbitrary winding numbrers were given .
in fluid dynamics , within a isentropic process , the pression is not constant , you have a law like $\frac{p}{\rho^\gamma} = constant$ , or $\frac{p}{t^{\frac{\gamma}{\gamma -1}}} = constant$ here $p$ is the pressure ( called too a static pressure , $\rho$ is the density , $\gamma$ is the ratio of the specific heats of the fluid . because this is an isentropic process , the pressure $p$ can be called " isentropic pressure " . you have a compressible flow equation , which could be written , in the simplest case : $$\frac{v^2}{2} + \frac{\gamma}{\gamma -1}\frac{p}{\rho} = constant = \frac{\gamma}{\gamma -1}\frac{p_0}{\rho_0}$$ here $v$ is the speed of the fluid , $p_0$ is called the " total pressure " ( or stagnation pressure ) , and $\rho_0$ is called the " total density " . at zero velocity $v=0$ , the notions of isentropic pressure and total pressure coincide . idem for density and total density .
we have : $\vec r = ( a+x ) \vec e_r$ $\dot{\vec{ r}} = ( a+x ) \dot \theta \vec e_\theta + \dot x \vec e_r$ $\ddot{\vec{ r}} = - ( a+x ) \dot \theta^2 \vec e_r + ( a+x ) \ddot \theta \vec e_\theta+ \dot x \dot \theta \vec e_\theta + \dot x \dot \theta \vec e_\theta +\ddot x\vec e_r $ finally : $$\ddot{\vec{ r}} = ( \ddot x - ( a+x ) \dot \theta^2 ) \vec e_r + ( ( a+x ) \ddot \theta + 2\dot x \dot \theta ) \vec e_\theta \quad\quad\quad\quad ( 1 ) $$ the equations of movement are : $$m ( \ddot x - ( a+x ) \dot \theta^2 ) = - v' ( a ) - x v'' ( a ) \quad \quad \quad\quad ( 2 ) $$ $$ ( a+x ) \ddot \theta + 2\dot x \dot \theta = 0 \quad \quad \quad \quad \quad \quad \quad \quad\quad ( 3 ) $$ we know suppose that the angular velocity is slowly varying with $x$ , with the law : $$\dot \theta = \dot \theta_0 ( 1 +\gamma x ) \quad \quad \quad \quad \quad \quad \quad \quad\quad ( 4 ) $$ we now plug this law in equation ( 3 ) and keeping only the term in first order in $x , \dot x$ , which gives : $$a\dot \theta_0 \gamma \dot x + 2 \dot x \dot \theta_0 = 0\quad \quad \quad \quad \quad \quad \quad \quad\quad ( 5 ) $$ so , finally $\gamma = \frac{-2}{a}$ , and :$\dot \theta = \dot \theta_0 ( 1 - 2 \frac{x}{a} ) $ . getting the square of the last expression , and keeping only the first term is x , we get : $$\dot \theta^2 = \dot \theta_0^2 ( 1- 4 \frac{x}{a} ) \quad \quad \quad \quad \quad \quad \quad \quad\quad ( 6 ) $$ applying equation $ ( 2 ) $ with $x = 0$ , we get : $$ma\dot \theta_0^2 = v' ( a ) \quad \quad \quad \quad \quad \quad \quad \quad\quad ( 7 ) $$ finally , from equation $ ( 2 ) , ( 6 ) $ , we get : $$m ( \ddot x - ( a+x ) ( 1- 4 \frac{x}{a} ) \dot \theta_0^2 ) = - v' ( a ) - x v'' ( a ) \quad \quad \quad\quad ( 8 ) $$ keeping only the first order in $x$ , we get : $$m\ddot x - ma \dot \theta_0^2 + 3xm \dot \theta_0^2 = - v' ( a ) - x v'' ( a ) \quad \quad \quad\quad ( 9 ) $$ finally , using ( 7 ) , we get : $$m\ddot x + ( 3 \frac{v' ( a ) }{a} +v'' ( a ) ) x = 0\quad \quad \quad\quad ( 10 ) $$ with $v ( a ) = kmr^3$ , this gives : $$\ddot x + ( 15ka ) x = 0\quad \quad \quad\quad ( 11 ) $$
even in the " empty " space there are fields . in particular magnetic field is going to bend a trajectory of a charged particle . the earth 's magnetic field is strong enough to trap electrons of energies up to 10 mev and protons with energies up to 100 mev . the charged particles from solar wind and cosmic rays form van allen radiation belt but this magnetic field would also prevent electrons ' fired ' at low earth orbits from escaping . outside the earth magnetosphere the magnetic field is generated by the currents around the sun , and though the fields are smaller ( away from the sun and other magnetic planets ) it still will bend trajectories of the charged particles . so a precision ' shooting ' with charged particles would be very difficult . nevertheless , there is a proposed system of space propulsion , the electric sail , which is somewhat similar to your suggestion . there , positively charged wire is used to deflect protons from the solar wind to produce the thrust .
pure states are a convenient abstraction for studying tiny , specially-prepared quantum systems . states of complicated systems are never pure . one makes ( very small ) systems pure by careful preparation . for example , a simple spin by means of a stern-gerlach magnet and a screen where only the up particles can pass . more complex systems need more complex preparation to make them ( at least approximately ) pure . note also that pure states can be entangled ; indeed , entanglement is usually defined only for pure states . thus there is no conflict between entanglement and pureness .
the ions and the electrons do not necessarily have the same temperature ( non-thermal plasma ) , but if you leave them for a while , they will undergo equilibration . i would not overestimate the value in kelvins of different degrees of freedom of subsystems . the temperature is associated with a mean kinetic energy . if you tackle an electron , you can accelerate it easily because of its low mass . conversely , even a fast electron will not give raise to the same momentum transfer as a heavy particle . so a fast electron is " not as powerful " as an equally fast ion . if you have an application like the ball , there the effect is mainly generated by accelerating of electrons in the electric field . if you go away from the electrodes , the field gets weaker and there the electrons lose their kinetic energy due to collisions with heavier particles . this is why a too high particle density ( or pressure ) is not the friend of open corona discharges - the glow effect can not extend too far away without an opposing charge somewhere else , such that there is a relevant electric field in between . of course , if the temperature is generally high ( thermal plasma ) as in the sun , then you will have charges flying around in any case . but for the earthly applications you have in mind , the area containing free electrons/ions does not extend forever and the temperature will not kill you unless the electric field that produced it is super strong . then as shaktyai pointed out , plasmas are not always totally ionized , usually the opposite is the case . for some cases the saha equation holds and there you get an idea about the functional dependence of the ionization degree with temperature . for high $t$ , the factor goes against 1 ( graph exp ( -1/x ) in wolfram alpha or so ) .
the sun is not " made of fire " . it is made mostly of hydrogen and helium . its heat and light come from nuclear fusion , a very different process that does not require oxygen . ordinary fire is a chemical reaction ; fusion merges hydrogen nuclei into helium , and produces much more energy . ( other nuclear reactions are possible . ) as for rockets , they carry both fuel and oxygen ( or another oxidizer ) with them ( at least chemical rockets do ; there are other kinds ) . that is the difference between a rocket engine and a jet engine ; jets carry fuel , but get oxygen from the air .
there is a sense in which this is right . if we model the laser as a stream of photons , hitting some surface , then it is the change in the momentum of the photons due to their interaction with the surface that causes the pressure . for example , if the laser shines on a mirror , then the photons will bounce back after hitting the mirror , and there momenta will change by an amount equal to twice the magnitude of their original momentum . however , it is also possible for the photons to be absorbed by the material . in this case , the photons do not bounce back , but their momentum change by an amount equal to the initial magnitude of the their momenta , and this momentum change due to interaction with the surface is what causes the pressure . on the other hand , if you are referring to the object that emits the laser , then it will certainly be the case that this object will feel a force in a direction opposite that of the beam travel direction . by conservation of momentum , if the laser emits a photon of momentum $p$ , then its momentum must increase in the opposite direction by that same amount $p$ as well . the change in momentum of the object then leads to propulsion . see also the wiki and especially the second paragraph in the quantum theory argument section .
a very general discussion-not specific to a system : the internal energy , $u$ , of a system is a function of state , which means that its value only depends on the thermodynamic variables ( $p , v , t ) $ for example , at a given state ( this means for a given set of values of these variables ) . let us make this more concrete : imagine the system is in a thermodynamic state where the thermodynamic variables have the values ( $p_i , v_i , t_i$ ) ( $i$ stands for initial ) . at these values of the thermodynamic variables the internal energy has a value : internal energy at the initial state $i$: $u ( p_i , t_i , v_i ) $ . you can think of a gas at pressure , volume and temperature condition ( $p_i , v_i , t_i$ ) . now imagine you change the thermodynamic variables to these ones ( $p_f , v_f , t_f$ ) ( $f$ stands for final ) . the internal energy now has a new value internal energy at the initial state $f$: $u ( p_f , t_f , v_f ) $ . in this process you have changed the internal energy of the system by an amount : change in u : $\delta u= u ( p_f , t_f , v_f ) - u ( p_i , t_i , v_i ) $ i hope it is clear to observe that the system could have followed an infinitely large set of $ ( p , v , t ) $-points , along an infinitely large number of different paths in order to go from state $i$ to state $f$ . however , these are not , in any way , influencing by how much $u$ will change , you can take which ever path you please to go from state $i$ to state $f$ . so the system has no memory of the intermediate states . in mathematical terminology , this means that the differential change , $du$ , is a perfect differential and this is stated by the simple mathematical expression $\oint_c du=0$ it is very similar to the gravitational potential of the earth , for example , which tells us that the amount of energy we need to spend to lift an object by 3m , does not depend whether we bring it straight vertically up or we follow some other path .
there might be several reasons , some more obvious than others . the quality of the cell ( including its orthogonal quality , aspect ratio , and skewness ) also has a significant impact on the accuracy of the numerical solution . orthogonal quality is computed for cells using the vector from the cell centroid to each of its faces , the corresponding face area vector , and the vector from the cell centroid to the centroids of each of the adjacent cells ( see equation 5–1 , equation 5–2 , and figure 5.22: the vectors used to compute orthogonal quality ) . the worst cells will have an orthogonal quality closer to 0 , with the best cells closer to 1 . the minimum orthogonal quality for all types of cells should be more than 0.01 , with an average value that is significantly higher . aspect ratio is a measure of the stretching of the cell . as discussed in computational expense , for highly anisotropic flows , extreme aspect ratios may yield accurate results with fewer cells . generally , it is best to avoid sudden and large changes in cell aspect ratios in areas where the flow field exhibit large changes or strong gradients . skewness is defined as the difference between the shape of the cell and the shape of an equilateral cell of equivalent volume . highly skewed cells can decrease accuracy and destabilize the solution . for example , optimal quadrilateral meshes will have vertex angles close to 90 degrees , while triangular meshes should preferably have angles of close to 60 degrees and have all angles less than 90 degrees . a general rule is that the maximum skewness for a triangular/tetrahedral mesh in most flows should be kept below 0.95 , with an average value that is significantly lower . a maximum value above 0.95 may lead to convergence difficulties and may require changing the solver controls , such as reducing under-relaxation factors and/or switching to the pressure-based coupled solver . source url
strictly speaking it is a unit of energy . but using $m=\frac{e}{c^2}$ , you can convert energy into mass . operating , we get $1{\rm\ , ev}/c^2 =1.78\cdot 10^{-36}\rm{\ , kg}$ . ( the $c^2$ is usually ommited . )
if the mass of the penny is negligible comparing to the hammer , the speed of the hammer at the end of the track should be the same in both cases . with the collision in the first scenario , the speed of the penny should be twice of the hammer if the collision is complete elastic . but , for the second move-along scenario , the speed of the penny will be just the same as the hammer at the end of the track .
the fundamental mechanism is the same . they are both photovoltaics . the cells on satellites are far more expensive and efficient . because satellite launch costs dominate any other cost , you might as well pay for a high-efficiency cell . terrestrial cells are 10-20% efficient , usually made from silicon . the ones in satellites can be double that efficiency , and are made from triple-junction ( recently maybe even quadruple-junction ) tandem cells with dozens of layers grown by an expensive epitaxial process ( usually mocvd ) . the cells on satellites have different design specs . . . weight , radiation-hardness , operating temperature , form factor , etc . they are also optimized for a slightly different spectrum of light , because there is no atmosphere that scatters the blue light etc . despite great efforts to make radiation-hard space solar panels , the radiation in space will certainly hurt their lifetime . but it seems not by much . this press release says that spectrolab has a solar cell that is expected to work almost as well ( 88% relative ) after 15 years in space . by comparison , i know terrestrial silicon solar cells can last well over 30 years , but i do not know exactly how the performance is at the end versus the beginning . by the way , you should keep in mind that the surface of earth can be a pretty tough environment too -- wind , weather , scratches , dirt , reactive oxygen and pollutant gases , heat-cycling , etc .
assuming $v-t$ graph is quadratic ; we can write velocity , acceleration and distance functions as $$ f ( x ) = ax^2 + bx + c \mbox{ } ( \mathrm{velocity} ) $$ $$f' ( x ) = 2ax + b \mbox{ } ( \mathrm{acceleration} ) $$ $$ f ( x ) = ax^3/3 + bx^2/2 + cx + k \mbox{ } ( \mathrm{distance} ) $$ from given requirement , we know that initial velocity is $v$ where $t=0$ . then : $$ f ( 0 ) = v$$ $$c = v $$ again from given requirement , we know that $v=0$ where $t=t$ . $$f ( t ) = 0$$ $$ at^2 + bt + c = 0$$ we also know that all movement will complete in a distance of $d$ . so integral from $0$ to $t$ should result $d$ . $$ d = f ( t ) - f ( 0 ) $$ from here we can calculate that $$ b = ( 6d - 4vt ) / t^2$$ $$ a = - ( ( ( 6d - 4vt ) / t^2 ) t + v ) / t^2$$ now we can write $f ( x ) $ and $f' ( x ) $ in terms of given $v$ , $t$ and $d$ .
if $z \mapsto \mathrm{e}^{\mathrm{i}\delta}$ , then $z^p \mapsto ( \mathrm{e}^{\mathrm{i}\delta} z ) ^p = \mathrm{e}^{\mathrm{i}\delta p} z^p$ . what we are actually looking at is that the " transformation of the $p$-th power of $z$" is a way to speak of the representation of the circle group $\mathrm{u} ( 1 ) $ labeled by $p$ .
since the magnetic field lines have to close themselves , when it transverses the superconductor it has to do it in a continuous fashion . this means , since the superconductor expels the field when in the sc state , the field gets trapped because it has no way of transverse the cylinder ring without opening the magnetic field lines ( some geometric imagination is useful here ; ) . hope it is clear enough .
the article looks indeed wrong . in fact , there are two mistakes . first , you are right that the acceptance-rejection method has to be applied to $\rho ( r ) $ , and not to $m ( r ) $ . to understand how this idea works , suppose we want to generate a one-dimensional normalized distribution function $p ( y ) $ . now , let 's assume we can rewrite this distribution function in terms of a variable $x$ , such that it takes the form of a uniform distribution . that is , $$ p ( x ) = \begin{cases} 1 and \text{for $0\leqslant x \leqslant 1 , $}\\ 0 and \text{elsewhere} . \end{cases} $$ given $p ( y ) $ , what is $x$ ? we have the jacobian transformation $$ p ( y ) dy = p ( x ( y ) ) \left|\frac{dx}{dy}\right|dy = \left|\frac{dx}{dy}\right|dy , $$ which implies $$ p ( y ) = \frac{dx}{dy} , $$ assuming that $x ( y ) $ is an increasing function . thus $$ x = \int_0^y p ( y' ) dy ' = f ( y ) . $$ in other words , the integral of $p ( y ) $ ( or equivalently , the area under the curve ) follows a uniform distribution . with this in mind , there are essentially two ways to perform a monte-carlo simulation . the first way is the acceptance-rejection method : plot the curve $p ( y ) $ and uniformly generate a pair of numbers $ ( a , b ) $ in the interval $ ( [ 0 , y_\max ] , [ 0 , p_\max ] ) $ , where $y_\max$ and $p_\max$ are the upper bounds of $y$ and $p ( y ) $ . if the coordinate $ ( a , b ) $ lies under the curve $p ( y ) $ , accept it ; otherwise , reject it . if the coordinate is accepted , $y=a$ is generated point . there are major drawbacks to this method : $y_\max$ and $p_\max$ can be infinite , so one would need a cut-off . and if $p ( y ) $ has a sharp peak , one ends up rejecting a lot of points . a far more efficient method is to uniformly generate $x$ , and calculate the corresponding $y$ by inverting $x=f ( y ) $: $$ y = f^{-1} ( x ) . $$ this automatically fills up the area under the curve , without rejecting points . if the calculation of $f^{-1} ( x ) $ is too numerically involved , one can use a combination of both methods : introduce another ( simpler ) function $f ( y ) $ that lies everywhere above $p ( y ) $ . apply the inversion method to $f ( y ) $ , generating a point $y$ . then uniformly generate a value $b$ in the interval $ [ 0 , f ( y ) ] $ . if $b\leqslant p ( y ) $ , accept $y$ ; otherwise , reject it . now , consider the hernquist distribution . since it has a cusp at the origin , and the cumulative mass $m ( r ) $ is a simple function $$ m ( r ) = m\frac{r^2}{ ( a+r ) ^2} , $$ i would definitely recommend the inversion method . but there is an important caveat here , and that is the second mistake in the article : $\rho ( r ) $ is not really a one-dimensional distribution . instead it is a distribution in 3-dimensional space , and it is only a function of one variable due to spherical symmetry . in order to apply the monte-carlo method , we have to express $\rho$ as a truly one-dimensional distribution function , which we can do by expressing it in terms of the volume $$ y = \frac{4\pi}{3}r^3 . $$ now we have $$ p ( y ) = \rho ( y ) = \frac{m}{2\pi}\frac{a\ , ( 3y/4\pi ) ^{-1/3}}{\left [ a + ( 3y/4\pi ) ^{1/3}\right ] ^3} , \\ f ( y ) = m ( y ) = \int_0^y\rho ( y' ) dy ' = m\frac{ ( 3y/4\pi ) ^{2/3}}{\left [ a + ( 3y/4\pi ) ^{1/3}\right ] ^2} . $$ once we generated a point $y$ , the corresponding radius is $$r=\left ( \frac{3y}{4\pi}\right ) ^{1/3} . $$ there is an important consequence : there are likely more particles at large radii than around the centre , even though $\rho ( r ) $ is much larger at small radii . the reason is that particles between two radii $r$ and $ ( r+\delta r ) $ occupy a shell with volume $$v = \frac{4\pi}{3}\left [ ( r+\delta r ) ^3-r^3\right ] . $$ the larger the radius $r$ , the larger the volume of the shell , which means you need more particles to fill it and get $\rho ( r ) $ . this is obvious in the case of a constant density , but it is also true for general densities .
the unique thing about sno was that it was simultaneously sensitive to charged-current and neutral-current interactions , because they used deuterated water . the three main interactions are neutrino capture on deuterium , $\nu + n \to e + p$ , which generates a fast electron and a slow proton . the lepton and the baryon exchange a $w$ boson ( the " charged weak current" ) . only electron-type neutrinos may participate in this interaction ; $\nu_\mu$ and $\nu_\tau$ would have to generate heavier leptons , but solar neutrinos do not carry enough energy to make those more massive particles . deuterium dissociation due to neutrino scattering , $\nu + ( np ) \to \nu + n + p$ . the free neutron will wander around for a while before getting captured on another deuteron and emitting a gamma ray . because the neutrino 's charge does not change this reaction is mediated by the " neutral current " ( the $z$ boson ) and all neutrinos contribute equally . elastic scattering from electrons , $\nu + e \to \nu + e$ . this interaction has both charged- and neutral-current contributions , so neutrinos of all flavors may contribute , but electron neutrinos contribute more heavily than the other flavors . these different interaction channels gave independent measurements of the total neutrino flux and the electron neutrino flux . it is worth noting that the neutral current had only just been predicted in 1967 , and was not discovered until the early 1970s . for the most part the solar neutrino community believed that there was some misunderstood property of neutrino detection that caused everybody to measure one-third the predicted solar neutrino flux . it took many years before the possibility that the misunderstood bit was a property of the neutrino itself was really taken seriously . i do not know for certain , but i would expect that the design discussions for sno began in the early 1990s . there are many technical challenges associated with the detector — not least that they have many tons of heavy water suspended in many tons of light water in a thin , transparent membrane . the heavy water is on loan from the canadian nuclear power industry ; sno has a hefty insurance policy to pay to replace it if the membrane ruptures and the heavy water mixes with the light water and is ruined .
to answer your question , you should first understand when is a system most stable . firstly it should not have a tendency to move or change state , thus it should be under equilibrium conditions , i.e. the net force should be zero . we know that $$f = - \frac{du}{dx}$$ putting $f=0$ , we get $$\frac{du}{dx}=0 \tag{1}$$ secondly , it should be able to maintain that equilibrium condition by itself . this can be tested by displacing the system by a small distance $dx$ . if the force on the system then becomes opposite to direction of $dx$ , we can say that the system has a tendency to restore back to its original equilibrium position . an example of this would be a ball kept at the bottom of a spherical valley . displace the ball a little towards the right , and the net force on it acts towards left , bringing it back to its original position . you will realise that i just described a stable equilibrium condition . what this proves is that it is the stable equilibrium condition in which the system is most stable . from the above description we have that the small displacement $dx$ and net extra force $df$ should be in opposite directions . $$\frac{df}{dx} &lt ; 0$$ $$-\frac{d^2u}{dx^2}&lt ; 0$$ $$\frac{d^2u}{dx^2}&gt ; 0\tag{2}$$ from $ ( 1 ) $ and $ ( 2 ) $ it is evident that the graph of $u$ should have a minima at stable equilibrium condition , i.e. the potential energy should be minimum when a system attains maximum stability .
the reference you link to is for objects orbiting sun ( i.e. . comets or asteroids ) . if you are wanting to deal with satellites in low earth orbit , you will need a good book on orbital dynamics or astrodynamics . objects in solar orbit are typically dealt with in celestial mechanics . in both cases , the physics is the same ( two body gravitational interaction ) but the terminology is different . for satellites , one does not speak of " perihelion " or " aphelion " but rather " perigee " and " apogee . " some of the orbital parameters have slightly different names too . for satellites , orbital elements are disseminated in a highly standardized form called tle , which stands for two line element . there are also highly standardized algorithms for taking elements in tle form and turning them into topocentric ra and dec values as a function of time . your best bet , aside from an appropriate textbook on orbital dynamics , is to look at the source code for the algorithms themselves . the free satellite program predict http://www.qsl.net/kd2bd/predict.html stands out as being reliable and well documented . of course , the source code is included in the download . if you google around you may even find the nasa papers that document the sgp4/sdp4 algorithms . here are some c++ and c# implementations of the norad sgp4/sdp4 algorithms ( i have not tested or otherwise evaluated them ) . http://www.zeptomoby.com/satellites/ here 's another site i just found via google that looks potentially useful . http://satelliteorbitdetermination.com
the length of one second in meters is the distance traveled by light in one second . $1$ sec $=c\times1$ sec $= 299,792,458$ m the reason we use the same units for time and distance is special relativity , whose foundation rests on the speed of light ( in vacuum ) being constant in all inertial frames of reference . its universality allows us to use the same units for both time and distance .
there is no escaping the lie theory if you want to understand what is going on mathematically . i will try to provide some intuitive pictures for what is going on in footnotes , i am not sure if it will be what you are looking for , though : on any ( finite-dimensional , for simplicity ) vector space , the group of unitary operators is the lie group $\mathrm{u} ( n ) $ , which is connected . lie group are manifolds , i.e. things that locally look like $\mathbb{r}^n$ , and as such possess tangent spaces at every point spanned by the derivatives of their coordinates - or , equivalently , by all possible directions of paths at that point . these directions form , at $g \in \mathrm{u} ( n ) $ , the $n$-dimensional vector space $t_g \mathrm{u} ( n ) $ . 1 canonically , we take the tangent space at the identity $\mathbf{1} \in \mathrm{u} ( n ) $ and call it the lie algebra $\mathfrak{g} \cong t_\mathbf{1}\mathrm{u} ( n ) $ . now , from tangent spaces , there is something called the exponential map to the manifold itself . it is a fact that for compact group , such as the unitary group , it is surjective onto the part containing the identity . 2 it is a further fact that the unitary group is connected , meaning it has no parts not connected to the identity , so the exponential map $\mathfrak{u} ( n ) \to \mathrm{u} ( n ) $ is surjective , and hence every unitary operator is the exponential of some lie algebra element . 3 ( the exponential map is always surjective locally , so we are in principle able to find exponential forms for other operators , too ) so , the above ( and the notes ) contain the answers to your first three questions : we can always represent a unitary operator like that since $\mathrm{u} ( n ) $ is compact and connected , the exponential of an operator means " walking in the direction specified by that operator " , and while $\mathcal{u}$ lies in the lie group , $\mathcal{t}$ lies , as its generator , in the lie algebra . one also says that $\mathcal{t}$ is the infinitesimal generator of $\mathcal{u}$ , since , in $\mathrm{e}^{\alpha \mathcal{t}}$ , we can see it as giving only the direction of the operation , while $\alpha$ tells us how far from the identity the generated exponetial will lie . the physical meaning is a difficult thing to tell generally - often , it will be that the $\mathcal{t}$ is a generator of a symmetry , and the unitary operator $\mathcal{u}$ is the finite version of that symmetry , for example , the hamiltonian $h$ generates the time translation $u$ , the angular momenta $l_i$ generate the rotations $\mathrm{so} ( 3 ) $ , and so on , and so forth - the generator is always the infinitesimal version of the exponentiated operator in the sense that $$ \mathrm{e}^{\epsilon t} = 1 + \epsilon t + \mathcal{o} ( \epsilon^2 ) $$ so the generated operator will , for small $\epsilon$ be displaced from the identity by almost exactly $\epsilon t$ . 1 think of the circle ( which is $\mathrm{u} ( 1 ) $ ) : at every point on the circle , you can draw the tangent to it - which is $\mathrm{r}$ , a 1d vector space . the length of the tangent vector specifies " how fast " the path in that direction will be traversed . 2 think of the two-dimensional sphere ( which is , sadly , not a lie group , but illustrative for the exponential map ) . take the tangent space at one point and imagine you are actually holding a sheet of paper next to a sphere . now " crumble " the paper around the sphere . you will end up covering the whole sphere , and if the paper is large enough ( it would have to be infinte to represent the tangent space ) , you can even wind it around the sphere multiple times , thus showing that the exponential map cannot be injective , but is easily seen to be surjective . a more precise notion of this crumbling would be to fix some measure of length on the sphere and map every vector in the algebra to a point on the sphere by walking into the direction indicated by the vector exactly as far as it is length tells you . 3 this is quite easy to understand - if there were some part of the group wholly disconnected to our group , or if our group had infinite volume ( was non-compact ) , we cannot hope to cover it wholly with only one sheet of paper , no matter how large .
well , $\widehat{p^2} = \hat{p}^2= \hat{p} \hat{p}$ . so , in the position basis it is $-\hbar^2 \frac{d^2}{dx^2}$ , and $\langle p^2 \rangle = \int_{-\infty}^\infty \bar{\psi}\left ( -\hbar^2 \frac{d^2}{dx^2} \right ) \psi dx$ . note : $\hat{p}$ is technically not equal to $-i\hbar d/dx$ , but rather in the position basis $\langle x | \hat{p}| x ' \rangle = -i\hbar d/dx \delta ( x-x' ) $ .
what the definition needs to capture is that a black hole is not ( 1 ) a naked singularity , or ( 2 ) a big bang ( or big crunch ) singularity . we also want the definition to be convenient to work with so that , for example , it is possible to prove no-hair theorems . since we want to exclude naked singularities , it is natural that we require an event horizon . event horizons are by their nature observer-dependent things . for example , if we have a naked singularity , we can always hide its nakedness by picking an observer who is far away from it and accelerating continuously away from it . such an accelerated observer always has an event horizon , even in minkowski space . this example shows that it makes a difference what observer we pick . actually , we can not have a material observer at null infinity , since timelike infinity , not null infinity , is the elephants ' graveyard for material observers . however , the choice of null infinity is the appropriate one because a black hole is supposed to be something that light can not escape from . of course the actual universe is not asymptotically flat , but that does not matter . in practice , all we care about is that the black hole is surrounded by enough empty space so that the notion of light escaping from it is well defined for all practical purposes . there are other possible ways of defining a black hole , e.g. , http://arxiv.org/abs/gr-qc/0508107 .
set $x = r \cos \theta$ , $y = r \sin \theta$ . taking the total differentials , $$\mbox{d}x = \mbox{d}r \cos \theta - r \sin \theta \mbox{d} \theta , $$ $$\mbox{d}y = \mbox{d}r \sin \theta + r \cos \theta \mbox{d} \theta . $$ squaring and simplifyng $$\mbox{d}s^2 = {\mbox{d}x}^2 + {\mbox{d}y}^2 = {\mbox{d}r}^2 + r^2 {\mbox{d}\theta}^2 . $$ hence $$\frac{\mbox{d}s}{\mbox{d} t} \mbox{d} t = \sqrt{ \left ( \frac{\mbox{d}r}{\mbox{d}t}\right ) ^2 + r^2 \left ( \frac{\mbox{d}\theta}{\mbox{d}t}\right ) ^2 }\ , \mbox{d}t . $$ now , the property of being extremal is a characteristic of the curve , not of the coordinate system , so it is independent on the local chart you choose . in particular , the euler-lagrange equation retains the same form in both systems ( obviously , one changes the labels : $ ( x , y ) \to ( r , \theta ) $ ) . this remarks answer to point $ ( a ) $ and $ ( b ) $ . point $ ( c ) $ is a simple verification you can do eventually after have inverted the previous relations between $ ( x , y ) $ and $ ( r , \theta ) $ . for a brilliant discussion of this and more subtle points , let 's see arnold , mathematical methods of classical mechanics , paragraph 12 . c , 12 . d .
ds , as a segment of the wire carrying current , produces a magnetic field at " o " from all points that the wire is curved . when the wire is in the aa ' section or the cc ' section ds ( as an arbitrarily small section of the wire ) must now point at the origin and therefore it will not produce a magnetic field at the origin . in your question you probably mistakingly omited the $\hat{r}$ part which is why i asked for clarification .
what level are you at ? when you totally internal reflect light on a surface there is an electric field which extends a very small distance out of the surface - this is called the evanescent wave and excites states on the surface called surface plasmons . the interesting thing is that if you very slightly change the electrical characteristics of the surface you can make a big change in the internally reflected light . so a clever way of making extremely sensitive sensors is to bind some metal nanoparticles to the surface and coat them with some chemical that reacts with what you are looking for . the chemical you are sensing sticks to the nanoparticles , changes the electrical field and changes the reflected light = very sensitive chemical/biochemical detector .
as others have said , in young 's double slit experiment , $d &lt ; &lt ; l$ . this means , that that $\mathbf{ep} \approx \mathbf{r_1} \approx \mathbf{r_2}$ . if you draw the picture in scale , you will see , that it is really the case . $\implies$the angle $p$ - $b$ - ''point at the right side wall at same height as $b$'' is equal to $\theta$ also , because it is just the same angle as $pec$ . ( and because $pe$ || $r_1$ ) $\implies dba = 90^\circ - \theta \implies bad = 90^\circ - ( 90^\circ - \theta ) = \theta \text{ }_\box$
it seems most experts think the abiotic theory is nonsense see http://www.theoildrum.com/story/2005/11/4/15537/8056 for example dr . jon clarke : . the fact remains that the abiotic theory of petroleum genesis has zero credibility for economically interesting accumulations . 99.9999% of the world 's liquid hydrocarbons are produced by maturation of organic matter derived from organisms . to deny this means you have to come up with good explanations for the following observations . the almost universal association of petroleum with sedimentary rocks . the close link between petroleum reservoirs and source rocks as shown by biomarkers ( the source rocks contain the same organic markers as the petroleum , essentially chemically fingerprinting the two ) . the consistent variation of biomarkers in petroleum in accordance with the history of life on earth ( biomarkers indicative of land plants are found only in devonian and younger rocks , that formed by marine plankton only in neoproterozoic and younger rocks , the oldest oils containing only biomarkers of bacteria ) . the close link between the biomarkers in source rock and depositional environment ( source rocks containing biomarkers of land plants are found only in terrestrial and shallow marine sediments , those indicating marine conditions only in marine sediments , those from hypersaline lakes containing only bacterial biomarkers ) . progressive destruction of oil when heated to over 100 degrees ( precluding formation and/or migration at high temperatures as implied by the abiogenic postulate ) . the generation of petroleum from kerogen on heating in the laboratory ( complete with biomarkers ) , as suggested by the biogenic theory . the strong enrichment in c12 of petroleum indicative of biological fractionation ( no inorganic process can cause anything like the fractionation of light carbon that is seen in petroleum ) . the location of petroleum reservoirs down the hydraulic gradient from the source rocks in many cases ( those which are not are in areas where there is clear evidence of post migration tectonism ) . the almost complete absence of significant petroleum occurrences in igneous and metamorphic rocks ( the rare exceptions discussed below ) . the evidence usually cited in favour of abiogenic petroleum can all be better explained by the biogenic hypothesis e.g. : rare traces of cooked pyrobitumens in igneous rocks ( better explained by reaction with organic rich country rocks , with which the pyrobitumens can usually be tied ) . rare traces of cooked pyrobitumens in metamorphic rocks ( better explained by metamorphism of residual hydrocarbons in the protolith ) . the very rare occurrence of small hydrocarbon accumulations in igneous or metamorphic rocks ( in every case these are adjacent to organic rich sedimentary rocks to which the hydrocarbons can be tied via biomarkers ) . the presence of undoubted mantle derived gases ( such as he and some co2 ) in some natural gas ( there is no reason why gas accumulations must be all from one source , given that some petroleum fields are of mixed provenance it is inevitable that some mantle gas contamination of biogenic hydrocarbons will occur under some circumstances ) . the presence of traces of hydrocarbons in deep wells in crystalline rock ( these can be formed by a range of processes , including metamorphic synthesis by the fischer-tropsch reaction , or from residual organic matter as in 10 ) . traces of hydrocarbon gases in magma volatiles ( in most cases magmas ascend through sedimentary succession , any organic matter present will be thermally cracked and some will be incorporated into the volatile phase , some fischer-tropsch synthesis can also occur ) . traces of hydrocarbon gases at mid ocean ridges ( such traces are not surprising given that the upper mantle has been contaminated with biogenic organic matter through several billion years of subduction , the answer to 14 may be applicable also ) . the geological evidence is utterly against the abiogenic postulate . also abiogenic origin of hydrocarbons : an historical overview by dr . geoffrey lasby abstract : the two theories of abiogenic formation of hydrocarbons , the russian-ukrainian theory of deep , abiotic petroleum origins and thomas gold 's deep gas theory , have been considered in some detail . whilst the russian-ukrainian theorywas portrayed as being scientifically rigorous in contrast to the biogenic theory which was thought to be littered with invalid assumptions , this applies only to the formation of the higher hydrocarbons from methane in the upper mantle . in most other aspects , in particular the influence of the oxidation state of the mantle on the abundance of methane , this rigour is lacking especially when judged against modern criteria as opposed to the level of understanding in the 1950s to 1980s when this theory was at its peak . thomas gold 's theory involves degassing of methane from the mantle and the formation of higher hydrocarbons from methane in the upper layers of the earth 's crust . however , formation of higher hydrocarbons in the upper layers of the earth 's crust occurs only as a result of fischer-tropsch-type reactions in the presence of hydrogen gas but is otherwise not possible on thermodynamic grounds . this theory is therefore invalid . both theories have been overtaken by the increasingly sophisticated understanding of the modes of formation of hydrocarbon deposits in nature .
for compactified minkowski space see , e.g. , conformal infinity . this is sometimes useful to prove mathematical statements about general relativity and its relatives . but to reach infinity or to get information from there takes infinitely long , given the finite speed of light . this is why we can never find out whether or not spacetime is compactified .
look at around 0:34 in the video . i want to point out something relevant to the question here . the end of the tube is narrowed . that is , in technical terms , a nozzle . nozzles are extremely common in engineering and they work as a form of mechanical leverage just like a lever . i should also note that the straw itself is already a form of nozzle and allows ( i think ) a more potent blow than would otherwise be possible with the lips . a human mouth has limitations . the most accurate way to frame this would be to say that one 's mouth can only produce a given flow rate , $\dot{m}$ , at a certain pressure above atmosphere , $\delta p$ . combined , these give an energy rate , or power , that can be produced by the mouth . the comment by steve melvin does apply - that the balls can not move faster than the fluid that is passing by it . however , the small outlet of the straw he uses is a way to make a tradeoff , getting high fluid velocity by sacrificing volume of flow . this would , in fact , be rather more difficult to do as accurately and gracefully with mechanical forces . this type of easy conversion ability of forms of fluid mechanical work is a major reason that hydraulics is such a useful science .
a quick answer : " screening " currents in the superconductor are proportional to the vector potential . with an appropriate choice of gauge , the screening current appears as a mass term in the wave equation for the vector potential . from " an informal introduction to gauge field theories": ( this excerpt from google books )
you want to make sure that the heat flow is always between fluids at the nearest possible temperature , to minimize the entropy production from the flow of heat . the best method is to flow the cold coolant in the opposite direction as the hot coolant , so that as the cold coolant gets hotter , it is transferring heat to hotter hot-coolant . if you adjust the pipes right , and make them long , you can do the entire circuit with as close to zero entropy generation as you like . this is the principle by which ducks can send blood to their feet and do so without losing any significant body heat in the feet , although they are immersed in cold water . for the temperature profile , if you have the hot water be 100 degrees , and the cold water be 0 degrees , and you have a long linear pipe where they touch , then the profile can be exactly linear with a 1 degree difference in temperature at all points , assuming that that heat diffusion constant for the metal is constant over the range of temperature , and the specific heat of water is constant for the range of temperature , and both of these are close enough approximations for a practical heat exchanger . if the pipe runs from 0 to l , the profile for hot water temperature is : $t_h ( x ) = {100x\over l}$ the profile for the cold water $t_c ( x ) = {100x\over l} - 1$ so that the difference between them is always 1 degree . you adjust the flow rate so that the heat transfer moves c units of heat energy in a length $l/100$ , where c is the specific heat of water , and then the exchanger works with this profile . you can adjust the temperatures to be as close to each other as you like , and the entropy gain from the heat flow is : $ \delta s = {q\over t^2} \delta t \approx \delta s_0 {1\over 230}$ where you use the absolute temperature t in the denominator , so that in this system you only make about 1 percent of the entropy you would if you let the hot and cold water transfer heat by direct contact without an exchanger . these profiles are universal attractors , so if you just set up the appropriate flow rate , you will approach the linear profile with time .
this is a heisenberg picture/schrodinger picture confusion . the operator " r " is not time dependent in the schrodinger picture , but in the heisenberg picture , it is . the time dependent version of the operator is the operator r ( t ) which has the property that $$\langle\psi ( 0 ) |r ( t ) |psi ( 0 ) \rangle = \langle\psi ( t ) | r ( 0 ) |\psi ( t ) \rangle $$ for all choice of $\psi ( 0 ) $ . it is a unitary transformation which shifts the time dependence from the state to the matrix . $$ |\psi ( t ) \rangle = e^{-iht} |\psi ( 0 ) \rangle $$ so that $$ r ( t ) = e^{iht} r ( 0 ) e^{-iht} $$ and you can check that everything works formally . whenever you hear somebody talking about the time derivative of an operator , they are always talking in the heisenberg picture , because otherwise you need explicit time dependence in the operator for this to be meaningful . most modern people talk in the path integral , where you have both pictures simultaneously . schrodinger picture is moving the boundary condition forward in time , heisenberg picture is moving the insertion point of a local object forward in time .
they are two different limits in which two different constants are sent to zero and the resulting limiting theory has different names . however , both of them are limits for dimensionful constants and the analogy is perfect . the properly derived $\hbar\to 0$ limit of a quantum mechanical theory is a classical theory – its classical limit – in the very same sense in which the properly derived $k\to 0$ limit of the laws of statistical mechanics produce the laws of thermodynamics . now , the limit $k\to 0$ and $n\to \infty$ really means the same thing because the implicit assumption in all these limiting procedures is that the macroscopic quantities known from the everyday life are kept finite – and essentially fixed . this is especially the case for the energy and temperature . the thermal energy of $n$ atoms is something like $$ e=3ktn/2 $$ if both $e , t$ are fixed , it is clear that the $k\to 0$ limit is exactly the same thing as the $n\to\infty$ limit . the thermodynamic limit simply means that we are neglecting all effects in a fixed-energy system that are caused by the finiteness of the number of atoms – or , equivalently , the finiteness ( nonzero value ) of the contribution of a single atom to the whole ( which is proportional to $k$ ) . one has the freedom to describe the limit in many ways in the quantum mechanical case , too . we may say that the classical limit appears as $\hbar\to 0$ . but we may also say that the classical limit emerges when $n_j\to \infty$ where $n_j=j_z/\hbar$ , for example . when the angular momentum ( or the action $s$ ) is written as a multiple of $\hbar$ , the dimensionless coefficients ' condition $n_j\to\infty$ is the same thing as $\hbar\to 0$ because their product is fixed . classical physics needs to neglect all effects caused by the situation when the overall quantities ' describing the system are so small that they are comparable to $\hbar$ ( which is the regime where the quantum phenomena become important ) . again , we are comparing two things , so saying that one of them is infinitely larger than the other is the same thing as saying that the other one is infinitely smaller . in both situations , one has many options what $n$ or $n_j$ may exactly be . but in both cases , the limit for the dimensionful constant going to zero , whether it is $k$ or $\hbar$ , is equivalent to some dimensionless numbers ' ( those that measure how much the system is larger relatively to the statistical mechanical or quantum " basic blocks " where the more general theory shows in its full glory ) going to infinity . because the ratio of probabilities of an entropy-changing process and its time reversal goes like $\exp ( ( s_b-s_a ) /k ) $ , we see that for fixed $s_a , s_b$ in macroscopic units , the ratio becomes strictly infinite . so in thermodynamics , i.e. the thermodynamic limit of the statistical considerations , a decreasing entropy is strictly impossible . finally , let me mention that the nonrelativistic limit is analogous to the two limits above , too . we may say that the limit involves $c\to\infty$ which is clearly the same thing as $1/c\to 0$: it plays the same role as $k\to 0$ , for example . however , we may also say that the actual velocities are much smaller than $c$ in the limit , so $\beta=v/c\to 0$ or $1/\beta\to \infty$ . that is analogous to $n\to\infty$ or $n_j\to\infty$ above .
if you poured just one liquid its level would be equal on both sides , now when you pour the liquid on one side , the liquid already present in the u tube rises by lets say h1 height . lets assume the height of liquid column of the different liquid on other side is of height h2 . we can find out that h2-h1 would be equal to 5 cm ( difference in level of two liquids ) now to equate pressure on both sides you can use h1 × d1 × g = h2 × d2 × g here d1 and d2 are respective densities , remember the liquid having column of height h2 will be less dense as it stays above other liquid . on solving the equation you can find that h1 = 5 cm and h2 = 10 cm . the separation line would be at the bottom of the lighter liquid column ( the one of height h2 )
toby , i agree that this is really counter intuitive and i was also quite surprised as well when i first saw this very demonstration . i am an undergraduate ta and this is how i explained it in my lab section . i hope this helps . i see two parts to a full explanation : ( 1 ) why is the electric field constant and ( 2 ) why does the potential difference ( or voltage ) increase ? why is the electric field constant as the plates are separated ? the reason why the electric field is a constant is the same reason why an infinite charged plate’s field is a constant . imagine yourself as a point charge looking at the positively charge plate . your field-of-view will enclose a fixed density of field lines . as you move away from the circular plate , your field-of-view increases in size and simultaneously there is also an increase in the number of field lines such that the density of field lines remains constant . that is , the electric field remains constant . however , as you continue moving away your field-of-view will larger than the finite size of the circular plates . that is , the density of field lines decreases and therefore , the electric field decreases as well as the potential field . to show this mathematically , the easiest way to show this for e = constant is using the relation between the electric and potential fields : $$e = -\frac{\delta v}{\delta d} \longrightarrow \delta v =-e \delta d$$ i would expect the voltage to increase linearly as long as the field is constant . when the electric field starts decreasing , the voltage also decreases and the fields behave as finite charged plates . although i’ve only talked about one plate , this idea immediately applies to two plates as well . why does the work increase the electrical potential energy of the plates ? one way to interpret why the voltage increases is to view the electric potential ( not the electrical potential energy ) in a completely different manner . i think of the potential function as representing the “landscape” that the source ( of the field ) sets up . let me explain what the gravitational potential acts like when a ball is thrown upwards ( of course , you know what happens in terms of the force of gravity or in the conservation of energy scenario ) . i claim that the potential function is related to the “gravitational landscape” that the earth sets up , which is derived from the potential energy and is equal to the potential energy per mass : $$ {\delta u = mg\delta y} \longrightarrow \frac{\delta u}{m} = \delta v = g\delta y$$ plotting these functions , the constant gravity field sets up a gravitational potential ramp ( linear behavior ) that looks like in terms of energy , the ball moves up this gravitational ramp were the ball is converting its kinetic energy into potential energy until the ball reaches it maximum height . however , the gravitational ramp exists whether the ball is thrown up or not . that is , gravity sets up a gravitational ramp ( the landscape ) and this is what the ball “sees” before it is thrown up . if we now apply the above thinking to a constant electric field between the parallel plates , the electric potential function is derived in a similar manner : $$ {\delta u = qe\delta r} \longrightarrow \frac{\delta u}{q} = \delta v = e\delta r$$ if we look at the electric potential of the negative plate ( it’s easier than the positive plate ) , it has a negative electrical ramp that starts at 0v . so as your ta pulls the plates apart , the work she does moves the positive plate up the electrical ramp and increases the potential of the positive plate . so this interpretation of the electric potential is what you intuitively already think about in terms of mechanical situations like riding your bike up a hill . there is no difference in the electrical situation .
superfluids can " climb walls " and whatnot . http://en.wikipedia.org/wiki/superfluid superfluids have zero viscosity , but may have surface tension . those with surface tension creep up walls in a capillary-like fashion ( except here , they can creep up a single wall , whereas capillary action requires a tube ) . i do not know about the immiscible liquids , though . what you are saying may be true for certain anisotropic liquids . but i am not sure of this . or maybe weird stuff will happen if you mix immiscible superfluids . ( speculation follows ) they may actually form alternating bands , when one liquid climbs up the wall , the other liquid climbs up the film of the first liquid on the wall . this may happen for two superfluids with different densities and surface tensions .
the probability current is just that - the rate and direction that probability flows past a point . it is analogous to electric current or to a fluid current , and the continuity equation is the same as for those concepts . for example , if the the probability current is high on the left-hand side of a region and low on the right hand side , more probability is flowing in from the left than out from the right , and the total probability for the particle to be found in that region is increasing . to calculate this , the probability that a particle is found in a region is $$\int_{region} \phi^* \phi \ , \ , \mathrm{d}x$$ the time derivative of this is the rate that the probability for the particle to be in that region changes . $$\int_{region} \left ( \frac{\partial \phi^*}{\partial t} \phi + \phi^*\frac{\partial \phi}{\partial t}\right ) \ , \ , \mathrm{d}x$$ we know what the time derivative of $\phi$ is , though , from the schrodinger equation . if you plug that in and assume the potential is real , this simplifies to $$\frac{i \hbar}{2m} \int_{region} \left ( ( \nabla^2\phi^* ) \phi - \phi^* ( \nabla^2 \phi ) \ , \ , \right ) \mathrm{d}x$$ if you integrate this by parts , you see it is the same as the integral of the flux of the probability current over the surface . thus the probability current is a flow of probability the same way the electric current is a flow of charge . the continuity equation is just the differential form of this same relation . since we had to use the schrodinger equation to find $\frac{\partial \phi}{\partial t}$ , we have shown that the continuity equation follows from schrodinger 's equation .
different wave functions with the same $|\psi ( x ) |^2$ represent different physical states ( unless they are proportional ) . different states means that one gets different measurable results on at least one kind of measurements . the same $|\psi ( x ) |^2$ gives the same probability density for position measurements ( only ) , but generally not for measurements of other observables such as momentum . for the momentum probability density , the absolute squares of the fourier transform counts , and this is usually different if only the $|\psi ( x ) |^2$ are the same . the mathematical content of the wave function is the following ( from which the above follows ) : the inner product of $\psi$ with $a\psi$ gives the expectation value of the operator $a$ for a system in state $\psi$ . for example , if you take $a$ to be multiplication by the characteristic function of a region in $r^3$ you get the probability for being in that region . the position operator is simply multiplication by $x$ , while the momentum operator is a multiple of differentiation . for going deeper , try my online book http://lanl.arxiv.org/abs/0810.1019 , written for mathematicians without any background knowledge in physics .
construction of the helicity formula using 3-vector notation the zero component of the pauli lubanski vector $w^0 = \epsilon^{0 ijk}j_{ij}p_k = \epsilon^{ijk}j_{ij}p_k $ the angular momentum genrerators $ j^k = \epsilon^{ijk}j_{ij}$ thus $w^0 = j^k p_k = \vec{j} . \vec{p} $ the orbital angular momentum $ \vec{l} = \vec{x} \times \vec{p}$ is orthogonal to the momentum : $ \vec{l} . \vec{p} = 0$ and since the total angular momentum is the vector sum of the orbirtal and the spin angular momenta $ \vec{j} = \vec{l} + \vec{\sigma}$ thus $w^0 = j^k p_k = \vec{j} . \vec{p} = ( \vec{j-l} ) . \vec{p} = \vec{\sigma} . \vec{p} $ now , since $w^0 = \hat{h} p_0$ and for a massless particle $ p_0 = p$ we obtain : $ \hat{h} = \frac{\vec{\sigma} . \vec{p}}{p_0} = \vec{\sigma} . \hat{p}$ the helicity operator $$\hat{h} = \sigma . \hat{p}$$ where $\sigma$ is the spin operator and $\hat{p}$ is the momentum unit vector is a projection along the axis $\hat{p}$ of a spin operator , thus one might expect it to have for a helicity $\lambda$ the eigenvalues $\lambda$ , $\lambda-1$ , . . . , $-\lambda$ . however , the eigenvectors corresponding to all eigenvalues except $\pm \lambda$ are not physical , because they describe longitudinal polarizations which do not exist in free massless particles . here is an example of the massless spin-1 case ( photon ) . in this case , we may choose the spin operators as : $ \sigma_x = \begin{bmatrix} 0 and 0 and 0\\ 0 and 0 and -i\\ 0 and i and 0 \end{bmatrix}$ $\sigma_y = \begin{bmatrix} 0 and 0 and i\\ 0 and 0 and 0\\ -i and 0 and 0 \end{bmatrix}$ $\sigma_z = \begin{bmatrix} 0 and -i and 0\\ i and 0 and 0\\ 0 and 0 and 0 \end{bmatrix}$ the action of the helicity operator on ( say ) , the electric field in the momentum representation is : $$\hat{h} \vec{e} = i\begin{bmatrix} 0 and -\hat{p}_z and \hat{p}_y\\ \hat{p}_z and 0 and -\hat{p}_x\\ -\hat{p}_x and \hat{p}_x and 0 \end{bmatrix}\begin{bmatrix} e_x\\ e_y\\ e_z \end{bmatrix} = i \hat{p}\times \vec{e}$$ thus : $$\hat{h}^2 \vec{e} = - \hat{p} \times ( \hat{p}\times \vec{e} ) = \vec{e} -\hat{p} ( \hat{p} . \vec{e} ) $$ but , since for a free electromagnetic field : $$\hat{p} . \vec{e} = 0$$ we get : $$\hat{h}^2 = 1$$ , and the only admissible eigenvalues are $\pm 1$
colin 's comment is spot on , but to expand a bit on the " lots of details " he mentioned , heat radiated from the earth 's surface is partially absorbed by greenhouse gases in the troposphere , and because the troposphere is turbulent this heat gets redistributed throughout the troposphere instead of escaping into space . if you e.g. double the co$_2$ content of the troposphere it will intercept and redistribute more of the heat radiated from the earth , so the earth will overall radiate less heat into space . because the earth is now radiating less heat than it receives , it gets hotter . but as it gets hotter more heat is radiated from the surface and more escapes into space . eventually the temperature rises until the heat radiated once again matches the heat received , and the temperature stabilises .
the phenomena that take place is one aspect of the urban heat island effect . the large thermal mass ( aka thermal inertia or heat capacity ) of the urban fabric heats up from solar radiation during the day . once the night-time air temperatures drop below the temperature of the buildings , they give up some of that heat again , heating the urban air . the rate at which this happens , and the rate at which the heat is kept in the city , will depend on wind speeds , the size of the city and the urban layout , amongst other things . so the place where you measure temperature , will affect the pattern you see , depending on how close to a building the measurement is , where in the city it is , and what the wind patterns are .
they are different because in a transmission line we have distributed resistance , capacitance , conductance and inductance ( meaning that each tiny segment of transmission line has its own tiny resistance , capacitance , conductance and inductance ) while in rlc circuits we have lumped resistance , inductance and capacitance . also rlgc does not model a transmission line with the circuit you have shown above but with infinite number of them in series . we know well how to deal with lumped elements and circuits containing them , but dealing with distributed elements and circuits ( e . g transmission lines ) is often much harder and we have to resort to solving maxwell equations directly . so i think not only there is no point in approaching rlc circuits from rlgc model but also it is impractical .
the thermoelectric effect is the direct conversion of temperature differences to electric potential differences ( seebeck effect ) and vice-versa ( peltier effect ) . when considering the electrical currents and heat fluxes involved , there is a size dependency , but such is not the case for the temperature differences and the electric potential differences involved . the proportionality factor between the temperature difference and the electric potential difference is a material dependent constant .
apparently that is possible . from http://en.wikipedia.org/wiki/formula_one_car: indycars , for example , produce downforce equal to their weight ( that is , a downforce:weight ratio of 1:1 ) at 190 km/h ( 118 mph ) , while an f1 car achieves the same at 125 to 130 km/h ( 78 to 81 mph ) , and at 190 km/h ( 118 mph ) the ratio is roughly 2:1 . from http://www.formula1.com/inside_f1/understanding_the_sport/5281.html: a modern formula one car is capable of developing 3.5 g lateral cornering force ( three and a half times its own weight ) thanks to aerodynamic downforce . that means that , theoretically , at high speeds they could drive upside down .
if you see it staying in one spot you can infer that it is moving directly along your line of sight , as you say . but in the more likely event that you see it move across the sky you can determine that it is moving in a given plane only . unless you have some independent way of judging its size or distance you can not tell any more . this actually happens all the time when a bug flies in front of someone 's camera and they think they have seen an interstellar visitor .
the atmospheric pressure at stp is 101325 n/m$^2$ , so 100 times this is 1.01325 $\times$ 10$^7$ n/m$^2$ . you just have to work out the height of a column of water with a 1 m$^2$ base and weighing 1.01325 $\times$ 10$^7 /g$ kg , where $g$ is the acceleration due to gravity . i make it about 1.03 kilometers , though note that it will vary slightly with temperature because the density of water varies with temperature .
yes , but . the quality of the collimated beam may not be fantastic . and it depends on how the light is launched into the fiber . if the incident light has a narrow angular dispersion , and the fiber is short and/or of very good quality with few bends , then the light coming out will have a low angular dispersion . but if the incident light is converging at an angle close to the na of the fiber , then your analysis should be ok .
i would guess there is not much gain compared to solar power generation at the sea level in fair weather , when about 10-15% of solar energy is absorbed in the atmosphere ( http://enso.larc.nasa.gov/arm/pub/journals/valero.etal.jgr00.pdf ) , but there can be significant gain compared to generation at sea level in cloudy weather . my bet would be against any exotic platforms that you describe - power delivery from the platform , even where feasible , would result in heavy losses and high costs , but i may be mistaken .
the momentum eigenstate is not normalizable . suppose $x$ is a periodic variable with period $2\pi r$ -- the periodicity is important as momentum is then a `good ' operator . then one has the allowed values of momenta are quantized i.e. , $p_n = \frac{n \hbar}{ ( r ) }$ with $n\in \mathbb{z}$ . for the values of $p$ mentioned above , the momentum eigenstates are normalizable with $c=1/\sqrt{2\pi r}$ . let us call this normalized state $\langle x|n\rangle$ in the coordinate basis . explicitly , one has $$\langle x|n\rangle = \tfrac1{\sqrt{2\pi r}}\ e^{ip_n x/\hbar}\ . $$ ( you should treat the remark in the ucsd page about one particle to mean that your normalize your state to one . ) the eigenstates are orthogonal to each other as one can check explicitly . $$ \langle n | m\rangle =\delta_{n , m}\ . $$ the next step is to go from the box normalizable states to the delta function normalizable states . one needs to take $r\rightarrow\infty$ and convert kronecker delta into the dirac delta function . one uses the following identification which follows from the properties of the dirac delta function and standard integration : $$ \sum_m = \frac{r}{\hbar}\int dp \quad \mathrm{and}\quad \delta_{m , n} = \frac{\hbar}{r} \delta ( p_n-p_m ) \ . $$ with these identifications , we define $$ |p_n\rangle =\sqrt{\tfrac{r}{\hbar}}\ |n\rangle\ . $$ it is now easy to see that the dirac delta normalization implies that in the $x$-basis , one has $$u_p ( x ) :=\langle x|p_n\rangle = \tfrac1{\sqrt{2\pi \hbar}}\ e^{ip_n x/\hbar}\ . $$ this leads to a simple mnemonic : replace $r$ by $\hbar$ to go from box normalizable states to delta function normalization . of course , momentum which was discrete in a box now takes values in the continuum .
first regarding : is there any appreciation for how the incompleteness theorems might apply to physics ? to put this in perspective , image newton said " oh , looks like my $f = m a$ is pretty much a theory of everything . so now i could know everything about nature if only it were guaranteed that every sufficiently strong consistent formal system is complete . " and then later lagrange : " oh , looks like my $\delta l = 0$ is pretty much a theory of everything . so now i could know everything about nature if only it were guaranteed that every sufficiently strong consistent formal system is complete . " and then later schrödinger : " oh , looks like my $i \hbar \partial_t \psi = h \psi$ is pretty much a theory of everything . so now i could know everything about nature if only it were guaranteed that every sufficiently strong consistent formal system is complete . " and so forth . the point being , that what prevented physicists 300 years ago , 200 years ago , 100 years ago from in principle knowing everything about physics was never any incompleteness theorem , but were always two things : they did not actually have a fundamental theory yet ; they did not even have the mathematics yet to formulate what later was understood to be the more fundamental theory . gödel 's incompleteness theorem is , much like "$e = m c^2$" in the pop culture : people like to allude to it with a vague feeling of deep importance , without really knowing what the impact is . gödel incompletenss is a statement about the relation between metalanguage and " object language " ( it is the metalanguage that allows one to know that a given statement " is true " , after all , even if if cannot be proven in the object language ! ) . to even appreciate this distinction one has to delve a bit deeper into formal logic than i typically see people do who wonder about its relevance to physics . and the above history suggests : it is in any case premature to worry about the fine detail of formal logic as long as the candidate formalization of physics that we actually have is glaringly insufficient , and in particular as long as it seems plausible that in 100 years form now fundamental physics will be phrased in new mathematics compared to which present tools of mathematical physics look as outdated as those from a 100 years back do to us now . just open a theoretical physics textbook from the turn of the 19th to the 20th century to see that with our knowledge about physics it would have been laughable for the people back then to worry about incompleteness . they had to worry about learning linear algebra and differential geometry . and this leads directly to second : has any progress been made on hilbert 's 6th problem for the 20th century ? i had recently been giving some talks which started out with considering this question , see the links on my site at synthetic quantum field theory . one answer is : there has been considerable progress ( see the table right at the beginning of the slides or also in this talk note ) . lots of core aspects of modern physics have a very clean mathematical formulation . for instance gauge theory is firmely captured by differential cohomology and chern-weil theory , local tqft by higher monoidal category theory , and so forth . but two things are remarkable here : first , the maths that formalizes aspects of modern fundamental physics involves the crown jewels of modern mathematics , so something deep might be going on , but , second , these insights remain piecemeal . there is a field of mathematics here , another there . one could get the idea that somehow all this wants to be put together into one coherent formal story , only that maybe the kind of maths used these days is not quite sufficient for doing so . this is a point of view that , more or less implicitly , has driven the life work of william lawvere . he is famous among pure mathematicians as being the founder of categorical logic , of topos theory in formal logic , of structural foundations of mathematics . what is for some weird reason almost unknown , however , is that all this work of his has been inspired by the desire to produce a working formal foundations for physics . ( see on the nlab at william lawvere -- motivation from foundations of physics ) . i think anyone who is genuinely interested in the formal mathematical foundations of phyiscs and questions as to whether a fundamental formalization is possible and , more importantly , whether it can be useful , should try to learn about what lawvere has to say . of course reading lawvere is not easy . ( just like reading a modern lecture on qft would not be easy for a physicist form the 19th century had he been catapulted into our age . . . ) that is how it goes when you dig deeply into foundations , if you are really making progress , then you will not be able to come back and explain it in five minutes on the dicovery channel . ( as in feynman 's : if i could tell you in five minutes what gained me the nobel , then it would not have . ) you might start with the note on the nlab : " higher toposes of laws of motion " for an idea of what lawverian foundations of physics is about . a little later this month i will be giving various talks on this issue of formally founding modern physics ( local lagrangian gauge quantum field theory ) in foundational mathematics in a useful way . the notes for this are titled homotopy-type semantics for quantization .
the orbits of comets are distinguished , as you note , by their eccentricities or equivalently by their orbital energies or orbital velocities . if the comet is approaching or receding from the sun at less than escape velocity , it is on an elliptical trajectory and will end up in a regular elliptic orbit , or just crashing into the sun . elliptical comets , which make up the majority of known comets , are divided into " short period " ( &lt ; 200 yrs ) and " long period " ( > 200 yrs ) . the eccentricity of an elliptical comet is on average less than 1 , but can have instantaneous values > 1 . if the comet is approaching or receding from the sun at escape velocity , it is on a parabolic trajectory . it is in the process of being either captured by or ejected from the sun . as @vince mulhollon indicates in his answer , an exactly parabolic orbit is a fleeting state . if the comet is approaching or receding from the sun at greater than escape velocity , it is on a hyperbolic trajectory and will not be back . there is some dispute as to whether hyperbolic comets originating outside the solar system have truly been observed , and if we really know the periods of certain extremely long-term elliptical comets . as you ask , parabolic comet can " switch over " to being a hyperbolic comet ( and vice versa ) if something adds or removes energy from the orbit . for example , gravitational interaction with another solar system body could add energy to a parabolic orbit , and eject the comet from the solar system along a hyperbolic trajectory . as to how to tell the difference , just take a set of accurate enough position measurements spaced far enough apart , and calculate the orbital elements . software will help .
i can not really answer your question because calculating a comet 's orbit is not something i can describe in a few lines ( actually it is not something i can describe at all , but as always google is your friend ! ) . as martin says , finding your comet is easy in principle . you photograph the same bit of sky on ( at least ) three occasions a few days apart , then you look for anything that has moved i.e. that is in different positions in the three pictures . this sounds easy , but there is a lot of sky and only a few comets so it is incredibly painstaking work . professionals have automated systems for comparing images and looking for moving objects , but amateurs just have to get on with it . anyhow , once you have found your moving object you can use the position in the sky and the change in position to calculate the orbit . martin casually describes this as " a bit of algebra " but actually it is rather a lot of algebra . i found this book online if you really want the gory details . i imagine most amateur astronomers would just feed their measurements into some software rather than do all the sums themselves . i confess i know little about this area but a quick google found several calculators e.g. this one . incidentally , in a previous question you asked about how to tell a hyperbolic from a parabolic ( and elliptical ? ) orbit . there is no easy way to tell except by calculating the orbit to see what shape it is .
a damped harmonic oscillator with a sinusoidal driving force is represented by the equation $$\ddot{x} + \gamma\dot{x} + \omega_0^2x = \frac{f_d \sin ( \omega_d t ) }{m}$$ where $\gamma = b/m$ ( $b$ is the damping coefficient , $b=f/v$ ) and $\omega_0^2 = k/m$ is the resonant frequency of the oscillator . the particular solution to this equation can be determined by taking the imaginary part of the solution to $$\ddot{x} + \gamma\dot{x} + \omega_0^2x = \frac{f_d}{m}e^{i\omega_d t}$$ if you assume* the solution takes the form $$x ( t ) = a e^{i ( \omega_d t + \phi ) }$$ and plug that in , you get $$-a \omega_d^2 + \omega_0^2 a = \frac{f_d}{m}\cos ( \phi ) $$ and $$\gamma\omega_d a = \frac{f_d}{m}\sin ( \phi ) $$ solving for the phase difference gives $$\tan\phi = \frac{\gamma\omega_d}{\omega_0^2 - \omega_d^2}$$ this depends on the frequency of the driving force and the resonant frequency of the oscillator , but not on the amplitude of the driving force . you can express this in terms of the dimensionless variable $x = \omega_d / \omega_0$ as $$\tan\phi = \frac{\gamma}{\omega_0}\frac{x}{1 - x^2}$$ and if you graph it , ( graph generated by wolfram alpha ) you will see how the response of the oscillator jumps from leading to lagging when $\omega_d = \omega_0$ ( at $x=1$ ) , that is , when the driving and resonant frequencies are equal . *the same solution can be obtained from fourier decomposition without making this assumption .
it does . to convince yourself , remember that rising hot air does experience a coriolis force , so i am quite sure that your bubble does too . also , think of what the coriolis acceleration is - it is an apparent acceleration due to the fact that you , the observer , are in a rotating reference frame , so your definition of " straight up " is actually a curve . when the bubble goes up you see it as having a curved trajectory - but this has to do with your rotating definition of straight up . therefore this acceleration applies to it .
at the risk of telling you how to " suck eggs " ( your level in these things is not altogether clear ) , here goes . ingredients : the essential ingredients to this explanation are : a physical " system " which evolves in and whose " events " happen in some space $\mathcal{u}$ ( ordinary euclidean 3-space or minkowsky spacetime , for example ) ; in physics this space is always a linear ( wontedly it is minkowsky spacetime ) space wherein stuff happens : let 's call $\mathcal{u}$ the " scene " of where stuff we want to talk about happens ; a connected lie group $\mathfrak{g}$ which represents the co-ordinate transformations the a system can undergo : in physics these are all linear transformations $\mathcal{u}\to\mathcal{u}$ of the scene $\mathcal{u}$ . wontedly in physics , $\mathfrak{g} = so^+ ( 1,3 ) $ ( the identity connected component of the lorentz group comprising all rotations and boosts of " physical space " , sometimes called the " proper , orthochronous lorentz group " ( proper = unimodular determinant =1 , i.e. " does not invert space " and orthochronous = does not switch the time direction ) or the poincaré group ; a cover of $\mathfrak{g}$ ; this is almost always ( i have never seen it not so ) the universal cover $\tilde{\mathfrak{g}}$ of $\mathfrak{g}$ as explained in my article " lie group homotopy and global topology " on my website here ; a vector space $\mathcal{v}$ which can be , for example , a quantum state space , possibly infinite dimensional and its group $gl ( \mathcal{v} ) $ of linear endomorphisms , i.e. bijetive , linear maps $\phi:\mathcal{v}\to\mathcal{v}$ . informally , $gl ( \mathcal{v} ) $ is the group of invertible matrices acting on $\mathcal{v}$ . most importantly : this space is different from the physical " scene " $\mathcal{u}$ . the scene is $\mathcal{u}$ spacetime all around us , the state space $\mathcal{v}$ is a hilbert space of quantum states . and , actually , although we talk about a " linear " state space $\mathcal{v}$ , we are a bit sloppy : sure , all quantum states are linear superpositions of the basis for $\mathcal{v}$ but they are always of unit magnitude : the probabilities of a measurement 's " collapsing " the state into one of the basis vectors must all sum up to one - " we have to end up in some state " . so , if we are being precise , we take heed that we are actually talking about the unit sphere within $\mathcal{v}$ as the state of quantum states . this state space is very different in character from spacetime , where there is no obligation for 4-positions of events to be unit magnitude ; representations $\rho : \mathfrak{g}\to gl ( \mathcal{v} ) $ , $\tilde{\rho}:\tilde{\mathfrak{g}}\to gl ( \mathcal{v} ) $ of both $\mathfrak{g}$ and its cover $\tilde{\mathfrak{g}}$ , respectively . recall that a representation of a lie group $\mathfrak{g}$ is a homomorphism from from $\mathfrak{g}$ to $gl ( \mathcal{v} ) $ , i.e. a transformation that " respects the group product " so that , given $\gamma , \ , \zeta\in\mathfrak{g}$ , we have $\rho ( \gamma\ , \zeta ) =\rho ( \gamma ) \ , \rho ( \zeta ) $ . and , as discussed above , the linear transformations of the form $\rho ( \gamma ) , \ , \tilde{\rho} ( \tilde{\gamma} ) \in gl ( \mathcal{v} ) $ for $\gamma \in\mathfrak{g}$ and $\tilde{\gamma} \in\tilde{\mathfrak{g}}$ must be unitary so that the transformed quantum states stay normalised . so we can see that $gl ( \mathcal{v} ) $ is very different from $\mathfrak{g}$ or $\tilde{\mathfrak{g}}$: lorentz boosts most assuredly are not unitary ! we say that $\mathfrak{g}$ or $\tilde{\mathfrak{g}}$ " act on the state space $\mathcal{v}$ through the respresentations $\rho$ , $\tilde{\rho}$" . i am using here more of the mathematician 's description of a representation , because here ( i am not always so fussed ) i believe it is clearer than the physicists because we need to take heed that there are two different classes of representations in our discussion : those whereby the group of co-ordinate transformations $\mathcal{g}$ act on the state space $\mathcal{v}$ and those whereby its cover $\tilde{\mathcal{g}}$ acts on $\mathcal{v}$ . baking instructions : wigner 's theorem and why covers are interesting why are we interested in covers at all ? after all , elements of $\tilde{g}$ are not the " physical " co-ordinate transformation . this is where we meet our baking oven for our ingedients : wigner 's theorem . clearly , when our scene $\mathcal{u}$ undergoes a co-ordinate transformation , then the transformations wrought on the quantum state has to preserve inner products in the quantum state space so that the state stays properly normalised . from this assumption alone , i.e. one does not have to assume linearity , wigner proved that the when the scene $\mathcal{u}$ undergoes a " symmetry " ( a lorentz transformation ) , the state space must undergo a " projective homomorphism " $\sigma$ , i.e. if $\gamma , \ , \zeta$ are two lorentz transformations , then the state space transformation corresponding to their product is : $$\sigma ( \gamma\ , \zeta ) = \pm \sigma ( \gamma ) \ , \sigma ( \zeta ) \tag{1}$$ the fact that we do not get exactly a homomorphism is why we are interested in covers : the image of the representation $\tilde{\rho} ( \tilde{\mathfrak{g}} ) $ ( recall that this is a group of unitary operators in $gl ( \mathcal{v} ) $ acting on the state space ) of the cover $\tilde{\mathfrak{g}}$ contains both the transformations that fulfill the genuine $+$-sign homomorphism in ( 1 ) ( which are simply unitary operators in the image $\rho ( \mathfrak{g} ) $ of the co-ordinate transformation group $\mathfrak{g}$ ) and those that flip the sign . so if we allow representations of the cover , we get every possible unitary transformation ( even without an assumption of linearity - this automatically follows ) that can be wrought on the state space $\mathcal{v}$ when the scene $\mathcal{u}$ is transformed . here 's the punchline . quantum states that transform by the transformations belonging to the image $\rho ( \mathfrak{g} ) $ of $\mathfrak{g}$ under the genuine homomorphism $\rho$ are called vectors . quantum states that transform by the transformations belonging to the image $\tilde{\rho} ( \tilde{\mathfrak{g}} ) $ of the cover $\tilde{\mathfrak{g}}$ under the " projective homomorphism " $\tilde{\rho}$ are called spinors . the above can be intuitively thought of as follows : in quantum mechanics , a global phase $e^{i\phi}$ multiplying a system 's quantum state does not affect measurements we make on the system . so quantum systems " do not care whether a homomorphism is genuine or projective " . the universal ( only ) cover of the lorentz group $so ( 1,3 ) $ is the group $sl ( 2 , \ , \mathbb{c} ) $ . so " spinors " transform by a representation of $sl ( 2 , \ , \mathbb{c} ) $ . vectors transform by a representation of $so ( 1,3 ) $ . the word " spinor " can be pretty vague in my experience : it can refer to the transformation in $sl ( 2 , \ , \mathbb{c} ) $ rather than the quantum state that is transformed by it , and people often speak of the unit quaternions as " spinors": roger penrose 's " road to reality " chapter 11 simply defines a spinor as something that takes a negative sign when rotated through $2\ , \pi$ and comes back to its beginning point after a rotation through $4\ , \pi$ . this is actually a pretty good definition , for that is exactly how elements of a representation of $sl ( 2 , \ , \mathbb{c} ) $ act on the state space $\mathcal{v}$ , and is the essential difference between how elements of a representation of $so ( 1,3 ) $ act on state spaces . forget about " quantities with direction " as a vector 's definition : in physics the word " vector " always talks about how something transforms when our scene $\mathcal{u}$ undergoes a symmetry . remember this is pretty near to the word 's literal meaning vehor ( transliterated as vector ) literally means " i am borne " or " i am carried " in latin , so it is all about how the " vector " is borne , either by a transformation in physics or as a pathogen in biology ( the word 's original english meaning ) .
i have to think david will agree , on reflection , that his answer has failed to capture the essence of entanglement . any stream of particles , if not specially prepared , will measure +h/2 or -h/2 at detector a ; they will do with respect to the x axis , or the y axis , or any axis . exactly the same is true at detector b . how can this very ordinary circumstance illustrate the mystery of entanglement ? but that seems to be what david has said : that if you prepare the particles in the entangled state , you get this " strange result " . i see nothing strange about it since it seems to be exactly the same result if you set up two detectors far apart and measured streams of particles that were totally random . i am going to suggest that the mystery of entanglement lies in the perfect correlation ( or anti-correlation ) that you get when you set up both detectors along the x axis . some people think there is nothing mysterious about this because it is exactly what you would expect if the two particles were created with equal and opposite spins . these people are very wrong . the reason they are wrong is that the experiment works the same no matter how you align the detectors with respect to the source of the particles . we can imagine an experiment where particles are created with opposite spins , but assuming the spin axis is random at the moment of creation , there is no way a pair of detectors should show 100% correlation no matter what angle you set it to . in fact , in the case of entanglement , there is 100% correlation regardless of the orientation . that is a real problem and it is really the only problem . edit : i explain this issue in more detail in my blog article , " entanglement and the crossed polarizers " .
in classical mechanics , you can make up a complicated system with many different natural frequencies . in general , these frequencies are completely independent of each other . due to non-linearities in the coupling forces , it may happen that when two modes are vibrating simultaneously , you get a new frequency appearing in the spectrum as the sum or difference or the two primary modes . but in quantum mechanics , you never see the primary modes at all . . . you only see the sum or difference frequencies . furthermore , if you try to explain them by non-linear forces , you should also expect to see multiples of the fundamental frequencies . these are absent in , for example , the spectra of atoms . it is hard to explain by a classical model involving things like masses and springs . it manifests itself in qm , of course , because the " fundamental " frequencies , the natural modes , evolve in time without any oscillating charges associated with them . the oscillating charges only appear when you have the superposition of two fundamental modes . this is how quantum mechanics is very different from classical .
you can not argue with a black object and a white object alone , as i think you partially understand in trying to build your thought experiment . you need a little bit more to define things properly . see whether the following helps . imagine a black object at a temperature $t_0$ and a white object also at $t_0$ inside a perfectly isolating box full of blackbody radiation at some higher temperature $t_1&gt ; t_0$ ( i.e. without the black and white objects , this radiation is in thermodynamic equilibrium ) . to understand exactly what would happen , you would have to describe the " colour " of your objects with emissivity curves that show emissivity as a detailed function of frequency . so your " black " and " white " would need to be defined in much more detail . you would also have to define the surface areas of the two objects and what they are made of ( i.e. define their heat capacities ) . but all of this only effects the dynamics of how the system reaches its final state , i.e. these details only influence how the system evolves . what it evolves to is the same no matter what the details : the box would end up with everything at the same temperature such that the total system energy is , naturally , what it was at the beginning of the thought experiment . " blacker " as opposed to " whiter in this context roughly means " able to interact , per unit surface area , with radiation more swiftly": the blacker object 's temperature will converge to that of the radiation more swiftly than does that of the whiter object , but asymptotically the white object " catches up " . blacker objects absorb more of their incident radiation its true , but they also emit more powerfully than a whiter object at the same temperature . the one concept emissivity describes the transfer in both directions . think of emissivity as being a fractional factor applied to the stefan-boltzmann constant for the surface as well as being the fraction of incident light absorbed by the surface relative to a perfect blackbody radiator . this description is altogether analogous to that of the situation where $t_0&lt ; t_1$ . begin with $t_0=t_1$ , and you have got thermodynamic equilibrium from the beginning . nothing happens , of course . maybe the following will help thinking about what is a really quite a complex question : it would be a fantastic last question for an undergrad thermodynamics exam btw : you can abstract detail away by saying lets define object $a$ to be blacker than object $b$ if , when both objects are made of the same material , are the same size and shape , the temperature of $a$ converges to the final thermodynamic equilibrium temperature more swiftly than that of $b$ when they are both compared in the box-radiation-object thought experiment above . thinking about this now , i am not sure whether the above definition would hold for every beginning temperature of the radiation . maybe there are pairs of surfaces whose relative blackness is different at different beginning temperatures such that $a$ is blacker than $b$ with some beginning temperature whilst the order swaps at a different beginning temperature . i think it is unlikely , but that is probably a different question altogether . by the way , which pub do you drink in ? i might come along . afterword on a heater 's colour : you ask by implication what is the best colour to paint a heater . this is not a simple question and involves the dynamics of the heater system . it is really an engineering question . i suspect in general it is better for them to be blacker rather than whiter . here 's a glimpse of the kind of factors bearing on the situation . if you can say a heater has a constant nett input of $p$ watts , then at steady state that is going to be its output to the room , altogether regardless of its colour . there may be a materials engineering implication here : if you paint the heater whiter , and if its dominant heat transfer to the room is by radiation ( rather than by convection or conduction ) , then it has to raise itself to a higher temperature than it would were it blacker so as to radiate $p$ watts into the room . so its materials might not be as longlasting , and it might be more of a fire hazard than it would be were it blacker . if the heater is the hot water kind , and again if radiative transfer is significant , then the heating system has to run hotter to output power at a given level if the heater is whiter . at a given flow rate and given temperature of heating water , the heat output of heater is lower if it is whiter . you are trying to design the heater to be an " anti-insulator": you want the heat to leak out of the flow circuit in at the heater , not through the lagging on the hot water pipes outside the building channelling the water from the boiler to the heaters . if the hot water pipes leak heat in the same room , then that is no problem . recall the quartic dependence of the stefan boltzmann law . at room temperatures with a low temperature heater ( the hot water kind ) $\sigma\ , t^4$ is likely to be pretty small compared with other heat transfer mechanisms , in contrast to my idealised scenarios above . so the heater 's colour is likely to be pretty irrelevant .
with a delta function potential , the particle is free on either side of the barrier : $$ \psi ( x ) =\begin{cases}\psi_l ( x ) =a_re^{ikx}+a_le^{-ikx} \\ \psi_r ( x ) =b_re^{ikx}+b_le^{-ikx}\end{cases} $$ where $a_i , \ , b_i$ are constants such that $a_r+a_l=b_r+b_l$ ( i.e. . , $\psi ( x ) $ satisfies the continuous function condition ) . but at the barrier we have the issue that $v ( 0 ) =\infty$ . so to resolve this issue , we use schroedinger 's equation and integrate it over some small region $\left [ -\epsilon , \ , \epsilon\right ] $ and then let $\epsilon\to0$: $$ -\frac{\hbar^2}{2m}\int_{-\epsilon}^\epsilon\psi''\ , dx+\int_{-\epsilon}^\epsilon v\psi\ , dx=e\int_{-\epsilon}^\epsilon\psi\ , dx $$ the first term is clearly $d\psi/dx$ evaluated at two points . the last term goes to zero in the limit $\epsilon\to0$ ( recall that $e$ is constant and finite , so that as $\epsilon\to0$ , the width goes to 0 and so does the whole value ) . for the potential term , the delta function has the great property that $$ \int\delta ( x-a ) f ( x ) \ , dx=f ( a ) $$ thus , that middle term becomes $\left . \psi ( x ) \right|_{-\epsilon}^\epsilon$ . we then combine these two to get $$ -\frac{\hbar^2}{2m}\left [ \psi'\left ( +\epsilon\right ) -\psi' ( -\epsilon ) \right ] +\left . \lambda\psi ( x ) \right|_{-\epsilon}^{\epsilon}=0 $$ as $\epsilon\to0$ , we can get the relation you are confused over : $$ \psi'_r ( 0 ) -\psi'_l ( 0 ) =+k\psi ( 0 ) $$
the fresnel equations describes the portion of the electromagnetic field that is reflected at a surface . for any indefinite and non-engineered material , in order to have refraction you must have an index greater than 1 , and thus you must have reflection at the surface . there are two significant exceptions to this . first , if you create a slab of material such that the reflected wave off of the second boundary is both perfectly out of phase with the wave reflected off of the first surface and of equal amplitude , the reflected waves will destructively interfere causing no reflection , but transmission will still occur . this will lead to refraction but no perceived reflection . secondly , if you were to engineer an isotropic metamaterial such that it had an index of -1 , the surface will have no index mismatch so there will be no reflected wave , but " refraction " will still occur . this is only a theoretical condition , as no isotropic negative index metamaterial has been fabricated . for a brief overview of metamaterials see here .
concerning ( 1 ) , i strongly suggest you b . o'neill 's textbook about semi-riemannian geometry . it is written for mathematicians interested in gr without any particular background in physics . the last chapters focus on the causal structure of spacetime and singularity theorems . there is a large part completely devoted to analyze schwarzschild 's metric and kruskal 's manifold . ( 2 ) i guess that for " lorentzian manifold " you actually means minkowski spacetime . it seems that you are mostly interested in general realtivity , so what you should learn is the general theory of semi-riemannian manifolds . ( 3 ) - ( 4 ) - ( 5 ) schwarzschild metric is the metric of a very important semi-riemannian manifolds describing the spacetime in the presence of spherical symmetry . that metric is indeed a solution of einstein field equations . though the maximal extension of schwarzschild manifold contains a metrical singularity , the singularities considered in hawking-penrse 's theorems are of more general nature . there are several statements of the singularity theorems . however , all these statements say that : under some physically sensible hypotheses on : ( a ) the global spacetime geometry ( e . g . global hyperbolicity ) , ( b ) its energy-momentum content ( e . g . , validity of some energy condition ) , and ( c ) assuming that the gravity is somewhere strong enough ( typically , existence of trapped surfaces ) , then there exists a maximally extended causal geodesic that is incomplete .
energy is associated with work not with force as work energy theorem states . in other words , force can be exerted without generating any work , in such a case whatever exerted the force does not lose or gain any energy because no work was done . in your example , the rubber slope exerted force ( friction force ) but no work was done ( the metal block did not move ) . so there is no exchange of energy although force is exerted . think of the force in this case as energy-free action . the energy of the slope has nothing to do with the gravitational potential energy of the metal block
i suggest to see the scenario physically first , and proceed from there . here , the gravitational force does not change the distance from the center , but it provides the centripetal force for the circular motion ( or any conic section for that matter , based on initial values ) . assuming it is a circular motion for now , we can just get right into the mathematical description of the motion . for such systems , masses move in a circle with their com at the center . because there is no other force in system , the com does not accelerate . it is free to move at a constant velocity though , but this is ( usually ) irrelevant when analyzing the system . because they move in a circle about the com , just get the distances from their com , and equate the gravitational force to the centripetal force . this way you have fully described the motion . example for $m_2:$ $$ \dfrac{gm_1}{\underbrace{r}_{\text{distance between bodies}}} = \dfrac{{v_2^2}}{\underbrace{r_{2_{cm}}}_{\text{distance of 2 from com}}} $$ similar for $m_1$ .
i take this question to mean : why does the laguerre-gaussian ( lg ) modes have an $e^{i\ell\phi}$ dependance on the azimuthal coordinate $\phi$ ? why is $\ell$ required to be an integer ? question 1 the lg modes are solutions to the paraxial wave equation in cylindrical coordinates . this means that we get solutions that reflect this symmetry . in particular the solutions should only trivially change if you make the change $$\phi\to\phi+\delta\phi . $$ if we define a rotation operator $r_{\delta\phi}$ such that this operator acting on any function $f ( \phi ) $ gives $$r_{\delta\phi}f ( \phi ) \equiv f ( \phi+\delta\phi ) $$ cylindrical symmetric solutions will be the eigenfunctions of $r_{\delta\phi}$ , i.e. $$r_{\delta\phi}f ( \phi ) =\lambda f ( \phi ) , $$ where $\lambda$ is a constant . the solution to this equation is of the form $$f_\ell ( \phi ) \sim e^{i\ell\phi} , $$ i.e. $$r_{\delta\phi}f_\ell ( \phi ) =e^{i\ell\delta\phi}e^{i\ell\phi}=\lambda_\ell e^{i\ell\phi} . $$ therefore cylindrically symmetric solutions such as the lg modes will be of the from $$lg ( r , \phi ) = f ( r ) e^{i\ell\phi} . $$ question 2 the reason $\ell$ has to be an integer ( i.e. . quantized ) is because $\phi$ is periodic . what this means is that $\phi$ and $\phi+2\pi$ are the exact same point , therefore all functions of $\phi$ must meet the requirement $$f ( \phi+2\pi ) =f ( \phi ) . $$ if our function is $e^{i\ell\phi}$ , as we saw in part 1 , then this means $$e^{i\ell ( \phi+2\pi ) }=e^{i\ell\phi}\to e^{i\ell 2\pi} =1 , $$ which is only true if $\ell$ is an integer .
to be honest , i just learned about all this myself in the last months , so i am not sure whether this is actually correct . since you have this spherical symmetry , i think that you need spherical harmonics . they are orthogonal functions , think of them as a fourier series on the surface of a sphere . your charge density $\sigma$ does not depend on $\phi$ , therefore we can use the simpler legendre-polynomials , where $m = 0$ . fist , we want to express the potential like so : $$ \varphi ( \vec x ) = \frac1{\varepsilon_0} \sum_{l=0}^\infty \frac{1}{2l+1} \varrho_{l} \frac{1}{r^{l+1}} y_{l , 0} ( \theta , \phi ) $$ the coefficients $\varrho_l$ are given by : $$ \varrho_l = \int \mathrm d^3 x'\ , r'^l \varrho ( \vec x ) y^*_{l , 0} ( \theta ' , \phi' ) $$ now with $$ y^*_{l , 0} ( \theta ' , \phi' ) = \sqrt{\frac{2l+1}{4\pi}} p_l ( \cos \theta' ) $$ and $$ p_0 ( x ) = 1 , \quad p_1 ( x ) = x $$ we can calculate the coefficitents . but first , we need to convert the surface charge density $\sigma$ into a volume charge density . for that , we use the $\delta$-distribution : $$ \varrho ( \vec x ) = \delta ( r-r ) \sigma_0 \cos ( \theta ) $$ if you plug those into the $\varrho_l$ , you will get $\varrho_0 = 0$ and $\varrho_1 = \sqrt{\frac{3}{4 \pi}} \frac{4}{3} \pi r^3 \sigma_0$ . i hope this is correct . then we can put this into the first formula and get $\varphi$: $$ \varphi ( \vec x ) = \frac{1}{\varepsilon_0} \sqrt{\frac{3}{4 \pi}} \frac{4}{3} \pi r^3 \frac{1}{r^2} \sigma_0 \cos \theta $$ since this is a pure dipol potential , the $1/r^2$ seems about right . and if you look at the dimensions , the $r^3 \sigma_0 / r^2$ have just the needed charge/length . spherical harmonics might be overkill , maybe there is a simpler method to do this .
something i posted on reddit answers this question quite well , i think : " rational " and " irrational " are properties of numbers . quantities with units are not numbers , so they are neither rational nor irrational . a quantity with units is the product of a number and something else ( the unit ) that is not a number . by choosing the unit you use to express a quantity , you can arrange for the numeric part of the quantity to be pretty much any number you want ( though switching units will not let you change its sign or direction ) . in particular , it can be rational or irrational . and choices of units are a human convention , so it would not make any sense to extend the idea of rationality or irrationality to the quantity itself . you can use a natural unit system , where certain physical quantities are represented by pure numbers . for example , if you use the same units to measure time and space , $c = 1$ . in such a unit system , it does make sense to say the speed of light is rational , but that is kind of a special case . that reasoning does not really work with other physical quantities . and you really do have to be using natural units . ( technically , you could make a natural unit system where $c = \pi$ , but it would have very complicated and perhaps even inconsistent behavior under lorentz transforms , so nobody does that . ) by the way , empirical measurements always have some uncertainty associated with them , so they are not really numbers either and are also neither rational nor irrational . a measurement is probably better thought of as a range ( or better yet , a probability distribution ) which will necessarily include both rational and irrational numbers .
potentials are curious in that there is not exactly one way to define them . because a potential $\phi$ is related to a field $\mathbf{e}$ by \begin{equation} \mathbf{e}=\nabla \phi \end{equation} the class of potentials that satisfy the field equation is infinite . if a potential $\phi$ satisfies the field equation , then so will any potential $\phi'$ , where \begin{equation} \phi'=\phi+\lambda \end{equation} where $\lambda$ is any function that satisfies $\nabla \lambda=0$ . the simplest example is for $\lambda$ to be some constant value , but in general , an even larger class of functions works . by taking a difference of potentials , there is no ambiguity . if the potential at point $a$ and the potential at point $b$ are defined in terms of $\phi$ , their potential difference is \begin{equation} \delta \phi = \phi ( b ) -\phi ( a ) . \end{equation} doing this same exercise in terms of $\phi'$ gives \begin{equation} \delta \phi ' = \phi' ( b ) -\phi' ( a ) =\left ( \phi ( b ) +\lambda \right ) -\left ( \phi ( a ) +\lambda \right ) = \phi ( b ) -\phi ( a ) =\delta \phi . \end{equation} so while potentials are potentially ambiguous , differences in potentials are not . by convention , physicists frequently assign a potential of "$0$" to a point at infinity . this fixes the value of $\lambda$ and removes the ambiguity in the definition of potential . this is only convention , however , and there are certain problems that this approach cannot resolve . by defining the potential in terms of differences , your author avoided the issue entirely .
if you turn a glass upside down and put it in a sink full of water you will find the air stays in the glass and stops the water filling the glass . this is exactly how the undersea lab works . the lab is sealed apart from the opening in the bottom , so the air inside it can not escape , and the trapped air keeps the water out . the air has to be at the same pressure as the water ( otherwise the water pressure would compress it and allow some water in ) so the pressure is higher than on the surface . that is why the chaps in the lab have to open a valve on the barrels containing the towels before they can open them . you have to be very sure there are no leaks in the undersea lab . any leak would allow the air in the lab to leak out and bubble to the surface , and sea water would rush in to replace the leaked air .
the bicep2 paper reports a tensor/scalar ratio r=0.20+0.07−0.05 , but then says : this is the value taken before corrections . there exist contributions to the b- mode due to changes in the photon polarization of the cmb while it is traveling before reaching the detector . the dust is the interstellar dust that has to be modeled . subtracting the various dust models and re-deriving the r constraint still results in high significance of detection . for the model which is perhaps the most likely to be close to reality ( ddm2 cross ) the maximum likelihood value shifts to r=0.16+0.06−0.05 which value is proper to compare with planck 's r&lt ; 0.11 ? assuming that planck has corrected for the dust before giving its limit , ( a reasonable assumption since the existence of dust is known ) it is the second value you have to compare with , which is different only by 1 sigma from the bound given by planck , i.e. is consistent . at these errors one does not call measurements definitive . once many standard deviations separate old from new measurements , old ones can be ignored .
i think it is pretty obviously in your last line . so what are the results of the commutator $ [ p_r , p_\theta ] $ and $ [ p_\theta , p_\theta ] $ ? note that $p_\theta$ is not commute with $\theta$ . then you should get the answer .
no , it would not violate the principle of special relativity , which proscribes information traveling faster than the speed of light . let 's call the farthest object we can see x . let 's call x 's neighbor on the far side from us y . the only way for us to get information about y is for information to travel to x ( at or below the speed of light ) , which would then influence x 's behavior . then , information about x 's behavior would have to travel to us ( at or below the speed of light ) . so , nowhere is information traveling faster than light . the only information we can get from x about y is already " out of date"* enough that special relativity is not violated . *the technical term relativists use is " retarded , " but be careful how you use it in everyday conversation . edit : we can make a distinction between the theoretically observable universe according to relativistic considerations , which is what i was talking about above , and the universe observable with present technology and practical limitations . getting sneaky like inferring about y from x actually does increase the latter meaning of observable universe .
there is an excellent talk by lawrence krauss on precisely this subject . i can not recommend watching it highly enough , you should start watching it before even reading the remainder of this post . in summary , we can model the matter just after the big bang at the time we see the cosmic microwave background and determine the characteristic distance scales of the " lumpiness " of the universe at that point . we can view the lumpiness of the universe then by observing the cosmic microwave background radiation at high resolution . now we have something that we can compare the expected visual size of to the apparent visual size , giving us information about the shape of the universe in between .
cms has a preprint out where they are searching for compositeness in dijet angular distributions . the measured dijet angular distributions can be used to set limits on quark compositeness represented by a four-fermion contact interaction term in addition to the qcd lagrangian . they set limits . i will guess that angular distributions of two lepton events will be in the search of lepton compositeness . considering that the compositeness of nuclei and compositeness of nucleons were cleanly found by deep inelastic scattering , i would be very doubtful of interpretations using levels of monte carlo calculations that would give such a drastic conclusion to deviations from qcd . one would have to wait for lepton colliders . from lhc i would need two leptons at a vertex to get the other end of deep inelastic scattering . there is nothing that can beat form factors , imo .
i am no expert on aerodynamics but according to wikipedia a sears–haack body has the lowest theoretical wave drag , where wave drag occurs when moving at transonic and supersonic speeds due to the presence of shock waves . however you are saying that your object would not move faster than $20 ms^{-1}$ , which is far below the speed of sound ( $mach 1\approx 340 ms^{-1}$ ) assuming this would be in earths atmosphere near sea-level . and according to another wikipedia page about nose cone designs : for aircraft and rockets , below mach . 8 , the nose pressure drag is essentially zero for all shapes . the major significant factor is friction drag , which is largely dependent upon the wetted area , the surface smoothness of that area , and the presence of any discontinuities in the shape . for example , in strictly subsonic rockets a short , blunt , smooth elliptical shape is usually best . so elliptical cone would be a good choice , however it is not given which length would give the lowest drag . i would suspect that the length would be larger than the radius , because the drag coefficient ( for $f_d\propto v^2$ ) for a half-sphere ( elliptical cone with length equal to the radius ) is higher that a elliptical cone with a slightly longer length . edit : after a bit of searching i did found some experimental results which are in the region of your maximum speed ( $39.2 mph\approx 17.4 ms^{-1}$ ) . the results from this experiment showed that a long elliptical nose cone had the least drag , which had a length to diameter ratio of approximately 3 . the experiment also contained a shorter elliptical nose cone with a ratio of approximately 1.4 . i would not be able to say that a ratio of 3 would be optimal , but it should at least be bigger than 1.4 .
refs . 1 and 2 define a canonical transformation ( ct ) $$\tag{1} ( q^i , p_i ) ~\longrightarrow~ ( q^i , p_i ) $$ [ together with choices of hamiltonian $h ( q , p , t ) $ and kamiltonian $k ( q , p , t ) $ ] as satisfying $$ \tag{2} ( p_i\mathrm{d}q^i-h\mathrm{d}t ) - ( p_i\mathrm{d}q^i -k\mathrm{d}t ) ~=~\mathrm{d}f$$ for some generating function $f$ . on the other hand , wikipedia ( march 2014 ) calls a transformation ( 1 ) a canonical transformation ( ct ) if it transforms the hamilton 's eqs . into kamilton 's eqs . a ct ( 2 ) according to the definition of refs . 1 and 2 is a ct according to the definition of wikipedia but not necessarily vice-versa , cf . e.g. this phys . se post . considering op 's second question , it seems that what op is really after is not the notion of a ct per se , but rather the notion of a symplectomorphism $^1$ $f:m\to m$ on a symplectic manifold $ ( m , \omega ) $ , i.e. $$\tag{3} f^{\ast}\omega=\omega . $$ here $\omega$ is the symplectic two-form , which in local darboux/canonical coordinates reads $\omega= \mathrm{d}p_i\wedge \mathrm{d}q^i$ . it is straightforward to see that a symplectomorphism $f:m\to m$ preserves the canonical volume form $$\tag{4} \omega~:=~\frac{1}{n ! }\omega^{\wedge n}$$ in phase space $ ( m , \omega ) $ , i.e. $$\tag{5} f^{\ast}\omega=\omega . $$ references : h . goldstein , classical mechanics , chapter 9 . see text under eq . ( 9.11 ) . l.d. landau and e.m. lifshitz , mechanics , $\s45$ . see text between eqs . ( 45.5-6 ) . -- $^1$ a ct ( 2 ) according to the definition of refs . 1 and 2 is locally a symplectomorphism ( by forgetting about $h$ and $k$ ) .
i would like to know how the string deforms over time as it is pulled from that point ( orange ) . does the end closest to the pulling force move first and straighten up , wrapping around the object ' a ' early on , or does the whole string change shape continuously ? for a better understanding , take a look at this figure ( i have modified the diagram in order to explain things in a better way ) when the string is pulled , the coil of wire ( initially slack ) as shown in region $b$ begins to uncoil . what you will observe is that the portion of the wire in region $a$ begins to move only after the coil of wire in region $b$ gets completely uncoiled ( it remains slack until then ) . try it out with a thread or a chain . this is to show you that the portion of the wire close to the pulling force will move first followed by the other portions of the wire .
an atom in isolation offers a potential well , and electrons form bound states in the well . the energy of those bound states can be calculated exactly in the case of a single-electron ( hydrogen-like ) atoms or by variational computational methods for more complicated cases . now when you put several atoms together in a tight and regular array , they offer a combined well resulting from the sum of all their potentials . that combined well might look ( in cartoon form ) something like this : here the red lines represent electron energy level that are still contained by the locally stronger effect of their " own " nucleus , but the blue line represents those slightly higher energy levels that see the combined potential as a single large energy well ( with some high-frequency detailed structure ) . in a conductor , non-conduction electrons fill all the red ( local ) energy levels and the remaining electrons must ( because of pauli exclusion ) then occupy the blue ( non-localized ) conduction levels .
all points in the observable universe are " connected " in the sense that they can be acted upon by forces that have an infinite range ( gravity and electromagnetism ) . however , points that are outside of our cosmological horizon ( due to the expansion of the universe ) are no longer causally connected with points in our local vicinity , since they are receding from us faster than light . the same is true of points that are inside the event horizon of a black hole .
in linear electrodynamics ( i.e. . low intensities ) , the dielectric constant and refractive index remain unchanged . a different thing is nonlinear optics ( applying to ed in general as well ) , for more theory see e.g. here . but this happens only for materials with strong non-linear parameters ( usually $\chi^{ ( 2 ) }$ or $\chi^{ ( 3 ) }$ ) and for rather high field intensities . this usually yields effect such as the second harmonic generation , but there is also the effect of self-focusing among others , which stems from the dependence of the refractive index on the field amplitude .
these terms apply when you are solving the schrodinger equation with a potential that goes to zero at large distances . in this situation , the solutions with $e&lt ; 0$ have the property that $\psi$ dies away to zero for large distance . so the particle is , with high probability , guaranteed to be in a confined region ( not at large distance ) . so those are bound states . the solutions with $e&gt ; 0$ , on the other hand , do not die away to zero at large distances -- instead , they go like $e^{ikr}$ where $k=\sqrt{2me}/\hbar$ . so these solutions represent particles that have high probability to be arbitrarily far away . physically , they are useful when describing particles that start far away , approach the scattering center , and end up far away again . hence the name " scattering states . "
the general expression for calculating kinetic energy is $$ke = \frac{m v^2}{2} + \frac{i \omega^2}{2}$$ however , $v$ means the velocity of the center of mass and $\omega$ is rotational velocity around the center of mass . $i$ is moment of inertia about center of mass . you cannot do the above expression just for arbitrary point of the body . as for the second question and pulses of rotational movement : i think during the collisions both pucks roll against each other or against the wall for a very small fraction of time . when rolling you have static friction forces , which have great and temporal effects on speed , rotational speed and their relation .
good question . it has made me think . strictly speaking , it is not possible to compute $\theta ( t-t' ) \langle \dot\phi ( t ) \phi ( t' ) \rangle+\theta ( t'-t ) \langle \phi ( t' ) \dot\phi ( t ) \rangle$ ( shorthand notation : $\langle\ , \equiv \langle 0 |\ , $ . also note that i am omitting the spatial arguments of the fields ) using the lagrangian version of the path integral , because to derive this , we are to assume that the insertions ( factors multiplying $e^{is}$ in the integrand of the path integral ) are functionals of the fields at a given time ( i.e. . , they are independent of momenta $\pi$ ) . thus , $$\partial_t \ , \left [ \theta ( t-t' ) \langle \phi ( t ) \phi ( t' ) \rangle+\theta ( t'-t ) \langle \phi ( t' ) \phi ( t ) \rangle\right ] =\lim_{a\to 0}{1\over a}\left [ \theta ( t+a-t' ) \langle \phi ( t+a ) \phi ( t' ) \rangle+\theta ( t'-t-a ) \langle \phi ( t' ) \phi ( t+a ) \rangle - \theta ( t-t' ) \langle \phi ( t ) \phi ( t' ) \rangle+\theta ( t'-t ) \langle \phi ( t' ) \phi ( t ) \rangle\right ] =\lim_{a\to 0}{1\over a}\int d\phi\ , ( \phi ( t+a ) \phi ( t' ) -\phi ( t ) \phi ( t' ) ) \ , e^{is}=\int d\phi\ , \dot\phi ( t ) \phi ( t' ) \ , e^{is}$$ which agrees with $\theta ( t-t' ) \langle \dot\phi ( t ) \phi ( t' ) \rangle+\theta ( t'-t ) \langle \phi ( t' ) \dot\phi ( t ) \rangle$ only if $t\neq t'$ . $^1$ example . let $a ( t ) $ and $b ( t ) $ two functional at a given time , then you can check that $$\int \dot a ( t ) b ( t ) \ , e^{is}=\langle\dot a ( t ) b ( t ) \rangle+\lim_{a\to 0}{1\over 2a}\langle [ a ( t ) , b ( t ) ] \rangle$$ the last term represents the dirac delta you found after deriving the step function . while your question is very interesting , i think that your example is unfortunate since $\delta ( t-t' ) [ \phi ( x ) , \phi ( x' ) ] =\delta ( t-t' ) [ \phi ( t , \vec x ) , \phi ( t , \vec x' ) ] =0$ . let me choose an example in which this last commutator is different from zero to show how the time derivative of a correlation function splits in the two terms you mention : $\partial_t \langle \ , t\ , \phi ( t ) \ , a ( t' ) \ , \rangle$ , where $a$ is a functional of fields and momenta at a given time . let 's make a general variation ( which does not modified the path integral measure ) of the hamiltonian or phase-space path integral ( $s=\int dt\ , \dot \phi\ , \pi-h \ , $ ) . since the momentum is an integration variable , the integral may not change : $$0={\delta\over \delta \pi ( t ) }\int d\phi d\pi \ , a ( t' ) \ , e^{is}\ , \delta \pi=\\ \int d\phi d\pi\ , \left ( {\delta a ( t' ) \over \delta \pi ( t ) }+a ( t' ) i\dot\phi ( t ) -a ( t' ) ( -i ) {\delta h ( t ) \over \delta \pi ( t ) }\right ) \ , e^{is}\ , \delta \pi$$ as $\delta \pi$ is a general variation and : $${\delta a ( t' ) \over \delta \pi ( t ) }=\delta ( t-t' ) ( -i ) \ , [ \phi ( t ) , a ( t' ) ] $$ $${\delta h ( t ) \over \delta \pi ( t ) }=-i\ , [ \phi ( t ) , h ] $$ we obtain : $$\partial_t \langle \ , t\ , \phi ( t ) \ , a ( t' ) \ , \rangle=i\langle \ , t\ , [ \phi ( t ) , h ] \ , a ( t' ) \ , \rangle+\delta ( t-t' ) \ , \langle \ , [ \phi ( t ) , a ( t' ) ] \ , \rangle \\ =\theta ( t-t' ) \langle \ , \dot\phi ( t ) \ , a ( t' ) \ , \rangle+\theta ( t'-t ) \langle \ , a ( t' ) \ , \dot\phi ( t ) \ , \rangle+\delta ( t-t' ) \ , \langle \ , [ \phi ( t ) , a ( t' ) ] \ , \rangle$$ if , for example , $a ( t' ) =\pi ( t ' , \vec x' ) $ , the last term gives $i\ , \delta ^4 ( x-x' ) $ . $\\$ just in case it is not clear enough , let me remark that derivatives do commute with the path integral measure . the key point is that $$\partial_t \ , \left [ \theta ( t-t' ) \langle \phi ( t ) \phi ( t' ) \rangle+\theta ( t'-t ) \langle \phi ( t' ) \phi ( t ) \rangle\right ] =\int d\phi\ , {\phi ( t+\epsilon^+ ) -\phi ( t ) \over \epsilon ^+}\phi ( t' ) \ , e^{is}\neq\theta ( t-t' ) \langle \dot\phi ( t ) \phi ( t' ) \rangle+\theta ( t'-t ) \langle \phi ( t' ) \dot\phi ( t ) \rangle$$ in addition , i would like to emphasize that ${\delta h ( t ) \over \delta \pi ( t ) }$ is a functional at a given time , while $\dot\phi ( t ) $ in the integrand of a path integral must be interpreted as ${\phi ( t+\epsilon^+ ) -\phi ( t ) \over \epsilon ^+}$ , that is , as a difference between fields evaluated at different times . $^1$ note that the derivative can be defined in different ways giving rise to different operator orderings , see maimon 's excellent answer path integral formulation of quantum mechanics , be careful with some typo in the expressions : where says $x ( t ) p ( t ) $ should say $p ( t ) x ( t ) $ . edit : to derive some of the results above , one needs to take $\theta ( 0 ) =1/2$ . however , one can proceed in a slightly different manner to avoid such choice ( whi , in my opinion , is totally right ) . for example , $$\int \dot a ( t ) b ( t ) \ , e^{is}=\int {a ( t+a ) -a ( t ) \over a } b ( t+a/2 ) \ , e^{is}=\langle\dot a ( t ) b ( t ) \rangle+\lim_{a\to 0}{1\over a}\langle [ a ( t ) , b ( t ) ] \rangle$$
you need the clebsch-gordan decomposition , at least in the case $n = 3$ . the reason that we decompose a rank $2$ tensor in the way you describe is that $$\mathbf{1} \otimes \mathbf{1} = \mathbf{2}\oplus \mathbf{1} \oplus \mathbf{0} $$ where the bold numbers denote spin representations . here 's a bit more detail . in quantum physics we are really interested in representations of the lie algebra of $so ( n ) $ namely $\mathfrak{so} ( n ) $ . the most useful case for physical purposes is $n = 3$ , where there is an isomorphism $$\mathfrak{so} ( 3 ) = \mathfrak{su} ( 2 ) $$ the clebsch-gordon result comes from the structure of representations for $\mathfrak{su} ( 2 ) $ . in brief , $\mathfrak{su} ( 2 ) $ has irreps $\mathbf{n}$ for each half-integer $n$ . each irrep has $2n+1$ characteristic labels called weights , evenly spaces between $-n$ and $n$ . physically one interprets these as the component $j_3$ of spin . when you take a tensor product of irreps the weights add up , to give you weights for the tensor product representation . a theorem says that this decomposes into the direct sum of irreps in the only way that uses up all these weights . in case that all sounds absurd , let 's do a concrete example . the tensors you mention are elements of tensor products of the vector representation of $\mathfrak{su} ( 2 ) $ typically denoted $\mathbf{1}$ . we want to prove the result above that $$\mathbf{1} \otimes \mathbf{1} = \mathbf{2}\oplus \mathbf{1} \oplus \mathbf{0} $$ well $\mathbf{1}$ has weights $+1,0 , -1$ so the tensor product will have weights $$-2 , -1 , -1,0,0,0 , +1 , +1 , +2$$ which are all possible ways of adding the weights for $\mathbf{1}$ . now rewrite this list suggestively $$-2 , -1,0 , +1 , +2 , \ \ \ \ \ \ -1,0 , +1 , \ \ \ \ \ \ 0$$ these are just the weights for a $\mathbf{2}$ plus the weights for a $\mathbf{1}$ plus the weights for a $\mathbf{0}$ . now it is not hard to identify $\mathbf{2}$ with the traceless symmetric matrices , $\mathbf{1}$ with the antisymmetric ones and $\mathbf{0}$ with the trace , checking that these all transform correctly under the relevant representations . as an exercise you now have all the tools to prove that $$\mathbf{1} \otimes \mathbf{1} \otimes \mathbf{1} = \mathbf{3}\oplus \mathbf{2} \oplus \mathbf{1} \oplus \mathbf{0}$$ can you identify what these are , in terms of decomposing the rank $3$ tensor ? hint : there exist totally symmetric tracefree tensors , totally antisymmetric tensors , a trace term , and tensors of mixed symmetry . here 's a good reference for the lie algebra stuff . let me know if you need any further details ! p.s. i do not know what one can do for general $n\neq 3$ . the clebsch-gordan niceness is a specific property of $\mathfrak{su} ( 2 ) $ so i expect it becomes quite messy . perhaps somebody else has some expertise here ?
this is your circuit : the current that comes from the source , when reaches the point that must choose it is way , sees no difference between the two paths ( symmetry ) , so half of it flows through one way and the other part flows in the second way . it means that , $i_1=i_2$ , so the potential difference across yellow resistors is the same . it means that the potential of point $\mathbf{a}$ is equal to potential of point $\mathbf{b}$ : $$i_1=i_2\to v_a=4.4-i_1r \text{ , }v_b=4.4-i_2r\to v_a=v_b$$ so there is not any potential difference across the blue resistor , and the current through it is 0 , and it can be omitted from the circuit without any change in the behavior of the circuit .
energy and momentum are both conserved . working in the centre of momentum frame the momenta of the incoming photons are equal and opposite so the total momentum is $p=p+ ( -p ) =0$ . the energies of the photons are also equal and equal to $pc$ . the total energy $e=2pc$ . now let the photons scatter into two photons of energy/momentum $e_1 , \ p_1$ and $e_2 , \ p_2$ respectively . since the total momentum is conserved we must have $p_1 + p_2 = 0$ , so $p_2 = - p_1$ . the momenta remain equal in magnitude . further , the energies are equal : $e_1 = |p_1| c$ and $e_2 = |p_2| c = |p_1| c = e_1$ . since the energies are equal and the total energy is still $e=2pc$ , we have that the energy of the final photons must be $pc$ . so nothing can change except for the direction of the outgoing photons . you can get the answer in any other frame by doing a lorentz boost of this result .
first the answer : the motion is not quite stable , but only because of two subtle thing that your brain probably can intuitively feel : the rate at which the outer ring ( the one that is rotating around the vertical axis ) rotates has to speed up and slow down as the inner ring becomes vertical and horizontal respectively . once you slow down and speed up the outer ring , the motion is fine . but if you do not do this , you need to provide twisting torques at the top and bottom periodically to compensate . the rate at which the inner ring spins has to speed up and slow down as the ring becmes horizontal and vertical , due to the centrifugal force in its frame pulling it out and pushing it in . it is these two things that make the motion nonuniform rotation speed , and this makes the motion seem off intuitively . you can not make both motions uniform with the same period--- the inner ring has to speed up and slow down even in the limit of large mass of outer ring . when you do the slowing down and speeding up , as required by the changing moment of inertia , and also the slowing down and speeding up required by the centrifugal pushing and pulling , this motion is natural . it does require stresses and torques on the system , but they are the kind that are naturally provided by the mount , they are just the forces holding the mount in place . solving the lagrangian the analysis kostya did for the lagrangian is correct ( althugh i originally swapped his angle names ) . the outer ring is rotated by a rotation matrix $r_z ( \phi ) $ , while the inner ring is rotated by $r_z ( \phi ) r_x ( \theta ) $ , meaning first rotate around the x axis , then around the z axis . the motion of any point from a change in $\theta$ is always perpendicular to the motion from $\phi$ , so there are no cross-terms . the total moment of inertia for the $\phi$ motion is the sum of the moment of inertia of the outer ring , and the moment of inertia of the tilted inner ring . the inner ring , as it is tilted , goes from having a moment of inertia i when it is horizontal to i/2 when it is vertical . this means that the whole lagrangian is $$ l = {1\over 2} ( a + c \cos^2 ( \theta ) ) \dot{\phi}^2 + {1\over 2} b \dot{\theta}^2 $$ just as kostya says ( for a planar internal motion c=2b ) . this lagrangian has a conserved energy and a conserved $\phi$ momentum , since $\phi$ does not appear in l ( the system is symmetric with respect to rotations around the z-axis ) , and this reduces it to a 1 degree of freedom system . from the conservation of $\phi$ momentum , $$ p_\phi = ( a+c \cos^2 ( \theta ) ) \dot{\phi} = p $$ which gives the rate of change of $\phi$ , and the coefficient b tells you how it speeds up and slows down as you make the inner disk vertical or horizontal . the conservation of energy now tells you $\dot{\theta}$ $$ h = p_\phi \dot{\phi} + p_\theta \dot{\theta} - l = l $$ so that $$ b\dot{\theta}^2 + {p^2\over a+ c \cos^2 ( \theta ) } = 2e $$ the point of this is only that the $\phi$ motion is nonuniform only to the extent that c is nonzero ( the same reason the outer ring is rotating nonuniformly ) , so you get a perfectly fine periodic motion in $\phi$ , and you can adjust the total $\theta$ period to equal the $\phi$ period to make a motion which is qualitatively like the one shown in the film . the film is never exact , even though the outer motion can be much heavier than the inner ring , the inner ring ( if it is flat ) must speed up and slow down due to centrifugal force . i felt i needed intuition about the stresses involved to keep the rings rotating , which you do not see in the lagrangian formulation . some intuition on the stresses to see how the stresses work , imagine the outer ring is infinitely heavy , and rotating with a constant angular velocity ( this is what is shown in the movie ) . now transform to the rotating frame . there are two ficititious forces . the centrifugal force is cancelling on the ring between the two sides , and the effect of this is just to make the inner ring it want to explode outward when it is horizontal ( there is no effect when it is vertical ) . the effect of this is to introduce the centrifugal potential term that pulls the ring horizontal , and this is the source of kostya 's pendulum force ( which is the reason the inner ring can stably oscillate around the horizontal position ) . the coriolis force is $\omega\times v$ , and when the ring is partway between horizontal and vertical , it turns the inner ring like a steering wheel in a definite direction . this needs to be counteracted by the outer ring , and this turning push is provided by the contacts . the turning push is what is responsible for the ice-skater-like slowing down of the outer ring ( but here we are assuming the outer ring is very massive ) . the result is completely intuitive , and you can understand all the effects--- there is a steering wheel pull on the inner ring in opposite directions as it is rotating , which only has the effect of slowing down the outer ring , and which can be provided by the grip of the outer wheel on the inner one .
straight from the horse 's mouth : source : bureau international des poids et mesures ( search for " dimensionless " for all guidelines . ) the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
the waves are not necessarily sinusoidal , but any description of a function which is integrable ( i.e. . , has a finite area or " energy" ) can be decomposed into superpositions ( sums ) of sines and cosines , or alternatively ( and equivalently ) complex exponentials . this is why they are shown as sine or cosine waves , because that is the simplest object to think about . in reality , they are a ( possible infinite ) sum of the sine and/or cosine waves . also , you can build an antenna , and if you modulate it very carefully with a sine wave electrically , it will radiate a sine wave . . .
you can have superfluids that are not becs and becs that are not superfluid . let me quote a text , " bose-einstein condensation in dilute gases " , pethick and smith , 2nd edition ( 2008 ) , chapter 10: historically , the connection between superfluidity and the existence of a condensate , a macroscopically occupied quantum state , dates back to fritz london 's suggestion in 1938 , as we have described in chapter 1 . however , the connection between bose-einstein condensation and superfluidity is a subtle one . a bose-einstein condensed system does not necessarily exhibit superfluidity , an example being the ideal bose gas for which the critical velocity vanishes , as demonstrated in sec . 10.1 below . also lower-dimensional systems may exhibit superfluid behavior in the absence of a true condensate , as we shall see in chapter 15 .
your equations ( 2 ) , ( 3 ) and ( 4 ) are all correct . i do not like the way ( 1 ) is written , because it relies on an unstated understanding that the $e$ 's represent only mass energy , but with that understanding it is also correct . this is just the conservation of energy in it is relativistic form where mass is just another kind of energy ( previously you had to worry about kinetic energy and various kinds of potential and internal degrees of freedom expressed as heat and so on ; with the advent of relativity you have to add mass to that list ) . $q$ is the difference of the masses expressed in terms of energy . your idea for getting the distribution of energy between the product nucleus on the neutron ( use the conservation of momentum ) is correct , but you must work in two dimensions not one ( you can choose to work in the $x$--$y$ plane however ) . choosing the direction of the photon as $\hat{x}$ ( ) which you can do without loss of generality ) it should be $$ h \nu \hat{x} = \vec{p}_n + \vec{p} $$ where i have not written the momenta in the newtonian approximation , because you do not yet know that this is correct . you have a couple of choices here : do in the full relativistic expression do it in the newtonian regime and then check to see if that was reasonable assume that the nucleus will be newtonian but treat the neutron relativistically , and then check to make sure the result was reasonable . the important thing is that if you make the newtonian approximation , you must go back and check to see that it was reasonable after . questions for the student : what condition or conditions make the newtonian approximation " reasonable " ? can you deduce if the approximation will likely be reasonable before you begin calculating ? why did i suggest treating only the recoiling nucleus in the newtonian approximation as an option above ?
it was not a black hole because the density was not sufficiently high . the density was lower than what is needed for a black hole because the volume was larger . the volume was larger because the atoms ( mostly hydrogen ) were kept away from each other by the pressure produced by the fusion processes . once the fusion processes stop , this source of repulsion between the atoms disappears , the volume shrinks , the density goes up , and the black hole threshold may be surpassed .
in the equation $$ c_v = at+ bt^3\ln t$$ the logarithmic term may be explained by paramagnons , i.e. fluctuations in which the adjacent atoms are demanded to be aligned . such fluctuations are long-lived . this explanation of the non-analytic term was found by doniach and engelsberg as well as berk and schrieffer . both articles are in prl 1966 . http://prl.aps.org/abstract/prl/v17/i14/p750_1 http://prl.aps.org/abstract/prl/v17/i8/p433_1 a full text of berk and schrieffer : http://books.google.cz/books?hl=enlr=id=yqu2bjfykrgcoi=fndpg=pa90dq=berk+schriefferots=vcx7dizdxgsig=hcfjbki-54txrpywjj9of7ovuqeredir_esc=y#v=onepageq=berk%20schriefferf=false brinkman and engelsberg discuss some limitations of the applicability of the log term in 1968: http://prola.aps.org/abstract/pr/v169/i2/p417_1 pethic and carneiro with their fermi liquid explanation came later . 1966 was before the renormalization group but it is not really needed for the calculations . the logarithmic corrections do arise from one-loop processes and similar corrections have been known in condensed matter physics and particle physics long before we knew about the right philosophical words linked to the renormalization group from the 1970s .
yes , he did . there is a press release at ucsb that acknowledges it as his second win : http://www.ia.ucsb.edu/pa/display.aspx?pkey=3161 the reason for this is , presumably , that the committee is considering giving him the bigger "2014 fundamental physics prize . " in essence , the physics frontiers prize is a nomination for that . ( this is based on the description on the press release , and past reading . )
the thing that is going wrong with your manual calculation is you are taking the velocity to be constant in every interval i.e. , you are taking velocity to be $10m/s$ from $0$ to $1s$ , $9m/s$ from $1s$ to $2s$ and so on which is incorrect . the velocity is continuously decreasing . you may calculate like this : at $t=0 , v=10m/s , a=-1m/s^2$ which means from $t=0$ to $t=1$ , car has travelled a distance , $$s=10\times1+\frac{1}{2} ( -1 ) \times 1^2$$$$=10-\frac{1}{2}$$ and the velocity has become $9m/s$ at $t=1s . $ so , from $t=1$ to $t=2$ , car has travelled a distance , $$s=9\times1+\frac{1}{2} ( -1 ) \times 1^2$$$$=9-\frac{1}{2}$$ and so on . $$10-\frac{1}{2}+9-\frac{1}{2}+ . . . . . . $$$$=55-5=50$$
if $\psi_1 ( x_1 ) \psi_1 ( x_2 ) $ is antisymmetric ( and i understand this is impossible , since the ground state is not degenerate ) the ground state is degenerate , since both particles have the same $n$ ( principal ) quantum number and thus the same energy . in general , for $n$ particles , the symmetric and antisymmetric wavefunction may be constructed as \begin{align}\psi_{_s} and \equiv\sqrt{\frac{n_1 ! \cdots{n}_k ! }{n ! }}\sum_p\hat{p}\ , \phi_{n_1} ( \zeta_1 ) \phi_{n_2} ( \zeta_2 ) \ldots\phi_{n_n} ( \zeta_n ) \\ [ 0.1in ] \psi_{_a} and \equiv\sqrt{\frac{n_1 ! \cdots{n}_k ! }{n ! }}\begin{vmatrix}\phi_{n_1} ( \zeta_1 ) and \cdots and \phi_{n_1} ( \zeta_n ) \\\vdots and and \vdots\\\phi_{n_n} ( \zeta_1 ) and \cdots and \phi_{n_n} ( \zeta_n ) \end{vmatrix}\end{align} respectively , where $\zeta_i$ are the internal degrees of freedom and $n_i$ is the degeneracy of the $i$-th set of degenerated particles ( for the antisymmetric part , most usually $n_1 ! \cdots{n}_k ! =1$ ) . in your case ( given that you can always write the wavefunction as a product of the spatial and spin parts ) , $$\psi_{_a}=\begin{vmatrix}\psi_1 ( x_1 ) and \psi_1 ( x_2 ) \\\psi_1 ( x_1 ) and \psi_1 ( x_2 ) \end{vmatrix}=0$$ which is why the spatial antisymmetric part is impossible for the ground state . for fermions this is a natural consequence of the pauli exclusion principle , since you would allow the possibility of two particles being in the same state , given that the spin part would be symmetric . now , for the first excited level there is no restriction about considering both the symmetric and antisymmetric parts , in fact you must consider them both . just as when you must consider the three posibilities from the triplet spin state $$\chi_{_t}=\begin{cases}\chi_\alpha\\\chi_\beta\\\chi_+\end{cases}$$ you may consider both solutions ( 4 in total , as you say ) , $$\psi=\begin{cases}\frac{1}{\sqrt{2}}\left [ \psi_1 ( x_1 ) \psi_2 ( x_2 ) +\psi_1 ( x_2 ) \psi_2 ( x_1 ) \right ] \chi_-\\\frac{1}{\sqrt{2}}\left [ \psi_1 ( x_1 ) \psi_2 ( x_2 ) -\psi_1 ( x_2 ) \psi_2 ( x_1 ) \right ] \chi_{_t}\end{cases}$$ the thing is that this is a set of possible solutions , just as what you found out for the spin part with the triplet state , the particles may have this or that state , usually when dealing with fermions the only restriction to take care of is pauli exclusion principle . you may thus consider them all to construct the total wavefunction . now , the thing trimok says , note that , mathematically , you can have a total antisymmetric wave function , without having a specific symmetry in the spatial part or in the spin part , for instance : $$\psi_1 ( x_1 ) \psi_2 ( x_2 ) \alpha ( s_1 ) \beta ( s_2 ) - \psi_2 ( x_1 ) \psi_1 ( x_2 ) \beta ( s_1 ) \alpha ( s_2 ) $$ could be misleading . this can be seen if you construct the first excited state from the slater determinant ( the general expression for $\psi_{_a}$ ) , say , $n_1=1$ , $n_2=2$ , $\alpha ( 1 ) $ , $\beta ( 2 ) $ , i.e. $$\mathcal{s}_1\equiv\frac{1}{\sqrt{2}}\begin{vmatrix}\psi_1 ( x_1 ) \alpha ( 1 ) and \psi_1 ( x_2 ) \alpha ( 2 ) \\\psi_2 ( x_1 ) \beta ( 1 ) and \psi_2 ( x_2 ) \beta ( 2 ) \end{vmatrix}$$ which is the expression given by trimok , but the thing , again , is that you must consider all possible solutions for this state , meaning that for $n_1=1$ , $n_2=2$ , you can have \begin{align}\alpha ( 1 ) , and \ , \alpha ( 2 ) \\\beta ( 1 ) , and \ , \beta ( 2 ) \\\beta ( 1 ) , and \ , \alpha ( 2 ) \\\alpha ( 1 ) , and \ , \beta ( 2 ) \end{align} you can interchange $n_1 , \ , n_2$ also if you please ( there is no new information ) . for the first , the slater determinant pops out the antisymmetric spatial solution times $\chi_\alpha$ , the second , the antisymmetric spatial part times $\chi_\beta$ , but you may take the third , $$\mathcal{s}_2\equiv\frac{1}{2}\begin{vmatrix}\psi_1 ( x_1 ) \beta ( 1 ) and \psi_1 ( x_2 ) \beta ( 2 ) \\\psi_2 ( x_1 ) \alpha ( 1 ) and \psi_2 ( x_2 ) \alpha ( 2 ) \end{vmatrix}$$ and the fourth to build the antisymmetric part times $\chi_+$ as $\mathcal{s}_1+\mathcal{s}_2$ and to build the symmetric part times $\chi_-$ as $\mathcal{s}_1-\mathcal{s}_2$ . here both must be taken in count because of the indistinguishability of particles , considering $\mathcal{s}_1$ or $\mathcal{s}_2$ alone is just insufficient . as i showed first , all this is taken care of if you just factor the spatial and spin parts of the wavefunction and treat each one apart , as you were doing .
after some search , i found a video on youtube which was about the rainbow . it says that the set of all points which have a fixed angle between the sunlight , the raindrop and the observer creates the circular arc of the rainbow . this picture helped me . for the second question about the reason that we just see a special color at each angle , i watched lewin 's lecture at delft university of technology . he mentioned the reason and said at the maximum values of $\varphi$ , there is a peak in the intensity of that special light . because of that , although all colors exist at e.g. $40.7^\circ$ but just one color is seen because the peak intensity of blue occurs at $40.7^\circ$ .
what is a local lagrangian density ? a classical field theory on minkowski space $\mathbb r^{d , 1}$ is specified by a space $\mathcal c$ of field configurations $\phi:\mathbb r^{d , 1}\to t$ , and an action functional $s:\mathcal c\to\mathbb r$ . the set $t$ is called the target space of the theory , and is often a vector space . if there exists a function $l:\mathcal c\times\mathbb r\to \mathbb r$ for which \begin{align} s [ \phi ] = \int_{\mathbb r} dt\ , l [ \phi ] ( t ) , \end{align} then we call $l$ a lagrangian for the theory . if , further , there exists a function $\tilde l$ such that \begin{align} l [ \phi ] ( t ) = \int_{\mathbb r^d} d^d\mathbf x \ , \tilde l [ \phi ] ( t , \mathbf x ) \end{align} then we call $\tilde l$ a langrangian density for the theory . finally , if there exists a positive integer $n$ and a function $\mathscr l$ such that \begin{align} \tilde l [ \phi ] ( t , \mathbf x ) = \mathscr l ( t , \mathbf x , \phi ( t , \mathbf x ) , \partial\phi ( t , \mathbf x ) , \dots , \partial^n\phi ( t , \mathbf x ) ) \end{align} then we say that the lagrangian density is local . in other words , the lagrangian density is local provided its value at a given spacetime point depends only on that point , the value of the field at that point , and a finite number of its derivatives at that same point . an example of a non-local lagrangian density . consider $t = \mathbb r$ , namely a theory of a single real scalar field . let $\mathbf a\in\mathbb r^d$ be given , and define a lagrangian density by \begin{align} \tilde l [ \phi ] ( t , \mathbf x ) = \phi ( t , \mathbf x ) + \phi ( t , \mathbf x+\mathbf a ) . \end{align} this lagrangian density is not local because the value of the lagrangian at a given point $ ( t , \mathbf x ) $ depends on the value of the field at that point and on the value of the field at the point $ ( t , \mathbf x+\mathbf a ) $ . if we were to taylor expand the second term $\phi ( t , \mathbf a ) $ about $\mathbf x$ , then we would see that the lagrangian density depends on an infinite number of derivatives of the field , thus violating the definition of a local lagrangian density . what is the issue with theories with non-local lagrangian densities ? i am no expert on this , so i will divert to another user . i will say , however , that people do study theories with non-local lagrangian densities in practice , so there is nothing a priori " wrong " with them , but they might generically exhibit some pathology that you might prefer not to have . perhaps most relevant , though , if you are taking qft from a high energy theorist for example , is that the lagrangian density of the standard model is local , so there is no need to consider non-local beasts if one is studying the standard model .
the hamiltonian $$h=aj_z^2=a\left ( \sum_j s_z^{ ( j ) }\right ) ^2\tag1$$ is a function of the individual $z$ spin projections $s_z^{ ( j ) }$ , and all of those commute . therefore , the eigenstates will be product states of the form $$|\psi\rangle=|a_1\rangle\otimes\cdots\otimes|a_n\rangle , \tag2$$ where each $|a_j\rangle$ is either $|\ ! \uparrow\rangle$ or $|\ ! \downarrow\rangle$ . as such , the system is easily solvable , and you simply need to phrase your questions correctly . the expectation value $\langle j_x^2\rangle$ , for example , is constant and equal to $n$ for all eigenstates of $h$ of the form ( 2 ) ( though there are degeneracies and superpositions of such eigenstates may yet be squeezed ) . edit : ok , i think i know what is confusing you . in particular , from your question edit : starting with an initial ( q- ) distribution in phase space , the distribution evolves with precession frequency proportional to $j_z$ . but from there i do not see how the variance along $\hat z$ would change . the second paper starts off the atoms in a spin-coherent-state cloud along the $+x$ pole on the bloch sphere , and then lets them evolve according to the hamiltonian ( 1 ) . this means that points closer to the $+z$ pole have more positive energy , accumulate more phase on their up components than their down ones , and therefore rotate towards the right . similarly , points closer to the $-z$ pole have more negative energy , accumulate more phase on their down components , and rotate towards the left . the net effect of this is to shear the cloud on the bloch sphere , whilst preserving its height . in particular , the $z$ marginal , i.e. the distribution of ups and downs in a $z$ measurement , is not affected , as it should . as a consequence , the variance in $z$ is not ( yet ) affected . you can see , of course , that the cloud is now longer ( in that its variance in $y$ is now much greater ) , and also thinner , in a slightly off-diagonal direction . thus , to obtain squeezing in the $z$ direction , you need to rotate slightly about the $x$ direction . by how much , of course , is a question of exactly what the circumstances are , and it is a function of the product of $a$ and the interaction time ; if you want the details , you should first give kitagawa and ueda 's paper a thorough read , i think . this is quite a popular protocol for obtaining spin-squeezed states , i think : prepare a spin coherent state , shear it using interactions , and finally rotate it to the measurement direction . the third step is crucial , because the shear does not affect the width along the interaction direction , which is what i was nagging you about in the first part of this answer . once you rotate it , though , you can get reduced ( or vastly increased ) variances in any component .
well , if i read the problem correctly then your kinetic energy is wrong . your $\dot{x}_m$ is the $x$-component of the velocity of the ball , but you are missing the $y$-component and also the velocity of the cart , not to mention you are multiplying $\dot{x}_m^2$ by $m$ when you should be multiplying it by $m$ . the $\cos^2\phi$ goes away because the $y$-component of the ball 's velocity has $\sin \phi$ in it , and as we all know $\cos^2 \phi + \sin ^2 \phi = 1$ .
no , you have a differential equation to solve : $$m\ , \frac{{\rm d} v}{{\rm d} t} = m\ , \frac{{\rm d} x}{{\rm d} t} \frac{{\rm d} v}{{\rm d} x} = m\ , v\ , \frac{{\rm d} v}{{\rm d} x}=-\beta \ , v$$ where $\beta = 200$ . since we are interested in speed $v$ against distance $x$ , we choose the form $v\ , \mathrm{d}_x v$ for the acceleration . so we are left with : $$m\ , \frac{{\rm d} v}{{\rm d} x} = - \beta$$ which i believe will tell you what you need . it looks as though you may still a bit shaky with underlying meanings of derivatives and integrals : are you taking a calculus course at the moment ?
this paper claims that consistent histories is an extension of the feynman path integral approach .
as long as the flying object stays in orbit round the earth you have not changed the net mass or centre of mass of the earth . we have succeeded in reducing the net mass of the earth by the mass of the voyager probes and the like , but this is insignificant compared to the mass of meteoric dust the earth accumulates every day ( allegedly 100-300 tonnes per day ! ) . so launching satellites and planes taking off is not going to directly affect earth 's orbit . i suppose it slightly changes the ( gravitational ) shape of the earth and might change forces from other objects , but this effect is likely to be immeasurably small .
i am pretty sure that even the brief summarization of all the alternatives will take a book or two . i will try to give a review of basic things from my perspective . let me from the beginning note that the following classification is not accurate -- different classes may and do overlap . more scalar doublets ( multiplets ) first of all one can introduce more scalar multiplets . two higgs doublet model ( 2hdm ) is the most favored , because it is also naturally arises from mssm . nhdms are also considered . doublets are usually considered , because there is a basic constraint on the quantum numbers of the fields , coming from " rho parameter": $\rho = \frac{m_w^2}{m_z^2\cos^2\theta_w}=1$ which can be satisfied only if $ ( 2t+1 ) ^2-3y^2=1$ with the most natural solution $y=1 , t=1/2$ . of course there are other solutions , leading to bigger values , but i have never seen anyone seriously considering those . there is still a lot of freedom to impose some extra discrete symmetries , continuous symmetries , the way these scalars interact with fermions , e.t.c. , which leads to many subclasses of such models . composite higgses the central example is the little higgs model where higgs arrives as a ( pseudo- ) goldstone boson from some higher global symmetries . changing the underlying symmetry one obtains the whole class of such models . extra gauge symmetries are also considered -- they are usually broken dynamically . technicolor was the most popular one -- now it not so favored , while i do not think that it was refuted completely . top condensate is another dynamical model . originated from extra dimensions lots of geometries , compactifications , boundary conditions -- i feel completely lost with those . most popular are higgsless models -- attempts to get rid of higgses completely . they are usually based on some specific boundary conditions . of course the list in incomplete . there are a lot of different " mixtures " between those models , usually with some new funny names . here is a nice recent reference that reviews some of the mentioned models going more deeply into some tehnical details .
space_cadet mentioned already work about deriving spacetime as a smooth lorentzian manifold from more " fundamental " concepts , there are a lot of others -like causal sets , but the motivation for the question was : the reason for my interest in this regards one of the mysteries of quantum mechanics , that of quantum entanglement and action at distance . i wondered whether , if space is imagined as having a topology that arises from a notion of neighbourhood at a fine level , then quantum entanglement might be a result of a ' short circuit ' in the connection lattice . i am not convinced that such an explanation is possible or warranted , the reason for this is the reeh-schlieder theorem from quantum field theory ( i write " not convinced " because there is some subjectivity allowed , because the following paragraph describes an aspect of axiomatic quantum field theory which may become obsolete in the future with the development of a more complete theory ) : it describes " action at a distance " in a mathematically precise way . according to the reeh-schlieder theorem there are correlations in the vacuum state between measurements at an arbitrary distance . the point is : the proof of the reeh-schlieder theorem is independent of any axiom describing causality , showing that quantum entanglement effects do not violate einstein causality , and do not depend on the precise notion of causality . therefore a change in spacetime topology in order to explain quantum entanglement effects will not work . discussions of the notion of quantum entanglement often conflate the notion of entanglement as " an action at a distance " and einstein causality - these are two different things , and the first does not violate the second .
note that $\partial v=s$ , so that $$\tag{1} c~=~\partial s~=~\partial^2v~=~\emptyset$$ is the empty set . ( topologically , the boundary of a boundary is empty , or equivalently , the boundary operator $\partial^2=0$ squares to zero . ) on the other hand , the circulation $$\tag{2} \gamma~=~\oint_{c=\emptyset}\vec{a}\cdot d\vec{r}~=~0$$ along the empty curve $c=\emptyset$ vanishes identically for any vector field $\vec{a}$ . in particular , one can not conclude from ( 2 ) that the magnetic potential $\vec{a}$ should be a gradient field .
angles are defined as the ratio of arc-length to radius multiplied by some constant $k$ which equals one in the case of radians , $360/2\pi$ for degrees . what you are effectively asking is what is natural about setting $k$ = 1 ? again it is tidyness as pointed out in dmckee 's alternative answer .
the simple answer is no . materials for an earth elevator are at least one order of strength too weak at this time . mars gravity is around 0.378 of earth , so materials are still too weak . the long answer is much more complicated : 1 . taper of the tether plays a role as much as the safety factor you want to engineer into your elevator , how much a tether can hold , and how much material you can put into space to construct it . strength , taper , and tether mass are related . you can check for the space elevator feasibility condition via isec . org or google . it is basically some form of decay equation : how much do you need to keep lifting to maintain or grow your elevator , basically taking away from transport payload for maintenance/repairs . 2 . mars has some interesting options with deimos being made of mostly carbon . material for a tether could be refined on site , e.g. , when planetary resources engineers related technology for asteroids . deimos ' orbital period is with 30.30h is close to the sidereal rotation period of mars with 24.6229 h . you could work a tether that drops from deimos part way into the mars atmosphere and get something that you could attach to with much smaller delta v than getting to orbit , thus limiting tether strength and mass requirements substantially further . i strongly believe that that the deimos model is probably close to being within reach of today 's advanced materials . of course you need to work out some control to avoid phobos dropping by every so often below deimos . 3 . the best anchor point for a mars elevator , once material becomes sufficently strong for surface attachment would be on olympus mons . that puts it above sand storms and other mars weather . finally : i believe that mars is destined to become the shipyard for the solar system . it is sufficiently friendly for construction sites and the lower gravity makes it much more amenable to lift things to space than earth . hope this helps . let me know if you have any more questions about the material strength . martin
the $3/2kt$ is the expectation value of kinetic energy of non-interacting particles in thermodynamic equilibrium , i.e. , particles with random velocities in an equilibrium probability distribution . it does not apply to convective " bulk flow " of electrons in an electric field , where in the ideal case all electrons are at the same velocity . now there is such a thing as equilibrium electronic temperature . this is temperature of an equilibrium distribution electrons . for examples , the conduction bad electrons in a solid under zero potential . that is an equilibrium condition not a non-equilibrium bulk flow situation . ideally bulk flows due to conservative fields have no entropy , and their exergy equals energy . the best way to think about it , is consider you flow to be made of classical particles that all get accelerated the same way when imposed to a macroscopic potential energy gradient . the entropy comes when these particles collide with each other and develop different velocities ( i.e. . , scramble ) . this is what happens in a real system , e.g. , wind flowing due to some pressure gradient . entropy is generated due to viscous dissipation of kinetic energy while flowing . but exergy is defined as the maximum -reversible work you can get and that is the same as the kinetic energy for a bulk flow , i.e. , 100% .
it is not so much as one single particle will be seen with different masses as it is that that one type of particle will be seen as having multiple different masses when it is detected multiple times . for example if the extra dimension is like a rolled up microscopic cylinder , the particle can have an infinite number of discrete masses starting from the mass which has no extra kinetic energy in the rolled up cylindrical direction , to masses that have 1 , 2 , . . . units of momentum in the extra rolled up cylindrical direction . these momenta are quantized since only whole wavelengths of the particles wavefunction are allowed around the cylindrical dimension and each unit of extra momentum around the cylinder will be seen as additional rest mass . these are called the kaluza-klein tower of excitations of the basic particle . kaluza-klein theory was developed in the 1920s in an attempt to unify gravitation ( general relativity ) and electromagnetism ( maxwells equations ) . by the way , according to this source : you may also be interested to know that the original 1921 theory has evolved into today 's string theory , as both share the idea of using multiple extra space dimensions to describe the world . the most advanced version of strings is known as m-theory , which utilizes an 11-dimensional spacetime having seven compactified extra space dimensions – a far cry from kaluza 's original single extra dimension ! by the way , kaluza originally came up with his idea in 1919 and communicated it to einstein in hope that the great scientist would recommend it for publication . but einstein , who expressed great admiration for kaluza 's idea , sat on it for two years before recommending it . i am sure this did not sit well with kaluza .
the wikipedia article on color can clear the confusion on color perception . the electromagnetic spectrum in the visible range corresponds to colors as the human eye observes them , but color perception is more general and depends on the phsyiology of the eye and brain . one can make color photographs using for illumination only two frequencies , for example , as polaroid inventor e . land demonstrated . a hot iron rod is red because the upper end of the infrared spectrum , which is the heat you are feeling/measuring , not seeing , goes into our perception of red . the overwhelming majority of the energy is in the infrared for which we have no retina sensors , only skin ones . a book may be red because it is reflecting the red frequency , or we may perceive it as red because of the cones in our retina ( read the article of color in wiki ) . the cover absorbs the non reflected part of the spectrum but that is a very small amount of energy to be converted to heat from room light , so we do not perceive the difference in temperature .
apart from motor and bearing noise , most of the acoustic power comes from the eddy swirls following the trailing edge of the blade after it passes by . there is also an outward pulse of air as the leading edge of each blade pushes forward cutting the air . the trailing eddies produce a broad spectrum of random noise , modulated by the fan blade frequency . the outward pulses , of course , occur at the blade frequency , with harmonics . both are stronger near the tip , because the tip is moving faster . faster splitting of the air means a sharper leading edge of the pressure pulse , so the higher frequency noise is especially concentrated near the tips . a small microphone near the spinning blades would pick up a repeating soft step function from the blades ' leading edges , with faster-rising step functions ( thus composed of higher frequencies ) farther from the hub . that answers one part of your question . by " blade frequency " i mean how often a blade passes any specific point in space , per second . ( i am not a fan engineer , so my jargon may be off . ) ( although , do not misunderstand me - i am a big fan of engineers ! ) actual noise from a fan will show variations at the spin frequency , or full cycles of the blade assembly , not just at what i am calling the blade frequency . this is because the blades are not perfectly identical . who knows , maybe there is the residue of a dead bug on one blade and not on the others . imbalances increase noise . note that fans will put out acoustic noise at frequencies lower than the blade frequency . the eddy swirls are not the same each time a blade passes by - they are random and the variations from one cycle to the next to the next mean subharmonics . fan blades rarely spin bare naked in air . there is probably a grille , a wire cage , something protective to keep kids ' fingers safe and insects out . whatever the structure , there is turbulence as air is pushed by . the noise will be mostly from pushed air leaving the fan , and little ( but some ) from the new air sucked in to replace it . there is a formula for estimating the acoustic power produced by a fan . all i can find at the moment is this from a pdf of unknown origin . the formula is alleged to come from a handbook provided by ashrae ( american society of heating , refrigerating and air-conditioning engineers ) which is , alas , not online for free . $l_w = k_w + 10 log_{10} q + 20 log_{10} p + bfi + c_n$ $l_w$ is the sound power level in db . $k_w$ is a specific constant amount of noise , stated in the manufacturer 's data . it is usually 20-something or 30-something db , unless you have a nasty industrial fan it could be up to 40-something db . q is air flow , cubic feet per minute . ( no offense to the n-1 countries in the world not using us units of measure ! obviously this pdf originated in the us . ) p is air pressure , as inches water ( ! ) . bfi is some sort of correction relating to blade frequency , usually a single digit number of db . finally $c_n$ corrects for inefficient fans . zero for a " perfect " fan , to about 12db for a fan that ' only 40% efficient . efficiency here is " hydraulic efficiency " based on air flow , pressure , and motor power in horsepower ( hp ) : ${q p}\over{6350 ( {hp} ) }$ have fun plugging in numbers . . . i brushed off motor noise at the start of this answer , but of course that is a big factor . unless you pay big $$$ for an exquisitely well-balanced motor , the motor is shaking its mounting , the entire apparatus it is in . a computer motherboard makes a great soundboard to transmit vibrations into the air . a fan in the window or hanging from a ceiling ( vital in florida ! ) or in a stand of any kind , is transmitting vibrations to the structure and , if there is some loose parts involved , will often satisfy the requirements of a chaotic system . that means more subharmonics and plenty of harmonics . good research was done on this back in the 1950s , with renewed interest over the last twenty years or so due to the desire for quiet computers . ( especially in amateur audio recording studios ! ) the best material is in books that i have no easy access to at this time . some references that exist online , at least abstracts of papers : source of noise in fans - howard hardy j . acoust . soc . am . volume 31 , issue 6 , pp . 850-850 ( 1959 ) see http://www.arca53.dsl.pipex.com/index_files/ventnoise1.htm about halfway down the page , " fan noise " " visualization of aerodynamic noise source around a rotating fan blade " , a . nashimoto , n . fujisawa , t . nakano , t . yoda , j . visualization 11 ( 4 ) :365-373 ( 2008 ) ( avail . at acm digital library , springer . com and other purveyors of academic papers , but not for free online . are you near a university with a library ? ) for noises created by aircraft , this article is informative and non-technical : http://www.noisequest.psu.edu/sourcesaviation.overview.html this company making fans , blowers etc . has some lightweight info on fan noise physics : http://www.jmcproducts.com/acoustic-noise/ though i dislike using wikipedia due to its ever-changing nature , at this time this article does have relevance : http://en.wikipedia.org/wiki/quiet_pc
in minkowski spacetime the one way light travel time to a galaxy at proper distance $\chi$ is just : $$ t = \frac{\chi}{c} $$ so : $$ \chi = ct $$ as you say . however in an frw universe the travel time is given by a different equation so the proper distance is not simply $ct$ . let 's assume all motion is in the $x$ direction , so the metric simplifies to : $$ c^2ds^2 = -c^2dt^2 + a^2 ( t ) dx^2 \tag{1} $$ we will take our position to be $ ( 0 , 0 ) $ and the galaxy to be at $ ( 0 , \chi ) $ , and we will adopt the usual convention that $a = 1$ at the current time . to get the proper distance we integrate $ds$ , and since $dt = 0$ and $a = 1$ the proper distance is just : $$ \delta s = \int_0^\chi dx = \chi $$ now let 's calculate the time it takes the light beam to get from the galaxy back to us ( i.e. . one half of the journey ) . light travels on a null geodesic so $ds = 0$ and putting this into the metric ( 1 ) and rearranging we get : $$ \frac{dx}{dt} = \frac{c}{a ( t ) } $$ if the universe is static $a ( t ) = 1$ for all $t$ , and we get $x = ct$ so you would be correct that the proper distance is equal to half the total travel time times $c$ . but then with $a = 1$ we just have minkowski spacetime so that is hardly surprising . to calculate the trajectory of the light we need to assume a form for $a ( t ) $ so let 's make the approximation : $$ a ( t ) = 1 + ht $$ where $h$ is the current value of the hubble constant . then we get : $$ \frac{dx}{dt} = \frac{c}{1 + ht} $$ and this integrates to give us : $$ \chi = \frac{c}{h} \log ( h\tau + 1 ) $$ or rearranging this to get the travel time : $$ \tau = \frac{exp ( \frac{\chi h}{c} ) -1}{h} \tag{2} $$ so the proper distance $\chi$ is not simply the travel time times $c$ . just to reassure ourselves that we get the correct result in the limit of $h \rightarrow 0$ , i.e. minkowski spacetime , note that for small $h$: $$ exp ( \frac{\chi h}{c} ) \approx 1 + \frac{\chi h}{c} $$ put this back into equation ( 2 ) and we get : $$ \tau \approx \frac{\chi}{c} $$ which is where we came in .
i agree with jwenting but in some sense , i feel that he is not answering the question : why there is no " combined $\alpha$ plus $\beta$ decay in which a nucleus emits e.g. a helium atom ? well , let me start with the $\beta$-decay . nuclei randomly - after some typical time , but unpredictably - may emit an electron because a neutron inside the nuclei may decay via $$ n\to p+e^- +\bar\nu$$ which may be reduced to a more microscopic decay of a down-quark , $$ d\to u + e^- +\bar\nu . $$ this interaction , mediated by a virtual w-boson , is why a nucleus - with neutrons - may sometimes randomly emit an electron . so the $\beta$-decay is due to the weak nuclear force . on the other hand , the $\alpha$-decay is due to the strong nuclear force : the nucleus literally breaks into pieces , with a very stable combination of 2 protons and 2 neutrons appearing as one of the pieces ( helium nucleus ) . the two processes above are independent , and each of them can kind of be reduced to a single elementary interaction whose origin is different . this independence and different origin is why the " combined " decay , with an emission of both electron ( or two electrons ) and a helium nucleus , is extremely unlikely . such an emission of a whole atom ( which is electrically neutral but it is surely not " nothing " ! ) could only occur if several of the elementary decay interactions would occur at almost the same time which is extremely unlikely .
the concept of entanglement still applies in qft . what the reeh-schlieder theorem tells you is that there is entanglement in the qft vacuum state . so i take the question to be asking whether we could use the methods employed in proving the theorem , to decide that there is entanglement in biological systems . it seems possible , though first you have to find a biological system that you can describe in terms of quantum field theory . let 's take a much-hyped example , the microtubule . just for argument 's sake , suppose that one of the various possibilities listed in this paper made sense , and that there was an effective field theory describing the dynamics of delocalized electrons in the shell of the microtubule . then maybe you could try to prove the reeh-schlieder property for the ground state of that effective theory at biological temperatures , along these lines .
the mixing matrix tells you exactly the correspondence between the mass states and the flavor states . this is true in the quark sector , too , but unlike the quarks where the mass--flavor identification is pretty strong , it is very weak in the neutrino sector . the elements of the mixing matrix are exactly the flavor content of each mass state $$ \nu_\alpha = \sum_{i=1}^3 v_{\alpha , i} \nu_i \ , , $$ for $v$ the mixing matrix ( in the notation used in 2 ) . this image ( linked rather than imported because i can not find any license information ) shows the flavor makeup of each mass state graphically for both sectors . i believe that there is an assumption of normal hierarchy in that figure as the results do depend a little on the hierarchy . ( image from here . ) the strongest identification in the neutrino sector is between $\nu_1$ and $\nu_e$ , and even that is very rough : flavor changes could occur almost immediately , as evidenced by the recent success of the $\theta_{13}$ experiments ( daya bay , reno and double chooz ) in observing electron-anti-neutrino oscillation to more than five sigma at ranges on order of 1 kilometer .
the core of perturbative string theory has a mathematically rigorous formulation . in fact much of mathematical physics and mathematical insight into quantum field theory as such has been gained from the study of the low-dimensional qfts that constitute the worldvolume theories of the string and the various branes . for instance the axiomatization of qft in the “fqft” flavor ( roughly dual to the aqft picture ) historically originates in insights gained in the study of ( topological ) string ( namely the moore-seiberg axioms ) . on the other hand , the attempted implementations and applications of core string theory are vast and numerous , and when it finally comes to string phenomenology the usual level of rigor is just that common among practicing quantum field theorists . on the far end , deep aspects of string theory that are felt by many researchers to be of metaphysical relevance , such as the “landscape of string theory vacua” have led and are leading to speculations that are not anymore backed up by any disciplined reasoning . more in detail : the quantization of the string sigma-model may be obtained cleanly via the mathematical sound process of geometric quantization , see the references on the nlab at string – symplectic geometry and geometric quantization . the famous weyl anomaly of the string is formally understood in terms of anomalous action functionals , see for instance ( freed 86 , 2 . ) . various other obstructions to quantization ( quantum anomalies ) in the background fields for the string sigma-model such as notably the freed-witten-kapustin anomaly , have been understood in fine detail in terms of obstructions in differential cohomology , see for instance ( distler-freed-moore 09 ) . particularly well analyzed are the two special sectors of first quantized string theory , that of rational conformal field theory , which contains the example of strings propagating on lie group manifolds – the wess-zumino-witten model ; as well as the example of topological strings . rational conformal field theories indeed stand out as one non-trivial and rich class of qfts which have been subject to complete mathematical classification ( in the same sense in which mathematicians for instance do the classification of finite simple groups ) . for details on this classification see on the nlab at frs formalism . for the topological string much more is true . the topological string has effectively become a subject in pure mathematics , with its rigorous axiomatization via the tcft version of the cobordism hypothesis-theorem , its formulation as mathematical homological mirror symmetry , its relation to geometric langlands duality etc . but the fqft-axiomatics that serves to mathematically formalize the topological string is not restricted to the topological sector , it also applies to the physical string . for instance huang’s theorem shows that the familiar description of physical string via vertex operator algebra is an instance of the fqft-formalization . indeed , in frs formalism these two formalizations , vertex operator algebras ( via their modular tensor categories of representations , and tqft combined via the rigorous ads3-cft2 and cs-wzw correspondence give the classification of rational cft ) . ( in particular this says that in this low dimensionl holography and ads-cft duality is rigorous , of course this is far , far from true in higher dimensions . ) in summary this is a level of rigour with which the worldsheet 2d qft of the string is understood which is well beyond of what one typically encounters for non-trivial interacting ( non-free ) qft . and this is full non-perturbative quantum field theory ( on the worldsheet ! ) , not just the approximation in perturbation theory . from here on , also string field theory ( its action functional , that is ) , has a completely rigorous formulation in terms of operads and l-infinity algebras ( lie n-algebras for n→∞ ) . a snapshot of the state of the art of rigorous foundations of string theory as of 2011 is in ( sati-schreiber 11 ) . the above text with hyperlinks for all technical terms is also on the nlab at string theory faq -- is string theory mathematically rigorous ? .
starting with : $$u ( t , t_i ) = e^{\frac{-i}{\hbar }h ( t-t_i ) }$$ if $t_i=0$: $$u ( t , 0 ) = e^{\frac{-i}{\hbar }ht}$$ using the identity : $\sum\limits_i \left|\lambda_i\right&gt ; \left&lt ; \lambda_i\right|=\mathbb{i}$ $$u ( t , 0 ) = \sum\limits_i e^{\frac{-i}{\hbar }ht}\left|\lambda_i\right&gt ; \left&lt ; \lambda_i\right|$$ since the exponential of an operator is ( by taylor expanding ) : $e^h=\mathbb{i}+h+\frac{1}{2}h^2+\dots$ and : $h\left| \lambda_i \right&gt ; =\lambda_i \left| \lambda_i \right&gt ; $ you should be able to see that : $$u ( t , 0 ) = \sum\limits_i e^{\frac{-i}{\hbar }\lambda_it}\left|\lambda_i\right&gt ; \left&lt ; \lambda_i\right|$$
see this better introduction to the history : http://en.wikipedia.org/wiki/principle_of_least_action#origins.2c_statement.2c_and_the_controversy of course , the notion of " action " only becomes meaningful when one actually knows what the purpose of the action is - to be minimized . around 1744 and 1746 , pierre louis maupertuis figured out that this could be a way to formulate laws of physics when he generalized fermat 's 17th century " principle of least time " ( for the trajectory taken by light in any environment with a variable index of refraction ) by this proverb : nature is thrifty in all its actions . obviously , some word had to be constructed or borrowed to describe the new quantity whose importance was previously unknown to the humans ( and remains to be unknown to most humans even today ) . note that the word " actions " appeared as the only noun of the quote in the context of these minimization problems , so it became known as wirkung ( $w$ ) in german and action ( $s$ ) in english . i am actually not sure why the letter $s$ was chosen . there have been claims that leibniz had found the principle as early as in 1707 . maupertuis also wrote : the laws of movement and of rest deduced from this principle being precisely the same as those observed in nature , we can admire the application of it to all phenomena . the movement of animals , the vegetative growth of plants . . . are only its consequences ; and the spectacle of the universe becomes so much the grander , so much more beautiful , the worthier of its author , when one knows that a small number of laws , most wisely established , suffice for all movements .
that statement , that space and time are equivalent , is not really correct . in special relativity there is a distinction between spacelike and timelike events , those are events that cannot or can ( respectively ) be causally connected . this replaces the notion of " simultaneous " and " before or after " to something all inertial observers can agree on . in general relativity , this distinction is made locally - causal influence propagates locally in speeds less than c , but that constraints changes from point to point according to the local metric ( which encodes the force of gravity ) . all of this is encoded in the statement that the metric is indefinite , with a specific signature that supports one time direction . while the mathematics of gr can be generalized to other signatures , the physics cannot - the lorentzian signature is essential in correctly interpreting the theory . anyone can play all kinds of mathematical games , but those are meaningless unless you are very clear on how the calculations you perform are related ( at least in principle ) to physically observable quantities .
when an electron is promoted from the valence band to the conduction band it leaves a hole in the valence band . when the electrons falls back down from the conduction band to the valence band ( to fill the hole ) energy is released , but not necessarily as light . a photon can be emitted only when the material has a direct band gap i.e. when the electron in the lowest energy state in the conduction band and the hole in the highest energy state in the valence band have the same momentum . if this is not the case ( and for most semiconductors it is not ) the electron must interact with the crystal lattice , and energy is lost to the lattice rather than as a photon . it actually takes careful design to make diodes emit light ! because the conduction band is technically still an orbital , which means it can have " holes " , right ? no , by definition you can only have a hole in an energy band that is full e.g. the valence band . a hole is what is left behind when you remove an electron from a full band . re your last question , it depends on how localised the electron wavefunctions are . you are quite correct that the inner electrons on the silicon atoms are localised and can not jump from atom to atom . when the average distance of the electron from the nucleus is comparable to the interatomic spacing the electrons feel an " average " force from all the nuclei in the crystal and they may form delocalised wavefunctions . exactly when this happens will depend on the fine detail of the atoms and the crystal structure .
[ quote ] if i understood correctly what i have been taught so far , in qft one must find some way to quantize the fields obeying the field equation in question . [ \quote ] this is correct . in this particular case you start with the lagrangian that has schrodinger 's equation as its eom . the following turns out to be the correct one : $$\mathcal{l} = i\psi^*\partial_{t}\psi - \frac{1}{2m} ( \partial_{j}\psi^* ) ( \partial_j \psi ) . $$ ( the $j$ 's are summed over ) . now to quantize , we can proceed in the way you suggested , i.e. impose ccr on the field $\psi$ and $\psi^*$ and then express them in terms of the creation and annihilation operators . here it turns out that you can define the creation and annihilation operators to be the fourier transform of the fields $\psi$ and $\psi^{\dagger}$ , since you can show that the fourier transforms obey the same commutation relations as the creation and annihilation operators need to , i.e. $$ [ \hat{\psi} ( \mathbf{k} ) , \hat{\psi}^{\dagger} ( \mathbf{k'} ) ] = ( 2\pi ) ^3\delta ( \mathbf{k}-\mathbf{k'} ) . $$ though one thing i am unsure why your professor did was summation instead of integration , since creation and annihilation operators usually depend on a continuous degree of freedom $k$ and thus need to be integrated over . that is , if i was to take that approach , i could postulate that $$\psi ( \mathbf{x} ) = \int \frac{d^3\mathbf{k}}{ ( 2\pi ) ^3} e^{i\mathbf{k . x}}a_{\mathbf{k}} , $$ and from there show that $\psi$ and $\psi^{\dagger}$ obey the correct commutation relations if i impose the commutation relations for the $a$ 's .
how the tap work ? and how we can apply equation of continuity to the water flow when we turn the knob and when we cover the tap with thumb the tap works by changing the minimum cross-sectional area of the flow . for a given pressure difference ( upstream pressure minus downstream pressure ) flow rate is a function of minimum cross-sectional area . using your thumb would do the same thing . you can stop the flow with your thumb if you are strong relative to the force of the flow . http://people.uncw.edu/lugo/mcp/diff_eq/deproj/orifice/orifice.htm where am getting wrong with my understanding of the hydraulic analogy . probably you are misunderstanding the equation of continuity . the equation of continuity only means that the mass flow rate in equals the mass flow rate out . it does not mean that the flow in and the flow out never change . flow rate in and flow rate out can change simultanteously . your statement " on the other hand removing pipe 2 will not change water flow [ rate ] " is incorrect . removing pipe 2 will make a big difference in total flow if it is large in cross section compare to pipe 1 . it will make a small difference in total flow if it is small in cross section compare to pipe 1 . your statement " when we decrease the area of the mouth of tap by our thumb the amount of water flowing out remains same " is also incorrect . instead , the flow rate approaches zero as you make the cross-sectional area of the unblocked portion of the mouth small .
to understand this explanation , you need to understand fourier decomposition of the electromagnetic field . in any homogeneous medium , any electromagnetic field can be thought of as a linear superposition of plane waves , all in different directions . because they run in different directions , the phase delays they undergo in propagating from , say , your aperture to another , parallel plane are all different . therefore the wavefront gets " scrambled " owing to these direction-dependent phase delays . this interference between the different plane wave components of the electromagnetic field is what we commonly call " diffraction " . i further explain this idea , as well as draw some diagrams in this answer here as well as this one here . so with this introduction in mind , let 's look at your paragraph . for simplicity , assume only one transverse direction and one axial ( in the direction of propagation ) direction . let 's also assume scalar optics , i.e. that the electromagnetic field is well represented by the behaviour of one of its cartesian components , so that we can do fourier optics on scalar field . so we have a uniformly lit aperture of width $a$ . its transverse profile is therefore the function ${\rm rect} ( 2 x/a ) $ where ${\rm rect} ( x ) = 1 ; \ , |x| \leq 1$ and ${\rm rect} ( x ) = 0 ; \ , |x| &gt ; 1$ . we take a fourier transform to find the superposition weights of each plane wave component , because each such component has a transverse variation $\exp ( i\ , k_x\ , x ) $ where $k_x$ is the fourier transform variable with units of reciprocal length . the fresnel distance is , as the paragraph says , simply the axial distance needed for this spread to double the beam width . so it is a rough measure of how quickly the light spreads . so this is how the " divergence " arises from diffraction , i.e. the interference between an optical field 's plane wave components as they propagate . also $\sin\theta = k_x/k$ where $k = 2\pi/lambda$ defines the angle that this plane wave component makes with the axial direction . we take the fourier transform , we find that there is a spread of $k_x$ values such that the plane wave components most skewed to the axial direction make an angle without direction of roughly $\lambda/a$ . so , owing to these skewed components , the field 's energy spreads out . the beam width diverges slowly at first and then , after an axial distance of several fresnel distances , the divergence speeds up so that the propagation becomes well modelled by the cone of rays diverging from the centre of the aperture . indeed if you plot contours of constant intensity , they are hyperbolas which begin at right angles to the aperture but bend so that their asymptotes are the cone defined by ray theory . the fresnel distance defines how far the " knee " of the hyperbol is from the aperture . for your question : is it not that the validity holds when all objects are comfortably larger , and not smaller , than the wavelength of light ? this is in general right , but it breaks down near focuses and in situations like this where we are near and aperture and if the aperture is comparable to the light wavelength . in this case you should be able to understand from the fourier analysis the reciprocal relationship between the aperture width and the angular spread .
planck did not know bose-einstein statistics at the time around 1900 . with the existence of minimal unit , or quantization $e=hf$ , in mind , he derived the planck 's law which describe the black body radiation . two decades late , after the establishment of the bose-einstein statistics , then it is known that plank 's law is a special case of bose-einstein distribution by simple using $e=hf$ .
edit 1: i think i just understood you question : you are actually trying to calculate some sort if “internal” inductance , i.e. the contribution to the inductance of only the field inside the conductor . when calculating the flux , you have to choose a closed path over which you would want the electromotive force , and then integrate the magnetic flux over the surface limited by this path . normally the path would be the whole electrical circuit , but since you are only interested in the contribution of the internal field , you chose the return path along the edge of the wire , which is fine . now you have to choose the forward path . the forward path should be along the lines of current . the problem is that , different lines of current give different fluxes . then you can calculate the flux as a function of where , in the conductor 's cross-section , you take the forward path . but since you are using the low-frequency approximation ( no skin effect , then uniform current density ) , you can just average the forward-path dependence over the whole cross-section . then you get the missing factor two . a somewhat different argument is given in this old bulletin of the bureau of standards : the author instead weights individual flux lines as per the fraction of the conductor they enclose . this gives the same factor two . edit 2: as requested , a few clarifications . by “integrate the magnetic flux” i really mean “calculate the magnetic flux” . i used “integrate” because the calculation involves an integral : $$ \phi = \int_a \mathbf{b}\cdot\mathbf{n}\ ; \mathrm{d} a $$ where $\mathbf{n}$ is the unit normal to the surface . it is not exactly the same as “integrate the magnetic field” because of the dot product with $\mathbf{n}$ . i talked about “forward path” and “return path” because , if it is not an antenna ( as the low-frequency approximation suggests ) , a wire is usually part of a transmission line which consists of at least two conductors . assume for example that you use a pair of wires to connect a source to a load , like in the figure below ( i hope everyone can see box drawing characters ) : where the arrows represent the electric current . i assume the wire you are interested in is the top one , which i called “forward path” . the bottom wire , which i called “return path” , brings the current back to the source . taken together , these two wires form a loop and the current will make some magnetic flux through the loop . then , if you try to change the current , some electromotive force will appear because of this flux , and you will be able to model this as the effect of an inductor along the transmission line , as below : this is the self inductance of the transmission line , and is what i first thought you where trying to calculate . the self inductance of a bare wire is somewhat ill-defined . well , it is defined , but with some assumptions about the surface over which to integrate the flux , and it scales as $l\log\frac{l}{r}$ , which makes it is value per unit length diverge logarithmically when considering an arbitrarily long wire , as pointed out by zassounotsukushi and mmc . once you add the second wire , the surface over which you have to integrate the flux is clearly defined , and the inductance of the line scales like $l\log\frac{d}{r}$ , where $d$ is the distance between the wires . no more logarithmic divergence with respect to $l$ . on the other hand , it depends logarithmically on the distance between the wires , therefore you cannot just assume that the return path is just far enough to be ignored . btw , the return path is not necessarily a wire , it could be , e.g. , a ground plane . for the particular calculation you are doing ( only the contribution of the field inside the conductor ) , you use a very narrow loop where the return path is replaced by a line along the edge of the conductor , in order to enclose only the internal field . original answer below , which is somewhat bogus , as i thought you where after the total self-inductance ( including external field ) per unit length of an infinite wire . the comments of georg refer to this original version . you cannot assign an inductance to a long wire alone : you have to consider the whole circuit . the current carried by the wire has to come back in some way , and you need to know how far from your wire is the way back . assume for a moment that the wire is actually the inner conductor of a coaxial cable . you can easily calculate the linear inductance of the cable as a function of the inner an outer conductor radii . now make the outer radius go to infinity and you have a diverging self-inductance ! this means that in practice you can never assume that the way back is “far enough” to ignore it .
this is because of the symmetry of the problem . using coulomb 's law for each point of the charge distribution ( summing over each point ) $$ e ( \vec{r} ) = \frac{1}{4\pi\varepsilon_0}\int \frac{\rho ( \vec{s} ) ( \vec{r}-\vec{s} ) }{|\vec{r}-\vec{s}|^3} d\vec{s}$$ since the charge only depends on $|x|$ , you can view this as a infinite sheet of charge place at any value of $x$ . thus , if you consider a test charge at $r_1 = ( x_1 , y_1 , z_1 ) $ , the contribution from $y&gt ; y_1$ is opposite to that from $y&lt ; y_1$ , and therefore cancels . the same holds in the $z$ direction . thus , the components of the field in $y$ and $z$ are zero . the antisymmetry of $e$ , i.e. $e ( x ) = - e ( -x ) $ , comes from the fact that the charge distribution depends on $|x|$ . indeed , considering 1d , $\rho ( -x ) \cdot ( -x ) = -\rho ( x ) \cdot x = - [ \rho ( x ) \cdot x ] $ because of $\rho = f ( |x| ) $ .
by noether’s theorem , the generators of the lorentz group are the zero components of the currents , i.e. , the lorentz charges : $$s^{\alpha\beta} = s^{0 , \alpha\beta} = i\bar {\psi}\gamma^{0}\eta^{\alpha \beta}\psi = \psi^{\dagger}\eta^{\alpha \beta}\psi $$ these charges generate the lorentz transformations on the spinors by the canonical poisson brackets : $$\left \{ \psi , \psi^{\dagger} \right \}_{p . b . } = -i \mathbb{i}$$ ( with all other poisson combinations vanishing ) . the poisson brackets can be obtained from the time derivative term in the dirac lagrangian : $$i \psi^{\dagger}\partial_0\psi $$ which implies that $i \psi^{\dagger}$ is the canonical momentum of $\psi$ , thus satisfies the canonical poisson brackets . the action of the lorentz charges correctly generates the lorentz transformation : $$\delta \psi = \left \{ \frac{1}{2} \omega_{\alpha\beta }s^{\alpha\beta} , \psi^{\dagger} \right \}_{p . b . } = \frac{1}{2}\omega^{\alpha \beta}\eta_{\alpha \beta}\psi$$
if you close both switches 1 and 2 and redraw the circuit , it will look like this so bulbs a , b and c are connected in parallel . the current will flow in all the bulbs , all of them will be working
you look at the distance between two infinitesimally different points . let the two coordinate systems be x and y , where x is four numbers and y is four numbers . consider an infinitesimal displacement from y to y+dy . you know this distance in the x coordinates , so you find the two endpoints of the displacement $$x ( y ) $$ $$x^i ( y + dy ) = x^i ( x&#39 ; ) + {\partial x^i \over \partial y^j} dy^j $$ this is using the einstein summation convention--- repeated upper/lower indices are summed automatically , and an upper index in the denominator of a differential expression becomes a lower index , and vice-versa . the distance between these two infinitesimally separated points is : $$ g_{ij} ( x ) {\partial x^i \over \partial y^k} {\partial x^j \over \partial y^l} dy^j dy^l $$ and from this , you read off the metric tensor coefficients--- since this is the quadratic expression for the distance between y and y+dy . $$ g&#39 ; _{kl} ( y ) = g_{ij} ( x ( y ) ) {\partial x^i \over \partial y^k} {\partial x^j \over \partial y^l}$$ this is a special case of the tensor transformation law--- every lower index transforms by getting contracted with a jacobian inverse , and every upper index by getting contracted with a jacobian .
if you continued to read on , it goes on to say : actually , all bodies are electrified , but may appear not to be so by the relative similar charge of neighboring objects in the environment . an object further electrified + or – creates an equivalent or opposite charge by default in neighboring objects , until those charges can equalize . therefore , since all bodies are electrified or can be electrified , your statement is correct .
the issue is that by touching a wire , you are augmenting the circuit with yourself as a resistor . ( at first i wrote " inserting yourself " but as mmc 's comment pointed out , that is a misleading phrase to use . ) and whenever you change the layout of an electrical circuit , all the potentials and currents are subject to change . so the wire that is at ground potential before you touch it will not necessarily still be at the same potential after you touch it . in this specific case , i would guess that the $30\ \mathrm{a}$ value is the current that flows with only the battery and the load . you can calculate that the resistance of the load has to be $7.3\ \omega$ . if you could actually insert yourself into the circuit as shown in the diagram , you would be adding to the resistance of the circuit , which reduces the current . the resistance of the human body varies greatly depending on several factors , but a " typical " value might be on the order of $10\ \mathrm{k\omega}$ which reduces the current to a small fraction of an ampere . even with that small current , though , you are going to experience a large voltage drop because your resistance is so large compared to the rest of the circuit . in the diagram above , there would be a voltage drop of $219.8\ \mathrm{v}$ across your body . by inserting yourself in the circuit , you have prevented the wire coming out of the load from being grounded . note that if you are trying to determine whether this is a dangerous situation to be in , the value you should be looking at is the current of $0.022\ \mathrm{a}$ , not the $219.8\ \mathrm{v}$ voltage drop . that is what it means to say that it is the current that kills you , not the voltage . if you instead grab on to the wire while standing on the ground ( which seems like a more realistic situation ) , you are not actually inserting yourself in the circuit . instead you wind up with a setup more like this , in this case the wire coming out of the load resistor is still at potential zero because it is connected to ground via a zero-resistance path . ( keep in mind that this is an ideal model ; real wires do have some small nonzero resistance so in reality the wire would not quite be at zero volts . ) so the voltage difference across your body is going to be basically zero . besides , any current that flows between the circuit and the ground can do so by one of two paths , either through you or through the open wire . since your bodily resistance is much higher than that of the wire , essentially all the current will go through the wire , not through your body .
yes , a body 's mass is sufficient to cause melting . for example , suppose the moon , instead of orbiting the earth , fell onto the earth . the moon weighs about 7.35e22 kg , and is a distance of about 3.8e8 m from the earth which weighs about 6e24 kg and has a radius of around 6.4e6 m . so the difference in potential energy $u = -gm_1m_2/r$ for the earth-moon system at 3.8e8m and 6.4e6m is . $$ -g ( 7.35\times 10^{22} ) ( 6\times 10^{24} ) ( 1/3.8\times 10^8-1/6.4\times 10^6 ) = 4.5\times 10^{30}\ ; \textrm{joules}$$ where $g= 6.7\times 10^{-11}$ is the gravitational constant . the combined mass is still about 6e24 kg so the collision creates heat of $$ 4.5\times 10^{30}/6.0\times 10^{24} = 750,000\ ; \textrm{joules/kg}$$ to heat a kilogram of iron up by 1 degree kelvin requires about 460 joules . granite needs about 790 while basalt is 840 . other materials making up most of the earth have similar specific heats . so the collision will heat up the material by something around 1000 degrees kelvin . now that was just for the moon whose mass is only a tiny fraction of the earth 's . if a larger mass collided with the earth the temperature would be proportionally larger . if the body were about 4x the mass of the moon , the temperature increase would be around 4000 k and that would certainly thoroughly melt the planet . so yes , it is possible for gravity alone to melt planets .
not only is the experiment possible , a version of the experiment at the atomic level was done by einstein and de haas [ 1 ] . einstein and dehaas showed that the angular momentum inhering in the aligned electron spins in a ferromagnet can be exhibited on a macroscopic scale when the sample is demagnetize . apparently , his is the only experiment einstein ever performed himself [ 2 ] . references 1 . einstein and de haas paper [ pdf ] http://www.dwc.knaw.nl/dl/publications/pu00012546.pdf wikipedia on the einstein de haas effect https://en.wikipedia.org/wiki/einstein%e2%80%93de_haas_effect
see for instance the comments on are w and z bosons virtual or not ? . basically the claim is that the observed particle represents a path internal to some feynman diagram and accordingly there is a integral over it is momentum . i am not a theorist , but as far as i can tell the claim is supportable in a pedantic way , but not very useful .
there is a number of interesting points to this . the passage from 1 . to 2 . is not trivial . if you do the calculation , you will see that the laplacian $\nabla^2\vec{e}$ from the wave equation gives rise to the term in $\left ( \nabla\chi\right ) ^2$ you mention as well as a term in $\nabla^2\chi$ . this second term only goes away in the small $\lambda$ limit and it is the essence of the eikonal approximation . it is not a calculation you should wave away : work it out in full and implement the approximation , noticing that locally $\chi ( \vec{x} ) =\vec{k}\cdot\vec{x}+\text{slow factors}$ , where $\vec{k}$ is large . ( you will of course need to quantify " slow " . ) ( the calculation that $\vec{s}=\frac{c^2}{n^2\omega}\nabla\chi$ , on the other hand , is trivial . ) as kdn mentioned , the integral curves of $\vec{s}$ and its unit vector $\vec{s}$ are the same . this follows from the definition of integral curves : they are curves such that the vector field is tangent to them throughout . this is independent of the length of the vector . ( in terms of the curve it corresponds to a reparametrization of the " time": it changes the speed but not the direction of the velocity . ) using a unit vector means that light rays will be parametrised by path length . one can simply define light rays to be the integral curves of $\vec{s}$ and be happy about it , though of course that is simply missing the physics . the key fact about the light rays , so defined , is that they are everywhere normal to the surfaces of constant $\chi$ , i.e. the surfaces of constant phase , i.e. the wavefronts . plane waves propagate in straight lines normally to the wavefronts in free space , and so do light rays ( so defined ) . it is the normal to the wavefronts that matters when working out fresnel equations , and therefore the ( so defined ) light rays will obey snell 's law . ultimately , proving 3 . is a matter of definition : what are light rays ? write down any defining property and you will be able to prove the integral curves of $\vec{s}$ obey it . it is important to note that in isotropic media $\vec{s}$ is not only the local unit poynting vector , but it is also the local unit wave vector . ( essentially , this is the same point as above . ) intuitively , light rays ought to follow wave vectors because it is wave vectors that tell light waves where to go . in a birefringent ( not isotropic ) medium the phase propagation direction ( wave vector ) and the energy propagation direction ( poynting vector ) are not necessarily the same ( and the snell law does not apply ) . proving 4 . is an interesting exercise ( i.e. . do it ! ) but it is essentially trivial . it relies on the identity $\frac{d\vec{x}}{d\tau}=\vec{s}$ , which defines light ray curves $\vec{x} ( \tau ) $ , on judicious use of the total derivative $\frac{d}{d\tau}= ( \frac{d\vec{x}}{d\tau}\cdot\nabla ) $ , and some interesting vector calculus manipulations . ( hint : prove $ ( \nabla\chi\cdot\nabla ) \nabla\chi=\frac12\nabla\left ( \nabla\chi\right ) ^2$ . ) presumably you know by now that what you get is called the ray equation , what it means , and how to use it , or you would not have stopped there ; ) . this looks like enough to get you going but if you have more questions , do ask .
just because $f^{\mu\nu}$ has two indices does not mean that it represents a spin-2 particle . note that the metric $g^{\mu\nu}$ is a symmetric two indexed object while the em field strength $f^{\mu\nu}$ is antisymmetric . in fact , the metric $g^{\mu\nu}$ is analogous to potential $a^\mu$ in em and the field strength of gravity is the four indexed riemann tensor $r_{\mu\nu\rho\lambda}$ . what spin the field represents depends on the symmetries of the indices and the field equations that it obeys . in particular , the physical degrees of freedom of a massless field of spin $ ( a/2 , b/2 ) $ can be written in terms of dotted and undotted indices $g_{\alpha_1 , \dots , \alpha_a , \dot\beta_1 , \dots , \dot\beta_b}$ totally symmetric in the dotted and undotted indices . in order for it to be a representation of the poincare group it must satisfy the supplementary conditions $$ \begin{align} \partial^{\gamma\dot\gamma}g_{\alpha_1 , \dots , \alpha_{a-1}\gamma , \dot\beta_1 , \dots , \dot\beta_b} and = 0 \\ \partial^{\gamma\dot\gamma}g_{\alpha_1 , \dots , \alpha_{a} , \dot\beta_1 , \dots , \dot\beta_{b-1}\dot\gamma} and =0 \ . \end{align}$$ these supplementary conditions imply that each component of $g$ satisfies the klein-gordon equation . ( the only exception is the scalar case where there are no supplementary conditions because there is no indices . in this case there is just the kg equation $\box g = 0$ . ) it is also shown that these conditions imply that the field has a definite helicity of $ ( a-b ) /2$ and there is only one degree of freedom in such a field . physical fields are made of the sum of two such fields of opposite helicity this is clearly the case for the em field strength $f_{\mu\nu}$ when written in the form $f_{\alpha\beta\dot\alpha\dot\beta}= ( \sigma^\mu ) _{\alpha\dot\alpha} ( \sigma^\nu ) _{\beta\dot\beta}f_{\mu\nu}=2\varepsilon_{\alpha\beta}\bar{f}_{\dot\alpha\dot\beta} + 2\varepsilon_{\dot\alpha\dot\beta}f_{\alpha\beta}$ , so the field strength decomposes into the sum of two massless fields carrying helicity $\pm1$ . there is a good description of the field representations of the poincare group in section 1.8 of ideas and methods in superspace and supergravity . section 1.8.3 deals with the massless representations applicable in the two cases raised in your question . section 1.8.4 has the examples of massless scalar , spin-1/2 , em ( spin-1 ) , spin-3/2 and linearized gravity ( spin-2 ) .
a wheatstone bridge is only a very sensitive way of finding a match between two resistances . you have got to have a nice , well calibrated variable resistance on the opposite side to the unknown resistance . the accuracy with which you can judge the unknown is limited by the calibration accuracy of the variable resistance . using ohm 's law to find an unknown resistance is a great way to do it . you just match the voltage you apply to the unknown based on the range of resistance you expect to encounter . suppose you had a short piece of heavy copper buss bar you wanted to measure the resistance of . i have a nice ac welder that can put out 400 amps for brief periods . i will short the output with the buss bar , put 400 amps to it and while the current is flowing , i will measure the voltage difference between each end of the bar . with such a small resistance in the unknown resistor , it takes a lot of amps to generate a nice measurable voltage . conversely , the mega high resistance of thick layers of insulation can be measured by putting a couple of tens of kilovolts to it and reading the milliamperage . i use neon sign transformers and large resistor banks for stepping down and for amp measurements . i have a series of calibrated shunts that are useful . they run a millivolt per amp . you read . 5 volts of potential from one end to the other of the shunt - that is 500 amps . i have a big brass shunt on my car dash that gives me a reading of starting amps . i ran old welding cable to bring the starter amps into the passenger compartment . i have a giant switch on the dash , custom made of 1/4" x 2" copper bussbar in several pieces up to a foot long . no auto thief would ever have the balls to close that switch . i have skulls and red warnings . high voltage , etc .
i think the answers in the duplicate have covered most of the key points . i will just add to them the feeling of weight , strain , stress etc . is due to the differential force acting on different points of the body . even while floating in curved space-time you may feel strain , weight etc . because your body is not point size , the particles of your body will be accelerated in different directions relative to each other . to maintain the rigidity of your form , the body exerts a force to hold itself together . this force is weight , strain , stress etc . so irrespective of your state of motion from any frame of reference , if the particles of body are being forced in different directions relative to each other , your nervous system will register this as weight , stress , strain in the respective parts of the body . the cause of relative acceleration between parts of your body is twofold- tidal acceleration due to the structure of space time , the action of non gravitational forces on the different particles in your body . so even in non-inertial frames without the effects of curved-space you might feel this weight , strain etc . the only time you will feel absolutely weightless , is if the relative acceleration between all the particles in your body is equal to 0 . due to the fact that the earth is curved and that you are not a point particle , there is a slight inward lateral strain even when you free fall in vacuum on earth .
there are in fact two field lines that depart each charge headed towards the other . these lines meet at the origin ( the mid-point of the two charges ) , where the field is zero , and vanish there . there are also two other lines , which are born at the origin and depart along the vertical axis . thus , formally , two lines go in and two lines go out , so no lines actually die in empty space . these lines are actually a limiting case of lines that leave the point charges at a small angle $\epsilon$ from the intercharge axis ; these lines make increasingly close approaches to the origin as $\epsilon\rightarrow0$ , and then they shoot off to infinity , increasingly close to the vertical axis . ( if you are sharp , you will notice there is actually an infinity of such lines , since there is also lines that go off perpendicularly to the screen and at any angle in between . thus my " two-for-two " argument is not actually quite right . can you see the limiting behaviour that makes it right ? ) pictures of this were relatively hard to find , but you can see them in this wolfram web app : you also have to consider one key point : at the origin , the field is zero , so actually there should be no field lines through it . or , more formally , the density of field lines should be zero . this comes about in that the angle $\epsilon$ should be really small for the lines to actually approach the origin . you should then plaster the diagram with lines leaving equiangularly at angle $\epsilon$ from each charge , and that will mean a lot of lines on the " outside " of the charges . ultimately , though , the lesson is that individual field lines are not that important , and it is the set of lines , equiangularly leaving the charges ( in 3d ! ) , that makes a physically relevant diagram . and even then , field line diagrams are only of limited utility in understanding electric fields , mostly because they only incorporate with the utmost difficulty the superposition principle , which is at the real heart of classical electromagnetism .
one may prove for an arbitrary rigid body ( and wrt . to an arbitrary choice of pivotal point for the rigid body ) that the three moments of inertia $i_x$ , $i_y$ , and $i_z$ , around the three principal axes ( which we will call $x$ , $y$ , and $z$ ) satisfy the triangle inequality , $$\tag{1} i_x +i_y ~\geq~ i_z , \qquad i_y +i_z ~\geq~ i_x , \qquad i_z +i_x ~\geq~ i_y . $$ in other words , if a semi-positive definite symmetric real $3\times 3$ matrix with non-negative eigenvalues $i_x$ , $i_y$ , and $i_z$ does not satisfy the triangle inequality ( 1 ) , it does not represent a physically possible distribution of mass . ( the proof follows straightforwardly by writing down the definition of moment of inertia . ) moreover , one may show that such three eigenvalues $i_x$ , $i_y$ , and $i_z$ that satisfy ( 1 ) may be reproduced by a solid ellipsoid with a unique choice of non-negative semi-axes $a$ , $b$ , and $c$ ( unique up to the scaling of the total mass $m$ ) . $$ \frac{2}{5}m a^2~=~i_y +i_z -i_x~\geq~0 , $$ $$ \frac{2}{5}m b^2~=~i_z +i_x -i_y~\geq~0 , $$ $$ \tag{2} \frac{2}{5}m c^2~=~i_x +i_y -i_z~\geq~0 . $$
@ron maimon 's answer is the most elegant and @ja72 's is the most general . . . but there is a dirty " trick " you can do with levers : treat them as ideal leavers with an extra weight concentrated at the centre of mass . this works as long as you do not need to take bending into account . if the density is $ \rho ( \ell ) $ as a function of horizontal position $\ell$ then the torque is $$ \tau = \int \mathrm{d}\ell\ \rho g \ell = m g \frac{\int \mathrm{d}\ell\ \rho \ell}{\int \mathrm{d}\ell\ \rho} = m g \ell_{\text{cm}} $$ where $ \ell_{\text{cm}} $ is the centre of mass and $ m $ is the total mass .
in gilmore 's book : http://www.amazon.com/single-particle-detection-measurement-gilmore/dp/0850667550 chapter 5 is on solid state ionization and it is use in detectors . that was hanging around the lab when i was working in particle physics . i would give that one a try . it is a bit old these days but the principles are there .
if you look at the structure of the circuit on the left , and where voltage must be , you will see that the wire connecting c1 and c2 ensures that the plates connected to point a are at one voltage , let 's call it va , and the other plates connected to voltage vb . so this can be thought of as two batteries , each with voltage differential of va-vb in parallel . for the second circuit , the two capacitors are in series with respect to the battery ( as mentioned by @frankh )
it is same if radius of axle is same for both . torque is defined as moment of force about axis of rotation as : $$\tau=\vec r\times \vec f$$ $\vec r$ is the position vector of point of application of force from axis of rotation . in your case it it $\tau=rf$ where $r$ is radius of axle of wheel .
one weber is a unit of magnetic flux – it is $$\int\vec b\cdot d\vec s . $$ so it depends on the area $s$ one integrates over . that is why it is misleading to associate a certain number of webers with the adjectives " strong " and " weak " . the magnetic flux is not really an intensive quantity : it is an extensive quantity of a sort . the larger area you integrate over , the more webers you get . one tesla is one weber per squared meter . so you may imagine $x$ webers as $x$ teslas integrated over a squared meter and use the table for $x$ teslas from wikipedia . of course , you may choose a different area than one squared meter ; the numbers must be changed appropriately . you may get rid of the dependence on the area if you consider the magnetic flux around a magnetic monopole – which has not been observed yet . the dirac quantization considerations tell you that $e\int_d\vec b\cdot d\vec s$ must be a multiple of $2\pi\hbar$ which means that the elementary magnetic monopole has the magnetic flux ( over any spherical surface that surrounds it ! ) $2\pi\hbar/e$ , about $10^{-15}$ weber , calculate the exact number if you want . note that dimensionally , one weber is one joule per one ampere ( and many equivalent ways to write it ) so $e$ times the magnetic flux is one ampere-second ( coulomb ) times one joule over one ampere . the amperes cancel and you are left with joule-seconds , the unit of $\hbar$ .
time zones moving on from the calendar to time , we recommend the abolition of all time zones , as well as of daylight savings time , and the adoption of atomic time—in particular , greenwich mean time , or universal time , as it is called today . like the adoption of a modern calendar , the embrace of universal time would be beneficial . for example , the adoption of universal time would give new flexibility to economic management in the vast east-west expanse of russia : everyone would know exactly what time it is everywhere , at every moment . opening and closing times of businesses could be specified for every class of business and activity . if thought desirable , banks and financial institutions throughout the country could be required to open and to close each day at the same hour by the world time . this would mean that bank employees in the far east of russia would start work with the sun well up in the sky , while bank employees in the far west of russia would be at their desks before the sun has risen . but , across the country , they could conduct business with one another , all the working day . ( this would have a second benefit : at least in the far east and far west , the banks would be open either early , or late , convenient for those who are working “sunlight hours , ” such as farmers . ) with universal time , agricultural workers , critically dependent on the position of the sun , could rise with the sun , without producing any impact on other aspects of cultural and economic life . the readings on the clocks , and the date on the calendar , would be the same for all . but , times of work would be attuned with precision to russia’s local and national needs . china already has adopted a single time zone for the same purposes . and all aircraft pilots , worldwide , use universal time exclusively , for exactly the same reason that we are advocating its broad adoption—plus avoiding collisions . moscow could introduce both a simplified calendar , identical each year ( harmonized with the seasons by rare full-week adjustments at year’s end ) , and universal time , which would abolish the international date line , making the date and the time identical everywhere , including alaska and the farthest eastern regions of russia . there , and also in the center of the pacific ocean , the date would change at 00:00:00 , just as the sun passed overhead . source and context . also see this and this for discussion . dst one of the biggest reasons we change our clocks to daylight saving time ( dst ) is that it reportedly saves electricity . newer studies , however , are challenging long-held reason . a report was released in may 2001 by the california energy commission to see if creating an early dst or going to a year-round dst will help with the electricity problems the state faced in 2000-2002 . you can download an acrobat pdf copy of the staff report , effects of daylight saving time on california electricity use , publication # 400-01-13 , ( pdf file , pages , 5.2 megabytes ) . the study concluded that both winter daylight saving time and summer-season double daylight saving time ( ddst ) would probably save marginal amounts of electricity - around 3,400 megawatt-hours ( mwh ) a day in winter ( one-half of one percent of winter electricity use - 0.5% ) and around 1,500 mwh a day during the summer season ( one-fifth of one percent of summer-season use - 0.20% ) . winter dst would cut winter peak electricity use by around 1,100 megawatts on average , or 3.4 percent . summer double dst would cause a smaller ( 220 mw ) and more uncertain drop in the peak , but it could still save hundreds of millions of dollars because it would shift electricity use to low demand ( cheaper ) morning hours and decrease electricity use during higher demand hours . the energy commission has also published a new report titled the effect of early daylight saving time on california electricity consumption : a statistical analysis . publication # cec-200-2007-004 , may 27 , 2007 . ( pdf file , 592 kilobytes ) a more recent study concludes that daylight saving time in indiana actually increases residential electricity demand . that study titled " does daylight saving time save energy ? evidence from a natural experiment in indiana " . ( pdf file ) looked at the electricity use when portions of the state finally started to observe dst . before the new extended dst , portions of indiana did not observe dst . some have wondered whether this study would be true for the entire united states . initial analysis by staff of the california energy commission says a similar study may not yield the same results for california because : the use of residential air conditioning is relatively low in indiana , and the saturations are low . where as california has high usage of air conditioning in the summer . heating use is relatively high in indiana , while it is relatively low in california . the diurnal variation in temperature is low while california is very high . indiana is located in western edge of the same time zone as maine and florida , but the sun actually comes up at an earlier time than those other two states . indiana 's north-south location will affect how long the days are in the summer and might very well lead to different results in different areas . so , while the analysis is of interest to indiana , it is conclusions may not be totally correct for california or the rest of the country . the first national study since the 1970s , was mandated by congress and was done by the u.s. department of energy . the doe study can be downloaded at : http://www1.eere.energy.gov/ba/pba/pdfs/epact_sec_110_edst_report_to_congress_2008.pdf ( pdf file , 285 kb ) [ actually : here ] the key findings in the report to congress are : the total electricity savings of extended daylight saving time were about 1.3 tera watt-hour ( twh ) . this corresponds to 0.5 percent per each day of extended daylight saving time , or 0.03 percent of electricity consumption over the year . in reference , the total 2007 electricity consumption in the united states was 3,900 twh . in terms of national primary energy consumption , the electricity savings translate to a reduction of 17 trillion btu ( tbtu ) over the spring and fall extended daylight saving time periods , or roughly 0.02 percent of total u.s. energy consumption during 2007 of 101,000 tbtu . during extended daylight saving time , electricity savings generally occurred over a three- to five-hour period in the evening with small increases in usage during the early- morning hours . on a daily percentage basis , electricity savings were slightly greater during the march ( spring ) extension of extended daylight saving time than the november ( fall ) extension . on a regional basis , some southern portions of the united states exhibited slightly smaller impacts of extended daylight saving time on energy savings compared to the northern regions , a result possibly due to a small , offsetting increase in household air conditioning usage . changes in national traffic volume and motor gasoline consumption for passenger vehicles in 2007 were determined to be statistically insignificant and therefore , could not be attributed to extended daylight saving time . source and further references and context
the basic classical interpretation of this expression is that em waves , or any wave described by a null ' wave-4-vector ' like gravitational waves , travel at the speed of light . you can easily see this by considering the classical definition of the wave-4-vector $k = ( \frac{\omega}{c} , \mathbf{k} ) $ , where $\mathbf{k}$ is the usual wavevector in 3-space . this definition is required so that the function $$f ( x ) = a e^{i k_{\mu} x^{\mu}}$$ describes a wave propagating with frequency $\frac{\omega}{2\pi}$ and wavelength $\frac{2\pi}{|\mathbf{k}|}$ . the null condition translates to $$ k_{\mu}k^{\mu} = 0 \rightarrow \frac{\omega}{|\mathbf{k}|} = c . $$ in other words the phase ( and group ) velocity of the waves is equal to the speed of light . this makes sense , since the wavevector points along the direction of propagation in spacetime , and you know that light follows null geodesics . this is obviously independent of the particle interpretation . however , if you want to describe the waves as associated with particles due to qm considerations , it also follows that those particles must be massless .
here is a link that allows you to simulate the field lines and equipotentials of both point charges and charged plates : http://www.falstad.com/emstatic/ perhaps you will find what you are looking for here .
provided its a general situation , the heat will circulate through the pipe ( and the water of course ) but most of it will be radiated out . at no point will a measurable amount of water reach a $1000 ^\circ c $ . even if some of the water reaches that temperature or close to it , it will lose it to the colder surroundings . in the end at some point this heat leak out to the surroundings . assuming that somehow there is no heat loss , eventually the whole pipe will approach $1000^\circ c$ as there is an inflow of energy but no place for it to escape to .
in the early days of radio , the resonance of the antenna in combination with its associated inductive and capacitive properties was indeed the item which " dialed in " the frequency you wanted to listen to . you did not actually change the length of the antenna , but by changing the inductor ( a coil ) or capacitor connected to the antenna you tuned the resonance . the output signal is an alternating voltage , and by rectifying it with a diode ( called a " crystal " then . . ) you could extract a signal modulated as a varying amplitude of the carrier wave . all this without any battery ! : ) but actually the antenna in a normal modern radio is not the component that " dials in " the selected broadcast frequency . the antenna circuit should indeed have a resonance within the band of frequencies you are interested in but this wide-band signal is then mixed with an internally generated sinusodial signal in the radio in an analog component , this subtracts the frequencies and lets the rest of the radio operate on a much easily handled frequency band ( called the intermediate frequency ) . it is in the mixer you tune the reception in a modern superheterodyne radio receiver . it is much easier to synthesize an exact mixing frequency to tune with than to change the resonance of the antenna circuit . the rest is not really physics , but the difference between an analog and a digital radio comes in the circuits after this and basically an analog radio extracts a modulation from the intermediate frequency which is amplified and sent to the speakers or radio output . in a digital radio , the signal represents a digital version of the audio , just like a wav or mp3-file on a computer is a digital representation which can be turned back into an analog signal you can send to a speaker . the benefit of this is that the digital signal requires ( potentially ) less bandwidth in the air so you can fit more signals in the same " airspace " and that the digital signal can be less susceptible to noise . i write " can " , because unfortunately many commercial digital radio/tv stations do not do this to improve the viewing or listening quality but just to fit in more content . let me reiterate that in a " digital " radio , the component that selects the reception frequency is still analog but the mixing ( tuning ) frequency is digitally controlled and selected . there is also a very interesting thing called software defined radio , sdr , which is the principle where the intermediate frequency ( or in some cases the antenna frequency directly ) is turned into a digital signal and demodulated by a signal processor which is completely software-upgradeable . since it is much easier to program new software than to solder electronic components around , this created large interest in the radio hobby community where you can completely change the properties of a radio receiver just by downloading someone else 's software from the net or write a new one yourself . if you include sdr , and apply it without any intermediate frequency ( take the antenna directly to an analog/digital converter and into a signal processor ) , you do indeed have a purely software-way of tuning your source like you ask for , although this is not how the most common digital radios work currently .
look up the catenary curve properties and notice that the weight of the cable causes a reaction in both $x$ and $y$ axis as you noted . the vertical components of the reactions sum up to the weight of the cable , and the horizontal components are such as the tension being tangent to the shape of the cable . now what you " feel " when you move the support is the inertial force of moving the cable which i suppose it too small for you to feel . if you try with a heavy steel cable , and you accelerate in the same order of magnitude as gravity then you will feel inertial resistance . notice that you also feel an effective stiffness as you pull on the cable as the sag decreases and the incident angle creates higher horizontal reactions . in addition you might have elastic deformation also decreasing the above stiffness by $1/k_{eff} = 1/k_{elastic} + 1/k_{geometric}$ .
to change the past you require a closed timelike curve . stephen hawking proved that closed timelike curves cannot be created in a finite system without using exotic matter . i think the proof was in his paper on the chronology protection conjecture but i do not have access to the paper at the moment . this far we have a reliable grasp on whether causality can be violated , but from here things get speculative . it seems likely that no infinite structures exist , if only because the universe has not existed for an infinite time ( though if there was a big bounce we could be wrong about this ) . the big question is whether exotic matter exists . the trouble is that there is no proof that exotic matter either exists or does not exist . it is like negative mass - there is nothing to stop you plugging a negative value for mass into the gr equations , but that does not mean it is a physically meaningful thing to do . we have never observed exotic matter , but that does not necessarily mean it does not exist . the chronology protection conjecture that i mentioned above is the closest we have to a mathematical approach to constraining causality violations , but it is just a conjecture and has not been proven - though it has not been disproven either . at the moment we simply do not understand the physics well enough to give a definitive answer .
there are many different software programs that will do what you want , but planetarium software is very complex and takes a long time to learn before you can use it at its best . i am a technical writer and software support person for starry night software and , as part of my job , write a weekly article for space . com which is almost always illustrated using starry night software . i have been using various versions of starry night for over a decade and can generally get it to do what i want for my illustrations , though i sometimes need to fudge things a bit with other software . i doubt whether you will ever find one program which will do everything you want , so my suggestion is to do what i did : find a program that does most of it , and then work with that program until you can get the best from it .
let 's see two charges $q_1$ and $q_2$ , we have to find electric field at some point between them . let 's assume $d$ to be the distance between the charges , which is constant and $x$ ( from the center of the line joining the two charges , $\frac{d}{2}$ from the chagres ) to be the distance where we want to measure electric field at . now , $\large e_{q_1x} = \frac{1}{4 \pi \epsilon} \frac{q_1}{\left ( \frac{d}{2}+x\right ) ^2}$ and $\large e_{q_2x} = \frac{1}{4 \pi\epsilon} \frac{q_2}{\left ( \frac{d}{2}-x\right ) ^2}$ total electric field at $x$ , $$e_{x} = \frac{1}{4 \pi \epsilon} \frac{q_1}{\left ( \frac{d}{2}+x\right ) ^2} + \frac{1}{4 \pi\epsilon} \frac{q_2}{\left ( \frac{d}{2}-x\right ) ^2}$$ i tried to make it as generalized as possible so you do not have to face problems with such type of problems in future . i hope it helps you and good luck for your exam ! i tried to attach a picture but because of low reputation i could not , sorry .
because you were also in orbit around the sun with the earth and still have that velocity . you may be imagining this in terms of stepping off of a slow moving vehicle on the earth : you jump off , you come to a stop relative the ground and watch the trolley car go it is merry way . but that is a feature of friction between you and the ground . there is no such thing as a absolute reference frame in the universe and when you " leave the earth " you do not come to stop relative anything so that you can watch the earth fly away . newton 's laws apply here : " a body in motion ( that is the you or the planet ) will continue in motion unless acted on by an external force " . you just keep going except for changed induced by your drive .
i think perhaps what you are missing is in the " skipping through the commutator " part . do you understand where we get this equation ( try computing it yourself , if not ) : $$a_{-}a_{+} = \frac{1}{2 \hbar m \omega} [ p^{2} + ( m\omega x ) ^{2} ] - \frac{i}{2\hbar} [ x , p ] $$ now , the canonical commutator , i am sure you noticed ( as it is boxed on the same page in griffiths ) is $ [ x , p ] = ih$ . insert this into the above equation and note that we now have : $$a_{-}a_{+} = \frac{1}{2 \hbar m \omega} [ p^{2} + ( m\omega x ) ^{2} ] + \frac{1}{2}$$ all you need to do from there recognize the first term as $\frac{1}{h\omega}h$ . looking at the original equation , we factored $ [ p^{2}+ ( m\omega x ) ^{2} ] $ , so we can replace this with a+a− . under this , could not we just say that $h=\frac{1}{2} ( a_{+}a_{−} ) $ careful here . . . remeber that $p$ and $x$ in this expression ( and in the hamiltonian generally ) are operators , not scalars . this is why our " intuitive guesses " of $a_{\pm}$ are not exact factors of $ [ p^{2}+ ( m\omega x ) ^{2} ] $ , and why the canonical commutator above is important . edit : i just noticed that griffiths does include this intermediate step in computing $a_{-}a_{+}$: $$a_{-}a_{+} = \frac{1}{2 \hbar m \omega} [ p^{2} + ( m\omega x ) ^{2}-im\omega ( xp-px ) ] $$ notice that if $x$ and $p$ were scalars , the rightmost term would be 0 , and your intuition about $a_{-}$ and $a_{+}$ being " factors " would be correct . once you realize they are operators , however , it is obvious that we need to substitute $ [ x , p ] = xp-px = ih$ .
you conjecture is correct . one can relate the 2d ising model with the bond correlated percolation model . the details are in the paper percolation , clusters , and phase transitions in spin models . the basic idea is to consider interacting ( nearest neighbor ) spins as forming a bond with a certain probability . one can then show that the partition function of the ising model is related to the generating function of the bond-correlated percolation model . the above paper demonstrates that the bond-correlated percolation model has the same critical temperature and critical exponents as the 2d ising model . however , the values of $t_c$ and the critical exponents seem to be dependent on exactly how one defines a bond . see section iii . a . 1 in universality classes in nonequilibrium lattice systems ( or arxiv version ) . nonetheless your intuitive picture that there would be spanning clusters below $t_c$ and no such clusters above $t_c$ remains valid . edit 21 may 2012 i found a pedagogical paper that discusses this issue .
as steve said , you need to specify the frequency you are looking for . that is because any apparatus will have a given temporal response , which determines what time-scale you can resolve . for example , the response of the probe you cited is of the order of the microsecond according to the component details . if the field is oscillating faster than that , then you will not see the oscillations . this sets the maximum frequency at about 1 mhz , which is way higher than the usual 50 hz or 100 hz of ac currents in a household .
to keep with your for simplicity 's sake =p , let 's say the force exerted on the platform while the ball is in contact with the platform is always the average force ; that is , at no point in time while the ball is in contact with the surface is the force greater than any other point in time it is in contact with the surface . let 's also assume the surface to be 100% elastic ( elasticity coefficient of 1 ) , so that the ball would bounce exacty as high as when it began ( 100 meters ) . then when the ball leaves the surface , it must have a veloctiy $v = 63$ m$/$s . that means it is change in momentum $p = \delta v \cdot m$ and since $\delta v = 63 \cdot 2 = 126$ ( since it orginally was going down with 63 m$/$s and now is going up with 63 m$/$s ) then we have $p = 1260$ . now force $f_{avg} = \delta p/\delta t$ ( note only use this for a constant force , otherwise we need calculus ) . expanding , we get $f = m ( v_{f} - v_{i} ) /t$ which i believe is exactly what you have
this a bit of a sketch ; the $s$-matrix acts on shift the state or momentum state of a particle . a state with two particle states $|p , p’\rangle$ is acted upon by the $s$ matrix through the $t$ matrix $$ s~=~1~–~i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) t $$ so that $t|p , p’\rangle~\ne~= 0$ . for zero mass plane waves scatter at almost all energy . the hilbert space is then an infinite product of n-particle subspaces $h~=~\otimes_nh^n$ . as with all hilbert spaces there exists a unitary operator $u$ , often $u~=~exp ( iht ) $ , which transforms the states s acts upon . $u$ transforms n-particle states into n-particle states as tensor products . the unitary operator commutes with the $s$ matrix $$ sus^{-1}~=~ [ 1 – i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) t ] u [ 1~+~i ( 2π ) ^4 \delta^4 ( p~–~p’ ) t^\dagger ] $$$$ = u~+~i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) [ tu~–~ut^\dagger ] ~+~ [ ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) ] ^2 ( tut^\dagger ) . $$ by hermitian properties and unitarity it is not difficult to show the last two terms are zero and that the s-matrix commutes with the unitary matrix . the lorentz group then defines operator $p_\mu$ and $m_{\mu\nu}$ for momentum boosts and rotations . the $s$-matrix defines changes in momentum eigenstates , while the unitary operator is generated by a internal symmetries $a_a$ , where the index a is within some internal space ( the circle in the complex plane for example , and we then have with some $$ [ a_a , ~p_\mu ] ~=~ [ a_a , ~m_{\mu\nu} ] ~=~0 . $$ this is a sketch of the infamous “no-go” theorem of coleman and mundula . this is what prevents one from being able to place internal and external generators or symmetries on the same footing . the way around this problem is supersymmetry . the generators of the supergroup , or a graded lie algebra , have 1/2 commutator group elements $ [ a_a , ~a_b ] ~=~c_{ab}^ca_c$ ( $c_{ab}^c$ = structure constant of some lie algebra ) , plus another set of graded operators which obey $$ \{{\bar q}_a , q_b\}~=~\gamma^\mu_{ab}p_\mu , $$ which if one develops the susy algebra you find this is a loophole which allows for the intertwining of internal symmetries and spacetime generators . one might think of the above anti-commutator as saying the momentum operator , as a boundary operator $p_\mu~= -i\hbar\partial_\mu$ which has a cohomology , where it results from the application of a fermi-dirac operator $q_a$ . fermi-dirac states are such that only one particle can occupy a state , which has the topological content of $d^2~=~0$ . this cohomology is the basis for brst quantization .
as guillermo angeris correctly pointed out , this is essentially a numerical roundoff problem , not a physical situation . as a physical example , there are sungrazing comets that get very close to to sun , yet they maintain their original elliptical ( or hyperbolic ) orbit , without the orbit precessing a full third of a circle as you seem to be seeing . computationally , there are a few interesting issues . as kyle pointed out in a comment , many integration schemes are indeed unreliable in that roundoff error ( which is always present in floating-point computations ) can accumulate in a runaway feedback . indeed i often advise using leapfrog methods over euler ( used by box2d ) or even runge-kutta ( see for instance what is the correct way of integrating in astronomy simulations ? over at the computational science stackexchange ) . however , i suspect your problem is even simpler , in the sense that even an unstable numerical scheme should work for one or two orbits . given that everything is going wrong in just one pass , it seems that your timesteps are simply too large . a brief glimpse at the box2d documentation suggests you do not change the timestep mid-simulation , so i presume you are just using a good value to simulate the whole process in reasonable time . the problem is that when gravitating bodies get close in their orbit , they move quickly , sometimes very quickly . the way the code works is it updates each object 's position and velocity at each timestep , where the new velocity is determined by the force . as far as i can tell , this is done in line 206 of b2Island.cpp ( v . 2.2.1 ) : v += h * (b-&gt;m_gravityScale * gravity + b-&gt;m_invMass * b-&gt;m_force);  without looking at your code , i am guessing you simply calculate the gravitational force the body should feel at that moment , and have the simulation chug away . the problem is this moves the orbiting object in a straight line for the next timestep , and that straight line takes it too far away from the gravitating mass for that mass to properly curve its orbit into a closed ellipse . the quick schematic below shows the blue object moving to the tip of the red arrow , rather than staying on the path . physically , your timestep should be smaller than any timescale you encounter in the problem . now for an orbit conserving angular momentum , the product of the orbiting body 's mass , tangential velocity $v$ , and distance from the other object $r$ should be constant : $v \sim 1/r$ . at the same time , the acceleration $a$ it feels is given by newton 's law of gravity : $a \sim 1/r^2$ . so one natural timescale in this problem is $$ t \sim \frac{v}{a} \sim \frac{1/r}{1/r^2} \sim r $$ ( omitting dimensional constants ) , which goes to show that if your timescale is just barely small enough and then you tweak the orbit so as to half the periapsis distance ( distance of closest approach ) , then you would expect to need timesteps at least twice as small in order to preserve the integrity of the simulation .
the resonances are due mainly to plasma oscillations . but these metals are not simple plasmas ; there are interband transitions as well as collective plasma oscillations , and the frequencies of these various excitations can overlap causing interactions . in silver , there is no interband transitions at the plasm frequency , so the plasma resonance is not damped , whereas in other metal , that is not the case . even in silver interband transitions play a role in shifting and sharpening the resonance .
yes , with a relatively inexpensive solar filter in front of the telescope almost any telescope can be used . the solar filter can be based on bopet ( trade name " mylar" ) . to view flares and prominences a much more expensive ( on the order of usd 1000 for a 8" reflector ) hydrogen-alpha filter is needed .
i think lhs of eqn 2.7 is normalized , meaning $\frac{1}{z}\int\mathcal{d}\phi \mathcal{o} exp\left ( -s_{e}\left [ \phi\right ] \right ) $ evaluated on $\mathcal{m}_{n}$ if you put $\mathcal{o} =1$ , you get 1 . but $z$ itself is proportional to the correlation function of the two primary fields ref : http://arxiv.org/abs/hep-th/0405152 sect iiia hope this is useful
quantum field theory is a general framework . there are many different kinds of field theories , the most well-known of which are qed ( quantum electrodynamics ) and qcd ( quantum chromodynamics ) . qed has been shown to agree incredibly well with experiment . the anomalous magnetic dipole moment of an electron , as computed using qed , agrees with experiment to 10 significant figures , the best agreement between theory and experiment of any physical theory in history . the large coupling constant of the strong force makes qcd much more difficult to deal with theoretically , and confinement makes it difficult probe experimentally .
in the west the vast majority of towels are made from cotton , and cotton is basically cellulose . the surface of cellulose is fairly reactive ( the bulk is not unless you are a termite ! ) and will react with water to produce surface hydroxyl groups and negatively charged groups . both of these lower the contact angle of water on the fibres and hence increase capillary forces and wicking . towels from the factory with have been treated with materials not unlike fabric conditioner . this makes the towel feel nice and soft , but it make the surface more hydrophobic and therefore less able to absorb water by wicking . it takes a few uses for this surface treatment to wear off . you can do the experiment for yourself simply by using fabric conditioner when you wash the towel . if you compare two towels , one laundered with fabric conditioner and one not , then it will be immediately obvious that ( a ) the conditioned towel feels softer and nicer but ( b ) the unconditioned towel dries you better .
yes the limit of the velocity of light in vacuum can be described as the corner stone of special relativity . if it goes , special relativity goes , but it is important to keep in mind about context , and what " goes " means . in general physics theories are able to absorb small corrections to up to then absolute norms , and also in general , any new theories which emerge will include the thousand of successes of describing data by the use of special relativity . ( if this measurement is not due to a systematic error ) . after all special relativity is irrelevant to the physics of driving one 's car . the mechanics within the accuracies of speeds attained are newtonian and special relativity in the low speed limits becomes newtonian smoothly . this new measurement has a special context ( neutrino behaviour ) and a very small effect , so we wait for validation and see what theorists will devise to describe the situation . as for 1 ) . special relativity is one of the foundation stones of modern physics , not the only . another is quantum mechanics , then we go to field theories , general relativity , etc . 1 ) is not accurate . special relativity is a necessary but not sufficient condition for part of the modern theoretical framework . an equally important necessary condition for the physics theories we use to describe data is quantum mechanics . ( not sufficient ) since it is only in this one subset of thousands of reactions that a possible break down of the speed of light limit might be seen to be broken , certainly the damage is not as great as taking away a corner stone from a building .
yes , of course , symplectic groups describe generalized situations that reveal the uncertainty principle . the reason for the relationship is that the symplectic groups are defined by preserving an antisymmetric bilinear invariant , $$ m a m^t = a $$ where $m$ is a matrix included into the symplectic group is the equation holds and $a$ is a non-singular antisymmetric matrix . where does the uncertainty principle enter ? it enters because $a$ may be understood to be the commutator ( or poisson bracket ) of the basic coordinates $x_i , p_i$ on the phase space . if we summarize $n$ coordinates $x_i$ and $n$ coordinates $p_i$ into a $2n$-dimensional space with coordinates $q_m$ , their commutators are $$ [ q_m , q_n ] = a_{mn} $$ with an antisymmetric matrix $a$ . consequently , the symplectic transformations may be defined as the group of all linear transformations mixing $x_i , p_i$ , the coordinates of the phase space , that preserve the commutator i.e. all the uncertainty relations between the coordinates $q_m$ . curved , nonlinear generalizations of these spaces are known as " symplectic manifolds " and nonlinear generalizations of the symplectic transformations above are known as " canonical transformations " . i think it does not make sense to talk about this relationship too much beyond the comments above because the relationship is in no way " equivalence " . one may say lots of things about related concepts but they are in no way a canonical answer to your question – they do not follow just from the idea of the " relationship " itself . i just wanted to make sure that a relationship between mathematical structures on both sides , especially the antisymmetric matrix , certainly exists .
the forces are never balanced , as there is only ever one force - gravity . the key is to remember newton 's second law : $f = ma$ . force and acceleration are paired , not force and velocity . knowing just an object 's current velocity tells you nothing about what forces are acting on it . there are two ways to see how the velocity goes to zero . either the initial impulse ( instantaneous transfer of momentum ) is depleted by a force applied over time , $$ m \delta v = \int f \ \mathrm{d}t , $$ or the initial kinetic energy is depleted by a force applied over a distance , $$ \frac{1}{2} m \delta ( v^2 ) = \int f \ \mathrm{d}x . $$ in both cases , the force of gravity is acting continuously to slowly cancel the initial velocity , and there is nothing that turns off this force at the apex of the trajectory .
first remove the " process " step from your diagram so that you are comparing two beams of light with $\cos ( \omega t ) $ . let 's say , for the sake of simplicity , that they can be considered plane waves , and your interferometer combines them at a small angle , so that you see a series of stripes ( bright and dark fringes ) on your screen . now add the " process " step , being careful not to disturb any other part of the interferometer . this causes a phase difference $\phi$ between the beams . the positions of the bright and dark fringes will shift . say $d$ is the distance between two crests or troughs ( brightest parts of two bright fringes or darkest parts of two dark fringes ) , and $\delta x$ is the distance by which the fringes shifted when you introduced the phase difference . then , $$ \phi\bmod{2\pi} = 2\pi\delta x / d$$ note that you will not be able to tell if $\phi &gt ; 2\pi$ .
in the specific case of the hamiltonian the non-locality arises because the time evolution depends on values of the field which are arbitrary far away . in the one dimensional case we have \begin{equation} i \frac{\partial}{\partial t}\ , \psi ~~~=~~~ \tilde{h}\ , \psi ~~=\ \sqrt{~m^2+\mathbf{\tilde{p}}^2_x~}\ \psi\ =\ \nonumber \end{equation} \begin{equation} \sqrt{ ~m^2-\partial_x^2~}~~ \psi ~~=~~ \frac{m}{x} k_1\left ( mx\right ) ~*~ \psi ~~~~~~~ \end{equation} ( we used $\hbar=c=1$ ) . in the last term $*$ denotes a convolution , in this case with a bessel k function . it is clear that this instantaneous dependency violates the speed of light restriction . see also my stackexchange answer here : $\nabla$ and non-locality in simple relativistic model of quantum mechanics now in the general case the value of $\psi ( x ) $ will depend on $\psi ( y ) $ at other locations in the past an it will depend on other fields such as $a^\mu ( y ) $ at other locations in the past . mathematically these dependencies stem from " taylor-expanded series " of differential operators but as long as you do not violate the speed of light restriction then this is perfectly fine . hans
this appears to be related to the decomposition of a totally symmetric tensor into traceless parts , which is a fairly involved process . the general equation is $$\mathcal{c} q_{a_1 a_2\cdots a_s} = \sum_{k=0}^{ [ \frac{s}{2} ] } ( -1 ) ^s \frac{\binom{s}{k} \binom{s}{2k}}{ \binom{2s}{2k}} \delta_{ ( a_1 a_2} \cdots \delta_{a_{2k-1} a_{2k}} q_{a_{2k+1}\cdots a_s ) }{}^{c_1} {}_{c_1} {}^{c_2}{}_{c_2} {}^{\cdots c_k}{}_{\cdots c_k} , $$ where $ [ \cdot ] $ denotes the integer part , einstein summation is implied and $q_{ ( a_1 a_2 \cdots a_s ) } \equiv \frac{1}{s ! } \sum_{\sigma\in s_s} q_{a_{\sigma ( 1 ) } a_{\sigma ( 2 ) } a_{\sigma ( 3 ) } \cdots a_{\sigma ( s ) }}$ . for the quadrupole moment it is $\mathcal{c}q_{ab} = q_{ab} - \frac{1}{3} q^c{}_c \delta_{ab}$ , for the octupole $\mathcal{c}q_{abc} = q_{abc} - \frac{1}{5} ( q^d{}_{dc}\delta_{ab} + q^d{}_{da} \delta_{bc}+ q^d{}_{db}\delta_{ac} ) $ ; these yield the factors in your question . an indication ( perhaps proof , although i am not certain about this at the moment ) that the traceless part of a totally symmetric tensor is an irreducible representation is easy to see if one uses the hook formula in dimension 3 . a totally symmetric tensor of rank $s$ has $\frac{1}{2} ( s+1 ) ( s+2 ) $ degrees of freedom and the traceless one has the latter minus the number of ways to obtain the traces , $\binom{s}{2}$ , which yields $2s+1$ . this is the dimension of the irreducible representation of the algebra of so ( 3 ) with spin $s$ . a full proof of this statement is in maggiore gravitational waves - theory and experiments . reference : f.a.e. pirani lectures on general relativity 1965 .
this is a good challenge ! here is maybe a solution : ordering a 1 meter long si rod with the correct doping level . it seems they are already able to make 2 mm diameter rods of 1 meter long out of pure silicon . ( http://www.goodfellow.com/catalogue/gfcat4j.php?ewd_token=5iqzqhxaluzqcqfmm0hg7c5opn6zxsn=o76p2nzwunxfcv3bozryzd6ivaflft )
the lasing mode ( stimulated emission ) may have nothing to do with the direction of the pump laser . for instance , flashlamp pumped lasers are pumped from the side , e.g. a ruby laser . stimulated emission occurs in the same direction as the stimulating photons -- that refers to another photon in the laser mode , not in the pump . this begs the question of how the lasing gets started . we usually say it is started by ' vacuum ' , or the quantum fluctuations of the field , which get exponentially amplified once the process gets going . so , in general , there is no preference between clockwise and counterclockwise modes in a ring cavity , and so the system can be unstable by switching between the modes . in most lasers , people take great pains to make sure the system has a preferred mode . ring lasers need a way to break the symmetry , which is usually an optical diode ( a faraday isolator ) . as an interesting note , another way to break the symmetry is to have a non-planar ring oscillator and polarization optics ( it is quite a clever solution ! ) .
the two cars each have their own degrees of freedom , such that if the spring force is $f_s$ then $$ \begin{aligned} m_1 \ddot{x}_1 and = +f_s - c_1 \dot{x}_1 \\ m_2 \ddot{x}_2 and = -f_s - c_2 \dot{x}_2 \end{aligned}$$ you already mentioed that the spring force is $f_s = -k \left ( x_1-x_2 \right ) $ so now you have your ode .
by simply slowing down the angular velocity as simple as this sounds , it is a huge problem in practice . the orbital speed is roughly 17,000 mph . to reduce this to subsonic speed all the while maintaining altitude would require a rather large reentry " stage " similar in capability , i would imagine , to the third stage of the saturn v . keep in mind that the space shuttle reentry engine burn lasted roughly 3 minutes and reduced the orbital speed by less than 200 mph . we want to use the atmosphere to convert our kinetic energy to heat so that large " descent " engines and quantities of propellant are not required ( as they are to land on an airless moon or planet ) . with our real atmosphere though it seems that all reentries are at very steep angles and at very high radial velocities . i do not think that is the case for controlled reentries . if i am not terribly mistaken , reentry angles are quite shallow , e.g. , 6 to 7 degrees , i.e. , the tangential speed is far higher than the radial speed .
planck units $\hbar=g_n=c ( =k_b ) =1$ see f.e. http://www.scholarpedia.org/article/bekenstein-hawking_entropy equation ( 11 ) where $t=\kappa/ ( 2 \pi ) $ ( in planck units )
poincaré invariance is a fundamental requirement of relativistic ( quantum ) physics . in particular if $u_g : {\cal h} \to {\cal h}$ represents the ( non-necessarily linear ) action of a poincaré transformation $g$ on ( normalized ) vectors $\psi$ of the hilbert space $\cal h$ of the considered system , transition probabilities have to be preserved : $$|\langle u_g ( \psi ) | u_g ( \phi ) \rangle|^2 = |\langle \psi | \phi \rangle|^2 \quad \forall \psi , \phi \in {\cal h}\: , ||\psi||=||\phi||=1\: . \quad ( 1 ) $$ a celebrated theorem due to wigner establishes that a ( bijective ) map $u : {\cal h} \to {\cal h}$ verifying ( 1 ) must necessarily be linear and unitary or anti linear and anti unitary , depending on the physical nature of the transformation . concerning representations of poincaré group $\cal p$ , by definition they have to satisfy , in addition to ( 1 ) , ${\cal p} \ni g \mapsto u_g$ with $u_g u_h = u_{g\cdot h}$ ( $^*$ ) and $u_e = id$ , where $\cdot$ is the group product in $\cal p$ , just in view of the definition of group representation . in principle each $u_g$ has to be unitary or anti unitary . if $g\in \cal p$ belongs to the proper orthochronous subgroup ${\cal p}_+^\uparrow$ , it can always be decomposed as $g= h\cdot h$ where $h$ still belongs to the same subgroup . therefore $u_g= u_{h} u_{h}$ , thus $u_g$ must be unitary ( even if $u_h$ is anti unitary , as the composition of a pair of anti unitary operator is always unitary ) . we conclude that the orthochronous poincaré group ${\cal p}_+^\uparrow$ ( and consequently the orthochronous lorentz group $so ( 1,3 ) ^\uparrow$ ) can only be represented by unitary operators in quantum theories , when the action of the group is on states . non unitary representations arise dropping the last requirement . for instance dealing with dirac or weyl spinors . ( $^*$ ) actually a phase could take place , since states are represented by normalized vectors up to phases : $u_gu_h = e^{i\alpha ( g , h ) }u_{g\cdot h}$ , however it does not change the result of the subsequent reasoning . as a matter of fact , it is possible to prove that continuous ( projective ) unitary representations of ${\cal p}_+^\uparrow$ are not affected by such phases , differently form representations of galileo 's group where those phases play a crucial role .
key is to notice that your steps provide you with a unit length as well as a unit time . so , let 's measure distance in $steps$ and time in $ticks$ , with your speed being $1 \ step/tick$ . the length of the train is $x$ steps , and its speed is $v \ steps/tick$ ( $v&lt ; 1$ ) . it follows that $$x \ + \ 18 \ v \ = \ 18 $$ $$x \ - \ 11 \ v \ = \ 11 $$ adding 11x the first equation to 18x the second yields $29 x = 396$ . the train is $396/29 \ steps$ long . you also need to check if indeed $v &lt ; 1 \ step/tick$ . leave that to you to demonstrate .
actually what you have to transform to define a symmetry for a classical or quantum system are the dynamical variables describing the system and appearing in the action rather than the metric ( moreover time reversal could need a further complex conjugation ) . in any cases here you are thinking of a discrete symmetries . noether theorem instead implies the existence of dynamically conserved quantities provided the symmetries of the action are continuous : there is a dynamically conserved quantity for each continuous ( differentiable actually ) one-parameter group of symmetries of the action . passing to quantum systems ( fields in particular ) , dynamically conserved quantities may arise also for discrete symmetries , provided they are described by simultaneously unitary and self-adjoint operators . parity operator can be taken of this type , but time reversal one cannot ( if the hamiltonian is bounded below as is physically necessary for the stability of the system ) , as it is an anti unitary operator ( there are the only two possibilities permitted by kadison-wigner theorem ) .
notice that $a$ is a linear operator on $\mathbb r^n$ . suppose that $a$ is singular , namely $\det a= 0$ , then the kernel of $a$ is nontrivial . in other words , there exists some nonzero $v\in\mathbb r^n$ for which $av=0$ . it follows that the kinetic energy vanishes for this nonzero $v$ . there is nothing " wrong " with this mathematically speaking , but it is physically pathological because the kinetic energy represents energy due to the magnitude of the motion of the object , and we therefore expect that any state of the object for which $\dot q_i\neq 0$ for some $i$ should be assigned a nonzero kinetic energy .
it is a general theorem that to establish a tensor identity involving lie brackets , one actually only needs to consider the case where all lie brackets vanish . this is because we can always find a basis of commuting vector fields by taking any coordinates and using the coordinate vector fields $\partial/\partial x^\mu$ , and if a tensor identity holds for these , it holds for all vector fields by linearity . then equality between the two definitions is fairly clear : ignoring the last term in the coordinate-free expression , the second definition is just the first in index notation . more concretely in this particular case , we can take $x = x^\mu \partial_\mu , y = y^\mu \partial_\mu$ . note that $x^\mu , y^\mu$ are scalars . let us abbreviate $\nabla_{\partial_\mu}$ by $\nabla_\mu$ . then $$\nabla_x \nabla_y z = \nabla_x ( y^\nu \nabla_\nu z ) = x^\mu ( \nabla_\mu y^\nu ) ( \nabla_\nu z ) + x^\mu y^\nu \nabla_\mu\nabla_\nu z . $$ here i have used $c^\infty$-linearity of the covariant derivative in the subscript and the leibniz rule . of course for the second term we just swap $x \leftrightarrow y$ , $$\nabla_y \nabla_x z = \nabla_y ( x^\nu \nabla_\nu z ) = y^\mu ( \nabla_\mu x^\nu ) ( \nabla_\nu z ) + y^\mu x^\nu \nabla_\mu\nabla_\nu z . $$ we see that $$\nabla_x \nabla_y z - \nabla_y \nabla_x z = y^\mu x^\nu ( \nabla_\mu\nabla_\nu - \nabla_\nu\nabla_\mu ) z + ( x^\mu \nabla_\mu y^\nu - y^\mu \nabla_\mu x^\nu ) ( \nabla_\nu z ) . $$ but the second term is precisely $\nabla_{ [ x , y ] } z$ , so $$r ( x , y ) z = x^\mu y^\nu r_{\mu\nu}{}^c{}_d z^d . $$ this differs from wald 's definition , but it is just a matter of moving indices , $$r_{\mu\nu}{}^c{}_d z^d = r_{\mu\nu}{}^{cd} z_d$$ and contracting with the metric , $$r_{\mu\nu c}{}^d z_d = g_{c\alpha} ( \nabla_\mu\nabla_\nu - \nabla_\nu\nabla_\mu ) z^\alpha = ( \nabla_\mu\nabla_\nu - \nabla_\nu\nabla_\mu ) z_c$$ where the last step is that metric contractions commute with $\nabla$ , since we are using the levi-civita connection .
i think you are exactly right . in fact the method you used of imagining a second moving car is exactly what you would do if you were trying to find the pressure as a function of position in the original problem with the wall . the technique is called the method of images .
i am a student so please point out in gory detail anything i did wrong . for a process to be quasistatic , the time scales of evolving the system should be larger than the relaxation time . relaxation time is the time needed for the system to return to equilibrium . we have an adiabatic process , so equilibrium must be preserved at each point , that is to say ( working within the validity of the kinetic theory for ideal gases and ignoring friction ) $ ( a l ( t ) ) ^\gamma p ( t ) = ( a l_0 ) ^\gamma p ( t_0 ) $ momentum gained by the piston : $\delta p = 2 m v_x$ a molecule would impact the piston every $\delta t = \frac{2 ( l_0+ \delta x ) }{v_x}$ the force exerted on the piston is $f =\frac{\delta p}{\delta t} = \frac{m v_x^2}{l_0+\delta x}$ pressure would be $p = \frac{p}{a}$ and for $n$ such molecules $p = \frac{n m &lt ; v_x&gt ; ^2}{a ( l_0+\delta x ) } = \frac{n m &lt ; v&gt ; ^2}{3a ( l_0+\delta x ) }$ so at the instant $t=t&#39 ; $ where the piston has been displaced by $\delta x$ , we have $ ( a l ( t ) ) ^\gamma p ( t ) = \frac{n m &lt ; v&gt ; ^2}{3a^{1-\gamma}} ( l_0+\delta x ) ^{\gamma -1}$ expanding in series $ = \frac{n m &lt ; v&gt ; ^2 l_0^{\gamma-1}}{3a^{1-\gamma}} ( 1 + \frac{ ( \gamma-1 ) \delta x}{l_0}+ o ( \delta x^2 ) ) $ substituing $\frac{\delta x}{l_0} = \frac{\delta t v_x}{2 l_0} -1$ $ ( a l_0 ) ^\gamma p ( t_0 ) = ( a l_0 ) ^\gamma p ( t_0 ) ( 1 + ( \gamma-1 ) ( \frac{\delta t v_x}{2 l_0} -1 ) ) $ if we want our process to be reversibly adiabitic atleast to first order , we must have from above $\delta t = \frac{2 l_0}{&lt ; v_x&gt ; }$ now , this is time until collision for the starting case . investigating second orders $ ( a l_0 ) ^\gamma p ( t_0 ) = ( a l_0 ) ^\gamma p ( t_0 ) ( 1 + \frac{ ( \gamma-1 ) \delta x}{l_0}+ \frac{1}{2} ( \gamma-1 ) ( \gamma-2 ) ( \frac{\delta x}{l_0} ) ^2 +o ( \delta x^3 ) ) $ looking at just the series terms $ 1 + ( \gamma-1 ) \frac{\delta x}{l_0} ( 1 +\frac{1}{2} ( \gamma -2 ) \frac{\delta x}{l_0} ) \approx 1$ this would be true for $\delta t = \frac{4 l_0}{&lt ; v_x&gt ; } ( \frac{1}{2-\gamma} -1 ) $ now , this is the " time until next collision " for a gas molecule hitting the piston . to maintain reversibility , at least to second order , the piston should be moved from $l_0$ to $l_0 + \delta x$ in time $\tau = \delta t$ so that the system variables follow the adiabatic curve . the $&lt ; v_x&gt ; $ can be calculated from the maxwell distribution
your equation is the right solution to schrödinger 's equation in the momentum-energy representation . however , it is only that simple for schrödinger 's equation with no potential , $v ( x , y , z ) =0$ . if it is zero , the solution ( or , similarly , the reformulation of the equation ) is as easy as the algebraic relationship you wrote - but it is also uninteresting for the same reason . the interesting cases have e.g. the coulomb potential $k/r$ or the harmonic oscillator potential $kx^2/2$ and they can not be " solved " in the simple way you sketched . for a nonzero potential , the problem is genuinely equivalent to a partial differential equation . however , that does not mean that it is the only way in which the problem may be formulated or solved . both the harmonic oscillator and the hydrogen atom may be solved ( i.e. . their spectrum may be found ) algebraically , by the creation and annihilation operators in the harmonic oscillator case , or by a hidden $so ( 4 ) $ symmetry in the hydrogen atom case . a general schrödinger 's equation in quantum mechanics is really an ordinary differential equation for the state vector ; the " spatial derivatives " only appear as the action of particular operators on the hilbert space . some of these operators - e.g. the momenta in the position representations - are conveniently represented as partial derivatives with respect to spatial coordinates but that is only the case if we use a " continuous basis " for the hilbert space .
the reason for your confusion , and the answer to question 4 , is that the first equation you give ( although you copied it missing crucial primes on the indices or on the variables that serve to distinguish between the x and x ' coordinate labels , as lubos motl pointed out ) is the geodesic equation for a flat space-time in nonlinear coordinates , while the second form of the equation tells you the geodesics in a curved space-time with an arbitrary metric . these are not a 4 by 4 system of equations , but a system of only 4 equations , since there is only one free ( not summed over ) index . please do not be freaked out by the einstein convention--- it is simple to add the summations explicitly as you did , and if you know how to do this correctly , you are not confused , since there is nothing more to it . the einstein convention is the only simple way of keeping track of covariant/contravariant quantities , tensor products , and the covariant linear maps allowed between them . it is good to begin in two dimensions , as you did . the explicit equations are ( writing out the sum in full ) : $$ \ddot{x}^0 + \gamma^0_{00} \dot{x}^0 \dot{x}^0 + \gamma^0_{01} \dot{x}^0 \dot{x}^1 + \gamma^0_{10} \dot{x}^1\dot{x}^0 + \gamma^0_{11} \dot{x}^1\dot{x}^1 = f^0 $$ $$ \ddot{x}^1 + \gamma^1_{00} \dot{x}^0 \dot{x}^0 + \gamma^1_{01} \dot{x}^0 \dot{x}^1 + \gamma^1_{10} \dot{x}^1\dot{x}^0 + \gamma^1_{11} \dot{x}^1\dot{x}^1 = f^1 $$ where $f^0 , f^1$ are an additional non-gravitational force , which i will omit ( because it does not matter for the gravitational force , you understand that separately--- this extra force is in the equation in the nonhomogenous case ) . these are two equations , not a 2 by 2 system of equations ( question 5 ) , and in 4 dimensions they are 4 equations , not a 4 by 4 system ( although they can be a "4 by 4 system " if you mean that you need to solve 4 linear equations to find the accelerations in a new coordinate , see below ) . also , note that the cross terms appear , they are not zero , and this answers question 2 , although also note that the two cross terms are equal ( because the $\gamma$ is symmetric in the lower two indices for the usual metric connection ) , so that you can add the middle two terms in both equations , but you do not want to do this , so that the explicit einstein contractions in the equation are made obvious . the answer to question 3 is yes , you can use this convention , with two caveats : * make sure you know what the derivative means in all cases , in the case you give first , it is differentiating one coordinate with respect to another coordinate ( see below ) . * make sure that you do not confuse the quantity ${\partial y^\mu \over \partial x^\nu} = \partial_\nu y^\mu = y^\mu_{ , \nu} $ with the inverse quantity ${\partial x^\mu \over \partial y^\nu} = m^\mu_\nu $ where $m^\mu_\nu$ is the inverse matrix to $y^\mu_{ , \nu}$ . it is easy to lose sight of the meaning of the equations in the soup of symbols and formal relations , and this is no good . to keep the meaning of everything straight , you need a few examples in the back of your head , and these are useful anyway . explicit example consider 2d spacetime in the regular x , t rectangular coordinantes , and define the new coordinates $$t&#39 ; =t$$ $$ x&#39 ; = x-t^2$$ these coordinates are useless , the center of the x'-coordinate is accelerating to the right with a constant galilean acceleration , but they are sufficiently not special to explain equation 1 . the inverse map is simple $$ t = t&#39 ; $$ $$ x = x&#39 ; + t&#39 ; ^2 $$ in the original rectangular system of coordinates , the geodesic equation is $$\ddot{x} = 0$$ $$\ddot{t} = 0$$ you can find the new equations just by substitution ( but be sure to understand what this means--- you are substituting the x , t coordinate values as a function of the proper $\tau$ in terms of the x ' , t ' coordinates of the same trajectory ) . $${d^2\over d\tau^2} ( x&#39 ; + t&#39 ; ^2 ) = \ddot{x}&#39 ; + 2t&#39 ; \ddot{t}&#39 ; + 2 \dot{t}&#39 ; ^2 = 0$$ $$\ddot{t&#39 ; } = 0$$ so you see that to solve for the t ' and x ' accelerations , you need to invert the coefficients of the second derivative terms , which form the jacobian matrix of the coordinate transformation , and the coefficients of the first derivative squared terms are the second derivatives of the coordinate transformation . the result is silly n this case : $$ \ddot{x}&#39 ; = - 2 \dot{t}&#39 ; ^2 $$ $$ \ddot{t}&#39 ; = 0 $$ which solves to $$ t = a \tau $$ $$ x= b \tau - a^2 \tau^2 $$ which is as expected , it is the transformation of a straight line . it is easy to work out the general case from this . general linear/quadratic transformations if you look near a point x , t , you can define the general quadratic change of variables $$ x&#39 ; = ax + bt + p x^2 + 2qxt + r t^2$$ $$ t&#39 ; = cx + dt + s x^2 + 2txt + u t^2 $$ with constants a , b , c , d p , q , r , s , t , u . you should study this , because it is the general case ! any coordinate transformation is linear plus quadratic to second order near a point , and the cubic terms are not important for determining the transformation law of the connection coefficients $\gamma$ . you write this more formally in symbol-soup as $$ x^{\mu&#39 ; } = {\partial x^{\mu&#39 ; } \over \partial x^\mu} x^\mu + {1\over 2} {\partial ^2 x^{\mu&#39 ; } \over \partial x^{\alpha} \partial x^{\beta} } x^{\alpha} x^{\beta} $$ where the quantities ${\partial x^{\mu&#39 ; } \over \partial x^\mu}$ are just the ( a , b ; c , d ) coefficients , and the second derivative quantities are $a , c , b , p , q , r$ . remember that these are just some numbers at every point . this thing is valid to second order near x=0 ( which maps to x'=0 under the coordinate change--- this can be arranged just by additively shifting the x ' coordinate ) . you can formally write the inverse map too ( remember that here and above , primed indices or primed coordinate refer to the new coordinates , while unprimed indices or unprimed coordinates are the old ones ) . $$ x^{\mu} = {\partial x^{\mu} \over \partial x^{\nu&#39 ; } } x^{\nu&#39 ; } + {1\over 2} {\partial ^2 x^{\mu} \over \partial x^{\nu&#39 ; } \partial x^{\sigma&#39 ; }} x^{\nu&#39 ; } x^{\sigma&#39 ; } $$ then you can solve for the new form of the geodesic equation by taking the second $\tau$ derivative along a curve $x^{\mu} ( \tau ) $ , setting it to zero , and inverting the linear coefficients multiplying the second derivatives to solve for the second derivatives , as above ( except easier , because you only need to do it near the one point x=0 ) . $$ {\partial x^\mu \over \partial x^{\mu&#39 ; } } \ddot{x}^{\mu&#39 ; } + {\partial^2 {x^\mu}\over \partial x^{\alpha&#39 ; } \partial x^{\beta&#39 ; } } \dot{x}^{\alpha&#39 ; } \dot{x}^{\beta&#39 ; }$$ if you solve this " system of equations " ( i do not like this distinction--- 4 equations are 4 equations , whether the leading linear coefficients are explicitly diagonal or not ) for the accelerations using the formal inverse matrix to ${\partial x^\mu \over \partial x^{\mu&#39 ; }}$ , you get the form of the christoffel symbols in the new coordinates , as your book has it . the meaning of these formal equations is only fully clarified with appropriate examples , which you should provide for yourself .
the sun emits a spectrum from infrared to ultraviolet . solar irradiance spectrum above atmosphere and at surface . extreme uv and x-rays are produced ( at left of wavelength range shown ) but comprise very small amounts of the sun 's total output power . the plot tells us that infrared radiation is almost half of the spectrum . there exist measurements of solar radiation transmission through glass transmittance spectra of five types of glass these plots tell us that clear glass transmits 90% of the impinging radiations in all wavelengths . when in sunlight the heat felt on the body is from the infrared radiation and from ultraviolet that is absorbed and turned to heat . with clear glass the feeling of heat is not affected much , a slightly cloudy day would have the same effect . the most heat absorbing glass , green curve , lets visible light through more and blocks more infrared . all glasses block most of the ultraviolet . integrating over the transmittance one may see , depending on the glass , the total energy passing through the glass . eventually all of that energy will turn to heat due to multiple reflections in the room but direct heating will be felt by the percentage of infrared transmitted . note : heat absorbing glasses are used for special purpose in optical systems , and absorb the heat and then dissipate it in the air surrounding them . they will not be found in a usual window , but can be used in outer layers of glass windows to reduce transmission of infrared . you ask : and how fast beam will of light will lose energy while traveling from window 's pane ( what is the rate of heat loss ) ? there is no time lag , as the light passes through the glass at close to the velocity of light in vacuum ( modified by the index of refraction ) .
like get 1 kilogram of dirt and convert it to c2 joules ? what would it take to make this happen ? to make something close to this happen you would need one kilogram of antimatter dirt . then all the quantum number additions would be satisfied and a lot of radiation would come out , but not completely into kinetic or useful radiative energy . you will get pairs of weakly interacting neutrinos antineutrinos , and pairs of electrons and positrons that will have to meet each other to become pairs of photons . the resources you would have to spend to create 1 kilogram of antidirt would make the whole process economically unfeasible , considering that we have only managed to make a bit of antihydrogen up to now .
it is not particularly meaningful , in this case , where the energy ' comes from ' . it is called the zero-point energy and it is simply there , with not a lot to be done about it . there are some ways to justify it , of course . say you have a square potential well , whose bottom has $v=0$ , and say you try to put a particle with zero kinetic energy in the centre of the well . having zero kinetic energy means having zero momentum , and de broglie 's relation then demands that the particle 's wavefunction have infinite wavelength . however , having such a large wavelength means that with some probability the particle is in regions with nonzero potential energy , and therefore its total energy is positive . there is just no way to place the particle in a state with zero total energy . the way to avoid this is , of course , having a wavepacket just small enough that it will fit comfortably inside the well . however , having a limited spatial extent means having a finite wavelength , and therefore ( by de broglie 's relation ) a nonzero momentum and kinetic energy . you note rightly that if you were to manually ( and adiabatically ) decrease the size of the well , you would have to perform work to push the particle in , and this would increase its zero-point energy . if you have two particles in different wells , their difference in energy can best be thought of as a potential to perform work : there exists a physical transformation of the narrower well ( i.e. . adiabatically allowing it to expand ) which takes it to the state of the other particle and at the same time allows it to perform work . you are no doubt aware of the casimir effect , in which the effects of the zero-point energy are measurable . this is not simply some mathematical result that is an oddity within the theory ; you can indeed perform work using simply the vacuum energy .
i can not say i know any that do it . a small refrigerator modified with clear panels and post who tip acts as the seed-point for nucleation - this is the basic concept of the lab setup a for ice crystal growth . i think your objectives need to be stated better . " large " could be an imaginative thing wherein a real implementation will demand you grow on a substrate . this then creates its own problems regarding purity of the crystal and morphology . i am on mobile , so pardon the lack of references .
a quick google will find lots of analyses of interplanetary travel under constant acceleration . the best one i found is here , and gives results for travel between earth and mars . it even provides matlab code to do the calculation , and you could easily modify this to calculate travel between different planets . we are not supposed to just give links without discussion , but i am not sure how much there is to say . unsurprisingly there is no simple analytical solution to the problem so a numerical solution is necessary . the trajectory ends up looking like an s . i have nicked one of the pictures from the site to show this : green shows the earth 's orbit , cyan shows the orbit or mars , the red line is the constant outward acceleration and the blue line is the constant deceleration . the journey takes around 6 days .
from the dutch national science quiz 2006 ( my translation ) : question 14: you put a duvet cover together with smaller laundry in the washing machine . why , at the end of the program , all smaller laundry has twisted itself in the duvet cover ? due to the left-and-right cycling of the drum washing machines predominantly cycle one way . to loosen the laundry , the drum sometimes abruptly turns the other way . due to this opposite movement , suddenly a few liters of water bumps very forcefully into the laundry , and therefore the opening of the duvet cover will come to lie completely open . smaller laundry falls in one piece at a time . as soon as the machine goes into centrifuge mode , the smaller laundry pieces are being pushed in further . it is very difficult for laundry that is in the duvet cover to get out . nb : the false answers were ( my translation ) : because small laundry pieces are more sensitive to water vortices than large items due to the area ( size ) difference between the duvet cover and smaller laundry i would like to add , from personal experience , that it is very rare that all smaller laundry ends up in the cover . sometimes half of it will get in there , and sometimes just the odd sock . but perhaps that is just me and my machine .
measuring $w$ is actually what i do for a living . the current best measurements put $w$ at $-1$ but with an uncertainty of $5\%$ , so there is a little room for $w \ne -1$ models , but it is not big and getting smaller all the time . indeed , we had all be thrilled if , as measurements got more precise , $w \ne -1$ turns out to be the case because the $\lambda$cdm model ( where dark energy is just a scalar correction to general relativity ) is just horribly arbitrary and we had much rather get evidence for something new and exciting . you ask if models can reproduce a $w \approx -1$ and then ask about free parameters in such models . you are on the right track with asking about free parameters . there are some ways in which you can explain that $\omega_\lambda$ and $\omega_m$ are fairly close to each other today ( quintessence models do this with tracker models , inhomogeneity inducing corrections to the friedmann equations can do this because the inhomogeneities get stronger as structure grows ) while the $w \approx -1$ value is not a particularly unnatural value for many models ( it is for some , and those are already ruled out ) . so you are in an odd situation where you could certainly imagine , say , a quintessence model with say $w \approx -1$ , but you are not too tempted to because $\lambda$cdm is just much simpler . to get out of this impasse , you are on the right track when you ask about free parameters ( or loose ends as you call them ) . quintessence for example would have a total of $4$ , which represents the number of free parameters in your quintessence field that have an impact on the rate of evolution , whereas $\lambda$cdm has only one . this is why , for the time being , we go with $\lambda$cdm : our current probes can only constrain a single parameter , namely the current effective equation of state , and it is consistent with $\lambda$cdm , which then plays the role of a sort of " placeholder " model given that it requires the least number of degrees of freedom . current constraints on the evolution of dark energy properties are a joke , and you might as well consider them non existent . the image below is a confidence contour in the $w$ vs $w_a$ plane given the latest cmb , bao , and supernova data . $w_a$ is a somewhat arbitrarily chosen parameter ( $w ( a ) = w_0 + ( 1 - a ) w_a$ where $a$ is the scale factor ) but it gives you an idea of your probe 's sensitivity to the evolution of $w$ . future probes may be able to meaningfully constrain $w_a$ , and if it turns out to be different than $0$ , then that'll rule out $\lambda$cdm and open up a whole new ball game . remember , $\lambda$cdm corresponds to a static $w = 0$ . it'll be hard , since we will need to go to much higher redshifts while maintaining similar levels of accuracy in our measurements , if not improvements because given its current properties ( and whatever its evolution ) dark energy is weaker in the past ( $\omega \propto a^{-3 ( 1+w ) }$ , so while a $w \approx -1$ dark energy maintains a similar density , matter density , which corresponds to $w = 0$ , increases as you look back in time ) . being able to better constrain this evolution is the name of the game if you are interested in ruling out the current standard model . the classification of possible explanations that your review suggests is indeed the best way to broadly categorize the proposed solutions . a popular model which modifies the right hand side is quintessence . quintessence employs a common solution ( or common attempt at a solution ) to many problems : when in doubt , throw in a scalar field . no known field satisfied the required properties , so it would most certainly lie outside the standard model of particle physics . in general , one can consider the impact on expansion of the addition of any fundamental field to the lagrangian of particle physics . the only condition this field would have to satisfy for this field to be a candidate for dark energy is $w &lt ; -1/3$ . for a quintessence field $\phi$ with a potential field $v ( \phi ) $ , $p$ and $\rho$ are such that $w$ is given by : \begin{eqnarray} w_\phi and = and \frac{p}{\rho} and \mbox{ ( by definition ) } \\ and = and \frac{\frac{1}{2} \dot{\phi}^2 - v ( \phi ) - \frac{1}{6a^2} ( \nabla \phi ) ^2}{\frac{1}{2} \dot{\phi}^2 + v ( \phi ) + \frac{1}{2a^2} ( \nabla \phi ) ^2} and \mbox{ ( $p$ and $\rho$ for quintessence fields ) } \\ and = and \frac{\frac{1}{2} \dot{\phi}^2 - v ( \phi ) }{\frac{1}{2} \dot{\phi}^2 + v ( \phi ) } and \mbox{ ( use spatial homogeneity to get rid of $\nabla \phi$ terms ) } \\ and \stackrel{\text{$\dot{\phi} \rightarrow 0$}}{\approx} and -1 and \mbox{ ( for a slow rolling field ) } \end{eqnarray} the couple of $\nabla \phi$ terms in there disappear when you invoke a homogeneous universe ( using the comological principle ) . hence , many models with a $v ( \phi ) $ that yields a $w_\phi &lt ; -1/3$ have been considered simply by constructing the proper $v ( \phi ) $ , and it is not hard nor particularly unnatural to construct models where $w \approx -1$ today , simply by having a slow rolling field where $\dot{\phi}$ is small and therefore $w_\phi \approx -1$ . the difficulty lies in making such models distinguishable from the standard case of a $w = -1$ that is static in time , given our experimental insensitivity to the time evolution of the field . the field has not necessarily been in slow roll for the history of the universe . on the whole this is actually a whole class of models , and many models and couplings with matter can be invoked to explain all sorts of things about dark energy ( like the rough similarity of $\omega_\phi$ and $\omega_m$ today , and why this happens when the field is slowly rolling etc etc ) . again though , we are not yet sensitive to any of the observational implications of any of this . in the near future , we may be able to constrain 2 parameters of freedom with regards to the evolution of expansion , and not the 4 that are required to fully characterize a quintessence model , meaning that even then we will only be able to , at best , favor quintessence models over others , but it'll still be a stretch before we can definitively prove them . another right hand side explanation is k-essence , but this is essentially a modified version of quintessence where the kinetic energy term is not the canonical one , and rather than play around with $v ( \phi ) $ it is thought that it is the properties of this kinetic term that could lead to the required properties . recall that our own kinetic term was $\frac{1}{2} \dot{\phi}^2 - \frac{1}{6a^2} ( \nabla \phi ) ^2$ , and that we got rid of it entirely by assuming $\nabla \phi$ homogeneity and that $\dot{\phi}$ was small . the idea for k-essence is not to get rid of these terms , but to come up with a new form for the kinetic energy such that the k-essence field will have to desired properties . the novelty here ( relative to quintessence ) is that we do not need a very low upper bound on the mass of field to make it slow rolling . but observationaly , this remains just as hard to prove because the properties of the field ( your review gives a very comprehensive overview of this ) leave open a number of degrees of freedom as well . $f ( r ) $ models modify the left hand side . here , again , it is best to think of this as a class of models . as the name indicates , $f ( r ) $ gravity works by generalizing the einstein-hilbert action : $$ s [ g ] = \int \frac{1}{16 \pi g c^{-4}} r \sqrt{-|g_{\mu \nu}|} d^4x \rightarrow s [ g ] = \int \frac{1}{16 \pi g c^{-4}} f ( r ) \sqrt{-|g_{\mu \nu}|} d^4x $$ thus the action now depends on an unspecified function of the ricci scalar . the case $f ( r ) = r$ represents classical general relativity . it is this function that we tune in these models to get the desired properties . to test the general case rather than thest each $f ( r ) $ individually , you can taylor expand $f ( r ) = c_0 + c_1 r + c_2 r^2 + . . . $ and constrain your expansion to however many degrees of freedom you think your observations can constrain . just having $c_0$ obviously corresponds to a cosmological constant , and $c_1 = 1$ is , as we have said , the usual gr . note however , that because this modifies gravity as a whole , and not just cosmology , other observations could be brought into the fold to constrain more parameters , and we are not limited here by the number of degrees of freedom cosmological probes can constrain . if cosmological tests of gr agree with smaller scale tests of gr in a manner consistent with an $f ( r ) $ modified gravity , these models could be proven to be onto something . here of course we must look at all the tests of gr to get a fuller picture , but that is another review in and of itself . here we will simply note that more generally , the challenge when modifying the left hand side is to only change cosmology , but remain consistent with the smaller scale tests . the theory of general relativity is well tested on a number of scales . the accelerated expansion of the universe is the only potential anomaly when comparing general relativity to observations . to modify it such that the friedmann equations gain an acceleration term while all other tests remain consistent is tricky business for theorists working in the field . the $\lambda$ correction works because it essentially maintains the same theory except for adding a uniform scalar field in the theory . hence , while its impact on the global metric is significant , it has no impact on local tests of gravity because it does not introduce a gradient to the energy density field . other theories run into tuning problems precisely because they cannot naturally mimic this behavior . chameleon theories get around this problem by adding a field that becomes extremely massive ( i.e. . an extremely short scale force ) when surrounded by matter . hence , its local impact on gravity is negligible in all the contexts in which general relativity has been tested . ideally , your model would remain consistent with pre-existing tests , but still offer hope for some testable differences ( ideally with a rigorous prediction set by the scale of acceleration ) beyond cosmological acceleration so as to make your model more believable . finally , we can consider if the assumptions that went into the friedmann equations are correct . one approach ( not the one you mentioned ) is to consider the impact of inhomogeneities . while it was always obvious that homogeneity did not hold below certain scales , the traditional approach has been to simply conclude that while the friedmann equations will therefore not accurately describe the " local " scale factor , it would describe the " average " scale factor so long as we only considered it to be accurate above the inhomogeneous scales . in other words , they describe the expansion of the " average " scale factor . mathematically speaking , this is not the rigorous way to get around this problem . ideally , what one would do instead , is to calculate the average of the expansion of the scale factor . some theorists have proposed methods of doing this in ways that would add an acceleration term to the friedmann equations . for a matter dominated universe these become : \begin{eqnarray} \left ( \frac{\dot{a}}{a} \right ) ^2 and = and \frac{8 \pi g}{3} \left ( \left\langle \rho \right\rangle - \frac{k}{a^2} \right ) - \frac{1}{2} q ( t ) \\ \frac{\ddot{a}}{a} and = and \frac{- 4 \pi g}{3} \left\langle \rho \right\rangle + q ( t ) \\ \mbox{where } q ( t ) and = and \frac{2}{3} \left ( \left\langle a^2 \right\rangle - \left\langle a \right\rangle^2 \right ) - 2 \left\langle \sigma^2 \right\rangle \end{eqnarray} where $\sigma$ depends on the level of structure formation of the universe . because this model builds on the effect of inhomogeneities , it could explain why acceleration is a late time effect . earlier in the universe 's history , structure fromation was in its infancy and could not provide the necessary level of inhomogeneity . as the universe grows , structure formation kicks in , and so too does this effect become more significant . the biggest challenge for these models is providing a definite value for $q$ . this has proven to be a difficult task because estimating $\left\langle a^2 \right\rangle - \left\langle a \right\rangle^2$ analytically is extremely difficult , and simulating this sort of gr on such a large scale presents problems of its own . another potential effect of these inhomogeneities is the loss of energy as light enters large potential wells , only to exit them after they have significantly expanded . in this view , the observations would be explained by what happens to the photons along the line of sight , and no actual acceleration would take place . in some ways , this is equivalent to a sort of physically motivated tired light model . so far , these models have not been able to explain the observations assuming only a matter dominated universe . it is , however , entirely possible that a small portion of the observed effect is due to this . keep in mind that this explanation would only affect supernovae measurements , and hence could not explain the now independent confirmation of other probes regarding acceleration . at best , they could one day reduce tension between supernovae and other probes . the lemaitre-tolman-bondi model assumes that homoegeneity is just not true at all , and that we live inside a gravitationaly collapsing system , near its center . it is a little out there , but the thing about cosmology is that it is rather easy to make far fetched models and go " hey , you never know " , because hey , we do not :p it is a big universe out there and we only have access to a tiny portion of it , so why not consider i guess . the key to disproving this one will be to increase the agreement between the various probes . the metric it leads to does not have the same impact on all the different distance probes in the same way that $\lambda$cdm does . so it is disprovable , and for the time being can be tuned to explain the observations , but a little far fetched to begin with . i would guess that you can change the toy model they use to model the inhomogeneous shape of the universe and almost always get something that'll fit the data , but the more you do that the less convincing your theory becomes . as a side note , though this is not your main question , it is not odd that we are using an effective fluid model . it is not counting on dark energy having properties similar to a fluid in the traditional sense , it is just telling you that you are using certain symmetries to simplify you $t_{\mu \nu}$ tensor when solving for the friedmann equations . essentially , if you are in the reference frame where the momenta of the universe sum to $0$ then $t_{\mu \nu} = diag ( \rho , p_x , p_y , p_z ) = diag ( \rho , p , p , p ) $ , and $w$ is just the ratio of of $\rho$ to $p$ . it is just a way to simplify equations and clarify your degrees of freedom .
we should probably distinguish between a particle being " point-like " and a particle being " structure-less " . in classical mechanics we talk of " point-like " particles , objects with no extension . it is the case that in general relativity any " point-like " mass would be inside of its event horizon and so would be a black hole . in quantum-mechanics even a " structure-less " particle - a particle with no consitituent parts - is wave-like and has extension , though not a fixed size , and it can never be come exactly point-like since that would take an infinite amount of energy . i do not believe it to be the case , therefore , that quantum-mechanically all particles are black holes in any sense .
wave shape will depend on it initial pulse and some other factor too . if string has a rigid end then it can form a standing wave pattern in that case you get node and antinode . amplitude of antinode will vary from zero to some maximum value and node will be at zero amplitude . another possible way may be that a medium in which wave propagate , drag the wave and change its shape .
the individual streamline with velocity zero may not make much sense on its own , but it often does when you consider the bulk of the fluid as a whole . in the case of the surface of a body immersed in a fluid , you could trace a streamline starting at a point infinitesimally close to the surface , where the velocity would be infinitesimally small , but non-zero . the streamline on the surface would be the limit of the streamlines as your starting point moves towards the boundary . such analysis cannot be performed on stagnant fluid , i.e. it makes no sense talking of streamlines in the bulk of a stationary fluid .
a subtle problem you seem to overlook is that the proton-proton cross section is very small , about 0.07 barns ( a barn is $10^{-28}$ square meters ) at the lhc energies and not dramatically different at your lower " fusion energies " . it means that at the lhc , much like at your dream machine , most of the protons simply do not hit their partners . it is not really possible to focus the proton beams arbitrarily accurately , for various reasons ( the uncertainty principle is the truly unavoidable effect : you either localize the beams in the transverse direction , into a " thin pipe " , or you specify that the velocity in this transverse direction is zero which is needed if you want to preserve the location " in the thin pipe " in the future , but you can not do both at the same moment ) . if it were possible , the lhc would be among the first ones that would use the method , to increase the luminosity . so if you accelerate two beams of protons against each other , an overwhelming majority of them will simply continue in their original motion . ( the protons in the lhc have to orbit for half an hour or so – tens of millions of revolutions – before one-half of them collides or disappears . ) it costs some energy to accelerate the protons to these energies and you want this energy to be returned from fusion , with some bonus . but the fusion only returns you the energy from the protons that collided ( some of them could create helium at your energies but there will always be nonzero probabilities of other final states ; it is not a deterministic system that always produces the same final state for a given initial state ; quantum mechanics says that the outcomes are random ) which is a tiny portion of the protons . so you will be losing most of the energy you invested for the acceleration . note that the lhc consumes as much energy as the households in geneva combined and it just produces collisions of protons whose energy is smaller than a joule per pair . to increase the fraction of the protons that hit their partners , you either need to send them to the collision course repeatedly , like at the lhc , but then you need to pump extra energy to the protons that they lose by the synchrotron radiation ( which is always nonzero if the acceleration vector is nonzero , e.g. for all circular paths ) . or you will need to dramatically increase the density of the beams . but if there are many protons in the beam , they will electrically repel each other and you will become unable to focus them for collisions , too . so what you need to do is to electrically neutralize the high-density proton beam and then you have nothing else than the plasma and you face the usual tokamak problems how to stabilize it . note that the electrons respond totally differently to the external electromagnetic fields than the protons . the lhc uses both electric and magnetic fields to accelerate the protons but to keep the plasma neutral , you must avoid electric fields . tokamaks only work with magnetic fields . whether they will ever become fully working and feasible remains to be seen but the absence of the electric fields implies that they do not have much in common with particle accelerators .
firstly , is that correct ? yes your intuitive understanding for this part of the coriolis effect is correct . the second part , that is , why wind in the east direction is deflected south , is a bit trickier , and involves the use of centripetal force . this is given by the equation : $f = \frac{mv^2}{r}$ if we re-arrange the above equation , we can find $r$ in terms of $v$ , and we arrive at : $r = \frac{mv^2}{f}$ this tells us that as velocity increases , the radius required to maintain the orbit also increases . now let 's apply this concept to winds on the earth . if we feel no wind on the earth , then the air in the atmosphere is travelling at the same velocity as the earth . the earth is naturally spinning towards the east . in the case of an additional eastward wind felt on the earth , this wind has effectively increased its velocity , and therefore the above equation tells us that the radius of orbit must increase as well . radius in this case is the distance , measured perpendicularly of the earth 's axis , between the axis and the wind . in order for the radius to increase , the wind moves southwards , where the radius is larger . similarly , wind moving in the west direction , is moving in the direction opposite of that to the earth , and therefore its velocity is decreased . consequently this wind moves towards the north , where the radius is less . the above image shows what happens . the wind moving east begins to expand its radius , thus moving outwards . gravity pulls it back , and the wind moves south , in order to maintain the larger radius required for its increased velocity .
moshe is answer gets the main point across . if you are interested in trying to learn something more detailed , here 's a thought about where you might start . ( i am not even close to expert on these matters , but this is something that i have found readable and worthwhile . ) there is an interesting thread of literature including http://arxiv.org/abs/hep-th/9309145 by susskind and http://arxiv.org/abs/hep-th/9612146 by horowitz and polchinski , along with other papers that those might lead you to . the idea is that free strings have a certain entropy , in the sense that for a given mass , there are many different combinations of oscillator modes you can turn on to find a string state of that mass . the string energy is proportional to its length , and a typical string of a given energy looks roughly like a random walk of a particular length . once you turn on a coupling , at some point some of these states will become black holes , because their energy is contained in a region smaller than their schwarzschild radius . for a given mass , a black hole state also has some entropy . there are various consistency checks you can do on the way these things scale , to see that it is sensible to talk about a sort of smooth transition between string states and black hole states if you fix the energy and vary the coupling . there are more technical and more precise papers in the literature on black hole entropy and microstate counting , but for a non-expert like me this particular thread of literature seems interesting because it relies on parametric scaling laws that are pretty readily comprehensible , and paints a relatively clear picture of the physics .
the area under the curve in the pv-diagram is the integral $$ \int p \ ; \mathrm dv = \int \frac fa a\ ; \mathrm ds=\int f \ ; \mathrm ds \equiv w $$ by definition of pressure as force per area and ( infinitesimal ) volume as area times distance . this is the mechanical work done by the system on the environment in case of expansion or by the environment on the system in case of compression , which differ by sign . it is called external to emphasize the interaction with the environment .
jerk_dadt is correct . electric current is the flow of free electrons in the conductor . at any instant , the number of electrons leaving the wire is always equal to the number of electrons flowing from the battery into it . hence , the net charge on the wire is zero . if you say the current carrying conductor is charged , it will violate the kirchoff junction rule , which is based on the fact that in an electric circuit , a point can neither act as a source of charge , nor can the charge accumulate at that point .
the composition of martian atmosphere is quite different from the one on mother earth ( see mars-gram model developed by c.g. justus from msfc at huntsville , ala . for details ) . on this planet , colors at sunset ( and the unforgettable green ray ) are dependent on water content . on mars , it is the amount of dust in the air and related scattering that changes the colors from one season to another . to see for yourself , please visit jpl 's multimedia page with raw images from the curiosity rover . nasa rover spirit on the rim of gusev crater on may 19 , 2005 credit : nasa/jpl/texas a and m/cornell a video with a martian sunset seen from nasa rover opportunity . this said , for the same spectrum each camera has its own sensitivity curve , and cannot ( broadly speaking ) replace a human observer . even corrected colors are not quite the same as the real stuff . for reference : different color sets in gale crater . curiosity on august 23 , 2012 credit : nasa/jpl-caltech/msss on mercury , you will have to wait long to see the sunset , and will be disappointed by the colors due to lack of perceptible atmosphere . this assumes that your thermal control can cope with being continuously grilled by the sun during the " day " and frozen at " night " , and that your uv protection is strong enough to prevent rapid onset of cancer . would say , these are rather pressing issues that significantly impair your ability to conduct observations .
it is a bit hard to be sure without seeing the whole text , but it looks like they are discussing the problem of of obtaining finite minimum energy solutions of a gauge/higgs system . in 3 space dimensions , for example , for the georgi-glashow model , $$ \mathcal{l}= \frac{1}{2}tr ( f^{\mu\nu}f_{\mu\nu} ) +tr ( d_{\mu}\phi d^{\mu}\phi ) -\frac{\lambda}{4} ( |\phi|^2-v^2 ) ^2 $$ to minimize the energy you want the curvature to vanish at infinity , so the potential becomes pure gauge . moreover $\phi^a \phi^a=v^2$ defines a 2 sphere in internal group space . so looking at the behaviour of the higgs field on the $s^2$ at infinity we have a map $s^2\rightarrow s^2$ . now there is the boring solution where $a_{\mu}=0$ and $\phi$ is a constant at infinity . this is like your picture ( b ) . but there is also the possibility that $\phi$ follows the direction defined by the polar coordinates $\theta , \psi$ on the $s^2$ at infinity . this is a winding number 1 solution and is depicted in ( a ) - this is ' thooft 's hedgehog configuration . things like ( a ) are monopoles . now you can do the same thing in two spatial dimensions instead of three . $\phi$ is just a complex number , and the vacuum manifold in internal group space is an $s^1$ this time . if we work out what the gauge potential must do to make the energy of a two dimensional hedgehog finite , it turns out the the $a$ field is pointing in a direction tangential to the $s^1$ at infinity . this is the vortex - you can not extend $a$ back towards the origin without hitting singular behaviour
it is not very clear to me if you are asking about energy or momentum . you should also ask about a specific interaction process as there are many , this is required especially to answer your last , quantitative , question . however , generally speaking , a $\gamma$ photon cannot give some of its energy to anything else : it is all or nothing . even in the compton scattering , in which you get a less energetic photon , the initial photon is destroyed . the momentum must be conserved as well , so yes : when a photon hits another particle this is accelerated , you can even generate some measurable pressure with a very intense radiation !
when physicists say that a quantum field $\phi ( x ) $ is real-valued , they are usually referring to feynman 's path integral formulation of quantum field theory , which is equivalent to schwinger 's operator formulation . the values of a field $\phi ( x ) $ in the path integral formulations are numbers . e.g. : if the numbers are real , we say that the field $\phi ( x ) $ is real-valued . ( such a field $\phi ( x ) $ typically corresponds to a hermitian field operator $\hat{\phi} ( x ) $ in the operator formalism . ) if the numbers are complex , we say that the field $\phi ( x ) $ is complex-valued . if the numbers are grassmann-odd , we say that the field $\phi ( x ) $ is grassmann-odd . ( the numbers in this case are so-called supernumbers . see also this phys . se post . )
the flaw in your reasoning seems to be that $c$ is not in fact heat capacity . in newton 's law of cooling , the proportionality constant would be related inversely to the heat capacity of the two heated liquids/gasses/materials , and directly to the heat conductance of the object separating the two materials . a material with a higher heat capacity would have a smaller temperature change for a given temperature difference , and a thin piece of metal separating the materials would result in a much larger $\frac{dt}{dt}$ than a thick piece of styrofoam would . when bringing $c_1$ to $\infty$ , you are actually decreasing the heat capacity and increasing the conductance , both of which would cause $t_2$ to drop quickly as you observed .
1 . ) as the box moves to the left , the photon moves to the right , and their momenta is conserved . since the masses are moving proportionally and opposite to one another , the center of mass of that system remains fixed . 2 . ) it is the same as the center of mass of a system consisting of a large gymnasium and a tennis ball inside the gymnasium , if that helps make it clearer . it is just that photons are very , very , very , ( very ) " small " - but the idea behind it is the same . 3 . ) yes , it does mean that . the box has moved , but so has the photon , so the center of mass of the box-photon system has not moved , they have just shifted relative to one another . 4 . ) it means that the mass must be non-negligible , so that it is accounted for in calculating the center of mass of the system , so that 1 . ) is true . i hope this helps answer your questions , but please follow up if anything is unclear .
update - answer edited to be consistent with the latest version of the question . the different definitions you mentioned are not definitions . in fact , what you are describing are different representations of the lorentz algebra . representation theory plays a very important role in physics . as far as the lie algebra are concerned , the generators $l_{\mu\nu}$ are simply some operators with some defined commutation properties . the choices $l_{\mu\nu} = j_{\mu\nu} , s_{\mu\nu}$ and $m_{\mu\nu}$ are different realizations or representations of the same algebra . here , i am defining \begin{align} \left ( j_{\mu\nu} \right ) _{ab} and = - i \left ( \eta_{\mu a} \eta_{\nu b} - \eta_{\mu b} \eta_{\nu a} \right ) \\ \left ( s_{\mu\nu}\right ) _{ab} and = \frac{i}{4} [ \gamma_\mu , \gamma_\nu ] _{ab} \\ m_{\mu\nu} and = i \left ( x_\mu \partial_\nu + x_\nu \partial_\mu \right ) \end{align} another possible representation is the trivial one , where $l_{\mu\nu}=0$ . why is it important to have these different representations ? in physics , one has several different fields ( denoting particles ) . we know that these fields must transform in some way under the lorentz group ( among other things ) . the question then is , how do fields transform under the lorentz group ? the answer is simple . we pick different representations of the lorentz algebra , and then define the fields to transform under that representation ! for example objects transforming under the trivial representation are called scalars . objects transforming under $s_{\mu\nu}$ are called spinors . objects transforming under $j_{\mu\nu}$ are called vectors . one can come up with other representations as well , but these ones are the most common . what about $m_{\mu\nu}$ you ask ? the objects i described above are actually how non-fields transform ( for lack of a better term . i am simply referring to objects with no space-time dependence ) . on the other hand , in physics , we care about fields . in order to describe these guys , one needs to define not only the transformation of their components but also the space time dependences . this is done by including the $m_{\mu\nu}$ representation to all the definitions described above . we then have fields transforming under the trivial representation $l_{\mu\nu}= 0 + m_{\mu\nu}$ are called scalar fields . fields transforming under $s_{\mu\nu} + m_{\mu\nu} $ are called spinor fields . fields transforming under $j_{\mu\nu} + m_{\mu\nu}$ are called vector fields . mathematically , nothing makes these representations any more fundamental than the others . however , most of the particles in nature can be grouped into scalars ( higgs , pion ) , spinors ( quarks , leptons ) and vectors ( photon , w-boson , z-boson ) . thus , the above representations are often all that one talks about . as far as i know , clifford algebras are used only in constructing spinor representations of the lorentz algebra . there maybe some obscure context in some other part of physics where this pops up , but i have not seen it . of course , i am no expert in all of physics , so do not take my word for it . others might have a different perspective of this . finally , just to be explicit about how fields transform ( as requested ) i mention it here . a general field $\phi_a ( x ) $ transforms under a lorentz transformation as $$ \phi_a ( x ) \to \sum_b \left [ \exp \left ( \frac{i}{2} \omega^{\mu\nu} l_{\mu\nu} \right ) \right ] _{ab} \phi_b ( x ) $$ where $l_{\mu\nu}$ is the representation corresponding to the type of field $\phi_a ( x ) $ and $\omega^{\mu\nu}$ is the parameter of the lorentz transformation . for example , if $\phi_a ( x ) $ is a spinor , then $$ \phi_a ( x ) \to \sum_b \left [ \exp \left ( \frac{i}{2} \omega^{\mu\nu} \left ( s_{\mu\nu} + m_{\mu\nu} \right ) \right ) \right ] _{ab} \phi_b ( x ) $$
yes , the object would slow down to its terminal velocity . to see why , notice that the net force on a falling object of mass $m$ near the surface of the earth is \begin{align} f = f_\mathrm{drag} - mg \end{align} where $f_\mathrm{drag}$ is the force due to air resistance , and here i have assigned " up " to be the positive direction . on the other hand , the terminal velocity occurs when the force of drag equals $mg$ . when the force of drag is great than $mg$ , then then net force $f$ will be positive , and by newton 's second law , its acceleration will therefore point upward and will slow down the falling speed . this will happen until the object attains terminal velocity at which point the drag force and weight will again balance , and the object will remain at terminal velocity .
isotones are nuclides having the same number of neutrons . magic proton or neutron numbers give the nucleus greater stability . magic 82-isotone nuclides for instance : isobars are nuclides having the same mass number ( i.e. . sum of protons plus neutrons ) . the number of protons in beta-plus ( beta-minus ) decay decreases ( increases ) by a unit and the number of neutrons increases ( decreases ) by a unit , so that an isobar standing to the left ( right ) of the original nucleus is formed . it may be 1 , 2 or 3 beta-decay stable isobars . beta-decay energy of 154-isobar nuclides for instance : isotones and isobars have great significance for studying of nuclide stability .
hint : young 's modulus is given by the ratio of tensile stress over extensional strain ; the formula : $$y=\frac{ ( f/a ) }{ ( \delta l /l_0 ) }=\frac{fl_0}{a\delta l}$$ where $f$ is the force applied , $a$ is the cross-sectional area , $l_0$ the original length and $\delta l$ the extension or change in length of the object . in your case , $l_0=2 \ , \mathrm{m}$ . as stated in the question , the diameter of the wire is $d=0.0008 \ , \mathrm{m}$ . therefore the radius $r=d/2$ , and the area is given by , $$a=\pi r^2 = \frac{\pi}{4} d^2=5.027 \times 10^{-7} \ , \mathrm{m}^2$$ as we have assumed a cross-section of the wire is a perfect circle . the force applied is given by newton 's second law , $f=ma$ . can you take it from here ?
well , your lecturer certainly should not have put it like this , however it is true that you have got a lot wrong here . it is stuff you definitely will need to understand better if you are studying power engineering . first , you seem to think that electrons are attracted by magnetic north poles . they are not ; in fact stationary charges and magnetic fields are not concerned with each other in any way at all 1 ! next , you are talking about electrons in circular orbits about the nucleus . that is roughly the bohr model , which kind-of-sort-of-works , but not really . you want to familiarise yourself to the orbital model , which describes very well how bound electrons actually behave . even in an orbital , you might be inclined to talk about " the nucleus is off the center by a distance proportional to the voltage " . that is again kind-of-sort-of-right since the nucleus lies in a locally-harmonic potential which can be read as " pertubation by an electric field ( which in a fixed capacitor is proportional to the voltage ) will cause a proportional displacement of the nucleus " , but the way you phrase it it is still nonsense . voltage " is " not a distance , it is a potential ( i.e. . energy ) . anyway , this is not actually relevant to understanding rotating-magnet phenomena , i.e. inductance in coils . these are concerned only with conduction electrons , which are not bound to any particular atom at all but " move " through the entire conductor , which is why there can be currents . it is these moving electrons that experience a significant force in the presence of a magnetic field . what current actually is is the number and " speed " with which these electrons move through the conductor , while even a strong displacement of the bound ( valence ) electrons would not consolidate a current 2 . now , all of this seems to say there is not any such thing as inductance . sure there is ! only , it is rather more complicated : electrons at rest are not affected by stationary magnetic fields , but in the same way that moving electrons are affected by such fields , moving magnetic fields ( or , more generally , time-varying magnetic fields ) also cause a lorentz force upon resting electrons . so , effectively , what you are saying about electrons being moved around by moving magnetic fields is not all that wrong again , it only works quite a bit differently . a moving magnetic field will in fact " push resting conductance electrons " through a wire a bit , i.e. induce a voltage . but that voltage really can not be read as anything displacement-like , it is a fundamental electrodynamic phenomenon . in fact , the voltage in its pure , exact value can only be measured if you prevent the conductance electrons from moving , as otherwise they would themselves cause a magnetic field cancelling the inductance etc . pp . . as you see , the whole subject is quite a bit more complicated than you thought . i am sure you are capable of understanding it , but probably not in a few minutes , which is why your lecturer can not really be blamed for not trying to explain it right away . 1 actually , electrons are also small magnets themselves ( they have an instrisic quantum-mechanical spin ) and therefore are attracted to inhomogenic magnetic fields , but that is quite another issue . 2 actually , it would . . . but that is mostly relevant in the high-frequency-regime , i.e. bound electrons that jiggle back and forth very quickly .
i will start this with right hand grip rule for solenoids . . . " the coil ( solenoid ) is held in the right hand so that the fingers point the direction of current through the windings . then , the extended thumb points the direction of magnetic field " . ( which would be along the axis of the coil ) the higher the current , the more the magnetic field would be produced . . . for your example , let us assume the aluminium ring as a circular coil . when the uniform magnetic field is produced , there is a change in magnetic flux ( such as this increase in magnetic field ) along the axis of the ring , according to faraday 's law , induced current flows through the ring whose direction is given by lenz 's law . this induced current in the ring flows in a direction such that it opposes the magnetic field in the solenoid ( the one which actually produces it ) . ( but , the magnitude of induced magnetic field is always lesser than the field in the solenoid ) . anyways , there is a repulsion . with the maximum repulsive force produced , the ring is thrown off from the solenoid . this force always depends on the magnitude of $b$ in the solenoid .
approximate the person with a brick with a width of 0.9 m and a height of 1.2 meter . the torque around the tipping point caused by gravity is $mg\cdot l$ , where $l$ is the horizontal distance from the tipping point to the center of mass of the brick , i.e. half the brick width assuming it has a uniform density . you need to counteract this torque by exerting a horizontal force at the top . the torque caused by your force is $f\cdot h$ . to sum up : $$ mgl=fh \rightarrow f=\frac{mgl}{h}\approx\frac{70\cdot9.8\cdot0.45}{1.2}\approx257 \mathrm{n} $$ however , many of the assumptions made here may not be realistic in a real world person-toppling event . edit : the solution above assumes that the toppling force is horizontal . if you can apply the force at an angle you can get a slightly longer lever , maximally the length of the brick diagonal : $$ d=\sqrt{1.2^2+0.9^2}=1.5 $$ this gives the smallest possible toppling force : $$ f=\frac{mgl}{d}\approx\frac{70\cdot9.8\cdot0.45}{1.5}\approx206 \mathrm{n} $$
from what i have read in " american prometheus : the triumph and tragedy of j . robert oppenheimer " teller was the first one to express this concern before the trinity test . also quoting from : http://www.sciencemusings.com/2005/10/what-didnt-happen.html physicist edward teller considered another possibility . the huge temperature of a fission explosion -- tens of millions of degrees -- could fuse together nuclei of light elements , such as hydrogen , a process that also releases energy ( later , this insight would be the basis for hydrogen bombs ) . if the temperature of a detonation was high enough , nitrogen atoms in the atmosphere would fuse , releasing energy . ignition of atmospheric nitrogen might cause hydrogen in the oceans to fuse . the trinity experiment might inadvertently turn the entire planet into a chain-reaction fusion bomb . robert oppenheimer , chief of the american atomic scientists , took teller 's suggestion seriously . he discussed it with arthur compton , another leading physicist . " this would be the ultimate catastrophe , " wrote compton . " better to accept the slavery of the nazis than run a chance of drawing the final curtain on mankind ! " oppenheimer asked hans bethe and other physicists to check their calculations of the ignition temperature of nitrogen and the cooling effects expected in the fireball of a nuclear bomb . the new calculations indicated that an atmospheric conflagration was impossible . " bethe apparently then convincingly showed that the atmosphere would not be set on fire by a nuclear bomb .
there are many experiments , the most famous of them is probably the cavendish experiment , done in 18th century by a british scientist . i believe it was the first one to measure the force acting between two masses in a laboratory . he calculated the attraction between two lead spheres . http://en.wikipedia.org/wiki/cavendish_experiment so , i believe ' yes ' is the answer to your question .
the charge on the two plates is opposite , and equal to the key answer . the parameter q is the charge on one of the plates , not the sum of the absolute value of the two charges . the definition of capacitance is the charge per unit voltage , so that $q=cv$ , so 4f times 12v is 48 units of charge , which for the unit " f " is millionths of a coulomb c . this gives the book 's answer .
after a lot more searching , i have found the answer to my question ! :d below is a summary of the information i found . there is no specific webpage i can link to because i relied on sources who quoted other sources which no longer exist , but maybe this information can be useful to someone else someday . most of what i learned comes from professor lou bloomfield who currently teaches physics at the university of virginia . edit : none of this is quoted material : all information posted below has been completely reworded , and the analogies ( aside from the guitar string ) are mine . when surrounded by normal matter , a light wave 's electric field will cause electrons to jiggle at a rate equal to the frequency of the light wave : the electric component of the light wave will alternately attract and repel charged particles . when electrons in a material transparent to a certain frequency are excited by a light wave of that frequency , this takes energy away from the light wave . but surprisingly , no photons are absorbed : since the material is transparent to the frequency of the wave , there is no higher orbital which matches exactly the energy level an individual photon would impart to an electron . this means the energy transfer can not involve a real particle interaction . so what happens ? instead of absorbing one or more photons , the electrons enter a virtual quantum state : a temporary excitation that does not exactly match one of the states that the electron can occupy . this is very much like vibrating a guitar string by aiming sound at the string . if the sound you aim at the string matches a frequency that the string can vibrate at , it will cause the string to vibrate . if the sound you use is the wrong frequency , the string will wiggle a little bit as though trying to vibrate , then stop when the sound passes . that is what happens to the electrons : they borrow energy from the light wave , wiggle a little , and then return the energy . a virtual quantum state is very limited in duration , and does not count as a particle interaction . the light wave and the electron remain unentangled and continue to act as probability waves . the electron can only play with the light wave 's energy for a brief period before returning it . the characteristics of the light wave remain unchanged because there was no real particle interaction . so the light does not ricochet off of atoms , nor does it get emitted in the usual sense by the electrons which play with it . even though the interactions are all virtual , electrons are matter and they take time to jiggle . as this happens over and over and over again , it slows the progress of the wave . you might think of this like a kind of friction which acts against the progress of the wave . consider a car whose wheels turn at a constant speed , and imagine it encounters a series of large bumps that slow it down slightly . the speedometer is based on the wheel rotation , so it would say the car has not changed speed at all : it is just as fast as it was on flat terrain . the car will , however , cover less ground per time interval because some of the wheel-turning is used to surmount the humps . these humps are akin to the process of electrons temporarily borrowing energy from the light wave . so is the light wave truly slowed , or is the light still moving at c and only its progress is slowed ? this is not actually a well-formed question , and for all practical purposes the answer does not matter . however , i find it easier to think about it as slowing the wave 's progress . this means the characteristic that " light moves at speed c in all reference frames " still holds true , which makes it much easier for me to reason about relativistic effects . additionally , i was incorrect about different frequencies slowing by the same amount : lower frequencies are slowed less than higher frequencies . when the frequency is lower , even though the wave has less energy , the electrons will need to jiggle over a wider area ( they are pulled for a longer period , then pushed for a longer period ) . since the electrons remain bound to their atoms in this interaction , they can not be pulled out of the atom by a virtual excitation . so the slower the frequency is , the " more virtual " the excitation must be , and the less time the electrons have to play with the light . is this information useful ? if so , is there a way i could make it more accessible ? just curious , as i am very new to se .
i have written an answer to mathoverflow in which explicit formulas for the classical and quantum hamiltonians of a spin system ( generators of $su ( 2 ) ) $ were written explicitely . the classical hamiltonians are given by means of functions on the two sphere and the quantum hamiltonians by means of holomorphic differential operators ( which act on the sections of the quantum line bundle ) . for many spin system with a linear hamiltonian in each spin , one just has a distinct one particle hamiltonian per spin . sorry for referring to my own work , but it is by no means original .
you can make two parallel planes a distance $s$ apart in the way you describe , but you need an infinite stack of them $s$ apart to get diffraction . the reason there are no such stacks even though the distance between sites is $s$ , is because the honeycomb is actually a triangular tiling , where the corners of the triangle are every other atom . so the vector which connects the two adjacent is not actually a symmetry of the crystal ( i.e. . it is not a bravais lattice vector ) .
there are at least two mechanism of thermal conductivity - free electrons and thermal phonons . the first mechanism can be prevalent in metals , the second one is important in dielectrics . i did not look up thermal conductivity of glass , but such excellent dielectric as diamond has higher thermal conductivity than any metal , as far as i know .
in any supersymmetric theory you can choose the gauge coupling to be the coefficient of $w_\alpha^2$ in the superpotential . this gauge coupling runs only at one-loop , which is a fundamental consequence of the non-renormalization theorems . the other possible running coefficients are the kinetic terms , $z ( \mu ) qq^\dagger$ . these generally get renormalized to all orders in perturbation theory . in $\mathcal{n}=4$ the one-loop coefficient is zero . this is trivial ( just counting the fields ) . hence , the gauge coupling ( as defined above ) does not run . but $\mathcal{n}=4$ relates the gauge particles with the chiral superfields ( all the matter particles sit in one big representation of $\mathcal{n}=4$ ) and so the latter cannot get renormalized either . this is a slick and intuitive argument . . . similar logic operates in many $\mathcal{n}=2$ theories as well .
the reason why we consider the distortion of original field is superposition of nwe charge in system . but it is clear that in rule of superposition we should never consider the effect of any charge on itself . so after once defining the electric field we can use finite point charges to apply f=qe .
the pressure increases , not decreases , to $10^{-2}$ torr . as there is ( almost ) nothing in the chamber except oxygen , it contributes all the pressure . note that $10^{-6}+10^{-8} \neq 10^{-2}$ the flow does not matter at all .
you should visit the astronomical league site . they have several observational programs , ranging from naked eye to detailed telescopic spotting programs . i enjoyed the double star series myself . http://www.astroleague.org/
start from the beginning . why constraint relations ? why are they there ? let me emphasize : let 's take origin at top pulley which is at rest . note that length of top rope is constant : $a+b=k\implies a''+b''=0 \implies a''=-b''$ also length of second rope is constant : $ ( c-b ) + ( d-b ) =k\implies c''+d''=2b''$ note that $d$ is a constant as the top pulley and ground is rest : $c''=2b''$ hence , $c''=-2a''$ as stated in comments . also , everything we have done is futile and the block $m_2$ will hit the ground very quickly .
running the rges in reverse should be valid so long as you do not integrate over a scale where degrees of freedom enter/leave the theory . if you integrated out the electrons in qed , you had have irrevocably lost that information in your low energy description of interacting photons . you had see some non-renormalizable theory with interacting corrections to pure em but rg evolving to the uv would not tell you what that would be . just like rg evolving qed to the uv keeps you unaware of the strong or the weak sector physics . on the other hand , so long as you have not crossed any characteristic scale in your theory , the theory at the scales you have integrated out should be the same as the theory at the scale you are currently at . so you should be able to go back to where you came from . to summarize , so long as you do not integrate out some characteristic scale , you can keep going back and forth .
the earth is not a perfect sphere ( or even a perfect oblate spheroid ) so its gravitational field is not axially symmetric . you have probably seen the geoid measured by the goce and grace satellites . as the earth rotates the asymmetries in its gravitational field rotate with it , and any satellite whose orbital period is a ratio of one day can build up a resonance with the daily variations in the earth 's gravity . this is essentially the same physics as the resonances seen in , for example , the moons of jupiter . i had a quick google and found this paper that gives a fairly detailed analysis of the phenomenon . see in particular section 1.4 . the galileo satellites orbit 17 times every 10 days . this is sufficiently far from a simple ratio that resonances do not build up .
if your maxwell-boltzmann distribution is $\mu ( \vec v ) = \mu ( v ) = ( \frac{m}{2 \pi k t} ) ^{3/2} e^{- \frac{m v^2}{2 k t}}$ , then , if i am not mistaken , you should have to perform an integral with $\theta$ limited between $0$ and $\pi/2$ of kind $ i = na \int_{0 \le \theta \le \pi/2} d^3 \vec v \mu ( \vec v ) ( \vec v . \vec n ) $ , where $\vec n$ is the unit normal to $a$ , and we choose $\theta$ such as $\vec v . \vec n = v \cos \theta$ , so you would have : $ i = na\int_0^{2 \pi} d\phi \int_0^{\pi/2} d\theta \sin \theta \cos \theta \int_0^{+\infty} dv ~v^3 ~\mu ( v ) $ . note that $v_{avg} = \int_0^{2 \pi} d\phi \int_0^{\pi} d\theta \sin \theta \int_0^{+\infty} dv ~v^3 ~\mu ( v ) $ . your result should be $i = \large \frac{na v_{avg}}{4}$
i have just found this interesting paper by thomas breuer from 1995: https://homepages.fhv.at/tb/cms/?download=tbphilsc.pdf the paper seems to prove that for a system that includes observer himself there are quantum states which are indistinguishable by the observer however technical means he employs , while he can measure any such states in the brains of other people . the paper claims this proves that not only quantum mecanics is not universal theory that is applicable to all objects in the universe , but that no such universal theory can exist . it also follows that in the world that the observer himself observes there is hidden information in his brain which cannot be extracted and read by any means even with help of other people ( while the same information can be easily extracted from the brains of other people ) . i do not know however how to interpret it regarding the wavefunction . does it mean the observer 's wavefunction indeterminate , singular or inexistent ? update . and this this paper says it all . there will be subjective decoherence once the observer wants to measure himself . so he will see himself in a mixed state while others in the same situation will be observed as if they were in coherent state . note that this position is often taken for quantum mechanics . according to many interpretations , as for example the one of bohr , or the one of london and bauer ( 1939 ) and wigner ( 1961 , 1963 ) , or even perhaps19 the one of von neumann ( 1932 ) , the “true” observer ( or his mind ) cannot be described by quantum mechanics . these authors say that if quantum mechanics is universally valid at all , then it is so only in the relative sense that every observer can perhaps apply it to any selected part of the world , except himself . it supposedly applies to schroedinger’s cat , wigner’s friend and wigner himself under the condition that they lose their status of observer and are observed by something or somebody else .
this might be more of a math question . this is a peculiar thing about three-dimensional space . note that in three dimensions , an area such as a plane is a two dimensional subspace . on a sheet of paper you only need two numbers to unambiguously denote a point . now imagine standing on the sheet of paper , the direction your head points to will always be a way to know how this plane is oriented in space . this is called the " normal " vector to this plane , it is at a right angle to the plane . if you now choos the convention to have the length of this vector ( "the norm" ) equal to the area of this surface , you get a complete description of the two dimensional plane , its orientation in three dimensional space ( the vector part ) and how big this plane is ( the length of this vector ) . mathematically , you can express this by the " cross product " $$\vec c=\vec a\times\vec b$$ whose magnitude is defined as $|c| = |a||b|sin\theta$ which is equal to the area of the parallelogram those to vectors ( which really define a plane ) span . to steal this picture from wikipedia 's article on the cross product : as i said in the beginning this is a very special thing for three dimensions , in higher dimensions , it does not work as neatly for various reasons . if you want to learn more about this topic a keyword would be " exterior algebra " update : as for the physical significance of this concept , prominent examples are vector fields flowing through surfaces . take a circular wire . this circle can be oriented in various ways in 3d . if you have an external magnetic field , you might know that this can induce an electric current , proportional to the rate of change of the amount flowing through the circle ( think of this as how much the arrows perforate the area ) . if the magnetic field vectors are parallel to the circle ( and thus orthogonal to its normal vector ) they do not " perforate " the area at all , so the flow through this area is zero . on the other hand , if the field vectors are orthogonal to the plane ( i.e. . parallel to the normal ) , the maximally " perforate " this area and the flow is maximal . if you change the orientation of between those two states you can get electrical current .
it is also matter of convention . consider a portion $v$ ( of a continuous body $b$ ) bounded by a closed surface $\partial v$ and a point $p$ on $\partial v$ . if $n$ is the outward unit normal to $\partial v$ at $p$: $$f ( p , n ) _i = \sum_{j=1}^3\sigma ( p ) _{ij}n^j$$ is the surface density of force acting on $p$ and due to the part of $b$ outside of $v$ . ( one could adopt the other convention , where $n$ is the inward unit vector . ) with this definition if $f$ is parallel to $-n$ , $f$ is compressive . consequently , always referring to that definition , if $\sigma$ is diagonal , a positive eigenvalue means traction along the corresponding eigenvector $n$ interpreted as the outward unit vector to on closed surface , and a negative eigenvalue means compression along the corresponding eigenvector . the direction of $n$ as an eigevector is irrelevant , it just indicates where the closed surface is placed . indeed the stress tensor of a fluid in equilibrium is $-p \delta_{ij}$ with $p&gt ; 0$ . it implies that every portion of the fluid is compressed .
i am not entirely sure what you are trying to ask , but i think it is this : when is the schrodinger equation ( or a similar differential equation ) separable ? what conditions must the potential function satisfy ? the short answer is that the schrodinger equation is separable when the potential is independent of time ( though there maybe time independent potentials that also work ) . a differential equation of two independent variables is separable if the equation can be algebraically manipulated such that only one type of variable appears on each side of the equation . in the case of a partial differential equation ( i.e. . the schrodinger equation ) dependent variable can be written as a product of functions of the two independent variables ; that is $$ \psi ( x , t ) = \rho ( x ) \phi ( t ) $$ if we apply the schrodinger equation to this " guess " and assume $v$ is independent of time we find ( after a few steps ) : $$ -\frac{\hbar^2}{2m} \frac{d^2\rho}{dx^2} = ( e-v ) \rho $$ for and $e=$ constant . note that this equation is an ordinary differential equation though we started with a partial differential equation . more importantly , since $\rho ( x ) $ is independent of time , and therefore so is this entire equation . hence , it is called the time independent schrodinger equation . this is essentially a shortened version of the derivation provided in griffith 's book .
all of them . even molecules show their wave-like nature , as does , in principle , every object . speaking of these topics an interesting read about diffraction of c60 molecules is : http://www.univie.ac.at/qfp/research/matterwave/c60/ the point is that the wave-like nature of objects can only be observed at lengths comparable the object 's de broglie wavelength , defined by : $$ \lambda = \frac{h}{p} $$ where $h$ is the planck constant and $p$ is the object momentum . for an object moving at non-relativistic speeds you may remember that the momentum is defined as $\vec{p}=m\vec{v}$ ; the de broglie wavelength is then inversely proportional to the object mass . the bigger an object is , the less relevant its wave-like nature is , and that is why in everyday experience we are not used to observing the wave-like nature of massive objects .
if you are thinking about light the ' conservative answer ' would be : for this experment photon behaves as a wave , it doesnt have actual size , and so the only thing that will happen is that the wave will stretch its lenght , but the photon itself will not stretch as it has no diameter .
electricity ( and electromagnetic fields in general ) will invariably follow the " easiest " path . the entire point of a faraday cage is to provide the electricity from whatever discharge you are facing with an " easy " path that does not involve the person inside the cage . holes will not change a thing . why would the electricity even try to get through the holes if there is so much conducting chicken wire for it to go through ? electricity loves chicken wire , it hates air . so if there is chicken wire , it just goes through it . nonetheless it is perfectly possible to get electric shock inside a faraday cage . just install an electric socket inside the cage , linked to the electric grid outside , and then do something stupid like cram your fingers into the socket . . . ( mark the the frequency of any external discharge will not change a thing . electricity prefers conducting material to isolators like air and people , so the faraday cage will shield you from all external discharges . )
a few comments : 1 ) we do not know what happens with objects of volume smaller than $\ell_{p}^{3}$ , where $\ell_{p}$ is the planck length , which is approximately $10^{-35}$ m . the known laws of physics do not hold for anything that small . 2 ) general relativity , in its classical glory , predicts that objects will collapse down to a shape with zero volume . a spinning black hole will take the shape of a ring , with a radius determined by the angular momentum and mass of the black hole . since it is virtually impossible to tune down the angular momentum of physical objects to exactly zero , we will expect most singularities to be ring-shaped , perhaps having a $\ell_{p}$ thickness . 3 ) the singularity is not expected to be observable to outside observers , so most astrophysicists will instead talk about the size of the black hole horizon , which is a macroscopic size .
the raw data from modern particle physics experiments are many terabytes ( even petabytes ) in size , and quite complicated . for collider experiments the detectors are compound , layered devices with three or more different technologies used by five or more distinct subsystems , plus ancillary monitoring of detector performance , temperature and humidity conditions in the experimental hall , data provided by the accelerator operating crew on the state of the beam , and on and on and on . there are ten of thousands of individual detector channels and hundreds of " slow " devices ( like the thermoments , magnet currents , beam current monitors , etc . . . ) . all of this has been pre-filterd by the trigger hardware ( and exactly what filtering was applied changes over time ) . for neutrino experiments the data are detailed information about the charge detected from photo-tubes ( some combination of total charge in a window , peak voltage , peak time , onset time , and/or digitized wave forms ) for hundreds or thousands of pmts . plus environmental monitoring like that done by the collider people . in both cases there scads of calibration data , changes in operating conditions throughout the data taking period , and sometimes replacement of re-tuning of sub-systems part way through . there is typically many tens of thousands of lines of custom computer code for opening and processing the data files . code written by physicists . now , particle physicists are a little more professional about coding then some of their peers , but that does not mean state of the art process and beautiful code . it generally takes many thousands of grad-student and post-doc hours to reduce this to something physics can be extracted from . there is a reason we call this " big science " . that said , you generally can get the data . eventually . ( each collaboration will hold theirs for a while to insure they get to publish first . ) how do you get it ? just ask . but you will have to provide your own storage ( and possibly copying hardware ) ; come to where the data are kept ; understand that the documentation will be scattered over hundreds or thousands of internal ( to the collaboration ) documents written as they went along by diverse authors some of whom have english as a second or third language ( and may evidence some idiosyncrasies ) ; and that help interpreting all this will be terse as these people have moved on and have other projects keeping them busy . and you may have to convince the people with the data that you have the capacity to manage it . the availability of partly processed data sets is not something i am as sure about , but you could try asking for that too . the worst that can happen is you get told " no " . but even if you can get this , do not image that it is easy to work with . if i have not dissuaded you , let me suggest a practical method for getting started . go to the nearest university that has a nuclear or particle physics group , and ask to help out . really . there is always a need for lab monkeys , and you will learn as you go along because you can not do the work if they do not teach you stuff . in the process you will learn how some of the sub-systems work . get a feel for what kind of raw data they return and how it is processed into less raw data . if you ask people will tell you how the less raw data can be transformed into still more physics-like information and eventually reconstructed into particles . make some contacts in the business . begin able to say " i work with prof . smith and podunk u . " is much better than " i am interested . " when it comes to getting access to data .
the deal here is that you have to be very careful about what quantity you are interested in and what you have . assumptions you have measurements $f_i$ of the fraction of stuff ( does not really matter what ) during time interval $i$ . you may also have measurements of $o_i$ of the total output of the medium in which stuff makes up a fraction . alternately you may only know $\bar{o}$ the average output or the total output over the entire time range $\mathcal{o}$ . note that for $n$ measurements at uniform spacing $\mathcal{o} = n * \bar{o}$ . case 1: you want to know " how much stuff " totaled over several time periods and you have both fractional values and outputs for every period . this is as good as it gets . you need to add up the daily stuff . the daily amount of stuff is $s_i = f_i o_i$ . that is just the definition of $f_i$ . so assuming you have the daily outputs you get : $$ \mathcal{s} = \sum_i s_i = \sum_i f_i o_i \quad . $$ case 2: you want to know " how much stuff " totaled over several time periods , but you have the total output or average output without periodic output values . the best you can do is $$\mathcal{s} \approx \sum_{i=1}^n f_i \bar{o} = \mathcal{o} \frac{1}{n}\sum_{i=1}^n f_i = \bar{o} \bar{f}$$ where $\bar{f}$ is defined by this relation as the mean fraction . in this case it makes sense to take a mean of the fractional reading , but only because you do not have enough data to get the right answer . this will be approximately correct if ( 1 ) the $o_i$s have small variance or ( 2 ) the $f_i$s have a small variance or ( 3 ) you have a lot of data and the $f_i$s and $o_i$s are uncorrelated . case 3: you want to show that the fraction is uniformly above or below some limit . then you just need the $f_i$s , and computing their mean and variance is fine as long as ( 1 ) the answer is a long way from the limit and ( 2 ) the $f_i$ are uncorrelated with the $o_i$s . case 4: you want to show that the stuff is uniformly above or below some limit . then you need the daily values to do it right . in the absence of the daily output you can fall back on the average output again , but it will only be reliable if all three conditions listed for case 2 apply . side note : you were concerned in the comments that 2 million parts per million does not make sense . and you are partially right . you can not have a fractional concentration of 2 million ppm , but there are cases when that value is meaningful . you probably already understand this . people are happy to talk about 200% in some cases , but " percent " is literally " per one hundred " . two hundred pars per hundred is the number 2 and it makes sense in cases involving changes but not in cases involve the fraction of people who qualify for something .
yeah , definitely . one example is an inelastic collision , where both masses will have the same velocity after colliding . in this case , let 's say a bullet of mass $m$ and speed $v_0$ hits a stationary rock of mass $m$ and they stick together and move with a final speed $v$ . intuitively , you can already tell that their velocity must be in the same direction as the bullet 's initial velocity . but let 's make it explicit : from conservation of momentum , we have that $$mv_0=mv+mv$$ $$mv_0= ( m+m ) v$$ since mass must always be positive , $v$ must have the same sign as $v_0$ , which means that the bullet moves in the same direction as before-keeps moving forward ( although at a slower speed ) -and is not ' reflected ' by the rock . edit ( to respond to additional question ) . under what conditions does this happen ? essentially , the bullet would have to stick ' into ' the rock .
yes , the image is 3d , though the 2d approximation we get by using a screen is pretty good . because the different parts of the dog are at different distances from the lens the images of those parts of the dog are also formed at different distances from the lens . so the image is a faithful 3d replica of the dog . however we can move in or out from the point of perfect focus and still get a pretty good focus . this is the origin of depth of field . in fact it can be hard to work out exactly where the focus is sharpest , especially since in many cases the difference in distance between e.g. the dog 's nose and its ears is small compared to the average distance of the dog from the lens . so if we put a screen in the average position of the dog 's image the whole dog will appear to be in focus even though it is frontmost and rearmost parts will actually be slightly out of focus . the obvious way to record 3-d images is to use a hologram instead of a normal camera . although it is in priciple possible to get the 3-d image with a normal lens , in practise it is not possible to find the point of perfect focus precisely enough .
http://en.wikipedia.org/wiki/mathematical_descriptions_of_opacity the propegation constant has a real and an imaginary part . one of those is equal to the angular wavenumber , the other is proportional to the absorption coefficient . which is which ( which is the real part and which is the imaginary part ) depends on what definition you are using for the term " propegation constant " . there is more than one definition in common use .
yes , there are . such materials are called " saturable absorbers , " and are ( or at least , have been ) used as switches in some laser designs . the one i recall is a nickel acetate dye , although there are others . basically , the molecules absorb single photons at the laser wavelength , but when the intensity is great enough that two photons are absorbed simultaneously ( or precisely , within the decay time of the single-photon excited state ) , the excited molecule no longer absorbs at that wavelength and the high-power beam is transmitted . this was done in , e.g. , early hand-held laser rangefinders to produce a higher-power , shorter length output pulse . interestingly enough , there are also saturable transmitters . essentially this is what is layered onto eyeglasses to produce those lenses which are sunglasses in bright light but clear in dim light .
i will just try to answer 1 ) . my take is that , at least in some sense , there is no violation of the uncertainty principle , as the droplet 's coordinate and momentum are not defined very well : the droplet interacts with its own wave , so , for example , momentum is distributed between the droplet and its wave , and it is difficult to define the droplet 's coordinate , as the droplet influences its surroundings through its wave as well . you may tell me that the droplet is classical , so its coordinate and momentum should be determined independently of its wave . while such approach is certainly legitimate , when we are talking about the uncertainty principle , we should make an apples-to-apples comparison with microscopic interference , in which case we do not divide the particle and , say , its electromagnetic field . in quantum mechanics we tend to assume that wherever and whenever we detect some influence of the particle , we measure its coordinate with some precision , although , e.g. , the effective radius of the coulomb interaction is infinite . by the way , one of the reasons the couder experiment seems interesting to me , it shows that some traditional mantras about quantum interference , while relatively consistent , may look absurd when applied to the pretty similar case of the couder experiment .
be careful not to become confused between two distinct but somewhat similar concepts : thermal conductivity and specific heat . conductivity is the ability of a material to transfer heat . that is , if you had a long tube made of a given material , conductivity explains the rate at which heat would flow from one end to the other . specific heat , on the other hand , describes the amount of heat required to increase the temperature of an object . a material with a high specific heat requires a lot of energy transfer to change the temperature even a small amount , whereas a material with a low specific heat requires little energy transfer to change the temperature the same amount . for example , metals have low specific heats , since they heat up and cool down quickly . on the other hand , water has a very high specific heat , which is ( partially ) why water on a beach is still cold , even on a hot day . air is a very poor conductor , having a thermal conductivity of 0.024 w/m k . water is a slightly better conductor , with a conductivity of 0.58 w/m k . however , metals conduct much more than either of these two materials - for example , aluminum 's conductivity is 205 w/m k . which is over 350 times that of water . ( source : http://www.engineeringtoolbox.com/thermal-conductivity-d_429.html ) the specific heat values are radically different . air has a specific heat of approximately 1 j/g k , while water is at 4.186 j/g k , and metals are usually around 0.2 j/g k . that is , metals change temperature very significantly for a given heat input and water changes temperature very little , with air somewhere in between . electrical conductivity is not very closely related to thermal conductivity ( although for metals , there is the wiedemann–franz law ) , so if you are seeking information on thermal conductivity only , make sure you do not confuse it with electrical conductivity . if you want more guidance , reply to this post in a comment , and we will help you out .
a simple google-book search of " solar declination " lead me to this google-book preview of solar energy engineering : processes and systems by soteris a . kalogirou . this book gives the spencer formula as equation ( 2.6 ) , on page 55 .
your equation ( 1 ) describes approximately the centre-of-mass ( com ) coordinates of every atom = ( some nucleons + some electrons ) system . of course there are many other degrees of freedom that are not taken into account in this description . but those degrees of freedom can always be ignored unless they become correlated with the centre-of-mass coordinates . this will happen if , for example , molecules form , or if inelastic collisions result in transitions between internal electronic states . at low densities and ultracold temperatures ( the normal regime for alkali atom bec experiments ) , such processes are strongly suppressed . this is why the internal degrees of freedom are normally not written in the wavefunction explicitly . if you want to write them in , you would have something like $$ \psi^{\prime} ( \mathbf{r} , \mathbf{s} ) = \psi ( r_1 , r_2 , \ldots ) \times \phi ( s_1 , s_2 , \ldots ) , $$ where $\psi ( \mathbf{r} ) $ is given by your equation ( 1 ) and $\phi ( \mathbf{s} ) $ describes all the ( relative ) coordinates $s_1 , s_2 , \ldots$ of the electrons and nucleons . the lack of correlations between com and internal states means that $\phi ( \mathbf{s} ) $ appears as a boring common factor multiplying all expressions , so it can just be thrown away* . the internal degrees of freedom are nevertheless extremely important since they result in interactions between the atoms . this interaction can be modelled fairly well by a lennard-jones type potential . this can be understood intuitively as a sum of two parts : 1 ) a long-range attraction between the nucleons and electrons , 2 ) a short-range coulomb/pauli repulsion once the electron orbitals start to overlap . however , this intuition only makes sense within the born-oppenheimer approximation , in which the quantum correlations between nuclear and electronic degrees of freedom are neglected . in principle , this interaction is a horrendously complicated many-fermion problem . even for just a single pair of $^{87}$rb atoms , the full scattering problem taking all the nuclear and electronic degrees of freedom could not be solved on even the biggest supercomputer in the world . fortunately , there is never enough motional energy ( at low temperatures ) in the centre-of-mass degrees of freedom to cause transitions between the internal states , meaning that there is no chance for the internal structure to become correlated with the external motion . it is therefore perfectly fine in practice to treat the atoms as structureless blobs of matter , with some effective classical potential replacing the complicated quantum motions and interactions of their internal parts . the important role of the interatomic scattering interaction in cold atom experiments is that it correlates the centre-of-mass coordinates $r_1 , r_2 , \ldots$ of the atoms . since your equation ( 1 ) neglects these correlations , it can only be an approximation . what you have written is called the gross-pitaevskii ( gp ) approximation for the ground state . this is the zeroth-order term in a systematic expansion of the ground state in terms of the parameter $ ( na_s^3 ) ^{1/2}$ , where $n$ is the density and $a_s$ is the $s$-wave scattering length . the next order of approximation is called the " bogoliubov vacuum " , and takes the form $$\psi ( r_1 , r_2 , \ldots ) = \prod_{i&lt ; j} \psi ( r_i-r_j ) . $$ the reason that the gp approximation normally gives good results is that the pair wave function $\psi ( r ) $ only has a non-trivial behaviour for very small $r$ , i.e. less than 100 nm . if the spatial resolution of your measuring device is larger than this , which is normally the case for diffraction-limited light scattering , the gp and bogoliubov ground states give equivalent predictions . a good list of references on these issues can be found in the leggett rmp is a good source . leggett also wrote an interesting open-access paper on similar topics . *of course , if you assume that the positions of the electrons or nucleons could be measured , then $\phi ( \mathbf{s} ) $ would need to be taken into account .
as an orbit goes around the earth , your challenge if you launch vertically is that once you reach the desired height you will then need to accelerate sideways to orbital velocity . for an l2 orbit , this is around 1km/s with respect to the earth . so you need to carry all that fuel up with you , to then burn it . that is a vast amount of mass wasted in the initial launch , so realistically it is not going to happen . in reality , the launch profiles used give you orbital velocity and height as efficiently as possible , to maximise payload .
it is obviously not a sharp cut-off , but as a general guide sound waves cannot propagate if their wavelength is equal to or less than the mean free path of the gas molecules . this means that even for arbitrarily low pressures sound will still propagate provided the wavelength is long enough . possibly this is stretching a point , but even in interstellar gas clouds sound waves ( more precisely shock waves ) will propagate , but their length scale is on the order of light years .
no , physics is not rigorous in the sense of mathematics . there are standards of rigor for experiments , but that is a different kind of thing entirely . that is not to say that physicists just wave their hands in their arguments [ only sometimes ; ) ] , but rather that it does not come even close to a formal axiomatized foundation like in mathematics . here 's an excerpt from r . feynman 's lecture the relation of mathematics and physics , available on youtube , which is also present in his book , character of physical law ( ch . 2 ) : there are two kinds of ways of looking at mathematics , which for the purposes of this lecture , i will call the the babylonian tradition and the greek tradition . in babylonian schools in mathematics , the student would learn something by doing a large number of examples until he caught on to the general rule . also , a large amount of geometry was known . . . and some degree of argument was available to go from one thing to another . . . . but euclid discovered that there was a way in which all the theorems of geometry could be ordered from a set of axioms that were particularly simple . . . the babylonian attitude . . . is that you have to know all the various theorems and many of the connections in between , but you never really realized that it could all come up from a bunch of axioms . . . [ e ] ven in mathematics , you can start in different places . . . . the mathematical tradition of today is to start with some particular ones which are chosen by some kind of convention to be axioms and then to build up the structure from there . . . . the method of starting from axioms is not efficient in obtaining the theorems . . . . in physics we need the babylonian methods , and not the euclidean or greek method . the rest of the lecture is also interesting and i recommend it . he goes on ( with an example of deriving conservation of angular momentum from newton 's law of gravitation and having it generalized ) : we can deduce ( often ) from one part of physics , like the law of gravitation , a principle which turns out to be much more valid than the derivation . this does not happen in mathematics , that the theorems come out in places where they are not supposed to be .
i will take a swing at this , but bear in mind that you probably will not get definitive answers because you are asking about two active and difficult areas of research ( pop iii star formation and re-ionization ) . i will answer the particular questions , but i am hoping you get a feel for the fact that we do not have clear-cut answers yet . during what range of years after the big bang did the stars form ? the consensus is somewhere in the range $20&lt ; z&lt ; 50$ , which corresponds to 50 myr $&lt ; t&lt ; $ 200 myr . the spread is quite large because star formation depends broadly on gas density and temperature . because the big bang produces some big density perturbations and some small ones , they will form stars at different times . what is the expected range of masses of these stars . . . this is really , really hard to answer . until about a year ago , consensus was settled on a very heavy mass distribution , with many ( if not most ? ) stars in a range 100-1000 $m_\odot$ ( see e.g. the 2004 review by bromm and larson ) . this is argued on the grounds that the smallest gravitationally unstable mass in a homogeneous isothermal gas , the jeans mass , runs like $t^{3/2}$ and primordial gas cannot cool as much as metal-polluted gas . the difference in temperature is about a factor of 30 , so pop-iii stars would naively be about 100 times heavier than modern stars , whose mass distribution seems to peak around 0.5 $m_\odot$ . however , recently ( as in , articles in science in the last few weeks ) have reported very detailed simulations of early star formation where the stars stop their own growth as they start to radiate . the result is stars that are a few times 10 $m_\odot$ . still very big by modern standards , but not as big as previously thought . so the jury is out , imo . . . . and what is the expected lifetime before they supernova ? well , it depends on how large they are , but broadly stars of about 40 solar masses last about a million years or less . how they evolve is unclear because it is difficult to compute the rate at which they might lose mass from their surfaces . i assume these stars resulted in the re-ionization of ism ( interstellar medium ) , so what is the evidence and estimates of the age of the re-ionization era ? the role of pop iii stars in reionization is not at all clear . this is slightly outside of my own work , but from my own knowledge i think it is quite well known that ionization was complete by about redshift $z\approx6$ , which is about a billion years after the big bang . remember that many pop iii stars could have already been born , evolved , and died , producing enough metals to produce pop ii stars in the next generation . working out just how much radiation had been poured out is theoretically very difficult . however , as far as i know , observations of a $z=7.085$ quasar have given us some idea that the intergalactic medium around it was not re-ionized . so its more likely that stars after pop iii ( and maybe agn/quasars ? ) had more to do with re-ionization because it happened broadly later than they were born . that is just wild speculation on my part , though .
let $q$ denote the set of all possible configurations of the system ( the configuration manifold ) . consider a point $q_0\in q$ . for the sake of conceptual clarity , and to make contact with physics notation , let 's work in some local coordinate patch around $q_0$ . suppose that $q_0$ represents the position of the system under consideration at time $t_0$ . at a given time $t$ later , the system will be at some position say $q ( t ) $ that is determined by the evolution equations ( the euler-lagrange equations if we are doing lagrangian mechanics ) , and the quantity \begin{align} q ( t ) - q ( t_0 ) = q ( t ) - q_0 \end{align} would be the displacement of the system after a time $t$ . suppose , instead we consider some other curve $\gamma ( s ) $ in the configuration space which starts at the point $t_0$ ; \begin{align} \gamma ( s_0 ) = q_0 , \end{align} and suppose that we compute the displacement \begin{align} \gamma ( s ) - \gamma ( s_0 ) = \gamma ( s ) - q_0 \end{align} that would result from moving along this other curve of our choosing . we call this displacement the virtual displacement after a time $t$ corresponding to moving along the curve $\gamma$ . it is called virtual because it is the displacement in the position of the system that would occur if the system were to move along the curve $\gamma$ of our choosing -- a " virtual " curve as opposed to the " real " curve along which the system travels according to the lagrangian evolution of the system . note . i used the parameter $s$ for the curve $\gamma$ instead of $t$ to emphasize that moving along that curve does not correspond to time-evolution . now what about virtual " infinitesimal " displacements ? well , recall that the term " infinitesimal " in physics essentially always refers to " first order " approximations , see , e.g. this se post : rigorous underpinnings of infinitesimals in physics so when we are discussing a virtual infinitesimal displacement , what we have in mind is taking the virtual displacement $\gamma ( s ) - q_0$ , taylor expanding it to first order in $s$ , and extracting only the first order term . let 's do this : \begin{align} \gamma ( s ) - q_0 = \gamma ( s_0 ) + \dot\gamma ( s_0 ) t + o ( s^2 ) - q_0 \end{align} using the fact that $\gamma ( s_0 ) = q_0$ , we see that the taylor expansion of the virtual displacement is \begin{align} \gamma ( s ) - q_0 = \dot\gamma ( s_0 ) t + o ( s^2 ) , \end{align} and now we notice that to first order in $s$ , the size of the virtual displacement is controlled by the coefficient of $s$ , namely $\dot\gamma ( s ) $ . in other words , virtual infinitesimal displacements ( meaning we just keep the first order contribution in $s$ ) , are determined by the velocity vector of the chosen " virtual curve " at $s_0$ . but if you have taken a differential geometry course , then you know that velocities of curves on a manifold are simply tangent vectors to that manifold ! so virtual infinitesimal displacements can be associated with tangent vectors to the configuration manifold . the intuition to keep in mind here as that a virtual displacement just tells us how far we would get away from a certain point on the manifold if we were to travel on a certain curve of our choosing that may not coincide with the actual motion of the system determined by time evolution . the " infinitesimal " part and identifying this part with tangent vectors comes simply from considering what happens only to first order .
let 's take your last question first . let the stress tensor at a point ( x , y , z ) in the fluid be given as $\sigma$ . you can pick a cartesian basis $\{ e_1 , e_2 , e_3 \}$ and express the components of the tensor in that basis $$ \begin{bmatrix} \sigma_{xx} and \sigma_{xy} and \sigma_{xz} \\ \sigma_{xy} and \sigma_{yy} and \sigma_{yz} \\ \sigma_{xz} and \sigma_{yz} and \sigma_{zz} \end{bmatrix} $$ the normal stresses are simply $\sigma_{xx} , \sigma_{yy}$ and $\sigma_{zz}$ . it is important to realize that these stresses will have different values in another basis . clearly , you can not attach too much physical significance to things that are basis dependent . however , it is a theorem of continuum mechanics that you can always find at least one basis in which the off-diagonal ( shear terms ) are zero . in this basis , the tensor components are $$ \begin{bmatrix} \sigma_{1} and 0 and 0 \\ 0 and \sigma_{2} and 0 \\ 0 and 0 and \sigma_{3} \end{bmatrix} $$ these numbers have actual physical significance . $\max ( {\sigma_1 , \sigma_2 , \sigma_3} ) $ is the largest principal normal stress at the point . similarly , $\min ( {\sigma_1 , \sigma_2 , \sigma_3} ) $ is the smallest normal stress at that point . it is not too hard to realize that $\sigma_1 , \sigma_2 , \sigma_3$ are the eigenvalues of the stress tensor . on the other hand , the pressure is ( -1/3 ) times the trace of the stress tensor , i.e. $$ p = -\frac{1}{3} \sigma_{jj} $$ the trace is an invariant of the stress tensor , so if you take the sum of the diagonals of the stress tensor in any basis you will get the same value . mathematically , $$ tr ( [ \beta ] [ \sigma_{ij} ] [ \beta ] ^t ) = tr ( \sigma_{ij} ) $$ so you see that the pressure and the normal stress are very different entities indeed . in particular , the pressure is isotropic - it has no preferred direction . now consider a state of pure shear in a fluid . to keep matters simple , we will assume planar flow and ignore out of plane components . the stress tensor for pure shear in our standard basis looks like this $$ \begin{bmatrix} 0 and \tau \\ \tau and 0 \\ \end{bmatrix} $$ looks like the normal stresses are zero , right ? not so fast . as this is a symmetric real tensor , you can always find another basis in which you have normal stress components ! in fact , if you solve the eigenvalue problem setting $\det ( \sigma-\lambda i ) =0$ , you get principal normal stresses of $\pm \tau$ . so , in a coordinate system with basis vectors $e'_1 = \{ ( \frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} ) \}$ and $e'_2 = \{ ( -\frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} ) \}$ rather than $\{ ( 1,0 ) , ( 0,1 ) \}$ , you get a stress tensor from a situation of " pure " shear that looks like this $$ \begin{bmatrix} \tau and 0 \\ 0 and -\tau \\ \end{bmatrix} $$ you can easily verify this by carrying out the change of basis yourself . so what looks like " pure shear " in one basis is biaxial normal stresses in another basis . since the signs are different , you have both tensile and compressive normal stresses in your fluid .
if you look closely at the crocodiles ' tails you will see that they wave their tails from side to side to provide propulsion for the jump . compare this to a fish swimming : the side to side motion of the fish 's tail propels it forward , and the crocodiles are using exactly the same sort of side to side motion to propel themselves upwards .
from the lagrangian one can obtain the equations of motion , called the euler-lagrange equations . these equations are , in general and also in this case , differential equations . as far as i understand your level of knowledge , you do not know anything about this subject ? in differential equations you are looking for a whole function as a solution , not only a variable ( in this case the solution is the function you are looking for : plug in the time and get out the position of each mass ) . the functions not only occur in an algebraic equation , there are also derivatives of this function in this equations . so in your case the two functions you are looking for are $\theta_1 ( t ) $ and $\theta_2 ( t ) $ . unfortunately you can not analytically solve these equations stated in the wiki article : " it is not possible to go further and integrate these equations analytically , to get formulae for θ1 and θ2 as functions of time . it is however possible to perform this integration numerically using the runge kutta method or similar techniques . " this means : without numerics you have no chance , at least as far as i know .
it is not necessarily true . for a zero potential $v_2$ you have $p_2=0$ , whereas if $v_1$ is a rectangular pit , in general , $p_1&gt ; 0$ .
no , in general you cannot divide one vector by another . it is possible to prove that no vector multiplication on three dimensions will be well-behaved enough to have division as we understand it . regarding force , area and pressure , the most fruitful way is to say that force is area times pressure : $$ \vec f=p\cdot \vec a . $$ as it turns out , pressure is not actually a scalar but a matrix ( or , more technically , a rank 2 tensor ) . this is because , in certain situations , an area with its normal vector pointing in the $z$ direction can also experience forces along $x$ and $y$ , which are called shear stresses . in this case , the correct linear relation is that $$ \begin{pmatrix}f_x\\ f_y \\ f_z \end{pmatrix} = \begin{pmatrix}p_x and s_{xy} and s_{xz} \\ s_{yx} and p_y and s_{yz} \\ s_{zx} and s_{zy} and p_z\end{pmatrix} \begin{pmatrix}a_x\\ a_y \\ a_z \end{pmatrix} . $$ in a fluid , shear stresses are zero and the pressure is isotropic , so all the $p_j$s are equal , and therefore the pressure tensor $p$ is a scalar matrix . in a solid , on the other hand , shear stresses can occur even in static situations , so you need the full matrix . in this case , the matrix is referred to as the stress tensor of the solid .
in addition to fresnel equations , and in response to your question regarding the " . . . relation between the amplitude of the transmitted/reflected rays and the original ray": $$t_{\parallel}=\frac{2n_{1}\cos\theta_{i}}{n_{2}\cos\theta_{i}+n_{1}\cos\theta_{t}}a_{\parallel}$$ $$t_{\perp}=\frac{2n_{1}\cos\theta_{i}}{n_{1}\cos\theta_{i}+n_{2}\cos\theta_{t}}a_{\perp}$$ $$r_{\parallel}=\frac{n_{2}\cos\theta_{i}-n_{1}\cos\theta_{t}}{n_{2}\cos\theta_{i}+n_{1}\cos\theta_{t}}a_{\parallel}$$ $$r_{\perp}=\frac{n_{1}\cos\theta_{i}-n_{2}\cos\theta_{t}}{n_{1}\cos\theta_{i}+n_{2}\cos\theta_{t}}a_{\perp}$$ where $a_{\parallel}$ and $a_{\perp}$ is the parallel and perpendicular component of the amplitude of the electric field for the incident wave , respectively . accordingly for the $t$ ( transmitted wave ) and $r$ ( reflected wave ) . i think the notation is straightforward to understand . this set of equations are also called fresnel equations ( there are three or four representations ) .
for a planet which has a temperature gradient , hot in the center and cooler on the surface , why do we see absorption lines ? the hot center sends photons within the black body spectrum with appropriate energies to excite surface cold atoms , so the black body curve will have holes , where energy of the photons has been absorbed in exciting surface molecules . similarly , why do we see emission lines if the planet is hot on the surface and gets cooler as you move to the center ? the black body spectrum is a continuous spectrum from thermal excitations . there exists though a probability that from the high energy tail of the black body energy spectrum , electrons from atoms on the surface are taken to a higher energy level and then relax back to the ground state emitting the specific line of that atom .
your calculations are correct , provided the cylinder is indeed ohmic . the constant $e$ you are getting is the difference in electric field between both terminals . as for the current flowing from inside to outside , as you said the cross sectional area will be different , and so will the length . the length $l=r_b-r_a$ , but the cross sectional area is not uniform , because at the beginning of the wire ( the interior ) , $a=2\pi r_al$ , and at the " end " of the wire ( the exterior ) , $a=2\pi r_bl$ . so you will have to treat each portion of the wire as its own infinitesimal resistor $dr$ , and the total resistance is the series combination of them : $$dr=\rho\frac{dl}{2\pi ll}$$ $$r=\int_{r_a}^{r_b}\rho\frac{1}{2\pi ll}dl$$
mass is not always first . for example we write newton 's law for the force between two objects as : $$ f = \frac{gm_1m_2}{r^2} $$ i do not think there are hard and fast rules . i suspect conventions have arisen over the years and we have all got used to what we learned at school , which was taught by teachers who are used to what they learned at school and so on . we tend to put constants first , as in the case above where newton 's constant $g$ is first , and in many cases the mass is a constant . for example when we write : $$ f = ma $$ in the vast majority of cases $m$ is constant and that is probably why we put it first .
just in view of the double universal covering provided by $su ( 2 ) $ , $so ( 3 ) $ must a quotient of $su ( 2 ) $ with respect to a central discrete normal subgroup with two elements . this is consequence of a general property of universal covering lie groups : if $\pi : \tilde{g} \to g$ is the universal covering lie-group homomorphism , the kernel $h$ of $\pi$ is a discrete normal central subgroup of the universal covering $\tilde{g}$ of $g= \tilde{g}/h$ , and $h$ is isomorphic to the fundamental group of $g$ , i.e. $\pi_1 ( g ) $ ( wich , for lie groups , is abelian ) . one element of that subgroup must be $i$ ( since a group includes the neutral element ) . the other , $j$ , must verify $jj=i$ and thus $j=j^{-1}= j^\dagger$ . by direct inspection one sees that in $su ( 2 ) $ it is only possible for $j= -i$ . so $so ( 3 ) = su ( 2 ) /\{i , -i\}$ . notice that $\{i , -i\} = \{e^{i4\pi \vec{n}\cdot \vec{\sigma}/2 } , e^{i2\pi \vec{n}\cdot \vec{\sigma}/2 }\}$ stays in the center of $su ( 2 ) $ , namely the elements of this subgroup commute with all of the elements of $su ( 2 ) $ . moreover $\{i , -i\}=: \mathbb z_2$ is just the first homotopy group of $so ( 3 ) $ as it must be in view of the general statement i quoted above . a unitary representations of $so ( 3 ) $ is also a representation of $su ( 2 ) $ through the projection lie group homomorphism $\pi : su ( 2 ) \to su ( 2 ) /\{i , -i\} = so ( 3 ) $ . so , studying unitary reps of $su ( 2 ) $ covers the whole class of unitary reps of $so ( 3 ) $ . let us study those reps . consider a unitary representation $u$ of $su ( 2 ) $ in the hilbert space $h$ . the central subgroup $\{i , -i\}$ must be represented by $u ( i ) = i_h$ and $u ( -i ) = j_h$ , but $j_hj_h= i_h$ so , as before , $j_h= j_h^{-1}= j_h^\dagger$ . as $j_h$ is unitary and self-adjoint simultaneously , its spectrum has to be included in $\mathbb r \cap \{\lambda \in \mathbb c \:|\: |\lambda|=1\}$ . so ( a ) it is made of $\pm 1$ at most and ( b ) the spectrum is a pure point spectrum and so only proper eigenspeces arise in its spectral decomposition . if $-1$ is not present in the spectrum , the only eigenvalue is $1$ and thus $u ( -i ) = i_h$ . if only the eigenvalue $-1$ is present , instead , $u ( -i ) = -i_h$ . if the representation is irreducible $\pm 1$ cannot be simultaneously eigenvalues . otherwise $h$ would be split into the orthogonal direct sum of eigenspaces $h_{+1}\oplus h_{-1}$ . as $u ( -1 ) =j_h$ commutes with all $u ( g ) $ ( because $-i$ is in the center of $su ( 2 ) $ and $u$ is a representation ) , $h_{+1}$ and $h_{-1}$ would be invariant subspaces for all the representation and it is forbidden as $u$ is irreducible . we conclude that , if $u$ is an irreducible unitary representation of $su ( 2 ) $ , the discrete normal subgroup $\{i , -i\}$ can only be represented by either $\{i_h\}$ or $\{i_h , -i_h\}$ . moreover : since $so ( 3 ) = su ( 2 ) /\{i , -i\}$ , in the former case $u$ is also a representation of $so ( 3 ) $ . it means that $i = e^{i 4\pi \vec{n}\cdot \vec{\sigma} }$ and $e^{i 2\pi \vec{n}\cdot \vec{\sigma}/2 } = -i$ are both transformed into $i_h$ by $u$ . in the latter case , instead , $u$ is not a true representation of $so ( 3 ) $ , just in view of a sign appearing after $2\pi$ , because $e^{i 2\pi \vec{n}\cdot \vec{\sigma}/2 } = -i$ is transformed into $-i_h$ and only $i = e^{i 4\pi \vec{n}\cdot \vec{\sigma}/2 }$ is transformed into $i$ by $u$ .
if i correctly understood your misunderstanding , the answer is : operator is not always a matrix . technically , action of time inversion operator contains complex conjugation . e.g. , in spin up/spin down basis it is written as $-i\sigma_y\mathcal{k}$ , where $\mathcal{k}$ is complex conjugation .
for this sort of task , it is easier to check through wikipedia 's list of baryons and list of mesons . each article has a table listing the properties , including mass , of the known particles of the appropriate type , so you can just scan down the table and find the particle that matches your mass . in addition to mesons and baryons , in general , you would need to check the three charged leptons ( electron , muon , tau ) , the six quarks , the w and z bosons , and the higgs boson . plus neutrinos , although we do not really know their masses . those are all the massive particles that are not qcd bound states .
$\hat r , \hat \theta , \hat z$ form an orthogonal , right-handed triad of basis vectors in 3d . taking the cross-product with them is no more exotic or unusual than doing so for cartesian basis vectors . $\hat r \times \hat \theta = \hat z$ , and so on .
the light you see as the image of the sun on the sky is basically undeflected . http://en.wikipedia.org/wiki/diffuse_sky_radiation says it is 75 % when the sun is high and the sky is clear . the frequency dependency is due to rayleigh scattering . for the cloudy sky the fraction is much smaller , up to many orders smaller than unity ( maybe 1 millionth part as a wild guess ) .
for the field : $$ \int d^3x\phi ( \vec{x} ) e^{-i\vec{p}'\cdot\vec{x}}=\int\int d^3x\frac{d^3p}{ ( 2\pi ) ^3}\frac{1}{\sqrt{2\omega_{p}}}\left ( a_{\vec{p}}+a^{\dagger}_{-\vec{p}} \right ) e^{i\vec{p}\cdot\vec{x}}e^{-i\vec{p}'\cdot\vec{x}}=\\=\int d^3x e^{-i ( \vec{p}'-{p} ) \cdot\vec{x}}\int\frac{d^3p}{ ( 2\pi ) ^3}\frac{1}{\sqrt{2\omega_{p}}}\left ( a_{\vec{p}}+a^{\dagger}_{-\vec{p}} \right ) =\\=\int \frac{d^3p}{ ( 2\pi ) ^3}\frac{1}{\sqrt{2\omega_{p}}}\left ( a_{\vec{p}}+a^{\dagger}_{-\vec{p}} \right ) ( 2\pi ) ^3\delta ( \vec{p}'-\vec{p} ) =\frac{1}{\sqrt{2\omega_{p}'}}\left ( a_{\vec{p}'}+a^{\dagger}_{-\vec{p}'} \right ) \equiv\tilde{\phi} ( \vec{p}' ) $$ for the momentum : $$ \int d^3x\pi ( \vec{x} ) e^{-i\vec{p}'\cdot\vec{x}}=\int d^3x\int\frac{d^3p}{ ( 2\pi ) ^3} ( -i ) \sqrt{\frac{\omega_{p}}{2}}\left ( a_{\vec{p}}-a^{\dagger}_{-\vec{p}} \right ) e^{i\vec{p}\cdot\vec{x}}e^{-i\vec{p}'\cdot\vec{x}}=\\=\int d^3x e^{-i ( \vec{p}'-{p} ) \cdot\vec{x}}\int\frac{d^3p}{ ( 2\pi ) ^3} ( -i ) \sqrt{\frac{\omega_{p}}{2}}\left ( a_{\vec{p}}-a^{\dagger}_{-\vec{p}} \right ) =\\=\int \frac{d^3p}{ ( 2\pi ) ^3} ( -i ) \sqrt{\frac{\omega_{p}}{2}}\left ( a_{\vec{p}}-a^{\dagger}_{-\vec{p}} \right ) ( 2\pi ) ^3\delta ( \vec{p}'-\vec{p} ) = ( -i ) \sqrt{\frac{\omega_{p'}}{2}}\left ( a_{\vec{p}'}-a^{\dagger}_{-\vec{p}'} \right ) \equiv\tilde{\pi} ( \vec{p}' ) $$ so $$\boxed{\frac{1}{2}\left ( \sqrt{2w_{p'}}\tilde{\phi} ( \vec{p}' ) +i\sqrt{\frac{2}{w_{p'}}}\tilde{\pi} ( \vec{p}' ) \right ) =a_{\vec{p}'}}$$ $$\boxed{\frac{1}{2}\left ( \sqrt{2w_{p'}}\tilde{\phi} ( \vec{p}' ) -i\sqrt{\frac{2}{w_{p'}}}\tilde{\pi} ( \vec{p}' ) \right ) =a^{\dagger}_{-\vec{p}'}}$$ now we can use the fact that the ccr for the momentum and the field is almost the same as the ccr for the fourier transforms : $$ [ \phi ( \vec{x} ) , \pi ( \vec{y} ) ] -i \delta^{ ( 3 ) } ( \vec{x}-\vec{y} ) =0$$ multiplying by $e^{-i ( \vec{p}\cdot\vec{x} ) }$ and $e^{-i ( \vec{p}'\cdot\vec{y} ) }$ integrating with respect to $\vec{x}$ and $\vec{y}$ $$ \int d^3xd^3y\left ( e^{-i ( \vec{p}\cdot\vec{x} ) }\phi ( \vec{x} ) e^{-i ( \vec{p}'\cdot\vec{y} ) }\pi ( \vec{y} ) -\pi ( \vec{y} ) e^{-i ( -\vec{p}'\cdot\vec{y} ) }\phi ( \vec{x} ) e^{-i ( \vec{p}\cdot\vec{x} ) }-i\delta^{ ( 3 ) } ( \vec{x}-\vec{y} ) \right ) =0$$ we get the commutator in terms of the fields and momentum in the momentum basis : $$ [ \tilde{\phi} ( \vec{p} ) , \tilde{\pi} ( \vec{p}' ) ] = ( 2\pi ) ^3i\delta^{ ( 3 ) } ( \vec{p}-\vec{p}' ) $$ so that $$\left [ a_{\vec{p}} , a^{\dagger}_{-\vec{p}'}\right ] =\left [ \frac{1}{2}\left ( \sqrt{2w_{p}}\tilde{\phi} ( \vec{p} ) +i\sqrt{\frac{2}{w_{p}}}\tilde{\pi} ( \vec{p} ) \right ) , \frac{1}{2}\left ( \sqrt{2w_{p'}}\tilde{\phi} ( -\vec{p}' ) -i\sqrt{\frac{2}{w_{p'}}}\tilde{\pi} ( -\vec{p}' ) \right ) \right ] =\frac{1}{2}\left ( -i [ \tilde{\phi} ( \vec{p} ) , \tilde{\pi} ( \vec{p}' ) ] +i [ \tilde{\pi} ( -\vec{p} ) , \tilde{\phi} ( -\vec{p}' ) ] \right ) = ( 2\pi ) ^3\delta ( {\vec{p}+\vec{p}'} ) $$ final remarks 1 ) i think there are some $\hbar$ missing and 2 ) i am not sure about the symmetry between $\vec{p}$ and $-\vec{p}$ for the real kg field edit i set $\hbar=1$ , but the commutator still looks a little ugly with that $+$ sign edit given that the field is real may i say $\tilde{\phi} ( \vec{p} ) =\tilde{\phi}^* ( -\vec{p} ) =\tilde{\phi} ( -\vec{p} ) $
the problem is that your coordinates are not well defined at $\theta=0$ and $\phi=\pi/2$ . note in particular that $$ u|_{ ( 0 , \frac{\pi}{2} , \gamma ) } = \begin{pmatrix}1 and 0\\0 and 1\end{pmatrix} $$ for any value of $\gamma$ . a simpler choice is $$ \tilde{u} = \begin{pmatrix} x+iy and z+iw \\ -z+iw and x-iy \end{pmatrix} , $$ with $$ x = \sqrt{1 - y^2 - z^2 - w^2} . $$ differentiating this you find $$ d\tilde u = i\begin{pmatrix} dy and +i\ , dz + dw \\ -i\ , dz + dw and -dy \end{pmatrix} - \frac{y\ , dy+z\ , dz+w\ , dw}{\sqrt{1-y^2-z^2-w^2}}\begin{pmatrix}1 and 0\\0 and 1\end{pmatrix} $$ from which you can read off the pauli matrices at the point $ ( x , y , z , w ) = ( 1,0,0,0 ) $ .
internal forces are the only contributors to potential energy . potential energy is the energy associated with the configuration ( relative positions ) of a collection objects . the potential energy of a single point particle is not defined . as the configuration of the system changes , its potential energy changes according to the definition $\delta{\mathrm{ ( p . e . ) }} = -w_\mathrm{internal}$ where $w_\mathrm{internal}$ is the work done internally , that is , by the various internal forces against the internal components of the system . in order to define potential energy , you need to have internal components applying forces to one another , and the components need to be able to move relative to one another . you need at least two objects . consider the system consisting of only the dumbbell . you have implicitly modeled the dumbbell as a point particle . that is , the only attributes of interest to your analysis are its position and its mass . the internal structure , which would be of interest in other questions , say , concerning temperature , is ignored . you have applied two external forces , gravity , and the contact force of your hand . not only is there no change in potential energy , there is no potential energy at all . potential energy does not exist in a system consisting of , or modeled by , a single point particle . i am not saying that the p.e. is zero . i am saying that it is not defined . consider the system consisting of the dumbbell and the earth . now i have two objects modeled as point particles , one external force : the force of your hand , and one internal force : the mutual gravitational attraction between the earth and the dumbbell . the internal work done in lifting the dumbbell ( that is , increasing the distance between them ) is $w_\mathrm{internal} = -mgh$ . the minus sign comes from the fact that the displacement is in the direction opposite to the direction of the force . ( gravity down , displacement up . ) so the change in potential energy of the system is $$\delta\mathrm{ ( p . e . ) } = -w_\mathrm{internal} = - ( -mgh ) = mgh$$ the external force of your hand on the dumbbell plays no role at all . one of the roots of your misunderstanding is the unfortunate choice made by almost all introductory textbooks in introducing the subject of potential energy by looking at raising and lowering objects in a static gravitational field ( at the surface of the earth ) . i know of a particular textbook that had a wonderful and correct description of potential energy . until the next edition came out , when that description disappeared , and the conventional description appeared in its place .
you have said : if , for instance , the relative motion observed between two frames of reference is that of uniform acceleration , how can we determine which frame is the unaccelerated system ? it is obviously not possible . and another part of this very question is also : how can we call the occupied frame of reference as being inertial regardless of whether other frames of reference are accelerating with respect to the occupied frame of reference ? both these questions have been answered below . why would it not be possible ? if you are in a reference frame which is accelerating at all , then you will experience pseudo-forces ( forces whose source is not determined in that frame ) . that will tell you that your frame is accelerating . moreover , if the relative motion between two frames is that of uniform acceleration , then both are accelerating ! you do not have to determine which is accelerating ! the presence of acceleration ( uniform or not ) for any reference frame , guarantees that you will experience pseudo-force if you are in it . for example , if you throw a ball from a height , it seems to hit the ground after travelling a path perpendicular to ground . but the actual trajectory is not so . as the ball falls it is deflected due to coriolis force , which is a pseudo-force . so technically the earth is not an inertial frame of reference in any way since we can never point to a source who caused this coriolis force ! you have said : resnick states that the frame of reference he occupies is an unaccelerated one . with respect to what ? if accelerated motion were to be observed with respect to other frames of reference , how are we to determine that we occupy an inertial frame of reference at all ? according to resnick he occupies an inertial frame that means , in his frame , newton 's first law holds true . obviously you need a reference object . when we say a car travels at 75m/s then we actualy mean it travels 75m/s with respect to , say , a stationary tree . but it would travel at 50m/s with respect to another car travelling with 25m/s . so you need a reference object .
i think that the most prominent example of " prediction before observation " in statistical physics is the bose-einstein condensate . it was predicted in ~1925 by , well , bose and einstein , obviously . then after more than ten years it was proposed as an explanation for superfluidity and superconductivity . and the actual bec of atoms ( as a new state of matter ) was obtained only in 1995 .
if you travel with constant speed $v$ for a time $t$ , you will travel for $v\times t$ distance . example : if your speed were $2$ m/s , and you were walking for $3$ seconds , you had walk $2\times3 = 6$ meters . now , when calculating distance traveled while accelerating ( or decelerating ) , we can approximate it if we split total time of travel into sub-intervals , and calculate sum of $v_i\times t_i$ , where $v_i$ is speed at the beginning of $i$th sub-interval , and $t_i$ is its duration . the smaller intervals we take , the better is our approximation . and humans invented a way to calculate such sums using infinitely small sub-intervals - definite integrals . imagine we have some function $f ( x ) $ and interval $ [ a , b ] $ . how we can calculate area of region between graph of $f ( x ) $ and $x$-axis on this interval ? we can split interval $ [ a , b ] $ in sub-intervals , and approximate the area with sum of areas of rectangles like shown on this image in wikipedia . sounds familiar ? if we have a formula $v ( t ) $ ( speed from time ) , then we can calculate distance traveled during interval $ [ a , b ] $ as area of the region bound by graph of $v$ , $t$-axis , and two vertical lines at the ends of this interval . if starting speed is $v_0$ and acceleration $a$ is constant , then speed in given moment of time $t$ is $v ( t ) = v_0 + a\times t$ . if we start at time $0$ , then at time $t$ distance traveled is $$\int_0^t ( v_0 + a\times t ) dt = \left . ( v_0\times t + a\times t^2/2 ) \right|_0^t = v_0\times t + a\times t^2/2$$
it comes from the normalization of the polyakov action , \begin{align*} s=\frac{t}{2}\int d^2\sigma\ , ( \dot{x}^\mu\dot{x}_\mu-{x'}^\mu{x'}_\mu ) . \end{align*} the canonical momentum is \begin{align*} \frac{\partial \mathcal{l}}{\partial \dot{x}^\mu}=t\dot{x}_\mu , \end{align*} and this gives the equal time commutator ( or poisson bracket ) that you wrote down , \begin{align*} [ x^\mu ( \tau , \sigma ) , \dot{x}^\nu ( \tau , \sigma' ) ] =\frac{i}{t}\eta^{\mu\nu}\delta ( \sigma-\sigma' ) . \end{align*} the way i think about this is that as the tension of the string goes to zero , the string becomes less and less classical ( alternatively , $t$ plays the role of $1/\hbar$ on the worldsheet ) .
the density of water is , nominally , 1g/ml , but varies depending on several factors impurities ( dissolved ions , salts , other solutes will change the density ) salts and ions are typically higher mass atoms/molecules than the 18atm h$_2$o molecule , as such any dissolved ions increase the mass of the liquid in the same volume and therefore increase the density , ocean salt water can be 3-5% more dense than freshwater at the same temperature , for instance . salt water also depresses the freezing point and this affects the temperature dependence of the density . temperature water density varies greatly with temperature , fresh water near boiling can be as low as 950 g/l ( . 95g/ml or 950kg/m$^3$ ) increasing as temperature decreases to a maximum at 4$^\circ$c ( 1g/ml ) before decreasing slightly to freezing . for water room temperature and below the variation is less than $ . 5$% . this is by far the largest contribution to variation in density for fresh water at normal earth conditions . external pressure liquids are relatively poorly compressible , so at atmospheric levels the atmospheric pressure will contribute negligibly to the density of water , however freezing temperatures get depressed with changes in atmospheric pressure and can affect the density vs temperature . at higher pressures the effects are more pronounced for all of these conditions , there are really no equations to determine the density , and those that exist are typically quantitatively based on experimental measurement and apply to a certain set of conditions . the answer is that you have to look it up based on your conditions . ( most tables for density vs temperature online are given for fresh water at 1 atm ) depending on the accuracy you desire you can analyze all these properties without measuring its mass to determine the density with greater precision . however for most daily uses , an estimate of 1g/ml is close enough
it sounds as if you are describing flow through an orifice plate . in that case see this wikipedia article for how to calculate the flow rate or google for something like " gas flow orifice " . there are various web sites with flow rate calculators e.g. this one . we are not supposed to just give links as an answer , but the formulae for calculating the flow are quite complicated and i am not sure what would be achieved by copying and pasting them here .
1 ) notice that by inserting a complete set of position states we can write $$ \hat p \psi ( x ) = \langle x|\hat p|\psi\rangle = \int dx'\langle x|\hat p|x'\rangle\langle x'|\psi\rangle =\int dx'\langle x|\hat p|x'\rangle \psi ( x' ) $$ so if we set $$ \langle x|\hat p|x'\rangle = -i\hbar \frac{\partial}{\partial x}\delta ( x-x' ) =i\hbar \frac{\partial}{\partial x'}\delta ( x-x' ) $$ then we can use integration by parts to obtain $$ \hat p \psi ( x ) =i\hbar \int dx'\frac{\partial}{\partial x'}\delta ( x-x' ) \psi ( x' ) = -i\hbar \int dx'\delta ( x-x' ) \frac{d \psi}{dx'} ( x' ) = -i\hbar \frac{d\psi}{dx} ( x ) $$ so your expression is correct . the derivative of a delta function is essentially defined by the integration by parts manipulation that i just performed ; in fact derivatives of distributions in general are defined in an analogous way . see this lecture for example . hope that helps ; let me know of any typos ! cheers !
volume increase in the system is due to work done by the system . therefore w is negative using your notation . think of it this way , work done on the system would push the system inwards , decreasing volume . therefore a volume increase is work done by the system . alternatively you could reason using the formula : $du = q - dw$ ( using your notation conventions , were $u$ is internal energy , $w$ is work and $q$ is heat added to the system ) $dw = pdv . $ therefore $du = q - pdv$ . therefore if $dv$ ( change in volume ) is positive , $du$ ( change in internal energy ) is negative .
how come these ionized atoms are not displacing electrons to create more holes ( in the ionized p-side ) or accepting more electrons ( in the positively ionized region on the n-side ) ? it is only possible for the donor and acceptor atoms to de-ionise in the depletion region if they capture a free carrier ( electron and hole , respectively ) . but there are no free carriers in the depletion region because they have all be swept out by the strong electric field ( something like 30$-$40kv/cm$^{\textrm{-1}}$ ! ) . so why then do the electron from the n-side stay with the acceptor atoms on the p-side once the junction has formed ? the short answer is because the carrier trapped by the dopant atoms would have to gain almost a bandgaps worth of energy to de-ionize . the longer answer . let 's assume an acceptor atom in the p-side depletion region de-ionises by giving up it is captured electron . what happens ? the electron is pushed back to the n-side by the field . however , the system is now no longer in equilibrium because the p-side is charged to +1 and the n-side is charged to -1 . this is not stable ! you can see that if you run this forward in time , eventually an electron from the n-side will have to neutralise the acceptor , bringing the material back to charge neutrality . when you solve the poisson equation for the pn-junction this is what you are solving for : the equilibrium distribution for charge neutrality . there are probably carrier dynamics like de-ionisation happening but they only serve to push the system out of equilibrium temporarily , eventually equilibrium will always be restored .
just use the jacobian of the coordinate system transformation . if your cartesian coordinates are $\mu$ and $\nu$ and your cylindrical coordinates are $\mu ' , \nu'$ , then there is a jacobian ${f_\mu}^{\mu'}$ that allows you to write $$f^{\mu ' \nu'} = f^{\mu \nu} {f_\mu}^{\mu'} {f_\nu}^{\nu'}$$ where the jacobian is given by $${f_\mu}^{\mu'} = \frac{\partial x^{\mu'}}{\partial x^\mu}$$ now that is all well and good , but you might be thinking it is a bit abstract , and . . . it is . there is another way to do this instead , using what is called geometric algebra . in geometric algebra , the em tensor is called a bivector , taking on the form $$f = f_{tx} e^t \wedge e^x + f_{ty} e^t \wedge e^y + \ldots = \frac{1}{2} f_{\mu \nu} e^\mu \wedge e^\nu$$ where $e^\mu$ represent basis covectors . what we have used here is called a wedge product , and orthogonal basis vectors will anticommute under it . to extract the components in a new basis , you have a couple choices : ( 1 ) you can write the basis covectors in terms of the cylindrical basis and simplify . so that would entail writing $e^x$ and $e^y$ in terms of $e^\rho$ and $e^\phi$ . this is equivalent to finding the inverse jacobian . however , there is another choice ( 2 ) , which is to simply take the inner product of the basis vectors $e_\rho \wedge e_t , e_\phi \wedge e_t$ and so on with $f$ . this requires a little more knowledge of geometric algebra , but you can write $e_\rho \wedge e_t$ in terms of $e_x \wedge e_t , e_y \wedge e_t$ , and so on , which may be an easier computation . i will do the latter here to demonstrate the technique . see that $e^\rho = e^x \cos \phi + e^y \sin \phi$ . we can then find $f^{t \rho}$ as : $$\begin{align*}f^{t\rho} and = f \cdot ( e^\rho \wedge e^t ) \\ and = f \cdot ( e^x \wedge e^t \cos \phi + e^y \wedge e^t \sin \phi ) \\ and = f^{tx} \cos \phi + f^{ty} \sin \phi \end{align*}$$ this is no more exotic that finding the components of a vector in a new basis by finding the projection of the vector on each new basis vector .
the intensity of the light from the sun at the orbit of the earth is around 1.4 kilowatts per square metre . for comparison , a domestic heater is usually around 3 kw , so a satellite with a 2m surface area ( admittedly this is bigger than most satellites ) facing the sun needs to dissipate as much energy as used to heat your living room . this is in addition to the waste heat produced by the electronics on the satellite . for most satellites it is keeping cool that is the problem , and that is why they are wrapped in reflective foil . where you have a satellite that needs to stay really cool , such as the herschel space observatory , the satellite has to carry a supply of liquid helium to cool itself . in fact the herschel space observatory ran out of liquid helium at the end of april last year and can no longer operate . it is certainly true that if you can stay out of sunlight space is a pretty cold place . if you could avoid all reflected light then in principle you could cool to the 2.7k temperature of the microwave background , and this is cold enough to use superconductors . however this is not a practical way to operate most satellites .
interference of sound waves might work for you . one experiment that comes to mind is to have two speakers playing identical periodic sounds . ( i do not think they have to be pure sinusoidals , but " simple " so it sounds like a note . ) depending on where you stand relative to the speakers , you will hear " dull " areas where the sound is not loud , demonstrating deconstructive interference . depending on the patience of your group , you could even have them walk around and map out the destructive areas . ( i actually think mapping it out would be quite hard ; test it before doing it . ) the number of destructive areas and their areas can be adjusted by tuning the frequencies of your source and/or the speaker separation . there is a phet simulation here .
i have seen bubbles made with hydrogen . this is a popular trick with the various lecturers who do fireworks related lectures because the bubbles make a satisfying pop if you ignite them . a bubble is mainly stabilised by layers of surfactant adsorbed at the gas/water interface . as the bubble wall thins , the adsorbed surfactant layers at the opposite gas/water surfaces come into contact and prevent further thinning . this is a purely kinetic barrier as the gas/water surface tension is still greater than zero ( i.e. . you had reduce the overall energy by reducing the surface area ) but the rate of desorption of surfactant from the surface is slow . in principle the gas will affect the adsoption of the surfactant at the gas/water interface and possibly affect the stability , but in practice all common gases are so different from water that the relatively minor differences between gases makes little difference . bubbles can even be blown with steam as long as you keep the gas phase temperature above 100c . however , over medium timespans the gas inside the bubble will diffuse out through the water film and cause the bubble to shrink . the rate at which this happens will depend on the solubility of the gas and its diffusion rate in water . i am sure there will be differences between hydrogen and air , though i do not know of anyone who has actually measured it . i found papers reporting diffusion rates here and here , though both are behind paywalls . i had better luck with solubility figures . both hydrogen and helium are about a factor of ten less soluble in water than nitrogen , which would make their bubbles more stable than air bubbles though their greater diffusion rates will counteract this to some extent .
yes . it turns out that your $t_l$ is equal to $-t/\omega$ , where $\omega$ is the angular velocity and $t$ is the usual temperature . we normally work with the reciprocals of such quantities , and in the language of non-equilibrium thermodynamics we say that a gradient in $-\omega/t$ is the " thermodynamic force conjugate to " a flow of angular momentum . within the formalism of thermodynamics itself there is indeed nothing special about energy . ( there is , however , quite a lot that is special about energy when it comes to mechanics . ) however , the usual terminology and notation obscures this quite a bit . we usually write the fundamental equation of thermodynamics with the energy on the left-hand-side , like this : $$ du = tds - pdv + \sum_i \mu_i dn_i . $$ this equation can be extended with many other terms , including $\phi dq$ ( electric potential times change in charge ) and $\omega dl$ ( angular velocity times change in angular momentum ) . however , the " special " quantity here is the entropy , $s$ , which is non-decreasing while all the other extensive quantities are conserved . we can rearrange this to put the special quantity on the left , and to get $$ ds = \frac{1}{t} du + \frac{p}{t}dv - \sum_i \frac{\mu_i}{t}dn_i + \dots - \frac{\phi}{t}dq - \frac{\omega}{t} dl . $$ this observation is the basis of non-equilibrium thermodynamics . it follows immediately from this that $$ \frac{\partial s}{\partial l} = -\frac{\omega}{t} . $$ it also follows that angular momentum cannot be spontaneously transferred from one body to another while keeping all other quantities constant unless the second body has greater $-{\omega}/{t}$ . however , that " while keeping other quantities constant " is a bit tricky . in just about any reasonable situation , adding angular momentum to a system will also change its energy . the same is true of changes in volume , chemical composition or charge : changing these things will , generally speaking , also change the energy . this is probably the main historical reason why energy is seen as special in thermodynamics : it is the only thing you can practially change while keeping everything else constant . ( we call this " heating up " or " cooling down " a system . ) so while it is quite possible to define angular-momentum analogues of heat , free energy and the carnot limit , these do not tend to have the same immediate practical applications as the energy-based versions . nevertheless , i think the existence of such quantities is an enlightening and often-overlooked observation . i would encourage you to keep on thinking along these lines , since understanding the symmetry between energy and the other conserved quantities leads to a deeper understanding of thermodynamics as a whole .
if you go down further into the article , you will find this : uplift , called such as it tends to lift the dam upward , a condition which although many designers and builders of dams had become aware of by the late 1890s to early 1900s , was still not generally well understood or appreciated . in essence , the dam is lifted by the water seeping under it from the reservoir .
it can be argued that there are two primary challenges associated with quantum computing . one is a coding challenge , and the other is a purely mechanical ( or in some minds physical ) challenge . as ron indicated , viable quantum error correction code ( qecc ) is probably the largest breakthrough that makes quantum computers possible . peter shor was the first to demonstrate a viable quantum error correction code that can also be characterized as a decoherence free subspace . at this point , it can be argued that most of the problems associated with fault tolerant quantum computing are resolvable in terms of theory . however , their is a question of scaling as it relates to the number of qubits that are required to make the system viable , and the physical size of the system that is required to support those qubits . most arguments that say there is a needed breakthrough in physics can be traced to this question of scalability . in many cases , the difficulties are related to decoherence times and the speed at which the computers can perform calculations , which would allow the qecc time to operate . at this point those are now considered largely engineering challanges . as far as physical breakthroughs , it is likely that those conversations are in regards to topological quantum computers , which rely upon the construction of anyons in order to operate . currently these are largely viewed as pure mathematical constructions , however , the construction and use of stable anyons , even as quasiparticles , as part of topological quantum computer would be a major physical breakthrough .
i find it is not a problem if one simply omits the $o ( \epsilon ) $ terms since we are taking the $\epsilon \rightarrow 0$ limit . the author just did not state it clearly .
the set $g$ gives the representation of the identity and generators of the abstract group of quaternions as elements in $sl ( 2 , \mathbb c ) $ which are also in $su ( 2 ) $ . taking the completion of this yields the representation $q_8$ of the quaternions presented in the question . from the description of the symmetry group as coming from here , consider the composition of two $\pi$ rotations along the $\hat x$ , $\hat y$ , or $\hat z$ axis . this operation is not the identity operation on spins ( that requires a $4\pi$ rotation ) . however , all elements of $d_2$ given above are of order 2 . this indicates that the symmetry group of the system should be isomorphic to the quaternions and $q_8$ is the appropriate representation acting on spin states . the notation arising there for $d_2$ is probably from the dicyclic group of order $4\times 2=8$ which is isomorphic to the quaternions .
i assume it is referring to specific orbital energy $$ \epsilon=\frac{v^2}{2}-\frac{\mu}{r} $$ where $v$ , $\mu$ and $r$ are the velocity , gravitational parameter and distance to the sun . in that case it would be the other way around . so if negative then it comes from the solar system , if positive it is extrasolar . but our solar system does not only consist of the sun , other celestial bodies are also part of it . these bodies , especially the gas giants , can also have a big influence on the trajectory of other bodies when close enough ( see gravity slingshots ) . so this means that the specific orbital energy does not guarantee that something came from inside or outside the solar system . good examples are the two voyager probes . but you mention spaceships , which would suggest that there is some control over it and is not just surrendered to the external forces ( mainly gravity ) . edit the path of an object in a gravitational well is described by $$ r ( \theta ) =\frac{a ( 1-e^2 ) }{1+e\cos{\theta}} , $$ where $\theta$ is the true anomaly , $a$ is the semi-major axis , which can be expressed as a function of $\epsilon$ $$ a=\frac{-\mu}{2\epsilon} $$ and $e$ is the eccentricity , which can also be described as a function of $\epsilon$ $$ e=\sqrt{1+\frac{2\epsilon h^2}{\mu^2}} $$ where $h$ is the specific angular momentum . an object will be able to escape the solar system if its trajectory extend infinitely far away ( $r=\infty$ ) . and this will be the case when $1+e\cos{\theta}=0$ . since $\theta$ is a variable and the rest a constants , from this follows that $e\geq1$ , so $e^2=1+\frac{2\epsilon h^2}{\mu^2}\geq1$ . and therefore $\frac{\epsilon h^2}{\mu^2}\geq0$ , but because $h^2$ and $\mu^2$ are always positive this will only be true when $\epsilon\geq0$ ( so when positive ) .
gold foil is quite easy to hold you just hang it from a paperclip . the only difficulty is if there is a lot of static electricity in the air which makes it stick to things . ( this is the main reason for the cold damp cambridge 's supremacy in early particle physics ) photographic film at the time was not sensitive and so in marsden and geiger 's experiments they used a tiny target coated with phosphorescent crystals which emitted a spark of light when hist by a particle and viewed them through a microscope . this involved sitting in a dark cupboard for hours straining to see and count rare pinpoints of light while staring down a primitive microscope - this is why the job was given to grad students . edit : there is a very good book , the fly in the cathedral which describes this period , the science and the experimental techniques . high vacuum stuff is a pain the ___ even with modern quick connects and helium leak detectors , doing it with string and sealing wax ( literally ) was quite a challenge .
here is a visualization : momentum is mass times velocity , so draw it as the area of a rectangle : if we change the mass and velocity a little , we change the momentum : the total change in the momentum is the sum of green , blue , and purple rectangles . their sizes are just length times width , so overall we have $\delta p = m\delta v + v\delta m + \delta v \delta m$ this looks like the answer you are seeking except for the extra term at the end . suppose we cut $\delta m$ and $\delta v$ down to one tenth their current size . then the first two terms become one tenth as large , but $\delta m \delta v$ becomes one hundredth as large . the purple box shrinks away much faster than the blue and green ones . therefore , for very small changes , we can ignore the purple box and write $\delta p = \delta ( mv ) = m\delta v + v\delta m$ we usually indicate this limiting procedure by changing the $\delta$ to $\mathrm{d}$ , so $\mathrm{d} p = \mathrm{d} ( mv ) = m\mathrm{d} v + v\mathrm{d} m$
@brandon is correct . you can compute the average kinetic for any free particle using the equipartition theorem , which gives $\langle e \rangle = \frac{1}{2}k_b t$ per quadratic degree of freedom , where $t$ is the temperature and $k_b$ is boltzmann 's constant . for free particles in 3d this gives $\langle e \rangle= \frac{3}{2} k_b t$ ; equating $\frac{1}{2} m_e v^2 = \frac{3}{2} k_b t$ ( in a classical approximation ) shows that the root-mean-square velocity $v$ of free electrons is temperature dependent . $^1$ in the above , the velocities are measured with respect to the environment at temperature $t$ -- we do not even need to consider the fact that the electrons you are talking about are presumably on earth , being accelerated around the sun in centripetal motion . even light will change its speed $c=c_0/n$ in an explosion , because the index of refraction $n$ of the surrounding medium will become inhomogeneous and fluctuate : it is only the speed of light $c_0$ in vacuum that is constant . $^1$note , as also pointed out by brandon , electrons often move at relativistic speeds , so $\frac{1}{2}m_e v^2$ is a poor approximation . $v$ in this formula can exceed the speed of light $c$ , for example . to quantitatively calculate the velocity you need the correction from special relativity . this does not qualitatively change the answer though .
here 's what is really going on . in classical field theory , a basic set of objects that we often consider are scalar fields $\phi:m\to \mathbb r$ where $m$ is a manifold . now we can ask ourselves the following question : is there some natural notion of how a scalar field defined on a given manifold " transforms " under a coordinate transformation ? i claim that the answer is yes , and i will attempt to justify my claim both mathematically , and physically . the bottom line is that we ultimately have to define the way in which fields transform under certain types of transformations , but any old definition will not necessarily be useful in math or physics , so we must make well-motivated definitions and then show that they are useful for modeling physical systems . mathematical perspective . ( manifolds and coordinate charts ) recall that a coordinate system ( aka coordinate chart ) on an $n$-dimensional manfiold $m$ is a ( sufficiently smooth ) mapping $\psi:u\to \mathbb r^n$ where $u$ is some open subset of $m$ . we can use such a coordinate system to define a coordinate representation $\phi_\psi$ of the scalar field $\phi$ as \begin{align} \phi_\psi = \phi\circ\psi^{-1}:v\to\mathbb r \end{align} where $v$ is the image of $u$ under $\phi$ . now let two coordinate systems $\psi:u_1\to \mathbb r^n$ and $\psi_2:u_2\to\mathbb r^n$ be given such that $u_1\cap u_2\neq \emptyset$ . the coordinate representation of $\phi$ in these two coordinate systems is $\phi_1 = \phi\circ \psi_1^{-1}$ and $\phi_2 = \phi\circ \psi_2^{-1}$ . now consider a point $x\in u_1\cap u_2$ , then $x$ is mapped to some point $x_1\in \mathbb r^n$ under $\psi_1$ and to some point $x_2\in \mathbb r^n$ under $\psi_2$ . we can therefore write \begin{align} \phi ( x ) and = \phi \circ \psi_1^{-1} \circ \psi_1 ( x ) = \phi_1 ( x_1 ) \\ \phi ( x ) and = \phi \circ \psi_2^{-1} \circ \psi_2 ( x ) = \phi_2 ( x_2 ) \end{align} so that \begin{align} \phi_1 ( x_1 ) = \phi_2 ( x_2 ) \end{align} in other words , the value of the coordinate representation $\phi_1$ evaluated at the coordinate representation $x_1 = \psi_1 ( x ) $ of the point $x$ agrees with the value of the coordinate representation $\phi_2$ evaluated at the coordinate representation $x_2 = \psi_2 ( x ) $ of the same point $x$ . this is one way of understanding what it means for a scalar field to be " invariant " under a change of coordinates . if , in particular , the manifold $m$ we are considering is $\mathbb r^{3,1} = ( \mathbb r^4 , \eta ) $ , namely four-dimensional minkowski space , then we could consider the following two coordinate systems : \begin{align} \psi_1 ( x ) and = x \\ \psi_2 ( x ) and = \lambda x+a \end{align} where $\lambda$ is a lorentz transformation and $a\in \mathbb r^4$ , then the coordinate representations $\phi_1$ and $\phi_2$ of $\phi$ are , as noted above , related by \begin{align} \phi_1 ( x ) = \phi_2 ( \lambda x + a ) \end{align} if we switch notation a bit and write $\phi_1 = \phi$ and $\phi_2 = \phi'$ , then this reads \begin{align} \phi' ( \lambda x +a ) = \phi ( x ) \end{align} which is the standard expression you will see in field theory texts . physical perspective . here 's a lower-dimensional analogy . imagine a temperature field $t:\mathbb r^2 \to \mathbb r$ on the plane that assigns a real number that we interpret as the temperature at each point on some two-dimensional surface . suppose that this temperature field is generated by some apparatus under the surface , and suppose that we translate the apparatus by a vector $\vec a$ . we could now ask ourselves : what will the temperature field produced by the translated apparatus look like ? well , each point in the temperature distribution will be translated by the amount $\vec a$ . so , for example , if the point $\vec x_0$ has temperature $t ( \vec x_0 ) = 113^\circ\ , \mathrm k$ , then after the apparatus is translated , the point $\vec x_0 + \vec a$ will have the same temperature $113^\circ\ , \mathrm k$ as the point $\vec x_0$ before the apparatus was translated . the mathematical way of writing this is that if $t'$ denotes the translated temperature field , then $t'$ is related to $t$ by \begin{align} t' ( \vec x+\vec a ) = t ( \vec x ) \end{align} the a similar argument could be made for a scalar field on minkowski space , but instead of simply translating some temperature apparatus , we could imagine boosting or translating something producing some lorentz scalar field , and we would be motivated to define the transformation law of a scalar field under poincare transformation as \begin{align} \phi' ( \lambda x+a ) = \phi ( x ) \end{align}
they will equalize pressure at the entrance to the tube between them . that pressure is the density of the fluid times the height from the bottom to the lowest point in the vortex , because the fluid at the lowest point has zero velocity and so is equivalent to standing fluid .
i recommend you " symmetries " by griffiths , or " symmetry " by roy mcweeny ( it is a dover book ) . " geometry , topology and physics " by nakahara is a good option .
it is not often that dmckee and i differ ( mainly because he is usually right :- ) but we differ on this on . or at least we differ if i have correctly understood what you are asking . in a hydrogen atom the 1s , 2s , etc wavefunctions are ( subject to various approximations ) good descriptions of the single electron and have well defined angular momentums . in multielectron atoms it is convenient to think of electrons populating successive 1s , 2s , etc levels , but this is only a conceptual model and not an accurate representation . you are quite correct that while there is a well defined angular momentum for the whole atom , you cannot define the angular momentum of individual electrons . in the old days ( maybe it is still done ) we had calculate atomic structure using a hartree-fock method with individual electron wavefunctions as the basis , and as dmckee points out , atoms have spectral lines that can often be approximately thought of as exciting a specific electron between individual electron wavefunctions . however what you are really doing is labelling the whole atom as an $l , m$ state and not an individual electron .
suppose the radius of the sphere is $r$ . if you will permit me , let 's calculate the electrostatic potential $\phi$ inside of the sphere , and then let 's use the definition $$ \mathbf e = -\nabla\phi $$ to determine the electric field . if you want , i can directly do the integral for $\mathbf{e}$ , but it is just a bit messier . in any case , we have $$ \phi ( \mathbf x ) = \frac{1}{4\pi\epsilon_0}\left ( \int_{|\mathbf x'|&lt ; |\mathbf x|}d^3x'\ , \frac{\rho}{|\mathbf x - \mathbf x'|}+\int_{r&gt ; |\mathbf x'|&gt ; |\mathbf x|}d^3x'\ , \frac{\rho}{|\mathbf x - \mathbf x'|} \right ) $$ let 's choose $\mathbf x = r\mathbf e_z$ as you did , for simplicity , then in spherical coordinates we have $$ d^3x ' = dr'd\theta'd\phi'\ , r'^2\sin\theta ' , \qquad |\mathbf x - \mathbf x'|=r^2+r'^2-2rr'\cos\theta ' $$ so we have $$ \phi ( \mathbf x ) = \frac{2\pi\rho}{4\pi\epsilon_0}\int_0^r dr'r'^2\int_0^\pi d\theta'\sin\theta' ( r^2+r'^2-2rr'\cos\theta' ) ^{-1/2}+ \big ( \int_r^r\cdots\big ) $$ where the $2\pi$ came from the integration in $\phi'$ . now , we make the substitution $$ u = r^2+r'^2-2rr'\cos\theta ' , \qquad \frac{du}{2rr'} = \sin\theta ' d\theta ' $$ so the integral becomes $$ \phi ( \mathbf x ) = \frac{\rho}{4\epsilon_0r}\int_0^r dr ' r'\int_{ ( r-r' ) ^2}^{ ( r+r' ) ^2}du\ , u^{-1/2} + \big ( \int_r^r\cdots\big ) $$ performing the integral in $u$ gives \begin{align} \phi ( \mathbf x ) and = \frac{\rho}{2\epsilon_0r}\int_0^r dr ' r' ( \sqrt{ ( r+r' ) ^2}-\sqrt{ ( r-r' ) ^2} ) + \big ( \int_r^r\cdots\big ) \\ and = \frac{\rho}{\epsilon_0r}\int_0^r dr ' r'^2 +\frac{\rho}{\epsilon_0}\int_r^r dr'r'\\ and = \frac{\rho}{\epsilon_0}\left [ \frac{r^2}{3} - \frac{r^2}{2} + \frac{r^2}{2}\right ] \end{align} where we have used the fact that $\sqrt{ ( r^2-r'^2 ) }$ equals $r-r'$ for $r&gt ; r'$ and $r'-r$ for $r&lt ; r'$ . finally , taking the negative gradient of the potential in spherical coordinates gives $$ \mathbf e ( \mathbf x ) = \frac{\rho }{3\epsilon_0}r\ , \mathbf e_r $$ which is correct as you can check via gauss 's law .
what you want is not really possible . the reason is that the angular momentum of a particle may have a spin component , or there may be other particles for which you must also include the angular momentum . more specifically , all you may conclude from symmetry arguments is that in a rotationally invariant theory there exists a pseudovector operator $\hat{\mathbf{j}}$ whose commutation relations with the position and momentum components of any one particle in the system are the ones you posted . it will typically include the orbital angular momentum of the particle , $\hat{\mathbf{l}}$ , as well as other operators such as spin which will commute with all position and momentum operators . this , of course , obviates the fact that if you only do have the orbital degrees of freedom of a single particle , then there is nothing there that will have more angular momentum , and the equality you want should follow . the way to go about proving that is the following . you begin with a total angular momentum operator $\hat{\mathbf{j}}$ about which you know only its commutation relations with $\hat{\mathbf{r}}$ and $\hat{\mathbf{p}}$ . you then construct the orbital angular momentum operator $\hat{\mathbf{l}}$ and show that it has the same commutation relations with $\hat{\mathbf{r}}$ and $\hat{\mathbf{p}}$ as $\hat{\mathbf{j }}$ . this means , then that $\hat{\mathbf{j }}-\hat{\mathbf{l}}$ commutes with all components of $\hat{\mathbf{r}}$ and $\hat{\mathbf{ p}}$ . now , if your system really does consist of a single particle , then it carries the direct sum of three irreducible representations of the heisenberg group , the algebra of which is of course spanned by the components of $\hat{\mathbf{r}}$ and $\hat{\mathbf{p}}$ . this means that you can apply schur 's lemma to conclude that each component of $\hat{\mathbf{j }}-\hat{\mathbf{l}}$ must be a multiple of the unit operator . finally , in an isotropic system , such a vector would break the global rotational symmetry , and must therefore be zero .
the left parentheses are equal to zero due to $u_{\rho}u^{\rho}=-c^2$ . this is true for timelike vectors in the ( -1,1,1,1 ) signature .
the gregorian calendar that we use today is a compromise between having leap years spaced as evenly as possible and keeping the calculations relatively simple . ( the older julian calendar , which had a leap year every 4 years , was also such a compromise , one with a greater bias toward simplicity . ) in the gregorian calendar , every 4th year is a leap year , except that every 100th year is not a leap year , except that every 400th year is a leap year . the cycle repeats every 400 years , with 97 leap years in each cycle . but they are not distributed as evenly as they could be ; most successive leap years are 4 years apart , but some of them are 8 years apart . the gregorian calendar assumes that a year is 365.2425 days . in fact , it is slightly less than that ; wolframalpha says it is 365.242190419 days . what you are suggesting , i think , is a calendar in which leap years are always either 4 or 5 years apart , and are as evenly distributed as possible , so that the difference between the calendar and the astronomical year are minimized . to do this , start with a 365-day year , and keep track of the offset between the calendar the astronomical year . this offset increases by 0.242190419 days every year . year 0: offset = 0 year 1: offset = 0.242190419 days year 2: offset = 0.484380838 days year 3: offset = 0.726571257 days year 4: offset = 0.968761676 days year 5: offset = 1.210952095 days add a leap day , recompute offset = 0.210952095 days . . . every time the offset exceeds 1 day , add a leap day and subtract 1 day from the offset . this will keep the offset as small as possible over time , but at the expense of easy predictability ; it becomes much more difficult for those who are not mathematically inclined to understand when the next leap year is going to be . and the calendar would presumably have to be adjusted as the number of days in a solar year is computed more precisely , or as it changes as the earth 's rotation rate changes slightly . assuming that the figure of 365.242190419 is correct , your calendar would repeat itself , not every 400 years , but every billion years . an alternative would be to keep the gregorian calendar 's assumption of 365.2425 days per year , and just distribute the leap years more evenly through the 400-year cycle . for example , starting from 2000 , you have leap years in 2000 , 2004 , 2008 , 2012 , . . . , 2128 , 2133 , 2137 , 2141 , . . . , 2265 , 2270 , 2274 , . . . . in either case , most of the time leap years would not be years that are multiples of 4 , which is a nice property of the julian and gregorian calendars . that is the math , but it is absolutely trivial compared to the politics . i am afraid there is very little chance that such a system would be widely accepted . the gregorian calendar , which was a clear improvement over the julian calendar , was introduced in 1582 , but it was not fully adopted worldwide until the late 1920s ( and there are still pockets of resistance ) . there are tremendous advantages in the fact that almost the entire world uses the gregorian calendar , and it will remain within a day or so of the astronomical year for the next several thousand years . your proposed calendar has the advantage of remaining very slightly closer to the astronomical year , but the costs of universal adoption would be tremendous , and the costs of partial adoption would be even worse .
define " best " . as always , there is no one-size-fits-all answer . are you just a casual observer , looking mostly for naked-eye objects ? or are you looking through a telescope for deep-space stuff ? is your scope a go-to that can be interfaced with and controlled from the phone ? here are some examples , look at the features and decide what is best for you : skysafari 3 , either plain , or plus , or pro . my favorite . a lot of folks hauling big dobs and whatnot use it . star walk stellarium - good for casual naked-eye gazing but not much else . starmap 3d , either plain or plus pocket universe
the saturn v payload mass to leo was 118,000 kg . wikipedia has a decent comparison of super-heavy launch systems with a payload mass to leo of 50,000 kg or more . none are in current use , and only two systems are in development . there is also a " heavy " lift launch system list which includes the delta iv and ariane 5 you mentioned . the top operational system is the atlas v hlv with a mass to leo of 29,420 kg and a mass to gto of 13,000 . however , it has never been launched and the united launch alliance claims it needs a 30 month lead-time to produce the heavy launch vehicle variant of the atlas v . next on the list with mass to leo/gto : delta iv heavy : 22,950/12,980 kg , 3/4 successful launches . proton : 21,600/6,360 kg ( comparatively lower gto due to launch location ) , 295/335 successful launches ariane 5: 21,000/10,050 kg , 54/58 successful launches . so the answer is there are no currently operational launch systems which approach the saturn v mass to leo capability .
i am not sure what you meant by : " i figured i could simply calculate the magnitude of the components since that will give me the distance " but the idea is use the kinematics equations for x and y : $x ( t ) =x_{0}+v_{x0}t+1/2at^2$ and $y ( t ) =y_{0}+v_{y0}t+1/2at^2$ these equations are derived from integrating the acceleration function $a ( t ) =-g\hat{y}=-9.8m/s^2\hat{y}$ you are given the initial horizontal and vertical position ( $x_{0}=0$ and $y_{0}$ ) and the initial horizontal and velocities ( $v_{y0}$ and $v_{x0}$ ) i think i may have said too much . . .
i personally think the text is misleading . it is blindly applying gauss ' law while not considering its subtleties . here 's a more cause-and-effect way to look at it . after this , we will get to gauss ' law . let 's take a look at the positively charged plate . yes , the surface charge density on one side doubles . but the surface charge density on the other side goes to zero . a known result about infinite sheets of charge is that distance from the sheet does not affect the electric field . so , in effect , the region to the right of the positive plate is not affected by the redistribution of charge on this plate . the total surface charge density ( if you were to flatten the plate and look at it as truly two dimensional ) did not change . so , the positive plate 's effect on the electric field is not changed when the negatively charged plate is brought near . instead , it is the presence of the second negatively charged plate that doubles the electric field in the region between the plates , not the doubling of the surface charge density on one of the plates . there is only one doubling going on . in fact , if you were to somehow take two infinite sheets each with $\sigma$ , and separated them by some small distance , the electric field outside of the region between the plates would be identical to a single infinite sheet with $2\sigma$ . this is a crucial idea . i hope it is clear . why , then , is the text saying it has to do with the surface charge density ? they are applying gauss ' law to a gaussian pillbox with the left face in the middle of the positive conductor and the right face in the vacuum between the plates . with this choice of gaussian surface , there is only a flux through the right face . this is convenient for calculations . however , the electric field that gives rise to this flux is not due only to the enclosed charge , even though we mathematically calculate it like that . rather , the electric field one should use in gauss ' law is the total electric field due to all charge distributions , including charges outside the gaussian surface . this is one of the subtle but amazing facts of gauss ' law : to calculate the flux through a surface , you only need to mentally worry about the enclosed charge , but the result you get for the electric field ( if you can indeed extract the field from the flux , usually in a highly symmetric geometry ) is due to all of the charge , not just the enclosed charge . so , for your particular problem from h and r , the flux through the right face of the gaussian surface is caused by the superposed electric field from both plates . neither of these fields alone changed , but their effects superpose , causing a doubling of the net electric field . but when applying gauss ' law for this problem , we usually do not worry about what actually causes this electric field . the doubling of the electric field is " accounted for " mathematically by the doubling of the surface charge density on the positive plate . however , viewing this doubling of the charge as the cause of the now-doubled electric field is not correct in my opinion .
the poynting vector $\mathbf{s}$ represents the flow of energy in an em field . specifically , if $u$ is the energy density of the field , the poynting vector satisfies the continuity equation for it : $$\frac{\partial u}{\partial t}+\nabla\cdot\mathbf{s}=0$$ in vacuum . ( this is poynting 's theorem . ) in your particular problem , $e$ and $b$ are perpendicular and their cross product is proportional to the product of their amplitudes . thus $$s_z={c\over\mu_0}b^2 . $$ you then have to use your knowledge of $b$ to work out $s$ .
mathematically , it just gives a duality with vector fields and scalar fields in multivariable calculus , associated with conservative vector fields and line integrals . as such , the $\pm$ is irrelevant , because it can be absorbed into the force vector . for physics , we take the sign convention to be negative , so that it agrees with the fact that the force is restoring the object it acts on to a lower energy configuration . note that we could alternatively absorb the negative sign into the potential ! it is all a matter of sign convention , and when you define potential and force in physics ( as stated above ) , the negative sign appears in your equation .
it is not true that \begin{align} e_n = h+\hbar\omega_0 . \end{align} why ? well , $h$ is a linear operator while $e_n$ and $\hbar\omega_0$ are real numbers ; an operator cannot equal a real number . you might try to fix this by multiplying each of the real numbers by the identity operator and then claim that \begin{align} e_n i = h+\hbar\omega_0 i \end{align} this is still not correct . you can immediately tell that it can not be correct because the left hand side depends on $n$ , a non-negative integer , by the right hand side does not . so what is going on ? as essentially mentioned in the comments , the following statement is true : if $\psi$ is an eigenvector of the harmonic oscillator , then there exists a non-negative integer $n$ for which \begin{align} h\psi = \left ( n+\frac{1}{2}\right ) \hbar\omega\psi \end{align} where here we are using notation in which the harmonic oscillator hamiltonian is given by \begin{align} h = \frac{1}{2m} p^2 + \frac{1}{2}m\omega^2x^2 \end{align} where $p$ and $x$ are the position and momentum operators respectively . in other words , if you act the hamiltonian on an eigenvector , then it acts simply by multiplying that eigenvector by a real number , the corresponding eigenvalue . but when the hamiltonian acts on a vector that is not an eigenvector , this does not happen .
first of all , check your arithmetic on the x equation . you should get vg ( x ) = +1.74 m/s . second , in the y equation you should use the final orange speed 4.34 instead of 5.3 on the right hand side . you should get vg ( y ) = -2.48 or so .
they are stating that as part of the question , not stating that it is necessarily true . you often see things like " ignoring air resistance " , a " frictionless plane " or " massless spring " as part of questions to allow a simple analytical answer . in reality for a car moving at 40 m/s air resistance is likely to be the major source of drag and this is proportional to velocity^2 , but this makes the resulting equations trickier
at the particle level the verb " charge " has no definition . charge cannot be added to a particle . a particle has charge ( noun ) ; it is a quantum number that characterizes the particle , and its charge may be 0 , +/-1/3 , +/-2/3 , +/-1 ( and some resonances +/-2 ) . a photon has charge 0 , spin 1 and mass 0 . that is why it is called a photon and not an electron . if it is possible to bend it than why not charge that it can change direction ( bend ) is a kinematic effect and controlled by the equations of motion . the quantum numbers are intrinsic and unchangeable in the definition of each particle .
any reshaping of the droplet will require flow of water inside the droplet and there will be viscous losses . presumably the energy would come from an increased torque on whatever motor was moving the droplet and substrate .
the determinant is fairly easy to calculate . you know already , essentially , the eigenvalues of the stiffness matrix ; more accurately , you know the eigenvalues of the matrix $\mathbf{m}^{-1}\mathbf{k}$ , because the $\omega_i$ are zeros of the equation $$0=\det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) . $$ ( the more aesthetically minded would replace $\mathbf{m}^{-1}\mathbf{k}$ with $\mathbf{m}^{-1/2}\mathbf{k}\ , \mathbf{m}^{-1/2}$ to get a hermitian matrix , but no matter . ) if you express the second determinant in the corresponding eigenbasis , you get $$ \det ( \mathbf{k}-\mathbf{m}\ , \omega^2 ) =\det ( \mathbf{m} ) \det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) =\frac{m^3}2\det\begin{pmatrix} \omega_1^2-\omega^2 and 0 and 0 \\ 0 and \omega_2^2-\omega^2 and 0 \\ 0 and 0 and \omega_3^2-\omega^2 \end{pmatrix} , $$ which gives your textbook 's expression . more generally , this is an expression of the principle that a matrix 's determinant is the product of its eigenvalues . the adjunct , on the other hand , does not ( to my knowledge ) satisfy any such nice relation ; in any case it is a nasty beast to deal with and i think few people judiciously substituting in the definition $k=m\omega_2^2/2$ of $\omega_2$ instead of $k$ .
the physics of a gliding airplane are simple . there is potential energy , proportional to height above the ground . there is also kinetic energy , proportional to speed squared . first , understand the speed . if the plane is not slightly nose-heavy , it will fly a scalloped up-down cycle . if it does that , add a little weight to the nose , or distribute the wing area more toward the rear . assuming you have done that , you control the speed by turning up the trailing edges . the more they are turned up , increasing the angle of attack , the slower it flies . ( up to a maximum angle of attack , at which the wings stop working , or " stall " . ) back to energy . if there were no drag , the plane would never come down . since there is drag , the drag tends to slow the plane down , decreasing its kinetic energy . countering that is the plane 's tendency to maintain constant speed and kinetic energy , so it descends , turning potential energy into kinetic energy , just like a ball rolling down a slope . so the more drag , the more quickly it descends , the less drag , the more slowly it descends . a way to minimize drag is to minimize speed , because drag force is proportional to speed squared . ( therefore the sink rate is roughly proportional to speed squared . ) so the speed you trim it for depends on what you want to maximize : to maximize gliding range , you trim for a speed which is slow enough to have low drag , but not so slow that you do not cover much ground . to maximize time aloft , you trim for an even slower speed which has even lower drag , thus minimizing the sink rate . this speed is roughly half way between the speed for maximum range and the even slower stall speed $v_s$ . check these links : v-speeds , and gliding flight .
have a look at the graphic in lubos motl 's answer to another electromagnetic question . it shows a polarised plane wave propagating in time and you can take a time projection that answers your question . unpolarised plane waves have all possible orientations in space .
the x axis there is not time , it is just six bins . each pair of bins is labeled by year , and can be assumed to represent all the data taken during that calendar year ( or to-date in the case of 2011 ) . that is going to be multiple periods of " running " each year , with varying beam intensities and both regularly scheduled and unexpected beam stops during the running periods . you can probably find the beam logs for the sps with enough digging around cern 's websites ( and possibly emailing ) .
i am not sure the assumptions of your question are true . for example , in this picture , both the bones and the metal ring appear black . can you link to examples of the white bones/black metal x-ray images you describe ? there are ways to obtain contrast with x-rays that do not rely on absorption , such as scattering . however , i think most medical x-rays are based purely on absorption .
he did not mean ' forget ' it . he just meant that it is not really relevant . it is easy to understand the various relationships in dc . when you go to ac they are all the same at any given moment on the wave . it all boils down to v=ir and vi=w , ri^2=w , v^2/r=w . with dc it is just a constant . with ac they change with time . if you pick any moment in that time and apply the above , you will have your answer .
there are numerous excellent reviews out , two that come to mind are : r . hoffmann in angew chem int ed engl , 26 , 846 r . hoffmann in rev mod phys , 60 , 601
i would recommend steering clear of schwarzschild coordinates for these kind of questions . all the classical ( i.e. . firewall paradox aside ) infinities having to do with the event horizon are due to poor coordinate choices . you want to use a coordinate system that is regular at the horizon , like kruskal-szekeres . indeed , have a look at the kruskal-szekeres diagram : ( source : wikipedia ) this is the maximally extended schwarschild geometry , not a physical black hole forming from stellar collapse , but the differences should not bother us for this question . region i and iii are asymptotically flat regions , ii is the interior of the black hole and iv is a white hole . the bold hyperbolae in regions ii and iv are the singularities . the diagonals through the origin are the event horizons . the origin ( really a 2-sphere with angular coordinates suppressed ) is the throat of a non-traversable wormhole joining the separate " universes " i and iii . radial light rays remain 45 degree diagonal lines on the kruskal-szekeres diagram . the dashed hyperbolae are lines of constant schwarzschild $r$ coordinate , and the dashed radial rays are lines of constant $t$ . you can see how the event horizon becomes a coordinate singularity where $r$ and $t$ switch roles . now if you draw a worldline from region i going into region ii it becomes obvious that it crosses the horizon in finite proper time and , more importantly , the past light-cone of the event where it hits the singularity cannot possibly contain the whole spacetime . so the short answer to your question is no , someone falling into a black hole does not see the end of the universe . i do not know the formula you ask for for $t$ , but in principle you can read it off from light rays on the diagram and just convert to whatever coordinate/proper time you want to use .
this paper details the mathematical model behind what you are doing . by far the most difficult aspect is not the balloon itself but collisions with the environment : http://arxiv.org/pdf/physics/0407003.pdf
to address your first question : whether this is a good choice depends on what you consider " good " . it is clearly a simple choice and simplicity might be good , moreover it is an ansatz that minimizes the similar problem of the hydrogen atom . if you define " good " as giving you an accurate upper bound very close to the true ground state energy , you can not know unless you improve your ansatz or calculate the exact ground state energy . by improving your ansatz , which will lead a more complicated expression , or trying a new one you can compare the energies and see how good the initial one was . to address your second question : as said in the thread you asked a similar question in , the virial theorem only holds if ( and only if ) the considered wave function is an energy eigenstate of system . so in order to use the virial theorem you have to show that $$\hat h\ , \psi ( r , \xi ) =\epsilon ( \xi ) \ , \psi ( r , \xi ) , $$ which should nearly never be true . therefore you should actually never use the virial theorem within the variational method .
the most general , integral form of faraday 's law is ( see this physics . se question : faraday&#39 ; s law for a current loop being deformed ) \begin{align} \int_{c_t} ( \mathbf e+\mathbf v\times\mathbf b ) \cdot d\boldsymbol \ell = - \frac{d}{dt}\int_{\sigma_t}\mathbf b\cdot d\mathbf a \end{align} where $c_t$ is some closed curve that can depend on time , $\sigma_t$ is a surface with $c_t$ as its boundary , $\mathbf e$ and $\mathbf b$ are the electromagnetic fields as measured in some inertial frame , and $\mathbf v$ is the velocity of a point on the curve resulting from its time-dependence . now if we consider the situation you describe , then the $\mathbf v\times\mathbf b$ terms goes away if we choose a stationary loop $c=c_t$ , and we get \begin{align} \int_{c}\mathbf e\cdot d\boldsymbol \ell = - \frac{d}{dt}\int_{\sigma}\mathbf b\cdot d\mathbf a \end{align} now you say that all the magnetic field and hence flux is confined within the windings . this is true . however you also say that therefore , for a wire making a loop surrounding the toroid and passing through the centre , the fields ( electric and magnetic ) at the wire is zero this is not quite right . if the right hand side ( the rate of change of the flux ) is nonzero , then the line integral of the electric field around the loop must be nonzero . \begin{align} \int_{c}\mathbf e\cdot d\boldsymbol \ell \neq 0 \end{align} in particular , this means that the electric field itself cannot vanish along the loop , otherwise we would have a contradiction . in other words , it may be the case that there is no magnetic field along the loop ( at least at the initial instant before any current is generated ) , but there is an electric field along the loop , and this pushes charges around ( if the loop is a conductor with charges in it ) . as a side note , once the charges start moving , they create their own magnetic field even in the absence of a magnetic field produced by the solenoid .
sure you can clone a state . if you know how to produce it , you can just produce one more copy . the answer to your question therefore lies in the specifics of the no-cloning theorem . it states that it is not possible to build a machine that clones an arbitrary ( previously unknown ! ) state faithfully . stimulated emission does not fulfill this . given an atom , only a certain range of frequencies , etc . can actually be used to produce stimulated emission , so you can not faithfully clone an arbitrary state . it is just an approximation to cloning , which is not prohibited . see also : http://arxiv.org/abs/quant-ph/0205149 and references therein .
evaporative cooling works by removing the high-velocity tail of the kinetic energy distribution . that is , only the fastest molecules escape the liquid , leaving the rest to thermalize at a lower temperature . if there is capillary action taking water to the outside of the pot and that is evaporating , then the pot cools down as it is losing heat to the leaving molecules ' kinetic energy . this then cools down the water inside by conduction . one can then ask why , if the air is hotter than the water , can heat flow from the water into the hot air ? the answer to this is that there is also a reverse process which is also possible : on a wet day , water molecules in the air can rejoin the water on the pot walls , and if the air is hot then in the mean this process will heat the pot and the water . in an equilibrium situation , both of these processes happen at the same rate and there is no heat flow . for evaporative cooling to work , the air needs to be dry so that more molecules leave the water than condense into it . in a closed environment , though , evaporation will raise the air 's humidity until both processes are equally likely and everything thermalizes . on a windy day , though , the pot is trying to raise the humidity of the whole atmosphere , which is not going to happen soon . this is an open system , in contact with an infinitely dry reservoir of dryness . i heartily recommend feynman 's lectures on thermodynamics , such as the distinction of past and future , for getting a gut understanding of this aspect of the arrow of time .
a part of your question sounds like the covariant treatment of membranes ( and higher-dimensional branes ) in string/m-theory . of course , to do actual quantitative calculation , one has to choose a particular embedding and world volume coordinates along the membrane , and impose the coordinate redefinition symmetry ( the same thing is done for strings ) . however , the embedding in some actual spacetime still exists . if it is guaranteed or required to exist , it makes no sense to pretend that it does not exist : the system will effectively be all about wave equations for the transverse coordinates ( to the brane ) . if you wanted to encode the curvature of the brane in its metric tensor only , assuming that the metric tensor behaves just like an induced metric from a higher-dimensional space , you would get different equations of motion . the simplest equations for the metric are einstein-like equations which are second-order in the metric ; however , the induced metric is proportional to the derivatives of spatial coordinates , so the einstein-like equations would be third-order in the spacetime coordinates , and moreover nonlinear to contract the odd number of indices . but i probably do not understand what exactly you have in mind - and apologies , it is probably because what you have in mind is impossible mathematically .
" fully compatible with observations " is a rather vague statement . actually , two aspects of adequacy to reality have to be distinguished when a new theory reaches a degree of explicitation . these are compatibility with older theories , in domains where the new theory is not supposed to bring more than a new formulation . for instance , special relativity is compatible with newtonian mechanics when velocities are small compared with c . since older theories taken in reference have been usually thoroughly tested ( otherwise you do not take them as reference ) , this is a good first check for your new theory . compatibility with new phenomena . indeed what makes a new theory interesting is the change of insight that it might bring on reality . and this means that beyond proposing a new description of reality , it shall predict new observable features which older theories do not account for . as far as lqg is concerned , my understanding is that the first aspect has been addressed in the sense that right from the outset , conpatibility with gr has been used as a guide to develop the theory . for the second aspect , this one of the topics which focuses a good part of the efforts of the lqg community . this means finding new observable features that survive going from the planck scale to the scales that are accessible to us in experiments or astrophysical observations . it is tricky but not impossible . so as far as the statement " fully compatible with observations " , i would advise to replace it with " compatible with previous observation-tested theories , but still expecting genuine experimental predictions for testing " .
i think this is a combination of both a convention and a physical problem . you are equating the energy eigenvalue ( ie , the total energy ) to an expression that contains only $x_{zpf}$ , and does not contain $p$ at all . in other words , you are equating the total energy to a potential energy . this would be analogous to equating $e_\mathrm{total} = \frac{1}{2}ka^2$ to find the amplitude $a$ of a classical harmonic oscillator . the result is that you are using $x_{zpf}$ to mean the " amplitude " of the zero-point fluctuation . the true result , as ondrej cernotik 's answer derives , uses the rms value $x_{zpf} = \sqrt{\langle\hat x^2\rangle}$ . so that is the sense in which it is a convention . the sense in which it is a real physical problem is that the " amplitude " of a quantum oscillator is not really a well-defined , measurable thing . the quantum oscillator has a non-zero probability amplitude going all the way out to infinity . the rms value is well-defined and easy to measure . so that is the preferred definition .
why laundry dry up also in cold/frost ? probably because , initially , the clothes and the liquid water trapped in the clothes fibres , are both at a temperature well above 0 c . when you have frost , water in the clothes should freeze , and it does , when the temperature of the garment and water trapped within it have eventually reduced to below 0 c but if clothes are dry , then it should be possible that steam in the clothes if clothes are already dry , the situation is not relevant to your question of how ( wet ) clothes become dry ( or dryer ) when air temperatures are below 0 c . it should be possible that steam in the clothes does not have time to freeze . if you move warm wet clothes into a cold environment for a sufficiently short time , the water will indeed not have time to freeze . the temperature of the majority of water will not be sufficiently reduced . left longer in air at a temperature below 0 c the liquid water in the clothes will freeze and any water vapour in the clothes will form frost .
the kinetic energy of the stone will not be equal at the same point on the way up or the way down , due to the presence of friction from the air . to show this , it is enough to compare the speed of the stone just after it leaves your hand ( on the way up ) with the speed just before it lands back in your hand ( on the way down ) . first consider the case without air resistance . you launch the stone with speed $u$ directly upwards . the initial energy you give the stone is $$ ke = \frac{1}{2}mu^2 . $$ the stone goes up until it reaches its maximum height , where all of its initial kinetic energy is converted into potential energy . then the stone comes back down , and by the time it reaches your hand again all of this potential energy has been converted back to kinetic energy . let 's call the final speed of the stone $v$ . the final and initial energies must be the same . since all the initial kinetic energy has been converted back to kinetic energy , the initial and final kinetic energies are the same , or in maths : $$ \frac{1}{2} m u^2 = \frac{1}{2} mv^2 $$ . now add air resistance to the picture . as the stone moves through the air , some of its kinetic energy is used up and converted into heat energy , raising the temperature of the stone and the air slightly . the initial and final energies of the entire system ( stone and air ) still have to be the same , so we now have \begin{eqnarray} \text{initial energy} and = and \text{final energy} \\ \frac{1}{2}mu^2 and = and \frac{1}{2} m v^2 + ( \text{heat energy from friction} ) . \end{eqnarray} rearranging this equation , you get $$\text{final kinetic energy} = \frac{1}{2}mv^2 = \frac{1}{2}mu^2 - ( \text{heat energy from friction} ) , $$ which is smaller than the initial kinetic energy . a similar argument shows that the kinetic energy is less on the way down than the way up for any point on the trajectory .
imagine a finite segment of the rope , say on the left side . suppose the tension at the top of the segment is $t_t$ and the tension at the bottom of the segment is $t_b$ . then the segment of rope feels a force $t_t$ up and $t_b$ down , or a net force $t_t - t_b$ up . since the rope is massless there is no gravitational force so that $t_t - t_b$ is the net force on the rope . newtons law says that the acceleration of the rope is $\dfrac{t_t - t_b}{m}$ upward , where $m$ is the mass of the rope segment . however , since $m$ is zero this acceleration must be infinite , which is a problem . what is really going on is that if there were a net force up , the rope segment would immediately accelerate upward . this upward movement would relax the tension in the upper part of the rope ( $t_t$ decreases ) and increase the tension in the lower part of the rope ( $t_b$ increases ) . this will continue until $t_t$ equals $t_b$ and there is no net force on the rope . since the rope is massless , this process happens very fast so it can always be assumed that $t_t$ is equal to $t_b$ . since the segment of rope could have been anywhere , this means that any two points on the rope must have the same tension .
the problem is azimuthally-symmetric , so the tangential direction is the one with no azimuthal component , i.e. the one " straight up the side " if you were a small mountain climber climbing from the surface to the top of the drop . all three surface tensions are required because all three exert forces . for example , if the solid-air surface tension were extremely high , the system would try to minimize the area of contact between the surface and the air , which corresponds to spreading the drop out flat . if the solid-air surface tension were very low , the opposite would occur and the drop would be a sphere touching the surface in a very small area .
it is an interesting question and i know exactly what you mean . i often look at the likes of stoner performing what you describe and think " wow ! " . right , for the answer . . . although the front wheel of any bicycle plays a key role in providing stability , it is not required for cornering . the ability to change direction is provided by a centripetal force , which is usually provided by both wheels of the motorcycle . the front wheel in this case providing the stability to the bike via the ' mechanical trail ' ( see the aside below ) . however , if the front wheel is off the ground , this does not mean that the centripetal force required to corner cannot be achieved ( although the rider has to be much more skilful by using one wheel without the stability provided by the front wheel 's mechanical trail ) . note , in the case of a 230-260bhp motogp bike , cornering at high speed , having one wheel off the ground does not mean that the act of cornering is too-fine-a balancing act , the rapidly rotating wheels in this case will provide a ' large ' amount gyroscopic torque . you will often see riders turning the front wheel in the cases you mention . this acts to allows the rider to change the bikes trajectory , and pivot slightly about the rear wheel 's point of contact and provide some lateral stability . aside : an interesting thing about the use of the front wheel to steer a motorcycle ( or any bicycle for that matter ) is ' mechanical trail ' or ' caster ' . this quantity is one thing that i was not aware of until i designed and made my own downhill racing bike many years ago . mechanical trail is the perpendicular distance between the steering axis and the point of contact between the front wheel and the ground . it may also be referred to as ' normal trail ' . this quantity is what determines how a bicycle handles when steering . see the picture below if the trail is positive ( trail shown by the green arrows ) , the bike will be stable . the more trail , the " heavier " the steering . if this distance is ( too ) negative , you cannot steer you bike ( try turning the handle bars of your push bike the wrong way around [ we have all done it as kids but i do not recommend it ! ] . for me , the reasons for trail playing such a key role in the steering stability of a bike is ( very loosely ) ' causality ' . the trail or ' lead ' allows the rider to adjust the lateral force on the front tyre as required before the point of contact of the tyre travels the trail-distance . this is a poor , on-the-fly explanation of trail and why it works the way it does and i would be interested to hear some of the other guy’s thoughts on this . . . but that is another question entirely ! i hope this helps .
there are several occasions in the past when naturalness has been experimentally confirmed . one of the nicest is the successful prediction of the mass of the charm quark by mary k . gaillard and benjamin lee . in the standard model , the gim mechanism suppresses flavor-changing neutral current transitions . but if the charm quark were absent , this would not be true , and a number of observables in kaon physics would be very different . using one of these observables , the k-long/k-short mass difference , gaillard and lee realized that the mass of the charm quark needed to be low enough to cut off a divergence , putting it at about 1.5 gev . the true mass turned out to be about 1.3 gev , so their estimate was pretty accurate .
well first of all the major points here are potential difference across both capacitors will remain same after insertion of dielectric in $c1$ , and the total charge of the system will remain constant . now in $c1$ after insertion of dielectric , calculate change of capacitance , assume that it has new potential $v1$ and charge $q1$ , assume potential and charge for 2nd capacitor too as $v2$ and $q2$ . equate $v1$ and $v2$ , also equate $q1 + q2$ with initial total charge of system . this way you can calculate $v$ across capacitors and $q1$ and $q2$ . now to calculate polarisation charge , check the electric field $ e1 ( without dielectric ) $ now find electric field $e2 ( with dielectric ) $ this difference has come because of insertion of dielectric so the electric field of dielectric ( calculate for a hypothetical parallel plate capacitor ) is equal to the difference in electric fields , this will give you tue induced charge . lastly , since you know initial and final potential and charges , apply formula for potential energy of capacitors both before and after insertion of dielectric and take the difference , this is the change in potential energy . the system undergoes changes as the following , once the capacitors are charged , when dielectric is being inserted in the capacitor $c1$ charges are induced in it and it is pulled in by electrostatic interactions , this process consumes some energy and this changes the potential . also since the capacitors are in parallel and maintain same potential , the charges travel from one capacitor to another after insertion of dielectric to eliminate the potential difference between capacitors .
i will sketch one of the eternal inflation variants : " false-vacuum driven eternal inflation " . the idea is that you start with a spacetime manifold , on which is everywhere defined some scalar field . the scalar field ensures that whole region undergoes inflation . to ensure some sort of stability , the value of the field is chosen to be at a local minimum of the potential - it is not at the global minimum and hence tends to be called a false vacuum . now , due to some sort of perturbation , or due to quantum mechanical tunneling , the scalar field ends up , at least at some location , having a value at the other side of the potential barrier . one tunneling mechanism is described by the coleman de luccia instanton . this point with the new tunneled-to $\phi$ value is thought of as the origination point of a small bubble containing a different phase inside the inflating spacetime . it is sometimes called a nucleation point in analogy with bubble nucleation , in which bubbles form on particles suspended in a liquid . the potential is now able to roll down the hill and release energy inside the bubble . the bubble wall expands and the bubble contents continue to expand , but at an ever decreasing rate . anything inside this bubble , i.e. in the forward light cone of the nucleation point has the new $\phi$ value , and hence physics in there sees a different vacuum . the spacetime inside this bubble has a " natural " foliation by 3-dimenional hypersurfaces of constant $\phi$ . as $\phi$ decreases , these surfaces become spatially flat , like our observable universe . these bubbles ( "pocket universes" ) originate in regions where inflation actually ends . so in this soft of model , the orgin of our observed universe , is the end of inflation . outside of the bubble in question , other bubbles are continually nucleating . bubbles may even merge . of course this sort of model is only as good as the predictions it makes for our observable universe . there has been some discussion of the possible observability of bubble nucleation in our past . see here for example . so , getting back to the question , if you want to attach the term " multiverse " to this scenario , i would assume it refers to the ensemble of pocket universes , embedded in an inflating false vacuum spacetime which , at least in this toy model , obeys the usual laws of gr and quantum mechanics .
in the context of shm they are probably meant to be interpreted as both scalar equations ( ie $x$ , $v$ and $a$ are all one-dimensional ) , especially since they use $x$ and not $r$ , but they can also be written vector form . the first equation would be ( using $r$ for the position vector ) $$ \vec{a} = \frac{d\vec{r}}{dt} $$ no surprises here . the second would be $$ \vec{a} = \frac{d\vec{v}}{d\vec{r}} \vec{v} $$ this is strange - how can you differentiate a vector with respect to another vector ? well , let 's say we are working in three dimensions . then an expression like $$ \frac{d\vec{v}}{d\vec{r}} $$ would be something like , how does $\vec{v}$ change as i change $\vec{r}$ ? but you can change $\vec{r}$ by changing $x$ , $y$ or $z$ , then in each case there will be a change in $v_x$ , $v_y$ and $v_z$ , so you need 9 numbers to specify $\frac{d\vec{v}}{d\vec{r}}$ . as @pranav says , this is a second-order tensor ; in cartesian coordinates this would be $$ \frac{d\vec{v}}{d\vec{r}} = \begin{bmatrix} \dfrac{\partial v_x}{\partial x} and \dfrac{\partial v_x}{\partial y} and \dfrac{\partial v_x}{\partial y}\\ \dfrac{\partial v_y}{\partial x} and \dfrac{\partial v_y}{\partial y} and \dfrac{\partial v_y}{\partial y}\\ \dfrac{\partial v_z}{\partial x} and \dfrac{\partial v_z}{\partial y} and \dfrac{\partial v_z}{\partial y}\\ \end{bmatrix} $$ so the equation does make sense in the end .
i can not explain qm here . it takes a lot of reading and working things out for yourself . for this particular question , however , an analogy might help ( this may be far below your level , in which case apologies ) . qm is very often about " simple harmonic oscillators " ( shos ) , for which the oldest prototype is the pendulum ( approximately , if the amplitude is small ) . for a pendulum , if we want to know how far it will go from the vertical , we can wait to see how far it goes on each cycle . an alternative way is to measure how fast the pendulum goes when it passes through the vertical . for any given speed , there is a corresponding farthest distance from the vertical . we can equate these two , in a notional sort of way , by choosing units just so , $s_0=d_1$ , the speed at its maximum is the same as its farthest distance from vertical . [ if you do not want to choose such helpful units , write $s_0=kd_1$ . ] now , suppose that we measure the speed and the distance at some intermediate point , for which we obtain $s_t , d_t$ . for a simple harmonic oscillator , and approximately for a pendulum if its oscillations are small , we obtain $\sqrt{s_t^2+d_t^2}=s_0=d_1$ . the square root $\sqrt{s_t^2+d_t^2}$ is an invariant quantity of the coordinates $ ( s_0,0 ) $ and $ ( 0 , d_1 ) $ , which in general are $ ( s_t , d_t ) $ . anything that is a function of the square root $\sqrt{s_t^2+d_t^2}$ is also a function of $s_t^2+d_t^2$ , so we can work with whichever is more convenient . the effects of a given sho on other systems ---or of a system that contains many shos on other systems--- are determined both the phases and by the amplitudes , but the amplitude often determines the more obvious properties , with differences of phase causing important but often more subtle effects , which we typically might call interference ( but there are many other words , such as " caustics " , or even , in a new age sort of way , " sympathetic vibrations " ! ) . the effects of a given quantum mechanical system are , at an elementary mathematical level , sui generis with a classical sho or system of shos , but quantum mechanics describes the ways that the probabilities of discrete events evolve over time , instead of describing the evolution of a trajectory . the introduction of probability as an essential property makes qm a discussion of a higher order mathematical object . especially different is the fact that we can no longer talk about velocities , because individual events do not have velocities ( if we are determined to talk in terms of particles we cannot in general be sure which individual events go with which particle ) , however it is useful to introduce a notional object that we call momentum , which allows us to model patterns that we observe in the evolution of the probabilities as interference effects ( whether that is what they are not , we can model the patterns in the probabilities using patterns of varying phases and amplitudes ) . the mathematical quantity that we call momentum is , however , sufficiently different from the classical momentum that is associated with a particle trajectory that the analogy breaks down in various mathematically significant ways . i can not see how to address the final aspect of this that occurs to me , for now , at least not well . the much touted linearity of quantum mechanics is a consequence of the fact that qm describes the evolution of probabilities of individual measurement events . the object we call momentum is closely related to the mathematics of fourier transforms of probability distributions , which is essentially associated with a squared modulus like $s_t^2+d_t^2$ . one consequence of that is noncommutativity of the algebra of observables . this is a quick and very vague writing down of a lot of experience , without much editing , so take it with a pinch of salt and with a lot of other reading of what other people have to say about the hard questions that quantum mechanics poses for us . i hope you find it more useful than confusing , but hey , i can take a few downvotes , and it is been oddly useful to me to write this down in this somewhat wild way . in fact , if you can see the ways in which this answer is related to your question , you understand qm pretty well already .
this was demonstrated by " experiment 144" at slac in 1997 . here is a list of publications from that project , for instance " positron production in multiphoton light-by-light scattering " , whose abstract reads : a signal of 106±14 positrons above background has been observed in collisions of a low-emittance 46.6 gev electron beam with terawatt pulses from a nd:glass laser at 527 nm wavelength in an experiment at the final focus test beam at slac . the positrons are interpreted as arising from a two-step process in which laser photons are backscattered to gev energies by the electron beam followed by a collision between the high-energy photon and several laser photons to produce an electron-positron pair . these results are the first laboratory evidence for inelastic light-by-light scattering involving only real photons .
yes , kinetic energy is a relative quantity . as you might guess , this means that when you are using energy conservation , you have to stay within a single frame of reference ; all that energy conservation tells you is that the amount of energy as measured in any one frame stays the same over time . you can not meaningfully compare the amount of energy measured in frame a ( e . g . the ground ) to the amount of energy measured in frame b ( e . g . the train ) . however , you can convert an amount of kinetic energy measured in one frame to another frame , if you know their relative velocity . if you are working at low speeds , the easy ( approximate ) way to do this is to just calculate the relative velocity , as you did . so if the train observer measures a kinetic energy $k = \frac{1}{2}mv^2$ , the ground observer will measure a kinetic energy of $\frac{1}{2}m ( v + v ) ^2$ , or $$k + \sqrt{2km}v + \frac{1}{2}mv^2$$ ( in one dimension ) . if you get up to higher speeds , or you want an exact expression , you will have to use the relativistic definition of energy . in special relativity , the kinetic energy is given by the difference between the total energy and the " rest energy , " $$k = e - mc^2$$ one way to figure out the transformation rule is to use the fact that the total energy is part of a four-vector , along with the relativistic momentum , $$\begin{pmatrix}e/c \\ p\end{pmatrix} = \begin{pmatrix}\gamma_v mc \\ \gamma_v mv\end{pmatrix}$$ where $\gamma_v = 1/\sqrt{1 - v^2/c^2}$ . this four-vector transforms under the lorentz transformation as you shift from one reference frame to another , $$\begin{pmatrix}e/c \\ p\end{pmatrix}_\text{ground} = \begin{pmatrix}\gamma and \gamma\beta \\ \gamma\beta and \gamma\end{pmatrix}\begin{pmatrix}e/c \\ p\end{pmatrix}_\text{train}$$ ( where $\beta = v/c$ and $\gamma = 1/\sqrt{1 - \beta^2}$ ) , so the energy as observed from the ground would be given by $$e_\text{ground} = \gamma ( e_\text{train} + \beta c p_\text{train} ) $$ the kinetic energy is obtained by subtracting $mc^2$ from the total energy , so you had get $$k_\text{ground} = \gamma ( e_\text{train} + \beta c p_\text{train} ) - mc^2$$ which works out to $$k_\text{ground} = \gamma k_\text{train} + ( \gamma - 1 ) mc^2 + \gamma\beta c p_\text{train}$$ where $k$ is the relativistic kinetic energy and $p$ is the relativistic momentum . if you wanted it in terms of energy alone : $$k_\text{ground} = \gamma k_\text{train} + ( \gamma - 1 ) mc^2 + \gamma\beta\sqrt{k_\text{train}^2 + 2 mc^2 k_\text{train}}$$ you might start to notice a similarity to the non-relativistic expression above ( $k + \sqrt{2km}v + \frac{1}{2}mv^2$ ) , and indeed , if you plug in some approximations that are valid at low speeds ( $\gamma \approx 1$ , $\gamma - 1 \approx v^2/c^2$ , $k_\text{train} \approx \frac{1}{2}mv^2 \ll mc^2$ ) , you will recover exactly that expression .
it would crash into the earth because the earth 's gravitational field is not uniform and , even if said ring were to be perfectly positioned , ignoring the effects of wind , strikes from cosmic debris ( not a lot that low in the atmosphere ) , change in mass of the ring ( e . g . corrosion ) , change in shape of the ring ( due to e.g. gravitational forces , heat deformation from sunlight ) , etc . , the earth 's gravitational field distribution changes over time and the ring would eventually fall out of its perfect position and crash into the surface .
starting at ${position}_z$ = $z$ = 0 and $v ( z ) = 0$ and by tracking multiple acceleration values either with a time interval or at fixed intervals , $t$ , then you can get the position . . . . somewhat . it will drift over time . also , your device cannot rotate whatsoever , or else you need a gyroscope to track that and then use trigonometry to properly orient the x y and z values from the accelerometer . assuming it is always oriented such that the $a ( z ) $ is always perfect vertical acceleration ( if you are in a vehicle that is always flat , in which case z does not matter , or you are on a vertical guide rail ) , $$p ( z ) = \int_0^t v ( z ) ~dt = \iint_0^t a ( z ) ~dt $$ also , from here : short answer : forget about it . longer answer : unless you are on a perfectly straight rail , you will not achieve what you want to do without ( a ) a set of gyros ; and ( b ) far more accurate sensors than what you have . accelerometers measure acceleration in the body fixed reference frame , whereas you need some displacement in an earth-fixed frame . therefore , you need not only to integrate the accelerometers , but rotate them into the earth-fixed frame before doing the integration . this is assuming perfect sensors . mems sensors are far from perfect - i have written up a post on some of the errors here . consider two errors : 1 . a bias on the accelerometer . 2 . an initial attitude ( tilt ) error . in addition to whatever acceleration signal there is , integrate a bias and you get a ramp error with time . integrate the ramp and you get a quadratically increasing error with time . this will add up really , really quickly . consider a tilt error . you will now be measuring some of the gravity vector in the forward ( or whatever ) direction . integrate this error twice and you will have the same problem as the bias . so , my advice again is do not ! find another method . also , check this book out for more detailed designs , or use whatever sensors and algorithm these guys are on : http://www.youtube.com/watch?v=6ijarke8vku if you still want to give this a shot , use the trapezoidal method in excel , it is pretty easy . there is an explanation page here with a sample , but here 's a more complete way :
these problems relate to constraints . this particular subset of costraint problems will fall under atwood machines ( pulley problems ) , under laws of motion , under mechanics . these are not usually standalone problems per se , but constraints are required for one to be able to solve pulley problems . constraints basically , these do with the ' conservation of length of string ' . lets take a simple two-block-one-pulley system . the movable parts in it are the two blocks . now , lets assume each one moves a distance $s_1 , s_2$ . assume that by some mystical method , only one block moved . write down the ( signed ) elongation in the string required . do the same for the other block . now , for the block which moves up , the elongation will be $-s_1$ ( negative since its a contraction ) . for the block moving down , it will be $+s_2$ . since the string is not stretchy , the net elongation should be zero . so $-s_1+s_2=0\implies s_1=s_2$ . ok , we sort of knew that . but this method is scalable , unlike the intuitive method of staring at it for a while . we can differentiate this to get relations between velocities , accelerations , jerks , jounces , whatever . note that in the given problem , your pulley moves as well , and it elongates the string thrice as much as its motion . you can see the constraint relation peeking out in the nearby equation ( dunno what the $c$ and $t$ are ) shortcut-virtual work there is a shortcut method to this , which is even more scalable . assiume massless pulleys ( this method works for massive pulleys as well , as it is a geometric relation in the end ) for each string in the system , give it a tension $t_i$ . remember , tension in a string is constant , even if it wrapped around a ( masless ) pulley . note that there is a string between the lowermost pulley and the block b as well , even if it is not shown in the diagram . all connections should be done via a string ( except connections with walls , they can be whatever ) get relations between the tensions . remember that net force on a masless pulley must be zero , and tension always acts away from the object . now , since net work done by internal forces must be zero , and tension is an internal force , then at every edge of the string ( where a pulley is not connected ) , we can calculate work by simply multiplying tension with the displacement ( giving it a negative sign if they are opposite in direction ) ; adding the works , and equating to zero . in your problem , if $t$ is the tension of the large string and $t_1$ is the tension in the small ( nonexistant ) string , then by equating forces about the lower pulley , we get $t+t+t=t_1$ . taking the same signs and symbols for displacement in the diagram , and applying virtual work , we get $t_1s_b+ts_c=0$ . one can see that this simplifies to the constraint relation . this looks long , but thats because i explained the steps . with practice , all you have to do is choose a string , give it tension $t$ , and write all other tensions in terms of that tension . then you mark the displacements and can " read off " the constraint relation no matter how complicated it is .
the classical version of this problem was solved by henri poincaré way back in 1896 . this is also problem 5.43 in electrodynamics by griffiths . the classical trajectories are geodesics on the surface of a cone . a recent treatment of the classical version of this problem is here . the quantum mechanical version was also solved long back by igor tamm in 1931 . this is discussed in section 2.3 of the book magnetic monopoles by y m shnir , who follows the treatment in charge quantization and nonintegrable lie algebras by hurst . the quantum mechanical version of the problem turns out to be separable in spherical polar coordinates . the angular part has the generalized spherical harmonics as its eigenvalues , while the radial solution is the same as the radial wave function of the standard schroedinger equation . the centrifugal potential in the schroedinger equation turns out to be always repulsive which implies that there are no bound states for this system of an electron in a magnetic monopole field . however a dyon field does have bound state solutions .
we simply do not know for sure what factors are important to the generation of a magnetic field . mars and venus do not have planetary magnetic fields , and while mars is small enough for it is core to be frozen we would expect the core of venus to still be at least partially molten like the earth 's . a theory i have heard suggested is that the earth 's magnetic field is driven by convection currents . the earth 's core is currently solidifying , and latent heat of fusion released at the solid/liquid boundary will drive convection currents . the argument about venus is that either the core has no solid portion , or the cooling is too slow for convection currents to form . the venusian core could have no solid portion if it is composition is different to earth 's . wikipedia claims this could be due to different concentrations of sulphur . even if it does have a solid portion , venus appears to have no plate tectonics and this could mean heat transport from the core is so slow that its core is at a nearly uniform temperature , which would mean no convection currents . i have never heard it suggested that the presence of the moon has any effect on the earth 's magnetic field .
consider the environment in which the propellent burns in a firearm . it is cramped space formerly packed tightly with stuff ( the propellent , any necessary wadding and the bullet itself ) . there is damn little room for any atmosphere at all . where---especially in a cartridge system---do you think the oxidizer ( nb : not necessarily oxygen ! ) is coming from anyway ? most explosives do not run on atmospheric oxygen , then run on the oxidizer built in to the formulation . the only exceptions that i know of are fuel--air explosives and those are a specialized business . you should expect cartridge firearms to work perfectly in space unless their parts vacuum weld . i would be a little concerned about open-pan loose-power systems ( do they initially burn environmental $\mathrm{o}_2$ ? , and in microgravity will they blow the power away before they initiate burning down the hole ? ) , but i would still take even odds that they work .
for the photons that make up light to exist they have to be travelling at the speed of light . this means that to store them you have to put them in a container where they can move around at the speed of light until you want to let them out . you could build the container out of mirrors , but no mirror we can build is 100% reflective , or indeed can be 100% reflective . usually when a photon " hits " the mirror it is absorbed by one of the atoms in the mirror and then re-emitted back out into the container . however , occasionally the photon either will not get re-emitted ( leaving the atom in an excited state ) or it does not hit one of the atoms and makes it way through the mirror and out of the container . while the chances of this happening for an individual photon are low , there are lots of photons travelling very fast so it happens many times thus causing the light to " leak " or decay . building a near perfect mirror is hard , so it is easier to convert the light into something that can be stored and then convert that back into light when need it .
no , because even though the force that you exert on the earth is equal and opposite to the force it exerts back on you , you are not doing the same amount of work on the earth as the earth on you . your kinetic energy increases due to the work done by the earth on you . remember that $w = f \cdot d$ ; your bicycle moves a lot due to this force , but the earth does not really move much at all . another way to think about this is in terms of kinetic energy . $\mathrm{ke} = \frac{1}{2} mv^2$ , so if your velocity is high , so is your kinetic energy . the earth 's velocity is low , and so is its kinetic energy . so the forces are equal and opposite , and the impulse , or change in momentum , is too , but the kinetic energy stays mostly with you .
you will see it the same , regardless of the refraction index of your medium . the reason is as simple as that , when the light hits your retina , it will be travelling through the interior of your eye , so the only refractive index that matters is that of the eye . what is what we actually detect , wavelength of frequency ? frequency is the one related to energy , so my feeling is that that should be the one influencing chemical reactions , that is , at the end , the way cones can detect light . indeed , the vitreous humour ( the interior filling of the eyeballs ) looses water with age , to the point of getting deatached from the retina , something very common among old people ( wikipedia says 75% of > 65 ) . the main consequences are visual artifacts , but no one has claimed colours suddenly look different . physics books quote wavelengths because those are usually what one measures in the lab in the optical range . plus , the numerical values are ( and this is subjective ) more convenient .
basically you have a train with an observer a inside who emits a beam of light to the left which is reflected off a wall . . . let 's call that wall w , for reference below . . . at distance $d$ from a . . . . let 's call that distance $d := aw$ ( anticipating that distances between certain other participants will have to be considered , and distinctly named , below ) . the time it takes for the beam to get back to the observer is $t_0 = \frac{2 \ , d}{c}$ which is the proper time . . . . a.k.a. " ping duration " $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ . so far , so good . now consider an observer b outside the train . the train is moving at a velocity $v$ to the right relative to b . ( also : b as well as everyone at rest wrt . b are moving at velocity $v$ relative to a and w ; from a towards w . ) thus the time it takes for the light to hit the wall is $\frac{d}{c + v}$ that is eventually incorrect . let 's try to be more precise : b and a were passing each other ( which is supposed to be visible by everyone else ; a.k.a. " emitting a light signal " ) , there exists some participant ( let 's call it p ) who was and remained at rest wrt . b , who therefore of course was passed by w " sometime " , and specificly : whose indication of being passed by w was simultaneous to b 's indication of being passed by a , and there exists some participant ( let 's call it q ) who was and remained at rest wrt . b ( as well as p ) and who was passed w just as q and w observed ( together , at their meeting ) that b and a had passed each other . therefore $\frac{pq}{bq} = \frac{v}{c}$ , and $\frac{bp}{bq} = \frac{pq}{bq} + 1 = 1 + \frac{v}{c} = \frac{c + v}{c}$ . of interest is then b 's duration from the indication of being passed by a until the indication simultaneous to q 's indication of being passed by w ( and observing that b and a had passed each other ) . this is of course half of the ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_b [ q ] $ , i.e. half of $\frac{2 \ , bq}{c}$ ; thus $\frac{bq}{c}$ which is in turn equal to $\frac{bp}{c + v}$ . and the time it take for it to return to a is $\frac{d}{c - v}$ . arguing similarly to the above , this corrsponding duration of b is more precisely equal to $\frac{bp}{c - v}$ . now , in order to compare a 's ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ with the sum of the corresponding durations of b , i.e. with $\frac{bp}{c + v} + \frac{bp}{c - v} = \frac{2 \ , c \ , bp}{c^2 - v^2} = \frac{2 \ , bp}{c} \frac{1}{1 - \beta^2}$ we still need to establish value of the distance ratio $\frac{aw}{bp}$ . that means we are now left with having to derive " length contraction " ! but that is not difficult , given all of the explicit setup and named participants that were introduced above already : we should consider one more participant , j , who was and remained at rest wrt . a and w , and whose indication of being passed by b was simultaneous to w 's indication of being passed by q ( and observing that b and a had passed each other ) . therefore $\frac{aj}{aw} = \frac{v}{c}$ , and $\frac{jw}{aw} = 1 - \frac{aj}{aw} = 1 - \frac{v}{c} = \frac{c - v}{c}$ . considering the two explicit requirements of simultaneity above , the corresponding ratios of distances should be equal : $\frac{bp}{aw} = \frac{jw}{bq}$ . inserting expressions from above : $\frac{bp}{aw} = \frac{jw}{aw} \frac{aw}{bp} \frac{bp}{bq} = \frac{c - v}{c} \frac{aw}{bp} \frac{c + v}{c} = \frac{aw}{bp} \frac{c^2 - v^2}{c^2} = \frac{aw}{bp} ( 1 - \beta^2 ) = \sqrt{ 1 - \beta^2 }$ . consequently : $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c} = \frac{2 \ , bp}{c} / \sqrt{ 1 - \beta^2 }$ . calling b 's corresponding duration $\frac{2 \ , bp}{c} \frac{1}{1 - \beta^2} = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w}$ therefore $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w} \sqrt{ 1 - \beta^2 }$ , as may have been expected . last week in class we derived the formula for time dilation using light clocks well , should not that have been pretty much the derivation i just sketched ? . edit for completeness , and to emphasize a particular point in the following , here 's also the derivitation involving light clocks " perpendicular to the direction of motion " ( which seems to have been mentioned in passing in the op 's question ) : expanding on the setup described above , with the principal protagonists a and b and suitable auxiliary participants ( w and j at rest wrt . a ; p and q at rest wrt . b ) , and all of them " sitting or moving in one line " , we now also consider participant f ( at rest wrt . a , j , w ) with distance ratios $\left ( \frac{af}{fj} \right ) ^2 + \left ( \frac{aj}{fj} \right ) ^2 = 1$ , and ( without loss of generality , but just to re-use setup relations from above ) with $\frac{aw}{fj} = 1$ , therefore $\frac{af}{fj} = \sqrt{ 1 - \left ( \frac{aj}{fj} \right ) ^2 } = \sqrt{ 1 - \left ( \frac{aj}{aw} \right ) ^2 } = \sqrt{ 1 - \beta^2 }$ ; and participant g ( at rest wrt . b , p , q ) with distance ratios $\left ( \frac{bg}{gp} \right ) ^2 + \left ( \frac{bp}{gp} \right ) ^2 = 1$ , and such that g and f met each other in passing . importantly , the entire region containing the setup is of course supposed to be flat . therefore it can be demonstrated ( what otherwise may be glanced over for seeming " too obvious to even point out" ) , that f 's indication of having been passed by g was simultaneous to a 's indication of having been passed by b ; and vice versa that g 's indication of having been passed by f was simultaneous to b 's indication of having been passed by a . then , by the same argument that was used above for comparison of distance ratios between pairs of participants who were not at rest to each other , we set : $\frac{af}{bg} = \frac{bg}{af}$ , and therefore $\frac{af}{bg} = 1 . $ with $\mathop{\delta , \tau_a}\limits_{\text{ping trip } b_g} = \frac{2 \ , fj}{c} = \frac{2 \ , af}{c} / \sqrt{ 1 - \beta^2 }$ and $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \frac{2 \ , bg}{c}$ follows $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \mathop{\delta \ , \tau_a}\limits_{\text{ping trip } b_g} \sqrt{ 1 - \beta^2 }$ . finally , as can be shown explicitly , it holds symmetrically that $\mathop{\delta}\limits_{\text{ping}} \tau_a [ f ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_f} \sqrt{ 1 - \beta^2 }$ .
i would like to share my thoughts and questions on the issue . the boltzmann h theorem based on classical mechanics is well discussed in various literatures , the irreversibility comes from his assumption of molecular chaos , which cannot be justified from the underlying dynamical equation . here i will try to say something on quantum h theorem , the point i want to make is that , although seemingly h theorem can be derived from unitarity , the true entropy increase in fact comes from the non-unitary part of quantum mechanics . let me first recap the derivation using unitarity $^{1,2}$ . h theorem as a consequence of unitarity denote by $p_k$ the probability of a particle appearing on the state $|k\rangle$ , $a_{kl}$ the transition rate from state $|k\rangle$ to state $|l\rangle$ , then by the master equation $${\frac {dp_{k}}{dt}}=\sum _{l} ( a_{{kl }}p_{l }-a_{{l k}}p_{k} ) =\sum _{{l\neq k}} ( a_{{kl }}p_{l }-a_{{l k}}p_{k} ) \cdots\cdots ( 1 ) . $$ then we take the derivative of entropy $$s=-\sum_k p_k\ln p_k\cdots\cdots ( 2 ) , $$ we obtain $$\frac{ds}{dt}=-\sum_k\frac{dp_k}{dt}\left ( 1+\ln p_k\right ) \cdots\cdots ( 3 ) . $$ together with ( 1 ) we have $$\frac{ds}{dt}=-\sum_{kl}\left\{ ( 1+\ln p_k ) a_{{kl }}p_{l }- ( 1+\ln p_k ) a_{{l k}}p_{k}\right\}\cdots ( 4 ) . $$ for the seond second term let us interchange the dummy indices $k$ and $l$ , we get $$\frac{ds}{dt}=\sum_{kl} ( \ln p_l-\ln p_k ) a_{kl}p_l\cdots\cdots ( 5 ) $$ now use the mathematical identity $ ( \ln p_l-\ln p_k ) p_l\geq p_l- p_k$ , we obtain $$\frac{ds}{dt}\geq \sum_{kl} ( p_l-p_k ) a_{kl}= \sum_{kl}p_l ( a_{kl}-a_{lk} ) \\=\sum_{l}p_l\big\{\sum_{k} ( a_{kl}-a_{lk} ) \big\}\cdots\cdots ( 6 ) $$ now unitarity ensures $\sum_{k}a_{kl}$ and $\sum_{k}a_{lk}$ are both 0 , because as transition rates , $$\sum_{k}a_{kl}=\frac{d}{dt}\sum_{k}|\langle l|s|k\rangle|^2=\frac{d}{dt}\sum_{k}\langle l|s|k\rangle\langle k|s^{\dagger}|l\rangle\\=\frac{d}{dt}\langle l|ss^{\dagger}|l\rangle=\frac{d}{dt}\langle l|l\rangle=0\cdots\cdots ( 7 ) , $$ where $s$ is the unitary time evolution operator describing the system . this is nothing but saying the total transition probability from one state to all states must be 1 . it is clear ( 6 ) and ( 7 ) imply the h theorem : $$\frac{ds}{dt}\geq 0 . $$ where does the irreversibility come from ? now we are in a position to question ourselves with loschmidt 's paradox , analogously to its classical version : there are many unitary and time-reversible quantum mechanical systems , if we have just derived h theorem using unitarity alone , how can it be reconciled with time-reversibility of the underlying dynamics ? what sneaked into the above derivation ? the crucial thing to notice is that , in the quantum regime , the definition of entropy using equation ( 2 ) is inherently an impossible one : the value of the entropy in ( 2 ) depends on the basis we choose to describe the system ! consider a two-level system with two choices of orthogonal basis $\{|1\rangle , |2\rangle\}$ and $\{|a\rangle , |b\rangle\}$ related by $$|1\rangle=\frac{1}{\sqrt2} ( |a\rangle+|b\rangle ) , \\|2\rangle=\frac{1}{\sqrt2} ( |a\rangle-|b\rangle ) . $$ suppose the system is in the state $|1\rangle$ , then the entropy formula gives $s=0$ in the first choice of basis since it has 100% chance to appear in $|1\rangle$ , while in the other basis $s=\ln2$ since it has 50%-50% chance to appear in either $|a\rangle$ or $|b\rangle$ . now we may argue , it is one thing that to say the system is in $\frac{1}{\sqrt2} ( |a\rangle+|b\rangle ) $ and have the potential 50%-50% chance to transit into $|a\rangle$ and $|b\rangle$ after a measurement , but a different thing to say the transition has been realized by some measurement . two situations must be described differently . if we look back to our derivation , it is not hard to see what we really did was , after a basis state evolves to a new state which is a superposition of the basis states , we assumed transitions to original basis states have happened instead of just staying in that superposition state , and in fact the original definition of entropy is not capable of describing such situation , as explained just now . a plausible definition of quantum entropy is the von neumann entropy , which is a basis-independent definition of entropy , and in this description , the entropy of a unitarily evolving system is constant in time , while a ( projective ) measurement can increase the entropy . based on the above comparison , we see the irreversibility really comes as an assumption , the assumption that a measurement/decoherence has happened , and as we know , a ( projective ) measurement is a non-unitary , irreversible process , no paradox anymore . my own question on the issue is , what to make of the fact that von neumann entropy is constant in time ? does it mean it is incapable of describing a closed system evolving from non-equilibrium to equilibrium , or should we just reverse the argument and say any non-equilibrium to equilibrium evolution must be described by some non-unitary process ? 1 . rephrased from section 3.6 of the quantum theory of fields , vol1 , s . weinberg 2 . if i remember correctly ( which i am not quite confident on ) , such derivation was first given by pauli , and he correctly spotted the origin of irreversibility , which he called the " random phase assumption " .
$pdv$ is boundary work . $vdp$ is isentropic shaft work in pumps ( as you have identified above ) , gas turbines , etc . now you must realize that even in a pump or turbine the mechanism of work is still $pdv$ , i.e. , the gas pushing on the blade out of its way . but , then there is work required to maintain the flow in and out of the device/control volume , which requires flow work $pv$ so the net reversible work from a steady-flow device turns out to be shaft $vdp$ . why flow work $pv$ ? to push a packet of fluid with volume $v$ forward into a device you have to do work against the pressure of the fluid already in the device , i.e. , overcome the back force of that fluid . this implies the work you do in pushing your new packet of length $l$ and cross-section area $a$ into the device is : \begin{align*} \int fdx = \int_{0}^{l}padx = pv \end{align*} it must be noted that in a steady-flow device ( unlike in a piston ) the back pressure $p$ is constant . now consider the device ( e . g . , turbine to be a control volume ) . the energy of the fluid going in is its internal energy and the work invested into the fluid to enter the device : $u_{entry}+p_{entry}v_{entry}=h_{entry}$ . similarly for exit from the device . the net change across the device is $\delta h$ . for a differential device ( or across a small change ) this is $dh$ . the work output from the shaft of then device is the $\delta w= dh$ . now if the device is isentropic , i.e. , adiabatic-reversible . the gibbs equation provides : \begin{align*} and dh=tds+vdp=vdp\\ and \delta w =dh=vdp \qquad ( \text{isentropic}\ ; ds=0 ) \end{align*} therefore $vdp$ is isentropic shaft work from a flowing device . important points : 1 ) both internal energy and enthalpy are state variables , therefore can be measured for a system static or flowing . this is why sometimes there is a tendency to use $u$ and $h$ incorrectly . the true purpose of $h$ is to capture the work required to push/maintain a flow against a back pressure , i.e. , it incorporates the $pv$ part . therefore when you write an energy balance with flows coming in and out , the energy crossing boundary is not just $u$ but $h$ and this distinction must be kept in mind . 2 ) $vdp$ is isentropic steady-flow shaft work . the isentropic is key here .
a supersymmetric supermultiplet - that is , a set of fields that are related by supersymmetry transformations - must contain a equal number of fermionic and bosonic degrees of freedom . a majorana fermion has two ( on-shell ) degrees of freedom . the majorana 's supersymmetric scalar partner must , therefore , be a complex scalar with two degrees of freedom . a complex scalar field can be neutral under a $u ( 1 ) $ symmetry . because the $u ( 1 ) $ transformation , $$ \phi\to\exp ( iq\theta ) \phi , $$ makes no sense if $\phi$ is a real field , all real scalar fields are necessarily neutral under a $u ( 1 ) $ symmetry ; however , not all scalar fields that are neutral under a $u ( 1 ) $ are necessarily real fields .
the simplest way to explain the christoffel symbol is to look at them in flat space . normally , the laplacian of a scalar in three flat dimensions is : $$\nabla^{a}\nabla_{a}\phi = \frac{\partial^{2}\phi}{\partial x^{2}}+\frac{\partial^{2}\phi}{\partial y^{2}}+\frac{\partial^{2}\phi}{\partial z^{2}}$$ but , that is not the case if i switch from the $ ( x , y , z ) $ coordinate system to cylindrical coordinates $ ( r , \theta , z ) $ . now , the laplacian becomes : $$\nabla^{a}\nabla_{a}\phi=\frac{\partial^{2}\phi}{\partial r^{2}}+\frac{1}{r^{2}}\left ( \frac{\partial^{2}\phi}{\partial \theta^{2}}\right ) +\frac{\partial^{2}\phi}{\partial z^{2}}-\frac{1}{r}\left ( \frac{\partial\phi}{\partial r}\right ) $$ the most important thing to note is the last term above--you now have not only second derivatives of $\phi$ , but you also now have a term involving a first derivative of $\phi$ . this is precisely what a christoffel symbol does . in general , the laplacian operator is : $$\nabla_{a}\nabla^{a}\phi = g^{ab}\partial_{a}\partial_{b}\phi - g^{ab}\gamma_{ab}{}^{c}\partial_{c}\phi$$ in the case of cylindrical coordinates , what the extra term does is encode the fact that the coordinate system is not homogenous into the derivative operator--surfaces at constant $r$ are much larger far from the origin than they are close to the origin . in the case of a curved space ( time ) , what the christoffel symbols do is explain the inhomogenities/curvature/whatever of the space ( time ) itself . as far as the curvature tensors--they are contractions of each other . the riemann tensor is simply an anticommutator of derivative operators--$r_{abc}{}^{d}\omega_{d} \equiv \nabla_{a}\nabla_{b}\omega_{c} - \nabla_{b}\nabla_{a} \omega_{c}$ . it measures how parallel translation of a vector/one-form differs if you go in direction 1 and then direction 2 or in the opposite order . the riemann tensor is an unwieldy thing to work with , however , having four indices . it turns out that it is antisymmetric on the first two and last two indices , however , so there is in fact only a single contraction ( contraction=multiply by the metric tensor and sum over all indices ) one can make on it , $g^{ab}r_{acbd}=r_{cd}$ , and this defines the ricci tensor . the ricci scalar is just a further contraction of this , $r=g^{ab}r_{ab}$ . now , due to special relativity , einstein already knew that matter had to be represented by a two-index tensor that combined the pressures , currents , and densities of the matter distribution . this matter distribution , if physically meaningful , should also satisfy a continuity equation : $\nabla_{a}t^{ab}=0$ , which basically says that matter is neither created nor destroyed in the distribution , and that the time rate of change in a current is the gradient of pressure . when einstein was writing his field equations down , he wanted some quantity created from the metric tensor that also satisfied this ( call it $g^{ab}$ ) to set equal to $t^{ab}$ . but this means that $\nabla_{a}g^{ab} =0$ . it turns out that there is only one such combination of terms involving first and second derivatives of the metric tensor : $r_{ab} - \frac{1}{2}rg_{ab} + \lambda g_{ab}$ , where $\lambda$ is an arbitrary constant . so , this is what einstein picked for his field equation . now , $r_{ab}$ has the same number of indicies as the stress-energy tensor . so , a hand-wavey way of looking at what $r_{ab}$ means is to say that it tells you the " part of the curvature " that derives from the presence of matter . where does this leave the remaining components of $r_{abc}{}^{d}$ on which $r_{ab}$ does not depend ? well , the simplest way ( not completely correct , but simplest ) is to call these the parts of the curvature derived from the dynamics of the gravitational field itself--an empty spacetime containing only gravitational radiation , for example , will satisfy $r_{ab}=0$ but will also have $r_{abc}{}^{d}\neq 0$ . same for a spacetime containing only a black hole . these extra components of $r_{abc}{}^{d}$ give you the information about the gravitational dynamics of the spacetime , independent of what matter the spacetime contains . this is getting long , so i will leave this at that .
since there are no specialists in depletion mass spectrometry , i will try to answer in a more general way . with depletion spectroscopy you look at a small variation of a large signal so what you need is not high sensitivity but signal stability and high dynamic range of the detector . i assume that you have a continuous stream of ions , otherwise pulse to pulse fluctuations of ion concentrations will be the major limiting factor . probably , you will not need superb mass resolution so you would want to buy a quadrupole mass spectrometer - these are cheap , compact and there are plenty of companies that make them . dynamic range depends on the ion detector and since you would be looking for a wide dynamic range and stability with time , the very best choice is a simple faraday cup . you can be sure that , whatever your experiment is , the sensitivity will be limited not by the detector but by fluctuations of your signal - most likely , by how stable your depleting factor is .
you can see accurate visualizations of ( simulated ) eddy currents in multiple papers . one example is : efficient solvers for nonlinear time-periodic eddy current problems
you get a rise in a capillary tube because it reduces the energy stored in the surface tension at the air-water and air-glass interface . the water rises until the reduction in the surface tension energy is balanced by the increase in the gravitational potential energy of the water . but it is not at all obvious how you could extract energy from this . if you evaporate water from the top of the tube then you will certainly pull up more water to replace the water lost by evaporation . i suppose this is analogous to a tree pulling up water , though my limited memory of biology i think the sap is driven up the tree by osmotic pressure in the roots as well as by capillary action . i suppose you could put a microturbine at the bottom of the capillary tube then heat the top and extract energy as the water rises up the tube to replace the water that is evaporated . however i doubt this would be as efficient as just using the same amount of heat in steam engine . were you wondering if there was a way to make the water rise up the tube , then fall back , then rise up again , generating energy with each cycle ? the only way you could do this was if there was some way to change the air-water or air-glass surface tension in some reversible way . you can easily reduce the air-water surface tension by adding surfactant , and this will make the water drop , but you had need to get the surfactant back out to make the water rise again .
you have mentioned a number of pretty intense examples of symmetry breaking , but if i am reading your question rightly , all you are really looking for is " what does symmetry breaking mean when translated to everyday ( classical ) physics ? " that is actually a pretty easy question if that really is your intent : symmetry breaking just means being forced to making a choice . for example , a pencil balanced on its flat eraser end is perfectly symmetric with respect to every possible orientation on the flat surface on which it rests . but if you tip the pencil over , that perfect symmetry is lost , and the pencil must " choose " a specific orientation into which to fall . once that fall has taken place , the pencil has lost all of its original beautiful symmetry with respect to the plane , and will not be able to regain it unless you can " heat it up " ( energy was lost during the fall ) and return it to its original upright position . any form of crystallization is an other example . water is statistically isotropic in three dimensions in its liquid form , but as soon as ice begins to form , the molecules must give up their carefree ways and " choose " some very specific orientation . that too is a symmetry break , and if you think about it , it is not that different from the pencil example . one of my personal favorites is topological , and involves changing the number of available dimensions of an embedding space . imagine molding some clay in into a smooth , symmetric band . the two edges of the band are fully symmetric in 3d in the sense that they can always be rotated to replace each other . now paint one edge red and the other edge blue . next , transform the band into a 2d space ( reduce its embedding space0 by flattening it onto a table surface , trying as best you can to preserve its internal connectivity in the new version . you will find that a washer-like form is the best you can do , and that means you must make a choice : red edge on the inside , or blue edge on the inside ? the fully symmetric 3d form of the band thus breaks down into two non-exchangeable forms when the dimensionality of its embedding space is reduced to two . notice that while the pencil and ice both have an infinite number of choices when their symmetries are broken , in this case only two choices are available . that kind of twofold symmetry breaking is akin to the one between matter and antimatter . that symmetry can similarly be interpreted as the result of " locking down " the time vector of mass-energy in a 4d space , so that in 3d the local time vector must point in either in the same or the opposite direction as classical time . no matter how exotic the topic sounds in advanced physics , one way or another the same sort of " make a choice " process . making the choice lowers the energy of the system , but also destroys the lovely symmetry of the higher energy version . this in a nutshell is also why particle physics has been firmly devoted for many decades to building larger and larger particle accelerators . the higher energies they provide make it possible to search for those lost symmetries found at higher energies .
can any solid material with a low heat capacity exist that feels closer to human body temperature than another solid material with a higher heat capacity ; where both materials were previously kept in either a mundane oven or freezer for a sustained period ? let me rephrase to : is there any solid which disobeys the inverse proportionality of thermal conductivity and specific heat capacity ? consider $1000kg$ of wood and $1000kg$ of aluminium , both at $320k$ ( very warm ) . at the instant you place a finger on such large thermal masses , your perception of temperature comparison is dependent on heat conductivity of the materials , not their heat capacity ( their masses are so large compared to your finger , their temperature is almost constant depsite losing heat to your finger ) . using such large masses and ( equal masses for that matter ) is necessary since otherwise i can instantly answer yes to your question by giving you 100g of wood and 1g of gold ( beaten to the same surface area of the wood ) just taken from the freezer and you would perceive gold being closer to body temperature than the wood after a second . so lets define the question by specific heat capacity , and instantaneous perception of heat transfer . to answer it though , there is in fact no metal which disobeys this relation due to the electron sea being the majority carrier of kinetic energy in the bulk metal . their having large mean free paths and low masses allow them to attain very high velocities ( which is a property of high temperature ) and therefore are able to transfer energy quickly in the bulk material . in other words , if metals used anything heavier to transmit heat , like their nuclei , it would not only take much more heat to accelerate them to the same velocities the electrons could attain ( resulting in higher heat capacity ) , but the rate at which that kinetic energy is transmitted across the material is accordingly slower ( lower thermal conductivity ) . in fact the lattice of metal nuclei do in fact contribute to both properties via phonons not translational kinetic energy like in gases , but phonons are still greatly superseded by the effect from electrons . therefore the inverse relation between thermal conductivity and heat capacity is valid for metals . what you are looking for is a non conductor with both higher heat capacity and thermal conductivity than a conductor . for that i give you diamond ( figuratively . . . i can not afford one ) , which has a specific heat capacity of $0.5 j/gk$ , higher than that of any metal denser than vanadium ( which is almost all of them ) , but has a thermal conductivity of $&gt ; 900w/mk$ , trumping silver 's $421w/mk$ which is tops for all pure metals . indeed , $1kg$ of silver would feel much closer to body temperature than $1kg$ of diamond ( that is alot of diamond ! ) despite diamond having a higher heat capacity .
long distance quantum communication in earth 's atmosphere is possible . there are successful realizations of quantum key distribution and quantum teleportation , showing that it is possible to detect single photons and entangled pairs having traveled distances up to 300 km . of course , the losses are huge and unavoidable , so the total attenuation for a link between two canary islands in the mentioned experiments was around 30 db for single photons and 70 db for pairs . it means that the source of entangled photons has to be quite bright : the one used in canary experiments produced $~10^6$ pairs/s . and only about 0.07 pairs/s were detected at the receiving side . nevertheless , the noise , mostly coming from stray light and detector dark counts , can be made low enough ( ~200 hz ) to reach coincidence signal-to-noise ratio of 15:1 . this is achieved by frequency filtering and timing - the coincidence window was limited to 3 ns , and that required corrections for time drift of gps-based clock synchronization system . active synchronization was done utilizing temporal correlations of entangled photons : the local time was adjusted to maximize the coincident events rate . authors call it " entanglement-assisted clock synchronization " . there are several sources of losses in free space atmospheric links : scattering and absorption of ~0.07 db/km , diffraction losses due to limited aperture on the receiving side which is unavoidable , as well as beam wandering and distortion due to atmospheric turbulence . beam wandering may be to some extent fixed by active tracking systems . as for specific " shaping " of photons , there are some speculations on using the beams with orbital angular momentum to reduce the effects of turbulence , but these do not seem to be very realistic , and some authors tend to claim that to the contrary , higher-order laguerre-gaussian beams are more sensitive to turbulent distortions . satellite quantum communication is a feasible task , we will probably see it in the nearest future . in some aspects it is even simpler , since the path travelled in the dense layer of atmosphere is much shorter . inter-planet optical communication at the single-photon level would have to deal with enormous losses just due to diffraction spreading of the beam , it does not seem to be possible with current technology .
clouds do not absorb light ( much ) , they reflect and refract it , and this applies to uv light in the same way as visible light . so a 100% cloudy sky will block uv light in the same way it blocks visible light . a quick google suggests that heavy cloud cover will remove 80-90% of uv light . anyone disputing this should try sunbathing on a cloudy day :- ) i have heard occasional claims that broken cloud cover can actually enhance uv levels in the unshaded areas by reflecting uv light into the breaks . however i have never come across any studies that prove this happens . while it seems vaguely plausible i would be surprised if the effect was very big .
some more misconceptions : the chemguide website quoted above might be a useful reference for " uk-based exam purposes " as stated there , but it certainly does not help in solving the question . the arguments given above that followed the comments on chemguide are inaccurate . a simple quantum chemistry calculation of gold in its ground state will give you that the electron in the s orbital ( a1g ) is the most energetic in this atom . hence , ionization will most easily be accomplished by removal of this electron , and not of d electrons , and this is easily proved by another computation for ionized gold , which will show you that the 5d orbitals will remain filled while the 6s orbital is no longer occupied . actually , it is known that if the most external d shell is filled , the energies of these orbitals will be effectively lowered , and there is a very high probability that the ionized electron will not come from it , but from more energetic s or p orbitals . ( i have just done a few of these calculations in order to make sure this point is right ) in order to analyze why some metals are more inert than others , various effects come into play . relativistic effects , such as the contraction of s orbitals , for example , are a major factor in making gold less reactive than silver , and in lowering the oxidation potential of gold . so , in addition to looking at chemical potentials when discussing the inertness of metals in different environments , it is better not to reduce the arguments to simple electron configuration trends which usually work quite well for main group elements , since , though they might generate insights for the understanding of the behavior of metals , these insights might be either right or wrong .
here 's what happens when you apply $h$ to $|\mathbf k\rangle = a^\dagger_\mathbf k|0\rangle$ to find the energy of a single particle state : \begin{align} h|\mathbf k'\rangle and = \int \frac{d^3\mathbf l}{2 ( 2\pi ) ^3}a^\dagger_\mathbf la_\mathbf la^\dagger_\mathbf k|0\rangle \\ and = \int \frac{d^3\mathbf l}{2 ( 2\pi ) ^3}a^\dagger_\mathbf l ( [ a_\mathbf l , a^\dagger_\mathbf k ] -a^\dagger_\mathbf ka_\mathbf l ) |0\rangle \\ and = \int \frac{d^3\mathbf l}{2 ( 2\pi ) ^3}a^\dagger_\mathbf l ( ( 2\pi ) ^3 2\omega_\mathbf k\delta^{ ( 3 ) } ( \mathbf l - \mathbf k ) -a^\dagger_\mathbf ka_\mathbf l ) |0\rangle \\ and = \omega_\mathbf k a^\dagger_\mathbf k|0\rangle \\ and = \omega_\mathbf k|\mathbf k\rangle \end{align} the computation for a multi-particle state is simiar .
if you want to switch from $x , y$ to $z , z^*$ , you need to invert your relationships because the opposite direction is needed . the inverse maps are $$ x = \frac{z+z^*}2 \quad y = \frac{z-z^*}{2i}$$ and $$\frac{\partial}{\partial x}\equiv\partial_x = \frac{\partial }{\partial z} + \frac{\partial }{ \partial z^*} , \quad \frac{\partial }{ \partial y}\equiv \partial_y = i ( \frac{\partial }{ \partial_z} -\frac{ \partial }{ \partial z^*} ) $$ note that one has to use the partial derivatives because we are talking about functions of several variables . when you substitute the identities above into the definition of the angular momentum ( note that it is $\hbar$ , hbar , and not $h$ over there ) , we get $$ l_z = -i\hbar ( x \partial_y - y\partial _x ) =-\frac{i\hbar}{2} ( 2iz \partial_z-2iz^*\partial_{z^*} ) = \hbar ( z\partial_z - z^*\partial_{z^*} ) $$ the factors of $i$ and $2$ cancel , much like the terms $z\partial_{z^*}$ and its complex conjugate $z^*\partial_z$ . the latter have to cancel because $l_z$ is symmetric under rotations around the $z$-axis so the $l_z$ $u ( 1 ) $ charges have to cancel in all terms . the " charge " of $z$ exactly cancels with the opposite charge of $\partial / \partial z$ ( and similarly for the complex conjugate term ) but the mixed terms would not . also note that there is no prefactor $i$ in the formula for $l_z$ in terms of $z , z^*$ and their derivatives . that is ok because the $l_z=m$ wave functions behave as $z^m$ or $z^{*-m}$ or some compromise and one may pick an $m$ factor ( eigenvalue ) by differentiating $z^m$ with respect to $z$ , getting $mz^{m-1}$ , and multiplying by $z$ again to get $mz^m$ . the $i$ is hidden in $z=x+iy$ , the relative phase of $x , y$ .
as you noted , the ising model has spins that are $\pm 1$ whereas in a full quantum model such as the heisenberg model , the spins are represented by pauli matrices . this means that they are not interchangeable . the biggest difference is that at zero temperature , there are no spin fluctuations in an ising model , whereas there are fluctuations in the heisenberg model . the ising hamiltonian can be written as $$h = j\sum_{\langle i , j\rangle} \sigma_i \sigma_j$$ and all the $\sigma_i \in \{-1,1\}$ . whereas for quantum spins , we had have $$h = j\sum_{\langle i , j\rangle} \vec{s_i} \cdot \vec{s_j}$$ and the $\vec{s_i}$ are vectors with elements determined by the pauli matrices . this product can then be expanded as $$\vec{s_i} \cdot \vec{s_j} = s_i^z s_j^z + \frac{1}{2}\left ( s_i^+ s_j^- + s_i^- s_j^+\right ) $$ where $s_i^+$ and $s_i^-$ are the spin raising and lowering operators . thus , we get one term that looks just like the ising term , because $s_i^z$ can be either $-1$ or $1$ , but we also get terms that describe how the two spins can flip : they start with opposite spin and then both of them flip . for a ferromagnet in the ground state , this is not important , because there the spins are all in parallel and thus the flip terms give zero contribution , but in an antiferromagnet , it makes the ground state highly complicated , whereas in an ising model the ground state would just have neel order .
in the limit of very , very long time you can expect that situation to obtain , but the earth formed very violently and therefore started with a very high mean temperature . the earth is full of long lived radioactive materials ( u-238 , th-232 and k-40 ) whose decay introduces a steady heat flux in excess of 20 terawatts in this epoch . this heat input delays the cooling of the deep earth from it is fossil high temperature .
[ rewrote the answer because i found out that my initial approximation was too crude . ] in the wkb approximation , the tunneling probability is $\exp [ -\int_a^b dx \sqrt{ ( 2m/\hbar^2 ) ( v-e ) } ] $ , where the integral is over the classically forbidden region from $a\sim10^{-15}$ m to $b\sim 10^{-10}$ m . the first obvious thing to try is approximating the integrand as a constant , the width of the classically forbidden region as $b$ , $m$ as the mass of a proton , and $v-e$ as the nuclear energy scale of ~1 mev . this gives a tunneling probability of $\exp ( -10^4 ) $ . but this is a little too crude , because v is strongly peaked within a small range of $x$ . one way to tell that this is not a good enough estimate is that the answer did not depend on $e$ because $e\ll v$ , whereas in reality the sun really does shine , because the particles in the sun 's core have enough energy . it turns out that the wkb integral can be evaluated for a coulomb barrier , and the result , known as the gamow factor , is $\exp [ -\pi\sqrt{2m/\hbar^2} ( v_{max}a/\sqrt{e} ) ] $ , where $v_{max}$ is the maximum height of the barrier . in our case this comes out to be about $e^{-1000}$ , which although a lot bigger than $\exp ( -10^4 ) $ , is still ridiculously small . note that this result does depend strongly on $e$ , even for $e\ll v$ . in a tritium hydride molecule you do get the benefit of repeated barrier assaults at a high frequency . the frequency of barrier assaults might be $10^{14}$ hz or something ( i.e. . , molecular vibrations have frequencies that lie roughly in the infrared range ) . but clearly when you multiply the two factors , you get something that will never have happened in the observable universe in the time since the big bang . essentially the same estimate explains why cold fusion does not work .
one does not have to set $g=1$ or $8\pi g=1$ in a quantum theory of gravity ; it is just one possible choice that may be convenient . if one wants to study relevant and irrelevant operators in general relativity and its extensions , it is useful not to set $g=1$ or $8\pi g=1$ because by doing so , we would make all quantities dimensionless . instead , it is a good idea to reformulate general relativity in the same way as other quantum field theories . quantum field theories with a weakly coupled classical limit are usually described by lagrangians $$ {\mathcal l} = {\mathcal l}_\text{free} + {\mathcal l}_\text{interactions} $$ the fields are normalized and redefined so that the kinetic terms ( those with 2 derivatives in the case of bosons , 1 derivative in the case of fermions ) have the usual normalization , schematically $ ( \partial_\mu \phi ) ^2/2$ for bosonic fields and $\bar\psi \partial_\mu \gamma^\mu \psi$ for fermionic fields . this is possible for general relativity , too . note that its lagrangian is the integrand of the einstein-hilbert action $$ {\mathcal l}_{\rm gr} = \frac{1}{16\pi g} r $$ and it is proportionally to ricci scalar that schematically contains terms $g \partial^2 g$ , among others . we may expand the metric around a background , in the simplest case the flat background $$ g_{\mu\nu} = \eta_{\mu\nu} + \sqrt{8\pi g} \cdot h_{\mu\nu} $$ the $\eta$ tensor is the flat minkowski metric ; $h$ is the perturbation away from it which carries the " operator character " . i have conveniently added the coefficient in front of $h$ because when we insert it to the einstein-hilbert action , the leading term will generate $${\mathcal l} \sim ( \partial h ) ^2 $$ and the coefficients involving $g$ will cancel ; let me be sloppy about the numerical coefficients of order one . however , the nonlinear einstein-hilbert action will also produce terms that are of higher order in $h_{\mu\nu}$ but each $h$ will appear together with a factor of $\sqrt{g}$ , too . so the cubic and higher interaction vertices in general relativity are weighted by $\sqrt{g}$ and its higher powers . because $g$ has a positive dimension of length for $d\gt 2$ , the interactions in general relativity are irrelevant ( non-renormalizable ) . that is true even for $d=3$ . however , gravity in $d=3$ has no local excitations , it is kind of vacuous , so the non-renormalizability problem may be , to some extent , circumvented . at any rate , for $d=4$ and higher , even the leading interaction is irrelevant and leads to non-renormalizable divergences already at 2-loop level ( and even in ${\mathcal n}=8$ supergravity , which offers as many supersymmetric cancellations as possible , there are new divergences requiring counterterms at the 7-loop level ) . so the theory breaks down at a cutoff scale that is not too far from the planck scale , $m_{\rm pl} = g^{1/ ( d-2 ) }$ . that is where a consistent theory of quantum gravity i.e. string/m-theory has to cure the problems by providing the theory with new states ( strings , branes , and – more universally – black hole microstates ) and new constraints . only if we try to study distances shorter than the planck distance ( a sort of a meaningless exercise for many reasons ) or energy scales higher than the planck scale ( where the typical " particles " really look like ever larger black holes ) , we find out that the rg flows break down and become meaningless as a methodology . however , at distances much longer than the planck length , the rg flows for gr are just fine and behave as they do in any non-renormalizable theory . the naively quantized einstein 's theory is not predictive at the planck scale but at much lower energies , one may systematically add new corrections , with a gradually increasing number of derivatives , to make an effective field theory ever more accurate . some people , most famously steven weinberg , have speculated that there could be a " zero-distance " ultraviolet limit that could run to general relativity at long distances . i think that the research attempting to find evidence for this conjecture remains inconclusive , to say the least . most folks in quantum gravity are actually convinced that this can not work not only because of the apparent absence of the required scale-invariant field theory describe the uv ; but also because such a picture of gravity would contradict holography and black hole thermodynamics .
i thought i might just start with an introduction first . : ) the basic principal behind free electron laser is that of synchrotron radiation . when electrons or charged particles are made to change momentum ( like being bent in a in an arc where the force is radially inwards ) they emit electromagnetic radiation . if the particles are relativistic then the electromagnetic radiation the lab observer relative to the electron will observe the electromagnetic radiation being emitted in a cone in the direction of motion . ( i will post figures if people really want ! ) in the case of the free electron laser you need to have magnetic arrays which are simply dipole magnets aligned such that the electrons will see as its travelling in a straight line , alternating vertical magnetic fields . this causes the electrons to " undulate " horizontally . with relativistic electrons ( which is not hard to do ) you will emit synchrotron radiation like a pencil beam . the wavelength from an undulator is given by lambda = undulator_period/ ( 2gamma^2 ) * ( 1+a^2/2 ) , where gamma is the relativistic gamma factor , a = e*b*undulator_period/ ( 2*pi*electronmass*speedlight ) where b is the strength of the magnetic field and e the electric charge . but the radiation from all the electrons are not very temporally coherent ! now as to why its called a laser ! the fundamental feature of why lasers are powerful is because the waves emitted by all sources are emitted in a temporally coherent fashion . i.e. all the electric fields add constructively so the total power scales like pn^2 where p is the power emitted by a single source and n is the number of sources . in a fel the undulation is small enough that the electrons are irradiated by its own synchrotron radiation and is therefore travelling in an optical field as well as the magnetic field from the dipole magnets . the optical field causes the electrons to micro-bunch and cause further amplification of that particular wavelength ( micro-bunching effect ) and is referred to as self-amplified spontaneous emission ( sase ) . this positive feedback generates powers that give an additional gain of n . typical bunches of electrons are 10^11 electrons , so you can get orders of magnitude more power . i presume you can reduce the size of the magnetic arrays to just a stretch of material with carefully oriented domains however it will be difficult to reach the fields required . to measure thats the field of spectroscopy which i am less familiar . for uv you will have to have some material with dispersion properties like a prism of sorts and a detector that will respond to uv radiation . for x-rays you can use monochromators that use the bragg principal and determine the energy ( calibrated to know emission lines from elements ) . the energy loss along the undulator is a small factor . the emitted photons are very small fractions of its total energy . for example for 10 to 100 nm wavelengths you would need something like 300 mev electrons ( rest mass of an electron is 0.5 mev ) so beta = 0.9999986 , and 10 nm photons are only 10s of ev . i thought the military were focusing on uv fels like you said , to reduce water absorption . xrays are much harder to generate with enough power to cause damage . that was more than i initially intended to say . hope its useful .
the actual equivalent to a newtonian " slice of time " in gr is a " space-like hypersurface " . it is a 3 dimensional hypersurface in 4d space-time ( hence the hypersurface ) . it is " space-like " because any two points are connected by a " space-like " path in the metric . this means that they are independent from a causality perspective , and can form the basis of forward predictions . if this is a global hypersurface ( ie doesnt just stop somewhere , or miss out regions ) then it is called a " cauchy surface " . as remarked elsewhere there is more choice in gr involved . the simplest way to see this is to note that newtonian theory is $e^4$ with ( x , y , z , t ) and once t= const is chosen at the space origin , say , the entire hypersurface is now determined . in say minkowski space ( the nearest gr equivalent to newtonian space ) there is a choice of timelike vector to be made first . there are 4 degrees of freedom here , but there is the timelikeness restriction and the restriction on time direction . ( physically this is a choice of inertial frame . ) this allows a foliation to form . to obtain an actual hypersurface ( hyperplane in this case ) we can choose one point p on that hypersurface ( ie a specific time in the frame ) . now the timelike vector is a normal to that hyperplane and we can determine whether an arbitrary point ( x , y , z , t ) is on that hyperplane . there will be 3 degrees of freedom as expected . gr generalises the minkowski example in that the hypersurface might be curved and not flat , requiring an infinite number of independent points to identify it . one further generalisation is that in relativity we have a metric which allows null distances between different points . thus we can have a " null hyperplane " and " null hypersurface " too , which are quasi-generalisations of the timelike slice newtonian idea .
the claim is consistent with page 37 of this and page 11 of this . the $\beta$-function of $u ( 1 ) $ depends on the charge of the degrees of freedom , but since there are none below the scale $a$ , it is zero . as for hep-th/9707133 , i suppose that the coupling there is given for non-vanishing charges .
the analysis of the availability or not of the which-way information , and the consequences on the interference pattern , is purely a quantum analysis . now , there are different experimental devices to make appearing or disappearing the which-way information . you do not necessarily need to use polarizers for this ( you could have used mirrors , beam splitters and detectors ) . even , if , in this particular experiment , polarizers have been used , the interesting analysis is not a possible classical analysis ( orthogonal waves do not interfere ) , but it is the quantum analysis . more precisely , in your experiment , the two analysis ( classical and quantum ) are correct , because of the very particular experimental devices , but the fundamental analysis is quantum , and the quantum analysis will still be true , event if you do not use polarizers in your experiment , while , in this case , the classical analysis will fail .
entropy is a concept in thermodynamics and statistical physics but its value only becomes indisputable if one can talk in terms of thermodynamics , too . to do so in statistical physics , one needs to be in the thermodynamic limit i.e. the number of degrees of freedom must be much greater than one . in fact , we can say that the thermodynamic limit requires the entropy to be much greater than one ( times $k_b$ , if you insist on si units ) . in the thermodynamic limit , the concept of entropy becomes independent of the chosen ensembles - microcanonical vs canonical etc . - up to corrections that are negligible relatively to the overall entropy ( either of them ) . a single particle , much like any system , may be assigned the entropy of $\ln ( n ) $ where $n$ is the number of physically distinct but de facto indistinguishable states in which the particle may be . so if the particle is located in a box and its wave function may be written as a combination of $n$ small wave packets occupying appropriately large volumes , the entropy will be $\ln ( n ) $ . however , the concept of entropy is simply not a high-precision concept for systems away from the thermodynamic limit . entropy is not a strict function of the " pure state " of the system ; if you want to be precise about the value , it also depends on the exact ensemble of the other microstates that you consider indistinguishable . if you consider larger systems with $n$ particles , the entropy usually scales like $n$ , so each particle contributes something comparable to 1 bit to the entropy - if you equally divide the entropy . however , to calculate the actual coefficients , all the conceivable interactions between the particles etc . matter .
consider a patch of a world sheet and choose the coordinates so that it is extended in the $t$ and $z$ directions , setting $x=y=0$ - and similarly for the other transverse dimensions i omitted here . the spacetime dimensions is $d$ . the statement that the strings have tension equal to $t$ means that the energy density has to be $$t_{tt} = t \delta^{ ( d-2 ) } s ( x , y , \dots ) . $$ however , the world sheet is required to preserve the $so ( 1,1 ) $ symmetry rotating the $t , z$ axes , so it must be true that a part of the stress energy tensor , the components $$t_{tt} , t_{tz} , t_{zt} , t_{zz}$$ have to be proportional to the 1+1-dimensional metric tensor because multiples of the metric tensor are the only tensors that are invariant under the lorentz transformations , $so ( 1,1 ) $ in this case . it follows that the $t_{tz}$ and $t_{zt}$ components have to vanish while $$t_{zz} = -t \delta^{ ( d-2 ) } ( x , y , \dots ) . $$ the doubly spatial components of the stress-energy tensor represent the pressure but do not forget that the doubly transverse components such as $t_{xx}$ continue to vanish . if one has a gas of randomly oriented strings and he averages over all directions of the string , the average $t_{ii}$ will be $-t_{tt}/ ( d-1 ) $ where $ ( d-1 ) =3$ in our 4-dimensional spacetime so that $p=-\rho/3$ . that is the standard pressure from cosmic strings - domain walls would have $p=-2\rho/3$ for very similar reasons . ( the appearance of $1/3$ is not hard to understand : all the rotated strings will have the same trace over the spatial part of $t_{ii}$ . the $z$-oriented string has this trace equal to $-\rho/3$ so after the averaging over directions i.e. over $so ( d-1 ) =so ( 3 ) $ , when the spatial part becomes proportional to $\delta_{ij}$ , the coefficient of this kronecker delta inevitably has to become $-\rho/3$ to preserve the spatial trace . ) the negative pressure of strings is not too important if the strings remain small and compact but it is important e.g. in string gas cosmology http://scholar.google.com/scholar?hl=enq=string-gas-cosmology+negative-pressurebtng=searchas_sdt=0,5as_ylo=as_vis=0 so your number $p=-\rho$ is incorrect ; it should be $p=-\rho/ ( d-1 ) $ because the pressure only exists in the direction along the string , so the pressure in all directions get diluted by the averaging over directions . that is why the string gas has a different equation of state than the cosmological constant . like any gas , string gas picks a preferred reference frame , unlike the cosmological constant . the precision measurement are enough to exclude the possibility that dark energy boils down to a network of cosmic strings or domain walls because their $w$ is just too far from the " approximately observed " $w=-1$ . by the way , $w$ is defined as $p/\rho$ rather than $\rho/p$ .
there is no difference between those two operators you wrote , since $\frac{1}{i}=\frac{i}{i^2}=\frac{i}{-1}=-i . $ in qm , an operator is something that when acting on a state returns another state . so if $\hat{a}$ is an operator and $|\psi\rangle$ is a state , the quantity $\hat{a}|\psi\rangle$ is another state , which you could relabel with $|\phi\rangle \equiv \hat{a}|\psi\rangle . $ if this dirac notation looks unfamiliar , think of it as $\hat{a}$ acting on $\psi_1 ( x , t ) $ producing another state $\psi_2 ( x , t ) =\hat{a}\psi_1 ( x , t ) $ where $\psi_1$ and $\psi_2$ are just different states or wave functions . when you act on a state with your operator , you do not really " get " momentum ; you get another state . however , for particular states , it is possible to extract what a measurement of momentum would yield once you know what the resulting state is , but that is another question . the term eigenstate may help you in your discovery . but very briefly , if $\hat{a}\psi_1=a\psi_1$ ( note that its the same state on both sides ! ) , one can interpret the number $a$ as a physical observable if $\hat{a}$ meets certain requirements . this is probably what your textbook refers to .
nowadays there exists a more fundamental geometrical interpretation of anomalies which i think can resolve some of your questions . the basic source of anomalies is that classically and quantum-mechanically we are working with realizations and representations of the symmetry group , i.e. , given a group of symmetries through a standard realization on some space we need to lift the action to the adequate geometrical objects we work with in classical and quantum theory and sometimes , this action cannot be lifted . mathematically , this is called an obstruction to the action lifting , which is the origin of anomalies . the obstructions often lead to the possibility to the realization not of the group of symmetries itself but some extension of it by another group acting naturally on the geometrical objects defining the theory . there are three levels of realization of a group of symmetries : the abstract level : for example the action of the lorentz ( galilean ) group on a minkowski ( eucledian ) space . this representation , for example is not unitary , and it is not the representation we work with in quantum mechanics . the classical level : when the group action is realized in terms of functions belonging to the poisson algebra of some phase space . for example , the realization of the galilean or the lorentz groups on the phase space of a classical free particle . the quantum level when the group action realized in terms of a linear representations of operators on some hilbert space ( or just operators belonging to some $c^*$ algebra . for example , the realization of the galilean or the lorentz groups on a quantum hilbert space of a free particle . now , passing from the abstract level to either the classical or the quantum level may be accompanied with an obstruction . these obstructions exist in already quantum and classical mechanics with finite number of degrees of freedom , and not only in quantum field theories . two very known examples are the galilean group which cannot be realized on the poisson algebra of the phase space of the free particle , rather , a central extension of which with a modified commutation relation : $ [ k_i , p_j ] =-i \delta_{ij}m$ , is realized . ( $k_i$ are boosts and $p_i$ are translations $m$ is the mass ) . this extension was discovered by bargmann , and sometimes its is called the bargmann group . a second example , is the realization of spin systems in terms of sections of homogeneous line bundles over the two sphere $s^2$ . now , the action of the isometry group $so ( 3 ) $ cannot be lifted to line bundles corresponding to half integer spins , rather a $\mathbb{z}_2$ extension of which , namely $su ( 2 ) $ can be lifted . in this case the extended group is semisimple and the issue that $su ( 2 ) $ being a group extension of $so ( 3 ) $ and not just a universal cover is not usually emphasized in physics texts . the group extensions realized as a consequence of these obstructions may require : 1 ) ray representations of the original group which are true representations of the extended group . this is the case of $so ( 3 ) $ , where the half integer spins can be realized through ray epresentations of so ( 3 ) , which are true representations of $su ( 2 ) $ . in this the lie algebras of both groups are isomorphic . 2 ) group extensions corresponding lie algebra extensions . this is the more general case corresponding for example to the galilean case . now , in the quantum level , one can easier understand , why the obstructions lead to group extensions . this is because , we are looking for representations satisfying two additional conditions : 1 ) unitarity 2 ) positive energy sometimes ( up to $1+1$ dimensions ) , we can satisfy these conditions merely by normal ordering , which results central extensions of the symmetry groups . this method apply to the case of the virasoro and the kac-moody algebras which are central extensions to the witt and loop algebras respectively , and can be obtained in the quantum level after normal ordering . the relation between normal ordering and anomalies can be explained in that the quantization operators are needed to be toeplitz operators . a very known example is the realization of the harmonic oscillator on the bargmann space of analytic functions , then the toeplitz operators are exacly those operators where all derivatives are moved to the right . this is called the wick quantization and it exactly corresponds to normal ordering in the algebraic representation . the main property of toeplitz operators is that their composition is performed through star products , and star products of toeplitz operators are are also toeplitz operators thus the algebra of quantum operators is closed , but it is not closed to the original group but rather to a central extension of which . this important interpretation hasn’t been extended to field theories yet . it is worthwhile to mention that central extensions are not the most general extensions one can obtain when a symmetry is realized in terms of operators in quantum theory , there are abelian and even non-abelian extensions . one of the more known extensions of this type is the mickelsson-faddeev extension of the algebra of chiral fermion non-abelian charge densities when coupled to an external yang-mills field in $3+1$ dimensions : $ [ t_{a} ( x ) , t_{b} ( y ) ] = if_{ab}^c t_c ( x ) \delta^{ ( 3 ) }x-y ) +id_{ab}^c\epsilon_{ijk} \partial_i\delta^{ ( 3 ) } ( x-y ) \partial_j a_{ck}$ this extension is an abelian noncentral extension . the explanation of the existence " anomalies " in the classical case , i.e. , on the poisson algebra can be understood already in the case of the simplest symplectic manifold $\mathbb{r}^2$ , the poisson algebra is not isomorphic the translations algebra . a deeper analysis for example given in:marsden and ratiu page 408 for the case of the galilean group . they showed that on the free particle hilbert space , the galilean group lifts to a central extension ( the bargmann group ) which acts unitarily on the free particle hilbert space : $\mathcal{h} = l^2 ( \mathbb{r}^3 ) $ . now , the projective hilbert space $\mathcal{ph}$ is a symplectic manifold ( as any complex projective space ) in which the particle 's phase space is embedded . the restriction of the representation to the projective hilbert space and then to the particle 's phase space retains the central extension i.e. , is isomorphic to the extended group , thus the extended group acts on the poisson algebra . as a matter of fact one should expect always that the anomaly should be realized classically on the phase space . the case of fermionic chiral anomalies seems singular , because it is customary to say that the anomaly is existent only at the quantum level . the reason is that the space of grassmann variables is not really a phase space , and even in the case of fermions , the anomaly exists in the classical level when one represents them in terms of " bosonic coordinates " . these anomalies are given as wess-zumino-witten terms . ( of course these representations are not useful in perturbation theory ) . another reasoning why anomalies exist always on the classical ( phase space ) level is that in geometric quantization , anomalies can be obtained on the level of prequantization . now , prequantization does not require any more data than the phase space ( not like the quantization itself which requires a polarization ) . now , trying to respond on your specific questions . it is true that chiral anomalies were discovered in quantum field theories when no ultraviolet regulators respecting the chiral symmetry could be found . but anomaly is actually an infrared property of the theory . the signs for that is the adler-bardeen theorem that no higher loop ( than one ) correction to the axial anomaly is present and more importantly only massless particles contribute to the anomaly . in the operator approach that i tried to adopt in this answer the anomaly is a consequence of a deformation that should be performed on the symmetry generators in order to be well defined on the physical hilbert space and not a direct consequence of regularization . secondly , the anomaly exists in equally on both levels quantum and classical ( on the phase space ) . the case of fermions and regularization was addressed separately . update - elaboration of the spin case : here is the elaboration of the $so ( 3 ) $ , $su ( 2 ) $ case which contains all the ingerdients regarding the obstruction to lifting and group extensions , except that it does not have a corresponding lie algebra extension . we work on $s^2$ using the stereographic projection coordinate given in terms of the polar coordinates by : $z = tan \frac{\theta}{2} e^{i \phi}$ an element of the group $su ( 2 ) $ $g=\begin{pmatrix} \alpha and \beta\\ -\bar{\beta} and \bar{\alpha } \end{pmatrix}$ acts on $s^2$ according to the mobius transformation : $ z \rightarrow z^g = \frac{\alpha z + \beta}{-\bar{\beta} z + \bar{\alpha } }$ however , one observes that the action of the special element : $g_0=\begin{pmatrix} -1 and 0\\ 0 and -1 \end{pmatrix}$ is identical to the action of the identity . this element is an su ( 2 ) element that projects to the unity of so ( 3 ) ( this can be seen from its three dimensional representation which is the unit matrix ) . thus the group which acts nontrivially on $s^2$ is $so ( 3 ) $ now quantum mechanically spin systems can be realized on the sphere in hilbert spaces of analytic functions : $ ( \psi , \xi ) = \int_{s^2} \overline{\xi ( z ) } \psi ( z ) \frac{dzd\bar{z}}{ ( 1+\bar{z}z ) ^2}$ transforming under $su ( 2 ) $ according to : $ \psi ( z ) \rightarrow \psi^g ( z ) = ( -\bar{\beta} z + \bar{\alpha } ) ^{2j} \psi ( z^{g^{-1}} ) $ this is a ray representation of $so ( 3 ) $ as $so ( 3 ) $ does not have half integer representatioons . now , the first observation ( the quantum level ) is that the special element does not act on the wave functions as the unit operator , for half integer spins it adds a phase of $\pi$ . this is what is meant that the $so ( 3 ) $ action cannot be lifted to the quantum hilbert space . now turning to the the classical level . the symplectic form on $s^2$ is proportional to its area element . the proportionality constant has to be an integer in a prequantizable theory ( dirac quantization condition ) $\omega = 2j \frac{dz \wedge d\bar{z}}{ ( 1+\bar{z}z ) ^2}$ the corresponding poisson bracket between two functions on the sphere : $\{f , h\} =\frac{1}{2j} ( 1+\bar{z}z ) ( \partial_z f \partial_{\bar{z}} h - \partial_z h \partial_{\bar{z}} f ) $ the function generating the group action in the poisson algebra is given by : $f_g= ( \frac{\alpha \bar{z}z + \beta \bar{z} - \bar{\beta}z + \bar{\alpha}}{1+\bar{z}z} ) ^{2j}$ now , the function representing the unity of su ( 2 ) in the function $f=1$ , while the function representing the special element is $f=-1$ for half integer spins , which is a different function ( it has to be a constant because it belongs to the center of $su ( 2 ) $ , thus it has to poisson commute with all functions . thus even at the classical level , the action of $so ( 3 ) $ does not lift to the poisson algebra . now , regarding the question of classically distinguishing $su ( 2 ) $ of $so ( 3 ) $ . if you compute the classical partition function of a spin $\frac{1}{2}$ gas interacting with a magnetic field , it will be different than say spin $1$ , but spin $\frac{1}{2}$ exists in the first place only if $su ( 2 ) $ acts because $so ( 3 ) $ allows only integer spins .
the uncertainty principle in use here is not the usual heisenberg uncertainty principle , but the semi-classical quantization of phase space . if two operators have the relation $ [ x , y ] =ik$ , then the minimal area in phase space is $2 \pi k$ . for more info : how does one quantize the phase-space semiclassically ?
the problem is with your first calculation and also with the somewhat misleading equation that you have found . it is true that $$\frac{i_2}{i_1}=\left ( \frac{d_1}{d_2}\right ) ^2$$ but units are important here . in that formula , $i_1$ and $i_2$ would properly be expressed as power values . to compute with decibels , which are logarithmic quantities , one would instead use $$i_1 + 10\log\left ( \frac{d_1}{d_2}\right ) ^2 = i_2$$ or equivalently , $$i_1 + 20\log\left ( \frac{d_1}{d_2}\right ) = i_2$$ , where $i_1$ and $i_2$ are decibels and $d_1$ and $d_2$ are in identical linear units ( feet or meters , for example ) . with your particular numbers we get $$\begin{eqnarray} i_2 and = and 213\text{ db} + 20 \log\left ( \frac{75}{45000}\right ) \\ and = and 213\text{ db} + 20\log\left ( \frac{1}{600}\right ) \\ and \approx and 213\text{ db} + 20 ( -2.78 ) \\ and \approx and 213\text{ db} - 55.56 \\ and \approx and 157.4\text{ db} \end{eqnarray}$$ estimating manually you have correctly remembered that -3db is half the power . that is , $$\frac{1}{2}p = -3\text{db}$$ . another easily remembered fact is $$\frac{1}{10}p = -10\text{db}$$ . both are very commonly used in engineering for rough estimations . so in this case , because it is an inverse square law , we have $$\begin{eqnarray} \left ( \frac{75}{45000}\right ) ^2 and = and \frac{1}{600^2} \\ and = and \frac{1}{360000} \\ and \approx and \frac{1}{400000} \\ and \approx and \frac{1}{2^2\cdot 10^5} \\ and \approx and -6\text{db} - 50\text{db} \\ and \approx and -56\text{db} \end{eqnarray}$$ so this would give $213\text{db} - 56\text{db} = 157\text{db}$
the way i understand your description , the motion of the light and the spring tension are perpendicular to the direction of acceleration . consider the problem in the ( instantaneous ) rest frame of the mirrors and spring . let 's use coordinates where x is the direction of the photon 's motion ( in the unmoving rest frame ) , and y is in the direction of acceleration ( and motion ) of the mirrors . the relevant equations are spring force $f = kx$ , photon momentum in the mirror ( x ) direction = $p_x = \hbar k_x$ , photon rate $1/t$ where $t$ is the time between photons , and the photon force = change in momentum per unit time = $2\hbar k_x / t$ . the requirement for no change to the mirror is that $kx = 2\hbar k_x/t$ . clearly the spring is just a spring in the moving rest frame and so there is no change to the spring force $kx$ . as for the photons , their momentum perpendicular to the direction of velocity ( or acceleration ) is unchanged , so there is no change to the momentum any single photon imparts to the mirror $2\hbar k_x$ . uh , note that since the photon reverses its direction , it imparts twice its momentum to the mirror . another way of saying the same thing is to note that stationary and moving observers agree on the number of photon wavelengths $\lambda_x$ between the two mirrors . they also agree on the distance between the two mirrors . therefore they agree on the wave number $k_x = 2\pi/\lambda_x$ for the photon 's momentum in the x direction . so what is left is the rate at which the photons impact the mirror $1/t$ . this rate does not depend on the acceleration ( relativity problems rarely do ) . instead it depends on the velocity of the mirror . as the mirror velocity increases , the distance traveled by the photons in the moving frame increases . thus the time between photons increases and the photon force decreases . of course in the unmoving rest frame the photon rate is unchanged , it is only in the moving frame that the photon rate changes . this effect is called time dilation . consequently , the non moving observer , on noting the force on the mirror , must conclude that the spring constant $k$ has changed due to the motion of the spring . for a discussion of this interesting effect , see : http://www.mathpages.com/home/kmath068/kmath068.htm
attempts to find an average value of ac would directly provide you the answer zero . . . hence , rms values are used . they help to find the effective value of ac ( voltage or current ) . this rms is a mathematical quantity ( used in many math fields ) used to compare both alternating and direct currents ( or voltage ) . in other words ( as an example ) , the rms value of ac ( current ) is the direct current which when passed through a resistor for a given period of time would produce the same heat as that produced by alternating current when passed through the same resistor for the same time . practically , we use the rms value for all kinds of ac appliances . the same is applicable to alternating voltage also . we are taking the rms because ac is a variable quantity ( consecutive positives and negatives ) . hence , we require a mean value of their squares thereby taking the square root of sum of their squares . . . peak value is $i_0^2$ is the square of sum of different values . hence , taking an average value ( mean ) $i_0^2/2$ and then determining the square root $i_0/\sqrt{2}$ would give the rms . it is example time : ( i think you did not ask for the derivation of rms ) consider that both the bulbs are giving out equal-level of brightness . so , they are losing the same amount of heat ( regardless the fact of ac or dc ) . in order to relate both , we have nothing to use better than the rms value . the direct voltage for the bulb is 115 v while the alternating voltage is 170 v . both give the same power output . hence , $v_{rms}=v_{dc}=\frac{v_{ac}}{\sqrt{2}}=115 v$ ( but guys , actual rms is 120 v ) . as i can not find a good image , i used the same approximating 120 to 115 v . to further clarify your doubt regarding the peak value , it is simply similar to finding the distance between two points $ ( x_1 , y_1 ) $ and $ ( x_2 , y_2 ) $ in cartesian system represented as , ( sum of squares and then " root" ) $$d=\sqrt{ ( x_2-x_1 ) ^2+ ( y_2-y_1 ) ^2}$$
you are talking about the inductive effects of the coil of wire . essentially a wrapped up coil of metal with electrons running through it creates a linear magnetic field since moving electrons through a wire creates a redial field and if you approximate the coil to have infinite loops the field becomes liner . but , this effect would be very , very small for the wires you are talking about since ( a ) the coils are not very densely packed and ( b ) not very much current is flowing through them here is the wiki on inductors : http://en.wikipedia.org/wiki/inductor the simple relationship between voltage ( $v$ ) , inductance ( $l$ ) , and current ( $i$ ) is : $$v ( t ) = l \frac{di ( t ) }{dt}$$ one last thing to consider , magnetic field drops off with distance fast ( so as you move away from the source it gets really weak ) . the plastic protective covering around your wires are a relatively similar thickness to the wires themselves and would buffer anything nearby to most the magnetic effects ( which would be weak to begin with )
nothing gets " pushed away " . instead , if it was left to itself everything would fly off in a straight line ( newton 's law , right ? ) . this is easy to see if you are , say , hovering over the playground in a helicopter watching children fly off the merry-go-'round . i did a sketch . the dark haired kid is holding on ; he goes in a circle . the blond it not ; he moves at a constant velocity . remember that acceleration in a change in velocity and that velocity means both speed and direction . so , the key question is why does it look like they are getting " pushed away " ? the answer is that when you see that happening you are getting pushed toward the center ( that is the centripetal force ) . you are accelerating , while they maintain a constant speed and direction , but they certainly end up a long way away from you .
the notion of covariant derivative is equivalent to the notion of connection . more precisely , for every connection $\nabla$ and vector field $x$ , the operation $\nabla_x$ is a covariant derivative . connections $\nabla$ on the tangent bundle $tm$ of a manifold are usually induced by a metric , this is the so called levi-citiva connection . it is essentially determined by the requirement that the scalar product fulfills the product rule $$ d_x\langle v , w\rangle = \langle \nabla_x v , w\rangle + \langle v , \nabla_x w \rangle . $$ note that this does not yet determine the connection completely , one has to add the additional requirement that there is no torsion , $$ \nabla_x y - \nabla_y x = [ x , y ] . $$ this is a coordinate-free specification of a connection . note , however , that unlike the lie-derivative or the derivatives of differential forms , which are defined in terms of the manifold alone , connections represent additional data . many different connections are possible on a single manifold , whereas the other two notions of derivative are unique .
no , the friedmann equations assume the cosmological principle : the universe at large scales is homogeneous and isotropic . the metric that describes earth may be approximately isotropic ( the same in every direction ) , but it is not homogeneous ( the same in every place ) . a better approximation to the space-time around earth is actually the schwarzschild solution , the one that also describes static black holes and any other spherically symmetric mass .
your equations ( 1 ) ( 2 ) , saying $\delta \psi =-\frac{1}{4}\lambda ^{\mu \nu}\gamma _{\mu \nu}\psi$ with or without $^c$ , just says that both $\psi$ and $\psi^c$ are in the same representation , namely $ ( 1/2,0 ) + ( 0,1/2 ) $ . the third equation ( 3 ) , saying $ ( p_l\psi ) ^c=p_r \psi^c$ , just says that the charge conjugation swaps the two irreducible components of the reducible representation that is the dirac spinor .
the wave function itself can never be discontinuous . it is the derivative what it is discontinuous at $x=0$ , and that discontinuity can be calculated integrating the schrödinger equation between $ ( +\epsilon , -\epsilon ) $ and taking the limit $\epsilon \to 0$ . all terms but the proportional to the delta vanish , giving you $$\left . \frac{d\psi ( x ) }{dx}\right|_{\epsilon=0^+} -\left . \frac{d\psi ( x ) }{dx} \right|_{\epsilon=0^-}=-2\hbar \alpha\psi ( 0 ) $$
the answer is heavily dependent on the material . for most materials the electric constant $\epsilon$ is very far from being a simple scaling factor , it has a complicated frequency response so that we write $\vec{d} ( \omega ) = \epsilon ( \omega ) \vec{e} ( \omega ) $ ( assuming an isotropic and local medium , so that $\epsilon$ is scalar and local ) . for example , the sellmeier equation ( with different coefficients for different materials ) describes many materials at optical frequencies very well and it is simply a set of resonances , which you can think of as arising from electrons on springs ( i.e. bound to molecules ) so they undergo forced harmonic motion in response to the electric field . the " speed " of their reaction is set , as with any forced harmonic oscillator , by the strength of their binding ( the " spring constant" ) and their mass ( inertia ) . recall that each electron only has to shift a very short distance to beget the dielectric effect , so its not like electrons have to rush all over the place to beget the charge shifts : the polarization essentially arises from the appearance and alignment of dipoles . you should keep the system of electrons on springs rather than electrons running all over the place in your head . as the field 's frequency rises , charge separation and polarization stay pretty much in phase with the electric field until you get nearish the first resonance . the sellmeier model when losses are added is called the ketteler-helmholtz model and models essentially a system of damped harmonic oscillators .
first of all , this is a really amazing piece of technology , in particular because it has achieved something that optical engineers have dreamed of for a long time 1 , and done so with underlying techniques that we have had for quite a while . just to give you some impression of how cool this is , i am an optical engineer and the first time i heard about this i was certain that it was either a hoax , a massive exaggerated description of something less impressive ( like an unsharp filter in photoshop or something ) , or simply a piece of godawful technology journalism . however , once i found the doctoral dissertation of the guy who developed this technology dr . ren ng ) , i realized how it works , and really how clever it is . ( note : i will probably add on to this answer a couple times , because i do not have time to give a good and thorough summary right now . if you want a really thorough description , check out the thesis i linked above . it may be a little over your head if you do not have background in optics though , which is what my summary here should help with . ) hardware this technique depends on some very clever data processing techniques , and on the unusual type of camera ( ren ng calls it " plenoptic " which is not a term i have heard before ) that is used to capture the data . this camera has an array of very small lenses ( a " microlenslet array" ) at its image plane , where a normal camera would just have the image sensor . the image sensor is positioned slightly behind this . the microlens array alters the incoming light before it hits the sensor . if you were to look at this raw data as it is captured by the camera , it would look similar to the image you would expect from a normal camera , but it would be composed of thousands of little blobs rather than being a nice continuous image ( i am not talking about pixels , these dots are many pixels across ) . if you zoom in on this image , you would see that each dot is actually a very small , possibly blurry image of a portion of the scene being photographed . on its own , this image is ugly and not really good for anything , but because we know exactly how it was altered by the lenslet array , we actually have more information about the light that entered the camera 2 . with some clever data processing algorithms , we can retrieve this information and use it to determine the focus condition of each part of the scene being imaged . algorithms ( this section will be expand when i have more time ) in the most basic sense , the job of an imaging system is to produce an image where each point records the color and brightness of a corresponding point in the scene being photographed . a plenoptic camera also aims to determine , for each point in the image , how the light from the corresponding point in the scene was focused . this amounts to figuring out not only where that light hits your sensor ( which a normal camera measures ) but what path it took to get there . the data processing done after the image is captured is able to reconstruct this information because instead of having only one piece of information about each ray of light -- which pixel on our sensor it hit -- we now have a second piece of information -- which lenslet in the array did that ray pass through . once we have figured out the path of each bundle of light that we captured from the scene , we can calculate the image we would have gotten from a normal camera for any focus setting over a large range . 1: there are actually other way to achieve this , but the methods developed by ren ng are impressive and novel in that they are reliable and do not require insane amounts of computing power/time . this is what makes his technology marketable to consumers . 2: actually , we did not really get extra information , we just traded a little of one type of information for a little of another type . a normal camera would be able to produce one pixel in the output image for each pixel in its sensor , but this camera will produce an output image with about one pixel for each microlens in the array . the details of the algorithm may raise or lower that ratio a little , but the basic idea about trading one type of information for the other will always hold . this is , by the way , on reason that this technology did not happen sooner -- it was not until recently that we could produce high quality lenslet arrays with enough lenses to produce a good picture .
so . . . the delta function in time is defined as : $\delta ( t - t_0 ) = \int _0^\infty \delta ( t ) dt = \int_t^{t_0} \delta ( t ) dt \equiv 1$ which means that the delta function has the unit of $sec^{-1}$ hence i should divide by the time step .
i cannot tell you if 18kv is correct but yes - it must be a high voltage to create such a spark . addressing the current : it is not simply a u=ri behavior since you have a capacity in your circuit which means , that the current get less over time . this is what the circuit looks like : which means , that the current would behave like : the current behaves like : the current does not equal the charge on the plate but they are dependent on each other by the following equation : keep in mind that if the voltage gets too low for a spark on a certain distance the discharge process stops . i hoped that this answer helped you and contains the information you liked to know .
this answer is specifically about guitars because i have guitar building and repair experience . the strings and the truss rod are under tension so the neck itself is mainly under compression . there is some tension on the back side of the neck due to neck relief ( forward bow of neck ) but not much . necks are made from wood from the trunk , a material ' built ' to handle the compressive stress from the weight of the tree . the safety factor ( failure load/service load ) of guitar necks are very large . a very basic calc . to demonstrate this : compressive yield strength parallel to grain for the most common guitar neck wood ( maple ) = 21.5 m pa approximate cross sectional area = 0.0007 m ^2 f = pa = ( 21.5 m pa ) * ( 0.0007 m^2 ) = 15 kilo newtons . sf = ( failure load/service load ) = ( 15 kn ) / ( 800 n ) = 18.75 basically , guitar necks are super strong and only fail due to impact ( see gibsons broken headstock syndrome ) or warp due to poorly seasoned wood or extreme humidity or temp . changes .
my statistical physics professor call those ones laplace integrals $i ( h ) $ . $$i ( h ) =\int_{0}^{\infty}x^{h}e^{-a^2x^2}dx$$ note that $$\int_{-\infty}^{\infty}x^{h}e^{-a^2x^2}dx=2i ( h ) $$ some values $$i ( 0 ) =\frac{\sqrt{\pi}}{2a} , i ( 1 ) =\frac{1}{2a^2} , i ( 2 ) =\frac{\sqrt{\pi}}{4a^3} , i ( 3 ) =\frac{1}{2a^4} , i ( 4 ) =\frac{3\sqrt{\pi}}{8a^5} $$ you may brute force by integrating by parts to get rid of $x^{h}$ and use $i ( 0 ) $ a classical result , or you may use induction over $h$ or some other method .
having looked at a few more sources , i think i know the answer now , but please do correct me if i am wrong . for 1 . , i still find the appearance of the $\gamma_5$ matrix in dirac spinor description surprising . i suppose the resolution is that the weyl spinors are a more intuitive description in some cases . for 2 . , i think the source of my confusion is some ambiguous statements in the literature . i have heard that majorana fields " cannot absorb a phase , " which i have misunderstood to mean cannot absorb a phase from a complex mass term , whereas it means that majorana fields must be singlets in the fundamental representation/must have no non-zero quantum numbers , because the symmetry would violate the majorana condition itself . the statement that majorana fields cannot absorb phases from complex masses is true only in certain circumstances , namely , if there are interaction terms in the lagrangian which are not invariant under the field redefinition . in the case of the majorana neutrinos , because one no longer has the freedom to rotate right-handed fields only to avoid complications with left-handed weak interactions , fewer phases can be absorbed . for the gauginos , i think there are f-terms , which prevent the total removal of the phases , leaving physical phases e.g. $\text{arg} ( m_i \mu ) $ .
no , it is not a boolean property . entanglement between two quantum systems ( could be particles , or anything else ) could be partial , and can be quantified using different measures . in the specific example of bell states , the two systems ( each of them with 2 states $|0\rangle$ and $|1\rangle$ ) are said to be maximally entangled with the entanglement entropy being 1 qubit .
the available methods depend on what you know about the cft ( or another quantum field theory ) . for example , there are nice classes of two-dimensional conformal field theories ( i mean " minimal models " etc . ) that are almost uniquely ( up to several integer-valued labels ) determined by the conformal symmetry . all the dynamics – correlators etc . – of these cfts are fully encoded by the opes which means that all this dynamics is fully encoded in the coefficients $c_{ijk}$ . these coefficients may be calculated as solutions to the constraints equivalent to the conformal symmetry . once you calculate them , you should view them as a major part of the definition of the cft – they are the most fundamental numbers defining the identity of the cft so it is futile to try to calculate them from something more fundamental . if you have more constructive cfts , the cfts may be essentially " free fermions " or " free bosons " . in such a case , the opes may be calculated by " wick contractions " of some kind . for more general ( but non-free ) cfts , it is helpful to notice that the operators $a_j ( w ) $ are in one-to-one correspondence to the states in the cft ( quantized on a circular space times infinite time ) and the structure coefficients $c_{ijk}$ may also be calculated from the action of the operator $a_i$ on the state corresponding to the equally named operator $|a_w\rangle$ .
bosonic fields are " more significant " than fermionic fields because they may get large vacuum expectation values – from a condensate of many bosons in the same state . consequently , there may exist a meaningful classical field theory limit . massless fields are " more significant " than massive fields because the massive fields in string/m-theory because the massive ones have masses of order the string scale or the planck scale which is huge and at these short distances or high energies , the classical reasoning breaks down , anyway . so we apply the classical effective equations of motion only at distances much longer than the string or planck scale and at these low energies , only the massless fields are visible ( the massive fields can not be excited so they are " integrated out " and do not appear in the action ) . becker-becker-schwarz try to jump to the truly consistent full theory , which is the supersymmetric one , as quickly as they can so the general bosonic string theory 's effective action may be absent in the book . but the corresponding action for the superstring theory is on page 311 etc . – type ii supergravity . borrow another textbook such as polchinski if your primary interest is bosonic string theory . there are kinetic and potential terms for the tachyon , some kinetic terms for the dilaton , the einstein-hilbert action for the metric tensor , and some natural " squared field strength " from the $b$-field . strictly speaking , the tachyon terms should be removed if we talk about " massless fields " because the tachyon field is not massless . but because of its negative squared mass , it is even " less massive " than the ordinary massive states as well as the massless states – it is " below " the massless level – so we usually do include it , too .
where you went wrong was $$-mgk \times \frac{1}{2}at^2=-mgk \times \frac{1}{2}\frac{\delta v}{t}t^2$$ . instead it should be $$-mgk \times \frac{1}{2}\frac{\delta v}{\delta t}t^2$$ anyways i would do it as $$f_{friction}s = mgk\frac{1}{2}at^2 = \frac{v-u}{2t}mgkt^2 = \frac{v-u}{2}mgkt = \frac{27.7ms^{-1}}{2} \times 540kg \times {10ms^{-2}} \times 0.6 \times t = ( 44874kgm^2s^{-3} ) t$$ so $$60kw \times t = ( 44874kgm^2s^{-3} ) t + 207000j$$ and solving gives $$t=13.7s$$
yes . there is a set of metric tensors that describe flat spacetime--that is , the spacetime of special relativity . general relativity allows us to consider many kinds of metrics , but limiting ourselves only to those that are flat reproduces all the basic predictions of special relativity . a big thing that separates sr from gr is that gr demands that matter and energy couple to the underlying curvature of spacetime . in the absence of such coupling , spacetime would simply be flat .
some other " basic " arguments in favor of susy also prefer tev-like superpartner masses although not necessarily as light as 1 tev . the dark matter composed of neutral superpartners , to agree with observations of the current amount of dark matter and the required amounts in the cosmological past needed to preserve some important cosmological processes , should also have mass at most several tev , say at most 10 tev . the gauge coupling unification – which may exist in nature because it is elegant but not it is not inevitable – in mssm works with a satisfactory precision but only if the running is dictated by the spectrum of mssm on " most of the logarithmic interval " between the electroweak scale and the gut scale . well , here the tolerance is clearly higher for higher masses . also , one may imagine that some corrections or modifications of the spectrum do the job despite very heavy superpartners . but the simplest mssm-like way to achieve gauge coupling unification , which still looks intriguing to most physicists , also has to assume that all the superpartners are much closer to the electroweak scale than e.g. to the gut scale .
you should use $u=q\epsilon_1$ . with the total number of particles $n$ being constant , we have : $$\frac{\partial s}{\partial q}=\epsilon_1 \frac{\partial s}{\partial e}=\frac{1}{t}\epsilon_1\tag{1}$$ as you said : $$\frac{\partial s}{\partial q}=k_b\ln ( n/q - 1 ) =k_b\ln ( n\epsilon_1/q\epsilon_1 - 1 ) =k_b\ln ( n\epsilon_1/u - 1 ) \tag{2}$$ $$\to \ , \ , \ , \ , \ , u ( t ) =n\epsilon_1\frac{e^{-\epsilon_1/ ( kt ) }}{1+e^{-\epsilon_1/ ( kt ) }}$$
basically , in atomic physics , you would have two electrons , each with an angular momentum $l_1$ and $l_2$ and spin $s_1$ and $s_2$ , and you want to couple all those to get the best approximation for the resulting spectrum . so you have two options : 1- you couple $l_1$ and $l_2$ to $l$ , and $s_1$ and $s_2$ to $s$ , and then you couple $l$ with $s$ to get $ls$ . 2- you couple $l_1$ and $s_1$ to $j_1$ , and $l_2$ and $s_2$ to $j_2$ , and then couple $j_1$ and $j_2$ to to get $jj$ . so you have two ways to couple those , and the choice depends on how far the electrons are from each other where the specific angular momentum coupling is more pronounced . so if the electrons are close to each other , then you use ls coupling . while if you have them far apart , you use jj coupling . i hope this helps .
$\frac{\delta u}{\delta t}$ does not always have the dimension of speed . it is the change rate of physical quantities respect to time , $u$ can be mass or concentration of electric charge ( density ) or probability density $\rho$ in quantum physics . so if we only consider the classical physics ( i.e. . heat conduction can be described using this function ) , the $\frac{\delta u}{\delta x}$ can be looked as gradient of physical quantity ( in one dimension in this equation ) . since there is such a gradient , therefore , we can think that this gradient will produce a " force" ( not quite an actual force usually ) to drive the transport . therefore , this quantity will have a change rate respect to time . according to the conservation law , this change rate must be equal to the gradient times a constant $c$ , and this $c$ has a dimension of speed .
we can . but they do not scale well : to get grid-level storage , you need to be able to scale to gigawatts of power , and gigawatt-hours of energy . and to be able to cycle hundreds , or thousands , of times . to date , we have one technology which will do that , which is pumped storage hydro , which typically has a round-trip efficiency of 75% . so even though that is worse than a decent battery 's round-trip efficiency , its scalability means that it dominates grid storage . but that is just one small part of the picture . behind the question of storage , is the physics question of how do you balance an electricity grid . the grid typically has very low capacitance , so electricity in and electricity out must balance at every second . to manage that balance , you can either adjust the amount going in , or the amount going out , or both . there are lots of ways to integrate high wind penetrations into the grid : this is a solved problem technically . see , for example , the work in energy policy by delucchi and jacobson ; or the book by gregor czisch on renewable scenarios . there are more ways to do virtual storage than direct storage . for example , delaying consumption of 1gwh of electrical energy at 1gw power , for one hour , is equivalent to storing it at 100% efficiency for one hour . in the uk , a lot of energy is used for domestic hot water use . so thermal storage and delayed heating of that thermal storage , can act as a virtual storage for electrical heating of water . to put some numbers on that , uk domestic hot water storage is currently about equivalent to 60gwh @ 30gw for 24 hours . similarly , with 20 million cars , if they were all electrified , you might have 20 million 50kwh batteries , which is 1twh of electrical storage . v2g ( vehicle to grid ) studies often refer to a round-trip efficiency of about 75% , which comes from 10% loss on charge , 10% loss on discharge , and a few percent on transmission . to find out more , there is a wealth of literature on integrating renewables into the grid . the paper " energy-storage technologies and electricity generation " , hall and bain , energy policy 2008 available as a pdf here , sets out some of the issues of batteries and the grid .
if you go to this link you will see that the lifetime of the pi0 is orders of magnitude shorter than of the charged pions . 8.4 ± 0.6 × 10^−17 seconds , a time characteristic of electromagnetic reactions . it decays to two photons , which can be measured in the laboratory . if it is produced with some energy in the laboratory system , its speed can be estimated by measuring the four momenta of the photons and equating the sum to the four momentum of the pi0 . its speed then can be found for that individual measurement . there is no general " speed " of the pi0 , as there is no general speed of any elementary particle , their four momenta being dependent of the interaction that produced them and very variable . to have a speed a fraction of the speed of light any pion or other elementary particle should have an energy given by the relativistic formulae . have a look here where they calculate the energy necessary for a velocity 1% of the velocity of light for various particles .
escape velocity depends on what you are trying to escape from and how far away from it you are . that wikipedia reference makes that very clear . as sachin shekhar pointed out , if you are in the vicinity of the sun , and you are trying to escape the galazy , you need 525 km/s . if you are trying to escape the solar system , and you are at neptune 's distance from the sun , you only need 7.7 km/s . when did voyager 2 achieve escape velocity from the solar system ? according to this diagram from wikipedia , it occurred during its gravity assist from jupiter . voyager 1 was probably not too much different .
i recommend you chapter 5 ( page 150+ ) of the ads bible , http://arxiv.org/abs/hep-th/9905111 concerning your individual questions , which are mostly answered at the beginning of that chapter , the additional virasoro generators correspond to bulk coordinate reparametrizations that preserve the metric at infinity , but they do map the ground state to excited states yes , the cfts in ads/cft typically have a nonzero central charge which is directly related to the $ads_3$ curvature radius in the planck units ; there is no reason for $c=0$ here because the boundary cft is not really coupled to gravity ( which is what the world sheet cft is doing ) for the same reason , you can not directly interpret the cft as string theory ; the full string theory needs $c=0$ in total , so extra ghosts must be added ; also , the interpretation of " winding/twisted " sectors is different in boundary cfts and string cfts . of course , this does not eliminate the fact that similar " building blocks of cfts " are used in both kinds of cfts . . .
there are a lot of methods to calculate free surface or multiphase flows , and most of them have some implementation of surface tension forces . a small list : volume of fluid method level set method marker and cell method moving mesh techniques
lumo gives a very nice step by step calculation of this sum and a good discussion of the importance and application of such summation techniques in qfts here : http://motls.blogspot.com/2011/07/why-is-sum-of-integers-equal-to-112.html such mathematical calculations are not unrelated to physics ; on the contrary they are important . . .
in case of walking on horizontal plane chemical energy is turned into heat . ( muscles are constantly contracting and expanding and in this way your body 's temperature increases . ) moving your limbs is not very efficient way of moving , this is why there is a room for improvement . e.g. if you are cycling ( or skating . . . ) , you use the same quantity of chemical energy to make much larger distance . if one could make a perfect bycicle ( without any friction in the wheels and friction between the wheel and the ground ) , you could make miles of horizontal distance without any effort/energy at all !
the electron-phonon scattering is a process that takes the electron across its fermi surface ( from an occupied state to an empty state , or vice versa ) by absorbing or emitting a phonon . compared to the electron energy in most solid state materials , the phonon energy is neglectable , such that the electron will ( almost ) not change its energy when scattering with a phonon . therefore the phonon can only scatter off the electrons on the fermi surface . however the greatest possible momentum transfer on the fermi surface is $2k_f$ ( assuming spherical fermi surface with radius $k_f$ ) . so all scattered phonon must have a momentum $q\leq 2k_f$ .
i would guess you mean self diffusion : see http://en.wikipedia.org/wiki/self-diffusion for details . suppose you take an aqueous solution of ( for example ) salt that is uniform so there are no concentration gradients . there is no net diffusion , but the sodium and chloride ions wander around due to random thermal motion , so if you watch a particular sodium atom it will " diffuse " around in a random walk motion .
wikipedia to the rescue ! fj dyson and a lenard : stability of matter , parts i and ii ( j . math . phys . , 8 , 423-434 ( 1967 ) ; j . math . phys . , 9 , 698-711 ( 1968 ) ) ; fj dyson : ground-state energy of a finite system of charged particles ( j . math . phys . 8 , 1538-1545 ( 1967 ) ) i found the reference in ref 6 of http://en.wikipedia.org/wiki/pauli_exclusion_principle
if the container full of air is spinning around you , the drag will eventually set you spinning as well , regardless of the rotational speed or the air density . low air density just means that it will take much longer . eventually the air and you will share the same rotation , so that as you speed up , the air and the container will slow down . only in ( complete ) vacuum will you never start spinning . but there is no such thing as a complete vacuum , there are always at least some atoms or molecules around . when the density gets too low , quantum effect will start to take over , as individual particles push you one way or the other .
do not get too worried about fine meanings of the word : it is ultimately a little imprecise , and when you are thinking about real , physical problems , you are going to be working with equations . the more fundamental concept is interference , which is simply a manifestation of the linear superposition principle . amplitudes add , so magnitude and phase is important when summing up contributions to a field from different sources . diffraction works like this . suppose you know a monochromatic field 's values on one transverse plane . now fourier transform the values , to express the field on a transverse plane as a sum of plane waves . plane waves running nearly orthogonal to the transverse plane have almost the same phase over wide transverse regions . so they show themselves as low spatial frequencies in the transverse plane field pattern . plane waves running at steep angles to the transverse plane beget high spatial frequency components in that plane . so we have resolved our field into a linear superposition of plane waves . because these waves are propagating in different directions , they undergo different delays in reaching another transverse plane . the fourier co-efficients take on different phases , so the same constituent plane waves interfere together to make a different field configuration on other transverse planes . diffraction is thus the interference of a field 's ( e . g . an electromagnetic field following the linear maxwell equations ) plane wave constituents . these constituent plane waves beat differently on different transverse planes because they undergo different phase delays by dint of their different directions . see my answer here and also here for more info . another equivalent ( in the larger propagation distance limit ) is huygens 's principle . think of a single slit field . diffraction is the interference on a farfield plane between the different fields arising from the different huygens point sources at different positions in the slit .
from this google book hit , i think it is called a ketenyl radical . searching ketenyl radical seems to bring up hits with the same compound formula . incidentally , it was not that hard to find by searching hcco compound . ninth hit , i think . . .
only with this , as the waves have different wavelengths , i guess there can not be any interference , we will only see the difraction pattern , the two functions of the form sin2 ( x ) /x2 , with the principal maximums separated a distance d . am i right here ? sort of . the diffraction pattern is visible " at infinity " , which is in fact your case #3 . i will explain there . 1 . - in the first one , the system is configured such that the slits are far away from the lens . here , we can approximate the wave that arrives as a planar wave , and therefore the lens will perform the fourier transform in the focal plane of the screen . the diffraction of the slits also performs the fourier transform , so this configuration should lead to having only two bars of light in the screen , centered in the focus . am i right ? sort of . your lens has a limited diameter , so if you place it far from the slits , it will capture only the central portion of the diffraction pattern , i.e. the top of the sinx/x function . in other words , you will loose the fringe pattern and reconstruct slits without fringes . 2 . - the slits are in the focal plane on the lens , such that the lens is in the middle of slits-screen . here , the same thing should happen , right ? as the light comes from the focal plane , the lens must do the fourier transform with no extra things , and we should get the two bars , again both of them in the same line ( center of the screen ) . am i right here ? this will be somewhat different because you do the image of the slits at infinity , i.e. a blurry image at close distances . depending on how close is the lens , you may only be taking the " no-fringe " portion of the diffraction pattern . 3 . - the last one , i can not see . . . the lens is just behind the slits so the distance between slits and lens is 0 . that is exactly the typical school case . the diffraction pattern is , before the lens , located at infinity , or in other words , the fringes are defined as angles and not position ( $\sin\theta/\theta$ ) . the role of the lens is to bring these fringes at a finite distance ( the focal length ) . you probably learned that a lens makes an image , initially located at infinity , located at the focal length . that is the same with the diffraction fringes . now , as the two slits are both close to the lens , you will not do an image of them . that means that you should not see separated slit images , but instead , you should see two superimposed diffraction patterns , centered at the same point .
the rule is : with a diagonal metrics , the character of the coordinate $x^i$ , is given by the sign of $g_{ii}$ setting $t= \frac{u + v}{2}$ and $z= \frac{u - v}{2}$ , your metrics becomes ( with the constraint $z &gt ; 0$ ) : $$ds^2= -dt^2 + dz^2+ ( 1-t-z ) ^2dx^2+ ( 1+t+z ) ^2dy^2$$ so , the character of $t$ as a time-like coordinate and $z$ as a space-like coordinate ( with the constraint $z &gt ; 0$ ) appears clearly . and $x$ and $y$ are space-like coordinates too . the variables $u = t + z$ and $v = t - z$ are called light-cone coordinates , you could say that they are light-like coordinates . they are very often used in general relativity and string theory . they are also called null coordinates , because for instance , if $dx=dy=0$ , then you have $ds^2 = - 2dudv$ , so each particle with $u=$constant or $v=$constant corresponds to $ds^2=0$ , that is a light-like interval ( a light ray ) . but you cannot say that one of the coordinates $u , v$ is a time-like coordinate , and the other a space-like coordinate . in schwarzchild metric , it is not correct to say that $r$ and $t$ could change their sign . the schwarzchild metric describes a gravitational field only for $r&gt ; r_s$ . it is a limited description ( which does not cover the entire manifold ) , and you have to use kruskal-szekeres coordinates to have a glbal view of the black hole . mathematically , " these coordinates have the advantage that they cover the entire spacetime manifold of the maximally extended schwarzschild solution and are well-behaved everywhere outside the physical singularity . "
the details of your analysis are not quite right - that is not what the electric field of a moving charge looks like , for example . this is probably because you have not learned all the rules of electromagnetism yet . still , the spirit of your question is hitting at an important point . charges do not conserve momentum and do not obey newton 's third law . you have to include the momentum of the electromagnetic field to see conservation laws hold . there is an accessible discussion in section 8.2 of griffiths " introduction to electrodynamics " if you would like a little more math .
an electromagnetic wave with a well-defined frequency and direction , i.e. $\vec k$ , only has two possible truly physical i.e. transverse polarizations , i.e. the linearly polarized waves in the $x$ and $y$ direction ( or the two circularly polarized ones ) . that implies that a truly physical counting of polarizations gives you 2 , more generally $d-2$ in $d$ spacetime dimensions . starting from the $a_\mu$ potential fields , one component is unphysical because it is pure gauge , $a_\mu\sim\partial_\mu\lambda$ , and one of them is forbidden due to gauss ' constraint $\rm div\ , \vec d=0$ etc . that already constrains the allowed initial state of the electromagnetic field . both of these killed polarizations are ultimately linked to the $u ( 1 ) $ gauge symmetry . if one is allowed to count off-shell and unphysical fields , there may be many more components than two . but it is always possible to deduce that there are two physical polarizations at the end . for example , when we view $\vec b , \vec e$ as basic fields , there are six components , a lot . but these fields only enter maxwell 's equations through first derivatives , and not second as expected for " normal " bosonic fields , so these fields are simultaneously the canonical momenta for themselves . this brings us to three polarizations but one of them is killed by the constraints , the maxwell 's equations that do not contain time derivatives . the hertz vector is just the most famous " non-standard " example how to write the electromagnetic field as a combination of derivatives of some other fields . one must understand that the room for mathematical redefinitions etc . is unlimited and it is a matter of pure maths . all these descriptions may describe the same physics . at the end , the only " truly invariant because measurable " number of " fields " that all these approaches must agree about is the number of linearly independent physical polarizations of a wave/photon with a given $\vec k$ . if you can analyze any mathematical formulation of electromagnetism or another field theory and derive that there are $d-2$ physical polarizations ( this usually boils down to the difference of the number of a priori fields minus the number of independent constraints and the number of parameters defining identifications i.e. gauge symmetries – but the independence is sometimes hard to see and requires you to make many steps of the counting ) , then you have proved everything that is " really forced to be true " . various formalisms may offer you other ways to count the number of off-shell fields ( with different answers ) and they may be useful ( because they satisfy certain conditions or enter some laws ) but to discuss them , one has to know what the laws where they enter actually are . a truly physical approach is only one that counts the physical polarizations . the gauge symmetry is just a redundancy , a mathematical trick to get the right theory with 2 physical polarizations out of a greater number of fields with certain extra constraints or identifications . the precise number of constraints or identifications may depend on the chosen mathematical formalism and it is not a physically meaningful question – it is a question of a subjectively preferred mathematical formalism because the physics is equivalent for all of them .
why is that the case ? why does all the heat go towards its kinetic energy per unit mass ? that is my one question basically there are two places for the energy to go in the output stream : thermal energy kinetic energy the problem through ( a ) is a direct energy balance problem . in this part , we are treating $t$ as independent . if $t$ is the average of the inlet temperatures , then the velocity will be zero because all energy is accounted for , and $\delta q$ will be zero . mentally , i see a thermal cycle with the $t_1$ feeds the boiler and $t_2$ feeds the condenser . that cycle outputs useful work . the more useful work it produces , the more the average temperature of the streams is lowered . this is similar to a thermal power plant . the boiler produces more heat than the condenser rejects . typical efficiency is 33% , so the condenser removes only 2/3rd of the boiler 's heat . the rest of the heat went to turning the turbine because heat is a form of energy and energy is the ability to do work . in your case , the stream might have been accelerated by a pump that feeds into a cavity that has a nozzle where the stream is accelerated . the more work the pump does , the more the temperature of the water has to decrease because this is a closed system . practically , either the temperature change would be miniscule or the velocity would be gigantic . the reason is that thermal vibrations are fast compared to speeds we are used to . you can look at it this way in terms of your problem too ! the average kinetic energy of the molecules is the same going in and going out - it is just that some of that kinetic energy going out is from the bulk motion of the stream so we do not count it as temperature . that temperature would have to be measured by a thermometer moving along with the stream .
as far as i know , the area vector is a purely mathematical object whose definition is related to the orientability of the surface ( in this case , a disk ) . this is a property of surfaces embedded in an euclidean space that allows to choose surface normal vector to the surface at every point . for an oriented surface , this normal is determined so that we can use the right-hand rule to define a clockwise direction of loops on the surface , which by the way , is needed if we want to apply stokes ' theorem .
i can´t fully come up with an explanation from more basic principles , but in the case you describe you will have kinetic friction . or at least that is what all engineering books say . . . there are a number of situations where this effect is clearly demonstrated : pulling a cork out of a bottle , using a basic corkscrew such as this : if you simply pull on the corkscrew , it is harder to pull it out than if you first get it rotating and then pull . while not entirely the same , a similar situation arises when a car rolling down a road with a strong side wind brakes and locks the wheels . while the wheels were rolling , there is no relative motion between the road and the tire , so there is static friction in effect , and unless the wind is really strong , as in a hurricane , the force will not overcome friction and the car will not skid sideways . once the brakes are locked , the car starts skidding forward , because the force due to the inertia of the car overcomes friction . once this happens , the car will also start skidding sideways , due to two factors : the lesser important is that the coefficient of friction in effect once there is relative motion is the kinetic , not the static one . the main effect is due to the fact that the direction of movement does not really matter at all : at the contact point you have a force due to inertia and a force due to the side wind , and once their combined magnitude exceeds friction , you will start having movement in the direction of that combined force , so forward but also to the side .
feynman in multiple writings suggested thinking about " exchanging particles " in terms of exchanging them as they move through time . that is , they can either move in two parallel paths as they move forward , or they can cross paths ( exchange roles ) . the antisymmetric cancellation applies to the latter , but not to the former . now if you think that through , it means that the parallel path remains strong even as the crossover paths cancel out , resulting in the two particles avoiding each other and maintaining unique paths ( wave functions ) . the net result is not full cancellation , but cancellation at the edges , where the particles would cross . ( feynman goes into a lot more detail about rotations , but frankly that part can get you sidetracked a bit ; it is the " anti-crossover " part that counts in terms of actual outcomes . ) another consequence of identical fermions cancelling each other out is that packing more fermions into a tight space forces their space-filling wavelengths to become shorter also . since in quantum mechanics the spatial wavelength of a particle defines its momentum , particles that are squeezed in this fashion also get very , very hot . a neutron star is a good example . pauli exclusion -- the " constriction of space because crossover cancels but parallel does not " -- allows neutrons to pack together very densely indeed . there are limits , however . when gravity gets too monumental , even pauli exclusion is unable to keep up with the pace , and the entire star collapses , very quickly . thus is born a stellar-sized black hole , or at least this is one example of how one can form .
you will need to include a vertical and horizontal force at point c due to ladder ac ( since we should not assume a direction for the total force -- even though our intuition may prove to be correct ) . the vertical forces on ladder bc are then related by $f_v + n - w = 0$ and you have already established that $n = \frac{3w}{4}$ . there are now five forces acting on ladder bc , but if we look at the force moments about b , we can neglect the torque due to n and the static friction acting on the foot of the ladder . we can then sum the torques due to w , $f_v$ , and $f_h$ to zero . you should indeed find that the total force f at c ( using the pythagorean theorem ) comes to $\frac{w}{2}$ and the direction is 30º above the horizontal ( i find it pointing to the right ) .
my intuition was correct . for more information visit aps website .
note ; i will use the summation convention throughout here . in the context of differential geometry , the indices on tensorial objects are raised and lowered with the metric on the space ( manifold ) being studied . so for example $$ t^i_{\phantom i j} = g^{ik}t_{kj} $$ and $$ t^{ij} = g^{ik}g^{jl}t_{kl} $$ notice that if the metric is simply that of euclidean space , namely if $g_{ij} = \delta_{ij}$ , then raising and lowering does not change the numerical values of tensor components . in particular , one would have $$ t^{ij} = t_{ij} $$ notice that in expressions like $$ t^{ij}t_{ij} $$ both indices are being summed over , where as in the expressions $$ ( t^{ij} ) ^2 , \qquad ( t_{ij} ) ^2 $$ one usually ( this is actually a matter of notational preference ) does not intend for their to be any implied summation , so typically its notationally safe to assume that $$ t^{ij}t_{ij}\neq ( t^{ij} ) ^2 , \qquad t^{ij}t_{ij}\neq ( t_{ij} ) ^2 $$ but if the metric satisfies $g_{ij} = \delta_{ij}$ , then it is true that $$ ( t^{ij} ) ^2= ( t_{ij} ) ^2 $$
you start out getting a bachelor of science in a related field . this could be physics , astronomy , mathematics , or possibly chemistry . depending on which country you are planning to go to grad school in , specializing at this stage may not be as important as in later stages . however , note that in the uk , for example , it is almost unheard of for a student without a bachelors in physics ( almost always with a minor in astrophysics ) to gain a place in grad school for astronomy . after doing the bsc , you would go on to get a masters degree in astronomy , which would set the stage for a phd in astronomy . the phd ( and to some extent , the msc ) will have you specialize in a specific area in astronomy , such as star formation , planetary studies , or cosmology . after doing the phd , you would get a post-doc at a university or research institute . this is typically a three-year job where you do research in your chosen area . most astronomers do two or three post-docs . after this , you could become a professor or research associate at a university or an in-house research astronomer at an observatory . universities or observatories are typically the only places to be a " professional astronomer " , and depending on the position , would give you time to do your own research in conjunction with other duties like teaching .
the answer is here there exists the effect of the loss of mass and therefore gravitational attraction between the earth and the sun but it is small : if we assume that the sun 's rate of nuclear fusion today is the same as the average rate over those 10 billion years ( a bold assumption , but it should give us a rough idea of the answer ) , then we are moving away from the sun at the rate of ~1.5 cm ( less than an inch ) a year . i probably do not even need to mention that this is so small that we do not have to worry about freezing . there is also the even smaller effect of the tides induced on the sun by the earth : it turns out that the yearly increase in the distance between the earth and the sun from this effect is only about one micrometer ( a millionth of a meter , or a ten thousandth of a centimeter ) . so this is a very tiny effect .
it is simple . for people with myopia ( nearsighted ) the corrective lenses are concave . in the other case , the corrective lenses are convex . so just by looking at the type of lens you can tell the difference . if the dioptres are small , look to the edges of the lens , there you will see the difference .
generally the fifth equation is pressure conservation : $$ \frac{\partial p}{\partial t}+\mathbf v\cdot\nabla p+\gamma p\nabla\cdot\mathbf v=0 $$ which , depending on your particular subfield , is usually written in terms of the total energy : $$ \frac{\partial e}{\partial t}+\nabla\cdot\left [ \left ( e+p\right ) \mathbf v\right ] =0 $$ where $e=\frac12\rho v^2+p\left ( \gamma-1\right ) ^{-1}$ . however , if you want to do this in terms of the entropy and you are working with an ideal gas , then we can define a variable $$ s\equiv p\rho^{-\gamma} $$ as a measure for entropy ( not entropy itself because it is missing certain constants , e.g. heat capacity at constant volume $c_v$ ) . then you can use the total derivative to evolve $s$: $$ \frac{ds}{dt}=\frac{\partial s}{\partial t}+\mathbf v\cdot\nabla s=0 $$ the connection between this measure of entropy , $s$ , and the entropy density , $s$ , is $$ s=c_v\ln ( s ) =c_v\ln\left ( p\rho^{-\gamma}\right ) =c_p\ln\left ( p^{1/\gamma}\rho^{-1}\right ) $$ where we used $\gamma=c_p/c_v$ between the latter two equalities . the above relation can be found in section 83 of landau 's fluid dynamics text .
on the earth-sheltering question , the answer is yes , using material to increase the thermal mass of structures would work just as well on the moon as on earth . there might be minor differences due to different materials and lack of water in moon soils but the general principal would still apply . as for the moon 's core still containing significant heat , that answer is no . at least not compared to the earth . smaller bodies cool much more rapidly than larger bodies as their surface area to volume ratio is much higher and therefore they can radiate heat faster . the moon 's core cooled off much , much faster than the earth 's and most of the latent heat of formation is now gone . there is probably still some heat left from decay of radioactive materials but it is much lower than what the earth has . an indicator of the lack of interior heat is that the moon is very seismically quiet , there is not much going on under the surface . seismographs place by the apollo missions showed very , very little activity . if there were heat and significant liquid portions to the moon 's interior , there would have been much more activity on the seismographs .
i know that this does not directly answer your question about purcell 's reasoning ( see addendum i wrote after reading purcell 's argument ) , but here 's how a uniqueness proof would go . suppose that two fields $\mathbf b_1$ and $\mathbf b_2$ both satisfy the magnetostatics equations $$ \nabla\times\mathbf b_i = \mu_0\mathbf j , \qquad \nabla\cdot\mathbf b_i = 0 , \qquad i = 1,2 $$ let $$ \mathbf d= \mathbf b_2 - \mathbf b_1 $$ then the curl and divergence conditions give $$ \nabla\times \mathbf d= 0 , \qquad \nabla\cdot\mathbf d = 0 $$ now your question reduces to what we can say about the vector field $\mathbf d$ . take the curl of both sides of the first equation and use the following vector calculus identity : $$ ( \nabla\times ( \nabla\times \mathbf d ) ) _i = \partial_i ( \nabla\cdot \mathbf d ) - \nabla^2d_i $$ along with the zero divergence condition , to obtain $$ \nabla^2 d_i = 0 $$ in other words , each component $d_i$ satisfies laplace 's equation . now , if we assume that the magnetic field components vanish at infinity ( as would be the case for a bounded current distribution ) then we recall that the only solution to laplace 's equation that vanishes at infinity is the zero solution . this gives $\mathbf d = 0$ and thus $\mathbf b_1 = \mathbf b_2$ . addendum . after having read purcell 's argument , i am fairly confident that he also is making the physical assumption that bounded current distributions produce fields that vanish at infinity . such a boundary condition cannot be derived from the equations themselves . purcell is rather vague when he specifies this boundary condition ; what he says before the argument is " we do not consider sources that are infinitely remote and infinitely strong . "
the only force which works is gravity$^1$ . so , change in gravitational potential energy equals final kinetic energy ( assume initial is zero ) . $$mgh=mv^2/2$$ $$v=\sqrt{2gh}$$ here $h$ is vertical height traversed . see the velocity does not depend on angle of string , mass of body too . . let 's see the kinematics of body . the length of string is $h cosec\theta$ ( $\theta $ being angle with horizontal assumed $\pi/6$ ) acceleration of body along the string=$g\sin\theta$ now $\text{using} : v^2=u^2+2as$ $$v^2=0+2\times h cosec\theta\times g \sin\theta$$ $$v=\sqrt{2gh}$$ working in differentials for $v$ along the rope . $$dv/dt=v\dfrac{dv}{dx}=a$$ $$\int_0^{v_f} v . dv=\int_0^{hcosec\theta} a . dx=ax\bigg|_0^{hsosec\theta}$$ $$\dfrac{v_f^2}2=gsin\theta . hcosec\theta \ \ ; \ \ a=gsin\theta$$ $1 ) $assuming the pulley being used to slide to be friction less . though not possible . also the rope is assumed to be in-extensible and straight .
yes it can be seen , given that you do not get much noise and that your sensor is sensitive enough for it to be able to detect the signal . as you have not specified anything about the light not much can be concluded , more than that it depends on the sensor , the transmitter and the noise from everything else .
both " perfectly open " ( zero acoustic impedance ) and " perfectly closed " ( infinite acoustic impedance ) boundary conditions are only idealizations that never occur in practice . for the case of the human vocal tract , they are not even very good approximations . the " bottom end " of the resonating cavity is not , in fact , the lungs , but the vocal folds ( as georg pointed out ) . this end has some acoustic impedance that is neither extremely low nor extremely high . i am sure the impedance also changes somewhat with the pitch and volume of the phonation . the " top end " of the cavity is of course the mouth , and its impedance changes with the vowel sound you are pronouncing . for aptly-named " open " vowels such as " ah " , " oh " , and so on , the impedance is actually low enough that it might be a good approximation to say it is perfectly open . but for closed vowels ( "ee " , " oo " , . . . ) and especially for humming with closed lips , the acoustic impedance is much higher ( but still far from infinite ) .
photons go on for ever unless they hit something and space is pretty empty . so unless there is a grain of dust , or a star in the way a photon will travel across the universe . paradoxically it is harder for high energy photons such as x-rays to travel large distances in space . because of their energy they can be effected by passing close to even something as small as a single electron . the other reason we still do not see x-ray objects at the same vast distances that we see infrared sources is that , even with chandra , x-ray telescopes are smaller and less sensitive than optical or radio telescopes and so we need more photons from the source , so it needs to be brighter or closer . the reason distant objects are fainter is not so much that photons are blocked - it is that the photons spread out into a sphere . so at 2x the distance away they are spread over an area 4x as big and so are diluted .
if you defined $\delta \hat a=\hat a -\langle \hat a \rangle$ and then $\delta \hat b=\hat b -\langle \hat b \rangle$ no , one of the equalities is not true ! . $$\langle \{\delta \hat a , \delta \hat b\} \rangle\not=\langle \{\hat a , \hat b \}\rangle$$ . but this one is true : $$\langle [ \delta \hat a , \delta \hat b ] \rangle=\langle [ \hat a , \hat b ] \rangle . $$
maybe an example : a particle moving in 2 dimensions has a lagrangian $$l = \frac{\dot{x}^2 +\dot{y}^2}{2} $$ so $$p_x = \frac{\partial l}{\partial \dot{x}} = \dot{x}$$ $$p_y = \frac{\partial l}{\partial \dot{y}}=\dot{y}$$ suppose it is constrained to move on a circle $x^2+y^2=r^2$ now there is a constraint between the p 's which you can get from differentiating the constraining circle , namely $$x\dot{x}+y\dot{y}=0$$ this is a constraint , but not of the type you are talking about , since the lagrangian is still regular . to obtain a lagrangian which is singular rather than regular , we require c onstraints which result in the vanishing of the hessian matrix $\frac{\partial^2l}{\partial \dot{q}_i \partial \dot{q}_j}$ . this means that the legendre transform ( sometimes called the floer map ) from the tangent bundle to the cotangent bundle ( phase space ) $$\mathcal{fl} : tq \rightarrow t^{*}q$$ given by $$ ( q_i , \dot{q}_i ) \rightarrow ( q_i , p_i=\frac{\partial l}{\partial \dot{q}_i} ) $$ is not invertible . it is image is restricted by a bunch of constraint functions . ( caveat , assuming we are restricted to a neighbourhood where rank of hessian is constant ) . for example , for the following lagrangian $$l=\frac{1}{2} ( \dot{x}^2+\dot{y}^2 ) +\dot{x}\dot{y}+4x\dot{y}+2x^2+4xy$$ the hessian determinant is easily seen to vanish . the generalized momenta are $$p_x=\dot{x}+\dot{y}$$ $$p_y=\dot{x}+\dot{y}+4x$$ you can then eliminate $\dot{x}$ and $\dot{y}$ from these relations to find your constraint equation . ( edited to provide example appropriate to the op 's question )
the decay rate is $$\left|\frac{\mathrm{d}n}{\mathrm{d}t}\right| = \lambda n . $$ half-life is $$\tau_{1/2}=\frac{\ln 2}{\lambda} . $$ i think you can figure it now .
this question is answered by nima arkani-hamed in his simons center talk , at about 112 minutes in . his answer is that the structure of the amplituhedron itself does not directly use integrability of the theory in any way . it is only when you come to do the integrals themselves that integrability makes it possible . the amplituhedron itself is more linked to locality and unitarity conditions , while the positivity of the grassmannian is linked to the planar limit . since they only have the amplituhedron fully working for the n=4 theory at present it is impossible to say that integrability is not necessary to make the structure work , but he would be very surprised if it is not possible to extend the amplituhedron itself to n &lt ; 4 and perhaps even to ordinary n=0 yang-mills . the part which should change with n is the form defined on the amplituhedron which is integrated to give its " volume " . this form is simple for n=4 with just logarithmic singularities on the boundary . for n &lt ; 4 you need to multiply by another factor that has singularities elsewhere corresponding to unltraviolet divergences , although this extension of the theory is not in such a complete state as the n=4 case , they are optimistic that it still works without the integrability so there is no reason to think that integrability is hidden in all qfts by the way they also hope to go beyond the planar limit by replacing positivity with some more general structure and it sounds like that work is progressing well .
as mark wrote , your reasoning is ( almost ) correct and your friend 's is not . here 's the sort of explanation i use when i am teaching this topic : in order to apply newton 's second law , you need to choose one object to apply it to . ignore everything else except the object you choose and the forces that act on it . in your case , the problem asks about a force exerted on the bottom mass , so you should write out newton 's second law for the bottom mass . there are exactly three quantities that go into the equation : mass of the object this is given in the problem . ( typically the mass is given . ) acceleration of the object this is also given in the problem . ( in most cases , it is either given or it is what you will be solving for . ) when you are determining acceleration , only consider how the object is changing its velocity . gravity is irrelevant , the value and direction of velocity are irrelevant , whether the object is slowing down or speeding up or curving is irrelevant . the problem tells you that the elevator is accelerating at $5\ \mathrm{m/s^2}$ upward , so the only thing you need to assume is that the mass is moving along with the elevator , and that means the acceleration of the mass is $a = 5\ \mathrm{m/s^2}$ ( assuming positive values are upward ) .netforce on the object this is usually the part of newton 's law that takes a bit of thought . you have to enumerate each of the forces acting on the object and include the correct term for each one . in this case , there are two forces acting on the object ( which is the lowest mass ) : the force exerted by the spring above , and gravity . the spring force goes up and gravity goes down , so you would write the net force as $f_s - f_g$ , or $f_s - mg$ once you plug in the value of the gravitational force . note that each of these terms represents a force on the object . $f_s$ is the force that the spring exerts on the mass ; $f_g$ is the force that the earth exerts on the mass . the problem asks for the force that the spring exerts on the mass , so you do not need to invoke newton 's third law in this case . once you have all these quantities , you can put them into the equation $$\sum f = ma$$ and solve for whatever you need to .
if you use tight-binding hamiltonian , it is reasonable to start not from semiclassical , but one-particle approximation . in that case , you have an amplitude ( complex number ) at each site , the state is complex vector of length $n$ , hamiltonian is $n\times n$ ( sparse ) matrix and the problem of time evolution and/or eigenstates ( for one particle state ) is solvable for relatively large lattices . if you are interested in many particle physics , you may build a model on top of these oneparticle states . the details are dependent on what exactly you wish to compute . unfortunately , i do not know a reference with rigorous transfer from one formulation to another .
without seeing the quote/context i can only imagine that it means something like : if you take , say , a cube moving at close to c in the z direction , then ( in the frame in which it is moving ) its z extent gets lorentz contracted to virtually zero , so it is effectively now a square in the xy plane and has only the degrees of freedom that a square in the xy plane has .
i hope i understand this question properly . the text appears to focus on time loops , or closed timelike curves ( ctcs ) , while the title of the question concerns the creation of a universe from nothing . the holographic principle of black holes tells us the field theoretic information of strings on the event horizon is completely equivalent to field theoretic information in the spacetime one dimension larger outside . this physics is observed on a frame stationary with respect to the black hole . the question naturally arises ; what physics is accessed by the observer falling through the event horizon on an inertial frame ? this question is important for the black hole small enough to exhibit fluctuations comparable to its scale . a sufficiently small quantum black hole will be composed of strings in a superposition of interior and exterior configurations or states . the motion of a string onto a black hole approximates the dynamics of a string in a rindler wedge . the rindler wedge is defined by the frame of an accelerated observer , which is equivalent to the frame of a stationary observer near a black hole event horizon . the observer witnesses the final emission of radiation by the string just above the event horizon , where upon the string becomes frozen eternally on the particle horizon . of course in the rindler wedge case the string proceeds onwards on its geodesic or string world sheet with no apparent change due to this observed state of affairs . this is approximated as well with the black hole , where the string passes through the event horizon unaffected so long as the radius of curvature is much smaller than the string length . this picture persists until the string approaches the center or singularity of the black hole . at this point the rindler wedge model departs from reality . the interior perspective or the physics of a string as measured by an observer falling with the string , is outside the domain of the holographic principle . once the string passes through the event horizon it evolves on a domain of causal support not included in the data set accessible to an observer on an accelerated frame stationary with respect to the black hole . the observer that falls through the event horizon observes the further evolution of the string beyond the frozen state the stationary observer finds as its final state . further , the string evolves into a different state as it approaches the singularity . there the string will begin to experience a rapidly growing weyl curvature . the stationary observer measures transverse modes of the string on the black hole horizon , while longitudinal coordinates are compressed to near the planck length . the observer falling in with the string will witness the string distended by the growing weyl curvature in the direction of motion . consequently , this observer witnesses the extension of longitudinal extension of the string . the frozen state of the string measured by the exterior observer is cancelled by hawking radiation which escapes later . the string is entangled with the black hole , and there is a superposition of configuration spaces for the string ; the exterior and interior configuration variables . this may put a new twist on the holographic result that events in spacetime do not have the a realism as understood classically . quantum mechanics removes a measure of reality with the existence of incompatible measurements of observables which exist in incommensurate commuting sets of operators . the invariant interval , or event , is left as something which has an ontology or “realism . ” however , this indicates that a realism to any event is not supportable on fundamental grounds . further , superposition of exterior and interior states of a black hole prevents a sharp distinction between pre-selected and post-selected states . this means that cosmic censorship , chronology protection are aspects of “classical reality , ” where underneath the ontology of ordered events or their invariant meaning is lost . the creation of the universe from nothing might involves something called the biverse . the de sitter spacetime is a solution of the scale factor $a ( t ) ~=~\sqrt{\frac{3}{\lambda}}cosh ( t\sqrt{\frac{\lambda}{3}} ) $ , which has this strange backwards part of the hyperboloid . hawking and others have considered the idea of a universe connected at this minimal scale factor as two universes which are time reversed . it is possible that this is an elementary model for how a “blob” of vacuum energy in a spacetime cosmology quantum tunnels across a potential barrier to form a nascent spacetime cosmology . the blog of vacuum energy is annihilated at one side of the potential by its “opposite” ( other half of the biverse ) , where the “opposite” is annihilated at the other side of the potential by the occurrence of the vacuum blob . this is similar to the tunneling of an electron across a barrier , where we can think of it as annihilated by a positron traveling backwards in time , which in turn is connected to an electron on the other side of the potential barrier . at the end of the day one might question what are the role of the hawking-penrose energy conditions , such as the averaged weak energy condition $t^{00}~\ge~0$ . it makes sense that the gravitational states are the hartle-hawking states which are degenerate and zero . so ultimately the physical states of the universe globally are zero , or in other words the net total of everything is zero .
given a vector $$\vec{v} = \begin{pmatrix} x \\ y \\ z \end{pmatrix} $$ and a general rotation axis $$ \vec{k} = \begin{pmatrix} k_x \\ k_y \\ k_z \end{pmatrix} $$ then the resulting vector after a rotation by $\theta$ is $$\vec{u} = \vec{v}\cos\theta + \left ( \vec{k} \times \vec{v} \right ) \sin \theta + \vec{k} \left ( \vec{k} \cdot \vec{v} \right ) ( 1-\cos \theta ) $$ where $\times$ is the vector cross product and $\cdot$ the dot product . if you are looking for the angle to rotate , then look up angle between two vectors . the the rotation axis is defined by the cross product of the original vector and the target vector . i am not sure how familiar you are with C# but there is the code to do what i think you are asking .
when i use the linear equation , i have always written it as $r_{2}=r_{1} ( 1+\alpha \ \delta t ) $ . for this problem , $r_{1}=100\omega$ and $r_{2}=200\omega$ . also substituting the value for $\alpha$ will give you the change in temperature from 100 degrees . so remember to add the 100 to the $\delta t$ for the complete answer . regarding why you got different answers , it is because linear approximations are linearized about a point and do not always hold . the question indicates that you are to linearize about $100^{\circ}$ . keep in mind that a slope of $0.005\ /^{\circ}c$ is 50% per $100\ ^{\circ}c$ , which equates to $50\ \omega\ per\ 100\ ^{\circ}c$ . if you start at $100\omega$ at $100^{\circ}$ , if you get $100^{\circ}$ colder you will lose $50\omega$ . this gives a different result at 0 than what you had .
whether the conformal symmetry is local or global depends on the theory ! more precisely , the symmetry that may be local is not really conformal symmetry but ${\rm diff}\times {\rm weyl}$ . for example , in all the cfts we use in the ads/cft correspondence , for example the famous ${\mathcal n}=4$ gauge theory in $d=4$ , the conformal symmetry is global – and , correspondingly , it is a physical symmetry with nonzero values of generators . this is related to the fact that the cft side of the holographic duality is a non-gravitational theory so it avoids all local symmetries related to spacetime geometry . the previous paragraph holds even if the dimension of the cft world volume is $d=2$ . in $d=2$ , it may happen that the global conformal symmetry is extended to the infinite-dimensional local symmetry where $\omega ( x ) $ depends on the location . however , such an enhancement looks " automatic " only classically . quantum mechanically , a nonzero central charge $c\neq 0$ prevents one from defining the general local conformal transformations . in all the cfts from ads/cft , we have $c\geq 0$ . such a nonzero $c$ leads to the " conformal anomaly " ( proportional to the world sheet ricci scalar and $c$ ) . on the contrary , the world sheet $d=2$ cft theories used to describe perturbative string theory always have a local diffeomorphism and local weyl symmetry . this is needed to decouple all the unphysical components of the world sheet metric tensor ; and a necessary condition is the incorporation of the conformal ( and other ) ghosts so that in the critical dimension , we have the necessary $c=0$ . we say that the world sheet cft is " coupled to gravity " as we add the world sheet metric tensor , the diff symmetry , and the weyl symmetry . the weyl symmetry is the symmetry under a general scaling of the world sheet metric by $\omega ( x ) $ that depends on the location on the world sheet . one may gauge-fix this local weyl symmetry along with the 2-dimensional diffeomorphism symmetry , e.g. by demanding the $\delta_{ij}$ form of the metric tensor . this gauge-fixing still preserves some residual symmetry , a subgroup of the originally infinite-dimensional " diff times weyl " symmetry . this residual symmetry is nothing else than the infinite-dimensional conformal symmetry generated by $l_n$ and $\tilde l_n$ . because its being infinite-dimensional , we may call it a local conformal symmetry but it is really just a residual symmetry from " diff times weyl " . the global $sl ( 2 , c ) \sim so ( 3,1 ) $ global subgroup is the mobius group generated by $l_{0 , \pm 1}$ and those with tildes , too . as far as i know , this local conformal symmetry is a special case of some $d=2$ theories . in higher dimensions , the weyl and diff are not enough to kill all the components of the metric tensor and the " partially killed " theories with a dynamical metric are still inconsistent as the usual naively quantized versions of general relativity . in all the cases above and others , it is true that the local symmetries – where the parameter $\omega ( x ) $ is allowed to depend on time and space coordinates ( if the latter exist ) – are gauge symmetries ( in the sense that the generators are obliged to annihilate physical states ) while the global symmetries are always " physical " in your sense of the charge 's being nonzero . these equivalences follow from some easy logical argument . when you have infinitely many generators of the ( space ) time-dependent symmetry transformations , it follows that all the quanta associated with these generators exactly decouple – have vanishing interactions – with the gauge-invariant degrees of freedom . so we always study the physical part of the theory only , and it is the theory composed of the gauge symmetry 's singlets . greetings to david .
analysis for jesus 's molecule usage : our breathing rate changes a lot , but on average its about 1 breath every five seconds , or 12 breaths a minute , or 720 breaths an hour , or 17280 breaths a day or 6,307,200 breaths a year , and if we live for 32 years that gives us 201,830,400 breaths in his lifetime . how many atoms ? multiply 2.02e8 total breaths x 1.61e23 molecules per breath to get a total of 3.25e31 total molecules . . . . that means for jesus , there were 32,500,000,000,000,000,000,000,000,000,000 molecules ( 325 decillion ) that came into contact with his lungs during his lifetime . i did not see anything wrong with the author 's approach , so say 6,307,200 * 1.61e23 molecules/year ( based on 6 liters of air/breath not oxygen ) . since you want $o_2$ only then scale that down . at stp ( standard temperature and pressure : 1 atmosphere of pressure , 0c ) , one mole of any gas will occupy 22.4 liters of space . one mole of any substance contains 6.02 x 1023 particles . air is about 21% oxygen making 1 liter of air to be about 0.21 liters of oxygen . to find out how many moles that represents=0.21 / 22.4 . then use avogadro 's number 6.02 x 10^23 atoms/mole . 1 l = 5.64e21 atoms or 2.82e21 molecules/l . 6l/breath => 1.69e22 molecules/breath 6,307,200 breaths a year => 1.07e29 molecules of $o_2$/year . how long are you going to live * 1.07e29 = $o_2$ passing into your lungs . the only two caveats to this answer : some of these molecules will not be unique and 6l represents gas going into the lungs , not what is utilized . say you live 80 years , that is ~8.5e30 molecules of $o_2$ .
the first thing you need to get to grips with is that particles are waves . this can be shown with a simple experiment called the double slit experiment , which i will attempt to explain . imagine a water wave travelling across a tank . then imagine you place a wall in the middle of the tank , and place two thin slits in it . if you create a wave ( by dropping a stone etc ) on one side of the wall , it will travel through the two slits and interfere like this . the double slit experiment does the same thing , but for light . if you have a wall with two slits in it and shine a beam of light through the slits onto a flat screen behind , you can see a similar interference pattern on the screen . this shows that light acts as a wave . now imagine that rather than a beam of light you can create a steady stream of electrons . electrons are a small " fundamental " particle ( "fundamental " means they cannot be broken down into smaller components ) . if you point your electron stream at your two slits you will see a very similar interference pattern as before ! until this experiment was done it was believed that electrons were solid particles ( like billiard balls ) , but this showed that they also act as a wave ! since we have shown that particles can also show wave-like properties , can we show that waves can have particle-like properties ? it was shown by einstein and arthur compton that light can in fact be shown to be made up of particles , due to the fact that light must have momentum . this is known as wave-particle duality . as i said at the beginning , waves and particles are the same thing . there are some " waves " like electromagnetic waves which make particles move . these are only called " waves " because it is easier to model and calculate that way . it is possible to describe the interaction as 2 ( or more ) particles ( but it is considerably more difficult ) . i hope this answers your question .
i ) first of all , one should never use the dirac bra-ket notation ( in its ultimate version where an operator acts to the right on kets and to the left on bras ) to consider the definition of adjointness , since the notation was designed to make the adjointness property look like a mathematical triviality , which it is not . see also this phys . se post . ii ) op 's question ( v1 ) about the existence of the adjoint of an antilinear operator is an interesting mathematical question , which is rarely treated in textbooks , because they usually start by assuming that operators are $\mathbb{c}$-linear . iii ) let us next recall the mathematical definition of the adjoint of a linear operator . let there be a hilbert space $h$ over a field $\mathbb{f}$ , which in principle could be either real or complex numbers , $\mathbb{f}=\mathbb{r}$ or $\mathbb{f}=\mathbb{c}$ . of course in quantum mechanics , $\mathbb{f}=\mathbb{c}$ . in the complex case , we will use the standard physicist 's convention that the inner product/sequilinear form $\langle \cdot | \cdot \rangle$ is conjugated $\mathbb{c}$-linear in the first entry , and $\mathbb{c}$-linear in the second entry . recall riesz ' representation theorem : for each continuous $\mathbb{f}$-linear functional $f : h \to \mathbb{f}$ there exists a unique vector $u\in h$ such that $$\tag{1} f ( \cdot ) ~=~\langle u | \cdot \rangle . $$ let $a:h\to h$ be a continuous$^1$ $\mathbb{f}$-linear operator . let $v\in h$ be a vector . consider the continuous $\mathbb{f}$-linear functional $$\tag{2} f ( \cdot ) ~=~\langle v | a ( \cdot ) \rangle . $$ the value $a^{\dagger}v\in h$ of the adjoint operator $a^{\dagger}$ at the vector $v\in h$ is by definition the unique vector $u\in h$ , guaranteed by riesz ' representation theorem , such that $$\tag{3} f ( \cdot ) ~=~\langle u | \cdot \rangle . $$ in other words , $$\tag{4} \langle a^{\dagger}v | w \rangle~=~\langle u | w \rangle~=~f ( w ) =\langle v | aw \rangle . $$ it is straightforward to check that the adjoint operator $a^{\dagger}:h\to h$ defined this way becomes an $\mathbb{f}$-linear operator as well . iv ) finally , let us return to op 's question and consider the definition of the adjoint of an antilinear operator . the definition will rely on the complex version of riesz ' representation theorem . let $h$ be given a complex hilbert space , and let $a:h\to h$ be an antilinear continuous operator . in this case , the above equations ( 2 ) and ( 4 ) should be replaced with $$\tag{2'} f ( \cdot ) ~=~\overline{\langle v | a ( \cdot ) \rangle} , $$ and $$\tag{4'} \langle a^{\dagger}v | w \rangle~=~\langle u | w \rangle~=~f ( w ) =\overline{\langle v | aw \rangle} , $$ respectively . note that $f$ is a $\mathbb{c}$-linear functional . it is straightforward to check that the adjoint operator $a^{\dagger}:h\to h$ defined this way becomes an antilinear operator as well . -- $^{1}$we will ignore subtleties with discontinuous/unbounded operators , domains , selfadjoint extensions , etc . , in this answer .
i love manly arguments ! from this hail mary pass , it looks like , with initial angle 45deg and velocity $\sim 30$ m/s , the best football pass can go 70 yards in 4 sec hang time . if he had thrown it straight up , the maximum height would have been about 50 yards , just barely making it . give him a running start and no 300-lb defenders to worry about , and i say it is possible ! best nfl punts have hang time of 6-8 secs , so they also would have just made it .
you may just not bother to use a test function , here . this problem is so easy you can work it all just using the properties of the commutator . $$ [ xp_y , x ] =x [ p_y , x ] + [ x , x ] p_y$$ now $ [ p_y , x ] $ vanishes because of the fundamental commutation relation between $p_i$ and $x_i$ which is $$ [ p_i , x_j ] = -i\hbar \delta_{ij}$$ on the other hand $ [ x , x ] =0$ because anything commmutes with itself .
$2\pi\omega \cdot a = v_{max}$ so try $v_{max}=\lambda \omega=a \cdot 2\pi\omega$ . solve for what you want , $a=\frac{\lambda}{2\pi}$ where $\lambda$ is different for each wave , as i enumerated in the comments , $\lambda_a=2$m and $\lambda_b=4$m . i hope i answered your question right from what you are telling me . i will stress that in order to get a solid response , post your full question in clear terms and make sure you post everything you know and tried . try to focus it down to conceptual questions . we want to help , but we also do not want to explicitly do your homework . hope this helps , cheers .
i am happy to know that you are interested in space and the nature of time , your questions indicate that you dont have a very clear understanding of what is already understood about the subject , but show an interested student and potentially a good one . people talk about the expansion of the universe and black holes and all these things , but to talk about them correctly you need to learn how to talk about them . i would highly recommend einstein 's paper , " on the electrodynamics of moving bodies " . ( available online ) please look into it , if you have questions post them on this forum . saying that " is time the rate at which one moves through space " is not correct . one definition that can serves our purposes is that time is what is measured by clocks . to understand the behaviour of moving clocks in relation to stationary clocks , you have to understand this concept called the proper time . the concept of proper time is a generalization of the distance between 2 points you learn in high school . implication of the theory of relativity is that clocks measure proper time . definition of proper time is $$tp = ( c^2 dt^2-dx^2 ) ^{1/2}$$ where dt is the time elapsed and dx is the distance moved . ( it must be summed by dividing path into small segments but it does not matter if you move with constant velocity ) . now we know for something moving at the speed of light $$x=ct . $$ implies that $$tp = ( c^2t^2 - c^2 t^2 ) ^{1/2} = 0$$ which is why clocks moving at the speed of light dont show the movement of time . another way to understand this , is a clock moving at the speed of light shows the same time when you see it .
if you search the iter site , iter being the international prototype fusion reactor which will demonstrated the possibility of getting megawat useful energy from fusion , one sees that their main aim is to demonstrate this feasibility : the main carrier of energy out of the plasma is the neutron , and methods to efficiently use this energy have not been developed yet , but wait for the commercial prototype . the helium nucleus carries an electric charge which will respond to the magnetic fields of the tokamak and remain confined within the plasma . however , some 80 percent of the energy produced is carried away from the plasma by the neutron which has no electrical charge and is therefore unaffected by magnetic fields . the neutrons will be absorbed by the surrounding walls of the tokamak , transferring their energy to the walls as heat . in iter , this heat will be dispersed through cooling towers . in the subsequent fusion plant prototype demo and in future industrial fusion installations , the heat will be used to produce steam and—by way of turbines and alternators—electricity . they have developed methods for cooling the system and dissipating the energy to the environment .
there is a mistake in your diagram , in that you drew the x-axis for body one in the rest frame of body 3 as perpendicular in the euclidean sense to the t-axis of body one . the two lines are not euclidean perpendicular , but minkowski perpendicular . the t axis is correct for body 1 in the second diagram , but the x-axis for body 1 slopes up , not down . it slopes up by the same amount as the t-axis for body 1 slopes to the right . this looks awkward in a euclidean geometry diagram , but it is correct minkowski geometry . the direction of the slope is fixed by einstein 's argument about simultaneity at a distance , reproduced in this answer : einstein&#39 ; s postulates &lt ; ==&gt ; minkowski space . ( in layman&#39 ; s terms ) . this argument also fixes the relative non-simultaneity of e1 and e2 . when you use the correct notion of perpendicularity , you will find that t ( e2 ) is bigger than t ( e1 ) , not smaller . but they are indeed non-simultaneous , as you note .
in theories with spontaneous symmetry breaking , the phase transition can usually be characterized by a local order parameter $\delta ( x ) $ , which is not invariant under the relevant symmetry group $g$ of the hamiltonian . the expectation value of this field has to be zero outside the ordered phase $\langle\delta ( x ) \rangle = 0$ , but non-zero in the phase $\langle\delta ( x ) \rangle \neq 0$ . this shows that there has been a spontaneous breaking of $g$ to a subgroup $h\subset g$ ( where $h$ is the subgroup that leaves $\delta ( x ) $ invariant ) . what local means in this context , is usually that $\delta ( x ) $ at point $x$ , can be constructed by looking at a small neighborhood around the point $x$ . here $\delta ( x ) $ can be dependent on $x$ and need not be homogeneous . this happens for example when you have topological defects , such as vortices or hedgehogs . one powerful feature of these landau-type phases , is that there will generically be gapless excitations in the system corresponding to fluctuations of $\delta ( x ) $ around its expectation value $\langle\delta ( x ) \rangle$ in the direction where the symmetry is not broken ( unless there is a higgs mechanism ) . these are called goldstone modes and their dynamics are described by a non-linear $\sigma$-model with target manifold $g/h$ . an example is the order parameter for s-wave superconductors $\langle\delta ( x ) \rangle = \langle c_{\uparrow} ( x ) c_{\downarrow} ( x ) \rangle$ , which breaks a $u ( 1 ) $ symmetry down to $\mathbb z_2$ . but there are no goldstone modes due to the higgs mechanism , the massive amplitude fluctuations are however there ( the " higgs boson" ) . [ edit : see edit2 for correction . ] a non-local order parameter does not depend on $x$ ( which is local ) , but on something non-local . for example , a non-local ( gauge-invariant ) object in gauge theories are the wilson loops $w_r [ \mathcal c ] = \text{tr}_r{\left ( \mathcal pe^{i\oint_{\mathcal c}a_\mu\text dx^\mu}\right ) } , $ where $\mathcal c$ is some closed curve . the wilson loop thus depends on the whole loop $\mathcal c$ ( and a representation $r$ of the gauge group ) and cannot be constructed locally . it can also contain global information if $\mathcal c$ is a non-trivial cycle ( non-contractible ) . it is true that topological order cannot be described by a local order parameter , as in superconductors or magnets , but conversely a system described by a non-local order parameter does not mean it has topological order ( i think ) . the above mentioned wilson loops ( and similar order parameters , such a the polyakov and ' t hooft loop ) , is actually a order parameter in gauge theories which probe the spontaneous breaking of a certain center-symmetry . this characterizes the deconfinement/confinement transition of quarks in qcd : in the deconfined phase $w_r [ \mathcal c ] $ satisfies a perimeter law and quarks interact with a massive/yukawa type potential $v ( r ) \sim \frac{e^{-mr}}r$ , while in the confined phase it satisfy an area law and the potential is linear $v ( r ) \sim \sigma r$ ( $\sigma$ is some string tension ) . there might be other examples of spontaneous symmetry breaking phases with non-local order parameter . [ edit : see edit2 . ] let me just make a few comments about topological order . in theories with with spontanous symmetry breaking , long-range correlations are very important . in topological order the systems are gapped by definition , and there is only short-range correlation . the main point is that in topological order , entanglement plays the important role not correlations . one can define the notion of long-range entanglement ( lre ) and short-range entanglement ( sre ) . given a state $\psi$ in the hilbert space , loosely speaking $\psi$ is sre if it can de deformed to a product state ( zero entanglement entropy ) by locally removing entanglement , if this is not possible then $\psi$ is lre . a system which has a ground state with lre is called topological order , otherwise its called the trivial phase . these phases have many characteristic features which are generally non-local/global in nature such as , anyonic excitations/non-zero entanglement entropy , low-energy tqft 's , and are characterized by so-called modular $s$ and $t$ matrices ( projective representations of the modular group $sl ( 2 , \mathbb z ) $ ) . note that , unlike popular belief , topological insulators and superconductors are sre and are not examples of topological order ! if one requires that the system must preserve some symmetry $g$ , then not all sre states can be deformed to the product state while respecting $g$ . this means that sre states can have non-trivial topological phases which are protected by the symmetry $g$ . these are called symmetry protected topological states ( spt ) . topological insulators/superconductors are a very small subset of spt states , corresponding to restricting to free fermionic systems . unlike systems with lre and thus intrinsic topological order , spt states are only protected as long as the symmetry is not broken . these systems typically have interesting boundary physics , such as gapless modes or gapped topological order on the boundary . characterizing them usually requires global quantities too and cannot be done by local order parameters . edit : this is a response to the question in the comment section . i am not sure whether there are any reference which discuss this point explicitly . but the point is that you can continuously deform/perturb the hamiltonian of a topological insulator ( while preserving the gap ) into the trivial insulator by breaking the symmetry along the way ( they are only protected if the symmetry is respected ) . this is equivalent to locally deforming the ground state into the product state , which is the definition of short range entanglement . you can find the statement in many papers and talks . see for example the first few slides here . or even better , see this ( slide with title " compare topological order and topological insulator " + the final slide ) . let me make another comment regarding the distinction between intrinsic topological order and topological superconductors , which at first seems puzzling and contrary to what i just said . as was shown by levin-wen and kitaev-preskill , the entanglement entropy of ground state for a gapped system in 2+1d has the form $s = \alpha a - \gamma + \mathcal o ( \tfrac 1a ) $ , where $a$ is the boundary area ( this is called the area law , not the same area law i mentioned in the case of confinement ) , $\alpha$ is a non-universal number and $\gamma$ is universal and called the topological entanglement entropy ( tee ) . what was shown in the above papers is that the tee is equal to $\gamma = \log\mathcal d$ , where $\mathcal d\geq 1$ is the total quantum dimension and is only strictly $\mathcal d&gt ; 1$ ( $\gamma\neq 0$ ) if the system supports anyonic excitations . modulo some subtleties , lre states always have $\gamma\neq 0$ , which in turn means that they have anyonic excitations . conversely for sre states $\gamma = 0$ and there are no anyons present . this seems to be at odds with the existence of ' majorana fermions ' ( non-abelian anyons ) in topological superconductors . the difference is that , in the case of topological order you have intrinsic finite-energy excitations which are anyonic and the anyons correspond to linear representations of the braid group . while in the case of topological superconductors , you only have non-abelian anyons if there is an extrinsic defect ( vortex , domain wall etc . ) which the zero-modes can bind to , and they correspond to projective representation of the braid group . the latter type anyons from extrinsic defects can also exist in topological order , but intrinsic finite-energy ones only exist in topological order . for more details , see the recent set of papers from barkeshli , jian and qi . edit2: please see my comments below for some corrections and subtleties . such as , it is in a sense not correct that superconductors are described by a local order parameter . it only appears local in a particular gauge . superconductors are actually examples of topological order , which is rather surprising .
the obvious answer is their mass , accelerating a subatomic particle to relativistic velocity takes many orders of magnitude less energy than accelerating a macroscopic particle . more subtly , we can only accelerate subatomic particles that we can manipulate with electromagnetic fields , nobody has a proposed viable experiments to study relativistic neutron or neutrino reactions because we really have no way to manipulate them . what this means is that we need relatively light , charged , particles that we can accelerate to relativistic velocity in a reasonable space with reasonable amount of energy . which brings us to our next point . modern accelerators use what are called " rf cavities " to accelerate charged particles like protons and electrons , these are effectively giant pulsed capacitors with the particle flying in the space between the plates . we are governed by material science and the permiativity of the vacuum and have a limit to the maxium voltage we can put across the plates , and hence the maximum energy we can impart to a particle per unit length of the cavity . for modern accelerators this is on the order of 1 mev per cm . 1 mev on an electron goes a long way ( which is why your tv works ) . 1 mev on a baseball barely does anything . if the lhc seems big , an accelerator to collide two baseballs with the same velocity would be wider than the solar system ( using modern technology ) . more realistically though , we accelerate particles so we can study the physics during collisions of two particles . when we collide electrons and positrons for instance , nearly 100% of the energy carried by the particle is deposited into the reaction if they collide head on , its this fact that lets us see the creation of new and exotic particles following the collision ( and why lep began seeing z bosons as soon as they crossed the 180gev threshold ) . when we move to the lhc for example , we are colliding two protons- which , roughly speaking are sacks of consisting of 3 quarks . and the momentum ( and therefore the energy ) of the proton is more or less evenly distributed among the three quarks . so the energy available to us in the collision is actually about 6 times less than an eqiuivalent electron-positron collider . imagine doing this with a baseball , which is composed of trillions of trillions of particles . the energy that each particle carries is a tiny fraction of the total energy of the baseball , therefore every single collision event only has a tiny fraction of the total energy of the system , and in the end we get no useful reaction out of a collision that could not have just as easily been accomplished by putting two televisions face to face .
let 's look at the relationship between momentum and energy . as you know , for a mass $m$ kinetic energy is $\frac12mv^2$ and momentum is $mv$ - in other words energy is $\frac{p^2}{2m}$ now to counter the force of gravity we need to transfer momentum to the air : $f\delta t = \delta ( mv ) $ the same momentum can be achieved with a large mass , low velocity as with small mass , high velocity . but while the momentum of these two is the same , the energy is not . and therein lies the rub . a large wing can " move a lot of air a little bit " - meaning less kinetic energy is imparted to the air . this means it is a more efficient way to stay in the air . this is also the reason that long thin wings are more efficient : they " lightly touch a lot of air " , moving none of it very much . trying to replicate this efficiency with an engine is very hard : you need compressors for it to work at all ( so you can mix air with fuel and have the thrust come out the back ) and this means you will have a small volume of high velocity gas to develop thrust . that means a lot of energy is carried away by the gas . think about the noise of an engine - that is mostly that high velocity gas . now think of a glider : why is it so silent ? because a lot of air moves very gently . i tried to stay away from the math but hope the principle is clear from this .
we start with the definition $$\tag{1} s^{\alpha \beta}~:=~u^\alpha v^\beta-u^\beta v^\alpha . $$ indices are raised and lowered with the metric . up to an overall factor , one has $$\tag{2} \bar{s}_{\alpha \beta}~\propto~ \epsilon_{\alpha \beta \gamma \delta} s^{\gamma \delta} , $$ so that the matrix trace $$ \mathrm {tr} ( \mathbf{\bar{s}\cdot s } ) ~=~ \bar{s}_{\alpha \beta} s^{\beta\alpha} ~\stackrel{ ( 2 ) }{\propto}~ \epsilon_{\alpha \beta \gamma \delta} s^{\gamma \delta}s^{\beta\alpha} ~\stackrel{ ( 1 ) }{\propto}~ \epsilon_{\alpha \beta \gamma \delta} u^{\gamma}v^{\delta}u^{\beta}v^{\alpha}$$ $$\tag{3} ~\stackrel{ ( 4 ) }\propto~ \det [ \mathbf{u , v , u , v} ] ~=~0$$ is just the determinant of the $4\times 4$ matrix with column vectors $\mathbf{u , v , u , v}$ . this is zero , because the determinant is totally antisymmetric in its column vector entries . -- ( 4 ) : see e.g. wikipedia .
the most general relationship is $$c ( b ) = \frac{\int_0^b \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b}{\int_0^\infty \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b} = \frac{1}{\sigma_\text{inel}}\int_0^b \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b\tag{1}$$ ( source , one of many ) . in practice , we usually use the glauber model to describe heavy ion collisions , and this model predicts an impact parameter dependence of the differential cross section which can be ( very roughly ) approximated as $$\frac{\mathrm{d}\sigma}{\mathrm{d}b} \approx \begin{cases}2\pi b , and b \le b_\text{max} \\ 0 , and b &gt ; b_\text{max}\end{cases}$$ where $\pi b_\text{max}^2 = \sigma_\text{inel}$ . that reduces equation ( 1 ) to $$c ( b ) = \frac{\pi b^2}{\sigma_\text{inel}}$$ for $b &lt ; b_\text{max}$ . you do have to be careful because sometimes ( rarely ) a different definition is used , $c ( b ) = 1 - \pi b^2/\sigma_\text{inel}$ . just pay attention to whether large centrality values correspond to peripheral ( the former definition ) or central ( the latter ) collisions . in practice , this is all somewhat approximate anyway , because you can not definitively identify the centrality of a collision from the information collected by a detector . all you can do is estimate the centrality based on how many particles come out and how strongly they are scattered . if you get a lot of particles coming out roughly perpendicular to the beamline ( pseudorapidity $\eta\sim 0$ ) , then that means a lot of nucleons were involved in the collision , and thus it is characterized as central . if there are few particles coming out perpendicular to the beamline , then few nucleons were scattered , meaning the collision was peripheral .
i thought that since i brought this up and i did not get any good explanations , i would answer it myself . plugging in some numbers helps . i used this for light speed = 299,792.458 km/sec . hubble constant = 72 km/sec/megaparsec . the hubble constant is necessarily an approximation because nobody knows exactly what it is and they also do not know if it is a constant through time . the hubble constant is used to calculate the speed of distant galaxies due to the expansion of the universe . so based on some simple calculations using the above numbers , i come up with this : if you go out 13,573,936,292 light years , the universe is expanding at 0.99999999994 of the speed of light . i might point out that this is about 203 feet per hour slower than the speed of light so it is pretty close . using the lorentz transform ( relativity ) we can figure out the effect on a galactic mass moving at that speed away from us . suppose we observe a galaxy about the size of the milky way or andromeda moving away from us at this speed . the rest mass of the milky way or andromeda is roughly 700 billion solar masses . give or take . applying the lorentz transform to this observation , the rest mass of the distant receding galaxy would be about 7,493,550 solar masses . so even at speeds this close to the speed of light , this is still a large mass . it is not a tiny particle as i thought it might be . therefore we do not even have to use the hand waving argument about the expansion of the universe does not really count in relativistic calculations . i guess the real theoretical issue and question would be : what happens at the singularity ? that is right at the outer edge of the expansion , objects are traveling exactly at the speed of light , and here relativity breaks down . but nobody knows the answer to that . the interesting thing about plugging in the numbers is that when you are really close to light speed , there are no contradictions . and my calculations are really close because 203 feet per hour is a lot slower than i walk . i could crawl faster than that .
this answer contains some additional resources that may be useful . please note that answers which simply list resources but provide no details are strongly discouraged by the site 's policy on resource recommendation questions . this answer is left here to contain additional links that do not yet have commentary . b . falkenburg , particle metaphysics , springer , 2007 . also by t.y. cao ( btw , nitincr 's suggestion is a compilation of articles by various authors from a number of points of view ) , conceptual developments of 20th century field theories , cambridge up , 1997 . s.y. auyang , how is quantum field theory possible , oxford up , 1995 . try the stanford encyclopedia of philosophy entry on qft , which includes ( not too ) many citations to books and papers . for conceptual issues on physics generally , this is one place to look . look at tom banks ' book modern quantum field theory : a concise introduction it is not a book , but it might be helpful : the conceptual basis of quantum field theory by gerard ' t hooft . the file is on his web page . there is a book , edited by t.y. cao , a rather notable philosopher of science at boston university , that might be of interest . the title is " conceptual foundations of quantum field theory " . a . zee 's " quantum field theory in a nutshell " is also a very good reference . the series by e . zeidler are really worth reading through . credit : peter morgan , and deleted answers by nitincr , curious george and physicists .
what you are seeing are artifacts caused by the hard-wall type kernel you are using . these patterns consist of horizontal and vertical streaks with a definite periodicity , which gets more pronounced as you make the averaging box bigger . the streaks occur because the fourier transform of a hard-wall box has zeros at certain wavenumbers . to get rid of them , you can average with an approximately gaussian kernel , described below . when you apply an averaging , you are multiplying the fourier transform of the noise by the fourier transform of the averaging kernel . this is called the " convolution theorem " , and " convolution " is just a term for averaging with a kernel . in fourier space , convolution is just multiplication . in your case , you are convolving with a box function , which is the function $f ( x , y ) = 1 $ for |x|&lt ; 5 |y|&lt ; 5 , and f ( x , y ) =0 otherwise . this box function is the product of two step boxes : $$ f ( x , y ) = h ( x ) h ( y ) $$ where h ( x ) is the function which is 1 for |x|&lt ; 5 and 0 otherwise . the fourier transform of this box function ( for continuous x ) is $$ \tilde{h} ( k ) = {\sin ( 5k ) \over k} $$ which has zeros when $k = \pm {\pi\over 5}$ , $k= \pm {2\pi\over 5}$ , $k=\pm{3\pi\over 5}$ , $k=\pm{4\pi\over 5}$ . this formula is not right , it is making the continuum approximation that 5 is approximately infinite , but the zeros are in the same place for the correct fourier transform , and this is all i am using . the fourier space for a lattice repeats outside the region $-\pi$ to $\pi$ , so these are all eight zeros . the fourier transform of f is the product of the fourier transform in x and in y : $$\tilde{f} = \tilde{h} ( k_x ) \tilde{h} ( k_y ) $$ this has zeros on eight vertical lines , and eight horizonal lines . the fourier transform of the random noise is another random noise in fourier space ( this is only strictly true for gaussian noise , you are probably using a uniform random number between -1 and 1 . the difference does not matter for the purposes of this discussion ) . when you multiply by the fourier transform of the kernel , you get a random product , but you get zero on the special lines . these zeros produce the horizontal/vertical streaks . you should be able to generate a similar pattern just by generating random noise , zeroing out the eight lines , and fourier transforming . the proper pattern also modulates the fourier stuff away from the zeros by a smooth amplitude , but this is probably less noticible . to get rid of this , you can use a kernel which falls off smoothly , so that it does not have zeros in the fourier transform , but falls off smoothly . the easiest is a gaussian kernel : $$f ( x , y ) = e^{-{ ( x^2+y^2 ) \over 2a^2}}$$ where a is the analog of 1,2,3,4,5 . if you do not want to use this , you can use anything smooth that falls off without sharp corners .
the tristimulus values for ciexyz can be obtained by integrating the spectral curve with three distinct weight curves . from xyz there are documented formulas to convert to lab , hsl , csv , . . . http://www.zeiss.com/c12567bb00549f37/contents-frame/80bd2fe43b50aa3ec125782c00597389 http://en.wikipedia.org/wiki/cie_1931_color_space http://en.wikipedia.org/wiki/lab_color_space#cie_xyz_to_cie_l.2aa.2ab.2a_.28cielab.29_and_cielab_to_cie_xyz_conversions
it is happening because of the acceleration of the earth orbital speed around the sun ( earth is near the perihelion ) . between december 13 and december 31 the earth is speeding up and also it is normally rotating around its axis . these 2 movements ( constant rotation and increasing orbital speed ) add up to create the observed apparent movement of the sun on the earth sky . the sun rises later and also sets later every day . it is a bit tricky to visualize , so try it with a globe .
i think it is a question how hard you can suck the water in . the force $f$ you need to accelerate the water column depends on the mass of water : $$f=mg . $$ and the mass depends on the density $\rho$ and the volume $v=ha$ with the length $h$ and the surface area inside the straw $a$ . so , the force you need to accelerate the water column is proportional to the height of the water column : $$f=\rho a h g . $$ if you create a complete vaccum the maximum force $f_{max}$ acting on the surface is $p a$ . then following applies : $$pa=\rho a h g \\ \leftrightarrow h = \frac{p}{\rho g} $$
newton 's cradle fights air resistance and restitution ( efficiency of rebound ) . you then want the highest density , hardest balls with the highest restitution . cobalt-sintered tungsten carbide is magnetic . polished hardened tool steel ball bearings are a good start . http://www.wired.com/wiredscience/2011/10/what-went-wrong-with-the-mythbusters-newton-cradle/ platinum plus gallium and indium alloys heat treat to a very hard and springy state , density around 19 g/cm^3 versus less than 8 g/cm^3 for tool steel ( steve kretchmer , niessing co . , eastern smelting ; platinum sk ( tm ) alloys ) . for more shallow wallets , tungsten steel , thoriated tungsten . 95.5% pt , 3.0% ga , 1.5% in ; 95.2% platinum , 4.8% ga , in , cu ; 1550-1650 c melt . 700 c for 30 minutes and slow cool to harden ( not reducing atmospheres ) . vickers hardness 318/rockwell a 76/rockwell c 32 . 125,000 psi tensile , 104,000 psi yield .
if you say that earth 's velocity around the sun is 67,000 mi/h , your reference point is the sun itself , which makes the aeroplane 's velocity 68,000 mi/h , not 1000 . using special relativity only , and ( a ) observing from the sun , a clock on the plane would seem to run slower than a clock on earth . a person ( b ) on earth would measure also measure an aeroplane 's clock to be slower , but by a different factor . time dilation , $ \delta t ' = \frac{\delta t}{\sqrt{1-\frac{v^2}{c^2}}} $ ( a ) observation from the sun : time dilation of aeroplane clock vs earth clock , ( i ) $\frac{\delta t}{\sqrt{1-\frac{68000^2}{c^2}}}$ and ( ii ) $\frac{\delta t}{\sqrt{1-\frac{67000^2}{c^2}}}$ ( b ) observation from the earth : time dilation of aeroplane clock vs earth clock , ( iii ) $\frac{\delta t}{\sqrt{1-\frac{1000^2}{c^2}}}$ these are all different values due to different velocities as measured from different observation locations . this is what relativity is all about . ( side note , this equations are correct iff $c$ is in mi/h . ) up to this point we have ignored general relativity , which takes into account time dilation due to gravitational acceleration ( clocks are measured to be faster in lower g ) . this is an opposite effect from the time dilation due to sr . as it turns out , aeroplanes are travelling way too slow to have their clocks observed to be slowed at all --in fact they measure to be faster than earth clocks , because the difference in gravity outcompetes the difference in velocity in this case . if you are wondering if there is a sweet spot where time dilation due to sr and gr cancel out , there is . you can find out more by searching for time dilation due to gravitation and motion . an important thing to note is that the effect of time dilation are observational effects , and are due different conditions at the observation point and the point being observed . when two objects have relative velocity , they both measure the other 's clock to be slower than their own but a third object with the same velocity as one of the original two will clearly not measure those two as equally slow .
the above posters seem to have missed the fact that $\psi$ is not an eigenfunction , but an arbitrary wavefunction . the types of wavefunctions we normally see when we calculate things are usually expressed in terms of eigenfunctions of things like energy or momentum operators , and have little to do , if anything , with classical behaviour ( e . g . look at the probability density of the energy eigenstates for the quantum harmonic oscillator and try to imagine it as describing a mass connected to a spring ) . what you might want to do is construct coherent states which are states where position and momentum are treated democratically ( uncertainty is shared equally between position and momentum ) . then , the quantum number that labels your state might be thought of as the level of excitation of the state . for the harmonic oscillator , this is roughly the magnitude of the amount of energy in the state in that $e = \langle n \rangle \hbar= |\alpha^2| \hbar$ . if you naively take $\hbar \to 0$ then everything vanishes . but if you keep , say , the energy finite , while taking $\hbar \to 0$ , then you can recover meaningful , classical answers ( that do not depend on $\alpha$ or $\hbar$ ) .
i think you are misunderstanding the article . i also think that the name ' flying saucer ' is a bit misleading . i think referring to it with the actual project name : ' low density supersonic decelerator ' ( ldsd ) fits better . the concept of this project is that a spacecraft uses a inflatable saucer-shaped balloon to increase its reference area ( the surface area used in the drag equation ) to increase its atmospheric drag during atmospheric entry . during nasa 's test flight they want to test whether this system would suffice to land big spacecrafts safely on to mars . to simulate the atmosphere of mars they lift the test craft with a helium balloon high up into the atmosphere of earth such that the density is similar to that of mars at its surface . they than use a rocket to get up to speeds comparable with a atmospheric entry at mars . this video also gives a good explanation of the ldsd , and this video give a little longer and more detailed explanation .
this is known as ground effect . not to be confused with flaring , which is a technique used by pilots to gain lift by increasing the angle of attack as airspeed decreases . technicality , you can flare an aircraft at any altitude . the higher the altitude , the faster the airspeed of which you can flare an aircraft before stalling due to air thinning as altitude increases .
first , although it is common in some textbooks , i do not think it is a good thing to necessarily relate the equiprobability postulate to ergodicity . second , what this postulate enables is to estimate the probability distribution for the macrovariable you want to look at . you can of course look at the most probable value for this macrostate and this will correspond to a " thermodynamic interpretation " of what you can expect to observe . however , in statistical mechanics , what you can expect is not the most probable value but rather the average value and they need not be the same . moreover , you may want more than simply the average value ; you can also want to predict what is the free energy difference between one value of the macrostate and another in some kind of transformation in your sytem and for this you are in trouble if you do not try to get the probabilities right .
the starting point to understand adhesion is that all materials stick together because of interatomic forces . as a thought experiment take two pieces of iron with perfectly smooth surfaces , i.e. smooth down to the atomic scale , and press them together . the join disappears and you have a single continuous piece of iron , so the adhesive force is basically the fracture strength of iron . but in everyday life we do not see everything stick to everything else , and the reason is that real surfaces are rough on the atomic scale . so when you put you pen down on your desk the real area of contact of the pen with the desk is tiny . so although the pen adheres to the desk where it touches , it touches over such a small area that the adhesive force is negligable . however if you put a liquid between the pen and the desk then the liquid can flow into the irregularities in the two surfaces so now you have essentially perfect contact between the pen and the liquid and the liquid and the desk . now when you try to lift the pen you are pulling the liquid apart and this will require some force . for a thin liquid like water the force is small , but put your pen in a pool of treacle or tar and suddenly you will find the pen sticks to the desk quite strongly . the reason that this is relevant to scotch tape is that scotch tape is coated with a material called polyisobutylene and it is this that causes the adhesion just like the pool of treacle sticks your pen to the desk . like most polymers the properties of polyisobutylene are very variable and can be changed by changing things like the degree of cross-linking . at high levels of crossing-linking and high molecular weight you get a solid rubber while at low levels of cross-linking and low molecular weight you get a thin liquid . in between you get a material which is strong but has some ability to flow , and the combination of these properties makes it extremely tacky because it can wet both surfaces but be strong enough to resist being pulled apart . hopefully the above has answered your first question , and it also gives you the answer to your second question . if you pull the tape sideways you are trying to break apart all the polyisobutylene on the contact area and this takes a lot of energy . when you peel the tape you are only pulling apart the polyisobutylene at a thin line where the tape is leaving the substrate so this takes much less energy . re your third question , i do not know whether there are industry standards for the adhesion of butyl tape , though i assume there must be . i suspect the experiment would be hard as you had get rather variable results unless you control the experimental conditions very closely . in principle the adhesion will be related to the visco-elastic properties of the polyisobutylene , though as far as i know no-one has come with a quantitative description of the relationship .
unlike the case in psychology and sociology , in physics we are capable of repeating the same experiment/measurement in the same conditions over and over . any valuable , unexpected results that reflect reality should repeat also , so there is no such problem . this is true unless the experiment is not repeated enough to filter unexpected results out of noise . this is why the lhc collides particles over and over , to catch any such repeated unexpected events over the background noise with big certainty .
1 ) in general , $\psi ( \vec{r} , t ) = {\sf u} ( t , 0 ) \psi ( \vec{r} , 0 ) $ , where ${\sf u} ( t , 0 ) $ is the time-evolution operator ( a unitary matrix ) . 2 ) given your superposition state at initial time , after time $t$ the wave function would look like $$\psi ( r , \theta , \phi , t ) = a \left ( 2r_{10}y_{00} e^{-ie_1 t/\hbar} + 4 r_{21}y_{1 , -1} e^{-ie_2 t/\hbar} \right ) $$ where $e_1$ and $e_2$ are the ground and first excited energy eigenvalues of the hydrogen atom ( in atomic units , $e_1 = -1/2$ and $e_2 = -1/8$ ) . by the way , your wave function is equivalent if you divide it by $2$ ( you include $2$ in the normalization factor $a$ ) . 3 ) the probability of measuring $e_1$ and $e_2$ would remain constant : $p_1 = |\langle r_{10} y_{00} | \psi ( r , \theta , \phi , t ) \rangle |^2 = 4a^2 = 1/5$ ; $p_2 = 1 - p_1 = 4/5$ . you should by now understand that those are the same ( constant ) probabilities of measuring $l^2 = 0$ and $l^2 = 2\hbar^2$ .
1 ) photons are bosons and can exist with the same quantum numbers with an indefinite number of photons . 2 ) photons have zero mass but have energy $e=h\times \nu$ , increasing their number and frequency increases the energy per cubic centimeter . 3 ) photons move with the velocity of light , and trapping them presupposes reflectors of one kind or another . from 1 ) the answer is " no limit " from 2 ) and 3 ) the limit would come from the melting of the reflectors due to the high energy density . the number would be large and would depend on the frequencies present .
the magnetic field only induces currents when it is changing . in the standard electrophorus , you use a static electric field to induce a charge on one part of the metal , and then you manually drain the charge from another part of the metal . when it is a static magnetic field , nothing happens . you could make an electrophorus by using a coil attached to a pair of plates , then quickly push a magnet so that it runs by the coil , inducing a current which charges the plates , then ( quickly , while the magnet is still moving ) , disconnect the coil from the plates . this would work to charge the plates , but it is not an elecrophorus , but a minature dynamo used to charge a capacitor .
this is much to do with the possible eigenvalues of the operators . normal operators on a hilbert space are closely analogous to complex numbers , with the adjoint taking the role of the conjugate ; these relations are typically inherited directly to the operator 's eigenvalues . thus , if a linear operator $l$ has an eigenfunction $f$ with eigenvalue $\lambda$ , $$lf=\lambda f , $$ then saying "$l$ is self-adjoint " means that $l^\dagger=l$ which translates to $\lambda^\ast=\lambda$ , i.e. that $\lambda$ be real . similarly , $l$ being nonpositive implies that $\lambda\leq0$ . in your case , the behaviour can be reduced to an equation of the form $$ \frac{\partial^2 p}{\partial t^2} ( x , t ) =\hat lp ( x , t ) , $$ where $\hat l$ is some differential operator . in general the solution will not be of this form , but you can take a first stab of the problem by inserting in an eigenfunction of the differential operator for the spatial dependence . that is , you use the trial solution $p ( x , t ) =p_0 ( x ) t ( t ) $ , where $\hat lp_0=\lambda p_0$ . this hugely simplifies the time-propagation equation , which reduces to the solvable form $$ \frac{\partial^2 }{\partial t^2}t=\lambda t . $$ while the solutions of this equation are formally all the same ( i.e. . $t ( t ) =t_+e^{\sqrt{\lambda}t}+t_-e^{-\sqrt{\lambda}t}$ ) regardless of what $\lambda$ is , the behaviour will be very different and depend , sometimes sensitively , on $\lambda$: if $\lambda&gt ; 0$ , then at least one of the exponentials $e^{\pm\sqrt{\lambda}t}$ will have a blow-up . if $\lambda$ has an imaginary part , however small , then one of the two square roots $\pm\sqrt{\lambda}$ will have a positive real part , and the corresponding contribution to $t ( t ) $ will oscillate at a blowing-up amplitude . if $\lambda$ is negative or zero , then both roots $\pm\sqrt{\lambda}$ will be imaginary or zero , and both exponentials will be completely oscillatory and have bounded amplitude for all time . it is clear that only the third case is consistent with conservation of energy . in terms of the differential operator , it corresponds to a condition of self-adjointness ( i.e. . $\lambda\in\mathbb r$ ) and non-negativity of the operator .
on the upper end , coulomb 's law has not been observed to break for any large collection of charge that can be put together . in principle , if you tried to put more and more charge together then there would be a lot of energy stored in the field , and if the mass equivalent of this energy density got too high , there would be general relativistic effects to consider . in practice , unless you are dealing with a charged black hole , the charge distribution will tear itself apart many orders of magnitude before that . there is no corresponding breakdown as such for " very small " charges , on the other hand , because charge is quantized : no free charge smaller than the electron charge , $e=1.6\times10^{-19}\textrm{ c}$ , has ever been observed . coulomb 's law also breaks down if charges are moving , and particularly if they are moving fast ( comparably to the speed of light ) or there are charge movements in an otherwise neutral conductor . this is fixed by extending the electrostatic case into the full electromagnetic theory , as developed by maxwell , which is fully compatible with special relativity . in the domain of the small , the electrostatic force remains unaltered for standard quantum mechanics . it does change for the relativistic case , in which case you should use quantum electrodynamics ( qed ) , which describes a bunch of nonclassical phenomena that occur when charged elementary particles go fast . there is , however , one very interesting application of qed to stationary charges , and it happens in the short distance limit : as you get in closer , the electron looks like it has more charge , and the force goes up faster than $1/r^2$ . this is called charge screening and results from a cloud of virtual particle pairs that momentarily pop into and out of existence .
the candela is the si base unit of luminous intensity ; that is , power emitted by a light source in a particular direction , weighted by the luminosity function ( a standardized model of the sensitivity of the human eye to different wavelengths ( wikipedia ) therefore , 7500 cd ( 3x2500 cd ) is more visible than 6000 cd .
there are many quantum field theory models which are exactly solvable in the large $n$ limit , such that the $\mathbb{c}p^n$ model , the thirring model , the $o ( n ) $ vector model etc . please see the following review by moshe moshe and jean zinn-justin covering many of these models . the main idea is that feynman diagrams ( for example the vacuum diagrams in the case of the partition function ) are proportional to certain powers of $n$ depending on the number of vertices , lines and loops , and the leading order diagrams can be summed . there are other methods which lead to the same results such as variational computations . when the fields in the model belong to the fundamental vector representations of $o ( n ) $ or $u ( n ) $ , the computation of the large $n$ limit ( summation of the leading diagrams ) is quite easy , however , when the fields belong to the adjoint representation ( for example , the gluons in qcd ) , the analysis becomes more complicated . the case of large $n$ qcd was solved by t'hooft , please see the following review by aneesh manohar .
am radio typically transmits at around 1 mhz , fm radio at about 90 mhz . measurements of the rf spectrum of lightning strikes show a falloff with frequency of about 20 db per decade in that frequency range , so with fm about 2 decades above am , you had expect am to have about 40db higher interference from a lightning strike . in addition to that , fm signals attenuate faster with range , so depending on your distance from the lightning strike the effective am/fm interference ratio could be even larger .
in free space , this would be impossible , but there is friction , so if you impulse the chair forward , by moving yourself a little back ( by conservation of momentum the chair must move forward ) , and then you get back to you initial position , the chair will move back again , but there is friction and energy is being lost during the process so the distances will be smaller and the chair , at the end , will be a little bit moved forward .
yes , helium can leave the earth , and yes , we will run out of helium , but because of different reasons . when you buy a helium balloon and its contents get released , this helium goes into the atmosphere . it is not gone , and it could in principle be purified out of normal air . however , the total amount of helium in the atmosphere is so small it is technologically not feasible to do that . at some point the technology might be developed , but it is unlikely to be economical . on top of that , helium does also escape from the atmosphere . since it is so light , it drifts naturally to the upper layers , and there it is easily torn away by the solar wind . however , this process will occur on geological timescales , unless we were to waste so much helium that the total atmospheric content changed appreciably . keep in mind , though , that even if the helium does not leave earth it is lost to us once it is diluted in the atmosphere . so : yes , we will run out , and yes , it will make everything awful . and yes , you should cringe when you see helium balloons at a childrens ' party .
i take it you mean the determinant with the straight bars . . . ? if so , the only way to compute it for general background gauge field is , as nervxxx mentioned in the comments , to expand the determinant in $a_\mu$ . if you are considering a specific background gauge field ( like a constant magnetic field ) you should look whether you can find the eigenvalues of your operator , i.e. solve \begin{equation} ( i d\ ! \ ! \ ! / - m ) \psi = \lambda \psi \ ; . \end{equation} the determinant will then be given as the product of all the eigenvalues . ( in order to regularize the expression , it may be useful to rewrite it as the exponential of the sum of the logarithm of all eigenvalues , but that depends on the specifics of the problem . )
in most of the cases , the charge in dielectric material is created by imbalance in the charges existing in the dielectric . the dielectric is initially neutral . the balance is broken by certain “charging effects” such as friction , heat and pressure . the answer to which charging effect is responsible for the charge imbalance depends on the material and the surrounding conditions . also in most cases the generation of a charge on a dielectric requires another body to be involved . with respect to the rest of your questions : 1 . the charge does not go anywhere ; it stays where it was generated . charges can’t move in a dielectric as you know . 2 . the moving charges are mostly electrons , for electrons to move they need the necessary energy to overcome what is called the band gap . for dielectrics the band gap is large such that the electrons require a lot of energy to overcome the gap to arrive to conduction band where they can move freely . see the picture 3 . yes they are bounded to certain atoms , what makes those atoms specific is that they are the ones who experienced the imbalance of charges by the charging effects i mentioned above . the same things can be said for positive or negative charges . some dielectric materials have a tendency toward having negative charge when they are charged ; some have a tendency toward having a positive charge . that tendency is determined by materials properties . you can find a lot of information on this topic here
on the topic of the actual consequences of qm , here is answers with a few things that cannot be explained without qm . that aside . . . there is a golden rule that one should recite before all theoretical physics studies : " it all adds up to normality " . - greg egan , quarantine the thing with quantum mechanics and " logic " is that humans are really bad at qm and really good at " logic " . in fact , " logic " is an actual area of study in psychology : naïve physics . naive physics ( forgive me not using umlauts ) is an all-right approximation of anthropically scaled physical phenomena : object permanence , an exclusion principle based on volume , absolute time , a primitive notion of gravity . . . this is what humans think with , every day , on an instinctive level . over the times , you see refinements of naive physical concepts in works of aristotle and newton . object permanence , volume exclusion , gravity , absolute time . then relativity comes along and throws absolute time out the window . believe me , people cried " logic is meaningless " when relativity was new too . then comes quantum mechanics : throw away volume exclusion and even to a degree the intuitive notion of object permanence . there is one other concept of naive physics which qm seemingly violates : looking is a free action . suddenly you chance things by " looking " . so people do indeed cry out " logic is broken . " but it is not , because logic has nothing to do with qm . logic , and indeed all of mathematics is about axioms and theorems and the steps of inference in between . qm 's apparent weirdness has no effect on me using the peano axioms to say : $$ \begin{array}{l l}\vdash and 0 = 0 \\ \vdash and \forall x , y : s ( x ) = s ( y ) \iff x = y \\ \vdash and \forall x : x + 0 = 0 + x = x \\ \vdash and \forall x , y : x + s ( y ) = s ( x ) + y \\ \vdash and 2 = s ( s ( 0 ) ) \\ \vdash and 5 = s ( s ( s ( s ( s ( 0 ) ) ) ) ) \\ \vdash and 2 + 2 = 2 + s ( s ( 0 ) ) = s ( 2 ) + s ( 0 ) = s ( s ( 2 ) ) + 0 = s ( s ( 2 ) ) \\ \vdash and 2 + 2 \not = 5 \\ and \iff s ( s ( s ( s ( 0 ) ) ) ) \not = s ( s ( s ( s ( s ( 0 ) ) ) ) ) \\ and \iff s ( s ( s ( 0 ) ) ) \not = s ( s ( s ( s ( 0 ) ) ) ) \\ and \iff s ( s ( 0 ) ) \not = s ( s ( s ( 0 ) ) ) \\ and \iff s ( 0 ) \not = s ( s ( 0 ) ) \\ and \iff 0 \not = s ( 0 ) \\ \end{array} $$ captial l logic is set in mathematical stone and to say that qm 's " mysteries " imply $2 + 2 = 5$ is juvenile , trivially untrue , and shows that the speaker has not really understood anything at all . it is like saying : boy am i bad at abstracting from my everyday life and interactions , that i cannot even sit down and actually learn qm . i am going to put that on a t-shirt !
given boundary conditions allow for a unique solution of the wave equation . if you do it your way and immediately guess the retarded green 's function you are fine , but in principle there is an infinite amount of solutions which have to be fixed by boundary conditions . by imposing sommerfeld conditions , you make sure that only the retarded solution survives , which is the only physical solution : there is no incoming radiation from infinity . $ct+r=constant$ assures that you are actually treating past null infinity ( which is not clear from the first condition alone ) . furthermore , $r\bar{h}_{\mu\nu}$ and $r\partial_\rho\bar{h}_{\mu\nu}$ have to be bounded in order to assure that the limit at $-\infty$ is well defined .
kinetic friction applies only when there is relative motion between the two objects . the phrase " to keep from slipping " implies that the object is at rest : no relative motion . we are in the realm of static friction . static friction provides an upper limit to the magnitude of the force that friction can provide . the force due to friction can be very small if the angle is very small , but it cannot be very large . at some point , as you increase the angle , the force required to maintain static equilibrium ( keep the object from slipping ) will exceed $\mu_s |\vec{n}|$ . in your case the angle is fixed in advance . presumably , in this case , the object will slip if no additional forces are present , such as from your finger pushing up-ramp on the object . you need to supply an additional force to counter the portion of the force of gravity that exceeds the maximum force allowed by static friction . evidently , you have done the calculation correctly . this new applied force is a minimum , because you can if you wish apply a greater force with your finger , countering even more of the gravitational force . you can continue increasing the force of your finger until the limit of the static friction is exceeded in the upwards direction : the force delivered by your finger then exceeds the force due to gravity by $\mu_s |\vec{n}|$ . hope that helps .
this creates a point of extremely focused energy at the middle point where the bubble collapses . in theory , this point focuses enough energy to trigger nuclear fusion . it is not currently accepted mainstream science to say that collapsing bubbles focus energy enough to cause nuclear fusion . temperatures over 10,000k can be acheived , but are still well below the millions of degrees needed for fusion . see the extensive review article single-bubble sonoluminescence for detailed information .
will this " oldest light in the universe " ever completely cease to exist , or does light stay around indefinitely . light at the micro level is composed out of photons , the elementary particles of light . in addition to reflections light/photons undergo absorptions when hitting/interacting with matter . photons can be completely absorbed or change in energy after such a scatter and slowly the intensity of light will diminish . thus how long a beam of light will survive will depend on the matter found in its path . the oldest light that we managed to see as cosmic microwave background arrives into our instruments , is measured , and that is the extent of its lifetime , 13,9 billion years according to the prevailing cosmological model . in order to measure it some electrons and atoms/molecules have absorbed it . the same is true of its fellows which were intercepted in their path by the earth . if the non interacting part will exist until the death of our universe is matter for speculations because there is no standard model on how the universe will end , and there may never be . as long as they do not interact some photons will remain though whether they will still carry a coherent snapshot of universe at the time they decoupled about 380000 years after the beginning , is also a moot point . the cmb is shown here at 380000 thousand years after the big bang
notice that the factors of $r^2$ cancel , and $\hat r\cdot\hat r = 1$ so the integral expression you wrote down reduces to $$ \frac{q}{4\pi\epsilon_0}\int \sin\theta \ , d\theta \ , d\phi $$ the bounds of integration are $0&lt ; \theta&lt ; \pi$ and $0&lt ; \phi&lt ; 2\pi$ so we really need to compute $$ \int_0^\pi \sin\theta\ , d\theta\int_0^{2\pi}d\phi = 2 ( 2\pi ) = 4\pi $$ so the factors of $4\pi$ cancel and we are left with $q/\epsilon_0$ as desired .
i have never seen actual figures but , in general , articles i have seen about flight state that " most " lift is generated from the angle of attack and relatively little from the bernoulli effect . i suspect the exact figures are rather variable and probably depend on whether the plane is climbing , descending , banking , etc and will also vary from plane to plane . maybe this is why exact figures seem not to be quoted . the pressure difference between the top and bottom of the wing is quite real , though note that on the top of the wing it is not a vacuum as the pressure does not decrease that much . the lowered pressure above the wing will indeed tend to pull the skin off the wing , or more precisely the air within the wing that is at normal atmospheric pressure will try to push the skin off . once again i can not give you exact figures - i must admit i thought ballpark figures would be easy to calculate , but google has failed me . incidentally , there is a good nasa article on this subject at http://www.grc.nasa.gov/www/k-12/airplane/wrong1.html and it even includes a java applet for you to play with the details of the wing . a longer slightly more staid article is at http://www.free-online-private-pilot-ground-school.com/aerodynamics.html later : if an approximate answer would be ok then you could could use bernoulli 's equation as described in http://en.wikipedia.org/wiki/bernoulli%27s_equation#incompressible_flow_equation . although this really only applies to incompressible fluids , and air is obviously compressible , the article suggests it would be a reasonable approximation for low speeds . rewriting the equation to make it more useful for our purposes gives : $$p = \rho a - \rho \frac {v^2}{2} - gh$$ where $a$ is some constant and $h$ is the height . we do not know the constant , but let $p_{bot}$ be the pressure below the wing and $p_{top}$ be the pressure above the wing then we can take the difference between them i.e. the pressure drop between the bottom and top of the wing . if we assume the height is constant i.e. we can ignore the thickness of the wing we get : $$\delta p = p_{bot} - p_{top} = 0.5 \rho ( v_{top}^2 - v_{bot}^2 ) $$ i do not know what speed you plane flies at , but let 's guess at 30 m/s and let 's guess that there is a 10 m/s difference between the air speed at the top and bottom of the wing , so that is $v_{bot} = 30$ and $v_{top}$ = 40 . google gives the density of air at ground level as 1.225 kg/m3 . $$\delta p = 0.5 \times 1.225 \times ( 40^2 - 30^2 ) = 429 pa$$ 429 pa is 4.29 grams per square cm or 0.06 pounds per square inch , so it is completely insignificant .
hints : use $$\frac{\partial u}{\partial {\bf v}}= -q{\bf a} , $$ and the defining property of a velocity-dependent potential : $${\bf f}~=~\frac{d}{dt} \frac{\partial u}{\partial {\bf v}} - \frac{\partial u}{\partial {\bf r}} . $$ see e.g. herbert goldstein , classical mechanics and wikipedia for more details .
sure , it would be $1/c$ seconds , which is exactly $1/299792458\text{ s}$ . although , really , there is no point because time is defined as a multiple of $c$ , anyways . as for proof that time is the fourth dimension , there is no ' proof ' like any scientific theory ( there is just evidence ) and unlike mathematics , but one fairly large point of ( fairly accessible ) evidence is in your gps . look up " general relativity and gps . " it is not hard to imagine people 's reactions if their gps told them they were in the middle of antarctica flying 500 metres in the air .
since there is positive and negative plate on your question , the field you want to say to us might be an electric field . suppose that the positive plate on the left and the negative plate on the right creates an uniform electric field and the velocity of the particle is downward , it will make a projectile depending on the charged of particle . if the particle is proton , the trajectory will go to the right since it will be attracted by negative plate . if the particle is electron , the trajectory go to the left since it will be attracted by positive plate .
actually , the sun outputs much more than these six wavelengths . the figure provided shows the spectrum of the sun : as you can see , the sun outputs light along a continuous curve at all visible wavelengths . the combination of which appears white to our eyes as a byproduct of having evolved in orbit of this star . the image also shows the absorption effects of the atmosphere . if you specifically discuss the blue scattering from the atmosphere , the following figure is a nice depiction . as you can see , the blue scattering leaves only the more red visible wavelengths , which makes sunlight appear more yellow around midday and more red near dawn/dusk . as for why we can simulate white light perfectly without having to reproduce the solar spectrum ; that has everything to do with how humans perceive colour . the human eye has special cone cells capable of perceiving colour . as shown below , there are only three true colours we can see ; red , gree , and blue ( with more green cells than the others again due to the solar spectrum ) . it is by using a combination of these three colours that we interpret all possible visible wavelengths . our brains interpret the relative strengths by which each different colour cone is stimulated and assigns a different colour to each combination . the next figure shows the sensitivity ranges of these cones . the three types of cones are sensitive over all visible wavelengths but are stimulated differently by each wavelength . thus , we can interpret a nice teal when a 500nm wave stimulates the cones appropriately , or we can simulate this for human eyes ( and this is what we do with things like computer screens ) by combining certain levels of 400 , 535 , and 700nm ( thanks to peter shor for the correction ) lights to stimulate the cones by the same ratio as the 500nm light . being the same ratio , our brain is tricked into seeing it as teal , but the summed light itself is not actually teal . in the same way , a combination of the six colours you mentioned would look white to us , but to make a true white , the sun must output across all wavelengths in the visible spectrum at varying amounts .
there is a rigorous formal analysis which lets you do this . the true problem , of course allows both the proton and the electron to move . the corresponding schrödinger equation thus has the coordinates of both as variables . to simplify things , one usually transforms those variables to the relative separation and the centre-of-mass position . it turns out that the problem then separates ( for a central force ) into a " stationary proton " equation and a free particle equation for the com . there is a small price to pay for this : the mass for the centre of mass motion is the total mass - as you had expect - but the radial equation has a mass given by the reduced mass $$\mu=\frac {mm}{m+m}=\frac{m}{1+m/m} , $$ which is close to the electron mass $m$ since the proton mass $m$ is much greater . it is important to note that an exactly analogous separation holds for the classical treatment of the kepler problem . regarding self-interactions , these are very hard to deal with without invoking the full machinery of quantum electrodynamics . fortunately , in the low-energy limits where hydrogen atoms can form , it turns out you can completely neglect them .
yes . in general relativity and cosmology , the collection of galaxies is often even treated as an ideal fluid , in the thermodynamic sense , with temperature and pressure . the standard textbook on general relativity , the book gravitation by misner , thorne , and wheeler , discusses this model in section 27.2 , though only on the thermodynamic level . for the statistical mechanics of gravitation , see , e.g. , w . thirring , systems with negative specific heat , z . physik 235 ( 1970 ) , 339-352 . this is for inside a star , but a similar analysis works on the level of galaxies . see , e.g. , ahmad et al . , statistical mechanics of the cosmological many-body problem , the astrophysical journal 571 ( 2002 ) , 576-584 . a free online copy is at http://iopscience.iop.org/0004-637x/571/2/576
this is one of those cases when the ends do not justify the means . . . just because you get a result that is true , from laws that are not supposed hold in that situation , does not mean that the laws can be used there . if you are asking about whether there is a deeper connection between using $\frac{1}{2}mv^2$ and getting the radius , to the best of my knowledge , there is none : it just happens to be a coincidence that they match . like michael brown said , the schwarzschild radius can only be derived from gr , in which the schwarzschild metric \begin{align} ds^2 = - ( 1-2gm/r ) dt^2 + ( 1-2gm/r ) ^{-1}dr^2 + r^2d\omega^2 \end{align} is the unique maximally symmetric solution to einstein 's equations in vacuum . ( i have set $c = 1$ ) here . note that something interesting is happening when $r = 2gm$ , and one can go on to show that $r = 2gm$ happens to be the boundary of the causal part of the future null infinity , which we identify as the event horizon .
you are on the right track , but there is more that can be said . for an introduction to this topic , i highly recommend sean carroll 's spacetime and geometry , which i will follow below for the purpose of illustrating where that $\gamma$ comes from . the book grew out of lecture notes , the relevant chapter of which can be found online here . algebraic background note : this is long , but only because each step has been broken down into very simple parts . you want a directional derivative - something that tells you how a tensor changes as you move along some path in your manifold . just like in standard multivariable calculus , you define this as the sum of the derivatives of your object in each direction , weighted by how much your coordinate is changing in that direction : $$ \frac{\mathrm{d}}{\mathrm{d}\lambda} := \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \nabla_\mu . $$ here $\lambda$ parameterizes your path , and the appropriate derivative to use is the covariant derivative . the scalar function $x^\mu$ is being differentiated with the standard directional derivative : $$ \frac{\mathrm{d}}{\mathrm{d}\lambda} := \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \partial_\mu . $$ note that this is consistent with the fully covariant directional derivative in the case of scalar functions , and , despite appearances , this is not a circular definition ( you should be able to differentiate known functions of $\lambda$ with respect to $\lambda$ ) . the affine connection is simply defined as the set of coefficients needed to augment the partial derivative in order to make a covariant derivative : $$ \nabla_\mu v^\nu = \partial_\mu v^\nu + \gamma^\nu_{\mu\lambda} v^\lambda $$ for any vector $\vec{v}$ . you do not even need to know how to generalize this to covariant derivatives of arbitrary tensors . the geodesic equation arises from imposing the requirement that your curve parallel-transport its own tangent vector . 1 that is , the directional derivative of the tangent vector along the direction it is pointing vanishes . a path will be a set of smooth functions \begin{align} x^\mu : \mathbb{r} and \to m \\ \lambda and \to x^\mu ( \lambda ) , \end{align} one for each coordinate $\mu$ . if you denote the tangent vector at $\vec{x} ( \lambda ) $ by $\vec{t} ( \lambda ) $ , then $$ t^\mu = \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} . $$ putting all this together , we have \begin{align} 0 and = \frac{\mathrm{d}}{\mathrm{d}\lambda} \left ( \frac{\mathrm{d}x^\sigma}{\mathrm{d}\lambda}\right ) \\ and = \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \nabla_\mu \left ( \frac{\mathrm{d}x^\sigma}{\mathrm{d}\lambda}\right ) \\ and = \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \left ( \partial_\mu \left ( \frac{\mathrm{d}x^\sigma}{\mathrm{d}\lambda}\right ) + \gamma^\sigma_{\mu\nu} \frac{\mathrm{d}x^\nu}{\mathrm{d}\lambda}\right ) \\ and = \frac{\mathrm{d}^2x^\sigma}{\mathrm{d}\lambda^2} + \gamma^\sigma_{\mu\nu} \frac{\mathrm{d}x^\mu}{\mathrm{d}\lambda} \frac{\mathrm{d}x^\nu}{\mathrm{d}\lambda} \end{align} summary after all this ( possibly too much ) algebra , the take-home message is that the connection arose from the difference between partial and covariant differentiation along a curve . it is not that we want our curve 's tangent to never point in a different coordinate-dependent direction ( $\ddot{x}^\lambda = 0$ , which would be sufficient for " straight line " in euclidean space ) . rather we want the change in the coordinate-dependent direction we are pointing to be compensated by the fact that our tangent space is rotating with respect to our coordinates as we move along the curve . 1 some people define the geodesic in a more global sense , as the path that extremizes the arc length between two points . these definitions agree if and only if the affine connection you are using is indeed the metric compatible , torsion free christoffel connection .
yes , and this question filled a semester in my 4th year of physics . the schrödinger 's equation describes the problem exactly . . . with the caveat that it is impossible to solve exactly for more than one electron . a typical atomic physics course will then proceed to try to develop different methods and heuristics to solve this problem . the basic idea of the shell model is as follows : electrons are grouped at energy shells ( the ones given by the solution of schröodinger 's equation ) the complete shells behave similarly to an spherical envelope : the shells external to a given one do not affect it . they will partially screen the charge from the nucleus ; i.e. : if you have $_3li$ , the third electron will only see charge 1 , because the two electrons in the innermost shell will cover it . the electrons inside a shell behave more or less independently . the last premise requires some explanation . if you solve the angular part of the equation for one electron , you get a family of possible solutions , the spherical harmonics . each electron in a shell will follow one of them . the number of spherical harmonics there can be is tightly linked to the rule you asked . this model is not perfect , but it an be quite good . in order to improve it we can introduce corrections : each pair of electrons in the same shell will repel each other , so the atom will be a bit bigger , and the binding energy a bit smaller . it must be noted at this point that the exact wavefunction is , in many cases , not computed . what we see in most experiments are the energy levels , and this is what the methods are focused on ( and it also happens to be much much easier , lucky us ) .
there are two definitions for the wavenumber , one is $k=\frac{2\pi}{\lambda}$ , the number of wavelengths per $2\pi$ units of distance ( i.e. . on the circle ) , the other one is $k=\frac{1}{\lambda}$ , which is the number of wavelengths per unit distance . the formula for the raman shift is refering to the latter definition .
in fact , maupertuis or euler principles of least action are historically the first formulation of a least action principle , but one have to wait lagrange and hamilton to have a modern version , with the so-called euler-lagrange equations , which allow us to obtain the equations of movement . if i am not mistaken , you cannot deduce directly the equations of movement from the maupertuis/euler principles . the problem i see is that you cannot know the dependence of the potential energy $v$ in $x$ , in seeing only the kinetic energy $t$ . now , as you state , but written differently , for a movement with conservative energy , one may see that the variation of the maupertuis ' action is equivalent to the variation of the lagrange/hamilton action , for instance , starting with maupertuis ' principle : $$ \delta\int ( 2t ) dt = 0$$ we have : $2t = t + ( e - v ) $ , so maupertuis ' principle can be written : $$ \delta\int ( e + ( t-v ) ) dt = 0$$ but $e$ being a constant , this is not useful in the variation , so finally , we have : $$ \delta\int ( t-v ) dt = 0$$ which is the usual lagrange/hamilton action . but , to really have the equations of movement , you have to use a functional , that is : $$ \delta\int l ( x , \dot x , t ) dt = 0$$ and this gives you , thanks to the euler/lagrange equations , the equations of movement .
the worldsheet fermions have to do with internal degrees of freedom , namely the spin -- therefore better name for the superstring is the more old-fashioned " spinning string " ( since worldsheet susy should not be confused with spacetime susy ) . the worldsheet fermions generate multiplets of some internal symmetry group . if you want those internal degrees of freedom generated by ws fermions to transform under spacetime lorentz transformations , rather than an independent internal symmetry , you need to correlate the lorentz transformations of the worldsheet bosons and fermions . this is what worldsheet susy does for you . all of this is not specific to string theory . if you want to first-quantize a field theory , a " bosonic " worldline theory will give you a ( free ) scalar field theory . adding fermions and the corresponding worldline supersymmetries will generate ( free ) higher spin fields . it is probably a useful exercise to get e.g. classical ( free ) maxwell field from a ( n=2 susy ) worldline theory in order to appreciate precisely what the worldsheet structures mean precisely . wish i had a good reference , but maybe someone can help me out .
the field and its conjugate momentum are operators . they act on the hilbert space of the theory , which is not the space of square-integrable functions of position , as it is in single particle quantum mechanics . rather , the configuration space of the theory is the set of all field configurations and the wavefunction is actually a functional , which ascribes an amplitude $\psi [ \phi ( x^\mu ) ] $ for the field to be in a configuration $\phi ( x^\mu ) $ . qft is just quantum mechanics applied to an infinite collection of degrees of freedom labeled by positions $x$ . once you get used to the switch it comes to seem pretty simple , but the set of all field configurations makes a huge space so naturally it is very difficult to make things mathematically rigorous . for the most part physicists can live without rigourously defining the configuration space . the dictionary from ordinary quantum mechanics to quantum field theory is $$ \begin{array}{lcl} \mathrm{qm} and and \mathrm{qft}\\ t , i and \to and \left ( t , x , y , z\right ) \equiv\left ( t , {\bf x}\right ) \equiv x^{\mu}\\ q_{i}\left ( t\right ) and \to and \phi\left ( x^{\mu}\right ) \\ p_{i}\left ( t\right ) and \to and \pi\left ( x^{\mu}\right ) \\ \psi\left ( t , q_{1} , q_{2} , \cdots , q_{n}\right ) and \to and \psi\left [ \phi\left ( x^{\mu}\right ) \right ] \\ \left [ q_{i}\left ( t\right ) , \ p_{j}\left ( t\right ) \right ] ~=~i\hbar\delta_{ij} and \to and \left [ \phi\left ( t , {\bf x}\right ) , \ \pi\left ( t , {\bf y} \right ) \right ] =i\hbar\delta^3\left ( {\bf x}-{\bf y}\right ) \\ \hat{h}\left ( t\right ) and \to and \hat{h}\left ( t\right ) =\int\mathrm{d}^{3}x\ \hat{\mathcal{h}}\left ( x^{\mu}\right ) \end{array} $$ you can represent the field momenta explicitly by functional derivatives : $$ \pi ( x^\mu ) ~=~ - i\hbar \frac{\delta}{\delta \phi ( x^\mu ) } , $$ and check that this satisfies the commutation relationship .
* edit : * ooops . i was wrong , the correct answer is ( in the rr sector ) : $$m = \frac{2\pi t\ell_s}{c_0^2}\sqrt {{n + \tilde n+2}} $$ in the n-sn-s sector , the correct answer is : $$m = \frac{2\pi t\ell_s}{c_0^2}\sqrt {{n +\tilde n - 2}} $$ in the rn-s/n-sr sectors , $$m = \frac{2\pi t\ell_s}{c_0^2}\sqrt {{n+\tilde n}} $$ this is because its ramond sector normal-ordering constant is $-1 , $ while its neveu-schwarz sector normal-ordering constant is $1 . $
i have always found the cornell to be a good source of info ( and also the former home of carl sagan ) : blue stragglers are stars which stay on the main sequence ( the normal , hydrogen-burning phase of a star 's lifetime ) longer than they are expected to . the color of a star is a measure of its temperature and its mass - blue stars are hotter and more massive than red ones . the more massive a star is , the faster it burns up its hydrogen , so blue stars are expected to spend less time on the main sequence than red stars . therefore , when you look at a color-magnitude diagram of a globular cluster ( whose member stars all formed around the same time ) you expect to see an orderly transition ; stars which are bluer than a certain value ( known as the " turnoff " point ) will have already left the main sequence , while those which are redder will still be on it . the location of the turnoff point can be used to estimate the age of the cluster . but , it is usually the case that several stars in a cluster are observed along the main sequence past the turnoff point , and these are referred to as blue stragglers . the most likely explanation for blue stragglers seems to be that they are the result of stellar collisions or mass transfer from another star . that way , a star which is red , cool and already somewhat old can get extra mass and turn bluer . it spent most of its life as a red star and therefore burnt its hydrogen at a slow enough rate to still be on the main sequence , but then at a certain point it gets extra mass and effectively " disguises " itself as a blue star , which makes us think it is younger than it really is . i think the key takeaway is that we really are not 100% sure . however , that in no way means that anyone can simply assert that their pet theory ( guess , lie , whatever ) is the answer either . especially if they do not bring any evidence , testable hypotheses , or data to examine to the table . solstation also has quite a bit of good info on blue stragglers and what we know about them . of particular interest is this graphic : this paper from cornell also has some additional info . abstract : in open star clusters , where all members formed at about the same time , blue straggler stars are typically observed to be brighter and bluer than hydrogen-burning main-sequence stars , and therefore should already have evolved into giant stars and stellar remnants . correlations between blue straggler frequency and cluster binary star fraction , core mass and radial position suggest that mass transfer or mergers in binary stars dominates the production of blue stragglers in open clusters . analytic models , detailed observations and sophisticated n-body simulations , however , argue in favour of stellar collisions . here we report that the blue stragglers in long-period binaries in the old ( 7 × 109-year ) open cluster ngc 188 have companions with masses of about half a solar mass , with a surprisingly narrow mass distribution . this conclusively rules out a collisional origin , as the collision hypothesis predicts a companion mass distribution with significantly higher masses . mergers in hierarchical triple stars are marginally permitted by the data , but the observations do not favour this hypothesis . the data are highly consistent with a mass transfer origin for the long-period blue straggler binaries in ngc 188 , in which the companions would be white dwarfs of about half a solar mass .
warning : students , stay away from antiquities . the aim to learn is to survive . there is usually reasons that old materials are not cited . in this case , nakanishi is obsolete . nakanishi is not wrong , but unsatisfactory in two points . first , what happened to the gauge symmetry ? and it is limited to qed , and not applicable to non-abelian symmetries . the answer is known since late 70s , given by brst . the gauge symmetry is conserved even after gauge fixing , where the seemingly lost symmetry lies in zero-norm or ghosts , and they are unphysical i.e. harmless . this also means the lorentz covariance is preserved too in non-covariant gauges . you know , you definitely need ghosts ( negative-norm states ) for practical calculation for non-abelian gauges , unlike qed where ghosts are detached . i recommend weinberg 's qft book , vol 2 , sec 15.7 for brst introduction . even if you do not need non-abelian gauge , the cited section of weinberg is not difficult at all . ( there you encounter the structure constant $c^{\alpha\beta\gamma}$ . you can safely think of it as the levi-civita symbol $\epsilon^{ijk}$ , and $t_{\alpha}$ as pauli matrices . ) or rather , you will be stricken to find how easy it is in the functional quantization , compared to nakanishi 's machinery , ( fourier expansion of the field and heavy use of $\delta ( x ) $ and its derivative . ) - brst wins also in pragmatism . as a free material , see for example sredinicki 's qft book , sec 74 , but prerequisites sections may be a bit more cumbersome than weinberg . peskin and schroeder does not help .
