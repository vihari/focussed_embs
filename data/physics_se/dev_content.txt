i suppose there are many aspects to look at this from , anna v mentioned how calabi-yao manifolds in string theory ( might ? ) have lots of holes , i will approach the question from a purely general relativity perspective as far as global topology . solutions in the einstein equations themselves do not reveal anything about global topology except in very specific cases ( most notably in 2 ( spacial dimensions ) + 1 ( time dimension ) where the theory becomes completely topological ) . a metric by itself does not necessarily place limits on the topology of a manifold . beyond this , there is one theorem of general relativity , called the topological censorship hypothesis that essentially states that any topological deviation from simply connected will quickly collapse , resulting in a simply connected surface . this work assumes an asymptotically flat space-time , which is generally the accepted model ( as shown by supernova redshift research and things of that nature ) . another aspect of this question is the universe is usually considered homogenous and isotropic in all directions , topological defects would mean this would not be true . although that really is not a convincing answer per say . . .
a ( rank 2 contravariant ) tensor is a vector of vectors . if you have a vector , it is 3 numbers which point in a certain direction . what that means is that they rotate into each other when you do a rotation of coordinates . so that the 3 vector components $v^i$ transform into $$v'^i = a^i_j v^j$$ under a linear transformation of coordinates . a tensor is a vector of 3 vectors that rotate into each other under rotation ( and also rotate as vectors--- the order of the two rotation operations is irrelevant ) . if a vector is $v^i$ where i runs from 1-3 ( or 1-4 , or from whatever to whatever ) , the tensor is $t^{ij}$ , where the first index labels the vector , and the second index labels the vector component ( or vice versa ) . when you rotate coordinates t transforms as $$ t'^{ij} = a^i_k a^j_l t^{kl} = \sum_{kl} a^i_k a^j_l t^{kl} $$ where i use the einstein summation convention that a repeated index is summed over , so that the middle expression really means the sum on the far right . a rank 3 tensor is a vector of rank 2 tensors , a rank four tensor is a vector of rank 3 tensors , so on to arbitrary rank . the notation is $t^{ijkl}$ and so on with as many upper indices as you have a rank . the transformation law is one a for each index , meaning each index transforms separately as a vector . a covariant vector , or covector , is a linear function from vectors to numbers . this is described completely by the coefficients , $u_i$ , and the linear function is $$ u_i v^i = \sum_i u_i v^i = u_1 v^1 + u_2 v^2 + u_3 v^3 $$ where the einstein convention is employed in the first expression , which just means that if the same index name occurs twice , once lower and once upper , you understand that you are supposed to sum over the index , and you say the index is contracted . the most general linear function is some linear combination of the three components with some coefficients , so this is the general covector . the transformation law for a covector must be by the inverse matrix $$ u'_i = \bar{a}_i^j u_j $$ matrix multiplication is simple in the einstein convention : $$ m^i_j n^j_k = ( mn ) ^i_k $$ and the definition of $\bar{a}$ ( the inverse matrix ) makes it that the inner product $u_i v^i$ stays the same under a coordinate transformation ( you should check this ) . a rank-2 covariant tensor is a covector of covectors , and so on to arbitrarily high rank . you can also make a rank m , n tensor $t^{i_1 i_2 . . . i_m}_{j_1j_2 . . . j_n}$ , with m upper and n lower indices . each index transforms separately as a vector or covector according to whether it is up or down . any lower index may be contracted with any upper index in a tensor product , since this is an invariant operation . this means that the rank m , n tensors can be viewed in many ways : as the most general linear function from m covectors and n vectors into numbers as the most general linear function from a rank m covariant tensor into a rank n contravariant tensor as the most general linear function from a rank n contravariant tensor into a rank m covariant tensor . and so on for a number of interpretations that grows exponentially with the rank . this is the mathemtician 's preferred definition , which does not emphasize the transformation properties , rather it emphasizes the linear maps involved . the two definitions are identical , but i am happy i learned the physicist definition first . in ordinary euclidean space in rectangular coordinates , you do not need to distinguish between vectors and covectors , because rotation matrices have an inverse which is their transpose , which means that covectors and vectors transform the same under rotations . this means that you can have only up indices , or only down , it does not matter . you can replace an upper index with a lower index keeping the components unchanged . in a more general situation , the map between vectors and covectors is called a metric tensor $g_{ij}$ . this tensor takes a vector v and produces a covector ( traditionally written with the same name but with a lower index ) $$ v_i = g_{ij} v^i$$ and this allows you to define a notion of length $$ |v|^2 = v_i v^i = g_{ij}v^i v^j $$ this is also a notion of dot-product , which can be extracted from the notion of length as follows : $$ 2 v\cdot u = |v+u|^2 - |v|^2 - |u|^2 = 2 g_{\mu\nu} v^\mu u^\nu $$ in euclidean space , the metric tensor $g_{ij}= \delta_{ij}$ which is the kronecker delta . it is like the identity matrix , except it is a tensor , not a matrix ( a matrix takes vectors to vectors , so it has one upper and one lower index--- note that this means it automatically takes covectors to covectors , this is multiplication of the covector by the transpose matrix in matrix notation , but einstein notation subsumes and extends matrix notation , so it is best to think of all matrix operations as shorthand for some index contractions ) . the calculus of tensors is important , because many quantities are naturally vectors of vectors . the stress tensor : if you have a scalar conserved quantity , the current density of the charge is a vector . if you have a vector conserved quantity ( like momentum ) , the current density of momentum is a tensor , called the stress tensor the tensor of inertia : for rotational motion of rigid object , the angular velocity is a vector and the angular momentum is a vector which is a linear function of the angular velocity . the linear map between them is called the tensor of inertia . only for highly symmetric bodies is the tensor proportional to $\delta^i_j$ , so that the two always point in the same direction . this is omitted from elementary mechanics courses , because tensors are considered too abstract . axial vectors : every axial vector in a parity preserving theory can be thought of as a rank 2 antisymmetric tensor , by mapping with the tensor $\epsilon_{ijk}$ high spin represnetations : the theory of group representations is incomprehensible without tensors , and is relatively intuitive if you use them . curvature : the curvature of a manifold is the linear change in a vector when you take it around a closed loop formed by two vectors . it is a linear function of three vectors which produces a vector , and is naturally a rank 1,3 tensor . metric tensor : this was discussed before . this is the main ingredient of general relativity differential forms : these are antisymmetric tensors of rank n , meaning tensors which have the property that $a_{ij} =-a_{ji}$ and the analogous thing for higher rank , where you get a minus sign for each transposition . in general , tensors are the founding tool for group representations , and you need them for all aspects of physics , since symmetry is so central to physics .
no . frequency is defined as 2π*θ/t where theta is the angle rotated for a time t . you maybe tempted to equate frequency to angular velocity . but it is not so . angular velocity = dθ/dt . angular frequency= 2*pi* ( integral of x over time interval t ) /t
first it is important to note that gravitational waves do require energy to produce . a good example of this is a binary pulsar , where the emission of gravity waves carries energy away so the two pulsars spiral in towards each other and will eventually merge . having said this , it is theoretically possible to modulate a gravitational wave and use it to transmit information . you do not need a mass modulator , you just need something with a changing quadropole moment - the simplest example of this is a spinning dumbbell , and indeed this is basically what the binary pulsar system is . if you can change the rotation frequency you can frequency modulate the gravitational wave . however gravitational waves are exceedingly hard to generate in the sense that very little of the energy of your system is carried away as gravitational waves . at the moment we can not even detect gravitational waves let alone generate them . it does not seem likely we will ever use gravitational waves for transmitting information .
the question is clear enough to answer . the question does involve mass defect . the best place to start is with ordinary problems . we can consider a nonrelativistic problem of a particle in a potential well , $v ( r ) ~=~-r^n$ for some power of $n$ . for mathematical reasons $n~=~\pm 2$ have nice properties with closed form solutions . this is input into a schrodinger equation $$ i\hbar\frac{\partial}{\partial t}\psi ( r , t ) ~=~-\frac{\hbar^2\nabla^2}{2m}\psi ( r , t ) ~+~v\psi ( r , t ) $$ so if we consider a stationary phase $\psi ( r , t ) ~=~e^{-i\omega t}\psi ( r ) $ and the frequency determines energy $e~=~\hbar\omega$ $$ \hbar\omega~=~\frac{p^2}{2m}~+~v $$ where the potential energy is negative . if the potential energy were “turned off” so the particles are free then energy is larger . the energy of system is then lower , and if this energy $e~=~mc^2$ is some appreciable fraction of the mass-energy of the system $e’~=~mc^2$ , say $m/m~~\simeq~ . 01$ to $ . 1$ the system is not highly relativistic but the mass equivalence is measurable . for atomic physics the energy levels of electrons are on the order of electron volts , while the mass of electrons are $ . 51$mev . so the mass defect is pretty small . for nuclear physics of nucleons and mesons the energy levels are on the order of $10mev$ while the collection of nucleons has mass-energy in multiples of $1gev$ . the energy levels are determined by $\pi^0 , ~\pi^\pm$ mesons , which are the intermediary gauge bosons between the nucleons ${p , ~n}$ . this in fact forms a doublet which has energy level splitting due to the electric charge difference . the above model may be made more exact if the mesons are considered to be similar to a photon , with a gauge potential $$ {\vec a} ( k ) ~=~{\vec n} ( a_ke^{-ikr}~+~a^\dagger_ke^{ikr} ) $$ which interacts with a dipole formed from the nucleon doublet ${\vec{\cal p}}~=~p{vec\sigma}$ in an interaction hamiltonian $$ h_{int}~=~-{\vec{\cal p}}\cdot{\vec a} ( k ) . $$ we are only considering interactions with one momentum or wave number . now expand that out and keep terms $a ( k ) \sigma^+$ and $a^\dagger\sigma^-$ , which is the rotating wave approximation in atomic interactions with photons . this makes the above schrodinger equation and potential more exact . this interaction hamiltonian will then reproduce the mass-defect . this may be further improved of course . the nucleon doublet is su ( 2 ) , and the meson potential may be extended from this naïve u ( 1 ) approximation to su ( 2 ) as well . the momentum operator may be made covariant with respect of the gauge potential and the theory refined further . in fact the yang-mills theory was derived to understand this isopin theory of nuclear physics as understood in the 1950s .
you can analyze this by imagining a tiny tiny gap between the two masses . physically we have exactly that , as the electrons at the surface of the first block are certainly not in contact with the electrons at the surface of the second block . then we have a series of two collisions : the first is the initial impulse , the second is the collision between the two objects . the answer will differ , of course , depending on the degree of elasticity of the collision .
yes , at least for functional enough lens or mirrors . as long as the answer is " yes " , the lenses or mirrors will produce an image that is totally sharp . if the answer were " no " , a point-like source of light would always look like a fuzzy disk . the lenses and mirrors and telescopes may be optimized at least for a whole " two-dimensional locus " where the light source may be located to produce sharp images . the shape of the lenses may be constructed so that this condition is satisfied exactly : a real condition ( the light ray gets to the right point ) has to be satisfied for each distance $y$ from the axis - but one has at least one variable , $x ( y ) $ ( the thickness of the lens ) , to adjust for each $y$ , too . in the first subleading approximation , the shape is always the same : as a function of the vertical coordinate $y$ , the thickness of the glass in the horizontal direction goes like $a+by^2$ . that approximates a circle , parabola , hyperbola , or anything else , up to errors of order $o ( y^4 ) $ . if you want to totally neglect those terms , it is like neglecting $o ( \theta^4 ) $ terms , quartic in the angle . in this approximation , the angle between the light ray and the horizontal line that is needed for convergence is small and linearly depends on $y$ . the linear dependence of the angles on $y$ - how much the direction of the light ray is changed when switching from the air to the glass or back - is equivalent to the quadratic shape of the mirror sketched above ( the angle change is a derivative of the shape because this derivative determines the slope of the glass at a given point ) . this explains why it is not a " miraculous conspiracy": the change of the angle is a linear function of $y$ so the shape of the glass must be a quadratic function of $y$ . in reality , where the $o ( \theta^4 ) $ terms in the shape can not be neglected , the light sources may be away from the plane where the convergence was guaranteed , and in that case , the answer will be " no " and the image will inevitably be fuzzy . however , it is important to note that e.g. telescopes that observe stars " at infinity " may always be adjusted so that all the images are sharp . because the shape of the lens actually has two functions , $x_{left} ( y ) $ and $x_{right} ( y ) $ to be adjusted , one may actually guarantee that the condition " light rays converge " is exactly satisfied in a whole region of 3d space , at least in some situations .
to understand this paradox it is best to forget about everything you know ( even from sr ) because all of that just causes confusion and start with just a few simple concepts . first of them is that the space-time carries a metric that tells you how to measure distance and time . in the case of sr this metric is extremely simple and it is possible to introduce simple $x$ , $t$ coordinates ( i will work in 1+1 and $c = 1$ ) in which space-time interval looks like this $$ ds^2 = -dt^2 + dx^2$$ let 's see how this works on this simple doodle i put together the vertical direction is time-like and the horizontal is space-like . e.g. the blue line has " length " $ds_1^2 = -20^2 = -400$ in the square units of the picture ( note the minus sign that corresponds to time-like direction ) and each of the red lines has length zero ( they represent the trajectories of light ) . the length of the green line is $ds_2^2 = -20^2 + 10^2 = -300$ . to compute proper times along those trajectories you can use $d\tau^2 = -ds^2$ . we can see that the trip will take the green twin shorter proper time than the blue twin . in other words , green twin will be younger . more generally , any kind of curved path you might imagine between top and bottom will take shorter time than the blue path . this is because time-like geodesics ( which are just upward pointing straight lines in minkowski space ) between two points maximize the proper time . essentially this can be seen to arise because any deviation from the straight line will induce unnecessary space-like contributions to the space-time interval . you can see that there was no paradox because we treated the problem as what is really was : computation of proper-time of the general trajectories . note that this is the only way to approach this kind of problems in gr . in sr that are other approaches because of its homogeneity and flatness and if done carefully , lead to the same results . it is just that people often are not careful enough and that is what leads to paradoxes . so in my opinion , it is useful to take the lesson from gr here and forget about all those ad-hoc sr calculations . just to give you a taste what a sr calculation might look like : because of globally nice coordinates , people are tempted to describe also distant phenomena ( which does not really make sense , physics is always only local ) . so the blue twin might decide to compute the age of the green twin . this will work nicely because it is in the inertial frame of reference , so it'll arrive at the same result we did . but the green twin will come to strange conclusions . both straight lines of its trajectory will work just fine and if it were not for the turn , the blue twin would need to be younger from the green twin 's viewpoint too . so the green twin has to conclude that the fact that blue twin was in a strong gravitational field ( which is equivalent to the acceleration that makes green twin turn ) makes it older . this gives a mathematically correct result ( if computed carefully ) , but of course , physically it is a complete nonsense . you just can not expect that your local acceleration has any effect on a distant observer . the point that has to be taken here ( and that gr makes clear only too well ) is that you should never try to talk about distant objects .
i think you are observing " the hot chocolate effect " or something similar . see crawford , am . j . phys . 50 , 398 ( 1982 ) . i have to confess i have not read through the paper in enough detail to adequately summarize it .
i am going to offer a completely different way of doing this . sometimes it is nice to work in symbols before getting into the very specific numbers . i take my inspiration from the bohr atomic model here . the total energy of the electron in orbit around the nucleus at any given radius is calculated as follows ( wikipedia 's equation here , not mine ) . that is , the sum of the kinetic and potential energy is just half the potential energy ! neat , is not it ? so , how would we apply this to the earth ? well , $z k_e e^2$ is going to have to be replaced with $g m m$ . but let 's not forget , the entire point was to introduce a radius $r&#39 ; =1.05 r$ , and i am seeking a value of $\delta e = e&#39 ; -e$ . also , the kinetic energy is 1/2 the magnitude of this total energy metric , i will use $e_k$ for that . $$\delta e = gmm/2 \left ( -1/r&#39 ; + 1/r \right ) = e \left ( -1/1.05+1\right ) =e\frac{0.05}{1.05} = 2 e_k \frac{0.05}{1.05}$$ so the energy would change by about 9.5% times the original kinetic energy . given your original energy , i believe this would be $8.8 \times 10^9 j$ . this all said , your question says : if the initial magnitude of the satellite’s mechanical energy was $e_{m , i} = 9.26 \cdot 10^{10}$ j and it continues at the same speed , how much work was done by the rockets in moving the satellite to the higher orbit ? our work assumed that it would attain a new speed . maybe the question is written wrong . i do not know .
yes , poynting 's theorem holds even if charges or currents are crossing the surface . it is useful to study the proof of the theorem . it really boils down to maxwell 's equations only . write down some multiples and/or ( additional ) derivatives of maxwell 's equations , add them up , and you get poynting 's theorem . whenever maxwell 's equations hold , the theorem must hold , too . it would be bad if charges were not allowed to cross the surface . indeed , charges crossing the surface are nothing else than ( the elementary description of ) currents and the work done by/on currents is indeed one of the important contributions to the energy conservation law that the theorem describes very well . it is even clearer that you get a nonzero contribution to the $\vec e\times \vec b$ if you focus on the plane in between the charges . two repelling charges imagine they are separated in the $x$ direction . the electric fields on the $x$-axis go in the $x$-direction , too . the direction is appropriately tilted towards the $y$ or $z$-axes if we move away from the $x$-axis . as the charges start to accelerate , one effectively gets a current in the $x$-direction and there will be magnetic fields encircling the $x$-axis . the cross product of $\vec e$ mentioned in the previous paragraph and this " around " $\vec b$ vector will include the component in the $x$-direction again , with the opposite signs on the two sides of the two-charge system ( we are assuming the same signs of the charges ) . so this $\vec e\times b$ integrated over the transverse areas near the $x$-axis are exactly what will contribute to the equation of the poynting 's theorem . one point charge poynting 's theorem is a law in classical electrodynamics . classical electrodynamics does not contain " photons " . it only contains the electromagnetic waves – they are interpreted as a coherent state of many photons from the quantum theory . it is not just a matter of interpretations . classically , incoming electromagnetic waves are never " fully absorbed " by an object . it would violate the second law of thermodynamics , among other things . in the real , quantum world , absorption of electromagnetic waves occurs because the photons are being absorbed one by one and kick the atoms to higher discrete energy levels . but if the energy is continuous , and it always is in classical electrodynamics , there are always " soft photons " emitted and the electromagnetic field never disappears completely in the final state . at any rate , even if you could prepare such a fine-tuned situation in which the electromagnetic wave would be entirely absorbed , the theorem would work because it always works whenever maxwell 's equations do .
1 ) why does length contraction only occur in the direction of travel , ( not in all directions ) when approaching the speed of light ? there are a number of ways to answer that vary in the level of sophistication . but first , do understand that the lorentz transformation gives length contraction only in the direction of motion . so , if you accept the lorentz transformation as the correct coordinate transformation between relatively moving reference frames , you accept that there is no transverse contraction . also remember , the lorentz transformation was originally derived so that the measured speed of light would always be c in any inertial reference frame . but , assume for the sake of argument that transverse length contraction does occur . we almost immediately arrive at a contradiction . consider a wall upon which two parallel horizontal stripes have been painted . the vertical distance between the stripes is exactly 1 meter which can be confirmed by placing an ideal meter stick vertically on the wall and finding that the ends of the meter stick touch the top and bottom stripes . now , imagine that the same vertical meter stick moves horizontally with respect to the wall . if there is vertical contraction then , according to someone at rest with respect to the wall , the meter stick no longer reaches between both lines . since the stick is vertically contracted , the spacing between the lines is greater than the length of the meter stick . however , to someone at rest with respect to the meter stick , it is the wall that is moving and it is the wall that is vertically contracted . to this person , the meter stick overlaps the two lines ; the spacing between the lines is less than the length of the meter stick . but this is a contradiction ! it cannot be the case that the top end of the meter stick is both below and above the top stripe and similarly for the bottom end . we conclude that transverse contraction leads to a paradox . therefore an outside observer would then see the car expanded by a factor of 2 not contracted . no , that is not the correct conclusion . first , the phrase " outside observer " is ambiguous . what we should specify here is whether the observer is at rest with respect to the road or with respect to the car . for an observer at rest with respect to the road , your car is contracted and fits in between the markers . for an observer at rest with respect to your car , the distance between the markers is contracted and your car is longer than the markers . this is not a contradiction because , and this is crucial , of the relativity of simultaneity ; relatively moving observers do not agree on whether spatially separated events , along the axis of motion , are simultaneous . in order to tell if the car fits between the markers , we must determine the location of the rear and front of the car at the same time according to spatially separated clocks synchronized in our reference frame . so , imagine that there is a clock at the front of the car and at the rear of the car and that they are synchronized by einstein synchronization according to the observer at rest with respect to the car . then , at some time , the location of the clocks are recorded . according to the observer at rest with respect to the car , the recording of the location of the two clocks occurred at the same time . thus , the difference in the location of the clocks is the length of the car . however , according to the observer at rest with respect to the road , the locations were not recorded at the same time , i.e. , the clocks on the car are not synchronized . thus , the two observers do not agree on the length of the car .
your question " is the predictability of the future to whatever extent is possible ( based on the present and the past ) equivalent to the principle of causality ? " has the trivial answer ''no'' as the qualification ''to whatever extent is possible'' turns your assumption into a tautology . the tautology makes your statement false , as your question asks whether the universally true statement is equivalent to causality . an answer " true " would make any theory causal , thus making the concept meaningless . why is your assumption a tautology ? no matter which theory one considers , the future is always predictable to precisely the extent this is possible ( based on whatever knowledge one has ) . in particular , this is the case even in a classical relativistic theory with tachyons or in theories where antimatter moves from the future to the past . however , in orthodox quantum mechanics and quantum field theory , causality is related to prepareability , not to predictability . on the quantum field theory level ( from which all higher levels derive ) , causality means that arbitrary observable operators $a$ and $b$ constructed from the fields of the qft at points in supports $x_a$ and $x_b$ in space-time commute whenever $x_a$ and $x_b$ are causally independent , i.e. , if ( x_a-x_b is spacelike for arbitrary $x_a\in x_a$ and . $x_b\in x_b$ . loosely speaking , this is equivalent to the requirement that that , at least in principle , arbitrary observables can be independently prepared in causally independent regions . arguments from representation theory ( almost completely presented in volume 1 of the qft books by weinberg ) then imply that all observable fields must realize causal unitary representations of the poincare group , i.e. , representations in which the spectrum of the momentum 4-vector is timelike or lightlike . this excludes tachyon states . while the latter may occur as unobservable unrenormalized fields in qfts with broken symmetry , the observable fields are causal even in this case .
if you had a perfect scale , the reading would fluctuate based on $$\delta w = m\ddot{x}_{cm}$$ $\delta w$ is the size of the fluctuation in the reading , $m$ the total mass on the scale ( including fly and air ) , and $\ddot{x}_{cm}$ the acceleration of the center of mass . integrating over time , $$\int_{time} \delta w ( t ) = m\delta ( \dot{x}_{cm} ) $$ here , $\delta ( \dot{x}_{cm} ) $ is the change in velocity of the center of mass over the period you observe the readings . because the velocity of the center of mass cannot change very much , if you integrate the fluctuations over time , you wind find that their average tends towards zero . if the fly begins and ends in the same place and the air is still , the fluctuations integrate out to exactly zero . whenever the fly is accelerating up , we expect the reading to be a little higher than normal . when the fly accelerates down , we expect the reading to be a little lower than normal . if the fly hovers in a steady state , the reading will be the same as if the fly were still sitting on the bottom . a real scale cannot adjust itself perfectly and instantaneously , so we would need to know more details of the scale to say more about the real reading .
in general , the answer is no . this type of inverse problem is sometimes referred to as : " can one hear the shape of a drum " . the following extensive exposition by beals and greiner discusses various problems of this type . despite the fact that one can get a lot of geometrical and topological information from the spectrum or even its asymptotic behavior , this information is not complete even for systems as simple as quantum mechanics along a finite interval .
there is no " better " or worse here . it is just that " work " in physics is defined differently than in chemistry . in chemistry , all quantities follow this sign convention : they are positive if their effect is on the system . so , basically , $du$ is ( infinitesimal ) energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done on the system by the surroundings in physics , the sign convention of $w$ is the opposite $du$ is energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done by the system on the surroundings which means that $du_c = \delta q_c + \delta w_c$ ( $c$ means chemistry ) becomes $d u_p = \delta q_p - \delta w_p$ try to keep these conventions separate in your mind . do not use the physics flt for a chemistry problem and vice versa , many times problems specify values of $w , q , u$ and expect you to know the sign convention . note that these are the iupac/iupap conventions . some books ( as @dmckee mentions , feynman 's lectures is one of them ) use different conventions . in such cases , just make note of the convention and remember that the flt is just a statement of conservation of energy . here 's the menemonic i used to remember it . it is not a great one , but it works : chemists are interested in supplying hear/energy/pressure to a reaction to make it occur . thus , action done by the surroundings on the system is " good " or " positive . physicists are more interested in supplying heat/energy to a system and making it do work . so , supplying heat/energy is " good " , and getting work out is " good " .
it depends if you already have the mass moment of inertia tensor defined or not . if you know the body inertia is $i_{body}$ and the 3x3 rotation matrix is $e$ then the angular momentum vector at the center of gravity c is $$ \vec{h}_c = \left ( e i_{body} e^\top \right ) \vec{\omega} $$ and the linear momentum vector is $$\vec{l} = m \vec{v}_c$$ the mass moment of inertia tensor along the world coordinates on the center of gravity is $i_c = e i_{body} e^\top $ which transforms the rotational velocity $\vec \omega$ into local coordinates , multiplies by $i_{body}$ and transforms backs into world coordinates . now the equations of motion on the center of gravity are defined from the sum of forces and moments equals the rate of change of momentum $$ \sum \vec{f} = \dot{\vec{l}} $$ $$ \sum \vec{m}_c = \dot{\vec{h}}_c $$ or $$ \sum \vec{f} = m \vec{a}_c $$ $$ \sum \vec{m}_c = i_c \vec{\alpha} + \vec{\omega} \times i_c \vec{\omega} $$ since the time derivative of angular momentum on a rotating frame is $\dot{\vec{h}_c} = \frac{\partial \vec{h}_c}{\partial t} + \vec{\omega} \times \vec{h}_c $ note that $\dot{\vec{v}} = \vec{a} $ and $\dot{\vec{\omega}} = \vec{\alpha} $ . now to describe the equations on a frame a not on c use the following transformations ( with relative position of the c.g. $ \vec{c} =\vec{r}_c - \vec{r}_a $ . $$ \vec{a}_c = \vec{a}_a + \vec{\alpha} \times \vec{c} + \vec{\omega} \times \vec{\omega} \times \vec{c} $$ $$ \sum \vec{m}_c = \sum \vec{m}_a + \vec{c} \times \sum \vec{f} $$ so finally the equations of motion of a rigid body , as described by a frame a not on the center of gravity c is ( rather messy ) $$ \boxed{ \begin{aligned} \sum \vec{f} and = m \vec{a}_a - m \vec{c}\times \vec{\alpha} + m \vec{\omega}\times\vec{\omega}\times\vec{c} \\ \sum \vec{m}_a and = i_c \vec{\alpha} + m \vec{c} \times \vec{a}_a - m \vec{c} \times \vec{c} \times \vec{\alpha} +\vec{\omega} i_c \vec{\omega} + m \vec{c} \times \left ( \vec{\omega} \times \vec{\omega} \times \vec{c} \right ) \end{aligned} } $$ which is why people use the spatial notation ( look up screw theory ) to compact the above into $$ \sum \bf{f}_a = i_a \bf{a}_a + \bf{p} $$ $$ \begin{pmatrix} \sum \vec{f} \\ \sum \vec{m}_a \end{pmatrix} = \begin{bmatrix} m and -m \vec{c}\times \\ m \vec{c}\times and i_c - m \vec{c}\times \vec{c}\times \end{bmatrix} \begin{pmatrix} \vec{a}_a \\ \vec{\alpha} \end{pmatrix} + \begin{bmatrix} 1 and 0 \\ \vec{c}\times and 1 \end{bmatrix} \begin{pmatrix} m \vec{\omega}\times\vec{\omega}\times\vec{c} \\ \vec{\omega}\times i_c \vec{\omega} \end{pmatrix} $$ notice the above the $0$ and $1$ are 3x3 matrices and $\vec{c}\times$ is the 3x3 cross product operator defined by $$ \vec{ \begin{pmatrix} x\\y\\z \end{pmatrix} }\times = \begin{vmatrix} 0 and -z and y \\ z and 0 and -x \\ -y and x and 0 \end{vmatrix} $$ now the big 6x6 matrix multiplying the acceleration term is the spatial inertia at a . more here and here .
a caesium clock generates a microwave signal that it tunes to match the absorption peak in the caesium spectrum . a counter counts every oscillation of the generated microwave , and every 9,192,631,770 counts = 1 second . that is how the clock counts the seconds and keeps time . but if the peak in the caesium spectrum is broad it is difficult to tune the microwave generator to exactly the peak maximum . that means there is a potential error in the microwave frequency and hence in the measured time . this will make the clock run slow or fast . so the narrowness of the peak is very important to keeping accurate time . when i last looked the limitation on the peak width was doppler broadening . this happens because some of the caesium atoms are moving towards the microwave generator and some caesium atoms are moving away . this motion shifts the position of the absorption peak , and the end result is that the peak broadens . you can design the clock to reduce the speed the caesium atoms move , and in principle you can eliminate doppler broadening . however the uncertainty principle sets a lower bound for the peak width . for an electronic transition like the one in caesium the uncertainty principle tells us that : $$ \delta e \delta \tau &gt ; = \frac{\hbar}{2} $$ where $\delta \tau$ is the lifetime of the excited state i.e. the average time taken for the excited state to emit a photon and relax to the lower energy state . this $\delta \tau$ is a characteristic of the caesium atom and is effectively a constant beyond our control . this matters because the energy $e$ is related to frequency by $e = h\nu$ , so we end up with an uncertainty in the frequency given by : $$ h\delta \nu &gt ; = \frac{\hbar}{2} \frac{1}{\delta \tau} $$ this tells us we will never be able to tune the microwave frequency to better than $\delta \nu$ , so there is a fundamental source of error that we cannot eliminate .
in the position representation , the matrix elements ( wavefunction ) of a momentum eigenstate are $$\langle x | p\rangle = \psi_p ( x ) = e^{ipx}$$ the wavefunction shifted by a constant finite translation $a$ is $$\psi ( x+a ) $$ now the momentum operator is the thing which , acting on the momentum eigenstates , returns the value of the momentum in these states , this is clearly $-i\frac{d}{dx}$ . for our momentum eigenstate , if i spatially shift it by an infinitesimal amount $\epsilon$ , it becomes $$\psi ( x+\epsilon ) = e^{ip ( x+\epsilon ) } = e^{ip\epsilon}e^{ipx} = ( 1+i\epsilon p + . . . ) e^{ipx}$$ i.e. the shift modifies it by an expansion in its momentum value . but if i taylor expand $\psi ( x+\epsilon ) $ , i get $$\psi ( x+\epsilon ) = \psi ( x ) +\epsilon \frac{d}{dx} \psi ( x ) + . . . = \psi ( x ) +i\epsilon ( -i\frac{d}{dx} ) \psi ( x ) + . . . $$ so this is consistent since the infinitesimal spatial shift operator $-i\frac{d}{dx}$ is precisely the operator which is pulling out the momentum eigenvalue .
the other answers provide a first-order approximation , assuming uniform density ( though adam zalcman 's does allude to deviations from linearity ) . ( summary : all the mass farther away from the center cancels out , and gravity decreases linearly with depth from 1 g at the surface to zero at the center . ) but in fact , the earth 's core is substantially more dense than the outer layers ( mantle and crust ) , and gravity actually increases a bit as you descend , reaching a maximum at the boundary between the outer core and the lower mantle . within the core , it rapidly drops to zero as you approach the center , where the planet 's entire mass is exerting a gravitational pull from all directions . this wikipedia article goes into the details , including this graph . and there are other , smaller , effects as well . the earth 's rotation results in a smaller effective gravity near the equator , the equatorial bulge that results from that rotation also has a small effect , and mass concentrations have local effects .
the industrial tool is zemax ; however , it is very expensive . if you just want to make diagrams , the tex package pst-optics might do the trick . in the gaussian beam regime , optocad ( free ) is a tool often used in the laser interferometer gravitational wave detector community .
euler 's rotational theorem only states , that we can determine a unique axis of the rotation for any given moment . that does not necessarily mean that the axis of rotation 's direction is fixed forever . quite opposite , let 's suppose that we have body with no external force acting upon it . if axis of rotation at some ( starting ) moment does not coincide with one of the three principal axis of moment of inertia , the axis of rotation shall change and you shall have complex rotation of that body ( precession ) . you can see that directly from euler 's equations by putting torque to be zero .
you may have encountered it in a different context , but i recognize it from the topic of singularities of correlation functions in quantum field theory . in massless field theories such correlation fucntions becomes singular for points which are lightlike separated , and the structure of such singularities is determined by good physical principles such as locality and unitarity . then again , i may be completely off the mark . in any event , except for the linguistic similarity , i do not think it has to do with null singularities in spacetime .
there is no quantum mechanics of a photon , only a quantum field theory of electromagnetic radiation . the reason is that photons are never non-relativistic and they can be freely emitted and absorbed , hence no photon number conservation . still , there exists a direction of research where people try to reinterpret certain quantities of electromagnetic field in terms of the photon wave function , see for example this paper .
two things stand out to my eye : 1 ) in your volume integral , when you switched to spherical coordinates , you put in a $4 \pi r \ dr$ where there should be a $4 \pi r^2 dr$ . 2 ) your evaluation of the 1-d integral is not quite right . i think these two are enough to explain the discrepancy .
33 mj is the electrical energy . i think the projectile is about 1kg , so the efficiency is about 10% , not so bad . plasma from electrical breakdown , which then gets accelerated the same way the projectile does , with exb force
most of the popular science tv programmes and magazine articles give entirely the wrong idea about how the higgs mechanism works . they tend to give the impression that there is a single higgs boson that ( a ) causes particles masses and ( b ) will be found around 125gev by the lhc . the mass is generated by the higgs field . see the wikipedia article on the higgs mechanism for details . to ( over ) simplify , the higgs field has four degrees of freedom , three of which interact with the w and z bosons and generate masses . the remaining degree of freedom is what we see as the 125gev higgs boson . in a sense , the higgs boson that the lhc is about to discover is just what is left over after the higgs field has done it is work . the higgs boson gets its mass from the higgs mechanism just like the w and z bosons : it is not the origin of the particle masses . the higgs boson does not have zero rest mass . a quick footnote : matt strassler 's blog has an excellent article about this . the higgs mass can be written as an interaction with the higgs field just like e.g. the w boson . however matt strassler makes the point that this is a coincidence rather than anything fundamental and unlike the w and z the higgs boson could have a non-zero mass even if the higgs field was zero everywhere .
before i answer , a couple caveats : as adam said , the universe is not going to start behaving any differently because we discovered something . right now it seems much more likely ( even by admission of the experimenters ) that it is just a mistake somewhere in the analysis , not an actual case of superluminal motion . anyway : if the discovery turns out to be real , the effect on theoretical physics will be huge , basically because it has the potential to invalidate special relativity shows that special relativity is incomplete . that would have a " ripple effect " through the last century of progress in theoretical physics : almost every branch of theoretical physics for the past 70+ years uses relativity in one way or another , and many of the predictions that have emerged from those theories would have to be reexamined . ( there are many other predictions based on relativity that we have directly tested , and those will continue to be perfectly valid regardless of what happens . ) to be specific , one of the key predictions that emerges out of the special theory of relativity is that " ordinary " ( real-mass ) particles cannot reach or exceed the speed of light . this is not just an arbitrary rule like a speed limit on a highway , either . relativity is fundamentally based on a mathematical model of how objects move , the lorentz group . basically , when you go from sitting still to moving , your viewpoint on the universe changes in a way specified by a lorentz transformation , or " boost , " which basically entails mixing time and space a little bit . ( time dilation and length contraction , if you are familiar with them ) we have verified to high precision that this is actually true , i.e. that the observed consequences of changing your velocity do match what the lorentz boost predicts . however , there is no lorentz boost that takes an object from moving slower than light to moving faster than light . if we were to discover a particle moving faster than light , we have a type of motion that can not be described by a lorentz boosts , which means we have to start looking for something else ( other than relativity ) to describe it . now , having said that , there are a few ( more ) caveats . first , even if the detection is real , we have to ask ourselves whether we have really found a real-mass particle . the alternative is that we might have a particle with an imaginary mass , a true tachyon , which is consistent with relativity . tachyons are theoretically inconvenient , though ( well , that is putting it mildly ) . the main objection is that if we can interact with tachyons , we could use them to send messages back in time : if a tachyon travels between point a and point b , it is not well-defined whether it started from point a and went to point b or it started from b and went to point a . the two situations can be transformed into each other by a lorentz boost , which means that depending on how you are moving , you could see one or the other . ( that is not the case for normal motion . ) this idea has been investigated in the past , but i am not sure whether anything useful came of it , and i have my doubts that this is the case , anyway . if we have not found a tachyon , then perhaps we just have to accept that relativity is incomplete . this is called " lorentz violation " in the lingo . people have done some research on lorentz-violating theories , but it is always been sort of a fringe topic ; the main intention has been to show that it leads to inconsistencies , thereby " proving " that the universe has to be lorentz-invariant . if we have discovered superluminal motion , though , people will start looking much more closely at those theories , which means there is going to be a lot of work for theoretical physicists in the years to come .
the book answer seems correct ( it can be obtained from the equations for the bottom body ) . i do not know how you obtained your result , but there is a possibility that it is also correct and actually coincides with the book answer , just because the equations for the top body provide an extra relation between $\alpha$ and $\beta$ .
actually , the answer is a bit more subtle than just density . the principle that is behind floating objects is archimedes ' principle : a fluid ( liquid or gas ) exerts a buoyant force , opposite apparent gravity ( i.e. . gravity + acceleration of fluid ) on an immersed object that is equal to the weight of the displaced fluid . thus , if you have an object fully immersed in a fluid , the total force it feels is given by ( positive sign means down ) : $f = gravity + buoyancy = \rho_{object} v g - \rho_{fluid} v g = ( \rho_{object} - \rho_{fluid} ) v g$ thus , if the average density of the object is lower than that of the water , it floats . if the object is partially immersed , to calculate the buoyant force you have to consider just the immersed volume and its average density : $f = \rho_{object} v g - \rho_{fluid} v_{immersed} g$ note that when i was talking about density , i was talking about the average density of the object . that is its total mass divided by its volume . thus , a ship , even if it is made out of high-density iron it is full of air . that air will lower the average density , as it will increase the volume considerably while keeping the weight almost constant . if you want to understand this better you can give the following problem a try : ) what is the height an ice cube of side l floats in water ?
in my experience this is entirely possible in a ' warmer ' climate ie southern britain . i can give you advice on that basis , maybe someone else will advise on dealing with the cold ! can i suggest something like the robodome , this is an automated dome to house the scope ( upto 10" aperture is suitable but tight . ) which can be synced to a weather station and the telescope controls . operating can be done fairly simply via r232 and or usb cables , the usb will need to be powered to deal with the distance . remote operation is no different really from a wired connection while stood next to the scope . but ! remeber that a scope drive will have enough power to crush fingers , mains electricity is enough to kill and if something goes wrong you could a lot of equipment at the mercy of the weather . so some kind of safety plan is needed . . . minimum list : dome , scope , ccd , webcam , cabling , weather station , motorised mount . [ to track add a tracking camera/on ccd chip . ] setting up your own observatory is a lot of fun though , sitting in the warm is definitely the way to go !
acceleration is the derivative of velocity with respect to time , which is not the same as just dividing the difference in velocity by the difference in time . $$a=\frac{dv}{dt}\neq \frac{\delta v}{\delta t}$$ $dv$ and $dt$ can be thought of as infinitely small $\delta v$ and $\delta t$ . so if your $\delta v$ and $\delta t$ are quite small , the result of your calculation will be pretty close to the actual acceleration . but in your case , $\delta v$ and $\delta t$ are pretty big . therefore , your calculation is way of . we can see that our result becomes more accurate if we consider an eighth of a circle : $$\frac{\delta v}{\delta t}=\frac{ \sqrt{ ( v_{x0}-v_{x1} ) ^2 + ( v_{y0}-v_{y1} ) ^2} }{5/2}$$ now we plug in $v_{x0}=20$ , $v_{y0}=0$ and $v_{x1}=v_{y1}=20\cdot\sqrt{\frac{1}{2}}$ . $$\frac{\delta v}{\delta t}=2\frac{ \sqrt{20^2 ( 1-\sqrt{\frac{1}{2}} ) ^2 + 20^2 ( 0-\sqrt{\frac{1}{2}} ) ^2} }{5}=40\frac{ \sqrt{1-2\sqrt{\frac{1}{2}} + \frac{1}{2} + \frac{1}{2}} }{5}=8\sqrt{2-\sqrt{2}} \approx a$$ and we use $$a=\frac{v^2}{r} \rightarrow r =\frac{v^2}{a}$$ $$c=2\pi r = 2\pi \frac{v^2}{a} \approx 2\pi \frac{20^2}{8\sqrt{2-\sqrt{2}}} \approx 410.46\space m$$ as you can see , this is much closer to the real value of the circumference . this is because we chose a smaller part of the circle , and because of that a smaller $\delta v$ and $\delta t$ . if you pick smaller and smaller parts of the circle , your value will get closer and closer to the real value . and in the limit of choosing a part of the circle with length 0 , it will be the exact value . this is exactly what the '$d$' in $\frac{dv}{dt}$ means . you can also try choosing a larger part of the circle , and you ranswer will come out a lot worse . in the worst case , if you use the entire circle , you will find that $\delta v=0$ , and therefore $a=0$ , and conclude that the circle has an infinite circumference .
we can make it absorb a lot of energy but if you read about black-body radiation effect you will notice that as energy is introduced into the object it will similarly radiate a small amount of the energy back , at room temperature appears black , as most of the energy it radiates is infra-red and cannot be perceived by the human eye . at higher temperatures , black bodies glow with increasing intensity and colors that range from dull red to blindingly brilliant blue-white as the temperature increases . this means that later even if we did have an $100$ percent absorbing materials of all electro-magnetic radiation the object will radiate some energy due to heating or other similar process . that in mind , it will always leak out some energy . to conclude , we could say it will only absorb radiation for short-time before releasing enough photons which could be detected by an photon detector , therefore " breaking " the truly blackest or absorbing material due to this effect . as a summary as long as the object absorbs some form of energy it can never be completely black . to learn more pertaining black-body radiation , read this : http://en.wikipedia.org/wiki/black-body_radiation furthermore , an blackhole is very black however it even is not as the power in the hawking radiation ( if it was proven , currently it is hypothesized ) from a solar mass $m$ which is equal to $1.98855\pm 0.00025 * 10^{30}$ black hole turns out to be a minuscule 9 × 10−29 watts . it is indeed an extremely good approximation to call such an object ' black ' . as $$ p =\hbar c^2/15360\pi g^2m^2 = 9.004 * 10^{-29} $$ that being said , even black-holes one of the strongest objects cannot escape being truly black .
i can think of two simple ways . lever the first would be to eschew the scale altogether and build a rudimentary scale with a stick and a pivot , and balance out your nephew 's weight with a bunch of water in some buckets , or 2l bottles . knowing the density of water , you could equate the weight of your nephew to a volume of water . if you are interested in correcting for the weight of the buckets , you could weigh those using your available scales . or you could use a collection of solid objects and weigh these in turn on your existing scales . in the same spirit , with access to stick and pivot technology , along with a measuring stick , you could just make the lever arm on the side opposite your nephew a factor of 6 times longer ( 7 to be safe ) , and then find some objects around the house to balance out this rudimentary scale . that collection of objects ( or water again ) could be measured on your existing scales . knowing this mass , and the ratio of the lever arms on the two sides of the scale , you will get a weight for your nephew . the major source of error in both cases is likely to be the uncertainty in the location of the center of mass on both sides . i reckon you could get this down to about an inch or so without too much trouble , which would equate to an error in the mass of around 3% if your lever arm on each side was around 3 feet . not perfect but decent . with less certainty in the position of the center of mass , or care taken , i would expect an error at the 10s of percent level . sock scale in the interest of science , i will also report a method that i do not recommend . i was trying to think of an elastic medium that you would have access to in your home , which you could use to weigh your nephew incrementally . if you could imagine calibrating a single spring to a small mass , then you could use many such springs to measure your nephew . what spring-like material does everyone have access to many copies of ? socks ! so , in the interest of science i took off my two socks and tried to see if socks have a hookian enough response to be useful . experimental setup : i used two socks , a binder clip to secure the weights to it , a pen , some string , and multiple 20 fl oz ( = 1.3 lbs of water ) bottles , and tried to see if the response was hookian over several bottles . i got 1 bottle -- 1.5 distance units 2 bottles -- 2.8 distance units 3 bottles -- 3.9 distance units  i am being vague about the distance units because i did not actually have a ruler handy , so instead used the l scale on my sliderule to measure the extensions rendered as a plot we see : while this looks decent , i am troubled by the fact that it does not line up well with the zero point extension of the sock , also we were only able to take 3 measurements since with the addition of the 4th bottle , the binder clip gave way . regardless of the questions of hookian reliability of the socks , to weigh your nephew at around 30 lbs , you would need something like 25 nearly identical socks , or calibrate all of them individually , and figure out a way to reliably afix your nephew to the socks . due to the impracticality of the method , i can not recommend this approach , though in the interest of science , and so that others need not follow in my footsteps , i have shared this failure here .
electrically charged particles interact via their fields and so there is , in general , wide range interaction throughout the gas . the electromagnetic interactions between particles of the gas/plasma can give raise to effects which are significantly different from neutral gas , such as e . g waves . so to what extend the ideas gas law can be considered to " hold " for a plasma will depend on the parameters of the system , temperature , pressure , etc . , but foremostly of the ionization degree of the gas/plasma . it is an involved issue , as this quantity will depend on all the other parameters . one commonly cited relation for certain parameter ranges is the saha equation , which relates temperature and particle density - which are both part of $pv=k_b t\cdot n$ too . microscopic considerations in such a " chemical system " , where the constituents can be ionized and thereby change their properties , lead you to the observation that the value charge density depends on the surroundings . so e.g. the poission equation takes a nonlinear form $\delta\phi=\rho ( \phi ) $ . it is then also related to new'ish system parameters like the debye length , which caracterize the overall bahaviour you ask for . i am sure there are debye length-temperature ranges where it is perfectly reasonable to apply a gas law , just watch out which part of the system makes up charged particles or neutral ones . e.g. i think in space , there are a whole lot of charged particles , but people work with ideal gas laws . a general rigourous classical look at it will lead you to $pv=k_b t\cdot \text{ln} ( \mathcal z ) $ , where the partition function contains the hamiltonian of the system , which include the potentials $\phi$ = energy expressions involving multiple variable-integrals over statistically weighted interaction potentials , see this link .
because it was defined by measurements ( the force between to wire segments ) that could be easily made in the laboratory at the time . the phrase is " operational definition " , and it is the cause of many ( most ? all ? ) of the seemingly weird decision about fundamental units . it is why we define the second and the speed of light but derive the meter these days .
helioseismology is what you need to learn about . yes , there are sound waves in sun
for record : it looks like this topic really interests some people : http://markmail.org/message/nic7xrgf5uzed5c4 newport was obviously thinking in the same direction : http://www.newport.com/content/default.aspx?id=880 . however i think it is not exactly what you need . having background in both optical engineering and experimental optics , i can say that real experiments and setups are usually designed with a piece of paper if they are simple or with the professional software if they are not . real systems very quickly stop being a bunch of mirrors . this is , probably , why nobody is seriously considering creation of such a tool . in labs , we are usually trying to align all beams parallel or under 90 degrees to each other---not only for ease of work but also because otherwise polarisation effects start being a problem .
the general answer is " it depends . " light has energy , momentum , and puts a pressure in the direction of motion , and these are all equal in magnitude ( in units of c = 1 ) . all of these things contribute to the stress-energy tensor , so by the einstein field equation , it is unambiguous to say that light produces gravitational effects . however , the relationship between energy , momentum , and pressure in the direction of propagation leads to some effects which might not otherwise be expected . the most famous is that the deflection of light by matter happens at exactly twice the amount predicted by a massive particle , at least in the sense that in linearized gtr , ignoring the pressure term halves the effect ( one can also compare it a naive model of a massive particle at the speed of light in newtonian gravity , and again the gtr result is exactly twice that ) . similarly , antiparallel ( opposite direction ) light beams attract each other by four times the naive ( pressureless or newtonian ) expectation , while parallel ( same direction ) light beams do not attract each other at all . a good paper to start with is : tolman r.c. , ehrenfest p . , and podolsky b . , phys . rev . 37 ( 1931 ) 602 . something one might worry about is whether the result is true to higher orders as well , but the light beams would have to be extremely intense for them to matter . the first order ( linearized ) effect between light beams is already extremely small .
the effect in which two objects get charged by rubbing and remain charged is called the triboelectric effect , http://en.wikipedia.org/wiki/triboelectric_effect where the root " tribo " means friction in greek ( the greek word $\tau\rho\iota\beta\omega$ means ' to rub' ) . friction is actually unnecessary : contact is enough in principle . this effect should not be confused with the ( volta or galvani ) " contact potential " between metals which only exists as long as the two metals remain in contact , and especially not with " contact electrification " which was a name of a scientifically incorrect theory of electricity at the end of the 18th century that attempted to overgeneralize the interpretation of the triboelectric effect . " electrophorus " was a gadget , first produced by volta , that used the triboelectric effect . the cause of the triboelectric effect is adhesion - the atoms on the surface literally form chemical bonds . materials such as fur are ready to lose electrons and become positively charged while the materials such as ebonite or glass gain electrons and become negatively neutral . to get some idea about which atoms are likely to lose or gain electrons , it is useful to know their electronegativity : http://en.wikipedia.org/wiki/electronegativity#electronegativities_of_the_elements the redder atom , the higher electronegativity , and the more likely it is for the atom to gain electrons and become negatively charged . that is especially true for light halogens ( fluorine , chlorine ) and oxygen . that is partly why glass - with lots of $sio_2$ - likes to get negatively charged in the triboelectric effect . even sulfur ( 40% of ebonite ) has a higher electronegativity than e.g. carbon and hydrogen that are abundant in the fur which is why fur loses electrons and becomes positively charged . of course , the actual arrangement of the atoms in the molecules matters , too . so this overview of the periodic table was just an analogy , not a reliable way to find out the results of the triboelectric effect .
electrostatic induction is good to use . it is the phenomenon of inducing electric charges without any direct contact with a charge . this principle is used in capacitors . even , rubbing materials produce static electricity . insulators could be charged by rubbing . but , metals are probably charged using insulators . . ! when you bring a charged plastic or glass rod ( probably negatively charged ) near a metal piece positive charges which experience attractive coulombic force move towards the end of metal nearer to the rod , while negative charges move to the other end due to repulsive force . if you ground the metal piece , negative charges flow to ground while positive charges stick to the end ( due to attractive force ) . . . however after removing the ground , the positive charges are distributed throughout the metal piece . edit : after charging insulators , charges could be transferred from insulator to your metal piece by simply touching it . . ! ( thanks to @john ) also , van de graaff generator is based on both electrostatic induction and corona discharge ( action of points ) to produce high voltage of the order of $10^7v$ . but , it is been in use to accelerate ions for nuclear disintegration purposes instead of charging metal pieces . . !
since the resistors are in parallel , we have $$i_1 r_1 = i_2 r_2$$ thus $$i_2 = i_1 \frac{r_1}{r_2}$$
the fastest fully localized process is the evaporation of the smallest black hole worth the time - it takes one planck time or so , $10^{-43}$ seconds . there are other characteristic processes in quantum gravity that take a planck time - the shortest time scale for which the usual spacetime geometry works . however , if you allow changes in collective properties of large objects , there are much shorter times . for example , the mass of the visible universe is $3\times 10^{52}$ kilograms or so . the corresponding energy , via $e=mc^2$ , is $10^{61}$ joules . the periodicity of the corresponding quantum wave , via $e=\hbar\omega$ , is about $10^{-95}$ seconds . so the wave function of the universe periodically changes its phase $10^{95}$ times every second . i guess that you mean local processes , and moreover some processes accessible experimentally . that is a question with no permanent answer . the whole field of particle physics may be classified according to the time scale we can resolve . the time scale you mention is that of atomic physics ; the time scales in nuclear physics are up to 10 orders of magnitude shorter , $10^{-24}$ seconds or so . the lhc is probing time scales that are 4 orders of magnitude shorter than that . in principle , this progress could continue . the " higher energy " we have , the shorter times ( and distances ) we may resolve .
first , note that this comes from the heisenberg uncertainty principle , $$ \delta x\delta p\geq \frac\hbar2 $$ where $\hbar\approx10^{-34}$ j$\cdot$s ( i.e. . , a very small number ) . this is a constraint on the simultaneous measurements of momentum and position . if you know the position of the coin , then it can not actually be anywhere else because it is measured to be there . next , i quote sean carroll : quantum mechanics features a " classical limit " in which objects behave just as they would had newton been right all along , and that limit includes all of our everyday experiences . for objects such as cats that are macroscopic in size , we never find them in superpositions of the form "75 percent here , 25 percent there" ; it is always "99.9999999 percent ( or much more ) here , 0.0000001 percent ( or much less ) there . " classical mechanics is an approximation to how the macroscopic world operates , but a very good one . the real world runs by the rules of quantum mechanics , but classical mechanics is more than good enough to get us through everyday life . it is only when we start to consider atoms and elementary particles that the full consequences of quantum mechanics simply can not be avoided . for your coin , we can describe it correctly by classical mechanics ( meaning we can measure its position and momentum simultaneously ) , so there is no need to invoke quantum mechanics in regards to this thought experiment .
this is a deep mystery , nobody knows for sure . it is possibly related to establishing an elastic wave profile along the bottom which is in a dynamic steady-state with the jostling of the contact points , but you need good measurements to make a theory , and good measurements are difficult because of the issue of solid-solid sticking . the research field which studies questions of this sort is moderately active still , and was very active twenty years ago . it is called depinning . in depinning , there are models which partially explain this effect , although whether these models are relevant for a real solid-on-solid interface remains to be seen . friction phenomenology is counter-intuitve sliding friction is approximately constant . this is unusual , because physical quantities tend to go to zero smoothly . for example , the force on a spring is zero at position zero , and it is linearly increasing as a function of displacement . similarly , the friction force for a slow moving object in a viscous fluid is zero at zero velocity , and is linear in the velocity . this is the general expectation that the friction depend analytically , or at least differentiably , in a small parameter . solid-on-solid friction violates this expectation . another way of saying why a constant friction force is counterintuitive is the belief that there should be a unique steady state motion at each force . if the microscopic contacts are doing some sort of markov chain , by general principles of probability , it should have a unique steady state distribution at any fixed force . two different steady states must be separated enough so that they cannot fluctuate into each other . if this is true , the force must determine the velocity . but this is not true if the force is constant as a function of the velocity--- you can keep an object sliding stably at any velocity with the same force . so the general expectation is that the friction force is not really completely constant , but slowly varying with the velocity , and that there is a point of statistical non-analyticity , an interesting phase transition , at zero velocity . why constant friction force ? the friction force times the velocity tells you the energy lost to heat per unit time , the work done by friction . this work is the constant friction force times the velocity . this means that friction will remove an equal increment of energy in equal distance . you can understand this rule heuristically as follows : the loss in energy is due to an elastic deformation in the solids relaxing . in order to slide to each position , the solid will rearrange its shape slightly to fit into the microscopic best-stick points on the matching surface , and stick to some bump or dirt on the surface . each rearrangement of the solid produces sound waves which carry away energy , and transform it into heat . each sticking and unsticking event produces a given amount of sound , and the sticking and unsticking is a property of the surface only . so that to slide a given distance , you need an average number of sticking/unsticking events , and the friction energy loss is equal to the number of such events times the average energy lost to sound per event . this model explains why there is a roughly the same amount of work done per unit distance moved . this model of sticking and unsticking in response to forcing is called " depinning " in the condensed matter literature , and it was widely studied in the 1990s . narayan fisher model solid on solid friction is only the most familiar of a host of phenomena which involve sticking and slipping when something is pushed forward . the traditional examples listed in papers on depinning are * charge density waves * pulling pinned vortices in superconductors * crack propagation , with starting and stopping the list of applications for some reason usually excludes solid-on-soild friction , even though this is the most familiar application . here is a classic depinning model : consider a position function defined on a two dimensional grid . you can think of the position function as representing the displacement of the surface of contact of the two solids . there is a constant force on every site , and there is an additional elastic force on each site , which is positive when the neighbors are further along and pull the site forward , and negative when the neighbors have not caught up yet , and hold it back . the total force is the constant $f$ plus $h ( u ) +h ( d ) +h ( l ) +h ( r ) - 4 h ( c ) $ where u , d , l , r are the four neighbors , h is the position function , and c is the center point . then there is a sticking force at each point $s ( x ) $ , which is a random number between 0 and 1 ( the details of the distribution are not so important , it could be a gaussian of unit width too , just do not make the sticking distribution a powerlaw ) . the law of motion is that wherever the total force is bigger than the sticking force , the site moves forward by one unit . this model has a second order phase transition at a critical force from a stationary state to a moving state . past the phase transition , the velocity goes as $ ( f-f_c ) ^\alpha$ , where f_c is the critical force and $\alpha$ is a critical exponent . because the transition is second order , this model does not explain static friction . nor does it produce a constant friction with velocity , the force goes up with velocity . but it does produce a critical force which is nonzero , and determined by the sticking force distribution in steady state . fisher 's overshoots the model above fails to account for static friction , and its analog in other depinning models . to fix this , fisher introduced the concept of a stress overshoot . the idea here is that right after a site depins and hops , it has made sound-waves all around it which jostle the site for a while , and these sound waves momentarity increase the force randomly in various directions . this means that the mean elastic force does not have to overcome the pinning force by itself , it can get some momentary help from the transient soundwave profile . the model for this is that right after a site hops , on the next time step , there is a parameter m called the overshoot parameter , which weakens the on-site pinning force , and that of the nearest neighbors . the total force only has to exceed the pinning force minus m in order to get the site to hop again . this model takes into account the local stress profile , and fisher 's physical intuition led him to propose that the transition in this model will be first order , not second order , because there should be hysteresis . the reason is simple : while the object is moving , it feels the overshoots from the transient stresses , and when it stops it does not , because the sound modes die away . so it should take more force to get something to move than to keep in moving once it starts . this means that you have to increase the force to more than the amount required to maintain the motion in order to start the motion , and this is a hysteresis loop . the hysteresis loop is usually a property of first order transitions . phony hysteresis fisher 's physical intuition is correct in natural models , but the original conclusion that the transition is first order is not correct . to explain this , the statistical theory of the steady state motion is a renormalization group fixed point , and the m perturbation is much like a mass term--- it adds a little bit of inertia to a site . a site which hopped tends to hop again . mass terms in viscous systems are generically irrelevant perturbations . they do not affect the statistical description at long wavelengths in any way . but these perturbations do produce hysteresis in some models , so there seems to be a contradiction with the general principles of renormalization group theory . the resolution of this mini-paradox is a little subtle . it is the subject of this paper http://arxiv.org/abs/cond-mat/0301495 ( nonfree version is slightly different in terminology , but not in content : http://prl.aps.org/abstract/prl/v92/i25/e255502 , i coauthored this with jennifer schwarz ) . i will run through the argument in the paper , which shows that the transition is hysteretic--- you need more force to get the thing started than to keep it moving . the transition is still second order , and in the same universality class as the fisher narayan model . these two conclusions are not contradicting . the demonstration is easiest by considering a series of models , each of which is progressively less obvious , but each of which have the same behavior . model 1: global overshoot model this model is not physical at all , it is just used for the sake of mathematical argument . consider the discrete narayan fisher hopping model on a lattice ( the stick-force lattice model described above without overshoots ) , and add to it an overshoot force which is global and nonadditive--- whenever at least one site somewhere has hopped , all sites feel an overshoot force m . when the system is moving , this means that the pinning force distribution is just shifted over by a constant m , and the steady state is exactly the same as the narayan fisher model with the shifted distribution . when the system is stopped , you need to make the force m units bigger than normal to get one site to hop , but the moment it does , the force on every site goes down by m , so you can quickly drop the external force by $m-\epsilon$ and keep it moving . so there is exactly m units of hysteresis in this model , but because of the stupid global nature of the force , it is manifestly obvious that this model is completely equivalent to the narayan fisher model , and it has the same second order phase transition . i lobbied to call this behavior " phony hysteresis " , because it is so fake . the hysteresis loop is not an indication of a first order phase transition , it is just a stupid thing overlayed on top of a fundamentally second order transition . prl 's referees did not like phony hysteresis , but this is i what i will call it below . model 2: local nonadditive overshoot model the local nonadditive overshoot model says that when a site hops , it feels m units of overshoot force , and its neighbors do too , on the next time step . this force is nonadditive , meaning that it is the same m if one neighbor hops or if all four neighbors hop . this model is superficially different from the global overshoot model , but they are actually the same in steady state . when you have a steady state motion , the only sites that hop on time t are those whose local environment has changed , and these are the sites whose neighbors have hopped . these sites feel the overshoot force . so when the steady state is set up , the hopping sites all feel the overshoot . further , the sites which do not hop felt the overshoot at the last time they or something around them moved , so that their stability at that time means that they do not move even if you put the overshoot on them too . so you might as well apply a nonadditive m overshoot to all the sites . it makes no difference in steady state . this proves the equivalence of the nonadditive model to the global model . you conclude that the nonadditive model has exactly m units of phony hysteresis , just like the global model . but the nonadditive model is local , so phony hysteresis is actually happens in realistic things . the transition is still second order , and completely rigorously equivalent to narayan fisher model 3: additive stress overshoot now you get to fisher 's original overshoot model , where the overshoot force is additive , and equal to m times the number of neighbors that hopped on the last timestep ( where neighbor includes yourself if you hopped then too--- this is crucial ) . in this case , you can separate out the model into two parts : m units of nonadditive stress overshoot , which is felt by every site whose neighborhood changed the last timestep . an additional stress overshoot equal to m times the number of hopping neighbors minus 1 . you analyze the two contributions separately . the second overshoot can be simulated directly , and it has a second order phase transition in the same universality class as the narayan fisher model . so this behaves as expected from renormalization theory , it has no effect on the continuum limit . the first contribution just tacks on exactly m units of phony hysteresis on top of the no-hysteresis second model . so the additive local overshoot model , the realistic model , has m units of phony hysteresis disguising a completely fine second order transition point . this is a realistic model with static friction greater than dynamic friction . it is not a correct model , because forces in a solid are nonlocal . complications fishers original model had m units of overshoot stress on the neighbors , but no overshoot stress on the site that just hopped . this is because you do not think that a site which just hopped would need to hop again . while this is true , the double-hops are rare , the lack of the self-overshoot ruins the proof of the phony hysteresis theorem , so you do not have phony hysteresis , and the system eventually settles down to a nonhysteretic steady state . but it takes forever because the system is so close to the system with true phony hysteresis . this led to much confusion . is static friction phony hysteresis ? the phony hysteresis means that the transition going down in force is second order , but you see a hysteresis going up , just because you did not set up the overshoots . the overshoots are models of sound waves in the bottom of the material , which locally help push the material , to overcome the sticking points . these soundwaves always fall off in amplitude slower than elastic deformations . so the overshoots are always important in a larger range than the elasticity . this is exactly the domain where you get phony hysteresis , of magnitude equal to the minimum of the overshoot stress on the moving filament of active sites . the filament is narrow , and the soundwaves are broad , so this minimum should be nonzero . because of this , i do believe that the static friction is the phony hysteresis . this means that the transition to zero velocity when going down in force is the true statistical stick-slip dynamics of depinning , while you need an extra force to get the thing moving because the soundwaves have not been set up yet . this is not a complete answer , because the actual magnitude of the hysteresis loop has not been estimated , and there is no real demonstration that the minimum overshoot on the active filament is nonzero . further , this predicts that the force for sliding friction should go up with a nonzero critical exponent as a function of the velocity , although when the exponent small as $1.6$ you can model the force as effectively constant , especially since the range of variations is defined by the enormous scale of the speed of sound in the material .
you already received several answers . however the fundamental physical reason is elementary : in classical , quantum and relativistic physics the physical laws describing an isolated physical system in an inertial reference frame are the same ( are invariant ) if you rotate ( with an element of $so ( 3 ) $ ) the system ( there are many other symmetries depending on the theory , but the point is that rotations are symmetries ) . this is a fundamental postulate of all physics , valid at small ( noncosmological ) scales at least . so , for instance a curve describing the evolution of the system in the space of states remains a curve describing ( another ) evolution of the system if we apply the same rotation to the former curve at every time . there are many other examples . you understand that , unless the space of states be the physical space isomorphic to $\mathbb r^3$ , the action of $so ( 3 ) $ on the states cannot be implemented directly using the matrices $r\in so ( 3 ) $ . instead you should faithfully " represent " the elements group $so ( 3 ) $ in terms of natural transformations of the space of states . in quantum mechanics , for many theoretical reasons ( already mentioned wigner and kadison theorems ) these natural transformations are given by unitary ( and anti unitary ) operators , as theoretical consequence of the fact that the most elementary expected preservation property of these symmetries is that of probabilty transitions of pairs of pure states . ( actually one should use projective unitary representations , but i do not think it is the case to enter into the details here . ) another intersting fact of $so ( 3 ) $ ( actually $su ( 2 ) $ ) representations is that , due to the compactness of the group , all possible unitary representations are constructed out of the irreducible ones via the standard procedure of direct sum ( this is not a trivial fact , because usually one should deal with the much more complicated tool of the direct integral ) .
if you think of the two parallel glass sides as canceling each other out you are pretty close to it . the first impact ( low to high indices ) does in fact disperse frequencies if the light is coming in at an angle , but the exits ( high to low ) mostly cancels that effect . there actually can be some small residual effects leading to small colored fringes . the prism works better because the oppositely angled sides enhance rather than cancel the dispersion effects . here are the results of some further analysis and experimentation . part of the answer for why color effects are so hard to find when light passes through flat glass plates appears to be in the eye of the beholder . . . literally ! here 's the scoop : angled light entering a flat plate should at first fan out its angles by color while within the plate . by symmetry , however , those slightly fanned out rays of colored light return to their original paths when they reach the second surface an re-emerge . so , the new rays will show essentially no difference in direction from their paths in the original beam , but will no be spaced very slightly apart from each other in rainbow order . for a typical plate of glass this separation would seldom be more than a millimeter , and for most glasses would be a lot less than that . now picture a point of light on one side of the glass and a human eye on the other side . arrange both so that the line between then is at a sharp angle to the surface of the glass . let 's look at the ray going from the point to the center of you pupil . your eye focuses that parallel white light as a single point on the retina , as expected . but when the angled glass plate is inserted , the same ray of light gets spread out along a tiny distance , usually much less than a millimeter . however , each colored ray in this bundle remains parallel to the original path . this little sub-millimeter bundle then enters the pupil of the eye , carrying pretty much the same light as before , all traveling in parallel . what does your eye do with it ? it forms the same white colored point image as before , since the light is all traveling in parallel . think for example of red and blue light entering opposite sides of a magnifying glass : both will end up near the center . there will be some chromatic aberration , sure , but it turns out that vertebrate eyes are very , very good at eliminating that form of chromatic aberration at the image level . the bottom line becomes this : as long as the plate is not too thick , the physical separation of chromatic components will fall within the size of the human eye pupil , and the image will appear to be white - color free and pretty much just like the original , with just a bit more blurring . that also leads to an experimental prediction that i have not yet tried : if you hold a pinhole in front of your eye while observing a pinhole light on the other side of an angled piece of glass , you may be able to see a short colored line instead of a white dot . i can not guarantee it , but it is likely enough that it would be interesting to try . now to the final part of the analysis : what if the glass is so hugely thick that there is no way the separated components can be captured by the human eye all at once ? should not that lead to some visible color effects , such as blue and red fringes on either side of a point of white light ? specifically , for a white dot or light , a blue fringe should appear towards the side angled away from the viewer , and a red fringe on the side of the glass that is closer to the viewer . for a black line on a light background this would be reversed , with the red on the glass-is-farther edge of the black line ( since that is the nearer edge of the lighter part ) , and blue on the glass-is-nearer edge of the black line . ( you can work out why that is with simple dispersion diagrams . ) but since the space-form chromatic separation effect is going to be small even for a quite thick piece of glass , where can you find something thick enough to show such fringes ? fish lovers have conveniently provided a solution : they are called aquariums ! the combination of glass and water makes a quite good approximation of a very thick piece of glass with decent chromatic dispersion . but does it really work ? if like me you do not currently have an aquarium , here is a convenient online image of a see-through aquarium that is angled sharply away on the right . on the other side of the tank are both vertical bright lights from curtain folds ( near the right side ) , and vertical dark lines from a picture frame ( on the left ) . if you magnify the image , you will see blue fringes on the right sides of the curtain folds . neither effect is intense , but both are definitely in this image . if you do happen to have an aquarium , you should of course try it for yourself , since good direct experiment always trumps theory if they disagree ! do not look directly into a light , since modern led lights are very bright and should never be gazed at directly . instead , place a thin vertical stripe of white paper on a black background and illuminate that with a bright light pointing away from the observer . you can also try holding a small pinhole in aluminum foil in front of you eye to enhance any color fringing effect you may see . and with that . . . i think i will give this one a rest . further discussion , especially actual results from experimenters with real aquariums , would be great though !
i can not improve on kdn 's answer , but given todd 's comments this is an attempt to rephrase kdn 's answer in layman 's terms . a system is only in an eigenstate of spin around an axis if a rotation about the axis does not change the system . take $z$ to be the direction of travel , then for a spin 1 system the $s_z$ = 0 state would be symmetric to a rotation about an axis normal to the direction of travel . but this can only be the case if the momentum is zero i.e. in the rest frame . if the system has a non-zero momentum any rotation will change the direction of the momentum so it will not leave the system unchanged . for a massive particle we can always find a rest frame , but for a massless particle there is no rest frame and therefore it is impossible to find a spin eigenfunction about any axis other than along the direction of travel . this applies to all massless particles e.g. gravitons also have only two spin states .
" restoring " forces refer primarily to forces that try to return a system to equilibrium . so a spring has a restoring force of $f = -k\delta x$ . this means that if you choose the origin as being $x = 0$ , then compressing the spring would correspond to a negative $x$ ( displacing the spring to the left ) , and stretching the spring would correspond to a positive $x$ ( stretching the spring to the right ) . in that sense , by extending the spring , a positive $\delta x$ creates a negative force ( $-1 \times \delta x$ ) that acts to restore the spring to equilibrium ( pulling back on the spring extension ) and by compressing the spring , you would have a negative $\delta x$ ( $-1 \times \delta x$ ) , which creates a positive force that restores the spring equilibrium . so hooke 's law is actually $f=-k \delta x$ hope that helps .
the notion of a particle in nonrelativistic quantum mechanics is very general : anything that can have a wavefunction , a probability amplitude for being at different locations , is a particle . in a metal , electrons and their associated elastic lattice deformation clouds travel as a particle . these effective electron-like negative carriers are electron quasiparticles , and these quasiparticles have a negative charge , which can be seen by measuring the hall conductivity . their velocity gives rise to a potential difference transverse to a wire in an external magnetic field which reveals the sign of the carriers . but in a semiconductor , the objects which carry the charge can be positively charged , which is physically accurate--- a current in such a material will give an opposite sign hall effect voltage . to understand this , you must understand that the electron eigenstates in a periodic lattice potential are defined by bands , and these bands have gaps . when you have an insulating material , the band is fully filled , so that there is an energy gap for getting electrons to move . the energy gap generically means that an electron with wavenumber k will have energy : $$ e= a + b k^2 $$ where a is the band gap , and b is the ( reciprocal of twice the ) effective mass . this form is generic , because electrons just above the gap have a minimum energy , and the energy goes up quadratically from a minimum . this quadratic energy dependence is the same as for a free nonrelativistic particle , and so the motion of the quasiparticles is described by the same schrodinger equation as a free nonrelativistic particle , even though they are complicated tunneling excitations of electrons bound to many atoms . now if you dope the material , you add a few extra electrons , which fill up these states . these electrons fill up k up to a certain amount , just like a free electron fermi-gas and electrons with the maximum energy can be easily made to carry charge , just by jumping to a slightly higher k , and this is again just like a normal electron fermi gas , except with a different mass , the effective mass . this is a semiconductor with a negative current carrier . but the energy of the electrons in the previous band has a maximum , so that their energy is generically $$ e = -bk^2$$ since the zero of energy is defined by the location of the band , and as you vary k , the energy goes down . these electrons have a negative nonrelativistic effective mass , and their motion is crazy--- if you apply a force to these electrons , they move in the opposite direction ! but this is silly--- these electron states are fully occupied , so the electrons do not move at all in response to an external force , because all the states are filled , they have nowhere to move to . so in order to get these electrons to move , you need to remove some of them , to allow electrons to fill these gaps . when you do , you produce a sea of holes up to some wavenumber k . the important point is that these holes , unlike the electrons , have a positive mass , and obey the usual schroedinger equation for fermions . so you get effective positively charged positive effective mass carrier . these are the holes . the whole situation is caused by the generic shape of the energy as a function of k in the viscinity of a maximum/minimum , as produced by a band-gap . bohr model holes you can see a kind of electron hole already in the bohr model when you consider moseley 's law , but these holes are not the physical holes of a semiconductor . if you knock out an electron from a k-shell of an atom , the object you have has a missing electron in the 1s state . this missing electron continues to orbit the nucleus , and it is pretty stable , in that the decay takes several orbits to happen . the many-electron system with one missing electron can be thought of as a single-particle hole orbiting the nucleus . this single particle hole has a positive charge , so it is repelled by the nucleus , but it has a negative mass , because we are not near a band-gap , it is energy as a function of k is the negative of a free electron 's energy . this negative-mass hole can be thought of as orbiting the nucleus , held in place by its repulsion to the nucleus ( remember that the negative mass means that the force is in the opposite direction as the acceleration ) . this crazy system decays as the hole moves down in energy by moving out from the nucleus to higher bohr orbits . this type of hole-description does not appear in the literature for moseley 's law , but it is a very simple approximation which is useful , because it gives a single particle model for the effect . the approximation is obviously wrong for small atoms , but it should be exact in the limit of large atoms . there are unexplained regularities in moseley 's law that might be explained by the single-hole picture , although again , this " hole " is a negative mass hole , unlike the holes in a positive doped semiconductor .
if you want to learn topology wholesale , i would recommend munkres ' book , " topology " , which goes quite far in terms of introductory material . however , in terms of what might be useful for physics i would recommend either : nakahara 's " geometry , topology and physics " naber 's " topology , geometry and gauge fields : foundations " personally , i have not read much of nakahara , but i have heard good things about it , although it may presuppose too many concepts . i have read selections of naber and it seems fairly well written and understandable and starts from first principles , but again , it may not focus as much on the fundamentals , if that is what you are looking for .
the problem you are having is that there are two different uses of the word " phase " . one is , as you point out , the argument of the $sin$ function . the other use of the word is the ordered pair $ ( x , p ) $ , that is , coordinate and momentum . $ ( x , p ) $ specifies a point in a two-dimensional " phase space " where one axis is $x$ and the other $p$ . the state of a one-dimensional mechanical system is completely specified by its point in phase space . the location of the point in phase space changes with time as the system evolves . one can also specify the state differently , using the coordinate and it time derivative : $ ( x , \dot{x} ) $ . this two-dimensional space is called " state space " . so a phase plot is a plot of $p ( t ) $ vs . $x ( t ) $ . that is , the points $ ( x , p ) $ for every instant of time .
i guess so - i mean , as far as i know , there is no law of physics that strictly prohibits those " exotic " states from being realized . as long as the state exists and can be reached by some path from the " center " of the state space where the likely states are , there should be a nonzero ( not even infinitesimal , really ) probability of accessing it . but for a typical system , that probability is really , really , really small . so small that it is impossible to intuitively comprehend just how unlikely such an event is . the thing is , a lot of people are not used to dealing with even moderately large or small numbers . if you confront them with a probability like $10^{-10^{23}}$ , they often fail to put the smallness of that value in perspective , and instead focus on the fact that it is not strictly equal to zero . from there they may start coming up with all sorts of nonsensical ideas about walking through walls and spontaneous combustion ( the weird kind ) and the like . so physicists usually find it easier to just say the probability is zero - and in fact , for any purpose other than a rigorous mathematical proof , it might as well be . ( sorry about the rant , i know most people are actually relatively sensible about these things , but it bothers me that the crazy ones seem to get all the attention despite being wrong . )
for this process , the interaction hamiltonian is given by : $$\mathcal{h}_{\rm int}=-\frac{g}{\sqrt 2}\left ( v_{cb}\bar{b}_l\gamma^\mu c_l w^-_\mu+\bar{\nu}_l\gamma^\mu\ell_l w^+_\mu\right ) . $$ after integrating-out the heavy bosons , we obtain the following hamiltonian $$\mathcal{h}_{\rm eff}=-\dfrac{g_f}{\sqrt{2}}v_{cb} [ \bar{b}\gamma^\mu ( 1-\gamma_5 ) c ] [ \bar{\nu}\gamma^\mu ( 1-\gamma_5 ) \ell ] , $$ where $g_f/\sqrt{2}=g^2/ ( 8 m_w^2 ) $ is the fermi constant . to obtain the tree-level amplitude for the process $b_c\to j/\psi \ell^+ \nu$ , we consider the following matrix element $$\mathcal{a} ( b_c\to j/\psi \ell^+ \nu ) =-i\langle j/\psi \ , \ell^+\ , \nu_\ell |\mathcal{h}_{\rm eff} | b_c\rangle . $$ if you write explicitly the leptonic fields in terms of creation and annihilation operators , then you will notice that $$\mathcal{a} ( b_c\to j/\psi \ell^+ \nu ) =i \dfrac{g_f}{\sqrt{2}}v_{cb}\bar{u}_\nu \gamma^\mu ( 1-\gamma_5 ) v_\ell \langle j/\psi| \bar{b}\gamma^\mu ( 1-\gamma_5 ) c| b_c\rangle . $$ note that we have isolated the hadronix matrix element from the rest . now , if we are able to find this element by using lattice qcd methods or experimental results , then we will be able to compute the decay rate and other observables . [ however , i do not think this is possible for this particular transition at present . ] for your second question , if you consider only valence quarks in the mesons , then you are using a tree-level approximation to describe hadronic states . this is a crude approximation , because qcd is non-perturbative at low energies . you can improve it by computing high-order qcd corrections , but you will never have a reliable result .
google led me to this page which goes through the math to answer your question about the theoretical size of a telescope needed to resolve the lunar modules . it should come as no surprise that no , it can not be done with a small telescope : according to the math found at that link , you would need a telescope 100m in diameter to just about be able to see the lm on the moon ( from earth ) . the largest telescope on earth is 10m in diameter . he also posits that building such a telescope would be more expensive than going there and taking a picture yourself . another thing to consider is the atmospheric effects . if you have ever seen an amateur video of jupiter made from pictures taken through the atmosphere , you have seen it bubble and deform as though you were observing it from underwater . the atmosphere is in fact a fluid and this would make observing details as small as footprints on the moon next to impossible . using an orbiting telescope would avoid this , but the situation 's not much better : it would have to be of similar size to the ground telescope ( a ~300km difference in altitude would not do much for the resolution ) , and you would actually have to get the thing into orbit . i think the lro pictures are the best we are going to get for a while . ( btw , moon hoax believers love to say that the lro pictures are also fake , since they , too , came from nasa ( they did not really ) . jaxa 's kaguya spacecraft , while not photographing the footprints etc . , did map the topography of the apollo landing sites and it matches perfectly with the pictures . if the landings were filmed in the desert , how did they get the scenery to match up exactly with what is on the moon at the " fake " landing sites , according to japan 's space agency ? )
i greatly sympathize with your question . it is indeed a very misleading analogy given in popular accounts . i assure you that curvature or in general , general relativity ( gr ) describe gravity , they do not assume it . as you appear to be uninitiated i shall try to give you some basic hints about how gravity is described by gr . in the absence of matter/energy the spacetime ( space and time according to the relativity theories are so intimately related with each other it makes more sense to combine them in a 4 dimensional object called space-time ) is flat like a table top . this resembles closely with ( not completely ) euclidean geometry of plane surfaces . we call this spacetime , minkowski space . in this space the shortest distance between any two points are straight lines . however as soon as there is some matter/energy the geometry of the surrounding spacetime is affected . it no longer remains minkowski space , it becomes a ( pseudo ) riemannian manifold . by this i mean the geometry is no longer like geometries of a plane surface but rather like geometries of a curved surface . in this curved spacetime the shortest distance between any two points are not straight lines in general , rather they are curved lines . it is not very hard to understand . our earth is a curved surface and the shortest distance between any two points are great circles rather than straight lines . similarly the shortest distance between any two points in the 4 dimensional spacetime are curved lines . an object like sun makes the geometry of spacetime curved in such a way that the shortest distance between any two points are curved . this is called a geodesic . a particle follows this curved geometry by moving along this geodesic . einstein 's equations are mathematical descriptions of the relation of the geometry to the matter/energy . this is how gravity is described in general relativity .
i found a compact version of the course notes , re-written in english by the same professor who gave the original lectures . you can find them at this link .
the electric and magnetic fields of a single photon in a box are in fact very important and interesting . if you fix the size of the box , then yes , you can define the peak magnetic or electric field value . it is a concept that comes up in cavity qed , and was important to serge haroche is nobel prize this year ( along with a number of other researchers ) . in that experiment , his group measured the electric field of single and a few photons trapped in a cavity . it is a very popular field right now . however , to have a well defined energy , you need to specify a volume . in a laser , you find an electric field for a flux of photons ( n photons per unit time ) , but if you confine the photon to a box you get an electric field per photon . i will show you the second calculations because it is more interesting . put a single photon in a box of volume $v$ . the energy of the photon is $\hbar \omega$ ( or $\frac{3}{2} \hbar \omega$ , if you count the zero-point energy , but for this rough calculation let 's ignore that ) . now , equate that to the classical energy of a magnetic and electric field in a box of volume $v$: $$\hbar \omega = \frac{\epsilon_0}{2} |\vec e|^2 v + \frac{1}{2\mu_0} |\vec b|^2 v = \frac{1}{2} \epsilon_0 e_\textrm{peak}^2 v$$ there is an extra factor of $1/2$ because , typically , we are considering a standing wave . also , i have set the magnetic and electric contributions to be equal , as should be true for light in vacuum . an interesting and related problem is the effect of a single photon on a single atom contained in the box , where the energy of the atom is $u = -\vec d \cdot \vec e$ . if this sounds interesting , look up strong coupling regime , vacuum rabi splitting , or cavity quantum electrodynamics . incidentally , the electric field fluctuations of photons ( or lack thereof ! ) in vacuum are responsible for the lamb shift , a small but measureable shift in energies of the hydrogen atom .
the likely answer is no , the whole signal is just an artifact of the difficult statistical manipulations . a weizmann institute physicist has noted that the peak has been shifted by one bin and the whole discovery could therefore be due to a pile-up effect or jet energy calibration ; a small shift of the jet energy removes the effect . see also http://motls.blogspot.com/2011/04/fermilab-cdf-new-force-press-conference.html for more details . additional doubts about the valid statistical procedures were mentioned on tommaso dorigo 's blog , too . an adjustment of jet energy by 3 percent is enough to make the signal totally insignificant . animation by tommaso tabarelli de fatis , a member of cms . the text above also reviews some theoretical literature and clarifies that if the signal happens to be real - and the d0 collaboration is going to release its own verdict about the phenomenon in a few weeks - the most likely explanations are a new , fifth force - one mediated by a z ' boson ( with the mass of 144 gev or so ) which is the particle that was decaying to two jets in those events . it must be leptophobic - ( almost ) no interactions with the leptons - and such z ' bosons , messengers of new $u ( 1 ) $ groups similar to the electroweak z ' boson but independent from them , are predicted by a very large fraction of grand unified theories and/or string theory models ; articles by alon faraggi and many others are listed above ; the simplest ( but not the only ) way to get a new leptophobic $u ( 1 ) $ is to obtain it as a piece of a color $su ( 4 ) $ broken to $su ( 3 ) \times u ( 1 ) $ a technipion , a particle analogous to a pion in technicolor theories that break the electroweak symmetry by similar composite particles , and not necessarily scalar ones ( so there is no true higgs particle in those theories ) ; a paper by kenneth lane et al . is linked above , too ; technicolor has been thought to be nearly dead for years - the optimistic proposal to interpret the bump as a technicolor effect does not solve all the detailed problems with technicolor theories a stop squark in a supersymmetric theory - but it must be an r-parity-violating version of supersymmetry ( e . g . because the new superpartners were not produced in pairs ) which is unattractive for many other reasons ( in simple terms , r-parity-violating theories are only consistent with proton stability given immense new assumptions , and they do not produce natural well-behaved dark-matter candidates ) ; an article is linked to by the weblog above , too
fundamentally , the spin originates from the fact that we want our quantum fields to transform well-behaved under lorentz transformations . mathematically , one can start to construct the representations of the lorentz group as follows : the generators $m^{\mu \nu}$ can be expressed in terms of the generators of rotations $j^{i}$ and those of boosts $k^{i}$ . they fulfill $$ [ j^{i} , j^{j} ] = i \epsilon_{ijk} j^k , \ , [ k^i , k^j ] = -i \epsilon_{ijk} j^k , \ , [ j^i , k^j ] = i \epsilon_{ijk} k^k . $$ from them one can construct the operators $m^i = \frac{1}{\sqrt{2}} ( j^i + i k^i ) $ and $n^i = \frac{1}{\sqrt{2}} ( j^i - i k^i ) $ . they fulfill $$ [ m^i , n^j ] = 0 , \ , [ m^i , m^j ] = i \epsilon_{ijk} m^k \ , [ n^i , n^j ] = i \epsilon_{ijk} n^k$$ these are just the relations for angular momentum that you should know from your qm introductory course . group theoretically this means that every representation of the lorentz group can be characterized by two integer of half-integer numbers $ ( m , n ) $ . if you construct the transformations explicitly you will find $ ( m = 0 , n = 0 ) $ is a scalar , i.e. does not change under lt . $ ( m = 1/2 , n = 0 ) $ is a left-handed weyl spinor $ ( m = 0 , n = 1/2 ) $ is a right-handed weyl spinor $ ( m = 1/2 , n = 1/2 ) $ is a vector a dirac spinor is a combination of a right and a left handed weyl spinor . actually , one can now use these objects and try to find lorentz invariant terms in order to construct a lagrangian . from that construction ( that is too lengthy for this post ) one finds that the dirac equation is the only sensible equation of motion for a dirac spinor simply from the lorentz group 's properties ! similarly one finds the klein-gordon equation for scalars and so forth . ( one can even construct higher spin objects than vectors , but those have no physical application except maybe in supergravity theories ) . so , as you can see now , spin is fundamentally a property of the lorentz group . it is only natural that we find particles with non-zero spin in our lorentz invariant world . sidenote : since we found the dirac and klein-gordon equations from lorentz invariance alone , and their low-energy limit is the schroedinger equation , we get a ' derivation ' of the schroedinger equation , too . most of the time the se is simply postulated and worked with : this is where it comes from !
a black hole will not form . the reason why is that the boosted particle is equivalent by a boost to a reference frame where there is no black hole , and the presence/abscence of a black hole is coordinate-independent . while the energy of , say , an object with earth 's density profile can be made arbitrarily large through a boost , the boosted earth will still have a distinguishable stress-energy tensor from a highly compact object that is not boosted . and , it will be distinguishable from the stress-energy tensor of a boosted black hole , which can be defined by noting that the kerr metric can be written in the form : $$g_{ab} = \eta_{ab} + c \ell_{a}\ell_{b}$$ where $\ell_{a}$ is a particular null vector relative to both $\eta_{ab}$ and $g_{ab}$ and $c$ is an exactly specified function . in this form , we can define boosts relative to the background minkowski metric , and find out what the spacetime of moving black holes is . which is a long way of saying that the math encodes the difference between an earth boosted to $ . 999999999c$ and something that is natively super-dense and is moving that fast . you really have to consider the whole stress-energy tensor , which does not just include energy density , but includes momentum and all of the internal pressures in the object .
spin is a technical term specifically referring to intrinsic angular momentum of particles . it means a very specific thing in quantum/particle physics . ( physicists often borrow loosely related everyday words and give them a very precise physical/mathematical definition . ) since truly fundamental particles ( e . g . electrons ) are point entities , i.e. have no true size in space , it does not make sense to consider them ' spinning ' in the common sense , yet they still possess their own angular momenta . note however , that like many quantum states ( fundamental variables of systems in quantum mechanics , ) spin is quantised ; i.e. it can only take one of a set of discrete values . specifically , the allowed values of the spin quantum number s are non-negative multiples of 1/2 . the actual spin momentum ( denoted S ) is a multiple of planck 's constant , and is given by $s = \sqrt{s ( s + 1 ) }$ . when it comes to composite particles ( e . g . nuclei , atoms ) , spin is actually fairly easy to deal with . like normal ( orbital ) angular momentum , it adds up linearly . hence a proton , made of three constituent quarks , has overall spin 1/2 . if you are curious as to how this ( initially rather strange ) concept of spin was discovered , i suggest reading about the stern-gerlach experiment of the 1920s . it was later put into the theoretical framework of quantum mechanics by schrodinger and pauli .
the fundamental effect of gravitational waves exploited by all detectors is the same : one tries to detect minute oscillatory changes in distances between different parts of the device . these changes are of the order of 1 part in 10 20 or smaller , so detecting them is a real challenge . different types of detectors do differ significantly when it comes to the techniques they employ to measure these minute changes . interferometers look out for relative changes in phase of two light beams travelling in perpendicular directions . weber bars exploit resonance to magnify vibrations caused by gravitational waves of a given frequency . pulsar timing arrays compare timing of pulsar signals coming from different directions looking out for small differences caused by tiny expansion and contraction of space through which the signals propagate . high frequency detectors examine microwaves circulating in a loop looking out for polarization changes caused by spacetime contraction and expansion in different directions . see this wikipedia article for details . depending on how good given detector 's isolation from unwanted influences is , it may require substantial effort to extract gravitational wave signature from data collected by these detectors . for example , ligo phase loops lose lock every time a freight train passes by . the einstain@home project allows volunteers to donate the computing power of their home computers to this effort .
yes , the expansion of space itself is allowed to exceed the speed-of-light limit because the speed-of-light limit only applies to regions where special relativity – a description of the spacetime as a flat geometry – applies . in the context of cosmology , especially a very fast expansion , special relativity does not apply because the curvature of the spacetime is large and essential . the expansion of space makes the relative speed between two places/galaxies scale like $v=hd$ where $h$ is the hubble constant and $d$ is the distance . when this $v$ exceeds $c$ , it means that the two places/galaxies are " behind the horizons of one another " so they can not observer each other anytime soon . but they are still allowed to exist . in quantum gravity i.e. string theory , there may exist limits on the acceleration of the expansion but the relevant maximum acceleration is extreme – planckian – and does not invalidate any process we know , not even those in cosmic inflation .
nowadays , rockets use a gimbaled thrust system . the rocket nozzles are gimbaled ( an appliance that allows an object such as a ship 's compass , to remain horizontal even as its support tips ) so they can vector the thrust to direct the rocket . in a gimbaled thrust system , the exhaust nozzle of the rocket can be swivelled from side to side . as the nozzle is moved , the direction of the thrust is changed relative to the center of gravity of the rocket . early rockets had vernier thrusters which uses small rocket engines on either sides , to control the altitude of a rocket . nowadays , they are common in most satellites . in this image , the middle rocket shows the normal flight configuration in which the direction of thrust is along the center line of the rocket and through the center of gravity of the rocket . on the left one , the nozzle has been deflected to the left and the thrust line is now inclined to the center line at a gimbal angle $a$ . as the thrust no longer passes through the center of gravity , a torque is generated about the center of gravity and the nose of the rocket turns to the left . if the nozzle is gimbaled back along the center line , the rocket will move to the left . on the right one , the nozzle has been deflected to the right and the nose is moved to the right . wikipedia says , in spacecraft propulsion , rocket engines are generally mounted on a pair of gimbals to allow a single engine to vector thrust about both the pitch and yaw axes ; or sometimes just one axis is provided per engine . to control roll , twin engines with differential pitch or yaw control signals are used to provide torque about the vehicle 's roll axis . the right and left gimbaling is necessary to direct the rocket to its original path , thereby maintaining its stability . . . this link gives a good explanation regarding the stability of rockets . this essay is also good , but it is somewhat big . . .
your assumption is very wrong . gravity is everywhere . or simply , the box has mass . it exerts its very own gravity . if your 1st location has the field , then the 2nd location is also influenced by it unless it is at $\infty$ . you can not just shield anything from gravitational field . so , energy is still conserved when you are moving the box from one place to another . . .
yes , in fact one of the comments made to a question mentions this . if you stick to newtonian gravity it is not obvious how a photon acts as a source of gravity , but then photons are inherently relativistic so it is not surprising a non-relativistic approximation does not describe them well . if you use general relativity instead you will find that photons make a contribution to the stress energy tensor , and therefore to the curvature of space . see the wikipedia article on em stress energy tensor for info on the photon contribution to the stress energy tensor , though i do not think that is a terribly well written article .
this problem is somewhat ill-formed . it is difficult to even define what ' static at the origin ' means--static relative to what ? what i will do is show the steps one needs to take to derive the motion of a particle , however . the following will be more constructive than explanatory , as explaining all of this thoroughly would be close to a whole chapter in a relativity/differential geometry textbook . given the metric tensor $g_{ab}$ , one can define the christoffel symbols $\gamma_{ab}{}^{c}$ by the equation$^{1}$ $\gamma_{ab}{}^{c} = \frac{1}{2}g^{cd}\left ( \partial_{a}g_{bd}+\partial_{b}g_{ad}-\partial_{d}g_{ab}\right ) $ . while the christoffel symbols serve an array of purposes in relativity , the simplest notion that one can tie to them is that they define a notion of parallel transport in the spacetime--a vector $v^{a}$ is parallel transported along a curve $x^{a}$ if $x^{a} \nabla_{a}v^{b} \equiv x^{a}\partial_{a}v^{b} + x^{a}v^{c}\gamma_{ac}{}^{b} =0$ . now , we can talk about paths whose tangent vectors are parallel translated relative to themselves . these paths are known as geodesics . if we parameterize these curves with respect to the arc length subtended by the curve$^{2}$ ( which we interpret as the clock of the observer following the geodesic ) , then the curves satisfy the equation $\ddot x^{a} + \gamma_{bc}{}^{a}\dot x^{b} \dot x^{c} = 0$ . if we choose the $x^{a}$ as a locally cartesian orthonormal set of coordinates , then we can interpret $\ddot x^{a}$ as the acceleration of the $x^{a}$ position along the curve , and then interpret the other term as the acceleration of the curve . practically , this is a difficult game to interpret in general relativity , since time is one of the coordinates ( making acceleration a subtle thing to define ) , and because our coordinates are completely arbitrary and need not even be ( and in fact , usually are not ) orthonormal . nonetheless , this interpretation can be useful for a few special cases--the schwarzschild black hole metric being one of them . here , it is actually easiest to look at the conserved energy of a particle on the geodesic first , and then to define a force from this . here , in units where $g=c=1$ , we have $e^{2} -1= \dot r ^{2} - \frac{2m}{r} + \left ( \frac{l}{r}\right ) ^{2} - \frac{2ml^{2}}{r^{3}}$ , where e is the energy per unit mass of the test particle , $m$ is the mass of the central gravitating object , and $l$ is the angular momentum of the test particle . taking a time derivative of this , and dividing by the common factor $2\dot r$ , we get : $$0=\ddot r + \frac{m}{r^{2}}-\frac{l^{2}}{r^{3}} + \frac{3ml^{2}}{r^{4}}$$ the first three terms should be familiar from newtonian theory--they are precisely the newtonian gravitational force term plus the centripetal inertia of the orbit . relative to the clock of an observer in orbit , so long as $\frac{3ml^{2}}{r^{4}}$ is small , newtonian motion should be indistinguishable from relativistic motion . so , the only difference in acceleration comes from the rotation of the orbit . it is precisely this term that created the anamolous acceleration of mercury that was such a puzzle to the astronomers at the turn of the century . when einstein observed that this term was of the right sign and magnitude to explain this anamoly was when he truly believed that he had found the correct theory of gravity . $^{1}$ where $\partial_{a}f$ is defined to be the derivative of $f$ with respect to the a$^{th}$ coordinate : $\partial_{a}f \equiv \frac{\partial f}{\partial x^{a}}$ $^{2}$ this sounds complicated , but is in fact what we do when measuring angles on a circle in radians--how many radians i trace out tells me how much length along the circle i have traced out since i have left the origin . same basic idea here .
it is easy to get mixed up between time and the flow of time , and i think you have done this in your question . take time first : since 1905 we describe any event as a spacetime point and label it with four co-ordinates ( $t$ , $x$ , $y$ , $z$ ) . saying that time does not exist means we can ignore the time co-ordinate and label everything by just it is spatial co-ordinates ( $x$ , $y$ , $z$ ) , which is contradiction with observations . the time co-ordinate obviously exists and be used to distinguish events that happen at the same place but at different times . now consider the flow of time : actually this is a tough concept , and relativity makes it tougher . we all think we know what we mean by the flow of time because we experience time passing . to take your example of movement , we describe this as the change of position with time , $d\vec{r}/dt$ , where we regard time as somehow fundamental . i am guessing that this is what you are questioning i.e. whether the flow of time is somehow fundamental . i do not think there is a good answer to this . to talk about the flow of time you had have to ask what it was flowing relative to . in relativity we can define the flow of co-ordinate time relative to proper time , $dt/d\tau$ , and indeed you find that this is variable depending on the observer and in some circumstances ( e . g . at black hole event horizons ) co-ordinate time can stop altogether . but then you had have to ask whether proper time was flowing . you could argue that proper time is just a way of parameterising a trajectory and does not flow in the way we think time flows . at this point i am kind of stuck for anything further to say . if i interpret your question correctly then you do have a point that just because we observe change of position with time ( i.e. . movement ) this does not necessarily mean time is flowing in the way we think . however i am not sure this conclusion is terribly useful , and possibly it is just semantics .
the reference you link to is for objects orbiting sun ( i.e. . comets or asteroids ) . if you are wanting to deal with satellites in low earth orbit , you will need a good book on orbital dynamics or astrodynamics . objects in solar orbit are typically dealt with in celestial mechanics . in both cases , the physics is the same ( two body gravitational interaction ) but the terminology is different . for satellites , one does not speak of " perihelion " or " aphelion " but rather " perigee " and " apogee . " some of the orbital parameters have slightly different names too . for satellites , orbital elements are disseminated in a highly standardized form called tle , which stands for two line element . there are also highly standardized algorithms for taking elements in tle form and turning them into topocentric ra and dec values as a function of time . your best bet , aside from an appropriate textbook on orbital dynamics , is to look at the source code for the algorithms themselves . the free satellite program predict http://www.qsl.net/kd2bd/predict.html stands out as being reliable and well documented . of course , the source code is included in the download . if you google around you may even find the nasa papers that document the sgp4/sdp4 algorithms . here are some c++ and c# implementations of the norad sgp4/sdp4 algorithms ( i have not tested or otherwise evaluated them ) . http://www.zeptomoby.com/satellites/ here 's another site i just found via google that looks potentially useful . http://satelliteorbitdetermination.com
as adam said this is not always true . this statement is only correct if you are in the ground state . if you want a path integral formalism demonstration , you can write a partition function introducing a counting field j in your action : $$ z [ j ] =\int \mathcal d\phi \exp\left ( -i\int dt \phi ( t ) a \phi ( t ) +2i\int\phi ( t ) j ( t ) \right ) $$ with a the propagator . if we introduce c the green function so that $ac ( t , t' ) =\delta ( t-t' ) $ , we can make the chage of variable : $$\tilde\phi ( t ) =\phi ( t ) -i\int dt'c ( t , t' ) j ( t' ) $$ you get $$ z [ j ] =\int \mathcal d\phi \exp\left ( -i\int dt \tilde\phi ( t ) a \tilde\phi ( t ) + -i\iint dt dt ' j ( t ) c ( t , t' ) j ( t' ) \right ) $$ so that : $$ \langle\phi ( \tau ) \rangle=\left . \frac{\delta \ln z [ j ] }{i\delta j}\right|_{j=0}=-2i\lim_{j\to0}\int dt'c ( \tau , t' ) j ( t' ) =0 $$
global gauge invariance , cf . wikipedia .
yes , it will be harmful , because the isotope will travel through your bloodstream and deliver radiation damage to cells all over your body . if you want to know how harmful it is , check the recent case of alexander litvinenko , who was assassinated with polonium-210 .
we have not ironed out all the details about how planets form , but they almost certainly form from a disk of material around a young star . because the disk lies in a single plane , the planets are broadly in that plane too . but i am just deferring the question . why should a disk form around a young star ? while the star is forming , there is a lot of gas and dust falling onto it . this material has angular momentum , so it swirls around the central object ( i.e. . the star ) and the flow collides with itself . the collisions cancel out the angular momentum in what becomes the vertical direction and smear the material out in the horizontal direction , leading to a disk . eventually , this disk fragments and forms planets . like i said , the details are not well understood , but we are pretty sure about the disk part , and that is why the planets are co-planar .
yes , massive particles such as w-bosons , z-bosons , quarks , and leptons couple to the higgs field via the cubic ( yukawa ) interaction , so they may also exchange the virtual higgs . yes , because the virtual particle is massive , one gets the yukawa potential that includes the exponential dumping with distance . this " higgs force " is much less fundamental and important than the four fundamental interactions ( strong and weak nuclear forces , electromagnetism , gravity ) because it is not made inevitable by any local/gauge/diffeomorphism symmetry the yukawa coupling is tiny so the force is extremely weak even before it drops exponentially with distance for stable particles such as electrons the yukawa coupling and the force is only strong for heavy enough particles such as the top quarks but those particles are unstable so before one may measure this weak , highly localized force , the particle decays . the third point is related to the fact that unlike electrostatic , magnetostatic , and gravitational static forces , the higgs exchange does not become " more important " when some objects are at rest . instead , the feynman diagrams with the higgs exchange are examples among many and they are usually important primarily for very quickly moving particles . when the speeds are close to the speed of light , one has to use the full quantum field theory and the concept of " force " , relevant only in mechanics , becomes inadequate .
this is a heavy question , that contains many topics in it that are worthy of their own questions , so i am not going to give a complete answer . i am relying mainly on this excellent review paper by nayak , simon , stern , freedman and das sarma . the first part can be skipped by anyone already familiar with anyons . abelian and non-abelian anyons anyons are emergent quasiparticles in two dimensional systems that have exchange statistics which are neither fermionic nor bosonic . a system that contains anyonic quasiparticles has a ground state that is separated by a gap from the rest of the spectrum . we can move the quasiparticles around adiabatically , and as long as the energy we put in the system is lower than the gap we will not excite it and it will remain in the ground state . this is partly why we say the system is topologically protected by the gap . the simpler case is when the system contains abelian anyons , in which case the ground state is non-degenerate ( i.e. . one dimensional ) . when two quasiparticles are adiabatically exchanged we know the system cannot leave the ground state , so the only thing that can happen is that the ground state wavefunction is multiplied by a phase $e^{i \theta}$ . if these were just fermions or bosons than we would have $\theta=\pi$ or $\theta=0$ respectively , but for anyons $\theta$ can have other values . the more interesting case is non-abelian anyons where the ground state is degenerate ( so it is in fact a ground space ) . in this case the exchange of quasiparticles can have a more complicated effect on the ground space than just a phase , most generally such an exchange applies a unitary matrix $u$ on the ground space ( the name ' non-abelian ' comes from the fact that these matrices do not in general commute with each other ) . the quantum dimension so we know that the ground space of a system with non-abelian anyons is degenerate , but what can we say about its dimension ? we expect that the more quasiparticles we have in the system , the larger the dimension will be . indeed it turns out that for $m$ quasiparticles , the dimension of the ground space for large $m$ is roughly $\sim d_a^{m-2}$ where $d_a$ is a number that depends on $a$ - the type of the quasiparticles in the system . this scaling law is reminiscent of the scaling of the dimension of a tensor product of multiple hilbert spaces of dimension $d_a$ , and for this reason $d_a$ is called the quantum dimension of a quasiparticle of type $a$ . you can think of it as the asymptotic degeneracy per particle . for abelian anyons we have a one-dimensional ground space no matter how many quasiparticles are in the system , so for them $d_a=1$ . although we used the analogy to a tensor product of hilbert spaces , note that in that case the dimension of each hilbert space is an integer , while the quantum dimension is in general not an integer . this is an important property of non-abelian anyons that differentiates them from just a set of particles with local hilbert spaces - the ground space of non-abelian anyons is highly nonlocal . more details on anyons and the quantum dimension can be found in the review paper cited above . the quantum dimension can be generalized to other systems with topological properties , maintaining the same intuitive meaning of asymptotic degeneracy per particle . it is in general very hard to calculate the quantum dimension , and there is only a handful of papers that do ( most of them cited in the paper by kitaev and preskill that inspired this question ) . relation to entanglement i can also try and give a handwaving argument for why the quantum dimension would be related to entanglement . first of all , the fact that the entanglement entropy of a bounded region depends only on the length of the boundary $l$ and not on the area of the region is very clearly explained in this paper by srednicki , which is also cited by kitaev and preskill . basically it says that the entanglement entropy can be calculated by tracing out the bounded region , or by tracing out everything outside the bounded region , and the two approaches will yield the same result . this means the entanglement has to depend only on features that both regions have in common , and this rules out the area of the regions and leaves only the boundary between them . now for a system with no topological order the entanglement would go to zero when the size of bounded region goes to zero . however for a topological system there is intrinsic entanglement in the ground space which yields the constant term $-\gamma$ in the entanglement . the maximal entanglement entropy a system with dimension $d$ has with its environment is $\log d$ , so in an analogous manner the topological entanglement is $\gamma=\log d$ where $d$ is the quantum dimension . again this last argument relies heavily on handwaving so if anyone can improve it please do . i hope this answers at least the main concerns in the question , and i welcome any criticism .
evaporation is a different process to boiling . the first is a surface effect that happens at any time , while the latter is a bulk transformation that only happens when the conditions are correct . technically the water is not turning into a gas , but random movement of the surface molecules allows some of them enough energy to escape from the surface into the air . the rate at which they leave the surface depends on a number of factors - for instance the temperature of both air and water , the humidity of the air , and the size of the surface exposed . when the bridge is ' steaming': the wood is marginally warmer than the air ( due to the sun shine ) , the air is very humid ( it has just been raining ) and the water is spread out to expose a very large surface area . in fact , since the air is cooler and almost saturated with water , the molecules of water are almost immediately condensing into micro-droplets in the air - which is why you can see them . btw - steam is completely transparent . if you can see it then it is water vapour . consider a kettle boiling - the white plume only occurs a short distance above the spout . below that it is steam , above it has cooled into vapour .
when we ask " how strong is this force ? " what we mean in this context is " how much stuff do i need to get a significant amount of force ? " richard feynman summarized this the best in comparing the strength of gravity - which is generated by the entire mass of the earth - versus a relatively tiny amount of electric charge : and all matter is a mixture of positive protons and negative electrons which are attracting and repelling with this great force . so perfect is the balance however , that when you stand near someone else you do not feel any force at all . if there were even a little bit of unbalance you would know it . if you were standing at arm 's length from someone and each of you had one percent more electrons than protons , the repelling force would be incredible . how great ? enough to lift the empire state building ? no ! to lift mount everest ? no ! the repulsion would be enough to lift a " weight " equal to that of the entire earth ! another way to think about it is this : a proton has both charge and mass . if i hold another proton a centimeter away , how strong is the gravitational attraction ? it is about $10^{-57}$ newtons . how strong is the electric repulsion ? it is about $10^{-24}$ newtons . how much stronger is the electric force than the gravitational ? we find that it is $10^{33}$ times stronger , as in 1,000,000,000,000,000,000,000,000,000,000,000 times more powerful !
the term " rosenberg-coleman effect " originates from the article heliographic latitude dependence of the dominant polarity of the interplanetary magnetic field . it is also referred to as the " dominant polarity effect " . as the earth orbits the sun , the earth travels above and below the equator of the sun . according to rosenberg and coleman , the polarity of the interplanetary magnetic field ( imf ) at a given location in the solar system , such as earth , depends upon the corresponding latitude of the sun . according to this proposal , the imf at earth should be dominated by the southern solar pole from december 7th to june 6 , and the northern pole the other half of the year .
consider a tiny part of th conductor 's surface . then the field at this part is approximately uniform so this is like an infinite parallel plane : $e = \sigma/2\epsilon_0$ . whence , the surface charge density is $\sigma = 2\epsilon_0 e$ . since it is a conductor , there is no volumetric charges : everything is concentrated in the surface .
for newtonian fluids ( such as water and air ) , the viscous stress tensor , $t_{ij}$ , is proportional to the rate of deformation tensor , $d_{ij}$: $$d_{ij} = \frac{1}{2}\left ( \frac{\partial v_i}{\partial x_j} + \frac{\partial v_j}{\partial x_i}\right ) $$ $$t_{ij} = \lambda\delta\delta_{ij} + 2\mu d_{ij}$$ where $\delta \equiv d_{11} + d_{22} + d_{33}$ . the navier-stokes equation for newtonian fluids can then be written as : $$\rho\left ( \frac{\partial v_i}{\partial t} + v_j\frac{\partial v_i}{\partial x_j}\right ) = -\frac{\partial p}{\partial x_i} + \rho b_i + \frac{\partial t_{ij}}{\partial x_j}$$ the navier-stokes equation above governs both laminar and turbulent flow using the same stress tensor . this shows that the definition of shear rate is the same in both laminar and turbulent flows , however , their values will be very different . for non-newtonian fluids , the same is true . instead of the stress tensor defined above , replace it with a non-newtonian stress tensor . still the same governing equation applies to laminar and turbulent flows so the definition of shear rate is the same for both regimes . as you mention , turbulent flow does not have nice , orderly layers . as a result , there can be acute stress localizations .
i think the answer to this depends a lot on your definition of " directly . " relativity of simultaneity is built into the lorentz transformation , and lorentz invariance is one of the most precisely tested physical theories in all of history . essentially you are asking for an experiment that verifies one element of the matrix involved in the lorentz transformation , but every element of the matrix is present in all cases . i would consider the sagnac effect to be a fairly direct test , and the sagnac effect was one of the effects observed in the hafele-keating experiment , as well as many other , earlier tests of relativity . every time you fly on a commercial jet , you are benefiting from a ring laser gyro , which works based on the sagnac effect .
these problems relate to constraints . this particular subset of costraint problems will fall under atwood machines ( pulley problems ) , under laws of motion , under mechanics . these are not usually standalone problems per se , but constraints are required for one to be able to solve pulley problems . constraints basically , these do with the ' conservation of length of string ' . lets take a simple two-block-one-pulley system . the movable parts in it are the two blocks . now , lets assume each one moves a distance $s_1 , s_2$ . assume that by some mystical method , only one block moved . write down the ( signed ) elongation in the string required . do the same for the other block . now , for the block which moves up , the elongation will be $-s_1$ ( negative since its a contraction ) . for the block moving down , it will be $+s_2$ . since the string is not stretchy , the net elongation should be zero . so $-s_1+s_2=0\implies s_1=s_2$ . ok , we sort of knew that . but this method is scalable , unlike the intuitive method of staring at it for a while . we can differentiate this to get relations between velocities , accelerations , jerks , jounces , whatever . note that in the given problem , your pulley moves as well , and it elongates the string thrice as much as its motion . you can see the constraint relation peeking out in the nearby equation ( dunno what the $c$ and $t$ are ) shortcut-virtual work there is a shortcut method to this , which is even more scalable . assiume massless pulleys ( this method works for massive pulleys as well , as it is a geometric relation in the end ) for each string in the system , give it a tension $t_i$ . remember , tension in a string is constant , even if it wrapped around a ( masless ) pulley . note that there is a string between the lowermost pulley and the block b as well , even if it is not shown in the diagram . all connections should be done via a string ( except connections with walls , they can be whatever ) get relations between the tensions . remember that net force on a masless pulley must be zero , and tension always acts away from the object . now , since net work done by internal forces must be zero , and tension is an internal force , then at every edge of the string ( where a pulley is not connected ) , we can calculate work by simply multiplying tension with the displacement ( giving it a negative sign if they are opposite in direction ) ; adding the works , and equating to zero . in your problem , if $t$ is the tension of the large string and $t_1$ is the tension in the small ( nonexistant ) string , then by equating forces about the lower pulley , we get $t+t+t=t_1$ . taking the same signs and symbols for displacement in the diagram , and applying virtual work , we get $t_1s_b+ts_c=0$ . one can see that this simplifies to the constraint relation . this looks long , but thats because i explained the steps . with practice , all you have to do is choose a string , give it tension $t$ , and write all other tensions in terms of that tension . then you mark the displacements and can " read off " the constraint relation no matter how complicated it is .
well , let 's see if i understood correctly the question . you are talking of this : you have some kind of pendulum attached to a point r . in fact , the total kinetic energy is $$t=t_{p}+t_{rot}$$ where $t_{p}$ is the translational energy of the point p and $t_{rot}$ the rotational energy around that point . realize that the expression of the two energies is different , depending of your axis . when you consider your axis system at a fixed point , you only have rotational energy , because the fixed point has not a transalation . the other common option is to take the center of mass , and , in that case , you also have to consider its translational energy . this is useful if you do not have any fixed point . however , $t$ is indepedent of the axis , so when you calculate rotational kinetic energy on p , you obtain the total energy ; in the other hand , the center of mass of the system is translating and rotating ( if the rod has mass ) , so translational energy of the ball will be lower than total energy . if the rod has no mass , then the center of mass is only translating and yes , in that case the rotational energy on p would be the same has the ball 's translational energy . to solve problems with fixed point , the best option is to take that point , because you do not have to deal with translational energy . in this particular case , p is a fixed point , so you can calculate the moment of inertia of the system on p ( using steiner 's theorem ) , write the position and the angular velocity as function of $\beta$ , ( angular velocity would be $\dot{\beta}$ ) , and apply energy conservation . if the rod has no mass , you also can try to calculate center of mass position in terms of $\beta$ , derive it to obtain the velocity and calculate the modulus . if the rod has mass , this second method becomes a difficult problem : you have to calculate center of mass of the two bodies , the inertia tensor at that point , and apply both translational and rotational energies . i hope this will be useful . if you need some aclaration , say it .
no , because even though the force that you exert on the earth is equal and opposite to the force it exerts back on you , you are not doing the same amount of work on the earth as the earth on you . your kinetic energy increases due to the work done by the earth on you . remember that $w = f \cdot d$ ; your bicycle moves a lot due to this force , but the earth does not really move much at all . another way to think about this is in terms of kinetic energy . $\mathrm{ke} = \frac{1}{2} mv^2$ , so if your velocity is high , so is your kinetic energy . the earth 's velocity is low , and so is its kinetic energy . so the forces are equal and opposite , and the impulse , or change in momentum , is too , but the kinetic energy stays mostly with you .
i think emilio pisanty 's answer is good enough . but here is another longer , ' magnetic charge ' approach . ( let 's specify the coordinates first ( sorry i borrow your picture ) . it is obvious that the toroid is symmetrical under rotation along $\hat{\phi}$ direction . thus we can not have magnetic field along $\hat{\phi}$ . which means it is sufficient for us to find the magnetic field on the $xz$ plane , and we can generalize later by rotating this $xz$ plane . we have some constrains to consider here due to the shape of torus : $\vec{b}$ is symmetrical under reflection over $\hat{x}$ axis and $\hat{z}$ axis . $\nabla\times b=0$ . $\nabla . b=0$ the most general field lines in $xz$ plane that satisfy these conditions roughly looks like this . now we only have two possible directions , the one shown in the picture or the opposite of that ( or zero everywhere ) . we can apply magnetic gauss law here with the gaussian surface marked with black dotted line ( in 3d point of view this black dotted line is rotated along $\phi$ so that the product looks like a mountain ) . the only part of the gaussian surface which has magnetic flux through it is the top part . the remaining area is intentionally shaped to follow the field lines tangentially so that there is no flux through it . now we only need to determine whether the magnetic charge inside the envelope is positive or negative ( positive charges are shown in blue , and negative charges in red ) . as we can see in the picture , the inner magnetic field which is represented by yellow lines may bend slightly from radial direction towards the direction which allows the envelope to catch more or less positive charge . so the total charge inside the envelope might be positive or negative . however if the yellow lines bend in a way like that , then we will meet a kink somewhere between the inner and outer field or we may also get a non-zero curl field and these are impossible . so we are only left with a radial inner field . and therefore the total charge inside the envelope is zero and there can not be any flux passing through it . so in conclusion the field outside must vanish everywhere .
if the cage is completely closed , it does not make a difference if the bird is hovering inside it or if it sits on the ground . when flying , the bird pushes air to the ground which will exert a downward force on the cage exactly equal to the weight of the bird . this is a direct consequence of the conservation of momentum and newton 's second and third law . since no additional external force is acting on the cage-bird-system when the bird is flying as compared to when it is not , the acceleration on the cage can be no different . the effect due to the flying bird only concerns the internal forces and since action=reaction , they cancel . however , if the cage would not be closed , some of the ' wind ' due to the bird could escape the cage and would become an external force , making the cage-bird-system lighter .
which particular observation , made us think that it could be the other way around retrograde motion must be a prime candidate . as seen from earth against star background , mars occasionally slows down and goes backwards . our moon does not . it probably became clear to people constructing orreries that heliocentric models were enormously simpler and more convincing . they also tied in with simple inverse square laws of gravitation and planetary motion . the discovery by galileo galilei of jupiter 's moons also provided firm evidence of the existence of heavenly objects that , perversely , did not orbit the earth . photo : thomas bresson ( galileo probably did not have a nikon / mobile phone handy ) luckily , he had available a corner of a napkin , a goose and some soot ( or equivalents )
light does not , in general circumstances , travel in straight lines ( although it does do so in the ones we usually encounter ) . for one , light is really a wave and can only approximately be thought of as consisting of independently-propagating rays . this happens when the wavelength of the light is much smaller than the distances it is propagating over , which is usually the case for light ( whose wavelength in the visible range is $0.4$ to $0.7\ , \mu\textrm{m}$ ) but is not necessarily the case e.g. for radio waves and when nanoparticles are involved . in this short-wavelength limit , wave propagation gives way to ray propagation ( which is a special , approximate case of the former ) , and specifically to fermat 's principle for the mathematical description of light . this principle states that light rays starting at $a$ and ending up at $b$ will follow the path that minimizes the travel time $$s=\int_a^b n ( s ) \textrm{d}s , $$ where $n ( s ) $ is the ( possibly spatially dependant ) refraction index along the path . for a homogeneous medium , this does indeed give straight lines for propagation . for a planar interface between two different media it gives snell 's law for refraction and it also describes reflection . ( however , because it does not account for the actual nature of light as an oscillating electric field , this description cannot predict transmission or reflection coefficients . however , if the medium is not homogeneous , then light will not travel on a straight line , and for complicated inhomogeneities the path can be correspondingly difficult to calculate . for an example , see the formation of mirages or more generally atmospheric refraction . conversely , if one has a path one wishes a given light ray to take , then it is possible to engineer a refractive index spatial dependence that will make light bend that way . ( of course , whether such a dependence is physically reasonable is another matter ; if the path bends too sharply then it may not be possible to find materials with the correspondingly large index and index gradients necessary . )
because as far as we understand general relativity , it is not doing " the opposite of what gravity does . " gravity can be locally attractive or repulsive , depending on whether the stress-energy content satisfies or violates the strong energy condition . for ordinary matter , the stress-energy is dominated by the mass , the sec holds , and its gravity is attractive . but this need not be the case in general . since gravity depends on more than the ( mass- ) energy content , in cosmological models , the universe accelerates or decelerates proportionally to $\rho+3p$ , where $\rho$ is the energy density and $p$ is the pressure . the factor of $3$ for the pressure comes from the fact that there are three spatial dimensions , but only one temporal dimension . in particular , the cosmological constant corresponds to the case of a perfect fluid with $p = -\rho$ ( which can be generalized to other " equations of state" ) , and so a positive " dark energy " density has a repulsive effect , not because it is " the opposite of what gravity does " , but rather because its negative pressure gravitates in addition to its energy .
yes . special relativity satisfies your question somehow . probably , there are many discussions in se based on the topic . but , there are several cons i would like to correct in your question . . . if movement occurs faster , time freezes . more the movement , more the time freezes . according to einstein - length , mass , time and space are interdependent variables . these motions depend on the speed of light $c$ in vacuum which is the only constant here and is what the second postulate says . ok , let 's take your " time " part . . . time does not freeze out anytime . it may freeze if you move at $c$ which is impossible ( practically and theoretically ) to achieve in vacuum . and , you say , " more the movement , more it freezes " . it is not a state of matter . and , no laws have ever proved that " time depends upon temperature " . but , time really slows down when you approach a good velocity comparable to $c$ . time dilation could be predicted indeed ( taking lorentz factor into account ) using $$t=\frac{t_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ where $t_0$ is the time measured by an observer at its rest frame . $t$ is the apparent time measured which is always less than $t_0$ and it reduces with increasing velocity . ( freezes more - in your words ) how come then theory of relativity claims that " moving faster may transport you ahead in time " ? short answer - no , there is no such thing in relativity . but , its the postulates that made the physicists think about the plausibility of time travel . these guys thought it in a different way . if there is a possibility for objects to travel faster than light ( assuming hypothetical particles called tachyons to study the behaviour ) , maybe it would forward time into future ( like that ) . . . but , i think it is unsuccessful . . !
ayush : is not the question telling that the bullet always loses 1/n th of its velocity no matter which plank ? based on the answer provided , it seems the writer wanted you to assume that the energy loss per plank is constant . this is not the same as the bullet losing $1/n^\text{th}$ of its velocity per plank ( however , the fact that the question does not mention this assumption arguably makes the question ambiguous ) . with this assumption , the energy loss becomes $$\delta e=\frac{1}{2}mv^2-\frac{1}{2}m\left ( v-\frac{v}{n}\right ) ^2$$ and the number of planks $n$ becomes $$n=\frac{\frac{1}{2}mv^2}{\delta e}=\frac{n^2}{2n-1} . $$ otherwise , if you assume that the bullet loses $1/n^\text{th}$ of its velocity per plank , then the answer is $n=\infty$ .
the answer is no . the pole would bend/wobble and the effect at the other end would still be delayed . the reason is that the force which binds the atoms of the pole together - the electro-magnetic force - needs to be transmitted from one end of the pole to the other . the transmitter of the em-force is light , and thus the signal cannot travel faster than the speed of light ; instead the pole will bend , because the close end will have moved , and the far end will not yet have received intelligence of the move . edit : a simpler reason . in order to move the whole pole , you need to move every atom of the pole . you might like to think of atoms as next door neighbours if one of them decides to move , he sends out a messenger to all his closest neighbours telling them he is moving . then they all decide to move as well , so they each send out messengers to to their closest neighbours to let them know they are moving ; and so it continues , until the message to move has travelled all the way to the end . no atom will move until he has received the message to do so , and the message will not travel any faster than all the messengers can run ; and the messengers can not run faster than the speed of light . /b2s
the claims are absurd--- you can not extract " zero point energy " , because it just is not there--- energy is extracted by moving something from a higher energy state to a lower energy state , and the vacuum is the lowest possible energy . nevertheless , there are many fraudulent claims to the contrary , and this is one of them . in response to comment it is easy to dismiss claims that there are new fields affecting material substances--- there are not any such fields . the author here does not provide evidence for these fields beyond speculating , and if these fields could produce macroscopic forces in matter , they will generically generate friction in materials , so that you would notice missing energy which is radiated into these modes . this is too obvious to miss . further , even if there were new fields , in their vacuum configuration , they would be useless for extracting energy . you would need to find an excited configuration and drain out the energy . this stuff is not worth the bother of reading it .
this was an idea einstein had soon after developing gr , and it is developed in the classical unified field literature , with the starting point being einstein 's " do gravitational fields play a role in the constitution of the elementary particles ? " this is one of the defining program papers of the unified field framework . this idea is also discussed off and on within heuristic models of charges throughout the 1950s-80s . the essential points are that as you make the electron smaller , at some point , gravity will become dominant . the problem with such ideas is that they generally do not have a good idea of quantum gravity to make the microscopic model precise . all these classically inspired ideas are obsoleted by string theory and subsumed into it . within string theory , the fundamental objects are dual to black holes , so that their classical limit is identifiable as a recognizable extremally charged black hole of the classical limit supergravity theory . aside from identifiable black holes , there is no other matter ( arguably--- there is the question of whether orbifolds count as " matter" ) . so for example , for the m-theory , the objects are the extremally charged m2-branes which are the extremally charged black holes you can make using the 3 form gauge field , and their 5-brane magnetic duals ( the magnetically extremally charged black holes ) . that is it for m theory . the brane-spectrum of a theory is the answer to classical question " what extremal black hole can i form ? " the identification of black holes with matter is important , because the internal construction of strings is fully specified by the theory . so that the electron , if it is a string theory excitation , is an object whose internal structure is completely known , because you know the scattering off the electron at arbitrarily high energies . further , in the strong scattering case , we can continuously link the electron to both netural and extremally charged black holes , so that the theory is a full realization of einstein 's program . since i believe string theory is the correct theory of everything , i do not think there is much point in investigating these types of ideas in a different direction . but some people who like loops disagree .
recent developments on lattice computations in qcd have shown that the beta function for a pure yang-mills theory ( let me emphasize that is not true for the full qcd ) goes to zero lowering momenta and so , the theory seems to reach a trivial infrared fixed point ( see here , fig . 5 ) . the matter about low-energy behavior of a yang-mills theory seems rather well settled through studies as the one i have just cited . a trivial infrared fixed point means that the theory is well described by a free one at that limit and , if one knows the form of the propagator , a form of the potential can be computed through wilson loop ( see here ) . i give this computation here . for a free theory , the case of the infrared trivial fixed point , the generating functional is gaussian ( i will consider landau gauge ) \begin{equation} z_0 [ j ] =\exp\left [ \frac{i}{2}\int d^4x'd^4x''j^{a\mu} ( x' ) d_{\mu\nu}^{ab} ( x'-x'' ) j^{b\nu} ( x'' ) \right ] \end{equation} and so , the evaluation of the wilson loop is straightforwardly obtained as \begin{equation} w [ {\cal c} ] =\exp\left [ -\frac{g^2}{2}c_2\oint_{\cal c}dx^\mu\oint_{\cal c}dy^\nu d_{\mu\nu} ( x-y ) \right ] \end{equation} being $c_2=$ the quadratic casimir operator that for su ( n ) is $ ( n^2-1 ) /2n$ . the fall-off to large distances of this propagator grants that ordinary arguments to evaluate the integrals on the path apply . indeed , using fourier transform one has \begin{equation} w [ {\cal c} ] =\exp\left [ -\frac{g^2}{2}c_2\int\frac{d^4p}{ ( 2\pi ) ^4}\delta ( p ) \left ( \eta_{\mu\nu}-\frac{p_\mu p_\nu}{p^2}\right ) \oint_{\cal c}dx^\mu\oint_{\cal c}dy^\nu e^{-ip ( x-y ) }\right ] . \end{equation} we need to evaluate \begin{equation} i ( {\cal c} ) =\eta_{\mu\nu}\oint_{\cal c}dx^\mu\oint_{\cal c}dy^\nu e^{-ip ( x-y ) } \end{equation} provided the contributions coming from taking into account the term $\frac{p_\mu p_\nu}{p^2}$ run faster to zero at large distances . this must be so also in view of the gauge invariance of wilson loop . with the choice of time component in the loop going to infinity while distance is kept finite , we can evaluate the above integral in the form \begin{equation} i ( {\cal c} ) \approx 2\pi t\delta ( p_0 ) e^{-ipx} \end{equation} and we are left with \begin{equation} w [ {\cal c} ] \approx \exp\left [ -t\frac{g^2}{2}c_2\int\frac{d^3p}{ ( 2\pi ) ^3}\delta ( {\bf p} , 0 ) e^{-i{\bf p}\cdot{\bf x}}\right ] \end{equation} yielding \begin{equation} w [ {\cal c} ] =\exp\left [ -tv_{ym} ( r ) \right ] . \end{equation} so , if you are able to compute yang-mills propagator in the low-energy limit you will get an answer to your problem .
in addition to what dmckee said , another hint at ( "large" ) extra dimensions would be the detection of kaluza-klein particles at the lhc for example . kaluza-klein particles are in principle nothing but the known standard model particles which can propagate into the extra dimensions if these are large enough . it can be shown that the angular momentum in these extra dimensions is quantized . this leads to the effect that particles propagating into the extra dimensions would be observed as heavier versions of the known standard model particles due to the additional momentum in the otherwise not directly visible dimensions . the energy ( or mass squared ) spectrum of the corresponding expected particle tower would have a step size proportional to 1/r ( where r is the radius of the extra dimension ) . as prof . strassler explains here , to determine the shape and extent of such large extra dimensions it would be necessary to measure the whole mass spectrum using more than one kk particle . up to now no kk particles have shown up at the lhc so far ( which was run only at 7tev and now continues at 8 tev ) . but note that even if there could be such large extra dimensions leaving hints at themselves at the " lhc scale " ( up to 14 tev ) , this does not have to be the case for st to work ; the " large " extra dimensions are only a feature of certain ( phenomenologica ) models . . .
entanglement is being presented as an " active link " only because most people - including authors of popular ( and sometimes even unpopular , using the very words of sidney coleman ) books and articles - do not understand quantum mechanics . and they do not understand quantum mechanics because they do not want to believe that it is fundamentally correct : they always want to imagine that there is some classical physics beneath all the observations . but there is none . you are absolutely correct that there is nothing active about the connection between the entangled particles . entanglement is just a correlation - one that can potentially affect all combinations of quantities ( that are expressed as operators , so the room for the size and types of correlations is greater than in classical physics ) . in all cases in the real world , however , the correlation between the particles originated from their common origin - some proximity that existed in the past . people often say that there is something " active " because they imagine that there exists a real process known as the " collapse of the wave function " . the measurement of one particle in the pair " causes " the wave function to collapse , which " actively " influences the other particle , too . the first observer who measures the first particle manages to " collapse " the other particle , too . this picture is , of course , flawed . the wave function is not a real wave . it is just a collection of numbers whose only ability is to predict the probability of a phenomenon that may happen at some point in the future . the wave function remembers all the correlations - because for every combination of measurements of the entangled particles , quantum mechanics predicts some probability . but all these probabilities exist a moment before the measurement , too . when things are measured , one of the outcomes is just realized . to simplify our reasoning , we may forget about the possibilities that will no longer happen because we already know what happened with the first particle . but this step , in which the original overall probabilities for the second particle were replaced by the conditional probabilities that take the known outcome involving the first particle into account , is just a change of our knowledge - not a remote influence of one particle on the other . no information may ever be answered faster than light using entangled particles . quantum field theory makes it easy to prove that the information cannot spread over spacelike separations - faster than light . an important fact in this reasoning is that the results of the correlated measurements are still random - we can not force the other particle to be measured " up " or " down " ( and transmit information in this way ) because we do not have this control even over our own particle ( not even in principle : there are no hidden variables , the outcome is genuinely random according to the qm-predicted probabilities ) . i recommend late sidney coleman 's excellent lecture quantum mechanics in your face who discussed this and other conceptual issues of quantum mechanics and the question why people keep on saying silly things about it : http://motls.blogspot.com/2010/11/sidney-coleman-quantum-mechanics-in.html
yes . there are only internal forces of your body . without external forces , the center of mass of your body cannot change position . as your center of mass did not move , the main body should move in the opposite direction .
according to quantum electrodynamics ( qed ) , light can be thought of as going along all paths . however , the only paths that do not experience destructive interference are those in the neighbourhood of paths with stationary ( e . g . , minimal ) action ( time ) , which , in your case , is the " equal angles " path . i strongly recommend reading feynman 's qed : the strange theory of light and matter . in the link you will also find a link to video . so , with qed in hand , anthropomorphically , photons do not need to know where to go , because they go everywhere . : )
water is transported through xylem tissue , which reassemble just a passive bundle of pipes . they are narrow enough to provide quite a huge capillary effect , but this is not a process of transport because it converges fast to equilibrium water levels and stops . the flow is powered by two other processes ; first and most important is evaporation of water from leafs ( transpiration pull ) and the second is a osmotic pressure in roots ( root pressure ) . the first one is a passive process , powered by the difference of water potentials in air and soil ; the second is active , plant consumes its own energy to produce ion gradient causing osmotic pressure . i will not give you the accurate numbers in the second part ; to give some imagination , about 90% of water absorbed from soil is transpired into atmosphere , so it is near transpiration rates . a single tree growing in humid continental climate zone ( most of europe and usa ) transpires yearly something like 100 tons of water .
start with force ; force does work when it acts on a body . this displaces the body from its original position . a force does work when it results in this body moving . unit wise , it is joules [ j ] or newton-meters [ nm ] or [ n-m ] . this would imply $w=f \cdot d$ where $d$ is the distance . i prefer this definition ; the work done by some force on an object travels along some curve s is given by a line integral $$w=\int_{s} \vec{f} \cdot d\vec{x}$$ when path dependent . when path independent , one obtains instead that $$w=u ( d_{1} ) -u ( d_{2} ) $$ where $u$ is called the potential energy , measured in joules . $d$ is a point at which the potential is evaluated ; this can be time dependent . hope this helps !
the levi-civita symbol in curved space-time , $\epsilon^{\alpha\beta\gamma\delta}$ , is defined as : $$\epsilon^{\alpha\beta\gamma\delta} = e~ \epsilon^{i'j'k'l'} e_{i'}^\alpha e_{j'}^\beta e_{k'}^\gamma e_{l'}^\delta , $$ where $\epsilon^{i'j'k'l'}$ is the standard levi-civita symbol ( flat spacetime ) . the presence of the $e$ is because $\epsilon^{\alpha\beta\gamma\delta}$ has to transform as a tensor . you could then use the properties of the standard levi-civita symbol . for instance , the product $\epsilon^{i'j'k'l'} \epsilon_{ijkl}$ could be obtained as a determinant of the matrix of kronecker delta symbols $\delta^m_n $ . the product equals $1$ if $i'j'k'l'$ is a even permutation of $ijkl$ , and $-1$ if $i'j'k'l'$ is a odd permutation of $ijkl$ , $0$ in other cases . and you are going to use the fact that the matrix $e_i^\alpha$ and $e_\beta^j$ , are inverse matrix . [ edit ] here is the complete calculus : \begin{align} s_{eh} ( g ) and =\int\sqrt{-g}r\ , \mathrm{d}^4x\\ and =\int \sqrt{-g} r_{\mu\nu} g^{\mu\nu}\ , \mathrm{d}^4x\\ and =\int e\ , e^{\mu}_i e^{\nu~i}r_{\mu\nu\rho\sigma}e^{\rho}_j e^{\sigma~j}\ , \mathrm{d}^4x\\ and =\int e\ , e^{\mu}_i e^{\rho}_j f^{ij}_{\mu\rho}\ , \mathrm{d}^4x \end{align} and \begin{align} s_{eh} ( e ) and =\int \frac{1}{2}\epsilon_{ijkl}~e^i\wedge e^j\wedge f^{kl} ( \omega ( e ) ) \\ and =~\int \frac{1}{2}~\epsilon_{ijkl}~ e_{\alpha}^i \ , \mathrm{d}x^{\alpha} \wedge e_{\beta}^j \ , \mathrm{d}x^{\beta} \wedge ( \frac{1}{2} f_{\gamma\delta}^{k l} \wedge \ , \mathrm{d}x^{\gamma} \wedge \ , \mathrm{d}x^{\delta} ) \\ and =~\int \frac{1}{4}~\epsilon_{ijkl}~ e_{\alpha}^i~ e_{\beta}^j ~f_{\gamma\delta}^{k l} \ , \mathrm{d}x^{\alpha}\wedge \ , \mathrm{d}x^{\beta} \wedge \ , \mathrm{d}x^{\gamma} \wedge \ , \mathrm{d}x^{\delta}\\ and =~\int \frac{1}{4}\epsilon_{ijkl}~ \epsilon^{\alpha\beta\gamma\delta}~e_{\alpha}^i ~e_{\beta}^j ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x\\ and =~\int e\frac{1}{4}~\epsilon_{ijkl} ~\epsilon^{i'j'k'l'} ~e_{i'}^\alpha ~e_{j'}^\beta ~e_{k'}^\gamma ~e_{l'}^\delta ~e_{\alpha}^i ~e_{\beta}^j ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x\\ and =~\int e \frac{1}{4}~\epsilon_{ijkl} ~\epsilon^{i'j'k'l'} ~\delta_{i'}^i ~\delta_{j'}^j ~e_{k'}^\gamma ~e_{l'}^\delta ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x\\ and =~\int e \frac{1}{4}~\epsilon_{ijkl} ~\epsilon^{ijk'l'} ~e_{k'}^\gamma ~e_{l'}^\delta ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x\\ and =~\int e \frac{1}{4} ~2 ~ ( \delta_{k}^l \delta_{k'}^{l'} - \delta_{k}^{l'} \delta_{k'}^l ) ~ e_{k'}^\gamma ~e_{l'}^\delta ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x\\ and =~\int ~e ~\frac{1}{4} 2*2~ ( \delta_{k}^l \delta_{k'}^{l'} ) ~ e_{k'}^\gamma ~e_{l'}^\delta ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x\\ and =~\int ~e ~e_{k}^\gamma ~e_{l}^\delta ~f_{\gamma\delta}^{k l}\ , \mathrm{d}^{4}x \end{align}
one 's naive expectation would be that as the object moves through the medium , it collides with molecules at a rate proportional to $v$ . the volume swept out in time $t$ is $a v t$ , where $a$ is the cross-sectional area , so the mass with which it collides is $\rho a v t$ . the impulse in each collision is proportional to $v$ , and therefore the drag force should be proportional to $\rho a v^2$ , with a constant of proportionality $c_d$ ( the drag coefficient ) of order unity . in reality , this is only true for a certain range of reynolds numbers , and even in the range of reynolds numbers for which it is true , the independent-collision picture above is not what really happens . at low reynolds numbers you get laminar flow and $c_d\propto 1/v$ , while at higher reynolds numbers there is turbulence , and you get $c_d$ roughly constant .
the matrices $\sigma_0\equiv \boldsymbol{1}_2$ , $\sigma_x$ , $\sigma_y$ and $\sigma_z$ form an orthonormal basis of your vector space w.r.t. the scalar product $$ ( x , y ) \equiv \frac{1}{2}\operatorname{tr} ( x\cdot y ) , $$ where $x$ and $y$ label any two complex $2\times 2$ matrices . the factor $1/2$ is just for convenience , you may as well normalise your pauli matrices by dividing them by $2$ . all you want to do now is to decompose an arbitrary element $$ m = \begin{pmatrix} a and b \\ c and d \end{pmatrix} $$ of your vector space into the above basis and figure out the coefficients . as usual , this is done by projecting onto that basis by means of the scalar product $$ m = ( \sigma_0 , m ) \cdot\sigma_0 + ( \sigma_x , m ) \cdot\sigma_x + ( \sigma_y , m ) \cdot\sigma_y + ( \sigma_z , m ) \cdot\sigma_z\ . $$ this has essentially been said in the above comments , particularly in the link posted by kostya .
your question has to do with addition of velocities in special relativity . for objects moving at low speeds , your intuition is correct : say the bus move at speed $v$ relative to earth , and you run at speed $u$ on the bus , then the combined speed is simply $u+v$ . but , when objects start to move fast , this is not quite the way things work . the reason is that time measurements start depending on the observer as well , so the way you measure time is just a bit different from the way it is measured on the bus , or on earth . taking this into account , your speed compared to the earth will be $\frac{u+v}{1+ uv/c^2}$ . where $c$ is the speed of light . this formula is derived from special relativity . some comments on this formula provide direct answer to your question : if both speeds are small compared with the speed of light , they approximately add up as your intuition tells you . if one of the speeds is the speed of light $c$ , you can see that adding any other speed to it does not in fact change it : the speed of light is the same in all reference frames . if you add up any two speeds below $c$ , you end up still below the speed of light . so , any material object which has a mass ( unlike light , which does not ) , moves at a speed less than $c$ . adding to it according to the correct rule makes it closer to the speed of light , but you can never exceed it , or in fact not even reach it . i would recommend wheeler and taylor 's " spacetime physics " to read about this . unlike the reputation of the subject it is actually pretty intuitive ( i learned that formula in high school ) .
the game acknowledges within the first 30 minutes of the game that they violate the law of conservation of momentum . glados says something along the lines of " now you will learn your first lecture about conservation of momentum . or rather , the lack thereof . " a feature of the game is that the portal seems to conserve the absolute value of momentum or speed , but not the vector direction $\vec p$ . going through the portal in the following picture . . . . . . would correspond to $\vec p \rightarrow -\vec p = \vec p-2\vec p$ , which would be like getting pushed by $2\vec p$ . but i am making a not here : even if it is not in the object-transportation sense of the game and although the seperation of the two points is not as arbitrary , you might wanna read up on quantum teleportation . there is still research to be done . lastly , the problem is that you can not really make statements about an object , where nobody knows how to construct it . so this is really a sci-fi question and therefore not suited for physicsse . on the other hand , i always wanted to hear some elaborations on the topology of a world with portals :
i think that you are referring to space-filling curves and how they can map a line segment to more than one dimension . for example the hilbert space filling curve can be used to map the interval $ [ 0,1 ] $ to $ [ 0,1 ] \times [ 0,1 ] $ . i am afraid while a continuous bijection is possible one-way , it is not possible to have a homeomorphism between two different euclidean spaces of different dimensions . a homeomorphism is a mapping that is continuous , bijective and has its inverse continuous . you cannot construct a homeomorphism . thus the earth cannot be 1d ! see : topological properties of real coordinate space
the short answer is : no , it is not true without other strong hypotheses . what it is true is that any completely antisymmetric wavefunction $\psi ( x_1 , \ldots , x_n ) \in l^2 ( \mathbb r^{3n} ) $ ( not necessarily solution of schroedinger equation ) can always be written as a , generally infinite , linear combination of slater determinants . indeed , if $\{\phi_k\}_{k=1,2\ldots , }$ is a hilbert basis of $l^2 ( \mathbb r^3 ) $ and $\phi \in l^2 ( \mathbb r^{3n} ) $ then : $$\phi ( x_1 , \ldots , x_n ) = \sum_{i_1 , \ldots , i_n} c_{i_1 . . . i_n} \phi_{i_1} ( x_1 ) \cdots \phi_{i_n} ( x_n ) $$ where the convergence is that in $l^2$ . then consider the orthogonal projector $a$ from $l^2 ( \mathbb r^{3n} ) $ onto the subspace of completely antisymmetric wavefunctions . if $\phi$ is generic , $\psi = a\phi$ is the generic completely antisymmetric wavefunction , so we have that any completely antisymmetric wavefunction of $n$ entries can be decomposed as : $$\psi ( x_1 , \ldots , x_n ) = \sum_{i_1 , \ldots , i_n} c_{i_1 . . . i_n} a ( \phi_{i_1} ( x_1 ) \cdots \phi_{i_n} ( x_n ) ) \: . $$ above $$a ( \phi_{i_1} ( x_1 ) \cdots \phi_{i_n} ( x_n ) ) $$ is nothing but the slater determinant of $\phi_{i_1} ( x_1 ) \: , \ldots\: , \phi_{i_n} ( x_n ) $ . the generalization to the case where $x_k$ includes spin variables is obvious .
you can not show that the higgs vev affects one particular component or the real and not imaginary part - all the components and phases are equally good . in fact , the potential only depends on $$ r = \phi^\dagger \phi = ( \phi_1 ) ^2+ ( \phi_2 ) ^2+ ( \phi_3 ) ^2+ ( \phi_4 ) ^2 $$ so only the value of $r$ is determined by minimizing the potential . its derivative is $$ 0 = \frac{\partial v}{\partial r} = \mu^2+2\lambda r $$ which implies that $$r = -\mu^2/2\lambda$$ note that $\mu^2$ should be negative – or you wrote a wrong sign in front of it – if you want to have solutions at a positive $r$ . this wrong sign is the reason why you concluded that there are no nonzero solutions : indeed , with the positive coefficients in all terms of the potential , zero is the only stationary point . the " length " of $\phi$ may be calculated as the square root of $r$ . but once you know the magnitude of $r=\phi^\dagger \phi$ , all the directions in the 4-dimensional space are equally good , so there is a whole 3-real-dimensional sphere of solutions . all of them are related by the $su ( 2 ) $ or $su ( 2 ) \times u ( 1 ) $ gauge transformations to any other solution from the 3-sphere worth of solutions . so by choosing an appropriate gauge transformation , we can make $\phi_3$ nonzero while others may be chosen to vanish .
the energy operator is obtained via the so-called correspondence principle . this means that one considers the classical expression for the total energy $$\frac{p^2}{2m}+v ( x ) $$ and replaces the momentum and position variables ( numbers in classical mechanics ) by the momentum and position operators . $p^2/2m$ is the kinetic energy ( it is just another way of writing $\frac{1}{2}mv^2$ ) and $v ( x ) $ is the potential energy . $v$ is first of all just a function of its argument . if you write $v ( x ) $ , you evaluate this function at ( the number ) $x$ . if you write $v ( \hat x ) $ , you replace the position ( number ) with the position operator , so the whole thing , $v ( \hat x ) $ is also an operator , specifically the potential energy operator . this is how you obtain the energy operator $\hat e$ ( also called hamiltonian and thus conventionally written as $\hat h$ ) from the correspondence principle . this is more of an axiom of quantum mechanics , there is no inherent motivation . the idea is that in the classical limit , the results of classical physics , specifically the classical expression for the energy , should be retrievable . now , when you want a specific realization of the position and momentum operators on the hilbert space the wave functions are going to live in , you replace $\hat x\psi ( x ) $ by $x\psi ( x ) $ , i.e. the position operator acts on a wave function by multiplying it with $x$ , and $\hat p\psi ( x ) $ with $-i\hbar\partial_x\psi ( x ) $ . by the same token , the energy operator is written as $i\hbar\partial_t$ . note , that the form of the momentum operator and the energy operator are somewhat similar they differ only by $\partial_x$ being replaced with $\partial_t$ . if you know noether 's theorem , you will be able to appreciate this fact . equating both forms of the energy operator gives the time-dependent schrödinger equation : $$i\hbar\partial_t\psi ( x ) =\hat h\psi ( x ) $$ where $$\hat h = \frac{\hat{p}^2}{2m}+v ( \hat x ) $$
a new edition of discovering the universe was just published this spring , so it is a good time to buy . my phd advisor is the managing author of the series now . he is really crazy-focused on not glossing over sticky details that make most textbook authors get sloppy and write descriptions that almost but not quite correct . i respect that . it is for his astronomy 101 class , which services the lab-science requirements for liberal arts majors and had no prereqs whatsoever . it ranges over just about everything you had be interested in . he also keeps really careful tabs on how many moons and exoplanets have been discovered and updates all the tables in the back with the very latest in research results . ( although those fields are progressing so rapidly that any printed page goes stale fast . ) lots of really majestic telescopic images . no or not much content on doing visual observations yourself . carroll and ostlie is a comprehensive introduction to the math and physics of astronomy , but will require some mathematical background knowledge through basic calculus . i mention this only because it is not clear just how limited your " limited basic scientific knowledge " is . there is no visual component to it at all , so you will not be hampered by your big city life . i used this in my second-year undergrad astrophysics course for astrophysics majors .
watts ( electrical power ) = volts $\cdot$ amps , so 25w = 12v $\cdot$ 2.1a 150amp hour is the total capacity so 150amp $\cdot$ 1hour , 1amp $\cdot$ 150hours , or 2.1amp for 72hours . that is in an ideal world of course , there are heating losses as you charge the battery , the voltage of the solar panel varies with the load and if you entirely empty a 12v lead acid battery you are likely to damage it . but basically you are looking at 10days of full sunshine
hint :- use plank 's relation , $$e=h \nu\ $$ where $h$ is the plank 's constant = $6.626 \times\ 10^{-34}$ and proceed . . .
the maxwell-boltzmann distributes $n$ particles in energy levels $e_i$ such that the entropy is maximized for a fixed total energy $e=\sum e_i n_i$ . the probability that a particle is in the energy level $e_i$ is proportional to the number of particles in the energy level $e_i$ in this particular arrangement of particles in which entropy is maximized ( the maxwell-boltzmann distribution ) , which is $n_i$ . it so happens that when we distribute the particles such that entropy is maximized , more particles populate the lower energy levels . i do not follow the above argument @ben crowell - we want to show that more particles are distributed in the lowest energy level . in the above , we write $\sum e_i = n_0 e_0 + e_r$ conclude that probability is maximized if the energies of the particles in the lowest energy state , $n_0 e_0$ , is minimized , which occurs for $n_0 = 0$ - the opposite of what was desired . i am not sure how to intuitively explain the solution to distributing the particles such that entropy is maximized . if we agree that by distributing the particles as evenly as possible in the energy levels , we will maximize the entropy , we can try : suppose we require $\sum n_i e_i \equiv \hat{e}$ and that we have plenty of particles $n$ , and that $e_i$ increases with $i$: starting from $e_0$ , put one particle in each energy level whilst $\sum e_i &lt ; \hat{e}$ . we need to distribute the remaining particles . starting from $e_0$ , again put one particle in each energy level whilst $\sum n_i e_i &lt ; \hat{e}$ . the lowest energy level , $e_0$ , now has 2 particles repeat 2 . , until all particles are distributed . we can see that the lowest energy level will be most populated , and that $n_i$ , and hence the probability that a particle is in state $e_i$ , decreases with $i$ . this algorithm will not exactly reproduce the maxwell-boltzmann distribution of particles in the energy levels , but it might help with an intuitive feel of why the lower energy levels are more probable .
hint : speed of particle remains constant as magnetic field is always perpendicular to it . also try to find a relation between the theta , d and h . you can also include r if you want . try to draw the circle and its centre by using that the fact that normal at a point of a circle passes through the centre . feel free to leave a comment below if you have any problems . use $\frac{mv^2}r = qvb$ in the image that d in diagram should be h . everything else seems fine .
i think you made a mistake . using your wavefunction and noting that $$\int^\frac{l}{2}_{-\frac{l}{2}} \sin\left ( \frac{\pi x}{l}\right ) \sin\left ( \frac{2\pi x}{l}\right ) = \frac{4l}{3\pi}$$ we see that $$1=\int^{l/2}_{-l/2}|\psi ( x ) |^2 dx = \frac{2}{l}\left ( |a_0|^2\frac{l}{2} +\frac{4l}{3\pi} \left ( a_0 + a_0^*\right ) + \frac{1}{4}\frac{l}{2}\right ) = \left ( |a_0|^2 +\frac{8}{3\pi} \left ( a_0 + a_0^*\right ) + \frac{1}{4}\right ) $$ and so we have the constraint $$\boxed{|a_0|^2 +\frac{8}{3\pi} \left ( a_0 + a_0^*\right ) + \frac{1}{4} = 1}$$ if we let $a_0 = a+ib$ , with $a , b$ real , this reduces to $$a^2+b^2 +\frac{16}{3\pi}a + \frac{1}{4} = 1 $$ if we make the choice that $a_0$ is real , that is $a_0 = a$ we find $$a_0^2 +\frac{16}{3\pi} a_0 + \frac{1}{4} = 1 ~~\implies ~~ \boxed{a_0 = -\frac{8}{3\pi}\pm\sqrt{\left ( \frac{8}{3\pi}\right ) ^2+\frac{3}{4}}}$$ which is what i am guessing you were trying to get . we see that the most general solution involves both $a$ and $b$ - it is complex . to find a general result , parameterized by $a$ we solve $$a^2+b^2 +\frac{16}{3\pi}a + \frac{1}{4} = 1~~\implies ~~ b = \pm \sqrt{\frac{3}{4}-a^2-\frac{16}{3\pi}a}$$ and so our general solution is $$\boxed{a_0 = a \pm i\sqrt{\frac{3}{4}-a^2-\frac{16}{3\pi}a}} $$ ( note we must restrict $a$ to take values for which $\sqrt{\frac{3}{4}-a^2-\frac{16}{3\pi}a}$ is real ) . so we have a range of possible solutions . i cannot see any way to make a choice of one solution over any other . added after comments below : it is likely that the question ( wherever you found it ) has a typo and the interval of interest is actually $ [ 0 , l ] $ . this makes more sense , because the first two energy eigenstates of an infinite square well potential on the interval $ [ 0 , l ] $ are ( up to a normalization ) $\sin\left ( \frac{\pi x}{l}\right ) $ and $\sin\left ( \frac{2\pi x}{l}\right ) $ . under this scenario , the problem of normalization is much simpler , because these two functions are orthogonal on this interval . this means that we have $$1 = \int^l_0 dx |\psi ( x ) |^2 = |a_0|^2 + \frac{1}{4} \implies |a_0| = \frac{\sqrt{3}}{2}$$ so we would have $a_0 = e^{i\theta}\frac{\sqrt{3}}{2}$ for arbitrary , real $\theta$ .
actually , it does oppose the change in the magnetic flux causing it . if the magnetic field from the source is increasing in some direction , the magnetic field from the induced current decreases in the same direction to oppose the increase , for example . the induced current opposing the change causing it ensures that the cause must supply energy to the system to increase the induced current , so guaranteeing conservation of energy . the magnetic field from the wire points towards $\hat y$ and is decreasing , which means the magnetic field has to increase in the same direction from the induced current . therefore the current runs clockwise around the loop . the negative sign in lenz 's law is needed when using the right hand rule to find the direction of the induced emf : the thumb points in the direction of the applied magnetic field , and the curled fingers point around the direction of the induced emf . in your example , the thumb points towards $\hat y$ and the fingers curl in a clockwise sense . the magnetic field decreasing cancles the minus sign so the direction is the same as the fingers - clockwise .
no . or at least , not without special caveats . edit after comments by mark beadles , david zaslavsky , and ron maimon , i should clarify that you cannot have a plain window that lets all light through in one direction but reflects all light coming the other direction . thus , you cannot have a window that does not absorb radiation at all and also has the prescribed one-direction property . imagine the glass separates two rooms which are completely isolated systems , except that they interact by sending electromagnetic radiation through the glass . we will say that light can only pass from left to right . further suppose the electromagnetic radiation in the rooms is a simple black-body spectrum , so each room has a well-defined temperature . imagine the temperature is higher on the left . because light only passes from left to right , heat will be transferred from the hotter room on the left to the colder room on the right with no other effect . this violates the second law of thermodynamics .
hi joe : there are many ways to concentrate solar light with flat mirrors but the easiest way would be to lay them out on a flat plane and only adjust the angles towards the focal point . one could call this a " fresnel mirror concentrator " like in here : http://www.findingnew.com/wp-content/uploads/2011/03/solar-thermal-concentrates.jpg here is an overview to give you some more inspiration : http://www.builditsolar.com/projects/concentrating/concentrating.htm regards , hans
dear qftme , i agree that your question deserves a more expansive answer . the answer , " pions " or " gluons " , depends on the accuracy with which you want to describe the strong force . historically , people did not know about quarks and gluons in the 1930s when they began to study the forces in the nuclei for the first time . in 1935 , hideki yukawa made the most important early contribution of japanese science to physics when he proposed that there may be short-range forces otherwise analogous to long-range electromagnetism whose potential is $$v ( r ) = k\frac{e^{-\mu r}}{r} $$ the fourier transform of this potential is simply $1/ ( p^2+\mu^2 ) $ which is natural - an inverted propagator of a massless particle . ( the exponential was added relatively to the coulomb potential ; and in the fourier transform , it is equivalent to the addition of $\mu^2$ in the denominator . ) the yukawa particle ( a spinless boson ) was mediating a force between particles that was only significantly nonzero for short enough distances . the description agreed with the application to protons , neutrons , and the forces among them . so the mediator of the strong force was thought to be a pion and the model worked pretty well . ( in the 1930s , people were also confusing muons and pions in the cosmic rays , using names that sound bizarre to the contemporary physicists ' ears - such as a mesotron , a hybrid of pion and muon , but that is another story . ) the pion model was viable even when the nuclear interactions were understood much more quantitatively in the 1960s . the pions are " pseudo-goldstone bosons " . they are spinless ( nearly ) massless bosons whose existence is guaranteed by the existence of a broken symmetry - in this case , it was the $su ( 3 ) $ symmetry rotating the three flavors we currently know as flavors of the $u , d , s$ light quarks . the symmetry is approximate which is why the pseudo-goldstone bosons , the pions ( and kaons ) , are not exactly massless . but they are still significantly lighter than the protons and neutrons . however , the theory with the fundamental pion fields is not renormalizable - it boils down to the lagrangian 's being highly nonlinear and complicated . it inevitably produces absurd predictions at short enough distances or high enough energies - distances that are shorter than the proton radius . a better theory was needed . finally , it was found in quantum chromodynamics that explains all protons , neutrons , and even pions and kaons ( and hundreds of others ) as bound states of quarks ( and gluons and antiquarks ) . in that theory , all the hadrons are described as complicated composite particles and all the forces ultimately boil down to the qcd lagrangian where the force is due to the gluons . so whenever you study the physics at high enough energy or resolution so that you see " inside " the protons and you see the quarks , you must obviously use gluons as the messengers . pions as messengers are only good in approximate theories in which the energies are much smaller than the proton mass . this condition also pretty much means that the velocities of the hadrons have to be much smaller than the speed of light .
yes it will . annihilation is a form of interaction which only happens between a particle and its anti-particle . you can sort of imagine it even though its not completely true i think , as destructive interference of the same particle field . its independent of charge .
if we model particle decay as a poisson process , then the probability density function for decay is an exponential distribution : $$f ( t ; \lambda ) =ce^{-\lambda t} , \text{where } t\in [ 0 , \infty ) . $$ the normalization factor $c$ can be found by integrating over the time domain and requiring that the total probability be equal to $1$ . $$1=\int_0^{\infty}f ( t ; \lambda ) \ , dt=\int_0^{\infty}ce^{-\lambda t}\ , dt=\dfrac{c}{\lambda} , $$ hence , $c=\lambda$ . the mean lifetime $\tau=\langle t\rangle$ under this distribution is $$\langle t\rangle=\int_0^{\infty}t\ , f ( t ; \lambda ) \ , dt=\int_0^{\infty}t\ , \lambda e^{-\lambda t}\ , dt=\frac{1}{\lambda} , $$ so we can replace $\lambda$ with $\frac{1}{\tau}$ in our formulas . then you can verify that the second moment of distribution is $$\langle t^2\rangle=\int_0^{\infty}t^2\ , f ( t ; \tau^{-1} ) \ , dt=2\tau^2 , $$ and so standard deviation is : $$\left ( \delta t\right ) ^2=\langle t^2\rangle-\langle t\rangle^2=2\tau^2-\tau^2=\tau^2\\ \implies\delta t=\tau . $$ as for the energy uncertainty , if the energy radiated by the decay is some value $e$ , then energy that has been released from decay at time $t$ is either zero ( if the decay has not happened yet ) or $e$ ( if the decay has already happened ) . so the spread of possible values is just $e$ . if i ask you to guess a number between 0 and 100 , you probably will not guess right but you will at least have a maximum uncertainty to within 100 . so it is not unreasonable in this case to estimate the minimum uncertainty of energy here as simply the minimum energy itself . it'll least give the correct order of magnitude .
it is not correct that runners lean forward to begin a race in order to increase friction . they lean forward because otherwise , they would experience no propulsion whatsoever because static friction is zero when the runner is completely upright . when the runner leans forward and flexes his leg muscles , he exerts a horizontal force on the track in the backward direction . the track responds by exerting an equal and opposite frictional force ( unless there is slipping ) on the runner in the forward direction that propels him forward . generally speaking , the more a runner leans forward at the start , the larger the horizontal component of the force exerted by his legs against the ground , and the larger the frictional force he will experience . as a result , his initial acceleration will be greater .
machinemessiah , if you understood hwlin 's answer please accept it . it is elegant , correct and really says all that needs saying . however , wouter raised a reasonable concern that you or others reading this might not be familiar with the concepts taken for granted in hwlin 's answer . in that case i offer what may be an intelligible answer , which also addresses the breakdown of the cosmological assumption made by hwlin ( and everyone else ) . apologies if it is too wordy . : ) a key insight of relativity is that any discussion of physics must be rooted in operational procedures for performing measurements . all invisible concepts in physics - space , time , energy , fields , etc . - are shorthands , or powerful organising principles , which govern relationships between measurements which are made by some procedure , however informal such procedures may be . ( have you ever seen space ? i have not . but i have noticed that i routinely avoid banging my head against the wall , though , when the circumstance warrants it , such contact can be easily arranged . these and many other routine observations support the notion of space as a powerful organising principle . usually the observations are so routine that we do not even think of them as observations , but observations they are . ) so to your question : is time speeding up ? how would you measure such a speed-up ? you are correct that if everything else is speeding up in the same way then there is no way to measure a speed-up . because of this the idea of a global speedup of time is operationally meaningless . it could only make sense to some god-like observer standing outside of our ( portion of ) spacetime looking in . this is essentially what hwlin told you . you can always come up with a notion of time in cosmology that just " trucks along , " without changing pace , and this cosmic clock serves as a good definition for time : it is simple , robust , ticks all the necessary boxes , makes the equations very nice and is hard to disagree with . another key insight of relativity is that every reference frame is as good as any other . so for intance , every observer has the right to construct their own clock and call the measurement " time . " though there is a complete democracy of observers , each with their own definition of time , the laws of relativity tell how any two definitions of time , measured by any two well constructed clocks are related . ( a bad clock would be a grandfather clock leaning over sideways , for example . no one is required to pay attention to a bad clock . ) so now we can ask how clocks in the local cluster of galaxies are related to clocks in the distant universe . this is where it gets interesting . cosmologists tend to make an approximation that the whole universe is homogenous . that is , all of the matter is spread evenly throughout the universe . this approximation dramatically simplifies the equations ! if the universe is not homogenous then you need to specify a bunch of numbers at every point of space and keep track of how they are all changing . if instead you pretend that every point in space is equivalent then you only need to keep track of one set of numbers ! hwlin 's reply was in the context of this approximation . it is correct insofar as the homogeneity assumption is correct . of course the universe is not homogenous , but on cosmological scales it comes pretty close . hwlin 's answer , addressed at these scales , is completely appropriate . unfortunately you mentioned galaxy clusters which , by definition , violate the homogeneity assumption . so let 's see what we can say about the local neighbourhood , where the cosmological assumption breaks down . suppose that the cluster is neighboured by a void ( perhaps the local void ? i do not know my extra-galactic geography ) . in this case clocks inside the cluster run slower than clocks outside the cluster . how to measure this ? we can arrange that somebody inside the cluster , " lower down in the gravity well , " sends a pulse of light once every second - by their clock - to an observer situated outside the galaxy cluster . in this case there is an observable difference : the observer outside the cluster receives the pulses at intervals greater than one second . he says the the clock of the guy inside is running slower . this is the well known gravitational redshift , and is well established by now . it was observed at two different heights on the earth in the pound-rebka experiment . an observer inside the cluster could attempt to observe the faint light emitted by extra-galactic hydrogen , which has a characteristic frequency determined by quantum mechanics . what we would see is a subtle blue-shift of radiation coming from the void . this measurement is similar in principle to the measurement of the lyman-alpha forest . it is also similar to the integrated sachs-wolfe effect ( thanks chris white ) , which measures the photons from the cosmic microwave background rather than nearby voids . i am not sure if present day observations are able to see it . if the cluster lost mass for some reason and the mass did not go into the void , then you could in principle measure a slight decrease of the blueshift . if the mass went into the void it becomes a very complicated thing that depends on the distribution of matter where you are looking . in any case the expansion of the universe would not cause this as the local cluster is held together by the gravity of the galaxies inside it , and the cosmic expansion really only separates unbound systems .
if i try to measure the field at one point in spacetime , i should get a real value which should be an eigenvalue of the quantum field , right ? i guess the eigenvectors of the quantum field also live in fock space ? yes , that is basically correct . if the value of the field at a point is observable , the eigenvalues of the operator representing it are the values the field can attain at that point . and the eigenvectors live in the hilbert space of states , which you can think of ( at least conceptually ) as $l^2 ( \{\mbox{initial boundary conditions}\} ) $ . this hilbert space is a fock space in free field theories . there is a couple of subtleties worth mentioning : the value of the field at a point might not be a physical observable . in electrodynamics , for example , you can not actually measure the value $a_\mu ( x ) $ of a component of the connection 1-form ; instead , you can measure gauge invariant quantities like the curvature $f_a ( x ) $ and the holonomy $hol_l ( a ) $ along a loop $l$ . likewise , in nonlinear sigma models , where the classical fields are maps $\phi : \sigma \to x$ to some curved manifold , you can not measure the value $\phi ( x ) $ . eigenvalues are complex numbers , not points on a manifold . but you do get a real observable $\mathcal{o}_f ( x ) $ for every function $f : x \to \mathbb{r}$ ; measure the value of $f ( \phi ( x ) ) $ . it is also not strictly correct to say that quantum fields are operator-valued functions on spacetime . the physical problem is that if you measure the value of the field at one point , you will disturb the field near that point , affecting the values at other nearby points . the closer you look to the place where you made the measurement , the bigger the disturbance ; even in free scalar field theory , the 2-point correlation function $\langle \phi ( x ) \phi ( y ) \rangle$ blows up as $x \to y$ . this tells you that the fields are not quite functions , because you can not multiply the ' value at a point ' observables when they live at the exact point . the mathematically correct thing to do is to think of the field ( and more generally local observables constructed from fields ) as an operator-valued distribution . distributions are mild generalization of functions ; they are objects which do not have values at a point , but which do have average values in an arbitrarily small ( but finite ) region . basically , for any test function $f$ on your spacetime , you get an operator $\phi ( f ) $ which you can think of as measuring the value "$\int f ( x ) \phi ( x ) dx$" of $\phi$ sampled by a probe with resolution $f$ . distributions can only be multiplied when their singularities do not coincide ; they exhibit the same obnoxious behavior that quantum field operators do . probably you do not have to worry about this too much . for one thing , even if you can not ( strictly speaking ) define an operator $\phi ( x ) $ , you can still safely talk about the correlation function $\langle \phi ( x ) \phi ( y ) \rangle$ . ( it is the kernel function of the multilinear map $ ( f , g ) \mapsto \langle \phi ( f ) \phi ( g ) \rangle$ . ) physicists do not spend a lot of time worrying about solving the eigenvalue problem for the field operators . usually the spectrum is all of $\mathbb{r}$ , and finding the eigenvectors is not worth the trouble . there is one important exception though : in the standard model , it is pretty important that the vacuum vector is an eigenvector of the higgs field operators , with non-zero eigenvalue .
no , they will not appear the same . humans have three color receptors so any possible color for us is just three numbers in rgb space . however , electromagnetic spectrum is continuous and there is an infinite number of spectra that would produce the same rgb stimulus . that is why you perceive the this page as white although it is in fact a combination of r , g , b tuned for a human eye . a creature with other set of photoreceptors would not see this page as white . actually , white is also subjective , see how many settings for white balance your digital camera has , but suppose that we set it to ' daylight ' and consider the continuous spectrum of sun as white . another interesting point : magenta hue cannot be represented by any single wavelength , check out this famous horseshoe diagram : so magenta is even more subjective than green or red . generally , most some animals do not perceive colors at all . there are some who have only two receptors and others who have four . mantis shrimp has 16 different photoreceptors !
the big bang was everywhere , because distance did not exist before it , so from one perspective , everywhere may be the centre ( especially as some theorists think , the universe does not have an edge ) the real issue is that the question should not matter , as we can only gain information from distances within our visible radius and once we get to that limit , what we see gets closer to the big bang so it all looks closer to the centre . tricky eh
the vacuum is polarizable . the polarization can be with respect to electric charge or color charge . in the presence of an electric field , virtual electron-positron pairs briefly exist ( created from virtual photons of sufficient energy ) . the virtual pairs act as dipoles and orient with respect to the field . for example , near a proton , the virtual electron of such a pair will orient nearer the proton and the virtual positron further away . the first experimental confirmation of vacuum polarization was through spectroscopy of the hydrogen atom . for the hydrogen atom , dirac’s relativistic quantum mechanics predicted that the $2s_{\frac12}$ and $2p_{\frac12}$ energy levels should be equal . however , experimentally the difference corresponds to 1058mhz , first measured by lamb in 1947 . this energy level difference is the " lamb shift " . the electron and proton of the hydrogen atom do not experience the classical coulomb potential . the polarization screens points away from the proton from the full change that it would otherwise have . because the s level has higher electron probability density at/near the proton , it experiences a less-screened proton charge . vacuum polarization contributes -27mhz to the lamb shift . as far as i know , electric polarization of the vacuum has not been experimentally measured across a range of applied frequencies point by point . however , through quantum electrodynamics , calculations of the lamb shift ( including the vacuum polarization contribution ) to increasingly high orders of virtual particle effects have been progressively made over the years , and compared to increasingly accurate experiments . current theoretical lamb shift 1057.833 ( 4 ) mhz current experimental lamb shift 1057.845 ( 3 ) mhz though vacuum polarization is a relatively small contribution to the lamb shift of ordinary hydrogen , it is the dominant contribution to the lamb shift in muonic hydrogen . study of the lamb shift of muonic hydrogen is an active area of research . other experimental confirmations of vacuum polarization involve the anomalous electron magnetic moment ( how much g deviates the value "2" predicted by dirac theory ) and josephson junctions . references : http://isites.harvard.edu/fs/docs/icb.topic473482.files/16-vacuumpol.pdf http://www.pha.jhu.edu/~rt19/hydro/node8.html http://courses.washington.edu/phys432/lamb_shift/lamb_shift.pdf http://seminar.physik.uni-mainz.de/uploadz/pachucki.pdf http://arxiv.org/pdf/1208.2637v2.pdf heisenbeg and euler ( 1936 ) http://arxiv.org/pdf/physics/0605038.pdf euwema and wheeler ( 1956 ) http://journals.aps.org/pr/pdf/10.1103/physrev.103.803
your question is addressed in the following paper : the twin paradox in compact spaces authors : john d . barrow , janna levin phys . rev . a 63 no . 4 , ( 2001 ) 044104 arxiv:gr-qc/0101014 abstract : twins travelling at constant relative velocity will each see the other 's time dilate leading to the apparent paradox that each twin believes the other ages more slowly . in a finite space , the twins can both be on inertial , periodic orbits so that they have the opportunity to compare their ages when their paths cross . as we show , they will agree on their respective ages and avoid the paradox . the resolution relies on the selection of a preferred frame singled out by the topology of the space .
the expression $ ( \hbar g/c^3 ) ^{1/2}$ is the unique product of powers of $\hbar , g , c$ , three most universal dimensionful constants , that has the unit of length . because the constants $\hbar , g , c$ describe the fundamental processes of quantum mechanics , gravity , and special relativity , respectively , the length scale obtained in this way expresses the typical length scale of processes that depend on relativistic quantum gravity . the formula and the value were already known to max planck more than 100 years ago , that is why they are called planck units . unless there are very large or strangely warped extra dimensions in our spacetime , the planck length is the minimum length scale that may be assigned the usual physical and geometric interpretation . ( and even if there are subtleties coming from large or warped extra dimensions , the minimum length scale that makes sense – which could be different from $10^{-35}$ meters , however – may still be called a higher-dimensional planck length and is calculated by analogous formulae which must , however , use the relevant newton 's constant that applies to a higher-dimensional world . ) the planck length 's special role may be expressed by many related definitions , for example : the planck length is the radius of the smallest black hole that ( marginally ) obeys the laws of general relativity . note that if the black hole radius is $r= ( \hbar g/c^3 ) ^{1/2}$ , the black hole mass is obtained from $r=2gm/c^2$ i.e. $m=c^2/g\cdot ( \hbar g/c^3 ) ^{1/2} = ( \hbar c/g ) ^{1/2}$ which is the same thing as the compton wavelength $\lambda = h/mc = hg/c^3 ( \hbar g/c^3 ) ^{-1/2}$ of the same object , up to numerical factors such as $2$ and $\pi$ . the time it takes for such a black hole to evaporate by the hawking radiation is also equal to the planck time i.e. planck length divided by the speed of light . smaller ( lighter ) black holes do not behave as black holes at all ; they are elementary particles ( and the lifetime shorter than the planck time is a sign that you can not trust general relativity for such supertiny objects ) . larger black holes than the planck length increasingly behave as long-lived black holes that we know from astrophysics . the planck length is the distance at which the quantum uncertainty of the distance becomes of order 100 percent , up to a coefficient of order one . this may be calculated by various approximate calculations rooted in quantum field theory – expectation values of $ ( \delta x ) ^2$ coming from quantum fluctuations of the metric tensor ; higher-derivative corrections to the einstein-hilbert action ; nonlocal phenomena , and so on . the unusual corrections to the geometry , including nonlocal phenomena , become so strong at distances that are formally shorter than the planck length that it does not make sense to consider any shorter distances . the usual rules of geometry would break down over there . the planck length or so is also the shortest distance scale that can be probed by accelerators , even in principle . if one were increasing the energy of protons at the lhc and picked a collider of the radius comparable to the universe , the wavelength of the protons would be getting shorter inversely proportionally to the protons ' energy . however , once the protons ' center-of-mass energy reaches the planck scale , one starts to produce the " minimal black holes " mentioned above . a subsequent increase of the energy will end up with larger black holes that have a worse resolution , not better . so the planck length is the minimum distance one may probe . it is important to mention that we are talking about the internal architecture of particles and objects . many other quantities that have units of length may be much shorter than the planck length . for example , the photon 's wavelength may obviously be arbitrarily short : any photon may always be boosted , as special relativity guarantees , so that its wavelength gets even shorter . lots of things ( insights from thousands of papers by some of the world 's best physicists ) are known about the planck scale physics , especially some qualitative features of it , regardless of the experimental inaccessibility of that realm .
first general relativity is typically taught at a 4th year undergraduate level or sometimes even a graduate level , obviously this presumes a good undergraduate training in mathematics and physics . personally , i am more of the opinion that one should go and learn other physics before tackling general relativity . a solid background in classical mechanics with exposure to hamiltonians , lagrangians , and action principles at least . a course in electromagnetism ( at the level of griffiths ) i think is also a good thing to have . mathematically , i think the pre-reqs are a bit higher and since the question asks about mathematical detail , i will focus on that . i learnt relativity from a very differential geometry centric viewpoint ( i was taught by a mathematician ) and i found that my understanding of differential geometry was very helpful for understanding the physics . i have never been a fan of hartle 's book which i think is greatly lacking on the mathematical details but is good for physical intuition . however having worked in relativity for some time now i think it is better to teach from a more mathematical point of view so you can easily pick up the higher level concepts . additionally , i think you really need to understand what is going on mathematically to understand why we must construct things the way we do . i am going to have to disagree with nibot here and say that you will need more then just linear algebra and college calculus . calculus you must have at least seen up to vector calculus and be familiar with it . linear algebra is something you should have a very good understanding considering that we are dealing with vectors . a good course in more abstract algebra dealing with vector spaces , inner products/orthogonality , and that sort of thing is a must . to my knowledge this is normally taught in a second year linear algebra course and is typically kept out of first year courses . obviously a course in differential equations is required and probably a course in partial differential equations is required as well . i do not think a course in analysis is required , however since the question is more about the mathematical aspect , i would say having a course in analysis up to topological spaces is a huge plus . that way if you are curious about the more mathematical nature of manifolds , you could pick up a book like lee and be off to the races . if you want to study anything at a level higher , say wald , then a course in analysis including topological spaces is a must . you could get away with it but i think it is better to have at the end of the day . i would also say a good course in classical differential geometry ( 2 and 3 dimensional things ) is a good pre-req to build a geometrical idea of what is going on , albeit the methods used in those types of courses do not generalise . of course , there is also the whole bit about mathematical maturity . it is a funny thing that is impossible to quantify . i , despite having the right mathematical background , did not understand immediately the whole idea of introducing a tangent space on each point of a manifold and how $\{\partial_{i}\}$ form a basis for this vector space . it took me a bit longer to figure this out . you can always skip all this and get away with just the physicists classical index gymnastics ( tensors are things that transform this certain way ) however i think if you want to be a serious student of relativity you had learn the more mathematical point of view . edit : on the suggestion of jdm , a course in classical field theory is good as well . there is a nice little dover book appropriately titled classical field theory that gets to general relativity right at the end . however i never took a course and i do not think many universities offer it anyway unfortunately . also a good introduction if you want to go learn quantum field theory .
there is a very precise reason why dark planets made of ' ordinary matter ' ( baryons - particles made up of 3 quarks ) cannot be the dark matter . it turns out that the amount of baryons can be measured in two different ways in cosmology : - by measuring present-day abundances of some light elements ( esp deuterium ) which are very sensitive to the baryon amount , and - by measuring the distribution of the hot and cold spots in the cosmic microwave background ( cmb ) , radiation left over from the early universe that we observe today these two methods agree spectacularly , and both indicate that baryons are 5% of the total stuff ( energy/matter ) in the universe . meanwhile , various measures of gravitational clustering ( gravitational lensing , rotation of stars around galaxies , etc etc ) all indicate that total matter comprises 25% of the total . ( the remaining 75% is in the infamous dark energy which is irrelevant for this particular question ) . since 5% is much less than 25% , and since the errors on both of these measurements are rather small , we infer that most of the matter , about 4/5 ths ( that is , 20% out of 25% ) is ' dark ' and not made up of baryons .
write things out more carefully . under the decomposition $\mathfrak{su} ( n ) \rightarrow \mathfrak{su} ( 2 ) $ , you decompose the $\mathfrak{su} ( n ) $ vector representation into $$ n \to \tfrac{1}{2} \oplus 0^{\oplus ( n-2 ) }\ , . $$ then , $$ n \otimes \bar{n} \rightarrow ( \tfrac{1}{2} \oplus 0^{\oplus ( n-2 ) } ) ^{\otimes 2} = 1 \oplus \tfrac{1}{2}^{\oplus ( 2n-4 ) } \oplus 0^{\oplus ( ( n-2 ) ^2 + 1 ) }\ , . $$ that is what you want . do not forget that one of those singlets is the singlet in ${\rm adj} \oplus {\rm triv}$ , since under the reduction $\mathfrak{su} ( n ) \rightarrow \mathfrak{su} ( 2 ) $ , ${\rm triv}$ must reduce to the spin 0 singlet representation .
in your comment you ask : that question only addresses gravitational force . is it the same for all kind of forces ? generally speaking a force transmitted by massless particles like the photon and graviton obeys an inverse square law while a force transmitted by massive particles falls off exponentially with distance . this means that only the forces transmitted by massless particles are long range and therefore that any long range force propagates at the speed of light . so for any macroscopic objects the interaction between them is going to travel at the speed of light . trying to assign a speed to the weak and strong nuclear forces is a bit complicated . at longish range , i.e. within a few proton diameters , the strong force is transmitted by massive mesons and therefor propagates at less then the speed of light . however the underlying force is transmitted by massless gluons . this does not give rise to a long range inverse square law because of confinement . the weak force is transmitted by massive w and z bosons at low energies , but above the electroweak transition these become massless and in principle the electroweak force is long range .
the $z$-boson mass is easily fully measurable since for e.g. the processes $z\to e^+e^-$ and $z\to\mu^+\mu^-$ the complete four-vectors $p_\mu$ of the two final state particles can be measured very precisely , and from this one can reconstruct the $z$ four vector and therefore its mass : $$m_z^2= ( p_1^\mu+p_2^\mu ) ^2$$ for the $w$ mass it is indeed a little more tricky : as you say , the neutrino cannot be directly measured , since it does not interact with the detector and only shows up as missing transverse energy ( "met" ) . since at hadron colliders , we do not know the what the momentum balance in the longitudinal direction is ( beam remnants disappearing along the beam line , lorentz boost due to unequal parton momentum fractions , etc . ) , we can only measure missing energy in the transverse plane , therefore for the neutrino we can only measure the transverse momentum $p_t^\nu$ the canonical method to determine the $w$-mass precisely at hadron colliders is to measure the so-called transverse mass , $m_t$ . the spectrum of the transverse mass has a very visible feature often called the jacobian peak , at the point $m_w$ . $m_t$ is defined as $$m_t = \sqrt{2p_{t}^{l}p_{t}^{\nu} ( 1-\cos\delta\phi_{l\nu} ) }$$ , where the $\delta\phi_{l\nu}$ is the angle in the transverse plane between the lepton and the direction of the missing transverse energy . the resulting histogram from the recorded data is fitted with a line-shape that has $m_w$ as a floating parameter . ( from http://arxiv.org/pdf/1203.0275v1.pdf )
the question is not what is possible , but what is useful to obtain analytic results . in multi-loop integrals one is often interested in analytic results , because it can be very hard to confirm numerical results to a level where you trust them completely . now the two-loop integrals usually involve propagators of the type $$ f ( q_1 , q_2 ) = \frac{i}{ ( p - q_1 - q_2 ) ^2 - m^2} = \frac{i}{p^2 + q_1^2 + q_2^2 - p\cdot q_1 - p \cdot q_2 - q_1 \cdot q_2 - m^2} $$ if the scalar products with the external momentum were not present , one could indeed go to 8d spherical coordinates ( after an appropriate wick rotation to euclidean space ) . but as soon as you care about external momenta , things get more complicated . there actually are books on techniques of dealing with two-loop integrals . edit to expand on why we do not want to do numerics : first of all , numerics in multiloop calculations can be done and there are a great deal of people doing this . this is a very complicated research subject of its own . the main issue is not solving the integral numerically , but making sure you trust the answer . in particle physics calculations we often have cancellations between different contributions . the terms that cancel can be much larger than the remaining contributions , see e.g. the gim mechanism . similar things can happen on a more basic level , just having two different bits of the integral canceling and leaving a smaller third bit over . making sure that everything that is supposed to cancel indeed does so to the required precision is hard . you have to apply your method tree times , to calculate both contributions and the final result . if the two contributions indeed cancel you can trust your result a bit . but then you have to perform a bajillion other tests on your numerics before you can be sure that 18.3922787 is really the number you were looking for . therefore , a lot of effort is spent on solving these integrals analytically , where cancellations are exact and you can understand in detail , where the result comes from . for these analytic calculations , 8d spherical coordinates would not do you any good , though .
the reason cosmology is taken to be spherically symmetric is because the observed macroscopic universe actually is spherically symmetric on large scales . the evidence for this is the blackbody radiation spectrum , which is the same in all direction to 1 part in 100,000 . the spherical symmetry cannot be an accident , it requires an explanation , and this explanation is provided by inflation . if you assume that the universe had an approximately desitter phase leading into the frw phase , during the desitter phase , it is driven to spherical symmetry , because the only stable maximum entropy configuration of a positive cosmological constant universe is the desitter vaccuum , which is spherically symmetric . a smooth end to inflation preserves the spherical symmetry , and sets the stage for the symmetric expansion that follows . the spherical symmetry is an extremely good approximation , in agreement with experimental data , not an ad-hoc simplifying assumption that makes the solution of the equations nice ( although it does that too ) . the universe , it turns out , is a spherical cow !
with a generator of some sort extracting energy from the system , the system will stop in all three cases . by the principle of conservation of energy , you know that the total energy neither increases or decreases through any process . if the generator extracts energy from the system , the man will slow down and eventually stop . intuitively , this is because of resistance in the generator . even in an ideal world without friction , turning a magnet within a coil of wire ( in order to generate an emf in the coil ) requires a constant force due to the retarding electromagnetic forces on the generator . ( consider science museum demonstrations in which you turn a wheel to light a light bulb . the wheel can be really tough to turn , and that is primarily because of electromagnetic force , not just friction . ) without a turbine , the question is marginally more interesting . in any environment with friction , the man will stop - again , since friction extracts energy from the system , it must slow down and eventually come to a halt . only in the last case - the frictionless environment without a turbine - will the man perpetually remain in motion . of course , this is a physically implausible situation , since any mechanical system will have at least some amount of friction .
this web page has a nice discussion on it : http://archive.ncsa.illinois.edu/cyberia/numrel/einsteintest.html basically the orbit is eccentricity would precess around the sun . classical stellar mechanics ( or newtonian gravity ) could not account for all of that . it basically had to do with ( and forgive my crude wording ) the sun dragging the fabric of space-time around with it . or as the web page says : mercury 's changing orbit in a second test , the theory explained slight alterations in mercury 's orbit around the sun . daisy petal effect of precession since almost two centuries earlier astronomers had been aware of a small flaw in mercury 's orbit around the sun , as predicted by newton 's laws . as the closest planet to the sun , mercury orbits a region in the solar system where spacetime is disturbed by t he sun 's mass . mercury 's elliptical path around the sun shifts slightly with each orbit such that its closest point to the sun ( or " perihelion" ) shifts forward with each pass . newton 's theory had predicted an advance only half as large as the one actually observed . einstein 's predictions exactly matched the observation . for more detail that goes beyond a simple layman answer , you can check this page out and even download an app that let 's you play with the phenomenon : http://www.fourmilab.ch/gravitation/orbits/ and of course , the ever handy wikipedia has this covered as well : http://en.wikipedia.org/wiki/tests_of_general_relativity#perihelion_precession_of_mercury although , truth be told , i think i said it better ( i.e. . more elegantly ) than the wiki page does . but then i may be biased .
your question all but includes the right search term for an answer from wikipedia , " conservative forces " , which gets you to http://en.wikipedia.org/wiki/conservative_forces . there is even what you ask for , a proof . there is also another link to http://en.wikipedia.org/wiki/conservative_vector_field , which gives some quite good visualizations that will probably help . loosely , there must not be any vortices in the force field for there to be a scalar potential energy that generates the force vector field as $\nabla\ ! \cdot\ ! \phi ( x ) $ .
1 ) the first thing i notice is that you have stated that the velocity at the end of the ramp is $2\textrm{ m/s}$ . remember that the can is accelerating as it rolls down the ramp , so the equation $v=\textrm{d}s/\textrm{d}t$ is not applicable here for finding the instantaneous velocity at the bottom . the can does indeed average $2\textrm{ m/s}$ during its trip , but this is not the final velocity of the can . use this new corrected value to calculate angular frequency . 2 ) i find this problem simpler to solve using energy analysis . take the can 's initial potential energy : $$ e_\textrm{pot} = m g h = 3.234\textrm{ j}\quad . $$ we also know that the final kinetic energy of the can must equal this due to the conservation of energy , but the final energy of the can must be broken into translational kinetic energy ( due to the can 's movement ) and rotational kinetic energy ( due to the rotation ) . ( this is why your solution above was giving incorrect answers as it did not take translational kinetic energy into account . ) thus , we also know that : $$ e_\textrm{pot} ( t=0 ) = e_\textrm{kin , trans} ( t=1.5\textrm{ s} ) + e_\textrm{kin , rot} ( t=1.5\textrm{ s} ) \quad , $$ which , for our case , is $$ 3.234\textrm{ j} = \frac{1}{2} mv^2 + \frac{1}{2}iω^2\quad . $$ plugging in known values to this equation with the correct value for angular velocity $\vec \omega = \vec r \times \vec v$ gives the accepted answer : $$ i = 0.000187\textrm{ kgm}^2\quad . $$ as for part b , the height of the can is irrelevant because as long as we know the mass and radius of the can , we can solve the problem . the ‘extra mass’ resulting from lengthening the can would be centered about the can 's original center of mass , and as such the moment of inertia would not be affected for this problem .
there are a bunch of questions here . let me try to take them in order : is it possible that our universe has the feature that if you travel far enough you return to where you started ? yes . the standard big-bang cosmological model is based on the idea that the universe is homogeneous and isotropic . one sort of homogeneous spacetime has the geometry of a 3-sphere ( like a regular sphere , but with one more dimension ) . in these cosmological models , if you travel far enough you get back to where you started . however , the best available data seem to indicate that the universe is very nearly spatially flat . this means that , if we do live in a 3-sphere universe , the radius of the sphere is very large , and the distance you had have to travel is much larger than the size of the observable universe . even if that were not true , the fact that the universe is expanding would make it hard or impossible to circumnavigate the universe in practice : no matter how fast you went ( short of the speed of light ) , you might never make it all the way around . nonetheless , 3-sphere universes , with the geometrical property you describe , are definitely viable cosmological models . does this give rise to a symmetry by noether 's theorem ? not really . noether 's theorem is generally applied to continuous symmetries ( i.e. . , ones that can be applied infinitesimally ) , not discrete symmetries like this . the fact that space is homogeneous gives rise to a symmetry , namely momentum conservation , whether or not space has the 3-sphere geometry , but the symmetry you are talking about here does not give rise to anything extra . would small curled up dimensions have the same sort of symmetry ? i will leave this for someone else , i think . not my thing . is it known exactly what the geomtrical shape of the universe is ? no , and do not let anyone tell you otherwise ! sometimes , especially in pop-science writing , people imply that we know a lot more about the global properties of the universe than we do . we often assume things like homogeneity to make our lives simpler , but in fact we have precisely no idea what things are like outside of our horizon volume . how to describe the " size " of a dimension ? if the universe 's geometry has enough symmetries , it makes sense to define an overall time coordinate everywhere . then it makes sense to imagine a " slice " through spacetime that represents the universe at an instant of time . if some of those slices have the geometrical property you are talking about , that traveling a distance r in a certain direction gets you back to your starting point , then it makes sense to call r the " size " of the corresponding dimension . if you can travel forever , then we say the size in that dimension is infinite . is it possible to describe to a layman the shape of the universe without resorting to inept analogies ? all analogies are imperfect . i think the best you can do is use a bunch of them and try to convey the limitations of each .
the simple answer is that no , time is not expanding or contracting . the complicated answer is that when we are describing the universe we start with the assumption that time is not expanding or contracting . that is , we choose our coordinate system to make the time dimension non-changing . you do not say whether you are at school or college or whatever , but i am guessing you have heard of pythagoras ' theorem for calculating the distance , $s$ , between two points $ ( 0 , 0 , 0 ) $ and $ ( x , y , z ) $: $$ s^2 = x^2 + y^2 + z^2 $$ well in special relativity we have to include time in the equation to get a spacetime distance : $$ ds^2 = -dt^2 + dx^2 + dy^2 + dz^2 $$ and in general relativity the equation becomes even more complicated because we have to multiply the $dt^2$ , $dx^2$ , etc by factors determined by a quantity called the metric , and usually denoted by $g$: $$ ds^2 = g_{00}dt^2 + g_{11}dx^2 + g_{22}dy^2 + . . . etc $$ where the $ . . . etc$ can include cross terms like $g_{01}dtdx$ , so it can all get very hairy . to be able to do the calculations we normally look for ways to simplify the expression , and in the particular case of the expanding universe we assume that the equation has the form : $$ ds^2 = -dt^2 + a ( t ) ^2 d\sigma^2 $$ where the $d\sigma$ includes all the spatial terms . the function $a ( t ) $ is a scale factor i.e. it scales up or down the contribution from the $dx$ , $dy$ and $dz$ , and it is a function of time so the scale factor changes with time . and this is where we get the expanding universe . it is because when you solve the einstein equations for a homogenous isotropic universe you can calculate $a ( t ) $ and you find it increases with time , and that is what we mean by the expansion . however the $dt$ term is not scaled , so time is not expanding ( or contracting ) .
the thing you throw in the air is also traveling at the same speed you are , in the same direction . when you throw it up , it does not matter that the earth below is moving backwards at speed , nor that the moon is moving past even more quickly , nor that the earth itself is spinning and moving relative to the sun . the ball has a speed and direction and currently that matches your speed and direction . when you throw the ball up , you have added force in a new direction , which alters its speed and direction , but only with respect to your speed and direction . in other words , to you the ball appear to go up and down , but to the earth it is falling like a projectile - forward up and down . since you are traveling forward at the same speed as the projectile , it appears to you that it only goes up , then down , even though during that time you both moved forward . i am not actually going to break out the math , but here 's the short version : you and the object are moving at a speed and in a direction that we will call vector p and b , respectively . currently your two vectors match . relative to some other reference frame you are both moving , but relative to you , since your vectors match , the object appears to be motionless . you apply a force on vector b , which alters its trajectory . now this force results in additional speed and direction described by vector t . the object , therefore , is now moving according to the vector b + t . however , again , since b = p , it appears to you that the object is only moving according to vector t . gravity is applying a force to the object , which will eventually reverse t in the down direction , unless the ball is acted upon by another force , such as your hand catching the ball again . so regardless of what vector you apply to it , it will be in addition to the vector you are already traveling at , and therefore it will appear to you as though it is only traveling along its new vector .
1 ) you surely feel the pressure when you accelerate . whether you attribute it to fictitious forces or other forces depends on your choice of the " reference frame " ( vantage point ) . from the viewpoint of your body 's reference frame , which is not an inertial frame , there exist fictitious forces ( inertia and/or centrifugal and/or coriolis ' force ) that are pushing your body towards the seat . in an inertial reference frame , such as the vantage point of people who stand on the sidewalk and watch you , the pressure is exerted by the seat because it is accelerating i.e. pushing ( you are pushing on the seat as well , by the third newton 's law ) and there are no additional fictitious forces . both of these descriptions are ok but the description from the inertial systems ( e . g . the sidewalk system ) is described by simpler , more universal equations . without a loss of generality , we may describe all of physics from these frames and these frames never force us ( and never allow us ) to add any fictitious forces . the frame of your ( accelerating ) body may be considered " unnatural " and therefore all the forces that appear in that frame are artifacts of the frame 's being unnatural , and therefore they are called " fictitious " . they may be avoided . 2 ) centrifugal forces are the textbook examples of fictitious forces ; they have to be added if you describe the reality from the viewpoint of rotating systems . they are avoided if you use non-rotating frames . however , the tides have nothing to do with centrifugal forces . the tidal forces appear because the the side of the earth that is further from the moon is less strongly attracted to the moon than the side that is closer to the moon . in other words , the tidal forces totally depend on the non-uniformity of the gravitational field around the moon – the force decreases with the distance . you could create the same attractive force as the moon exerts by using a heavier body that is further than the moon . the attractive i.e. " centripetal " force would be the same but the tidal forces would be weaker ! 3 ) in an inertial system – connected with the earth 's surface , for example – the force acting on you is $mg$ downwards from the earth 's attraction plus $ma$ from the extra upwards accelerating elevator . the part $ma$ has a clear new source , object that causes it , namely the elevator . however , in a freely falling frame , for example , the gravitational downward $mg$ force cancels against the fictitious inertial force $mg$ upwards . however , the material of the elevator is now accelerating by the acceleration $g+a$ upwards so the total force is $m ( g+a ) $ again . as you can see , whether there are fictitious forces depends on the reference frame . what i feel is your trouble is that you are not used to describe processes from the viewpoint of inertial reference frames . take a spinning carousel . there is a centripetal force acting on the children and this force , $f=mr\omega^2$ , is the reason why the children are not moving along straight paths with the uniform velocity ( as newton 's first law would suggest ) . instead , they are deviating from the uniform straight motion and move along circles . the centripetal , inwards directed force $mr\omega^2$ from the pressure from the seats is the reason . ( for planets , the centripetal force is the gravitational one . ) there are no fictitious forces , in particular no centrifugal force , in the description using the inertial system ( from the viewpoint of the sidewalk ) . however , from your rotating viewpoint , there is a centrifugal force $mr\omega^2$ acting outwards that is always there because the frame is rotating . this force is cancelled against the pressure from the seat , a centripetal force $mr\omega^2$ , and the result is zero which implies that in the rotating frame , the coordinates stay constant in this case , especially the distance $r$ from the axis of the spinning carousel . both frames are possible : one of them forces you to add fictitious forces , the other one ( inertial frame ) does not contain any such forces .
it is one of the weyl curvature scalars or coefficients , see the first page of http://arxiv.org/abs/1105.0781 - they are some " doubly light-like " , see the formulae , components of the weyl tensor , and because the ricci scalar is specifically removed from the weyl tensor , you may be sure that $\psi_4$ is not related to $r$ . but both $r$ and $\psi_n$ are linear combinations of components of the riemann tensor .
as i am not allowed to comment for lack of reputation and cannot find a way to message i would like to point you to a reasonably recent source on the kerr metric . i am by no means an expert but from what i have read and from what i understand the " lines " of space time do twist and become unstable at the cauchy horizon . from what i gather the rotation is not necessarily from the singularity but from the entire " mass " beyond the cauchy horizon thus allowing the black hole to rotate and have an angular velocity that is not infinite . this however could be argued as infinite could very well be defined as the seed of light that several super massive black hole have been measured to be spinning at or near . the following are a few resources that i have found to be helpful in my own limited understanding of black holes and space-time . http://www.eftaylor.com/pub/spinnew.pdf a wonderful project that illustrates and explains much theory as it goes . http://www.gothosenterprises.com/black_holes/rotating_black_hole.html very nice graphical representations . http://news.discovery.com/space/could-black-holes-give-birth-to-planck-stars-140211.htm this article explains much better some of what i was trying to get across . http://www.astro.sunysb.edu/rosalba/astro2030/kerrbh.pdf rather simplistic not highly recommended but it is there and would impart a base understanding with which to read the others .
i will address the physical intuition , and i will accept as a premise that neother 's theorem is not necessary for a conserved quantity to exist , which is what you are saying . sufficient but not necessary . i will remold your question and answer as : " why is global conservation of mass not considered a tautology ? " mass is conserved in our everyday life , and even in classical microsystems . it is only when we enter the realm of atomic that the anti-intuitive surprise of quantum mechanical systems catches up with physics . from the butcher to the baker/ to the candlestick maker , we knew mass was conserved . weight is money and money is a serious question . what happened with quantum mechanics ? mass became connected with momentum and energy in an intimate way , and our strong intuition that mass is conserved gave way to the observation , that this was not always true . and we fell back to mathematics to bring order out of chaos , hence the essential for logic noether 's theorem . there is no simple way , going back to the intent of your real question , of simply adding up mass , ( as we try to do when we measure the smoke from the candle and still find mass missing ) , so as to conserve it by enlarging the universe . we needed to develop an intuition for quantum mechanics and relativity . the underlying structure of the universe revealed that conservation of mass is a concept very useful for human life , but applicable in specific four dimensional coordinate systems only . the same is true for conservation of baryon number ( intimately connected with conservation of mass ) , as was discussed in another question on this site . when energies become large enough to turn everything to a quark gluon plasma , baryon number conservation becomes meaningless . we could define a quark number that would describe the plasma but it would have little meaning as most of the energy would be in the sea and the effective mass of the plasma would have little to do with the number . thus people were telling you , despite feynman 's interesting quote in your previous answer , that in a similar way , energy conservation can be well defined and very useful in various coordinate systems but cannot be a general law of the general relativity universe . hope this helps on the intuition front .
if you observe closely , the chain does not accelerate much once it starts to come out of the beaker , and so we can say that energy is not continuously being provided to the chain . if you think the part of the chain getting lifted up from the beaker requires energy , it indirectly comes from the work done by normal force from the ground in stopping the equivalent part of the chain which fell on the ground . you can think of it as the chain passing on energy to the next part , taking it from the part which came to rest . as to where it gets the initial velocity from in the first place ( it is obviously not being given be the person holding the beaker ) , i believe that it comes from the work done by gravity , because the center of mass of the chain has lowered significantly due to the heap of chain formed on the ground . but the center of mass still keeps moving down as the heap on the ground grows bigger and bigger . so gravity is still doing work . i cannot say for sure where this work goes , but i think this is what makes the loop over the beaker grow bigger and bigger . at some point this work might be compensating for all the loss in energy , and maybe that is why the loop stops growing . edit : the force exerted on the ground there will be the weight of the chain already on the floor . there will also be the impulse of the part of the chain currently falling which comes to rest . impulse is given by $ndt$ where $n$ is the normal reaction by the ground , and $dt$ is the infinitesimal time in which it is acting . we can say that $$ndt = \delta p$$ $$ndt = dm v$$ where $dm$ is the mass of the chain which falls on the ground in time $dt$ . assuming constant linear mass density $\lambda$ we get $$dm = \lambda dx$$ thus $$\frac {dm}{dt} = \lambda \frac{dx}{dt} = \lambda v$$ thus we get $n$ as $$n = \frac {dm}{dt} v = \lambda v^2$$ this $n$ is the extra force the ground needs to apply to stop the moving chain .
as other answers say , if someone just jumps off of the international space station ( iss ) , they would still be in orbit around the earth since the iss is traveling at 17,000 miles per hour ( at an altitude of 258 miles ) . instead of just jumping , imagine the astronaut had a jet pack that could cancel that speed of 17,000 miles per hour in a very short time ( that would take 77 seconds at 10 gs of deceleration ) . so , there the astronaut is , at 258 miles above the earth 's surface , stationary and starting to accelerate at 1 g towards the earth . from the web i find that many meteors burn up at around 30 miles above the earth where the atmosphere gets thick enough to decelerate the meteor due to the air compression in front of the meteor and air friction - this compression and friction also heats up the meteor and melts it . note that this is approximately the height that felix jumped from ! how fast will the astronaut be going when he gets to 30 miles ? the answer is he would be traveling at about 6000 miles per hour ( assuming no air friction till he gets to 30 miles ) . now , that is roughly 1/3 of orbital velocity and when satellites de-orbit , they need extensive heat shielding to avoid being incinerated . so that is the first problem - an ordinary space suit would not protect the astronaut - he would need very significant heat shielding - such as a mercury capsule used by america 's first manned space program . so it would not be someone just " jumping off " the iss for sure . for now , assume he is not burned up somehow . what about the g forces of deceleration ? when satellites de-orbit they have to carefully control the angle at which they are coming in - too shallow and they could skip off back into orbit , too steep and the heat load would be too high and the deceleration would also be too high to survive . but our astronaut is falling straight in - perpendicular to the atmosphere ! this is just a guess , but if he has to decelerate from 6000 mph to a terminal velocity of something like 600 mph within about 5 miles or so , the g forces would be something like 30 gs , so he would not survive and there is no way to protect yourself from that many gs . felix started at 0 mph at about that height which is why he survived .
the ionized particles from mainly solar wind are caught and trapped by earth magnetic field , which behaves like a magnetic bottle . ( the region in which ions are trapped is called van allen radiation belts . ) this trap is weaker in the polar regions , and there the ions are mainly released into the denser parts of atmosphere . there they collide with air particles ( mostly n$_2$ and o$_2$ ) causing their fluorescence seen as the northern lights .
to a first approximation distance covered by the moon is the same as the earth 's , but you can also estimate the correction to first order . assume both orbits are circular and in the same plane since any deviations will affect only smaller order corrections . represent the position in the orbital plane as a complex number $z = r e^{2\pi i ( t/y ) } + r e^{2\pi i ( t/m ) }$ where $r$ = radius of earth orbit , $r$ = radius of moon orbit , $y$ = one year and $m$ = a siderial month the velocity $v$ is given by $\frac{v}{2\pi i} = \frac{r}{y} e^{2\pi i ( t/y ) } + \frac{r}{m} e^{2\pi i ( t/m ) }$ the speed $s = |v|$ is given by $\frac{s^2}{4{\pi}^2} = \frac{r^2}{y^2} + 2\frac{rr}{ym}cos ( 2\pi t ( \frac{1}{m}-\frac{1}{y} ) ) +\frac{r^2}{m^2}$ take the square root assuming that $\frac{r}{y} \gg \frac{r}{m}$ and use $\sqrt{a+\epsilon} = a ( 1+\frac{\epsilon}{2a}-\frac{\epsilon^2}{8a^2}+o ( \frac{\epsilon^3}{a^3} ) ) $ $\frac{s}{2\pi}= \frac{r}{y} ( 1+\frac{ry}{rm}cos ( 2\pi t ( \frac{1}{m}-\frac{1}{y} ) ) +\frac{1}{2} ( \frac{ry}{rm} ) ^2 - \frac{1}{2} ( \frac{ry}{rm} ) ^2cos^2 ( 2\pi t ( \frac{1}{m}-\frac{1}{y} ) ) +o ( \frac{ry}{rm} ) ^3 ) ) $ we want the average speed , the cosine averages to zero , but the cosine squared averages to a half , so $\bar{s}= 2\pi \frac{r}{y} ( 1+\frac{1}{4} ( \frac{ry}{rm} ) ^2+o ( \frac{ry}{rm} ) ^3 ) ) $ $\frac{ry}{rm} = 0.03$ so the distance traveled by the moon is greater than the distance travelled by the earth by about one part in 4200
ok , i consider a magnet , falling inside a long , vertical cylindrical hole ( with radius $r$ ) , drilled in a non-magnetic conductor . for simplicity , i assume that the magnetic field $\overrightarrow b$ of the magnet is so strong that we can ignore induced magnetic fields by eddy currents . this assumes also that terminal velocity of the magnet is moderate ( i.e. . the magnet is light in weight ) . friction forces with the exception caused by the eddy currents are supposed to be negligible . i am using cgs system . external magnetic field produced by the magnet : $$\overrightarrow b=3\frac{\overrightarrow{r&#39 ; } ( \overrightarrow p_m\cdot \overrightarrow{r&#39 ; } ) }{r&#39 ; ^5}-\frac{\overrightarrow p_m }{r&#39 ; ^3}$$ here $\overrightarrow p_m $ is magnetic dipole moment of the magnet , $\overrightarrow{r&#39 ; }$ is radius vector of a point where $\overrightarrow b$ is calculated . let the axis of the cylindrical hole be the z-axis , pointing down . we place the center of the magnetic dipole at z=0 and assume that $\overrightarrow p_m $ is always parallel to the z-axis . let $v_t$ be terminal velocity of the falling magnet . then we can find $v_t$ from the formula : $$w=f\cdot v_t=mg\cdot v_t$$ where $w$ is power , generated by eddy currents in the conductor , $mg$ is the weight of the magnet . so we need to find $w$ . consider a thin wall cylinder ( concentric with the hole ) , with radius $r&gt ; r$ , wall thickness $dr$ , in the conductor . in this cylinder we consider a ring , with the cross-sectional area $dz\cdot dr$ . at this ring let $b_n$ be a coponent of $\overrightarrow b$ , perpendicular to the wall of the cylinder . then the magnitude of the electromotive force ( emf ) induced in the ring is : $$e=\frac{v_t}{c}b_n2\pi r$$ conductance of the ring : $$dg=\frac{\gamma drdz}{2\pi r}$$ where $\gamma$ is the electrical conductivity of the conductor . thus , the current induced in the ring : $$i=edg=\frac{v_t}{c}b_n\gamma drdz$$ power , released in the ring : $$dw=\frac{i^2}{dg}=e^2dg=\frac{v_t^2}{c^2}\gamma b_n^22\pi r drdz$$ the only thing that has yet to be found is $b_n$ . it comes from the expression of $\overrightarrow b$ above . because this is a pure math i skip this step and write down the final formula for $dw$: $$dw=18\pi\frac{v_t^2}{c^2}\gamma p_m^2\frac{z^2r^3drdz}{ ( z^2+r^2 ) ^5}$$ now , to get the whole power released in the conductor we need to integrate throughout the conductor : $$w=18\pi\frac{v_t^2}{c^2}\gamma p_m^2 \int_{-\infty }^{+\infty } \int_{r}^{+\infty } \frac{z^2r^3drdz}{ ( z^2+r^2 ) ^5}=18\pi\frac{v_t^2}{c^2}\gamma p_m^2\frac{5\pi}{384r^3}$$ so , terminal velocity of the falling magnet : $$v_t=\frac{64}{15\pi^2}\frac{mgc^2}{\gamma p_m^2}r^3$$ if you want to consider only a pipe then integrate $dw$ only by $z$ .
i think that mere touching does not bring the surfaces close enough . the surface of a metal is not perfect usually . maybe it has an oxide layer that resists any kind of reaction . if the metal is extremely pure and if you bring two pieces of it extremely close together , then they will join together . it is also called cold welding . for more information : what prevents two pieces of metal from bonding ? cold welding
luboš 's answer is of course perfectly correct . i will try to give you some examples why the straightest line is physically motivated ( besides being mathematically exceptional as an extremal curve ) . image a 2-sphere ( a surface of a ball ) . if an ant lives there and he just walks straight , it should be obvious that he will come back where he came from with his trajectory being a circle . imagine a second ant and suppose he will start to walk from the same point as the first ant and at the same speed but into a different direction . he will also produce circle and the two circles will cross at two points ( you can imagine those circles as meridians and the crossing points as a north resp . south poles ) . now , from the ants ' perspective who are not aware that they are living in a curved space , this will seem that there is a force between them because their distance will be changing in time non-linearly ( think about those meridians again ) . this is one of the effects of the curved space-time on movement on the particles ( these are actually tidal forces ) . you might imagine that if the surface was not a sphere but instead was curved differently , the straight lines would also look different . e.g. for a trampoline you will get ellipses ( well , almost , they do not close completely , leading e.g. to the precession of the perihelion of the mercury ) . so much for the explanation of how curved space-time ( discussion above was just about space ; if you introduce special relativity into the picture , you will get also new effects of mixing of space and time as usual ) . but how does the space-time know it should be curved in the first place ? well , it is because it obeys einstein 's equations ( why does it obey these equations is a separate question though ) . these equations describe precisely how matter affects space-time . they are of course compatible with newtonian gravity in low-velocity , small-mass regime , so e.g. for a sun you will obtain that trampoline curvature and the planets ( which will also produce little dents , catching moons , for example ; but forget about those for a moment because they are not that important for the movement of the planet around the sun ) will follow straight lines , moving in ellipses ( again , almost ellipses ) .
graphical explanations are most welcomed . this is my personal favorite one of those ( from mtw 's " gravitation" ) . for an animation , see , for example this java applet .
this is what i think that happens . the bottles with the soda have the liquid inside under pressure . under these conditions you have an amount of co2 dissolved in the liquid . if you shake it then it will make some bubbles but not a lot and it will return to the previous state . if you do not disturb the liquid and open the bottle , then the pressure will drop to atmospheric levels . the dissolved co2 in the liquid is in a grater concentration that the liquid can hold for that pressure . but there are no evaporation seeds for the bubbles to form . it is a similar thing with the superheated water , where you have crossed the threshold for a phase transition , but you need impurities to initiate the process in the liquid . if you disturb the liquid it will produce bubbles . if you first shake the bottle and then open it , then the agitated liquid will form bubbles of co2 with the pressure drop . now , once you have reduced the pressure on the liquids surface , there is no way to put the co2 back in to the liquid , except for increasing the pressure again .
in practice , i would say a particle is anything that can be treated as a single , bound , small object . this means that whether an object is considered a particle depends on how you are working with it . for example , an atomic nucleus is a particle if you are doing spectroscopy , because the structure of the nucleus is irrelevant to the changes in energy levels of the electrons and so you can basically consider it a charged point . but that same nucleus would not be considered a particle in an ion-ion collider , because the structure of the nucleus is highly relevant there . more generally : quarks , gluons , protons , neutrons , atoms , molecules , cells , grains of dust , raindrops , baseballs , satellites , planets , stars , and even galaxies can all be considered particles in an appropriate context . it does not fit the definition of a particle this is an example of why you should never trust a general-purpose dictionary to give the appropriate definition for a term used in a technical field like physics . i have been told on multiple cases that a particle is the smallest form of composite matter . that is not necessarily true ; as i have said , it is context-dependent . however , that is the definition implied in the term " particle physics " ( which is really meant to mean something like " fundamental particle physics" ) .
this is not true--- you made a mistake with the substitution and variation . as i am sure you know , experimentally , it is false , you can get strong bulk magnetic field penetration into any non-superconducting material . the substitution you made , replacing $b \times r$ with $a$ , is not valid for doing variations with respect to a . the electron operators " r " in $b \times r$ are not the same type of thing as the classical function $a ( r ) $--- this is a formal error . you can not replace $b \times r$ ( the classical function $\times$ an electron position operator ) with a ( the classical function ) , because then you are ignoring the fact that r is an operator that acts on electron wavefunctions . this is what comes back to bite you when you take the variation . you should sort it out physically--- in a superconductor , making an a field produces a coherent bosonic current which cancels the a . in a normal metal , you rearrange the fermi sea into landau levels , which are then filled up in opposite senses of current , up to the fermi energy , so that the net current gives a diamagnetic response . it is easiest to say the error in field theory--- the fermi field operator which has an expected value is a neutral bilinear $\bar{\psi}\psi$ which counts the number of electrons , not the charged bilinear $\psi\psi$ as in a superconductor , which gives a classical charged field response . the correct variation to take is using the a in the kinetic term for the many-electron hamiltonian , replacing $p$ with $p-ea$ , and then varying with respect to a . in this case , you get the current from the electron motion . the amount of this current is not proportional to a , unless the electrons are coherently superposed bosons . but the reason you got the boson answer in the fermi case is that you substituted for the psi field as if they were a classical field , and then you do get the same answer as for a classical charged field , and you get the exponential decay .
you are shooting photons out the back . now , they do not have ( rest ) mass , but they do carry energy . that energy came from on on-board store of some kind 1 and its loss to your craft making it less massive by $e/c^2$ where $e$ is the energy of the photons . 1 if it did not come from an on board store you are either ( 1 ) using a light sail which is a different problem or ( 2 ) are trying to pick up the energy from an external source which is transferring momentum to your craft and generates some serious limitations of the behavior of your craft relative your energy source .
dark matter can be hot , warm or cold . hot means the dark matter particles are relativistic ( kinetic energy on the order of the rest mass or much higher ) , cold means they are not relativistic ( kinetic energy much less than rest mass ) and warm is in between . it is known that the total amount of dark matter in the universe must be about 5 times the ordinary ( baryonic ) matter to explain the cmb as measured by wmap . now the neutrino oscillation experiments prove that neutrinos have a non-zero rest mass , however the rest masses must still be very small so they could only contribute to the hot dark matter ( see this reference ) however , cold dark matter must be a very significant component of the universe to explain the growth of structures such as galaxies and stars ( see this reference ) . thus cold dark matter is also required to explain the galactic rotation curves . according to this source : current estimates for the neutrino fraction of the universe’s mass–energy density lie in the range 0.1% &lt ; ∼ ν &lt ; ∼ a few % , under standard assumptions . the uncertainty reflects our incomplete knowledge of neutrino properties . so most cosmic neutrinos are probably less than 10% of the total dark matter in the universe .
see , you are required to find volume current density $j_\phi$ . though its name is volume current density , you know it is the current flowing per unit surface area . now the subscript $\phi$ in $j_\phi$ denotes it is flowing in the $\hat{\phi}$ direction . now in the spherical polar co-ordinate the infinitesimal length elements along the direction $\hat{r} , \hat{\theta} \rm\ and\ \hat{\phi}$ are dr , $rd\theta$ and $r\sin\theta d\phi$ respectively . so $dr\times rd\theta$ is the area you are interested in . now looking at your figure i am interested in rewriting the current i . $$i=i\int \delta ( r-a ) dr= i\int \delta ( r-a ) dr\ \delta\big ( \cos\theta-cos ( \pi/2 ) \big ) d ( cos\theta ) $$ $$=i\int \delta ( r-a ) dr\ \delta\big ( \cos\theta-0\big ) d ( cos\theta ) $$ $$=i\int \delta ( r-a ) dr\ \delta\big ( \cos\theta\big ) d ( cos\theta ) $$ $$=i\int \delta ( r-a ) dr\ \delta\big ( \cos\theta\big ) \sin\theta d\theta$$ $$=i\int \delta ( r-a ) \delta\big ( \cos\theta\big ) \sin\theta dr\ d\theta$$ $$=i\int \frac{ \delta ( r-a ) \delta ( \cos\theta ) \sin\theta}{r} {dr\ r d\theta}$$ so you see your $$j_\phi = i\frac{ \delta ( r-a ) \delta ( \cos\theta ) \sin\theta}{r}$$
the choice of the dependence is largely arbitrary at that point . in that chapter they just choose lagrangian as $l ( v^2 ) $ . if they chose it as $l^\prime ( |v|^n ) $ in equation $ ( 3.1 ) $ , they had just have to say $l^\prime ( |v|^n ) =\frac12 m\sqrt [ n ] {|v|^n}^2$ in equation $ ( 4.1 ) $ . also , this choice is simple enough because it is merely a dot product of velocity with itself , which is the simplest scalar function of a vector .
where a string carves out a $2$-dimensional world-sheet and a point particle carves out a $1$-dimensional world-line of spacetime , the instanton carves out a $0$-dimensional world-point . counting only spatial dimensions , a string is $1$-dimensional and a point particle is $0$-dimensional . by logical extension , an instanton has dimension $-1$ , if we only count spatial dimensions ; intuitively because , except for an single instant of time , it is not there . concerning resource recommendations for instantons : s . coleman , aspects of symmetry , chapter 7 . ( note that coleman calls solitons for lumps . ) g . ' t hooft and f . bruckmann , monopoles , instantons and confinement , arxiv:hep-th/0010225 . many qft textbooks has a section on instantons , e.g. , s . weinberg , qft2 , section 23.5 ; l.h. ryder , qft , section 10.5 .
the photons move at the speed of light in a straight line from the laser to the moon and back . the spot on the moon can move faster than light . there is no law against that . the spot is not a physical object , just an image . when you turn your wrist nothing happens to the photons which are already on the way to the moon - they continue on the same trajectory . but new photons are emitted in the new direction of your laser . it is like waving a garden hose back and forth .
can photons push the source which is emitting them ? yes . if yes , will a more intense flashlight accelerate me more ? yes does the wavelength of the light matter ? no is this practical for space propulsion ? probably not does not it defy the law of momentum conservation ? no in fact that last question is the key one , because photons do carry momentum ( even though they have no mass ) . photons , like all particles obey the relativistic equation : $$ e^2= p^2c^2 + m^2c^4 $$ where for a photon the mass , $m$ , is zero . that means the momentum of the photon is given by : $$ p = \frac{e}{c} = \frac{h\nu}{c} $$ where $\nu$ is the frequency of the light . let 's suppose you have a flashlight that emits light with a power $w$ and a frequency $\nu$ . the number of photons per second is the total power divided by the energy of a single photon : $$ n = \frac{w}{h\nu} $$ the momentum change per second is the numbr of photons multiplied by the momentum of a single photon : $$ p/sec = \frac{w}{h\nu} p = \frac{w}{h\nu} \frac{h\nu}{c} = \frac{w}{c} $$ but the rate of change of momentum is just the force , so we end up with an equation for the force created by your flashlight : $$ f = \frac{w}{c} $$ now you can see why i have answered your questions above as i have . the force is proportional to the flashlight power , but the frequency $\nu$ cancels out so the frequency of the light does not matter . momentum is conserved because it is the momentum carried by the photons that creates the force . as for powering spaceships , your 1w flashlight creates a force of about $3 \times 10^{-9}$ n . you had need a staggeringingly intense light source to power a rocket .
there are rules about things moving through space faster than light - there is no rule about space expanding faster than light . as long as it can not be used to transfer information then there is no problem with relativity .
the opposing force always work on a different body , thats why they dont cancel . for instance the moon pulls the earth and the earth pulls the moon with same force . also , the horse dont move by pulling the cart , but by pulling the earth .
this is , to the best of my knowledge , not doable without some sort of ' forgetfulness ' on your part , or on the part of the physical systems you use to prepare your state . the most obvious way is to flip a coin and decide which of $|1⟩$ and $|2⟩$ you want to prepare , and not record the outcome of the coin , and this is a perfectly fine way to generate $$ \hat\sigma=\tfrac12\left ( |1⟩⟨1|+|2⟩⟨2|\right ) . $$ of course , with a single realization this sort of feels like cheating , but the fact is that there is no way to measure , or corroborate , the full quantum state of a single realization of a system . if this feels inelegant , let me propose another way . if you are able to produce the pure state $$|+⟩=\tfrac{1}{\sqrt{2}}\left ( |1⟩+|2⟩\right ) , $$ then you will also be able to generate its orthogonal conjugate $$|-⟩=\tfrac{1}{\sqrt{2}}\left ( |1⟩-|2⟩\right ) , $$ for example by changing the phase relationship between the two waves you mention . as it happens , the mixed state that has 50% chance of being either in $|+⟩$ or in $|-⟩$ is also $\hat\sigma$ , so that flipping a coin to decide on the phase relationship will also generate $\hat\sigma$ . more elegantly , though , you can set that phase to drift naturally throughout the course of the experiment , and this will also result in a mixed state . ( you need to be careful , on the other hand , to ensure that this phase drift does not affect the resonant capture of your particles . ) finally , a third method is to use one half of an entangled state , which will locally look like a mixed state even though the global state is a pure one . assume , then , that you have some other two-level system , with states $|\ ! \uparrow⟩$ and $|\ ! \downarrow⟩$ , and that your preparation procedure is able to generate $|1⟩\otimes|\ ! \uparrow⟩$ or $|2⟩\otimes|\ ! \downarrow⟩$ , or arbitrary pure-state superpositions of those two . one simple way to do this is to spin-polarize the plane waves of your proposal . for a system like this , any experiment that is local to the $\{|1⟩ , |2⟩\}$ half of the hilbert space will be sensitive to the partial trace over $\{|\ ! \uparrow⟩ , |\ ! \downarrow⟩\}$ , and this will be exactly the mixed state $\sigma$ . to observe the pure state , you need an entangling measurement between the two degrees of freedom . it is important to note that this third method is actually a generalization of the previous two , and it is unclear whether it is the only way to generate mixed states or whether ' really ' mixed states are possible in nature . from a many-worldsian perspective , you can substitute the spin degree of freedom for a more macroscopic system , like $$ |\ ! \uparrow⟩=|\text{ruslan measures a quantum coin to give heads and runs experiment}⟩ , $$ $$ |\ ! \downarrow⟩=|\text{ruslan measures a quantum coin to give tails and runs experiment}⟩ , $$ and you get identical predictions for your experimental outcomes .
it is in fact a reflection of the fact that the rate of expansion has been nearly constant for a long time . mathematically , the expansion of the universe is described by a scale factor $a ( t ) $ , which can be interpreted as the size of the universe at a time $t$ , but relative to some reference size ( typically chosen to be the current size ) . the hubble parameter is defined as $$h = \frac{\dot{a}}{a}$$ and the hubble time is the reciprocal of the hubble parameter , $$t_h = \frac{a}{\dot{a}}$$ now suppose the universe has been expanding at a constant rate for its entire history . that means $a ( t ) = ct$ . if you calculate the hubble time in this model , you get $$t_h = \frac{ct}{c} = t$$ which means that in a linear expansion model , the hubble time is nothing but the current age of the universe . in reality , the best cosmological theories suggest that the universe has not been expanding linearly since the beginning . so we would expect that the age of the universe is not exactly equal to the hubble time . but hopefully it makes sense that if any nonlinear expansion lasted for only a short period , then the hubble time should still be close to the age of the universe . that is the situation we see today . for more information on this , i would suggest you check out these additional questions value of the hubble constant over time universe expansion as an absolute time reference and others like them .
mathematics does not need to bother itself with real-world observations . it exists independently of any and all real-world measurements . it exists in a mental space of axioms , operators and rules . geometry conforms to that description . physics depends on real-world observations . any physics theory could be overturned by a real-world measurement . none of maths can be overturned by a real-world measurement . none of geometry can be . physics starts from what could be described as a romantic or optimistic notion : that the universe can be usefully described in mathematical terms ; and that humans have the mental ability to assemble , and even interpret , that mathematical description . maths need not concern itself with how the universe actually works . perhaps there are no real numbers , one might think it is likely that there is only a countable number of possible measurements in this universe , and nothing can form a perfect triangle or point . maths , including geometry , is a perfect abstraction that need bear no relation to the universe as it is . physics , to have any meaning , must bear some sort of correspondence to the universe as it is . so although geometry appears to bear relation to the universe as it is , it need not do so in order to satisfy its own axioms : it has a consistency requirement , as opposed to a descriptive requirement .
all ideas are worth exploring :- ) this question is mosly light engineering , but there is also some elementary physics regarding forces . actually , an answer to your question depends very much on what you want to do with the barbell , on how much you want to move it around . if it is simply for weight lifting , this requires only vertical motion , an a bit of lateral ( horizontal ) motion to get some freedom of mouvement for the weight lifter . then the plates can be replaced by a rope pulling the ends of the bar towards the ground . the ropes can go through pulleys , so that the force pulling on them can come from any direction . now , all you need is a controlled device that will adjust the force pulling the rope independently of the motion of the bar . one simple way is to to attach the other ends of the two ropes to a lever , far enough from the axis so that it can be moved for about 2.5 meters , which is a bit more than what is needed for the weight lifter lifting the barbell from ground to vertical position with arms extended ( but you do not wnat him blocked as he is lifting . then , you can use a motor to move a weight along the arm of the lever so that the pull on the rope can be adjusted to whatever force is required . this of course takes much room . you can reduce the size of the lever by using blocks and tackles , so that the motion of the ropes can be increased , but this requires moving larger weights on the lever . there are probably other devices that can provide an adjustable constant force on one end of a rope which allowing some motion on the free end of the rope . i leave it to you to study the physics of the system . if you were thinking of changing the lifting force required by moving things around on an independent barbell device , the answer is no . you are simply lifting the weight of whatever constitute the device you lift , and that does not change , assuming you do not change its mass ( by adding or removing parts ) and you do not change your location ( the same mass can produce a different weight when gravity changes , for example on the moon ) . even relativity , very popular on this site , will not help you . though special relativity has the reputation of causing mass to increase with speed , it will not do that for you , because lifting the mass implies that you travel with it , and that the planet creating the gravity for the weights also travel with it . so that , as far as you ( or the weight lifter ) are concerned , there is no speed , no motion . thinking twice about it , relativity can help you , if you are willing to take the risk and can afford the energy and much else ( but check first with a relativity engineer ) . the trick would be to rotate the plates at relativistic speed . all you need is quasi infinitely strong material ( else the plates will explode under centrigugal forces ) and perfect frictionless bearings that will not transfer torque to the holding bar . also you have to use an external energy source to rotate the plate , else the total lifted would not change ( mass-energy equivalence ) . i probably forgot a few thousand things , but solving the above should already keep you busy for some time . one last piece of advice : avoid dropping this barbell on the ground , as i am not sure how much of the planet would survive . also , do not try to turn around while lifting to show how strong you are : it will simply not work ( and if anything did happen , you might not like it ) . real relativity specialists ( which i am not in the least ) may be able to say more on this : )
well , the sentence it seems like if it is an inherent property of the spring it should not change , so if it does , why ? clearly is not a valid argument to calculate the $k$ of the smaller springs . they are different springs than their large parent so they may have different values of an " inherent property": if a pizza is divided to 4 smaller pieces , the inherent property " mass " of the smaller pizzas is also different than the mass of the large one . ; - ) you may have meant that it is an " intensive " property ( like a density or temperature ) which would not change after the cutting of a big spring , but you have offered no evidence that it is " intensive " in this sense . no surprise , this statement is incorrect as i am going to show . one may calculate the right answer in many ways . for example , we may consider the energy of the spring . it is equal to $k_{\rm big}x_{\rm big}^2/2$ where $x_{\rm big}$ is the deviation ( distance ) from the equilibrium position . we may also imagine that the big spring is a collection of 4 equal smaller strings attached to each other . in this picture , each of the 4 springs has the deviation $x_{\rm small} = x_{\rm big}/4$ and the energy of each spring is $$ e_{\rm small} = \frac{1}{2} k_{\rm small} x_{\rm small}^2 = \frac{1}{2} k_{\rm small} \frac{x_{\rm big}^2}{16} $$ because we have 4 such small springs , the total energy is $$ e_{\rm 4 \ , small} = \frac{1}{2} k_{\rm small} \frac{x_{\rm big}^2}{4} $$ that must be equal to the potential energy of the single big spring because it is the same object $$ = e_{\rm big} = \frac{1}{2} k_{\rm big} x_{\rm big}^2 $$ which implies , after you divide the same factors on both sides , $$ k_{\rm big} = \frac{k_{\rm small}}{4} $$ so the spring constant of the smaller springs is actually 4 times larger than the spring constant of the big spring . you could get the same result via forces , too . the large spring has some forces $f=k_{\rm big}x_{\rm big}$ on both ends . when you divide it to four small springs , there are still the same forces $\pm f$ on each boundary of the smaller strings . they must be equal to $f=k_{\rm small} x_{\rm small}$ because the same formula holds for the smaller springs as well . because $x_{\rm small} = x_{\rm big}/4$ , you see that $k_{\rm small} = 4k_{\rm big}$ . it is harder to change the length of the shorter spring because it is short to start with , so you need a 4 times larger force which is why the spring constant of the small spring is 4 times higher .
i am not entirely sure that this is what you are looking for , but here 's a lagrangian take on the system . i choose coordinates so that $x_1 ( 0 ) =0 , x_2 ( 0 ) =0$ , so $$l=m/2 ( \dot x_1^2 + \dot x_2^2 ) - k/2 ( x_1-x_2 ) ^2$$ it is easier to solve if we go to coordinates $$x=1/2 ( x_1+x_2 ) , \delta x = 1/2 ( x_1-x_2 ) $$ so that $$l = m ( \dot x^2 + \delta x^2 ) - 2k\delta x^2$$ . this gives us the following euler-lagrange equations - $$\frac{d}{dt}\left ( 2m\dot x\right ) = 0$$ $$\frac{d}{dt}\left ( 2m\dot\delta x\right ) = -4k\delta x$$ as you had expect , the center-of-mass motion is constant and the second equation describes an oscillating $\delta x$ . what determines the amplitude of your oscillation here is the initial condition - the bullet strikes one end of the molecule and transfers momentum $p_0$there , so that you get initial conditions $$x_1=0 , x_2=0 , \dot x_1 = p_0/m , \dot x_2 = 0$$ . if we parametrize the oscillation as $\delta x = \alpha \sin ( \omega t + \phi ) $ , at $t=0$ we get $$\delta x ( 0 ) = \alpha \sin ( \phi ) = 0 , \delta \dot x = \alpha \omega \cos ( \phi ) = p_0/2m $$ which gives us $$\phi=0 , \alpha = \frac{p_0}{2\omega m}$$ . so of course $\alpha$ is completely determined in this picture . comparing with the model in your question , i would say you mistreat the collision by assuming that the bullet collides with the entire molecule as a rigid unit , while in fact the collision is between the bullet and one atom , both of equal mass , so that $v_b$ will be zero , $v_m$ becomes exactly $v_0/2$ and you can calculate your $\alpha$ from there . i am wondering a little about the validity of my approach when going to infinite $k$ since that should change the collision process to be ' with the entire system ' and i would like to see a kind of $k$ dependence in $v_b$ , but i can not see how to properly treat this at the moment .
euler 's rotation theorem states that every rigid motion that leaves some point fixed is equivalent to a rotation about an axis running through that point . it is easy to see that every rigid motion can therefore be decomposed in the prescribed manner ( i.e. , by first moving the centre of mass to its new location ) .
your result is correct and , of course , there is no problem with relativity . when you are on the train and you see how events unfold for an observer on the platform at point m , then you conclude that he will see the light from a first . when you are on the platform and you see how events unfold for an observer on the train , then you conclude that he sees the light from b first . there is no problem with that&mdash ; that is actually the essence of the theory of relativity : things look different from different reference frames , and every observer is right in their respective reference frame . when the two observers compare their conclusions , they will be able to attribute the difference to the fact that the speed of light is constant .
theoretically the answer is yes . that is because the sun is not a blackbody emitter , there is an excess of uv radiation . so if you were able to achieve radiative equilibrium with only uv light ( which is maybe 1% of bb radiation at those temps ) , you could do it . practically , i would think it would be just about impossible , as your filter would have to have it is innermost surface at nearly 6000k . note : the solar uv primarily comes from the chromosphere and corona , which is heated ( in some not too well understood way ) by mechanical/magnetic energy derived from convective processes . the x-ray excess is even greater than the uv excess . even the earth gives off detectable gamma rays , and that would be impossible thermally .
sure you have to transform the fields too . let 's say that your system rotates around the z -axis , so b remains unchanged , but e will move around a circle in the rotating system , so its coordinates will be : $$ e= ( e_x*\cos ( \omega_{rot} t ) , e_x*\sin ( \omega_{rot} t ) , 0 ) $$ where i used $e_x$ as an amplitude , and $\omega_{rot}$ as angular frequency of the rotating system .
what does it mean ? does it mean that the y component of the acceleration is always equal to acceleration due to gravity ? not always . but in this case , yes . all i got were few known results and other nonsensical ones ( like the one in this question . probably due to my erroneous application of the lagrange . ) you did not get the results you got because of your own fault but because of virtue of lagrangian formulation of mechanics . you got them because lagrangian formulation of mechanics works ! you took a simple system and got equations of motion for it which you already knew . do you know equations of motion for double pendulum ? how will you find them ( almost without using brain ) ? algorithm : find lagrangian for double pendulum get equations of motion using euler-lagrange equations
in order to use lagrangians in qm , one has to use the path integral formalism . this is usually not covered in a undergrad qm course and therefore only hamiltonians are used . in current research , lagrangians are used a lot in non-relativistic qm . in relativistic qm , one uses both hamiltonians and lagrangians . the reason lagrangians are more popular is that it sets time and spacial coordinates on the same footing , which makes it possible to write down relativistic theories in a covariant way . using hamiltonians , relativistic invariance is not explicit and it can complicate many things . so both formalism are used in both relativistic and non-relativistic quantum physics . this is the very short answer .
( i have a suggestion to make this question a cw . ) general physics : ( early undergrad and advanced high school ) problems in physics i.e. irodov - ( highly recommended ) problems in physics s s krotov - ( once again , highly recommended but out of print ) physics olympiad books - ( have not read but saw some olympiad problems back in the day ) physics by example ( like this book a lot , lower undergrad ) feynman 's tips on physics ( exercises to accompany the famous lectures ) general qualifying exam books : the following books are a part of a series dedicated to the qualifying exams in american universities and has a large compilation of problems of all levels . others in the series include mechanics , electromagnetism , quantum mechanics , thermodynamics , optics and solid state physics . unlike other compilation of exercises for qualifiers ( such as princeton or chicago problems , or the one mentioned below ) , they make no excuse for economy and include as many problems from all levels for each subtopic . another good book that i read recently for my exam is the two volume series : a guide to physics problems ( part 2 has some relatively easy but interesting problems . i have not gone through the first part , which is much much more challenging . )
imagine you have a star sized ball of gas that is in equilibrium i.e. the pressure of the gas exactly balances the inwards gravitational force . now imagine compressing the gas . this has two effects : the first effect is the obvious one that compressing the gas increases the pressure , so the result is an outward force and if we stop compressing the gas we expect it to expand again . this is the standard boyle 's law behaviour that we all learned in school . the second effect arises from general relativity . by compressing the gas we have done work on it , so the gas/star now has more energy than it did before . but in gr energy causes curvature just like mass does . in fact the stress energy tensor does not distinguish between mass and energy - it uses a single value for mass/energy density using the famous formula $e = mc^2$ to equate the two . so by compressing the gas we have increased the spacetime curvature caused by the star so we have made it is gravitational field stronger . under most circumstances the increase in the gravity caused by pressure is insignificant , and pressure has the effect we expect i.e. it causes the star to expand . however in extreme cases , like the neutron star collapse mentioned by chris in his comment , the gravitation effect of pressure overcomes its effect on expansion and pressure then contributes to the collapse . dark energy is rather different . i started by pointing out that by compressing the gas and increasing the pressure we are doing work on the gas/star and this contributes to its gravity . if you take some region of vacuum containing just dark energy and compress it the dark energy does work on you i.e. the energy of the space you have compressed decreases . this is what we mean by a negative pressure . because of this dark energy can not cause attraction in the way that pressure can .
relativistic lagrangian and hamiltonian mechanics can be formulated by means of the jet formalism which is appropriate when one deals with transformations mixing position and time . this formalism is much advocated by g . sardanashvily , please see his review article .
first consider faraday 's law , which states that $$ \nabla \times \textbf{e} = -\frac{\partial \textbf{b}}{\partial t} . $$ we can interpret this as follows : whenever we are generating a magnetic field that changes with time , there is an associated electric field , and vice-versa . an equivalent interpretation is that a changing magnetic field causes a spatially varying electric field , but this view is mathematically equivalent to the first . the only important point is that whenever there is a changing magnetic field , an electric field will be present as well . let 's now consider the situation where there is a coil ( with no current flowing through it ) and a changing magnetic field ( perpendicular to cross sections of the coil ) due to some external source . as i mentioned , because there is a changing $\textbf{b}$ field , there must also be an $\textbf{e}$ field as well ( whether or not the changing magnetic field is causing the electric field is a matter of semantics , as i mentioned ) . it is this electric field which causes the current to begin flowing . however , due to the magnetic field , there will also be an additional force on the electrons once they start moving . if we consider individual electrons , there will be a drift force from the magnetic field that changes the direction of their velocity . this is due to the $\textbf{v} \times \textbf{b}$ component of the lorentz force . to summarize : changing magnetic field implies that there is an associated electric field . the electric field causes a current to flow in the wire . the magnetic field produces a drift force on the charges in the current .
short answer you have hit upon the quirk that the si and cgs systems not only measure electric charge with different units , but also assign them different dimensionality . in si , the ampere is a base unit . amperes are not made out of anything else - they are primitive , like meters , kilograms , and seconds . one ampere is one coulomb per second , so the unit of electric charge , the coulomb , is equal to one ampere-second . when there is an equation that has amperes or coulombs on one side , and something without those units ( like force ) on the other side , the equation will always have a constant that has the correct units to balance things out . that job is done by $\mu_0$ and $\epsilon_0$ . in cgs , charge is measured in esu , but this is a derived unit . charge is considered to be made up out of length , mass , and time the way , for example , angular momentum is . charge has dimensions $$ [ m ] ^{1/2} [ l ] ^{3/2} [ s ] ^{-1}$$ and one esu is equal to the square root of a $\text{dyne-cm}^2$ . longer answer in si , the we start with meters , kilograms , and seconds . then we set $\mu_0$ to be $4\pi\times 10^{-7}\textrm{ohm-sec/m}$ . now take two wires very long compared to the distance between them , and run the same current through them . make the distance between them $d$ . a section of length $l$ of the wires will feel a force of attraction $f$ , which depends on the square of the current . the ampere is defined such that when the current is measured in amperes , we find that $$f = \frac{\mu_0 i^2 l}{2\pi d}$$ when $f$ is measured in newtons , $d$ and $l$ in meters . this definition requires that you can create the same current in each wire , that you can accurately measure the force per unit length , and that the force is perfectly proportional to the square of the current . in truth it will not be because the wires are not infinitely long and perfectly straight and parallel , but some equivalent operational definition might be used in practice . ( the fact that the definition is at all possible is a test of the physical hypothesis of the proportionality . ) the important point is that the ampere becomes a new basic unit . ( technically , what is been said so far would not define amperes , but give us a relationship between amperes and ohms . we could hash that out by looking at the units of $v=ir$ , for example ) . the coulomb used in coulomb 's law is then defined as one ampere-second . this would seem to allow us to experimentally measure $\epsilon_0$ because it is now the only unknown in coulomb 's law . originally , that was correct , but now we have defined the speed of light to be $c = 2.99792458 \times 10^8$ meters/second , and so we are constrained by $c = 1/\sqrt{\mu_0\epsilon_0}$ . this forces $\epsilon_0$ to be $$\epsilon_0 = \frac{1}{4\pi\times 8.9875517853681764}\frac{\textrm{sec}}{\textrm{ohm-m}}$$ , meaning that we could also define the coulomb directly from coulomb 's law - take two charged bodies , measure the force between them , and define the coulomb to be the unit of charge such that $$f = \frac{q_1q_2}{4\pi\epsilon_0 r^2}$$ with $r$ in meters and $f$ in newtons . the experimental observation that these two definitions of the coulomb ( one as the ampere-second and one directly from coulomb 's law ) agree then becomes a test of the physical theory of electromagnetism . in cgs units , charge is measured in esu , a derived unit defined by coulomb 's law as you have written it . one esu is the charge such that two charged bodies feel the coulomb force $$f = \frac{q_1q_2}{r^2}$$ with $f$ in dynes and $r$ in centimeters . if both charges are one esu , this gives $$1\text{ }\textrm{dyne} = \frac{1\text{ }\textrm{esu}^2}{1\text{ }\textrm{cm}^2}$$ or $$1\text{ }\textrm{esu} = 1\text{ }\textrm{cm}\sqrt{\textrm{dyne}}$$ finally , since the dyne is a derived unit , this could be written in terms of base units as $$1\text{ }\textrm{esu} = \sqrt{\frac{\textrm{g-cm}^3}{\textrm{s}^2}}$$ as a consequence , there is one less base unit involved in electromagnetic formulas when using cgs . also , there is no need for the constant $\epsilon_0$ used in si . there is also no need for $\mu_0$ . instead , things like maxwell 's equation explicitly include the speed of light $c$ . you can see a side-by-side comparison of basic equations of electromagnetism in si and cgs units here . converting cgs centimeters and grams to si meters and kilograms , then equating the two expressions for the force in coulomb 's law , we find the conversion that one coulomb is the same amount of charge as $2.99792458 \times 10^9$ esu . reference this answer is a summary of an appendix to purcell 's electricity and magnetism
to your question " how can the wave propagate to the right when the current is negative ? " i will answer that your statement that " the wave propagates to the right " is not exactly correct : what you have to consider here is group velocity , not each individual phase velocity . since both plane waves propagate to the right with $k_1 , k_2&gt ; 0$ , then you implicitly assume $\omega_1 \equiv \omega ( k_1 ) &gt ; 0$ and $\omega_2 \equiv \omega ( k_2 ) &gt ; 0$ , but your problem gives no more information of the dispersion relation . in order to have a better idea of what the flow of probability density is , you need to consider group velocity here given by $\dfrac{\delta \omega}{\delta k} = \dfrac{\omega_2-\omega_1}{k_2-k_1}$ , which could indeed have any sign , depending on the dispersion relation .
without a doubt , it is the zeroth law of thermodynamics , as it defines an equivalence relation . it states that if two systems are in thermal equilibrium with a third system , then they are in thermal equilibrium with each other .
this is similar to the reasons one way mirrors work . if you look through a black net then no light is reflected from the net so the eye sees only the light coming from the objects on the far side of the net . the amount of the external light that reaches you is reduced , but the brain is pretty good at reconstructing images from only partial data , so the view looks unchanged . if you look through a white net then the eye receives a mixture of the light reflected from the net and the light from outside transmitted through it . if the room you are in is dark and the outside is bright , then the amount of light reflected from the net is small compared to the transmitted light and you still do not see the net . however if the room is light and the outside dark then the light reflected from the net swamps the light transmitted and you only see the net . in between you will see both the net and the view .
to the points akhmeteli already mentioned there i want to add two reasons : in larger buildings there will always be someone using water , so the water coming from the ground at approximately 6°c will keep the pipes warm enough a brick wall with a thickness of a few centimeter provides already good isolation , so a little bit of heating at the inside will keep the temperature well above the freezing point especially the isolation is also true for pipes outside in the ground , the thermal conductivity of soil is so low that in a depth of 2 meters it will stay warm for the whole winter . this does not work in very cold climates though , when it will be below zero all year long . this permafrost can reach down hundreds of meters .
the question , as i interpret it , is about the conservation of energy during joule-thomson expansion . ( that is , expansion of a gas through a small hole or porous plug , where there is a pressure difference between the two sides , and no work or heat is exchanged with the environment , except for work associated with the pressure change . ) consider the classic joule-thompson experiment , where gas in a pipe is forced through a porous plug . let the pressure on one side be $p_1$ and the pressure on the other side by $p_2 &lt ; p_1$ , so that the gas flows from side 1 to side 2 . now consider the passage of a small amount of gas from one side to the other . on side 1 , the rest of the gas does work $p_1 v_1$ to push the packet of gas through the plug ( where $v_1$ is the volume that the packet of gas has at pressure $p_1$ ) . as the packet comes out of the other side of the plug it must displace a volume $v_2$ of gas , doing work $p_2 v_2$ . in your car tyre , example , $p_2 v_2$ is the work the outflowing gas does in lifting the atmosphere . in general , for real gases , $p_1 v_1$ will not equal $p_2 v_2$ , for the reasons explained in john rennie 's answer . it can be greater or lesser , depending on the properties of the gas and on the two pressures . this means that the energy lost as work on side 1 will not equal the energy gained as work on side 2 . this energy change must be compensated in order to satisfy the first law . since the gas can exchange neither heat nor work with its surroundings , the only other thing that can change is its internal energy . the first law implies that $u_2 - u_1 = p_1v_1 - p_2v_2$ , as wikipedia explains . it happens that for most gases at room temperature , $p_2v_2&gt ; p_1v_1$ , which implies that the gas 's internal energy ( and therefore its temperature ) must decrease as it goes through the plug . note that we did not assume the entropy stays constant , and in fact it increases : the entropy change associated with expanding the gas must be greater than the entropy change associated with reducing its temperature , otherwise the gas would not flow through the plug . in your car tyre example , the gas does indeed to work in lifting the atmosphere , but this is less than the work done by the gas inside the tyre forcing it out through the valve , and this is why the temperature has to decrease .
actually , you are right : the hubble constant is not really constant . at least , it is not constant in time . the reason it is called a constant is that , when edwin hubble originally compared the recession velocities of galaxies with their distances in 1929 , there was no reason to expect any particular pattern . after all , just a few years prior , people had thought there were no other galaxies . but what hubble found was that , except for a small amount of random variation , the velocities of galaxies were proportional to their distances ; in other words , the ratio $v/d$ was roughly the same for all galaxies he observed . the value of this ratio came to be known as hubble 's constant , $h_0 \equiv \frac{v}{d}$ , because it was constant from one galaxy to the next , rather than varying randomly as one might have guessed at the time . of course , it was not long before people realized that if the recessional velocity of each galaxy was proportional to its distance , you could extrapolate back to some point in the past at which $d = 0$: all the galaxies would have started out in the same place . this gives you an effective age for the universe . if you use a simple linear extrapolation , from basic kinematics you get $$t = \frac{d}{v} = \frac{1}{h_0}$$ so the hubble " constant " is not constant in time , but rather is inversely related to the age of the universe . as the universe gets older , the hubble constant gets smaller , as you would expect . this happens because the distance $d$ to any given galaxy increases with time . however , the fact that the universe has an age does not create an absolute time . sure , different observers at different points in spacetime will measure different values for the age of the universe . and sure , you could define a time coordinate system by specifying that the time coordinate for any observer is the age of the universe as measured by that observer . this is called the comoving time , and it is a useful and sensible way to set up a coordinate system in time . but it is not the only possibility , and there is certainly nothing so special about it that it deserves to be called " absolute . " any observer who is moving with respect to the universe as a whole ( i.e. . relative to the hubble flow ) would not measure time at the same rate as this comoving time .
this is because the classical system consisting of nucleons interacting with a realistic pair potential is chaotic . a classically chaotic system careens between unstable periodic orbits , which in a path-integralish view ( gutzwiller trace formula ) tells you that nearby energy levels are concentrated on completely different periodic orbits , so that they have mixed with each other strongly if you consider the unperturbed wavefunctions to be homogenous ( heller scarring ) the statistics of random matrix eigenvalues is based on the principle of heavy generic mixing between energy levels , leading to level repulsion . by contrast , classically integrable systems have energy levels which are regularly spaced , coming as they do from semiclassically setting the action variables to be integers . each action variable gives a smooth level spacing , and the spacings add up linearly , to produce a uniformish distribution of energy levels without level repulsion .
physically , if you look at the low level of cmb anisotropy and the bao power spectrum , you need some sort of mass that interacts gravitationally but is decoupled from the photons before recombination ( i.e. . - dark matter ) . otherwise there would be no way to seed the galaxy formation that we see occuring at lower redshifts . that is to say , dark matter needs to be primordial , so that it can dodge silk damping and provide the graviational " cores " around which galaxies form . which would also imply that dark matter cannot be the construct of an alien civilization .
here are the inverses of the running coupling constants : looking at the 1/coupling_constant it is true that the weak comes out as 1/30 to the 1/137 of the electromagnetic . in equation 5 of this document this factor is calculated , making as far as couplings go the weak four times stronger than the electromagnetic . but what makes the interaction so weak is the large mass of the relevant gauge bosons . the number given in hyper physics , ( and in the table of griffith* ) , compares decays of baryons to get the weak number of 10^-6 to the strong . this means that the propagator of the w boson that initiates the sigma decay is contributing to the measured weakness of the decay , reducing the effect of the coupling constant . so the 10^-6 is comparing lifetimes , and the 1/30 is the coupling constant in front of the matrix element that will calculate the lifetime . i found a pdf scan of griffiths which on page 55 has for strength : 10 10^-2 10^-13 10^-42 ( strong , em , weak , gravitational ) and cautions that strength depends on context ( particularly weak ) is quoted as different in other sources . what page is this 1/ 30 ? as it is a scan and not searchable .
this is a standard notation for the ( reducible ) representation the field transforms under . usually , the first number is the dimension of the representation for $su ( 3 ) _c$ . so a $\mathbf 1$ represents a lepton ( the one-dimensional representation is the trivial representation ) , a $\mathbf 3$ is a quark . in gut physics one needs the right-handed fields to transform like left-handed , so you the right-handed paricles by left-handed antiparticles . then a $\mathbf{\overline{3}}$ is an antiquark . the second number is the dimension of the $su ( 2 ) _l$ representation . a $\mathbf 1$ represents a right-handed field that does not interact via the $su ( 2 ) _l$ . a $\mathbf 2$ represents an isospinor . the third number is the hypercharge , not the electrical charge ! here we have a freedom to rescale the numbers , which is why there is no unique notation . there are two conventions , that differ by a factor of 2 . for the right handed fields , hypercharge and electrical charge coincide in one of the conventions , leading to the confusion in the comments to the question . the gell-mann nishijima formula that links hypercharge and electrical charge reads $$ q = i_3 + \left ( \frac{1}{2} \right ) y$$ where the factor of 1/2 is present , depending on the convention . $i_3$ is the third component of isospin , i.e. $i_3 = 0$ for right handed particles and $i_3 = \pm \frac{1}{2}$ for isospin 1/2 up ( + ) or down ( - ) states . the notation you gave does not use the factor of 1/2 that i put in brackets ! let 's take a look at the examples you gave : ( 1 , 1 , -1 ) : first 1 means that this state has no color charge . it is a lepton . the second 1 means it carries no weak isospin , i.e. it is right-handed . so it can be either the right handed electron or the right-handed neutrino . the electical charge is $q = 0 + ( -1 ) = -1$ so it clearly is the right handed electron ( 3 , 1 , 2/3 ) : the 3 means this is a quark ( comes in three colors ! ) . the second 1 again indicates a right-handed particle . its electric charge is $q = 0 + 2/3 = 2/3$ . we found the up quark and since this is fun , let 's do one more example : ( 3 , 2 , 1/6 ) : the $3$ indicates a quark . the 2 sais we are in the doublet ( so up and down ) . the 1/6 then tells us that the charges are $q = \pm 1/2 + 1/6 = 2/3$ or $-1/3$ , so exactly what we had expect for a pair of quarks .
to clear up your confusion about needing energy to add to the rotation of disk 1 , let 's take n easier linear problem for a second . imagine you are standing with your back against a solid wall , and you push for time $t$ with a force $f$ against an initially stationary mass $m$ . you imparted momentum $ft$ and the work you did was $ ( ft ) ^2: ( 2m ) $ . imagine doing the same experiment in orbit . now you would need to push against two masses - one in one direction and one in the opposite direction . in effect you are doing twice as much work - the two masses are separating twice as fast so you apply your force over twice as much distance and do twice the work . when you let go you have two masses with the same kinetic energy , so all is conserved . the same is true in rotation - for force , read torque ; for velocity read angular velocity , etc . it is harder to visualize but the underlying physics is the same : if just push off against something that is moving you have to do more work than of the something was fixed . the degree to which you do more work depends on the relative mass : the lighter your " thing you push against " the more work you have to do . a reason why the baseball pitcher firmly plants his feet before throwing . . . ?
elaborationg on david 's comment : you have said that " our world " particles might annihilate into your " dark photons " . but then we can apply crossing rule to the annihilation process and arrive at some more processes : annihilation of " dark photons " back into " our world " particles . scattering of " dark photons " off " our world " particle . so your " dark photons " are not so dark in the end . . .
ac or dc , you only get electrocuted if current passes through your body . ( current passing through any part of your body can be dangerous , and possibly cause an electrical burn , but current passing across your heart is the one that is really dangerous . ) touching just one wire at a time gives the current nowhere much to go . you are right to think that some electrons can get stripped from your body when you touch a bare wire . but not many . once they have gone , unless your body gets new electrons from somewhere else , the current stops . if you are standing in a pool of water , or touching a metal pole , or another wire that can conduct lots of electrons from somewhere else , you are fried . so how many electrons get conducted away from the human if it has no other source of electrons ? in this case , the human acts as a capacitor . now , wikipedia tells me that one standard for this approximates a human as having capacitance $c=100\ , \mathrm{pf}$ . this is pretty tiny . ( for comparison , the capacitors you might see if you open up a computer or other electronics can easily have capacitance billions of times larger . ) if the voltage is $v=100\ , 000\ , \mathrm{v}$ ( which is really quite high ) , the charge that would be transferred is $q = c\ , v = 10^{-5}\ , \mathrm{c} \approx 10^{14}$ electrons . you probably have $10\ , 000$ times more electrons in a single eyelash , so that is not many . the danger there is that at such high voltages , a lot of things become conductive that are not normally . p.s. most electrical wires that you see in a city ( though not those for trams , or the really big high-voltage lines ) are actually insulated . ( i worked as an electrician through college , and have touched many such wires . ) so the birds are not frequently touching anything dangerous to begin with . but even if they were , they had only be in danger if they had somewhere to get new electrons . there are plenty of places ( especially around those big transformers you see ) where you can find exposed power , though , so be careful .
consider a changing vector $\vec{a} ( t ) $ on a fixed coordinate system . the rate of change of the vector components are $$\frac{{\rm d}\vec{a} ( t ) }{{\rm d}t} = \frac{\partial \vec{a} ( t ) }{\partial t}= \dot{\vec{a}} ( t ) $$ consider a fixed vector $\vec{a}$ on a rotating coordinate system , with angular velocity $\vec\omega ( t ) $ . the rate of change of the vector components with respect to the attached frame are zero , but with respect to an inertial frame the rate of change is $$\frac{{\rm d}\vec{a} ( t ) }{{\rm d}t} = \vec{\omega} ( t ) \times \vec{a} $$ combined the changing $\vec{a} ( t ) $ vector on a rotating frame $\vec\omega ( t ) $ is $$\boxed{ \frac{{\rm d}\vec{a} ( t ) }{{\rm d}t} = \frac{\partial \vec{a} ( t ) }{\partial t} + \vec{\omega} ( t ) \times \vec{a} ( t ) }$$ the first part is the intrinsic change along the rotating frame , and the second part the change due to the rotation of the coordinates . example two bodies are connected with a slider joint , defined by an axis $\vec{e}$ fixed to the first body which is rotating by $\omega ( t ) $ . if the joint distance is $\chi ( t ) $ then the position of the 2nd body is defined relative to the first body as $$\vec{r}_2 ( t ) = \vec{r}_1 ( t ) + \vec{e}\ , \chi ( t ) $$ the position vectors are differentiated to derive the velocity kinematics as : $$ \vec{v}_1 = \frac{{\rm d}\vec{r}_1 ( t ) }{{\rm d}t} \\ \vec{v}_2 = \frac{{\rm d}\vec{r}_2 ( t ) }{{\rm d}t} $$ $$\begin{align} \vec{v}_2 and = \vec{v}_1 + \frac{{\rm d} ( \vec{e}\ , \chi ( t ) ) }{{\rm d}t} \\ and = \vec{v}_1 + \frac{\partial \vec{e} \chi ( t ) }{\partial t} + \vec{\omega} ( t ) \times \vec{e} \ , \chi ( t ) \\ and = \vec{v}_1 + \vec{e} \dot\chi ( t ) + \vec{\omega} ( t ) \times \left ( \vec{r}_2 ( t ) - \vec{r}_1 ( t ) \right ) \end{align}$$ see related answer : derivation of euler&#39 ; s equations for rigid body rotation
this is a link with the conclusions from the experiment and which observations lead to which conclusions . the conclusion when rutherford mathematically investigated the results he proposed a model that explained the results that geiger and marsden obtained . the fact that the vast majority of the alpha particles got straight through led rutherford to propose that the atom was composed primarily of empty space . the fact that backscattering occurred in 1 in 8000 alpha particles indicated that the nucleus was : small ( that was why so few were affected ) massive ( meaning containing lots of mass - he knew the electrons had very little mass and the fact that all of the positive charges were concentrated into a small area meant that the mass was concentrated there too ) positively charged ( because it repelled the alpha particles ) nucleus in the centre of the atom ( neutrons had not been discovered at that time - so he made no mention of them ! ) . all these are from the website linked .
we can certainly use the sun to heat things up hotter than sun , such as by using solar cells to generate electricity and using that to run a furnace . however to ensure that entropy increases we must perform additionally perform an irreversible process . otherwise we are simply taking heat out of a cold body and moving it to a hot body . optics are a reversible process , and therefore you cannot use them alone to make a heat pump . the reversibility manifests itself in the fact every optical path from the sun to the object can be traversed backwards . recall as well that the probability to absorb and the probability to emit must be related by thermodynamics . therefore once you object reaches the same temperature as the sun your object must be emitting as much radiation at the sun as the sun is emitting at you object . so its temperature cannot increase past the temperature of the sun . note that the sun is not a perfect blackbody . therefore , one may be able to heat a furnace slightly beyond the usual stated " surface temperature of the sun " . although this would all be practically indistinguishable , the correct quantity to use is the entropy of the radiation .
let 's put a more precise description to the other answers , particularly neil 's . first , note that there is a gauss law for static gravitational fields , owing to the inverse square nature of the static gravitational attraction . see this answer here and note that the argument it makes uses only the inverse square dependence . ( actually , the gauss law also holds for dynamic gravitational fields in the approximation to general relativity called gravitoelectromagnetism , but that is another story ) . so now we apply the integral form of gauss 's law to the earth , whose mass distribution is very nearly perfectly axisymmetric , i.e. depends on the distance $r$ from the earth 's centre . thus , by symmetry and gauss 's law , we know that the gravitational field at a distance $r$ from the centre is the same as that arising from a point mass whose mass equals the total mass enclosed within a sphere of radius $r$ . thus if the density as a function of radius $r$ is $\rho ( r ) $ we have : $$g ( r ) = 4\ , \pi\ , \int_0^r \ , u^2 \ , \rho ( u ) \ , \mathrm{d} u\ , \frac{g}{r^2}$$ where the field is of course always directed towards the earth 's centre . now the form of $\rho ( r ) $ is highly nontrivial , being determined by the different materials at different depths and the response of that material to pressure as described by e.g. the adams-williamson equation . but if we idealise the earth so that $\rho ( r ) = \rho_0$ we get : $$g ( r ) = \frac{4}{3}\pi\ , g\ , \rho_0\ , r$$ so that if we , like alice , dropped through an ideal diametrical tunnel through the earth we would undergo simple harmonic motion with : $$\ddot{r} = - \frac{4}{3}\pi\ , g\ , \rho_0\ , r$$ or , in terms of earth radius $r_\oplus$ and the value of $g_\oplus= 9.81{\rm m s^{-2}}$ at the earth 's surface : $$\ddot{r} = - g_\oplus\frac{r}{r_\oplus}$$ so that our period is : $$t = 2\pi\sqrt{\frac{r_\oplus}{g_\oplus}} \approx 5\ , 075{\rm s}$$ and it would take us about 21 minutes to fall to the centre of the earth , whence we would keep going to the other side , and then fall back and forth sinusoidally with time .
question : why do accelerating charges radiate , and can moving charges with zero acceleration also produce electromagnetic radiation ? quick answer : only accelerating charges ( and changing currents ) produce electromagnetic radiation . the power of fields generated by charges with zero acceleration dies out at far distances . full answer : the full answer to this question involves deriving the electric and magnetic fields for a point charge with nonzero velocity and acceleration . it can be shown that the electromagnetic power far away from the source charge is only nonzero if the charge is accelerating . electrostatics in physics we can always find the rest frame of the particle ( i.e. . , the frame in which the particle appears stationary ( $\textbf{v} = \textbf{0}$ and $\textbf{a} = \textbf{0}$ ) . in this frame where the particle is located at $\textbf{r}'$ , we can calculate the electric and magnetic fields with the mathematical machinery of electrostatics . assuming no external sources , the electric field ( also known as the coulomb field ) is given by $$ \textbf{e} ( \textbf{r} , t ) = \frac{q}{4 \pi \epsilon_0}\frac{\textbf{r} - \textbf{r}'}{|\textbf{r} - \textbf{r}'|^3} , $$ and the magnetic field is $$ \textbf{b} ( \textbf{r} , t ) = 0 . $$ electrodynamics however , for an arbitrary frame of reference , neither the velocity $\textbf{v}$ nor the acceleration $\textbf{a}$ are required to be $\textbf{0}$ . in these frames , the easiest way to calculate the fields is by first finding the retarded potentials . without going into too much detail , these are electric ( $\phi$ ) and magnetic ( $\textbf{a}$ ) potentials , in the lorentz gauge condition , generated by the particle at the retarded time $$ t_r = t - \frac{1}{c}|\textbf{r} - \textbf{r}'| . $$ the retarded time is used due to the fact that it takes a finite amount of time for the $\textbf{e}$ and $\textbf{b}$ fields to respond to the charge 's motion ( thanks to the finite speed of light , $c$ ) . for convenience , we will introduce similar notation to griffiths$^1$ . let $\textbf{r} \equiv \textbf{r} - \textbf{r}'$ and let $\textbf{u} \equiv c \frac{\textbf{r}}{r} - \textbf{v}$ . the resultant fields are : $$ \textbf{e} ( \textbf{r} , t ) = \frac{q}{4 \pi \epsilon_0} \frac{r}{ ( \textbf{r} \cdot \textbf{u} ) ^3} [ ( c^2 - v^2 ) \textbf{u} + \textbf{r} \times ( \textbf{u} \times \textbf{a} ) ] , $$ $$ \textbf{b} ( \textbf{r} , t ) = \frac{1}{c}\frac{\textbf{r}}{r} \times \textbf{e} . $$ electromagnetic radiation electromagnetic radiation , as defined in griffiths$^1$ , is the energy transfer ( power ) of the fields far from the source ( s ) . informally , it is the electromagnetic power that " survives at infinity . " mathematically , we can define the radiated power as $$ p_{radiation} = lim_{r \to \infty} p ( r ) , $$ where $p ( r ) $ is the flux of the the poynting vector $\textbf{s}$ through a sphere of radius $r$ . we will consider the following argument for why only accelerating charges produce radiation . holding $\textbf{s}$ constant , we see that the flux increases as $r^2$ ( due to the surface area of the sphere ) . for $p \neq 0$ , we require that $s$ falls off as $1/r^2$ or less . for a point charge , $s \propto e^2$ , so we are left with the requirement that the electric field decays no faster than $1/r$ . scanning the equation for the generalized electric field , we see that only the second term $$ \textbf{e}_{r} = \frac{q}{4 \pi \epsilon_0} \frac{r}{ ( \textbf{r} \cdot \textbf{u} ) ^3} [ \textbf{r} \times ( \textbf{u} \times \textbf{a} ) ] , $$ decreases slowly enough ( the other term falls off as $1/r^2$ ) . this is due to the presence of $\textbf{r}$ ( $\equiv \textbf{r} - \textbf{r}'$ ) in the cross product . this term is known as the radiation field or as the acceleration field because it is the term responsible for radiation and is proportional to the charge 's acceleration . the other term , known as the generalized coulomb field , dies out at large distances for all times . thus , only an accelerating charge is responsible for electromagnetic radiation ( or a changing current , which is composed of accelerating charges ) . visualizing radiation here 's a picture$^2$ of a charge , initially at rest , that accelerated quickly to some constant speed $v$ . the circular curve sketched by the apparent discontinuity in the field lines is the front of the electromagnetic wave , which is traveling at a speed $c$ . the electric field inside the circle is the " new " field produced by the charge moving at $v$ . the outside field is the " old " field that was generated when the charge was at rest . over time , the outside field is replaced by the new field . references : introduction to electrodynamics - david j . griffiths http://i.stack.imgur.com/puvzx.gif
the following answer is , i hope , at least plausible-sounding . it is by no means a proof , and ultimately you do need the equations to make a convincing argument , because not all plausible sounding arguments end up being correct when put to the test . the fundamental idea is that r2 is limited in its ability to set the total current in the circuit because the current will always have to travel through r1 . the answer is easiest to understand when r2 is tiny compared to r3 . then r2 acts to leech away current from r3 because it provides a path of lesser resistance . even though a small value of r2 will cause more current to flow through the entire circuit than if r2 were larger , the current can never get too large because it always has to travel through r1 first . on the other hand , the ability for r2 to leech current away from r3 is not really limited by anything . so it should seem reasonable that removing r2 in this case would increase the current flowing through r3 . the same argument basically applies for larger values of r2 , but the amount by which the current in r3 increases when you remove r2 will get smaller and smaller if you consider larger and larger values of r2 . in the end , you are always restoring more current to r3 that had been trickling through r2 than the amount of current you remove from the circuit by removing r2 . this comes back to the idea that r2 is limited in its ability to set the total current in the circuit .
note that a central idea of special relativity is that you can not define a frame of reference with respect to anything but another frame of reference . just keep in mind that this does not make the space any less general . provided that $ ( v_i ) $ is an acceptable basis , this is precisely the correct definition . lets take $p= ( 0,0,0,0 ) $ , where the coordinates are like $ ( ct , x , y , z ) $ . then take $v_1= ( \gamma , \beta \gamma , 0,0 ) $ , $v_2= ( \beta \gamma , \gamma , 0,0 ) $ , $v_3= ( 0,0,1,0 ) $ , $v_4= ( 0,0,0,1 ) $ . then , since $p$ is zero , we can let $ ( v_i ) $ be a nonaffine linear transformation . as usual with a change of basis ( "the new coordinates for a column vector v are given by the matrix product $m^{-1}v$" ) , the matrix that will change our basis is the inverse of $m= ( v_1 , v_2 , v_3 , v_4 ) $ where each basis vector is a column vector , or : $m^{-1}=\begin{pmatrix}\gamma and -\beta \gamma and 0 and 0\\-\beta \gamma and \gamma and 0 and 0\\0 and 0 and 1 and 0\\0 and 0 and 0 and 1 \end{pmatrix}$ the usual lorentz transform . ( in general an acceptable basis $v_i$ would be the composition of a rotation and a lorentz transformation ) so what gives us the right to say this represents a frame moving with respect to another ? if a particle in coordinate system $ ( v_i ) $ is at position $s= ( ct ' , 0,0,0 ) $ , then its position in the standard basis would be $ms= ( ct'\gamma , ct'\beta\gamma , 0,0 ) = ( ct , x , y , z ) $ . so since the position is linear , to find its velocity we find : $\frac{x}{t}=\frac{ct'\beta\gamma}{t'\gamma}=c\beta$ . ( i calculated $\frac{x}{t}$ because its meaning is physically clearer than $\frac{x}{c t}$ . ) therefore the basis $ ( v_i ) $ given represents a coordinate system moving at velocity $\beta c$ with respect to the current coordinate system .
motion may be relative , but only in certain reference frames will newton 's second law , $$ \vec{f} = m \vec{a} , $$ hold without having to invoke ad hoc forces $\vec{f}$ . in this case , suppose we did look at everything in the reference frame where the earth is not moving . then the sun would be moving in a giant circle , $1\ \mathrm{au}$ in radius , every year . but since even before newton we have known that objects should just move at the same speed and in the same direction unless acted upon by an external force . uniform circular motion requires something to be accelerating you ( $\vec{a}$ ) all the time . what , then , is acting on the sun to accelerate it ? since $m$ is so large for the sun , you need quite a large $\vec{f}$ to make the necessary $\vec{a}$ . the earth 's gravity will not suffice . in fact , the " force " here is known as centrifugal force , and it is " fictitious " in the sense that a better choice of reference frame would eliminate it . so yes , you can say that the sun moves around the earth , but only subject to the caveat that you are not talking about an inertial frame . instead , that statement only holds in a reference frame where objects experience some centrifugal " force , " which does not come from any fundamental interaction like gravity or electromagnetism .
maybe your intuition about energy and temperature need to be revisited . your system can exchange energy with the reservoir at a given temperature . the system+reservoir will iterate through all microstates with equal probability ( total energy being fixed ) , but you can show by using entropy arguments , that the probability of the system being in a state with energy $e$ is given by your first equation . the average energy of your system is then $\langle e \rangle = \sum_i e_i p ( e_i ) $ . that is not the same as the most probable energy , which is $e_0$ . when deriving the boltzmann factor from the reservoir argument , there are corrections to the factor when the reservoir is finite . you can write , for the system in uniquely labelled states $i$ and $j$ , $\frac{p ( i ) }{p ( j ) } = \frac{\omega_r ( i ) }{\omega_r ( j ) }$ where $\omega_r ( i ) $ is the number of microstates of the reservoir when the system is in state $i$ . writing this in terms of entropy gives a more fundamental form $\frac{p ( i ) }{p ( j ) } = e^{-\frac{s_r ( i ) - s_r ( j ) }{k_b} }$ if you label the states of the system by energy , and allow it to be continuous , then you can taylor expand $s_r ( e_i ) $ around $e=0$ , since by assumption the energy of the reservoir is vastly bigger than that of the system . changing variables to the energy of the reservoir $u_{res}$ , the linear term is $-\frac{\partial s_r}{\partial u_{res}} e_i = -\frac{e_i}{t}$ , from which the bolzmann factor follows .
first of all , photons are not a pure form of energy . they are particles without rest mass , which means that they travel at the speed of light . energy is a property of physical systems , the statement that something is energy makes no sense . to answer your question : yes , electromagnetic radiation/photons contribute/s to the curvature of spacetime and therefore to gravity . the einstein equations of general relativity related the curvature of spacetime to the stress-energy tensor , which contains not only mass but also energy . this also includes the energy of a photon .
the covariant hamiltonian version of relativistic classical or quantum mechanics of a single particle is just like the nonrelativistic one , with time replace by eigentime ; see , e.g. , thirring 's mathematical physics course . a covariant hamiltonian version of relativistic classical field theory is the multisymplectic formalism ; see , e.g. , http://arxiv.org/pdf/math/9807080 http://lanl.arxiv.org/abs/1010.0337 a covariant hamiltonian version of relativistic quantum field theory is the tomonaga-schwinger formalism ; see , e.g. , http://arxiv.org/pdf/gr-qc/0405006 http://arxiv.org/pdf/0912.0556 http://sargyrop.web.cern.ch/sargyrop/sdesummary.pdf
no for " dry " , yes for " wet " . for " dry friction " , such as a box on a floor , it is relatively constant . why is this ? most objects are microscopically rough with " peaks " that move against each-other . as more pressing force is applied , the peaks deform more and the true contact area is increases proportionally . the surfaces adhere forming a bond that will take a certain amount of shear force to break . since the molecules are moving much faster ~300m/s than the box ( due to thermal vibrations ) velocity will not affect how many molecules adhere ( with the exception of " static friction" ) . however , static friction is sometimes be higher , in one explanation because the peaks have time to settle and interlock with each-other . neglecting static friction , force is constant . the simplest case in wet friction is two objects separated by a film of water . in this case there is zero static friction , as the thermal energy is sufficient to disrupt any static , shear-bearing water molecule structure . however , water molecules still push and pull on each-other , transferring momentum from the top to the bottom . the rate of momentum transfer i.e. " friction " grows in proportion to how much momentum is available , which in turn grows with velocity . thus , force is linear with velocity . however , interesting things happen when the bulk mass of the water gets important . in this case , bumps , etc on the surface push on the water creating currents that can ram into bumps on the other surface . if you double the velocity , your bumps will push twice as much water twice as fast for 4 times the force ; force is quadratic to velocity . you can plug in formulas for the linear case ( which depends on viscosity ) and quadratic case ( which depends on density ) to see which one " wins " ( this is roughly the reynolds number ) , if there is no clear winner the answer is complex ( see the moody diagram ) . nevertheless these are approximations and the real answer could fail to follow these " rules " .
so in short plasma does not burn green in a chemical sense . it can rather be quite colorful as can be seen in this picture from the remains of a supernova explosion : if you have an oxygen plasma for example this can be used to " burn " organic compounds as is often used in the semiconductor industry to clean wafers . the plasma in this case is already at such high energies that it will immediately react with anything that can be oxidized . so you cannot ignite it even further , it is already quite reactive . if the plasma itself should burn we have to define burning differently . instead of a chemical reaction nuclear reactions are also possible . to ignite such a nuclear reaction in a plasma you need quite special conditions that exist for example in the sun . these processes are quite different from a chemical burning though and involve reactions at much higher energies .
actually given that the first postulate says that all physical laws are the same in all inertial frames , you could replace the second postulate by the postulate : " maxwell 's equations are the physical laws for electromagnetism " . from maxwell 's laws you can derive that the speed of light in vacuum has a specific , constant value , in si units $c=1/\sqrt{\epsilon_0\mu_0}$ . now there are three possibilities : maxwell 's laws are valid only in a specific inertial frame ( or rather , in a specific set of inertial frames at rest relative to each other ) . that is the essence of the aether hypothesis . it would violate the first postulate . also , experiments failed to measure that preferred frame . maxwell 's laws are not the correct description of electromagnetism ( that is , they are valid in no inertial frame ) . that option would , of course , have been compatible with the first postulate , but not very likely , given the huge experimental support for maxwell 's equations . maxwell 's laws are valid in all inertial frames . if that is the case , then all of the consequences of maxwell 's equations have to be valid in all inertial frames . one of the consequences of maxwell 's equations is the value of the speed of light in vacuum . however , it turns out that the only thing from maxwell 's equations you actually need in order to derive special relativity is the constant speed of light . therefore it makes sense to postulate that directly ; that way even if it turned out that maxwell 's theory had to be revised , you do not need to revise relativity as long as the revised theory still predicts a constant speed of light .
the angular resolution of a telescope is approximated by the formula : $$\sin \theta \approx\theta\approx1.220\frac{\lambda}{d}$$ so , if we know the angle , we can calculate the diameter $d$ . the rest really depends on how big the flag is and where on moon it has been planted . assuming 50$\text {cm}$ as the diameter of the flag , and assuming the flag is planted somewhere perpendicular to earth 's view line , and also doing it when moon is in its perigee ( $d=363,295\text{km}$ ) ; the diameter would be ( for visible blue light ) : $$d\approx\frac{1.22 \lambda}{\theta}\approx\frac{1.22 \times450\text{nm}}{\frac{50\text{cm}}{363295\text{km}}}\approx400\text{m}$$ if you do not trust my numbers , you may look at this link .
in physics , it is often implicitly assumed that the lagrangian $l=l ( q^i , v^i , t ) $ depends smoothly on the ( generalized ) positions $q^i$ , velocities $v^i$ , and time $t$ , i.e. that the lagrangian $l$ is a differentiable function . let us now assume that the lagrangian is of the form $$l~=~\ell ( v^2 ) , \qquad\qquad v~:=~|\vec{v}| , \qquad\qquad ( 1 ) $$ where $\ell$ is a differentiable function . the equations of motion ( eom ) become $$ \vec{0}~=~\frac{\partial l}{\partial \vec{q}} ~\approx~\frac{d}{dt}\frac{\partial l}{\partial \vec{v}} ~=~\frac{d }{dt} \left ( 2\vec{v}~\ell^{\prime}\right ) ~=~2\vec{a}~\ell^{\prime}+4\vec{v}~ ( \vec{a}\cdot\vec{v} ) \ell^{\prime\prime} . \qquad\qquad ( 2 ) $$ ( here the $\approx$ symbol means equality modulo eom . ) we conclude that on-shell $$\vec{a} \parallel \vec{v} . $$ ( the words on-shell and off-shell refer to whether eom are satisfied or not . ) therefore by taking the length on both sides of the vector eq . $ ( 2 ) $ , we get $$ 0~\approx~2a ( \ell^{\prime}+2v^2\ell^{\prime\prime} ) , \qquad\qquad a~:=~|\vec{a}| . $$ this has two branches . the first branch is that there is no acceleration , $$ \qquad \vec{a}~\approx~\vec{0} , \qquad\qquad ( 3 ) $$ or equivalently , a constant velocity . the second branch imposes a condition on the speed $v$ , $$\ell^{\prime}+2v^2\ell^{\prime\prime}~\approx~0 . \qquad\qquad ( 4 ) $$ to take the second branch $ ( 4 ) $ seriously , we must demand that it works for all speeds $v$ , not just for a few isolated speeds $v$ . hence eq . $ ( 4 ) $ becomes a 2nd order ode for the $\ell$ function . the full solution is precisely op 's counterexample $$l~=~ \ell ( v^2 ) ~=~\alpha \sqrt{v^2}+\beta~=~\alpha v+\beta , $$ where $\alpha$ and $\beta$ are two integration constants . this is differentiable wrt . the speed $v=|\vec{v}|$ , but it is not differentiable wrt . the velocity $\vec{v}$ at $\vec{v}=\vec{0}$ if $\alpha\neq 0$ , and therefore the second branch $ ( 4 ) $ is discarded . thus the eom is the standard first branch $ ( 3 ) $ . firstly , the definition of form invariance is discussed in this question . concretely , landau and lifshitz mean by form invariance that if the lagrangian is $$l~=~\ell ( v^2 ) \qquad\qquad ( 5 ) $$ in the frame $k$ , it should be $$l&#39 ; ~=~\ell ( v^{\prime 2} ) \qquad\qquad ( 6 ) $$ in the frame $k^{\prime}$ . here $$\vec{v}^{\prime }~=~\vec{v}+\vec{\epsilon}$$ is a galilean transformation . secondly , op asks if adding a total time derivative to the lagrangian $$l ~\longrightarrow~ l+\frac{df}{dt}$$ is the the only thing that would not change the eom ? no , e.g. scaling the lagrangian $$l ~\longrightarrow~ \alpha l$$ with an overall factor $\alpha$ also leaves the eom unaltered . see also wikibooks . however , we already know that all lagrangians of the form $ ( 5 ) $ and $ ( 6 ) $ lead to the same eom $ ( 3 ) $ . ( recall that acceleration is an absolute notion under galilean transformations . ) instead , i interpret the argument of landau and lifshitz as that they want to manifestly implement galilean invariance via noether theorem by requiring that an ( infinitesimal ) change of the lagrangian $$ \delta l~:=~l&#39 ; -l ~=~2 ( \vec{v}\cdot\vec{\epsilon} ) \ell^{\prime} $$ is always a total time derivative even off-shell . in general , how do we know if an expression $\delta l$ is a total time derivative ? well , one way is to apply the euler-lagrange operator on the expression , and check if it is identically zero off-shell . we calculate $$ \vec{0}~=~ \frac{d}{dt}\frac{\partial \delta l}{\partial \vec{v}} -\frac{\partial \delta l}{\partial \vec{q}} $$ $$~=~4\vec{\epsilon}~ ( \vec{a}\cdot\vec{v} ) \ell^{\prime\prime} +4\vec{v}~ ( \vec{a}\cdot\vec{\epsilon} ) \ell^{\prime\prime} +4\vec{a}~ ( \vec{v}\cdot\vec{\epsilon} ) \ell^{\prime\prime} +8\vec{v}~ ( \vec{v}\cdot\vec{\epsilon} ) ( \vec{a}\cdot\vec{v} ) \ell^{\prime\prime\prime} . \qquad\qquad ( 7 ) $$ since eq . $ ( 7 ) $ should hold for any off-shell configuration , we can e.g. pick $$ \vec{a}~\parallel~\vec{v}~\perp~\vec{\epsilon} . $$ then eq . $ ( 7 ) $ reduces to $$ \vec{0}~=~ 4\vec{\epsilon} ~ ( \pm a v ) \ell^{\prime\prime} . $$ we may assume that $\vec{\epsilon}\neq\vec{0}$ . arbitrariness of $a$ and $v$ implies that $$\ell^{\prime\prime}~=~0 . \qquad\qquad ( 8 ) $$ ( conversely , it is easy to check that eq . $ ( 8 ) $ implies eq . $ ( 7 ) $ . ) the full solution to eq . $ ( 8 ) $ is the standard non-relativistic lagrangian for a free particle , $$l~=~ \ell ( v^2 ) ~=~\alpha v^2+\beta , $$ where $\alpha$ and $\beta$ are two integration constants . for more on galilean invariance , see also this question .
light wavelengths are on a linear scale , but humans only measure " color " by the relative power in three regions of this linear spectrum . a whole gamut of relative weights of these three color components is possible . the possible colors ( hue and saturation , normalizing out intensity ) we can perceive can therefore be represented as a triangle in a plane , which is often simplified to a circle .
no , i think you are mistaken . entanglement cannot be used to transmit information . two distant experimenters each with one of two entangled electrons cannot communicate by performing measurements of their electrons . furthermore , entanglement does not imply that quantum mechanics is nonlocal , i.e. , that there is spooky , instantaneous action at a distance . if you follow the common copenhagen interpretation of the wavefuction , the wave function has no physical meaning prior to measurement . nothing passes between the electrons upon measurement . all that changes upon measurement is our state of knowledge .
if the particle moves from the point $x$ to $x+dx$ , and assume $dx\gt 0$ for simplicity , then its potential energy increases by $$ du = \frac{du}{dx}dx $$ well , it increases if $du$ is positive and decreases if $du$ is negative . so far i have only used the definition of the derivative – pure mathematics . however , the total energy is conserved . the sum of the kinetic energy and the potential energy $$ e = t + u = {\rm const} $$ is constant . it means that if the potential energy increases , the kinetic energy decreases , and vice versa . however , an increasing kinetic energy is exactly the situation when the force $f$ is positive ( directed in the same direction as the speed or $dx$ ) . in other words , the equation $$ du = \frac{du}{dx} dx $$ may be rewritten as $$ dt = -\frac{du}{dx} dx $$ because $t$ is effectively $-u$ , up to the constant whose differential is zero , but because the kinetic energy increases if the force and $dx$ have the same sign i.e. $$ dt = f\cdot dx $$ ( pushing a right-moving particle by a right-directed force accelerates the particle ; the expression above is the infinitesimal work ) , we may compare the two equations and see that $$ f = -\frac{du}{dx} . $$ so the sign effectively arises from the " anticorrelation " of the kinetic and potential energy ( along with the convention that all the terms are included in the total energy with the plus sign ; the convention that the kinetic energy is positive , and so on ) .
the difference between the higgs boson and the bosons of the three/four fundamental ( depending whether you include gravity as a quantized theory or not ) actions is that the latter are associated with gauge symmetries , while the higgs plays a role in spontaneous symmetry breaking . photons , w- and z-bosons , gluons and gravitons arise from the requirement that the theory should be gauge invariant , while the higgs field/boson does not . in this sense , the higgs field and the associated particle are not considered to form another fundamental force .
could someone tell me if this is some serious science or just a hoax ? this is definitely serious research and not a hoax . professor steven c . rand is a physics department faculty member at university of michigan . his research group studies ceramic materials using laser pulses . his publications , which are listed in the above links , include peer reviewed articles in mainstream journals such as journal of the optical society of america and journal of applied physics .
all mater we see and touch and manipulate macroscopically is composed of atoms , molecules and solids which are held together by intermolecular forces . all these forces are electromagnetic , i.e. any disturbance in the end of one end of a string cannot be transmitted faster than the velocity of light , and usually transmission is much slower ( acoustic , vibrations ) . therefore there cannot be any instantaneous transmission with any length string as you imagine .
suppose you move a small distance $\vec{dr}$ = ( $dx$ , $dy$ , $dz$ ) and you take a time $dt$ to do it . pre-special relativity you could say three things . firstly the distance moved is given by : $$ dr^2 = dx^2 + dy^2 + dz^2 $$ ( i.e. . just pythagorus ' theorem ) and secondly the time $dt$ was not related to the distance i.e. you could move at any velocity . lastly the quantities $dr$ and $dt$ are invarients , that is all observers will agree they have the same value . special relativity differs by saying that $dr$ and $dt$ are no longer invarients if you take them separately . instead the only invarient is the proper time , $d\tau$ , defined by : $$ c^2d\tau^2 = c^2dt^2 - dx^2 - dy^2 - dz^2 $$ in special relativity all observers will agree that $d\tau$ has the same value , but they will not agree on the values of $dt$ , $dx$ , $dy$ and $dz$ . this is why we have to talk about spacetime rather than space and time . the only way to construct laws that apply to everyone is to combine space and time into a single equation . you say : i am having a hard time understanding how changing space means changing time well suppose we try to do this . let 's change space by moving a distance ( $dx$ , $dy$ , $dz$ ) but not change time i.e. $dt$ = 0 . if we use the equation above to calculate the proper time , $d\tau$ , we get : $$ d\tau^2 = \frac{0 - dx^2 - dy^2 - dz^2}{c^2} $$ do you see the problem ? $d\tau^2$ is going to be negative so $d\tau$ is imaginary and has no physical meaning . that means we can not move in zero time . well what is the smallest time $dt$ that we need to take to move ( $dx$ , $dy$ , $dz$ ) ? the smallest value of $dt$ that gives a non-negative value of $d\tau^2$ is when $d\tau^2$ = 0 so : $$ c^2d\tau^2 = 0 = c^2dt^2 - dx^2 - dy^2 - dz^2 $$ or : $$ dt^2 = \frac{dx^2 + dy^2 + dz^2}{c^2} $$ if we have moved a distance $dr = \sqrt{dx^2 + dy^2 + dz^2}$ in a time $dt$ , the we can find the velocity we have moved at the dividing $dr$ by $dt$ , and if we do this we find : $$ v^2 = \frac{dr^2}{dt^2} = \frac{dx^2 + dy^2 + dz^2}{\frac{dx^2 + dy^2 + dz^2}{c^2}} = c^2 $$ so we find that the maximum possible speed is $v = c$ , or in other words we can not move faster than the speed of light . and all from that one equation combining the space and time co-ordinates into the proper time !
your reasoning is fine and indeed the band gap of silicon is $1.12$ ev , which is $43kt$ at room temperature , so thermal promotion of electrons from the valence to the conduction bands at room temperature should be negligable . the trouble is that it is exceedingly hard to get silicon so pure that there are no gap states , and while the conductivity of ( relatively ) pure silicon is about ten orders of magnitude lower than copper , it is still a lot higher than good insulators like glass .
simplified explanation : when you move fast enough , you create a " turbulent wake " . it is as though the column of air that you push out of the way ends up traveling at your speed . this requires energy proportional to $v^2$ since the kinetic energy of that column will be proportional to $\frac12\rho v^2$ . thus the work done needed per unit distance is proportional to this quantity , and you need a force proportional to this quantity . thus $f\varpropto v^2$ over a wide range of reynolds numbers .
$p = \frac{gm^2}{8\pi r^4}$ in the approximation of constant density . where $p$ is the pressure at the center , $g$ is the gravitational constant , $m$ is the mass of the planet and $r$ is the radius of the planet . see hydrostatic equilibrium and planetary differentiation pressure is maximal at the center of a planet .
your last expression is equals to zero , because the first term is symmetric in $\beta$ and $\nu$ , while $j_{\beta\nu}$ is antisymmetric in $\beta$ and $\nu$ .
for each $r&gt ; 0$ , the divergence of the magnetic field of the monopole is zero as you have already checked ; \begin{align} \nabla\cdot\mathbf b ( \mathbf x ) = 0 , \qquad \text{for all $\mathbf x\neq \mathbf 0$} \end{align} but what if we also want to find the divergence of this field at the origin ? after all , that is where the point source presumably sits . we might expect that there is some sense in which the divergence there should be nonzero to display the fact that there is a point source sitting there . the problem is that the magnetic field is singular there , and the divergence is therefore not defined there . however , in electrodynamics , we get around this by interpreting the fields not merely as functions $\mathbf e , \mathbf b:\mathbb r^2\to\mathbb r^3$ , namely ordinary vector fields in three dimensions , but as distributions ( aka generalized functions ) . as as it turns out , when we do this , there is a sense in which the magnetic field you wrote down has nonzero divergence at the origin ( in fact the divergence is " infinite " there ) . i will leave it to you to investigate the details , but the punchline is that you need something called the distributional derivative to perform the computation rigorously . physicists often perform the distributional derivative of the monopole field by " regulating " the singularity at the origin , but this is not necessary . whichever method you use , the result you are looking for is \begin{align} \nabla\cdot\frac{\mathbf x}{|\mathbf x|^3} = 4\pi\delta^{ ( 3 ) } ( \mathbf x ) \end{align} where $\delta^{ ( 3 ) }$ denotes the delta distribution in three euclidean dimensions . applying this to the magnetic monopole field , we see that its divergence corresponds to a magnetic charge density that looks like the delta distribution ; this is precisely the behavior expected to a monopole . addendum . since user physixxx has posted the procedure for proving the identity i claim above by using the regularization procedure to which i referred , i suppose i might as well show how you prove the identity when it is interpreted in the sense of distributions . a distribution is a linear functional that acts on so-called test functions and outputs real numbers . to view a sufficiently well-behaved function $f:\mathbb r^3\to\mathbb r$ as a distribution , we therefore need to associate a linear function $t ( f ) $ to it . the standard way of doing this is to define \begin{align} t_f [ \phi ] = \int _{\mathbb r^3} d^3x\ , f ( \mathbf x ) \phi ( \mathbf x ) \end{align} the delta distribution centered at a point $\mathbf a\in\mathbb r^3$ cannot be described as a distribution associated to a function $f$ in this way , instead , it is defined as \begin{align} \delta_{\mathbf a}^{ ( 3 ) } [ \phi ] = \phi ( \mathbf a ) \end{align} physicists will often write this as \begin{align} \delta_{\mathbf a}^{ ( 3 ) } [ \phi ] = \int_{\mathbb r^3}d^3 x\ , \delta^{ ( 3 ) } ( \mathbf x - \mathbf a ) \phi ( \mathbf x ) \end{align} as if there is a function that generates the delta distribution , even though there is not , because it makes formal manipulations easier ( but can sometimes get you into trouble ) . now , for the function \begin{align} h ( \mathbf x ) = \frac{\mathbf x}{|\mathbf x|^2} \end{align} i claim that if we use the expression $t_h$ with which to associate a distribution with $f$ , then , $t_h = -4\pi \delta_{\mathbf 0}$ . to prove this , if suffices to show that $t_h [ \phi ] = -4\pi\phi ( \mathbf 0 ) $ for all test functions $\phi$ . to do this , we note that \begin{align} t_h [ \phi ] and = \int_{\mathbb r^3} d^3 x\ , \left ( \nabla\cdot\frac{\mathbf x}{|\mathbf x|^3}\right ) \phi ( \mathbf x ) \\ and = \int_{\mathbb r^3} d^3 x\ , \nabla\cdot\left ( \frac{\mathbf x}{|\mathbf x|^3} \phi ( \mathbf x ) \right ) - \int_{\mathbb r^3} d^3 x\ , \frac{\mathbf x}{|\mathbf x|^3}\cdot \nabla\phi ( \mathbf x ) \end{align} the first integral vanishes because , by stoke 's theorem ( aka the divergence theorem in 3d ) , it is a boundary term , but in this case , the boundary is at infinity , and the thing of which we are taking the divergence is assumed to vanish rapidly at infinity ( this is part of the definition of test functions ) . for the second integral , we use spherical coordinates . in spherical coordinates , we can write \begin{align} d^3 x = r^2\sin\theta dr\ , d\theta\ , d\phi , \qquad \frac{\mathbf x}{|\mathbf x|^3} = \frac{\hat{\mathbf r}}{r^2} , \qquad ( \nabla\phi ) _r = \ \frac{\partial\phi}{\partial r} \end{align} combining these observations with algebraic some simplifications gives the desired result : \begin{align} t_h [ \phi ] and = - \int_0^{2\pi}d\phi\int_0^\phi d\theta\int_0^\infty dr \frac{\partial\phi}{\partial r} ( r , \theta , \phi ) = -4\pi \phi ( \mathbf 0 ) \end{align} in the last step we used the fundamental theorem of calculus , the fact that $\phi$ vanishes as $r\to\infty$ and the fact that when $r\to 0$ , the average value of a function over the sphere of radius $r$ becomes its value at the origin , namely \begin{align} \lim_{r\to 0} \frac{1}{4\pi}\int_0^{2\phi}d\theta\int_0^\pi d\phi\ , \phi ( r , \theta , \phi ) = \phi ( \mathbf 0 ) \end{align}
i am going to start with a very short answer , but will be happy to elaborate on any point you find confusing . steam tables for superheated steam are organized by pressure and temperature . in most examples i have seen , you look up the pressure first and then scan that row , column , or page for the relevant temperature . the entry for your temperature and pressure combination should give you things like specific volume , specific enthalpy , etc . the curve on the p-v and t-v diagrams in the solution is the saturation dome for water . below it , water is a liquid ; above it , water is a vapor ( steam ) . on the saturation dome water is a saturated vapor . the way to sketch a particular point on a p-v or t-v diagram if you do not have a numerical steam table handy and are given the " wrong " properties is to identify the correct isotherm on a p-v diagram ( you can see a small part of it drawn in your example p-v ) or the correct isobar on a t-v diagram ( also shown in the example ) . i have included a figure of isobars on a temperature-entropy ( t-s ) diagram ( sorry i did not have a t-v diagram handy ) and isotherms on a p-v diagram that i have used when teaching this material . in general , instructors do not give impossible problems on this topic . they either provide diagrams like the ones below ( except labeled ! ) when asking for a sketch so that you can find the correct isotherm ( -bar , -chor , etc . ) or they will accept any sketch that is close to correct .
if p is a point of the manifold then f at p is equal to ϕ∗f at ϕ ( p ) , since they are related by the tensor transformation law , and tensors are independent of coordinate choice . this is roughly true . initially , there is no meaning when one says that tensors at different tangent spaces are equal . however , the diffeomorphism induces an isomorphism between $t_p m$ and $t_{\phi ( p ) } m$ ( the isomorphism is nothing but the vector push forward ) . the two tensors are equal with respect to this isomorphism . i have a feeling that i am missing something crucial here , because this would seem to suggest that diffeomorphisms were isometries in general . . . this is actually true in a sense that is relevant . if $ ( m , g ) $ is a spacetime and $\phi \in \text{diff} ( m ) $ , then while there is no reason to think that $\phi$ is an isometry between $ ( m , g ) $ and itself , $\phi$ is always an isometry between $ ( m , g ) $ and $ ( m , \phi_{\star}g ) $ . this last point saves your concern about proper time . if $\gamma$ is a normalized timelike path between two events $a$ and $b$ , we can always consider $\phi \circ \gamma$ as a timelike path in $ ( m , \phi_{\star}g ) $ . you can check that the new path is normalized with respect to the new metric $\phi_{\star}g$ . the domains of the two paths are exactly the same so the proper time between $\phi ( a ) $ and $\phi ( b ) $ is the same as the original path 's proper time .
clipping to m means the maximum ( zero-topeak ) value does not exceed m . clipping is usually the reult of a nonlinear component which saturates beyond some point . for example , the dynamic gain of an amplifier ' flattens out ' to zero when the input signal exceeds its maximum . in practice , this does not occur instantaneously , but very quickly . similarly , the cone of a speaker is very light ( has low inertia ) so whilst it does not stop instantaneously , it stops very quickly . in any case , if the value does not exceed m , it is ' clipped to m ' . now the ' loudness ' of a sound signal is not determined by the peak voltage , but by the average power in the signal , which is proportional to the rms of the voltage . a square wave of a given peak magnitude will have a higher average power ( rms voltage ) than a sine wave with the same peak-to-peak magnitude . in a sense , the ' loudness ' of the sound is governed by the volume of air that is moved during every cycle , which is proportional to the area under the curve of the half-cycle , that is , the rms ( square root of the mean of the square ) of the voltage signal .
the standard nonperturbative way ( that provided rigorous constructions in 1+1d and 1+2d qfts ) is constructing the euclidean ( imaginary time ) field theory as a limit of lattice theories , and then using analytic continuation to real time via the osterwalder--schrader theorem . in 1+3d , there is so far no rigorous construction of an interacting qft , but neither is there a corresponding no-go theorem . in 1+1d , there are also lots of exactly solvable qfts , where the nonperturbative solution is obtained by the quantum inverse scattering method . http://en.wikipedia.org/wiki/quantum_inverse_scattering_method
as you are a mathematician , i will just discuss the physics prerequwisites . newtonian mechanics . you seem to know this . lagrangian mechanics . unarguably the most elegant formulation of all of classical mechanics . hamiltonian mechanics . an uglier , but equally useful ( almost ) , formulation . newtonian gravity . the theory of gravity as an inverse square field with the " charge " as the mass . maxwellian electromagnetism . a lorentz invariant formulation of em . formulated way before lorentz invariance and lorentz symmetry were ever thought of . special relativity ( the minkowskian formulation , of course , is needed here . ) . of course . as you are a mathematician , you had probably like wald , r . m 's general relativity , a mathematically rigorous textbook . though i do not like it as it is too rigorous . i prefer ludvigsen 's general relativity : a geometric approach . but that is not a book a mathematician would like .
i am not an expert in thermodynamics but i think the following is reasonable : when heat flows from a to b ( temperature $t_a$ and $t_b$ ) then you could theoretically do work - efficiency given by the ratio of temperatures . a peltier is an inefficient heat engine running in reverse ( a heat pump ) , and i thought the efficiency is the ratio between the work you need to do and the work you did do to move the heat . so taking water from 23 to 8 c with a heat sink at 31c means working against an average temperature difference of 15.5 c . if principle that should mean that you can move about 300 joules while expending only 15 ( 20x ) . in reality the peltier is at most 15% " efficient " , and can move only 45 joules for 15 joules expended . now your numbers . 250 ml is cooled by 15 degrees , requiring 15*250*4.2 ~ 15 kj in 20 minutes ( 1200 seconds ) or about 13 w of cooling power . a truly efficient heat pump could move that energy with 0.65 w . a peltier that is 15% efficient would require about 3w for the job . you are applying 24 w ( 4*6 ) to the peltier - this is 8x less than the most efficient . which seems reasonable .
regarding your second question , the requirement that the inner product space be complete is imposed to have available a number of important theorems , among which the spectral theorem is particularly important . after a bit of time , though , you might notice that there is a couple of fishy things in that hilbert space axiom . for one , we put a lot of stock in position and momentum eigenstates , i.e. delta-function and plane-wave wavefunctions , which are strictly speaking not inside the hilbert space . on the other hand , the normal hilbert spaces have a number of wavefunctions , such as $$\psi ( x ) =\frac{1}{\sqrt{\pi}}\frac{1}{\sqrt{1+x^2}}$$ which violate physical intuition in one way or another ( this one has infinite position dispersion ) . the resolution is an amendment to the hilbert space axiom in terms of rigged hilbert spaces .
much like the position itself , the velocity in quantum mechanics is not just a single number ; it is an operator with different probabilities of different outcomes that may result from the measurement of the velocity . the operator of velocity in the simplest quantum mechanical model is $$ v = p/m = -\frac{i\hbar}{m} \frac{\partial}{\partial x} $$ you may fourier-transform your wave function to the momentum representation and then you see different values of the momentum , and therefore velocity , and the probability densities of different values are given by $|\tilde \psi ( p ) |^2$ . if you consider a simple plane wave , $$ \psi ( x , t ) = \exp ( ipx/\hbar - iet /\hbar ) $$ then the operator $v$ above has an eigenstate in the vector above and the eigenvalue is $p/m$ . on the other hand , the phase velocity is given by $$v_p = \omega / k = \frac{e}{p} = \frac{pv}{2p} = \frac{v}2 $$ so the velocity of the particle is equal to twice the phase velocity , assuming that your energy ( determine the change of phase in time ) is only given by the non-relativistic piece , without any $mc^2$ . one may also calculate the group velocity of the wave $$ v_g = \frac{\partial \omega}{\partial k } = \frac{\partial e}{\partial p} = \frac{p}m = v$$ which is exactly the velocity of the particle . the advantage of this relationship is that it holds even in relativity . if $e=\sqrt{p^2+m^2}$ , then the derivative of $e$ with respect to $p$ is $1/2e\cdot 2p =p/e = v$ which is exactly the right velocity , too . it is not too surprising because if a wave packet is localized , the group velocity measures how the " center of mass " of this packet is moving but the packet 's position coincides with the particle 's position , so the two velocities must be equal .
i think it is a great question , and enjoyed it very much when i grappled with it myself . here 's a picture of some of the forces in this scenario . $^\dagger$ the ones that are the same colour as each other are pairs of equal magnitude , opposite direction forces from newton 's third law . ( w and r are of equal magnitude in opposite directions , but they are acting on the same object - that is newton 's first law in action . ) while $f_{matchbox}$ does press back on my finger with an equal magnitude to $f_{finger}$ , it is no match for $f_{muscles}$ ( even though i have not been to the gym in years ) . at the matchbox , the forward force from my finger overcomes the friction force from the table . each object has an imbalance of forces giving rise to acceleration leftwards . the point of the diagram is to make clear that the third law makes matched pairs of forces that act on different objects . equilibrium from newton 's first or second law is about the resultant force at a single object . $\dagger$ ( sorry that the finger does not actually touch the matchbox in the diagram . if it had , i would not have had space for the important safety notice on the matches . i would not want any children to be harmed because of a misplaced force arrow . come to think of it , the dagger on this footnote looks a bit sharp . )
your problem information : to keep the math simple , let 's say you can jump one meter off of solid ground , and also that the bridge is ten meters above the ground . i will not go into the qualifiers ( others already have ) , so we will take your formulation as absolute . but the problem information is not useful in this form ( as a height ) . it would be more helpful to get the velocity with which you jump off . $$ h = \frac{1}{2} g t^2 \\ v_j = g t =g \sqrt{\frac{2 h}{g}} = \sqrt{ 2 h g} = 4.427 \frac{m}{s} $$ that is the upward velocity you impart to yourself . we are assuming this would be the same , no matter where it happens in the fall . now we can do the straightforward kinematics . it gets a little messy . here are some variables i will have to introduce : vj - the jump velocity , found above v ' - the velocity of the person/bridge system right before the jump happens t ' - the time at which the jump happens , counted from when the bridge starts falling h - the height of the bridge h ' - the height at which the jump happens - independent variable . vf - velocity when hitting the ground . we expect this to be negative , indicating downward direction . t - the time taken for the second segment of the fall ( after the jump ) now you divide the fall up into two segments , and then it is kinematics plug and chug . $$ v ' = - \sqrt{ 2 ( h-h' ) g } \\ h ( t ) = h ' - \frac{ 1 }{ 2 } g t^2 + ( v ' + v_j ) t \\ v_f = v_j - g t $$ the second equation above is quadratic , so you will have to use the quadratic formula , and then plug into the 3rd equation . after that : $$ v_f =- \sqrt{ 2 g h - \sqrt{ 8 ( h-h' ) g } v_j + v_j^2 } $$ again , h ' is the independent variable , and we want to minimize the sign of v_f , preferably to less than what it would be if we did not jump . the bridge is 50 meters high . here is the plot . the red line is the final velocity if no jump is made . the blue line is the velocity given that you jump at some given height ( x-axis ) . we see that even if you jump at the worst point ( right as the bridge breaks ) , your final velocity will not increase by your jump velocity . this makes sense , because the system satisfies the energy balance , and not impulse balance , so there is a non-linear sensitivity . if you jump right before you hit the ground , obviously you have subtracted the jump velocity from your final velocity and this is the most effective point . there is only a small window where jumping hurts you , but this is in the height domain , so that is expected . if the independent axis was time , it would be a much larger window .
the universe must contain a finite energy - sum of all matter and fields - or the mass-equivalent would collapse within its own gravitation . said mass-energy is fractionally partitioned among elementary particles and their agglomerates . one then strongly expects there are a finite number of particles including extremely low energy photons and neutrinos .
here we will only discuss the case of finite-dimensional irreducible representations ( irreps ) of a complex semisimple lie algebra $l$ . recall that the set $z$ of casimir invariants is the center $z ( u ( l ) ) $ of the universal enveloping algebra $u ( l ) $ , cf . e.g. this phys . se post . op 's question is answered without proof on p . 253 in ref . 1: theorem 2 . for every semisimple lie algebra $l$ of rank $r$ , there exists a set of $r$ invariant polynomial of generator $t_a$ , whose eigenvalues characterize the finite-dimensional irreducible representations . ref . 2 ( which is one the most important books on lie algebras , at least if one is interested in the proofs ) does not bother to mention theorem 2 explicitly . however , it is possible to string together a set of more fundamental results ( and their proofs ) from ref . 2 to get the sought-for result . we outline the proof strategy below . recall furthermore that there is associated a root system $\phi$ to the lie algebra $l$ , and let us imagine that we have picked a base $\delta$ for $\phi$ . the order $|w|$ of the weyl group $w$ is equal to the possible choices of ( unordered ) bases and equal to the possible choices of ( fundamental ) weyl chambers . it is proven in chapters 20-21 of ref . 2 . that a finite-dimensional irrep has a unique highest weight vector ( unique up to normalization ) with some dominant integral weight $\lambda$ . we will from now on denote such irrep $v ( \lambda ) $ . ( ref . 2 . also defines a notion of a highest weight irrep $v ( \lambda ) $ when $\lambda$ is integral but not dominant . such irreps are necessarily infinite-dimensional , so we will ignore those . ) it follows that two irreps $v ( \lambda ) $ and $v ( \mu ) $ are equivalent ( i.e. . isomorphic ) iff their highest weights are equal $\lambda=\mu$ . as a consequence of harish-chandra 's theorem , the set $z$ of casimirs takes the same value on two highest weight irreps $v ( \lambda ) $ and $v ( \mu ) $ iff $\lambda+\delta$ and $\mu+\delta$ belong to the same weyl orbit , $$ \sigma ( \lambda+\delta ) ~=~\mu+\delta , \qquad \sigma \in w . $$ here $\delta$ is half the sum of the positive roots . however if both integral weights $\lambda$ and $\mu$ are dominant , then $\lambda+\delta$ and $\mu+\delta$ must both belong to ( the interior of ) the fundamental weyl chamber , so that the weyl reflection $\sigma={\bf 1}$ must be the identity element . in conclusion , we get that the set $z$ of casimirs takes the same value on two finite-dimensional irreps $v ( \lambda ) $ and $v ( \mu ) $ iff their highest weights are equal $\lambda=\mu$ . harish-chandra 's theorem is proven in chapter 23 of ref . 2 . see also this and this related math . se posts . example : consider the lie algebra $l=sl ( 3 , \mathbb{c} ) $ . the weyl group is $s_3$ . the lie algebra $l$ has two independent casimir invariants $c_2$ and $c_3$ , $$c_n ~:=~ {\rm str} ( {\rm ad} t_{a_1}\circ\ldots\circ{\rm ad} t_{a_n} ) t^{a_1} \otimes\ldots\otimes t^{a_n} , \qquad n~\in~ \{2,3\} . $$ consider the 3-dimensional fundamental representation $f$ and the dual/contragredient representation $\bar{f}$ of $l$ , which are non-equivalent irreps . they have highest weights $\lambda= ( 1,0 ) $ and $\mu= ( 0,1 ) $ , respectively . in detail , if $t_a$ , $a=1 , \ldots , 8$ are generators for $l=sl ( 3 , \mathbb{c} ) $ , then ( hattip : peter kravchuk ) $$\bar{f} ( t_a ) ~=~ -f ( t_a ) ^t , $$ so that the casimirs $c_2$ ( and $c_3$ ) take the same ( opposite ) value on $f$ and $\bar{f}$ $$ {\rm tr}_{\bar{f}}\bar{f} ( c_n ) ~=~ ( -1 ) ^n{\rm tr}_{f}f ( c_n ) , \qquad n~\in~ \{2,3\} . $$ one may prove that the values are non-zero , so that the casimirs $c_2$ and $c_3$ distinguish between the two non-equivalent irreps $f$ and $\bar{f}$ , as they should . references : a . o . barut and r . raczka , theory of group representations and applications , 2nd ed . , 1980 . j.e. humphreys , introduction to lie algebras and representation theory , ( 1980 ) .
first of all , the shape of the wave function of a photon that is emitted by an atom is independent of the number of photons because the photons are almost non-interacting and the atoms that emit them are pretty much independent of each other . so if an atom on the surface of a star spontaneously emits a photon , the photon is described by pretty much the same wave function as a single photon from a very dim , distant source . the wave function of many photons emitted by different atoms is pretty much the tensor product of many copies of the wave function for a single photon : they are almost independent , or unentangled , if you wish . the direction of motion of the photon is pretty much completely undetermined . it is just a complete nonsense that the wave function of a photon coming from distant galaxies will have the transverse size of several meters . the gentleman clearly does not know what he is talking about . if the photon arrives from the distance of billions of light years , the size of the wave function in the angular directions will be counted in billions of light years , too . i think it is always the wrong " classical intuition " that prevents people from understanding that wave functions of particles that are not observed are almost completely delocalized . you would need a damn sharp laser - one that we do not possess - to keep photons in a few-meter-wide region after a journey that took billions of years . even when we shine our sharpest lasers to the moon which is just a light second away , we get a one-meter-wide spot on the moon . and yes , this size is what measures the size of the wave function . for many photons created in similar ways , the classical electromagnetic field pretty much copies the wave function of each photon when it comes to the spatial extent . second , the thickness of the wave packet . well , you may just fourier-transform the wave packet and determine the composition of individual frequencies . if the frequency i.e. the momentum of the photon were totally well-defined , the wave packet would have to be infinitely thick . in reality , the width in the frequency space is determined up to $\gamma$ which is essentially equal to the inverse lifetime of the excited state . the fourier transform back to the position space makes the width in the position space close to $c$ times the lifetime of the excited state or so . it is not surprising : when the atom is decaying - emitting a photon - it is gradually transforming to a wave function in which the photon has already been emitted , aside from the original wave function in which it has not been emitted . ( this gradually changing state is used in the schrödinger cat thought experiment . ) tracing over the atom , we see that the photon that is being created has a wave function that is being produced over the lifetime of the excited state of the atom . so the packet created in this way travels $c$ times this lifetime - and this distance will be the approximate thickness of the packet . an excited state that lives for 1 millisecond in average will create a photon wave packet whose thickness will be about 300 kilometers . so the idea that the thickness is tiny is just preposterous . of course , we ultimately detect the photon at a sharp place and at a sharp time but the wave function is distributed over a big portion of the spacetime and the rules of quantum mechanics guarantee that the wave function knows about the probabilistic distribution where or when the photon will be detected . the thickness essentially does not change with time because massless fields or massless particles ' wave functions propagate simply by moving uniformly by the speed $c$ . cheers lm
using your definition of " falling , " heavier objects do fall faster , and here 's one way to justify it : consider the situation in the frame of reference of the center of mass of the two-body system ( cm of the earth and whatever you are dropping on it , for example ) . each object exerts a force on the other of $$f = \frac{g m_1 m_2}{r^2}$$ where $r = x_2 - x_1$ ( assuming $x_2 &gt ; x_1$ ) is the separation distance . so for object 1 , you have $$\frac{g m_1 m_2}{r^2} = m_1\ddot{x}_1$$ and for object 2 , $$\frac{g m_1 m_2}{r^2} = -m_2\ddot{x}_2$$ since object 2 is to the right , it gets pulled to the left , in the negative direction . canceling common factors and adding these up , you get $$\frac{g ( m_1 + m_2 ) }{r^2} = -\ddot{r}$$ so it is clear that when the total mass is larger , the magnitude of the acceleration is larger , meaning that it will take less time for the objects to come together . if you want to see this mathematically , multiply both sides of the equation by $\dot{r}\mathrm{d}t$ to get $$\frac{g ( m_1 + m_2 ) }{r^2}\mathrm{d}r = -\dot{r}\mathrm{d}\dot{r}$$ and integrate , $$g ( m_1 + m_2 ) \left ( \frac{1}{r} - \frac{1}{r_i}\right ) = \frac{\dot{r}^2 - \dot{r}_i^2}{2}$$ assuming $\dot{r}_i = 0$ ( the objects start from relative rest ) , you can rearrange this to $$\sqrt{2g ( m_1 + m_2 ) }\ \mathrm{d}t = -\sqrt{\frac{r_i r}{r_i - r}}\mathrm{d}r$$ where i have chosen the negative square root because $\dot{r} &lt ; 0$ , and integrate it again to find $$t = \frac{1}{\sqrt{2g ( m_1 + m_2 ) }}\biggl ( \sqrt{r_i r_f ( r_i - r_f ) } + r_i^{3/2}\cos^{-1}\sqrt{\frac{r_f}{r_i}}\biggr ) $$ where $r_f$ is the final center-to-center separation distance . notice that $t$ is inversely proportional to the total mass , so larger mass translates into a lower collision time . in the case of something like the earth and a bowling ball , one of the masses is much larger , $m_1 \gg m_2$ . so you can approximate the mass dependence of $t$ using a taylor series , $$\frac{1}{\sqrt{2g ( m_1 + m_2 ) }} = \frac{1}{\sqrt{2gm_1}}\biggl ( 1 - \frac{1}{2}\frac{m_2}{m_1} + \cdots\biggr ) $$ the leading term is completely independent of $m_2$ ( mass of the bowling ball or whatever ) , and this is why we can say , to a leading order approximation , that all objects fall at the same rate on the earth 's surface . for typical objects that might be dropped , the first correction term has a magnitude of a few kilograms divided by the mass of the earth , which works out to $10^{-24}$ . so the inaccuracy introduced by ignoring the motion of the earth is roughly one part in a trillion trillion , far beyond the sensitivity of any measuring device that exists ( or can even be imagined ) today .
in the 19th century , the physicists young and helmholtz proposed a trichromatic theory of color , in which the eye was modeled as three filters with overlapping ranges . this is essentially a physical model of the pigments in the eye , and it predicts the response of the nerve cells at the retina . helmholtz did related work on sound and timbre . ca . 1950 , hering , hurvich , and jameson proposed significant modifications to the trichromatic theory , called opponent processing . this models a later stage in the processing of the signals , after the retinal response but before the more sophisticated stages of processing in the brain . both the trichromatic model and opponent processing are needed in order to describe certain phenomena in human color perception . the complete theory can be modeled by two functions depending on wavelength . i will call these $rg ( \lambda ) $ and $by ( \lambda ) $ . these functions are drawn here . they both oscillate between positive and negative values . for any given pure wavelength $\lambda$ , the net result of pigment-filtering plus the later neurological processing produces these two numbers , which can be thought of as the final signals that go on to later processing in the brain . i am calling them $rg$ and $by$ for the following reasons . let 's pretend , for the sake of simplicity , that these functions oscillated between -1 and +1 . then the pair $ ( rg , by ) = ( 1,0 ) $ produces the sensation of red , ( -1,0 ) is green , ( 0,1 ) is blue , and ( 0 , -1 ) is yellow . there is various psychological evidence for this model , e.g. , no color is perceived as reddish-green or yellowish-blue . roughly speaking , what seems to be happening is that the eye-brain system is taking differences between signal levels of different cone cells . this sort of makes sense because , for example , the red and green pigments have response curves that overlap a lot , so if you want to place a pure-wavelength color on the spectrum , the difference between them is more a more direct measure of what you want to know than the individual signals . the $rg$ function actually has two different peaks , one at the red end of the spectrum and one , surprisingly , at the blue end . this implies that by mixing blue and red , you can produce an $ ( rg , by ) $ pair similar to what you would have gotten with monochromatic violet . if you look at other sources , e.g. , this one ( figure 3.3 ) , they seem to agree on the secondary short-wavelength peak of the $rg$ function , but the details of how the two functions are drawn at the short wavelengths are different and seem to make for a less convincing explanation of the observed perceptual similarity between violet and a red-blue mixture . i do not know if there is a valid reductionist explanation of the short-wavelength peak of the $rg$ function . like a lot of things produced by evolution , it may basically be an accident that got frozen in . however , it is possible that it serves the evolutionary purpose of helping us to distinguish different shades of blue and violet . if the $rg$ function was simply zero over the whole short-wavelength end of the spectrum , then the $by$ function would be the only information we had get for those wavelengths . but the $by$ function has a maximum , simply because the eye 's sensitivity to light fades out as you get into the uv . near this maximum , the ability of the $by$ function to discriminate between colors becomes zero . in the york university graph , it appears that the short-wavelength extrema of the $rg$ and $by$ functions are offset from one another , which would allow some color discrimination in this region . the physical information being preserved by the $by$ function would then be the difference in response between the blue and green cones . but the briggs graphs do not appear to show any such offset of the extrema , so it is possible that the explanation i am giving is a bogus " just-so story . " there may be a good analogy here with sound . the sound spectrum is linear , but there is a psychological phenomenon of octave identification , which makes the spectrum " wrap around , " so that frequencies $f$ and $2f$ are perceptually similar and can often be mistaken for one another even by trained musicians . similarly , the predictive power of the " color wheel " model shows that to some approximation we can think of the trichromatic/opponent process model as resulting in a wrapping around of the visible segment of the em spectrum into a circle . but in both cases , the wrap-around is only an approximation . in terms of pitch , $f$ and $2f$ are perceptually similar but not indistinguishable . for color , we have the 1976 cieluv color color diagram , which is a modification of the 1931 diagram meant to represent at least somewhat accurately the degree of perceptual similarity between different points based on the distance between them . the monochromatic spectrum constitutes part of the outer boundary of this diagram , and is more of a " v " than a circle ; there is quite a large gap between monochromatic violet and monochromatic red . it is trivially true that any such diagram has a boundary that is a closed curve . if the diagram is not constrained to give any accurate depiction of the sizes of the perceptual differences between colors , then it can be distorted arbitrarily , and we can arbitrarily define it such that its boundary is a circle . in this sense , the success of the color wheel model is guaranteed , and it follows from nothing more than the fact that humans are trichromats , so that the color space is three-dimensional , and controlling for luminance produces a two-dimensional space . but this fails to explain why there is some degree of perceptual similarity between the red and violet ends of the monochromatic spectrum ; for that you need the opponent processing model . there is also a slight variation in the absorbance of the pigment in the red cones at the blue end of the spectrum . i do not think this is sufficient to explain the perceptual similarity between violet and red , or the even closer similarity between violet and a mixture of red and blue light , i.e. , i do not think you can explain these facts using only the trichromatic theory without opponent processing . the classic direct measurements of the filter curves of cone-cell pigments were done with cone cells from carp by tomita ca . 1965 , but afaik the only direct measurement using human cone cells was bowmaker 1981 . bowmaker 's red-cell absorbance curve has a very slight rise at short wavelengths , but it is not very pronounced at all . you will see various other curves on the internet , often without any attribution or explanation of where they came from , and some of these show a much more pronounced bump rather than bowmaker 's slight rise . possibly some of these are from people using the cie 1931 curves , which were never intended to be physical models of the actual human cone-cell pigments . it should be clear , however , that the red and green pigments ' curves must have some variation near the violet end of the spectrum . if they did not , then the dimensionality of the color space would be reduced there , and the human eye would be unable to distinguish different wavelengths in this region , which is contrary to fact . bowmaker , " visual pigments and colour vision in man and monkeys , " j r soc med . 1981 may ; 74 ( 5 ) : 348 , freely accessible at http://www.ncbi.nlm.nih.gov/pmc/articles/pmc1438839/
you need to know the rudiments of the application of algebraic topology to the classification of bundles on manifolds . if you are self teaching using the internet , it would be useful to look up " characteristic classes " , and work backwards from there , filling in the gaps that you need . nakahara is a good introduction to this material , as is eguchi , gilkey and hanson
firstly , fusion does not happen in the way depicted in the question . four protons do not participate in a 4-body reaction . instead there are many intermediate steps : each step has its own reaction rate . the overall reaction rate is determined by the rate limiting step . the proton-proton reaction is the rate limiting step in this case . it is important to think in terms of the rate at which a reaction occurs , rather than whether or not it will occur . the reaction rate will depend on temperature and pressure . in the sun , pressure is ~265 billon bar , so the reaction can proceed at 10-15 million k . the rate of reaction is actually very low , the center of the sun only produces energy at rate of ~277 watts per cubic meter . on earth , we can not built a reactor with pressure this high , so higher temperature is needed . different reactions such as starting with deuterium or tritium are used to avoid the need for the proton-proton reaction . then fusion could be achieved in the ~100 million k range for example .
it is because they are based on the historical approach : schroedinger 's equation . schroedinger 's equation was discovered on its own before we knew about canonical quantization . dirac came up with the canonical quantization rules which re-wrote ( and generalized ) schroedinger 's equation into the familiar one we have today , $\hat{h} \psi = i \dot{\psi}$ . that said , there is an approach which uses the action ( and thus the lagrangian or lagrangian density ) due to feynman : the path integral approach . this approach has as its biggest advantage the ability to be reconciled with special relativity , which proved much too difficult a task for extensions of the schroedinger equation ( the dirac equation was the most successful attempt , but was not general enough to describe some phenomena ) . this is what is used in the most advanced quantum physics such as quantum field theory , quantum electrodynamics being the best example . but unless you are interested in high-energy particle physics or really advanced condensed matter physics , the traditional quantum mechanics is sufficient .
in fact you are more or less correct . i assume the increase in mass mentioned is that described in special relativity . the example given by your teacher is incorrect . as the speeds of 10m/s and 40m/s are hardly relativistic , so we can for now assume $e=mc^2$ . increasing the kinetic energy by $\frac{1}{2}mv^2$ thus increases the mass by $$\frac{\frac{1}{2}mv^2}{c^2}=\frac{1}{2}m\frac{v^2}{c^2}$$ this in fact , is incredibly small , due to the hugeness of $c$ . now back to why the mass of an object increases . according to special relativity , mass and energy are in fact equivalent . although not related by $e=mc^2$ ( actually $e=\frac{mc^2}{\sqrt{1-\frac{v^2}{c^2}}}$ ) , the equivalance means that an increase in the velocity of an object will yes , increase its kinetic energy and thus mass .
we have $40*100*1000=4\times10^6$ cubic meters of water per kilometer , which is $4\times 10^9$ kilograms of water per kilometer , average 20 meters above sea level , which leads to $4\times 10^9 \times 20 \times 10 = 8\times 10^{11} j/km$ in gravitational potential energy . kinetic energy should be about the same as potential so lets say $1.6\times 10^{12} j/km$ or $0.3$ kilotons per kilometer . for reference the hiroshima a-bomb was about $10$ kilotons yield .
where does it say that a newtonian consideration of gravity is required ? i would say that even when going coarse grain in ( bio ) chemical reactions , gravity is neglected . this is due that in comparison with electrostatic interactions , they are about 40 times smaller .
well , the information does not have to escape from inside the horizon , because it is not inside . the information is on the horizon . one way to see that is from the fact that from the perspective of an observer outside the horizon of a black hole , nothing ever crosses the horizon . it asymptotically gets to the horizon in infinite time ( as it is measured from the perspective of an observer at infinity ) . an other way to see that is the fact that from the boundary conditions on the horizon you can get all the information you need to describe the space-time outside but that is something more technical . finally , since classical gr is a geometrical theory and not a quantum field theory* , gravitons is not the appropriate way to describe it . *to clarify this point , gr can admit a description in the framework of gauge theories like the theory of electromagnetism . but even though electromagnetism can admit a second quantization ( and be described as a qft ) , gr can not .
the massless spectrum depends on the background . what one usually means by masses in a background describing a compactification are masses in the non-compact flat space ( most often 4d ) . that is , you take the momentum in the non-compact directions , you square it and you find the mass . about your second question , whenever the background has a light-like killing vector you can build a light-cone hamiltonian and a hilbert space . i am not sure if there are more general situations where a hilbert space description is available . for a general time-dependent background , i do not think much is known .
since the gravitational force only pulls the ball down , but not back or forth , it will not experience any acceleration changing its forward velocity but only downward acceleration . thus , the ball will return to the thrower . you can also imagine the train to have no windows and be moving extremely smoothly . the thrower will not know if the train is moving or not and so the ball will not " know " either . the scenario for the train at rest is quite intuitive , i think , and will not differ from the scenario of the train moving at constant velocity .
here are some online lecture notes by chetan nayak which look pretty good : introductory : http://www.physics.ucla.edu/~nayak/solid_state.pdf more advanced : http://www.physics.ucla.edu/~nayak/many_body.pdf
earth can lose heat to space through radiation . the earth behaves roughly as a blackbody and so radiates electromagnetic radiation into space at a rate of roughly 120 pw .
say you are looking at the piece of paper on your desk ; that is the $xy$ plane . you place a dot in the center of the paper ; that is your origin . your angular momentum is $\vec{l}=\vec{r}{\times}m\vec{v}$ . for this example , $\vec{\omega}$ points in the same direction as your angular momentum , because $\vec{l}=mr^2\vec{\omega}$ . the way i remember the directions for the right-hand rule are as follows : thumb points up ( this is $\vec{l}$ , the cross product ) index finger points forward ( this is $\vec{r}$ , the left vector being crossed ) middle finger points to the left ( this is $\vec{v}$ , the right vector being crossed ) let 's say the particle is on the paper , directly above the origin , and moving counterclockwise . then , $\vec{v}$ points to the left edge of the page . the direction of $\vec{r}$ goes from the origin to the point , so it points to the top of the page . my thumb points up , so this is the direction of $\vec{l}$ , and hence the direction of $\vec{\omega}$ . crane your hand around so that your middle finger points right and your index finger continues to point to the top of the page , and your thumb points down , which is the direction of $\vec{\omega}$ for clockwise motion .
you can take the last two equations to solve for the normal forces and use them in the torque equation $$ \left . \begin{align} n_f and = \frac{m g}{1+\mu^2} \\ n_w and = \frac{\mu m g}{1+\mu^2} \end{align} \right\} m g \ell \left ( \frac{1}{1+\mu^2}-\frac{1}{2} \right ) \sin\theta - m g \ell \frac{\mu}{1+\mu^2} \cos\theta =0 $$ $$ \left ( \frac{1}{1+\mu^2}-\frac{1}{2}\right ) \sin\theta = \frac{\mu}{1+\mu^2} \cos\theta $$ $$ \tan \theta = \frac{2 \mu}{2- ( 1+\mu^2 ) } = \frac{2 \mu}{1-\mu^2} $$ so it seems you did an algebraic error somewhere ( not shown ) to get $1+\mu^2$ in the denominator .
the moment of inertia is a rank 2 tensor not a scalar . you will commonly see it written as a scalar , but this is because by choosing your axes to line up with the principal axes of the object the matrix representing the moment of inertia can be diagonalised : $$ {\bf i} = \left ( \begin{matrix} i_{00} and 0 and 0 \\ 0 and i_{11} and 0 \\ 0 and 0 and i_{22} \end{matrix} \right ) $$ so if the rotation is about , for example , the $0$ axis you get : $$ \vec{l} = \left ( \begin{matrix} i_{00} and 0 and 0 \\ 0 and i_{11} and 0 \\ 0 and 0 and i_{22} \end{matrix} \right ) \left ( \begin{matrix} \omega \\ 0 \\ 0 \end{matrix} \right ) $$ or : $$ \vec{l} = i_{00} \vec{\omega} $$ where $i_{00}$ is indeed a scalar . however this is a special case and whenever you see the moment of inertia given as a scalar you will find this applies to only one axis of rotation and that axis is one of the principal axes .
i think that the question is why the si system of units considers one ampere , the unit of current , to be the elementary one , rather than the unit of the electric charge . recall that one ampere is defined in si as " the constant current that will produce an attractive force of $2\times 10^{–7}$ newton per metre of length between two straight , parallel conductors of infinite length and negligible circular cross section placed one metre apart in a vacuum " note that this definition relies on magnetic forces ; it is equivalent to saying that the vacuum permeability $$\mu_0=4\pi\times 10^{-7} {\text{v s/ ( a m ) }} $$ it is the magnetic force that has a " simple numerical value " in the si system of units , and magnetic forces do not exist between static electric charges , just between currents . if we tried to give a similar definition for the electric charge , using the electrostatic force , the numerical values would be very different . now , one may ask why the magnetic forces were chosen to have " simple values " in the si system . it is a complete historical coincidence . the si system was designed , up to the rationalized additions of $4\pi$ and different powers of ten , as the successor of cgsm , the magnetic variation of gauss ' centimeter-gram-second ( cgs ) system of units . these days , both methods would be equally valid because we use units in which the speed of light in the vacuum is fixed to be a known constant , $299,792,458\ , {\rm m/s}$ , so both $\mu_0$ and $\epsilon_0=1/ ( \mu_0 c^2 ) $ , the vacuum permittivity , are equal to known numerical constants , anyway . at any rate , the unit of the electric charge is simply " coulomb " which is " ampere times second " , so it is as accurately defined as one ampere .
your error lies in your last paragraph : because the angular momentum $\vec{l}'$ is not conserved due to changing direction , and a changing $\vec{l}'$ requires the presence of an external force to give rise to a net torque $\vec{\tau}'$ about the point that is used to measure $\vec{l}'$ , which is the origin $o'$ , the particle will not rotate about $p$ forever in the absence of any external force . everything in that sentence is factually correct , but the conclusion , " the particle will not rotate about $p$ forever in the absence of any external force " , does not follow from it . let 's step back and think about what the system actually looks like . in the first case , a particle rotates in a circle with the origin $o$ at its center , presumably due to some force ( like gravity towards $o$ ) which keeps the motion circular . in the second case , the origin is moved to $o'$ but the system itself is not changed , ie , the particle now rotates in a circle above the new origin $o'$ ( again , due to gravity towards $o$ ) . in the first case , the centrifugal force is parallel to the moment arm , since the particle is attracted to $o$ , which is the chosen coordinate origin . thus $\vec{l}$ is conserved , which makes sense . however , in the second case , the centrifugal ( gravity ) force is no longer parallel to the moment arm , since the particle is attracted to $o$ , not $o'$ . thus there is no reason to expect that $\vec{l}'$ is conserved . naturally this means that there is a nonzero torque $\vec{\tau}'$ with this choice of coordinates . what is the physical significance of this nonzero torque ? in the first case , the torque $\vec{\tau}$ is zero , because the particle orbits the origin $o$ . in the second case , the particle no longer orbits the origin $o'$ , but instead orbits a location $o$ above it , and thus from this coordinate viewpoint , there must be a nonzero torque $\vec{\tau}'$ , as otherwise if $\vec{\tau}'=0$ the particle would orbit $o'$ , rather than $o$ !
there are two contributions to the electric field in a dielectric : the field generated by the ' free ' charges , i.e. the ones on the capacitor plates . call it $e_0$ $e_0$ polarizes the dielectric , which in turn adds to the total electric field . call that polarization $p$ . the total electric field is $$e=e_0-\epsilon_0^{-1}p$$ ( the factor of $\epsilon_0^{-1}$ before $p$ is customary . ) a simplifying assumption that holds true in many cases of practical relevance is that of linear response . the polarization is taken to be proportional to the field $$ p =\epsilon_0 \chi e$$ $\chi$ is called the electric susceptibility . when plugged into the upper relation it yields $$e= ( 1+\chi ) ^{-1}e_0\equiv \frac{e_0}{\kappa} $$ the voltage between two points is generally given by integrating the electric field on a path between those two points . in order to not burden the discussion with to much math let 's just point out that in the setting of a plate capacitor the voltage between the two plates at distance $d$ is simply $$ v= -e\cdot d = -\frac{e_0}{\kappa}d$$ this demonstrates that the voltage between the plates is not oblivious to the presence of the dielectric . imagine placing a test-charge in the capacitor . without a dielectric the charge will move due to $e_0$ . the energy gained ( divided by the charge ) is by definition the voltage crossed . in the presence of a dielectric , the field $e_0$ is partially canceled , therefor a test-charge will gain less energy , i.e. the voltage is lower . now , how does that lead to the answer in your book ? two things : when the capacitor is disconnected from the source , the plates keep their charges . when a dielectric is introduced into the half-space as depicted , the voltage across that region changes as deduced above by a factor $\kappa^{-1}$ you now have two regions of different potential on each plate . but that cannot be maintained in a conductor . charges on each plate will redistribute in such a fashion that the potential of each plate becomes constant , say $v_1=v_2$ . the charges are not distributed evenly on the plates anymore ! as the electric field is simply $e_{1,2}=-v_{1,2}/d$ it is indeed the same within and outside the dielectric . what changes is $e_0$ , since it is generated by the charges on the plates only , which have redistributed . note : one usually does not consider $e_0$ but $d = \epsilon_0 e_0$ called the electric displacement field .
for question v1: since you are integrating the non-negative function $ ( x^2-l^2 ) ^4$ , you should not get zero . your mistake must be your expression for the antiderivative . expanding out the integrand is probably a safe way to start . there is something funky about the supposed answer for $n$ . look at the units , along with the entire expression for $\psi$ . the units of $\psi$ should be such that $\left | \psi \right |^2 dx$ is dimensionless . that tells you what the dimension of $n$ ought to be given that $\psi\sim l^4$ . the correct solution you gave is off .
the answer is that there is a flow of charge into the crow 's body to raise its potential to that of the wire , but this charge is miniscule . thereafter , there is a very small current ( microamps ) into and out of the crow 's body and a tiny heating effect ( microwatts ) as his or her potential varies with the ac cycle . this is very different from the sustained current that would flow through a crow of tens of kilohms resistance across a $22\mathrm{kv}$ potential difference between three phase conductors in the street . let 's calculate the orders of magnitudes of these effects . if the crow links two conductors , the current flow through its body is of the order of ampères and the heating effect of the order of kilowatts . the crow is quickly fried , literally . now let 's think about when the crow sits on one conductor . suppose we replace the crow by a conducting sphere of , say , $r=0.1\mathrm{m}$ radius . the crow for the purposes of this calculation is essentially a keratin ( feather ) covered sack of water and oil with ions in it , so its replacement by a conducting sphere will yield the right order of magnitude in our calculations . then let 's work out the potential ( defining $v=0$ at a point infinitely far away from the sphere ) at the sphere 's surface when that sphere has charge $q$ , it is : $$v ( q ) = \frac{q}{4\ , \pi\ , \epsilon_0\ , r}$$ therefore , let 's suppose the crow sits on a power line at $22\mathrm{kv}$ relative to the ground . from the above equation , he or she must take on a charge of $4\ , \pi\ , \epsilon_0\ , 0.1\ , 2.2\times10^4 = 2.45\times10^{-7}$ coulombs and the energy needed to charge him or her up is $$\int_0^{q_{max}} v ( q ) \mathrm{d}q = \frac{1}{2} v_{max} q_{max} = \frac{1}{2} \times 2.2\times10^4 \times 2.45\times10^{-7} = 2.7\mathrm{mj}$$ and this is the order of magnitude of the electrical energy dissipation in the crow 's body , which we can understand by thinking of the crow-ground system as a capacitor with capacitance ( from the equation above ) $c = 4\ , \pi\ , \epsilon_0\ , r \approx 11\mathrm{pf}$ and assuming that the crow 's breakdown resistance is a few tens of kilohms , the crow-ground system 's $r c$ charging time constant is of the order of $10^{-7}$ seconds . so there is a significant current spike ( roughly an ampere ) , but it is extremely fleeting . the energy dissipated as heat by this current spike in the crow 's body is of the order of millijoules . if we think of the crow as a series $r c$ circuit , once he or she is charged to the line potential , there is a current flowing in and out of his body with the alternating current , but the power levels and currents are very small indeed . the magnitude of the crow 's impedance is $\sqrt{r+\omega^{-2} c^{-2}} \approx 4\mathrm{m\omega}$ leading to a current of the order of $60\mathrm{\mu a}$ and heat dissipation of $5\mathrm{\mu w}$ . there are two threats to higher living things ( those with hearts and circulatory systems ) , the first arising at much lower currents and energies than the second : disruption of the heart because the electricity flow interferes with the heart 's own electrical signaling system ; destruction of tissue by heating ; the $2.7\mathrm{mj}$ is tiny , four or five orders of magnitude below the energy delivered by a defibrillator , for instance is tens to hundreds of joules . the sustained heating effect is also tiny , barely noticeable in the tissues of an animal the size of a crow . humans can also safely do the same thing when line workers tether themselves to the lines they work on an the power cannot be shut down for some reason . this is even though humans are much more susceptible to electric shock than small animals , whose hearts are so small that it is almost impossible to send into ventricular fibrillation owing to the tiny time delays and the attendant much greater stability margins in the heart 's electrical control ) . a small animal like a crow is likely only at risk from the second effect above : destruction of body tissue by heating .
even though the answer you chose is very good i will add my pov take a box of gas particles . at $t=0$ , the distribution of particles is homogeneous . there is a small probability that at $t=1$ , all particles go to the left side of the box . in this case , entropy is decreasing . take the statistical mechanics definition of entropy : where $k_b$ is the boltzmann constant . the summation is over all the possible microstates of the system , and $p_i$ is the probability that the system is in the $i$th microstate . the problem is that this one system you are postulating in your question is one microstate in the sum that defines the entropy of the system . a microstate does not have an entropy by itself , in a similar way that you cannot measure the kinetic energy of one molecule and extrapolate it to a temperature for the ensemble of molecules . an observation on systems with decreased entropy : entropy increases in a closed system . if an appropriate liquid is turned into a crystal , the ensemble of molecules will have lower entropy , but energy will have been released in the form of radiation , the system is closed only when the radiation is taken into account for the entropy budget .
assume that you jump straight up , standing on the equator . as soon as your feet leave the ground , you are in a highly elliptical orbit around the center of the earth . at that point you have the same angular velocity as the point you jump from . as you rise toward your one and only apogee , conservation of angular momentum requires that your angular velocity decrease very slightly , then increase again as you drop . of course , the orbital motion will stop when you hit the ground again . overall , the ground will have rotated slightly further . i doubt that the effect could ever be measured . at the poles , you land where you jumped from . mid-latitude jumps are left as an exercise for the reader . . .
there are two concepts here that may be getting mixed together , namely , conservation of linear momentum and conservation of angular momentum . newtons laws state that an object in motion will say in motion unless acted upon by an external force . so unless interstellar friction is a problem , the spaceship will keep travelling linearly in the same direction forever ( neglecting gravitational attraction to other bodies etc ) . the same applies to rotational motion . a rotating object will continue to rotate unless acted upon by an external torque . so again , unless interstellar friction is a problem , the spaceship will continue to rotate forever . to stress again , these two things are independent - one will not affect the other .
there is an excellent discussion of this at pauli principle for particles very far apart from each other . the question is not a duplicate of yours , so i have not flagged your question as a duplicate , but wouter 's answer is highly relevant . people have a tendancy to casually throw around the exclusion principle with hand waving arguments such as " when you bring the atoms together the electrons can not occupy the same orbitals " . wouter 's answer explains how to understand the exchange force in terms of the electron distributions , and specifically in terms of the overlap . ( apologies to the mods if this should have been a comment , but it went on a bit long for a comment ! )
so , it is actually only numerically solvable ? and is there no analytic expression for $h ( z ) $ ? in my answer i will be using the scale factor $a$ instead of the less wieldy redshift $z$ . the two are simply related by $a = 1/ ( 1+z ) $ . in general the first friedmann equation can be written as : $$ h^2 = \left ( \frac{\dot{a}}{a} \right ) ^2 = h_0^2 \left ( \sum_i \omega_i \times a^{-3 ( 1+w_i ) } + \omega_k a^{-2} \right ) $$ where $w$ is the equation of state parameter of each effective fluid $i$ . now let 's take the case of a flat universe ( $\omega_k = 0$ ) where a single $\omega_i$ dominates ( $\omega_j \approx 0$ for $j \neq i$ ) . we get that : \begin{eqnarray*} \left ( \frac{\dot{a}}{a} \right ) ^2 and \propto and a^{-3 ( 1+w_i ) } \\ \dot{a} and \propto and a^{-3/2 ( 1+w ) +1} \end{eqnarray*} defining $\gamma$ such that $a \propto t^\gamma$ we get : \begin{eqnarray*} t^{\gamma - 1} and \propto and t^{-3/2 ( 1 + w ) \gamma + \gamma} \\ \gamma - 1 and = and -3/2 ( 1 + w ) \gamma + \gamma \\ \gamma and = and \frac{2}{3 ( 1+w ) } \end{eqnarray*} this analytic solution is valid for all $w$ except when $w = -1$ . there you get an exponential solution instead , and i will leave it as an exercise for the reader to convince himself of it . as an example , for a flat matter dominated universe ( $\omega_m = 1$ and $\omega_k = 0$ ) we get $a \propto t^{2/3}$ because $w_m = 0$ . what happens when we are not in such a simple scenario and there is more than one contributing effective fluid ? in that case there the analytic solutions are more intricate , and a guide to some of them may be found here note that you had an analytic form for $h ( a ) $ right from the start , and if you want one for $h ( t ) $ then it is pretty straightforward to plug in the solution i just gave you into the friedmann equation .
for any function $f$ , we have $f ( |v| ) = f ( \sqrt{v^{2}} ) = g ( v^{2} ) $ , where $g = f\circ \sqrt{}$ , so we lose no generality by assuming that the function is a function of the square . also , it generalizes more nicely to vector spaces , where $v^{2} ={\vec v}\cdot {\vec v}$ is defined , but it is not necessarily the case that the absolute value function is defined .
when i was in school , i took great pleasure in the lectures by prof . walter lewin , physics professor at mit , but now retired . his style of lecturing is quite unique and got him rather famous on the internet . all of his undergraduate lectures have been video-taped and are available through mit is opencourseware program . as for the mathematical prerequisites , calculus is sort of required . i am afraid there is not so much you can do without it . but you may just ignore the bits you do not understand yet and review them later . anyways , classical mechanics is a very useful thing to have in mind when learning about calculus . it is what newton developed calculus for ! when you mastered classical mechanics , you can just proceed with electrodynamics which is a little tougher on your mathematics . but , again you can just give it a try and -- having the physical application in mind -- you might find multi-variable calculus way more intuitive when learning it with the neccessary mathematical rigor . regarding quantum mechanics , i would not touch that . often , when people are discussing quantum mechanics without the proper mathematical tools , they end up talking about things like " wave-particle duality " or ( even worse ) schrödinger 's cat . this will leave you more confused than before and give you the impression that there is some spooky magic to qm . it is not . it just requires lots of math .
the situation is not at all different from that in newtonian physics . if you have newton 's law for gravity , i.e. $$ \vec f = -\frac{g m_1 m_2}{\vert\vec x_1 - \vec x_2\vert^3} ( \vec x_1 - \vec x_2 ) $$ in order to exactly describe any motion , in principle you need to know some initial configuration in which you know the exact position of any object in any galaxy . but in practical calculations it is enough to know the state of some subsystem which interacts only weakly with the rest . like , for example , just consider the motion of one planet around the sun . or more accurately the motion of all planets in the solar system , but ignoring other stars in the galaxy . the situation is exactly the same in gr . you cannot measure all positions of each object ( and if you could the equations would be far to complicated to ever be solved ) , but you can consider subsystems . common solutions are for example the schwarzschild solution for a spherical object , or the friedmann–lemaître–robertson–walker solution , which models cosmology .
mathematically speaking , the square roots of $\epsilon\mu$ can be anywhere in the complex plane . but then the wavevector $k$ has to satisfy some physical requirements : the fact that the dielectric or magnetic permittivities are complex functions means absorption/dispersion in the material , which translates into an attenuation of the propagating wave only if $\beta &gt ; 0$: $$ e^{ikx} = e^{i ( \alpha + i\beta ) x} = e^{i\alpha x}\times e^{-\beta x} $$ together with this the wave has to propagate to the right , say for $x&gt ; 0$ , otherwise the real exponential blows up , therefore also $\alpha &gt ; 0$ . in general , if you define $k = \alpha + i\beta$ , if you do not want explosive behaviours , you need $\alpha$ and $\beta$ of the same sign : $\alpha\beta &gt ; 0$ .
i am going with no . if this was a question of sound spreading with an inverse square law , the answer would be yes . place a cymbal at a distance where it has the same apparent diameter as the sun . on a quiet day , it would be audible . place 4 cymbals at twice the distance . it would be just as loud . repeat this idea at the distance of the sun . if the average cymbal-sized patch of the sun 's surface is louder than a cymbal , you should be able to hear the sun on earth . given the sun 's atmosphere is full of shock waves and other violent events ( see this paper ) , it sounds likely we could hear it . but this is not the full story . air absorbs sound . the absorption coefficient is small , but the distance is $10^8$ miles or $1.5 * 10^{11}$ meters in round numbers . it would take 16 years for sound to reach the earth . here is a calculator for atmospheric absorption . absorption is lowest at low temperature , low humitidy , and low frequency . under the best conditions conditions , it is about $10^-3$ db/m . the intensity would be reduced by $1.5 * 10^{8}$ db . so not even if we were downwind . . . this post has a helioseismology link that shows sound waves travel from one side of the sun to the other with very large amplitudes . but this is not the same thing . these are more like earthquakes than audible sound . and the sun is a power source that keeps them going .
the first process corresponds to $e^{-}e^{+}\to e^{-}e^{+}$ ( bhabha scattering ) , where the final and initial states are the same , consisting of an electron and positron . however , the second process is $e^{-}e^{+}\to \gamma \gamma$ , where instead the final state is that of two photons . the scattering amplitudes will be different . notice that the first diagram requires an insertion of the photon propagator , $$-\frac{i\eta_{\mu\nu}}{q^2 +i\epsilon}$$ whereas the second diagram has a fermionic internal line , requiring a propagator , $$\require{cancel} \frac{i\left ( \cancel{q}+m_f\right ) }{q^2-m_f^2 +i\epsilon}$$ in addition , the second diagram will contain polarization vectors , as the photons are not internal lines but rather external . for a comprehensive overview of qed , see peskin and schroeder 's text .
quantum mechanics can be used to answer questions about the past in a fairly straightforward way as any question of that type can be phrased as a question about expectation value of operators ( or as transition amplitudes ) . as a simple example consider a two state system ( e . g . spin 1/2 ) . suppose someone else prepares the state in either spin up or spin down but does not tell you . also suppose that the dynamics are unitary and known ( $u$ ) . then you can use quantum mechanics to ask , for example , what is the probability that the state was prepared in the ' up ' state if i measure it in the up state now ? $$p = | \langle \mbox{up}|u|\mbox{up} \rangle|^2$$ so really there is nothing new , you just apply quantum mechanics to whatever question you mean to ask about the past . you might have to be a bit careful in phrasing the question however . for the general case of reconstructing the past state given present measurements , see for example the wikipedia article on quantum tomography ( http://en.wikipedia.org/wiki/quantum_tomography )
the question what is reflection ? is really a duplicate of yours , but i suspect the answer may be a bit brief for you . a light wave , like any electromagnetic wave , is a combination of an oscillating electric and magnetic field . these fields exert an oscillating force on the electrons in any material the wave hits , and those electrons start oscillating in response . however an oscillating electron emits electromagnetic radiation , and this radiation interferes with the incident light . to calculate what happens at the interface you have to take into account the incident light , the light reradiated away from the solid back into the vaccum , i.e. the reflected light ) and the light reradiated into the body of the solid , i.e. the transmitted light ) . when you do this you find light is reflected at the angle of incidence , and the light is transmitted at an angle given by snell 's law . the calculation is a bit messy , but if you are interested lots of examples are yours for the cost of a quick google . response to comment : the oscillations of the electrons are driven by the incoming em field , so the oscillation frequency is the same as the frequency of the incoming light . the phase need not be ( which is the origin of the refractive index change ) . in the real world the reflection is not independant of colour ( i.e. . the frequency of the light ) , and this phenomenon is known as dispersion . it is also not independant of light intensity , which is known as non-linearity , though non-linearity is usually an extremely small effect . the electrons do not react instantly when the light strikes . the fastest frequency at which they can react is the plasma frequency . response to response to comment : energy can be lost due to interactions with the lattice , and indeed this is the norm , so the sum of the reflected and transmitted waves is generally less than the incident wave . the lost energy ends up as heat i.e. lattice vibrations . however the fact energy is lost does not change the frequency of oscillation of the electrons , because that is determined by the incident light . that means the frequency of the reflected and transmitted light is the same as the incident light i.e. the same colour . if the incident light contains different frequencies , like white light , then the result can be a shift in the perceived colour . for example if you shine white light onto gold the reflected light is yellowish . however the process of reflection is not changing the frequency of the light , but rather it is changing the relative intensities of frequencies in the reflected light by absorbing some frequencies more strongly than others .
the examples you are giving of s and t do not leave a lot of room for debate : s is an open interval ( say $ ] 0,1 [ $ ) and t is a circle , $s^1$ . already , if you are adding the points a and b to s , you are turning your manifold into something which is called a manifold with boundary ( which is not a manifold in the technical sense of the term ) . i am not sure what you are looking for , but the ' loopiness ' of t is captured in the homology/homotopy of the manifold . s has trivial homology , while $h_1 ( t ) = \mathbb{z}$ and its fundamental group $\pi_1 ( t ) = \mathbb{z}$ as well . if you add more ' roads ' between p and q , you are indeed making these groups even larger . ( an extra road would yield $h_1 ( t ) = \mathbb{z}^2$ , for example . ) the fact that s is ' boring ' is because it is contractible . algebraic topology is the domain which tries to capture these phenomena ; i suggest you read the articles http://en.wikipedia.org/wiki/homology_theory and http://en.wikipedia.org/wiki/fundamental_group . however , in algebraic topology you are usually not very interested in the exact manifold structure , just in the topology of the space you are describing . a branch of mathematics which does care about manifolds is differential geometry , and in that case you have very similar tools to compute the number of loops etc . , see http://en.wikipedia.org/wiki/de_rham_cohomology .
the atmosphere rotates along with the earth for the same reason you do . force is not needed to make something go . that is a basic law of physics - that a thing that is moving will just keep moving if there is no force on it . force is needed either to make something change its speed , or to make its motion point in a new direction . a force can do both or just one of these . most forces do both , but a force that pushes in the exactly the same direction you are already going only changes your speed , and does not change your direction . a force that pushes at a right angle to the direction you are already going only changes your direction , and does not add any speed . a force at "10 o'clock " , for example , will change both your speed and your direction . as you stand still on earth , you continue going the same speed , but your direction changes ; between day and night you move opposite directions . so the forces on you must be at a right angle to your direction of motion . indeed , they are . your motion is from west to east along the surface of the earth , and the force of gravity pulls you down towards the center of the earth - the force and your motion are at right angles . similarly for the atmosphere . it is moving along with the earth , and moving at a constant speed . it does not need anything to push it along with the earth . since only its direction of motion is changing , it only needs a force at a right angle to its motion , the same as you , and the force that does the job is again gravity . that is not the whole picture , because the amount that your direction of motion changes depends on how strong the right-angle force is . it turns out gravity is much too strong for how much our direction of motion changes as the earth spins . there must be some other force on us and on the atmosphere canceling out most of the gravity . there is . for me it is the force of the chair on my butt . for the atmosphere , it is the air pressure . so gravity does not " make the air rotate " . the air is already going , and gravity simply changes its direction to pull it in a circle . you may be wondering why the air does not just sit there and have the earth spin underneath it . one answer to that is that from our point of view that would mean incredibly strong wind all the time . that wind would run into stuff and eventually get slowed down to zero ( that is from our point of view - the air would " speed up " to our speed of rotation from a point of view out in space watching everything happen ) . even the air high up would eventually rotate with the earth because although it can not slam into mountains or buildings and get stopped from blowing , it can essentially " slam into " the air beneath it due to friction in the air . ( this is a little redundant with dmckee 's answer ; i was half way done when he beat me to the punch )
at the simple level of approximation where you talk about kinetic friction , it does not depend on speed . it is not a great approximation ( the coefficients of kinetic friction that you find for materials tend to have huge uncertainties ) , though . the reason we use the approximation ( other than that it makes for good intro mechanics problems ) is that the microscopic physics is pretty complicated . at a very small scale , all objects are somewhat rough ( at the atomic scale , if not before ) , and friction is the result of trying to drag one corrugated surface over another . larger projections from the surfaces will snag against each other and require some force to dislodge , and the sum of all those microscopic snags and drags is the force we see as friction . as it is impossible to keep track of all those interactions in detail for any reasonable size object , we approximate the total force using the kinetic friction model . kinetic friction has nothing to do with the airplane-on-a-treadmill problem , though . kinetic friction involves two surfaces sliding across one another . in a situation involving rolling , however , there is no sliding . at the point where the wheels of the airplane come in contact with the surface of the ground ( or a treadmill ) , the wheel surface is not moving relative to the ground . the relative speed of the bit of the wheel touching the ground and the bit of ground that it is touching is always zero , no matter how fast the wheel is moving relative to the surface . a cute way to see this is to place a ruler on top of a couple of soda cans ( or other convenient round objects ) , and roll it some distance along the surface of a table next to another ruler . you will find that the distance covered by the ruler on top of the cans is double that covered by the center of one of the cans . this happens because the point where the cans touch the table is stationary relative to the table . the centers of the cans move , though , which means that the top part of the rolling can must be moving at twice the speed of the center for the average speed of the can to work out . you can think of the contact point , center , and top being ( at some instant ) like points connected by a stick that pivots about the contact point-- the point at the far end of the stick will move twice as fast as the point at the halfway mark ( obviously , this only holds exactly for the infinitesimal amout of time between when a given bit of can hits the table and when it lifts off again as the roll continues , but it gets you the right idea ) . thus , the ruler along the top moves twice as far as the center of the cans .
i assume that no electric flux from the inside point charge $q$ leaks from the conductor to outer space . now we try to build up the system step by step : first a charge density will be induced on the inner surface of the conductor . this induced surface charge on the inner surface distributes itself so that the total electric field will be zero everywhere ( including inside the conductor itself ) . then , a charge will be induced on the outer surface of the conductor . this induced charge will distribute itself as if there was not any cavity in the conductor and we just had a filled piece of conductor with a total charge of $q$ . so in your case , since the area of the conductor is infinite , the induced charge density on the outer side will be zero and no field will exist in outer space .
since $+1$ and $-1$ are multiples of each other and $ ( +1 ) + ( -1 ) $ makes perfect sense , these two would have to have the same dimension ( and hence would not help avoid confusion ) - which in fact already exists , it is the dimensionless " dimension " $1$ .
note that $j_{xy}$ and $j_z$ have the dimensions of energy while $j$ and $k$ have dimension of energy/ ( area x time ) . in the process of making the continuum hamiltonian into a discrete one , you will need to choose short distance scales for both space as well as ( euclidean ) time . you can choose a square lattice of length say , $a$ for space and split the time in scales of length $b$ . then , one dimensional grounds , you should expect to see the following map : $$ \frac{j_{xy}}{a^2b} \rightarrow j\quad , \quad \frac{j_z}{a^2b} \rightarrow k\ . $$ these are related to the naive scaling ( of space and time ) dimensions of $j$ and $k$ .
i think that the book is simply referring to the fact that , even in the case of non-constant acceleration , calculus can be used to find the position as a function of time if the acceleration as a function of time is known . in particular , whether or not the acceleration is constant , the definitions of acceleration in terms of velocity and of velocity in terms of position give $x$ in terms of $a$ as follows : $$ x ( t ) -x ( t_0 ) = \int_{t_0}^t d\alpha\ , v ( \alpha ) = \int_{t_0}^td\alpha\int_{t_0}^{\alpha} d\beta \ , a ( \beta ) $$
the second ( empty ) focus is relevant in the theory of tides . in an elliptical orbit , the line joining the planet and the empty focus rotates at the same frequency as the mean motion of the planet ; therefore , if spin rotation period is equal to the orbital period ( the planet is locked in a synchronous rotation ) , the planet rotates with one face pointing to the empty focus . importantly , a tidal bulge will try to point to the massive object ( the occupied focus ) while the planet itself will be pointing to the empty focus , causing a " librational tide " .
special relativity and general relativity are quite different beasts . sr is a group of symmetries that get embedded in the quantum theory , and gives us rqft . thus , you can have a field and therefore a quantum field theory on top of sr with no need to quantize sr . you can do quantum mechanics of em , for instance , with no need to talk about gravity ( remember , sr is a group of symmetries only ) . general relativity / gravity adds dynamical degrees of freedom , and while you can do quantum field theory on top of gr ( e . g . , hawking radiation ) , it is necessarily incomplete . here 's a previous question on why gravity needs to be quantized long story short : imagine doing a double-slit experiment with very heavy particles . since the particles couple to gravity , we would expect their gravitational back-reaction to exhibit quantum behaviour , and , therefore , gravity itself . unfortunately , just adding gravity as another field and plodding along does not work . there has been a lot of promising progress in string theory though . some would say too much progress ; )
the formula that you write out is just a consequence of linearity without any additional requirements . suppose you had a vector $\mathbf{d}$ that was linearly dependent on another vector $\mathbf{e}$ . then , one would write $d_i =\sum_j \epsilon_{ij}\ e_j$ . with no further conditions , $\epsilon_{ij}$ would have $9=3\times3$ coefficients . in your formula , the viscosity tensor $\tau_{ij}$ has $9$ components ( assuming no symmetry ) . if it depends linearly on the velocity gradients , $\partial_i v_j$ , which again has $9$ components . then , the most general formula compatible with linearity would be the one that you have written out . the tensor $\mu_{ijkl}$ has $9\times9=81$ components assuming no symmetry .
$$u_{g , i}=-g\dfrac{m_{moon} . m}{\underbrace{r_{moon}}_{ ( \ distance \ from \ moon\ center ) }}-g\dfrac{m_{earth} . m}{\underbrace{ ( 60r_{earth}-r_{moon} ) }_{ ( initial \ distance\ from\ earth ) } } $$ you missed $u_i$ due to earth !
like the comment aheadd said , i was doing it wrong , using perimeter instead of area ( $2\pi r$ instead of $\pi r^2$ ) . if you wan to watch the result look here
" escape velocity " is really just a measure of the kinetic energy an object near the surface of the earth would need to have to start with in order to just run out of energy at the point where it was infinitely far from earth , having converted all of its initial kinetic energy to gravitational potential energy . even if you built a giant ladder or a space elevator or whatever , the total energy required to get to orbit is exactly the same , it just comes in a less spectacular form . rather than burning a rocket the whole way , you would be doing a slower conversion of energy into gravitational potential energy-- electricity running a motor to winch the rocket up to the top of a space elevator , or chemical energy from food as you climbed a bazillion stairs to get there , or whatever . a space elevator would be an attractive way to get a rocket into orbit , or away from the earth because it reduces the amount of rocket fuel you need to use to get there , replacing it with some other source that is more convenient ( and less explosive ) to work with . but you still need the same total amount of energy to get your payload into orbit .
energy is any quantity - a number with the appropriate units ( in the si system , joules ) - that is conserved as the result of the fact that the laws of physics do not depend on the time when phenomena occur , i.e. as a consequence of the time-translational symmetry . this definition , linked to emmy noether 's fundamental theorem , is the most universal among the accurate definitions of the concept of energy . what is the " something " ? one can say that is a number with units , a dimensionful quantity . i can not tell you that energy is a potato or another material object because it is not ( although , when stored in the gasoline or any " fixed " material , the amount of energy is proportional to the amount of the material ) . however , when i define something as a number , it is actually a much more accurate and rigorous definition than any definition that would include potatoes . numbers are much more well-defined and rigorous than potatoes which is why all of physics is based on mathematics and not on cooking of potatoes . centuries ago , before people appreciated the fundamental role of maths in physics , they believed e.g. that the heat - a form of energy - was a material called the phlogiston . but it is a long , long time ago when experiments were done to prove that such a picture was invalid . einstein 's $e=mc^2$ partly revived the idea - energy is equivalent to mass - but even the mass in this formula has to be viewed as a number rather than something that is made out of pieces that can be " touched " . energy has many forms - terms contributing to the total energy - that are more " concrete " than the concept of energy itself . but the very strength of the concept of energy is that it is universal and not concrete : one may convert energy from one form to another . this multiplicity of forms does not make the concept of energy ill-defined in any sense . because of energy 's relationship with time above , the abstract definition of energy - the hamiltonian - is a concept that knows all about the evolution of the physical system in time ( any physical system ) . this fact is particularly obvious in the case of quantum mechanics where the hamiltonian enters the schrödinger or heisenberg equations of motion , being put equal to a time-derivative of the state ( or operators ) . the total energy is conserved but it is useful because despite the conservation of the total number , the energy can have many forms , depending on the context . energy is useful and allows us to say something about the final state from the initial state even without solving the exact problem how the system looks like at any moment in between . work is just a process in which energy is transformed from one form ( e . g . energy stored in sugars and fats in muscles ) to another form ( furniture 's potential energy when it is being brought to the 8th floor on the staircase ) . that is when " work " is meant as a qualitative concept . when it is a quantitative concept , it is the amount of energy that was transformed from one form to another ; in practical applications , we usually mean that it was transformed from muscles or electrical grid or battery or another " storage " to a form of energy that is " useful " - but of course , these labels of being " useful " are not a part of physics , they are a part of the engineering or applications ( our subjective appraisals ) .
the first and second laws are not original to newton , since we know hooke deduced that gravity must be an inverse square centripetal force in the 1670s without input from newton . the calculations of centripetal force were then current , and the first and second law naturally follow from galileo 's work on falling bodies . you can read a lucid discussion of the history in julian barbour 's " absolute or relative motion ? " which focuses on this time period . the idea that bodies stay in motion unless acted upon is already floating in the air due to the heliocentric model and the certainty of the rotating earth . the third law , however , is new to newton . he uses it both as a philosophical position and as a way to justify the first and second laws in composite bodies . the third law is essentially a statement of conservation of momentum , and newton also includes conservation of angular momentum in the first law ( as you can see by his example of a spinning body which keeps spinning and moving absent a disturbance ) . the third law allows newton to build up a complete physical science , since he is able to deduce the laws for large bodies assuming laws for the microscopic corpuscules he believed were down below , composing the large bodies . his implicit model for the world is one of atoms interacting by pairwise attractions and repulsions , and asymptotically interacting by gravity . he believes that it is the pairwise nature of the atomic forces that leads to the law of conservation of angular momentum , since there is no change in angular momentum during a pairwise radial attraction/repulsion event . it is this model that leads him to a corpuscular theory of light , since he cannot bear the idea that matter is one thing and light another .
the redshift due to cosmological expansion is identical to a doppler shift in its effect on the spectrum of any source . to be specific , both phenomena " stretch " all wavelengths by the same factor . there is a very good reason for this : in a suitable coordinate system , the cosmological redshift is a doppler shift . you will find statements in some textbooks saying that this is not true , but a weak version of this statement , which is nonetheless strong enough to explain why the effects on the spectra are identical , is uncontroversially true . to be specific , the redshift of a distant galaxy can be thought of as the accumulation of many infinitesimal doppler shifts along the line of sight . ( each member of a family of comoving observers is in motion relative to her neighbor , and each can " watch " the redshift build up gradually due to these relative velocities . ) one perspective on this subject ( mine , to be precise ) can be found in this paper . even if you do not like our point of view in this paper , our description of and references to other treatments may be of interest .
right now you have the equation $l\psi = 0$ where $l$ is the linear operator in parentheses in your first equation . the situation to use green 's functions is when you have a solution for $\psi$ of $l\psi = \delta ( x-x' ) $ , and you want to find a solution for $\psi$ of $l\psi = f ( x ) $ for some arbitrary function $f ( x ) $ . the situation you have is different . here it is not the right hand side that is changing$-$your right hand side is always zero$-$but it is the operator that is changing . so the method of green 's functions is not applicable here . as far as ways to solve the schroedinger equation , i think it is a hard problem . i want to say that most people just do it numerically . perhaps some one else will have a better answer .
since you do not fully understand the answer of jamals , i will try to explain it shorter and easier for you . if all other forces of nature have some particles associated with them why should gravity be an exception ? no , it is not an exception . physicists believe that they do appear , it is just they have not found it yet . standard model does not have gravity , but extended standard model may contain it . thanks to string theory . what exactly is the gravitational field and how does it spread over an infinite distance and cause the gravitational force to operate ? it is exactly the space and the time . how do space and time appear ? big bang . how does gravity operate ? a change in space and time give you a gravitation force . like a change in position gives you velocity , a change in energy gives you a work . a change is very important , it will give you another interesting point . if you have studied differential , you now know how important it is : describing a change . to real physicists : i am talking with a high school student . edit : by referring a change in space and time , i do not mean like a car , travels through cities from morning to afternoon . it is like reforming it . from wikipedia : matter changes the geometry of spacetime , this ( curved ) geometry being interpreted as gravity .
you land slightly ahead of where you jumped . as mentioned in the comments , see here . the coriolis effect only applies to things that are moving in the rotating reference frame . if the air is stationary in the rotating frame , it feels only the centrifugal force . there will be a pressure gradient , creating buoyancy , just like on earth , but all the air will have the same angular velocity . ( this means the air closer to the center of rotation has lower linear velocity , and you will feel a slight wind when you jump . ) yes , you become weightless . as seen from an inertial frame comoving with the center of the ship , you are now stationary . of course there would be aerodynamic effects to contend with .
you will not get a shock unless you complete the circuit to ground . this is why power lines can be worked on while live , from a helicopter : helicopter power line maintenance
a common structure for leds the ppin-junction , [p-layer][p electron blocking layer][i layer][n layer]  the p-layer , i-layer and n-layer is just your standard pin-junction structure . the p and n layers provide an electric field , which under forward bias will drive electron and hole towards i-layer where they can recombine radiatively . normally the p and n layers will be made of a higher band gap material ( e . g . algaas ) and form a barrier around the lower band gap i-layer ( e . g . gaas ) . this helps the i-layer act as a radiative recombination centre to improve efficiency . however , it is possible for electrons and holes to pass over the i-region without recombining radiative , in leds this is loss mechanism . electrons are much more mobile than holes ( by about an order of magnitude ) , so if we can prevent electrons from escaping we should be able to improve the efficiency of the led ( i.e. . the electrons are the rate limiting carrier type ) . this is the role of the second p-layer or electron blocking layer . if the electron blocking layer has a wider band gap ( so algaas with a higher al fraction than the n and p layers ) , and has more p-dopant , then it is possible to align the valence band with the p-layer ( obviously there is a space charge at the hetero-interface where the bands are not flat ) . this gives a large barrier only in the conduction band which prevents electrons from crossing the junction ( and escaping ) but does not restrict the hole current flow , thus improving the device efficiency . so this is why ppn or ppin structures are used in practice . it is an optimisation .
as mentioned in the comments , air from a fan feels colder because of two reasons . firstly , the air is cooler than your body so as it passes , your body heat transfers in part to the air , which is then carried away . second , as the air passes , it evaporates moisture on your skin , which takes absorbs heat in the process . as for the propagation of cold temperatures in general . if you put an ice block out on the counter , the air around it will get colder and this coldness will seem to radiate outward . this is not because the air molecules are passing on a lack of vibration , it is because they are passing their heat to the colder molecules of the ice block . the excited , warm air around the ice block bumps into the molecules of the ice block and transfers energy to it . this causes that air to lose heat and be colder . then the air immediately around that transfers heat to the first layer of air and loses heat itself . repeat ad nauseam . this is all because objects tend to want to be in thermal equilibrium . to make everything the same temperature , heat tends to flow towards colder areas , which can seem like cold flowing outwards to warmer areas . in short , what seems like cold flowing outward is actually heat flowing inward .
it does give the same result--- it would be good to give your steps , if you want to find the error . the easiest way to see this is to just use gauss 's law : if you make a cylinder of radius r , the electric field outward times the surface area is the charge inside . if the length of the gaussian cylinder is l , and the inner and outer radius of the physical cylindrical annulus is $r_0$ , $r_1$ , then for the exterior $$l 2\pi r e ( r ) = l \rho ( \pi r_1^2 - \pi r_0^2 ) $$ which gives you the standard 1/r field , log ( r ) potential , outside a charged cylinder and by its form is equivlent to the difference-of-two-cylinders solution . for inside the inner cylinder , the field is zero , and between the two , it is that sum of a 1/r and constant which matches the other two regions . the field in all regions can be found by gaussian cylinders , and the potential is found by integrating in r . the result satisfies laplace 's equation with source , since you can prove gauss 's law for solutions of this equation directly .
as has been remarked by others and explained clearly , and mathematically , the eigenvalues are important because a ) they allow you to solve the time-dependent equation , i.e. , solve for the evolution of the system and b ) a state which belongs to the eigenvalue $e$ , i.e. , as we say , a state which is an eigenstate with eigenvalue $e$ , has an expectation value of the energy operator which is easy to see has to be $e$ itself . but those explanations are advanced and rely on the maths . and they do not explain why $e$ should be considered ' an energy level ' . at some risk , i will try to answer your question more physically . what is the physical reason why the energy states of a system , e.g. , an atom , are the eigenvalues of the operator $h$ that appears in the time-independent schroedinger equation ? well , first , note that it is absolutely the same $h$ that appears in the time-dependent schrodinger equation , $$h\cdot \psi = -i{\partial \psi \over \partial t}$$ which controls the rate of change of $\psi$ . the answer does not come from the classical hamiltonian or lagrangian mechanics , but from the then-new quantum properties of nature . a non-classical feature of qm is that some states are stationary , which means they do not change in time . e.g. , the electron in a bohr orbit is actually not moving , not orbiting at all , and this solves the classical paradoxes about the atom ( why the rotating charge does not radiate its energy away and fall into the centre ) . the first key point is that an eigenstate is a stationary state : what is the explanation for this ? well , schroedinger 's time dependent equation clearly says that , up to a constant of proportionality , the time-rate of change of any state $\psi$ is found by applying the operator $h$ ( the hamiltonian : we do not yet know it is also the energy operator ) to it : the new vector or function $h\cdot\psi$ is the change in $\psi$ per unit time . obviously if this is zero , $\psi$ does not change ( this was the only classical possibility ) . but also if $h\cdot\psi$ is even a non-zero multiple of $\psi$ , call it $e\psi$ , then $\psi$ plus this rate of change is still a multiple of $\psi$ , so as time goes on , $\psi$ changes in a trivial fashion : just to another multiple of itself . in qm , a multiple of the wave function represents the same quantum state , so we see the quantum state does not change . now the next key point is that a state with a definite energy value must be stationary . why ? in qm , it is not automatic that a system has a definite value of a physical quantity , but if it does , that means its measurement always leads to the same answer , so there is no uncertainty . so if there is no uncertainty in the energy , by heisenberg 's uncertainty principle there must be infinite uncertainty in something else , whatever is ' conjugate ' to energy . and that is time . you cannot tell the time using this system , which implies it is not changing . so it is stationary . ( remember , we are not assuming that $h$ is also the energy operator and we are not assuming the formula for expectations ) . thus being an eigenstate of $h$ implies $\psi$ is stationary . and having a definite energy value implies it is stationary . being physicists , we now conclude that being an eigenstate implies it has a definite energy value , which answers your question , and these are the ' energy levels ' of a system such as an atom : a system , even an atom , might not possess a definite energy , but if it does not , it will not be stationary , and being microscopic , the time-scale in which it will evolve will be so rapid we are unlikely to be able to observe its energy , or even care ( since it will not be relevant to molecules or chemistry ) . so , ' most ' atoms for which we can actually measure their energy must be stationary : this is ' why ' the definite values of energy which a stationary state can possess are called the ' energy levels ' of the system , and historically were discovered first , before schroedingers equation . from a human perspective , most atoms that we care about spend most of their time that matters to us in an approximately stationary state . in case you are wondering why time is the conjugate to energy , whereas heisenberg 's original analysis of his uncertainty principle showed that position was conjugate to momentum , we rely on relativity : time is just another coordinate of space-time , and so is analogous to position . and in relativistic mechanics , momentum in a spatial direction is analogous to energy ( or mass , same thing ) . in the standard relativistic equation $$p^2-m^2=e^2 , $$ we see that momentum ( $p$ ) and mass $m$ are symmetric ( except for the negative sign ) with each other . so since momentum is conjugate to position , $m$ or energy must be conjugate to time . for this reason , bohr was able to extend heisenberg 's analysis , of the uncertainty relations between measurements of position and measurements of momentum , to show the same relations between energy and time .
good question ! since you are using the term tem mode , i suppose you are familiar with transmission line theory already . if not , have a look here . for more details consult any introductory electromagnetic books . in my answer i will assume you are familiar with it as a simple example to answer your question , i will focus at coaxial cable ( everything else have the same concept ) . the next figure shows a transmission line circuit assume that it is a coaxial cable ( inner conductor surrounded by outer conductor ) . a cross section of the transmission line looks like this i suppose everything is clear in the picture , now your question " how come the current is perpendicular to electric field " . as you know from the boundary conditions of perfect electric conductor ( as assumed here ) , the normal electric field at certain point gives rise to surface charge density . so in the cross section figure there is a positive surface charge on the surface of the inner conductor and a negative surface charge on the surface of the outer conductor . i hope it is clear up to here . now from em wave theory , we know that the electric field varies in time and in the direction of propagation of the wave , which means that the surface charge density varies in time and along the direction of propagation of em wave . from charge continuity equation ( equation in the second page of this presentation ) , we know that a time varying charge density gives a rise to net current ( non-zero divergence of current density ) . in the particular case in the cross section plot , the charge density only resides on the surface so it is a surface charge density . accordingly , the current should be flowing only at the surface of the conductors not within it . i took this picture from internet so it is not my mistake . at the load , the inner conductor is connected to the outer conductor through the load , since we said there is a positive surface charge on the inner conductor and negative surface charge on the outer , they move toward each other through the load . that is the source of current in the load . the way the load is connected to coaxial cable is shown below do not forget that the electric field is changing , so the sign and the magnitude of the surface charges on both conductors keep changing . i hope that helped
it is possible . but it does not change anything about the " we can not know through which slit it went and still get an interference pattern " issue . ( surprise ! ) in order to do this you need photons that can be timed very precisely , i.e. $\delta t$ needs to be small : smaller than $c/d$ where $d$ is the slit seperation . according to heisenberg uncertainty , this means that the energy uncertainty must be big for $\delta e\cdot\delta t&gt ; \hbar$ to remain satisfied . if $\delta e$ is big , then the wavelength uncertainty will also be big ; namely $$ \delta\lambda \approx \frac{\delta\nu}{c} = \frac{\delta e}{2\pi\hbar c} &gt ; \frac{d}{2\pi} $$ but if the wavelength uncertainty is in the order of the slit seperation , there will not be a proper interference pattern as this requires the light to be coherent on the length scales of the experiment !
the best intuition is a calculation but in this simple case , the calculation is really intuitive so you should not turn off when you hear the word " calculation " . the height reached by initial velocity $v$ is the height of the object after the initial velocity $v$ drops to $0$ ( and then reverts the sign ) because of the downward acceleration $g$ . how much time does it take to reduce the velocity from $v$ to $0$ ? well , the acceleration is the velocity change per unit time and it is $g$ . so the time needed to reduce the velocity to zero is $$t=\frac vg . $$ now , how far the object gets after time $t$ ? it is simple : the distance is the velocity times time . but the velocity is changing . you must use the average velocity to calculate the distance : $$h=\bar v\cdot t . $$ what is the average velocity ? because the velocity is dropping linearly , it is just $1/2$ of the sum of the initial velocity $v$ and the final velocity $0$: $$\bar v = \frac{v+0}{2} = \frac v2 $$ so we have $$h=\bar v\cdot t = \frac v2\cdot t = \frac v2\cdot \frac vg = \frac{v^2}{2g}$$ the factor of $1/2$ came from the need to calculate the average velocity and the average of two numbers is one-half ( $1/2$ ) of their sum . i do not need any kinetic energy to do the calculation , as you can see . but the factor $1/2$ in the formula for the kinetic energy $e=mv^2/2$ has a totally analogous origin : it is the total work you have to do to accelerate the object from velocity $v=0$ to velocity $v$ . the force one has to overcome is $f=ma$ and the work is $f\cdot \delta x$ but $\delta x=\bar v \cdot t = ( 0+v ) t/2$ so we have $$ e = ma\cdot x = ma\cdot vt/2 = mv^2/2$$ where i used $v=at$ because the velocity was assumed to increase uniformly again . once again , i calculated the average velocity which turned out to be $1/2$ of the maximal ( final ) one . equivalently , the factor $1/2$ comes from the fact that the indefinite integral of $x$ over $x$ is $x^2/2$ . for example , the integral $\int v\ , dt = \int at\ , dt=at^2/2$ gives the total distance travelled in accelerated motion – and all the other examples above are analogous . but one does not have to learn the full-fledged theory of integration for this outcome : the integral is the area under the curve and for a linear function $y=x$ between $x=0$ and $x=x$ , the area is just a triangle whose area is simply $1/2$ of the area of the square or rectangle ; again , this $1/2$ is the same $1/2$ we obtained from the averaging above . so the consequences of this particular simple integral may be understood even without any broader knowledge of integration ( or differentiation ) , and that is what was done above .
the statement you quote is correct and slightly profound till you understand it well enough that it becomes simple :- ) if a system is in an energy eigenstate , then it must exist in that state for all time -- from $t \rightarrow - \infty$ to $t \rightarrow + \infty$ . a few comments : clearly , any physical state you create in a lab does not have this property . it was created at a finite time and will be destroyed ( observed ) at a finite time . so any state you might create in a lab cannot be an exact energy eigenstate . it might be one , to a very good approximation . loosely , a naive " energy-time " uncertainty relation tells you that the longer the state survives , the more definite its energy . ( energy-time uncertainty in qm is a tricky thing , but think of the analogy with the accuracy in specifying the frequency of a wave compared to the time over which you observe the wave ) . when deriving the eigenstates of some system , you assume that the system is isolated . but , to deal with any system in the lab , you have to interact with it in some manner . in such a case , the energy eigenstates of the coupled system are no longer the energy eigenstates of the isolated system . with both those caveats , energy eigenstates still form a reasonably convenient basis for studying the system . you could in principle take the eigenspectrum of any suitable operator as a basis for your linear vector space , but more often than not , those states evolve in time ( since you do not look at the system only for an instant ) -- hence , energy eigenstates form a very convenient basis . if you ever have to study states which you actually create/measure in the lab , then you can consider them to be superpositions of energy eigenstates . since te structure of quantum mechanics is linear , all the analysis you might want to do proceeds in a fairly straight-forward manner .
in this context , the least ambiguous reference frame is the comoving rest frame . in essence , this is the local frame moving along with the local hubble expansion . we can accurately determine earth 's velocity with respect to this comoving frame ( and thereby obtain our so-called peculiar velolocity ) by subtracting out the dipole anisotropy from the doppler shift of the microwave background radiation ( the afterglow of the big bang ) . it follows that earth moves at a speed of 371 km/s in the direction of constellation leo . now you can do the math using 371 km/s and the fact that there are 3600 x 24 seconds in a day and 365.25 days in a year .
there is no unique way to answer this question . the problem is the following . you are asking if , at a specific proper time , a black hole of some radius exists . however , to do this you have to say something like " the black hole has a 10m radius at the same time that the observer 's proper time is 5sec " . such simultaneous events are not well defined in gr unless you make an arbitrary choice of a coordinate system with a time coordinate . nonetheless , if you make such a choice , it is true that the black hole will start with a radius of 0 inside of the infalling matter and then get larger . eventually it will reach a maximum radius and then get smaller through hawking radiation . so what will an observer literally see if she observes the star collapsing ? the answer is that the observer will see infalling matter slow down and thermalize . the matter will never appear to " fall in " as far as this observer is concerned . however , the particles/light emitted will be redshifted more and more ( and before long the only outgoing particles of reasonably not-low energy will be from hawking radiation ) .
nonzero fluxes are required because of some equations of motion linking them to a nonzero euler character . once they are there , they induce a superpotential that stabilizes some moduli , usually the complex structure moduli ( the very " stabilizes " means that the allowed values of these moduli at which the total potential has a local minimum is discrete , assuming fixed values of other moduli ) . the dilaton-axion field is stabilized by the gukov-vafa-witten superpotential while nonperturbative effects are typically needed to stabilize the kähler moduli . see http://motls.blogspot.com/2006/04/flux-compactifications-of-m-theory-and.html this is of course a big technical topic and the literature on stabilized flux vacua is large . one can not describe everything in a single comment . the becker-becker-schwarz textbook probably has a good treatment of these matters .
your question is asking you to calculate the de broglie wavelength of the electron , which can be obtained from its momentum $p$ via de broglie 's relation $$\lambda=\frac h p$$ where $h$ is planck 's constant . regarding units : you should work consistently with your units . you can work with the kinetic energy in joules , $h$ in j s and $p$ in kg m/s . but you can also work with $e$ in hartrees , $p$ in atomic units , $hbar=1$ , and get $λ$ in multiples of the bohr radius . the formula you mentioned in the comments , $$\lambda = \sqrt{\frac{150}{\text{k . e . in }e\text v}}\text å , $$ works too , though it has limited precision . i would encourage you instead to never drop the units from your calculation : $$\lambda=\frac{h}{\sqrt{2m k}}=\frac{6.6\times10^{-34}\ , \text{kg}\ , \text m^2\ , \text s^{-1}}{\sqrt{2\times9.1\times10^{-31}\ , \text{kg}\times 8\times13.6\ , e\text v}} =4.7\times10^{-20}\frac{\text{kg}\ , \text m^2\ , \text s^{-1}}{\sqrt{\text{kg}\ , e\text v}} , $$ and since $1\ , e\text v=1.6\times10^{-19}\ , \text j$ , you have $$1\frac{\text{kg}\ , \text m^2\ , \text s^{-1}}{\sqrt{\text{kg}\ , e\text v}} =\frac{1}{\sqrt{1.6\times10^{-19}}}\frac{\text{kg}\ , \text m^2\ , \text s^{-1}}{\sqrt{\text{kg}^2\ , \text m^2\ , \text{s}^{-2}}}=2.5\times10^9\ , \text m , $$ which gives $$\lambda=1.2\times10^{-10}\ , \text m=1.2\ , \text å . $$
the reason for this result is the sign in you damping term . for a damped harmonic oscillator you need to have a resistive force on the mass point at $x$ . that means if $x=0$ is the equilibrium position the damping term will be proportional to the velocity with an negative constant $f_{\text{damp}} = -ax&#39 ; , a&gt ; 0$ . i.e. the total force on the mass point is the damping term and the spring ( hooke 's laq ) term $f_{\text{spring}}=-kx$ . the total equation of motion then is : $$f=ma=-kx-ax&#39 ; $$ or $$mx&#39 ; &#39 ; +ax&#39 ; +mx = 0 , a , m&gt ; 0$$ your result then is going to infinite because your system is not actually damped but anti-damped . it means there is a force that accelerates your mass point away from the equilibrium position the more the faster it becomes , which will clearly lead to large values very fast .
if you believe wholeheartedly in mach 's principle , then there is no way to test empirically for rotation of the universe as a whole , since there is nothing else for it to be rotating relative to . however , general relativity is not very machian , and it offers a variety of ways in which an observer inside a sealed laboratory can detect whether the lab is rotating . for example , she can observe the motion of a gyroscope , or measure whether the sagnac effect is zero . there are alternative theories of gravity , such as brans-dicke gravity , that are more machian than gr , [ brans 1961 ] and in these theories there is probably no meaningful sense in which the universe could rotate . however , solar-system tests [ bertotti 2003 ] rule out any significant deviations from gr of the type predicted by brans-dicke gravity , so that it appears that the universe really is as non-machian as gr says it is . it is therefore possible according to general relativity to have cosmologies in which the universe is rotating . historically , one of the earliest cosmological solutions to the einstein field equations to be discovered was the gödel metric , which rotates and has closed timelike curves . if we lived in a rotating universe such gödel 's example , the rate of rotation would have to be expressed in terms of angular velocity , not angular momentum . angular velocity is what is measured by a gyroscope or the sagnac effect , and gr does not even have a definition of angular momentum that applies to cosmological spacetimes . a rotating universe does not have to have a center of rotation , and it can be homogeneous . in other words , we could determine a direction in the sky and say that the universe was rotating counterclockwise at a certain rate about the line connecting us to that point on the celestial sphere . however , aliens living somewhere else in the universe could do the same thing . their line would be parallel to ours , but there would be no way to tell whether one such line was the real center of rotation . to find out whether the universe is rotating , in principle the most straightforward test is to watch the motion of a gyroscope relative to the distant galaxies . if it rotates at an angular velocity -ω relative to them , then the universe is rotating at angular velocity ω . in practice , we do not have mechanical gyroscopes with small enough random and systematic errors to put a very low limit on ω . however , we can use the entire solar system as a kind of gyroscope . solar-system observations put a model-independent upper limit of 10^-7 radians/year on the rotation , [ clemence 1957 ] which is an order of magnitude too lax to rule out the gödel metric . a rotating universe must have a certain axis of rotation , so it must have a particular type of anisotropy that picks out a certain preferred direction . we can therefore look at the cosmic microwave background and see whether its anisotropy contains a preferred axis . [ collins 1973 ] such observations impose a limit that is tighter than provided by solar-system measurements ( perhaps 10^-9 rad/yr [ su 2009 ] or 10^-15 rad/yr [ barrow 1985 ] ) , but such limits are model-dependent . because all of the present observation are consistent with zero rotational velocity , it is not possible to attribute any prominent cosmological role to rotation . centrifugal forces cannot contribute significantly to cosmological expansion , or to the way your head feels when you are hung over . brans and dicke , " mach 's principle and a relativistic theory of gravitation , " phys . rev . 124 ( 1961 ) 925 , http://loyno.edu/~brans/st-history/ bertotti , iess , and tortora , " a test of general relativity using radio links with the cassini spacecraft , " nature 425 ( 2003 ) 374 clemence , " astronomical time , " rev . mod . phys . vol . 29 ( 1957 ) 2 collins and hawking , " the rotation and distortion of the universe , " mon . not . r . astr . soc . 162 ( 1973 ) 307 hawking , " on the rotation of the universe , " mon . not . r . astr . soc . 142 ( 1969 ) 529 barrow , juszkiewicz , and sonoda , " universal rotation : how large can it be ? , " mon . not . r . astr . soc . 213 ( 1985 ) 917 , http://adsabs.harvard.edu/full/1985mnras.213..917b su and chu , " is the universe rotating ? , " 2009 , http://arxiv.org/abs/0902.4575 [ this is a physicsforums faq entry that i wrote with input from users george jones , jim mcnamara , marcus , pallen , tiny-tim , and vela . ]
most of the loss is the simple thermal conductivity from the hot ( radioactive source ) to the cold ( atmosphere ) . if you have a pretty much infinite cold sink you are going to lose all the heat eventually so getting 6% of it as electricity is pretty good . do not know what the internal temperature of the source is , but guessing it is around 300-500c ( any higher and the metalurgy gets challenging ) . and a martian atmosphere of -50c so the maximum eff = 1 - ( 273-50 ) / ( 273+300 ) = 60% , 70% at 500c i suspect you could get closer to this with a stirling engine but a thermo-electric generator has no moving parts .
wow , this one has been over-answered already , i know . . . but it is such a fun question ! so , here 's an answer that has not been , um , " touched " on yet . . . : ) you sir , whatever your age may be ( anyone with kids will know what i mean ) , have asked for an answer to one of the deepest questions of quantum mechanics . in the quantum physics dialect of high nerdese , your question boils down to this : why do half-integer spin particles exhibit pauli exclusion - that is , why do they refuse to the be in the same state , including the same location in space , at the same time ? you are quite correct that matter as a whole is mostly space . however , the specific example of bound atoms is arguably not so much an example of touching as it is of bonding . it would be the equivalent of a 10 year old son not just poking his 12 year old sister , but of poking her with superglue on his hand , which is a considerably more drastic offense that i do not think anyone would be much amused by . touching , in contrast , means that you have to push - that is , exert some real energy - into making the two objects contact each other . and characteristically , after that push , the two object remain separate ( in most cases ) and even bound back a bit after the contact is made . so , i think one can argue that the real question behind " what is touching ? " is " why do solid objects not want to be compressed when you try to push them together ? " if that were not the case , the whole concept of touching sort of falls apart . we would all become at best ghostly entities who cannot make contact with each other , a bit like chihiro as she tries to push haku away during their second meeting in spirited away . now with that as the sharpened version of the query , why do objects such a people not just zip right through each other when they meet , especially since they are ( as noted ) almost entirely made of empty space ? now the reflex answer - and it is not a bad one - is likely to be electrical charge . that is because we all know that atoms are positive nuclei surrounded by negatively charged electrons , and that negative charges repel . so , stated that way , it is perhaps not too surprising that when the outer " edges " of these rather fuzzy atoms get too close that their respective sets of electrons would get close enough to repel each other . so by this answer , " touching " would simply be a matter of atoms getting so close to each other that their negatively charged clouds of electrons start bumping into each other . this repulsion requires force to over come , so the the two objects " touch " - reversibly compress each other without merging - through the electric fields that surround the electrons of their atoms . this sounds awfully right , and it even is right . . . to a limited degree . here 's one way to think of the issue : if charge was the only issue involved , then why do some atoms have exactly the opposite reaction when their electron clouds are pushed close to each other ? for example , if you push sodium atoms close to chlorine atoms , what you get is the two atoms leaping to embrace each other more closely , with a resulting release of energy that at larger scales is often described by words such as " boom ! " so clearly something more than just charge repulsion is going on here , since at least some combinations of electrons around atoms like to nuzzle up much closer to each other instead of farther away . what then guarantees that two molecules will come up to each other and instead say " howdy , nice day . . . but , er , could you please back off a bit , it is getting stuffy ? " that general resistance to getting too close turns out to result not so much from electrical charge ( which does still plays a role ) , but rather from the pauli exclusion effect i mentioned earlier . pauli exclusion is often skipped over in starting texts on chemistry , which may be why issues such a what touching means are also often left dangling a bit . without pauli exclusion , touching - the ability of two large objects to make contact without merging or joining - will always remain a bit mysterious . so what is pauli exclusion ? it is just this : very small , very simple particles that spin ( rotate ) in a very peculiar way always , always insist on being different in some way , sort of like kids in large families where everyone wants their unique role or ability or distinction . but particles , unlike people , are very simple things , so they only have a very limited set of options to choose from . when they run out of those simple options , they have only one option left : they need their own bit of space , apart from any other particle . they will then defend that bit of space very fiercely indeed . it is that defense of their own space that leads large collections of electrons to insist on taking up more an more overall space , as each tiny electron carves out its own unique and fiercely defended bit of turf . particles that have this peculiar type of spin are called fermions , and ordinary matter is made of three main types of fermions : protons , neutrons , and electrons . for the electrons there is only one identifying feature that distinguishes them from each other , and that is how they spin : counterclockwise ( called " up" ) or clockwise ( called " down" ) . you had think they had have other options , but that too is a deep mystery of physics : very small objects are so limited in the information they carry that they can not even have more than directions to choose from when spinning around . however , that one options is very important for understanding that issue of bonding that must be dealt with before atoms can engage in touching . two electrons with opposite spins , or with spins that can be made opposite of each other by turning atoms around the right way , do not repel each other : they attract . in fact , they attract so much that they are an important part of that " boom ! " i mentioned earlier for sodium and chlorine , both of which have lonely electrons without spin partners , waiting . there are other factors on how energetic the boom is , but the point is that until electrons have formed such nice , neat pairs , they do not have as much need to occupy space . once the bonding has happened , however - once the atoms are in arrangements that do not leave unhappy electrons sitting around wanting to engage in close bonds - then the territorial aspect of electrons comes to the forefront : they begin defending their turf fiercely . this defense of turf first shows itself in the ways electrons orbit around atoms , since even there the electrons insist on carving out their own unique and physically separate orbits , after that first pairing of two electrons is resolved . as you can imagine , trying to orbit around an atom while at the same time trying very hard to stay away from other electron pairs can lead to some pretty complicated geometries . and that too is a very good thing , be cause those complicated geometries lead to something called chemistry , where different numbers of electrons can exhibit very different properties due to new electrons being squeezed out into all sorts of curious and often highly exposed outside orbits . in metals it gets so bad that the outermost electrons essentially become community children that zip around the entire metal crystal instead of sticking to single atoms . that is why metal carry heat and electricity so well . in fact , when you look at a shiny metallic mirror , you are looking directly at the fastest-moving of these community-wide electrons . it is also why in outer space you have to be very careful about touching two pieces of clean metal to each other , because with all those electrons zipping around , the two pieces may very decide to bond into a single new piece of metal instead of just touching . this effect is called vacuum welding , and it is an example of why you need to be careful about assuming that solids that make contact will always remain separate . but many materials , such a you and your skin , do not have many of these community electrons , and are instead full of pairs of electrons that are very happy with the situations they already have , thank you . and when these kinds of materials and these kinds of electrons approach , the pauli exclusion effect takes hold , and the electrons become very defensive of their turf . the result at out large-scale level is what we call touching : the ability to make contact without easily pushing through or merging , a large-scale sum of all of those individual highly content electrons defending their small bits of turf . so to end , why do electrons and other fermions want so desperately to their own bits of unique state and space all to themselves ? and why , in every experiment ever done , is this resistance to merger always associated with that peculiar kind of spin i mentioned , a form of spin that is so minimal and so odd that it can not quite be described within ordinary three-dimensional space ? we have fantastically effective mathematical models of this effect . it has to do with antisymmetric wave functions . these amazing models are instrumental to things such as the semiconductor industry behind all of our modern electronic devices , as well as chemistry in general and of course research into fundamental physics . but if you ask the " why " question , that becomes a lot harder . the most honest answer is , i think , " because that is what we see : half-spin particles have antisymmetric wave functions , and that means they defend their spaces . " but linking the two together tightly - something called the spin-statistics problem - has never really been answered in a way that richard feynman would have called satisfactory . in fact , he flatly declared more than once that this ( and several other items in quantum physics ) were still basically mysteries for which we lacked really deep insights into why the universe we know works that way . and that , sir , is why your question of " what is touching ? " touches more deeply on profound mysteries of physics than you may have realized . it is a good question . 2012-07-01 addendum here is a related answer i did for s.e. chemistry . it touches on many of the same issues , but with more emphasis on why " spin pairing " of electrons allows atoms to share and steal electrons from each other -- that is , it lets them form bonds . it is not a classic textbook explanation of bonding , and i use a lot of informal english words that are not mathematically accurate . but the physics concepts are accurate . my hope is that it can provide a better intuitive feel for the rather remarkable mystery of how an uncharged atom ( e . g . chlorine ) can overcome the tremendous electrostatic attraction of a neutral atom ( e . g . sodium ) to steal one or more of its electrons .
the voltage drop across r 2 becomes 0 , and the full voltage is applied across r 1 instead .
you have everything pretty much correct . if you have a piece of semiconductor with $10^{18}$ electrons , a full valence band would be $$\tag{3} \psi ( x_1 , . . . , x_{10^{18}} ) = \left| \begin{matrix} \psi_1 ( x_1 ) and . . . and \psi_{10^{18}} ( x_1 ) \\ \vdots and and \vdots \\ \psi_1 ( x_{10^{18}} ) and . . . and \psi_{10^{18}} ( x_{10^{18}} ) \end{matrix} \right|$$ and then a valence band with five holes in it would be $$\tag{4} \phi ( x_1 , . . . , x_{10^{18}-5} ) = \left| \begin{matrix} \psi_1 ( x_1 ) and . . . and \psi_{10^{18}-5} ( x_1 ) \\ \vdots and and \vdots \\ \psi_1 ( x_{10^{18}-5} ) and . . . and \psi_{10^{18}-5} ( x_{10^{18}-5} ) \end{matrix} \right|$$ those five holes would have the wavefunctions $\psi_{10^{18}-4} ( x ) $ , $\psi_{10^{18}-3} ( x ) $ , $\psi_{10^{18}-2} ( x ) $ , $\psi_{10^{18}-1} ( x ) $ , $\psi_{10^{18}} ( x ) $ . so a single-particle wavefunction in a multiparticle system is just one of the entries of the slater determinant . most metals and semiconductors can be described in the " single-particle approximation " , i.e. there is at least one way to write the slater determinant so that to a very good approximation you can treat each entry $\psi_i$ as a separate particle , behaving like you would expect a typical particle to behave , and only weakly interacting with the other particles ( described by the other $\psi_j$ ) . that is the situation in which people are usually talking about single-electron or single-hole wavefunctions .
i do not work directly in the field , so i will probably give an incomplete answer , but to my knowledge there are two major obstacles . the first consists in keeping the accelerating structure inside the plasma stable . we are not talking about a solid metal radio frequency cavity here , but some charged-fluid nano structures which are very sensitive to a number of perturbations , included the one with themselves . the second problem is keeping the particle to accelerate within this structures both in the transversal plane ( which may come for free as long the plasma bubbles are stable ) but also in the longitudinal which is harder since the laser travels the plasma with velocity $&lt ; c$ while electrons approach $c$ very rapidly . a different approach consists in using a particle beam instead of a laser beam to excite the plasma , this may sound easier but let me report few lines from the awake project at cern : the awake experiment intends to use the existing conventional sps beam to drive a high amplitude wakefield [ - in the plasma ] which will then accelerate a witness electron beam . the sps beam is , however , too long to effectively drive a wakefield . the awake experiment will longitudinally micro-bunch the beam into hundreds of far shorter beams using a wakefield initially driven by a short laser pulse . these beams will then be able to resonantly excite a high amplitude wakefield . the sps aka super proton synchrotron is a 7km ring ( the one at which w and z bosons where discovered and that now injects protons in the lhc ) , so not quite a table-top experiment ! and it is just an experiment : something to prove the concept and develop some fundamental technology . it will require much more effort to develop something that can be really used . this technique is promising but very complex and immature . that is why all the major new projects of any kind of particle-accelerator machines still rely on rf acceleration . a 2 gev electron beam is already pretty interesting : you can have a nice fel , or , colliding two of them , a $\tau/$charm factory . these machines cost typically few hundreds millions $ , they are not at the energy fronter but can produce plenty of physics and eventually nobel prizes . having such a beam over 2cm is for sure very exciting , but the energy is just one of the beam parameters . how many particles are in a bunch , what is the phase space volume occupied by them , what is the repetition rate achievable , how much power is effectively transferred to the beam ? these are the kind of questions that you should ask and whose answers still let you go for rf acceleration .
this is a hard question , because thomas is using terrible language . most of the same calculation is done easily in my answer to this question : how did l.h. thomas derive his 1927 expressions for an electron with an axis ? the right way to do it is to just choose the x axis to be in the direction of the velocity at one time , and the x-y plane to include the acceleration and velocity , and the origin to be the position of the electron . then all of thomas 's manipulations can be quickly done with no vector notation . his results are needlessly obfuscated , especially in this follow-up paper , where the introduction of eulerian angles so deeply irrelevant to the underlying essentially trivial calculation , that i suspect that thomas was on a mission to make his paper unreadable . if you insist on an answer the eulerian angles introduced by thomas are not used later , they just serve to parametrize the rotation matrix $\zeta_0$ . the quantity $d\zeta_0$ is confusing , because it is not the infinitesimal change in the $\zeta_0$ matrix , which would be a tensor of second rank , but a vector , specifying an axis and a magnitude of a rotation . this vector is given by a well known procedure for specifying an infinitesimal rotation . when you have two rotation matrices which differ infinitesimally , $r$ and $r+dr$ , then $dr$ is a bad quantity , because adding rotation matrices does not produce a rotation matrix . the correct way to measure the infinitesimal difference is to consider $r^{-1} ( r+dr ) = i + r^{-1}dr$ , and this tells you that $r^{-1}dr$ is an element of the lie algebra ( by definition--- the product of two rotations is a rotation , and a rotation which is infinitesimally different from the identity is the identity plus an element of the lie algebra ) . there is also $dr r^{-1}$ which works too , they two differ by a rotation . the lie algebra in this case consists of all infinitesimal rotations , which are rotations about some axis with an infinitesimal angle . the axis unit vector times the infinitesimal angle is a vector , not a tensor , and this vector tells you what the infinitesimal difference between two nearby rotations is . the interpretation is that r+dr is r followed by a rotation specified by the vector of $dr r^{-1}$ . the interpretation of the other order $r^{-1} dr$ is that a rotation by this , followed by a rotation by r , gives r+dr . the quantity $d\xi_0$ is this infinitesimal rotation vector for the rotation between the two nearby frames he is considering . rotation vectors are well known from undergraduate mechanics courses--- they are the angular velocities--- the things you cross with the position to find the rate of change of the position under rotation . it is difficult to see this , because he did not say what he was doing . the only purpose of the euler angles is to specify that he is talking about the infinitesimal rotation vector , in the most obscure possible way . once you know that there is an explicit angular velocity vector which is doing a rotation in addition to the thomas precession rotation calculated in the linked answer , you just add the rotation vector of this additional rotation to the rotation vector of the thomas precession to get the full rotation of the frame . this gives his final formula . nowhere do you need to use his intermediate steps . if you want his euler angle conventions . . . his convention for euler angles is as follows : $$ \zeta_0 = a ( \phi ) b ( \theta ) c ( \psi ) $$ where $a$ is a rotation about the z axis by angle $\phi$ , $b$ is a rotation about the y axis by angle $\theta$ , and $c$ is a rotation about the z axis by angle $\psi$ . from this euler angle convention , you can find the formula for the infinitesimal rotation vector by the procedure above . done abstractly , you get : $$ d\zeta_0 \ , \ , \zeta_0^{-1} = da\ , a^{-1} + a\ , \ , db\ , b^{-1} \ , \ , a^{-1} + ab \ , \ , dc c^{-1}\ , \ , b^{-1}a^{-1} $$ the infinitesimal rotation m ( \omega ) corresponding to the vector $\omega$ has the property that $$ r m ( \omega ) r^{-1} = m ( r\omega ) $$ this is easy to prove--- it is transforming between the the two different orders $r^{-1}dr$ and $dr r^{-1}$ , which just changes the axis of rotation by r . mathematicians say that the lie algebra is the adjoint representation , which transforms by matrix-on-one-side , inverse-matrix-on-the-other . for 3d rotations , the adjoint representation is the vector representation , and the equivalence between vectors and adjoints is the formula above . but you can ignore all this jawboning--- the above is something you can understand and check directly . when you turn it into vectors , the first quantity is a rotation purely along the z axis by an amount d\phi , so it is omega vector is purely along the z axis : $$ \omega ( da a^{-1} ) = ( 0,0,1 ) d\phi$$ the second starts off as a pure rotation about the y-axis by an amount $d\theta$ , but it gets rotated about the z axis by $\phi$ ( because of the $a$ and $a^{-1}$ ) , so its omega vector is the y-axis rotated by $\phi$ around the z axis . $$ \omega ( a\ , \ , db\ , b^{-1}\ , \ , a^{-1} ) = ( -\sin ( \phi ) , \cos ( \phi ) , 0 ) d\theta $$ the third would be a rotation about the z axis , or ( 0,0,1 ) but it gets rotated around the y axis by $\theta$ , so into $ ( \sin\theta , 0 , \cos\theta ) $ , then it gets rotated around the z axis by $\phi$ , so into $ ( \sin\theta \cos\phi , \sin\theta \sin\phi , \cos\theta ) $ . this is the rotation vector of the third term . $$ \omega ( ab\ , \ , dc\ , c^{-1}\ , \ , a^{-1}b^{-1} ) = ( \sin\theta \cos\phi , \sin\theta \sin\phi , \cos\theta ) d\psi$$ adding the three angular velocities together is thomas 's formula . again , it is never used in the equations that follow .
consider two observers $o$ and $o'$ moving with velocity $v$ with respect to each other . both of them will use a photon bouncing off a mirror to define any kind of duration , so it is easy to show that their duration will satisfy the relation $$\delta t ' = \gamma \delta t \ , , $$ and a similar argument with measuring any relative distance $\delta l$ between points in the direction of relative movement gives $$\delta l ' = \gamma \delta l \ , . $$ the stated relations will hold under any coordinate convention . now suppose observer $o$ decides to establish a coordinate convention by setting $t=t'=0$ when her coordinate origin $x=0$ met with the coordinate origin $x'=0$ of $o'$ . she then asserts that the relative distance between their origins grows as $-vt$ , so they have after transforming any relative distances $$x'= \gamma ( x - vt ) \ , . $$ by the same argument , she concludes that $o'$ must see the relative distances as changing in the opposite way with $+v t'$ and after using the same transform ( due to the relativity principle ) , $o$ concludes that $$x = \gamma ( x ' + v t'\ ! ) \ , . $$ but , as we know , the relative growth $\delta t$ is not the same as $\delta t'$ . by substituting for $x'$ and turning the last equation into an expression for $t'$ we have . $$t'= \gamma ( t - \frac{vx}{c^2} ) $$ i.e. , it is a question of consistence of the coordinate convention picked out by $o$ that we need this correction $vx/c^2$ . if we chose $o'$ to fix her origin on the one of $o$ , we would get $$x'=\gamma x , \ , t'=\gamma t \ , . $$ the term in the transformation is thus a consequence of the fact that we choose to mix a $vt$ factor into the coordinate convention which in turn requires a skewing of time through space . it is very important during the discussion of special relativity to remember that this is a conventional ( non-physical ) coordinate transform fix . edit : as requested , i will present a direct derivation which gives first a conventional meaning to simultaneity ( the $t=\rm const . $ ) before requesting the consistency of the $x$ coordinate and without any explicit statement of length contraction . we only have to presume we already know the standard derivation of the relation $$\delta t ' = \frac{\delta t}{\sqrt{1- v^2/c^2}}$$ consider the following simultaneity convention : consider an array of clocks in rest with respect to $o$ , this is a fixed observable feature . a light signal is sent from the origin of $o$ ( this is the conventional point , the referential signal point could be anywhere in the coordinate system , even moving with respect to it ) denoting the $t=0$ moment . all the clocks set their time to $t=0$ upon receiving the signal even though there is a certain delay $\delta t_c$ for every individual clock $c$ and immediately send a light signal with their identification back to $o$ . $o$ receives the signal from a clock $c$ at a time $t_c$ . she naturally concludes that $t_c = \delta l/c + \delta l/c$ and from that computes that when she sends a signal to $c$ , the signal will be there at a delay $\delta t_c =t_c/2$ . she then sends a signal to $c$ at her own time $t$ to set itself to $t + \delta t_c$ . once this process is finished for every clock $c$ , a notion of the time coordinate $t$ or simultaneity is established globally for all points seen by $o$ . but consider now that $o$ watches $o'$ while establishing a similar convention with respect to a similar set of clocks , i.e. , a set of clocks which is moving with a velocity $v$ in the $x$ direction with respect to $o$ but is in rest with respect to $o'$ . $o'$ sends out the signal at $t'=0$ , but the clocks in front of her ( in the direction of motion ) are " rushing away from the signal " with the speed of light constant , so the delay will be longer . on the contrary , the clocks behind $o'$ are " rushing towards the signal " , so the delay is shorter . for clocks strictly in front of and behind $o'$ , we have a delay $$\delta t_{b/f} = \frac{\delta l}{c \pm v}$$ once the clocks send their signal back , an opposite effect takes place for the clocks in front/ behind . the resulting time the second clocks send back their signals is thus identical for both cases ( as measured by the clock system established by $o$ ) : $$t_c = \frac{\delta l}{c + v} + \frac{\delta l}{c - v} = \frac{2 \delta l}{c}\frac{1}{1 - v^2/c^2}$$ $o$ now uses the knowledge that $o'$ actually measured this time delay with a dilation , so she knows $o'$ will be sending out a correction time $$\delta t'_c = \frac{ \delta l}{c}\frac{1}{\sqrt{1 - v^2/c^2}}$$ $o$ is perfectly clear on the fact that the duration from the point of view of $o'$ $t'$ since the time they met will be rescaled by the factor $1/\sqrt{1-v^2/c^2}$ . however , $o$ concludes that also the sent out time/correction should be different to conform to her view of simultaneity . $o$ sees the duration of flight of the signal to be $\delta l/ ( c \pm v ) $ from her own point of view , but also realizes the clocks co-moving with $o'$ run slower , so she has to multiply the correction by $\sqrt{1-v^2/c^2}$ to compensate for the fact . she then sees a total discrepancy between her simultaneity and the $o'$ simultaneity of the magnitude $$\delta t'_{b , f} - \delta t'_c = \frac{\delta l}{c}\left ( \frac{\sqrt{1- v^2/c^2}}{1 \pm v/c} - \frac{1}{\sqrt{1-v^2/c^2}} \right ) $$ which gives after a bit of algebra $$\delta t'_{b , f} - \delta t'_c = \mp \frac{v \delta l}{c^2} \frac{1}{\sqrt{1- v^2/c^2}}$$ using a coordinate $x$ which is 0 at $o$ , and the positive sign means " in front " and negative " behind " , we can conclude that $o$ will summarize the " deformation " of $o'$ global notion of time as $$t ' = \frac{t}{\sqrt{1-v^2/c^2}} - \frac{ v x}{c^2} \frac{1}{\sqrt{1- v^2/c^2}} = \gamma ( t - vx/c^2 ) $$ $\blacksquare$
structure and interpretation of classical mechanics ( table of contents ) certainly deserves mention . it might not have as much differential geometry as you had like , though they have a followup article titled functional differential geometry .
what martin beckett says is absolutely correct . however , the procedure for calculating the db reduction of a particular type of wall insulation and a particular door slamming would be something like this : make a " dry " recording of the door slamming and put it on your laptop . do a spectral analysis on this signal . there will probably be a few frequency ranges that are much louder than others . take the " transmission loss " diagram for a type of wall insulation and convert it into a frequency response . that is , for each frequency , calculate $10^{-\frac{\text{power loss in db}}{20}}$ . this gives the amplitude of each frequency as a fraction of the original amplitude . multiply the two frequency responses together . ( multiply each frequency independently . ) this is the spectrum of the sound you will hear when someone slams the door once the insulation is installed . comparing it to the " clean " door slam ( or , even better , to a recording of how it sounds with the existing wall ) will show you how the insulation modifies the sound . this is useful information , because the bass and lower mid-range frequencies of a door slam are much more annoying than the treble ones , imho . calculate the power of the new spectrum and compare it to the old one .
