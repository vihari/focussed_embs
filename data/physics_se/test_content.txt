gravity is mediated by a spin two particle . electromagnetism by spin 1 . here is a link that answers your question : even and odd spin do differ in that they require a product of charges with different signs to get attraction or repulsion : spin even : $q_1 q_2 &gt ; 0$: attractive $q_1 q_2 &lt ; 0$: repulsive spin odd : $q_1 q_2 &lt ; 0$: attractive $q_1 q_2 &gt ; 0$: repulsive in the case of gravity , mediated by spin 2 particles , charge is mass , which is always positive . thus , $q_1 q_2$ is always greater than zero , and gravity is always attractive . for spin 0 force mediators , however , there is no restriction on the charges and you can very well have repulsive forces . a better rephrasing of the question is : " why do particles of odd spin generate repulsive forces between like charges , while particles of even spin generate attractive forces between like charges ? " goes on to derive this
no polarity reversal has occurred - you are dealing with a magnet that has an axial field ( pointing out through the flat face . ) when you break it , each half has similar field , pointing in the same direction , which is unstable . one piece will want to flip so that the fields line up antiparallel ( lower energy situation ) .
as understood by einstein 's general theory of relativity completed in 1915-16 , gravity is indeed a manifestation of ( nothing else than ) the curvature of space and i have some doubts about your implicit claim that you have made this discovery " independently " of einstein . according to the precise equations of general relativity , the so-called einstein 's equations $$ g_{\mu\nu} = \frac{8\pi g}{c^2} t_{\mu\nu} , $$ what influences the curvature of spacetime is the stress-energy tensor that knows about the density of energy and momentum and the flux of energy and momentum . terms like " flux of momentum " may sound obscure but they are described by well-defined mathematical formulae . in particular , " flux of momentum " is nothing else than the component of pressure . so pressure also influences the curvature of spacetime – and therefore the gravitational field and the behavior of objects in this field – according to general relativity . on the other hand , it is irrelevant for the curvature and gravity whether the same stress energy tensor – the density of mass , energy , momentum , and components of pressure and stress – are achieved by the electromagnetic field , one material , or another material . however , it is still impossible to " create " curvature of space without any material ( or energetic ) carrier . the equations explicitly show that the ricci tensor is zero if there is no energy/momentum density in the space . so one can not create a " black hole out of nothing " . nevertheless , black holes may suck all the material and make the spacetime around ricci-flat ; the ricci ( or einstein ) tensor is equal to zero almost everywhere in the space . this ricci-flatness is still importantly violated at the black hole singularity which is the reason why the black holes still carry a nonzero mass/energy . the question is getting increasingly impenetrable as one continues to read it so what you exactly wanted to do with the frame-dragging effect remained unknown to me ( and i guess that not only me ) . frame-dragging is a particular new gravitational effect that occurs in the gravitational field induced by rotating bodies .
let me first answer this question in a particular class of electrostatics problems . in the case of a localized charge distribution in electrostatics ( one in which the charge density vanishes outside of some ball around the origin ) , the general solution for a potential that vanishes at $\infty$ is $$ \mathbf \phi ( \mathbf x ) = k\int_{\mathbb r^3} d^3x ' \frac{\rho ( \mathbf x' ) }{|\mathbf x - \mathbf x'|} $$ in this case , we often want to show that if $\rho$ has certain symmetries , then $\phi$ shares those symmetries . let 's first derive a little result : suppose that we make an invertible transformation $t$ on the charge distribution ( say a translation for a rotation for example ) , then the new ( transformed ) charge distribution $\rho_t$ will be $$ \rho_t ( \mathbf x ) = \rho ( t^{-1}\mathbf x ) . $$ what will the resulting potential $\phi$ be ? well , let 's compute $$ \phi_t ( \mathbf x ) = k\int_{\mathbb r^3} d^3 x ' \frac{\rho_t ( \mathbf x' ) }{|\mathbf x - \mathbf x'|} = k\int_{\mathbb r^3} d^3 x ' \frac{\rho ( t^{-1} ( \mathbf x' ) ) }{|\mathbf x - \mathbf x'|} $$ we can perform the resulting integral via a change of variables $$ \mathbf u = t^{-1} ( \mathbf x' ) \implies \mathbf x ' = t ( \mathbf u ) $$ and the formula for changing the variable of integration for volume integrals gives $$ d^3 x ' = j_t ( \mathbf u ) d^3 u $$ where $j_t$ is the jacobian of the transformation , so that the transformed potential becomes $$ \phi_t ( \mathbf x ) = k\int_{\mathbb r^3} d^3u \ , j_t ( \mathbf u ) \frac{\rho ( \mathbf u ) }{|\mathbf x - t ( \mathbf u ) |} $$ now back to the show . to see how this formula for the transformed potential is used to answer your question about symmetries , let 's consider a translationally symmetric charge density ; $$ \rho ( \mathbf x - \mathbf x_0 ) = \rho ( \mathbf x ) , \qquad \text{for all $\mathbf x_0$} $$ in this case , the transformation $t$ is $t ( \mathbf x ) = \mathbf x + \mathbf x_0$ . the jacobian is just 1 , and our the formula we derived above for the transformed potential gives $$ \phi_t ( \mathbf x ) = k\int_{\mathbb r^3} d^3u \frac{\rho ( \mathbf u ) }{|\mathbf x - ( \mathbf u-\mathbf x_0 ) |} = k\int_{\mathbb r^3} d^3u \frac{\rho ( \mathbf u ) }{| ( \mathbf x - \mathbf x_0 ) - \mathbf u|} = \phi ( \mathbf x - \mathbf x_0 ) $$ on the other hand , the translational invariance of the charge density tells us that $$ \phi_t ( \mathbf x ) = k\int_{\mathbb r^3} d^3 x ' \frac{\rho_t ( \mathbf x' ) }{|\mathbf x - \mathbf x'|} = k\int_{\mathbb r^3} d^3 x ' \frac{\rho ( \mathbf x' ) }{|\mathbf x - \mathbf x'|} = \phi ( \mathbf x ) $$ so combining these results gives $$ \phi ( \mathbf x - \mathbf x_0 ) = \phi ( \mathbf x ) $$ namely , the potential is also translationally symmetric . a similar procedure can be used for other symmetries . try rotational invariance for example on your own ! hope that helps ! physics rocks . addendum . i think you can show similar things for generic neumann or dirichlet boundary value problems in electrostatics in which , for example , you do not just want to solve poisson 's equation for a localized charge distribution with vanishing potential at infinity , but i have not worked out the details .
if there are $\omega$ states and $p_i$ is constant by the fundamental postulate of statistical mechanics , then you have $$1=\sum_{i=1}^\omega\ p_i=\omega\ p_i\ \ \longrightarrow\ \ p_i=\frac{1}{\omega} , $$ and consequently $$s = - \sum_{i=1}^\omega\ p_i \ln p_i=- \sum_{i=1}^\omega\ p_i \ln \frac{1}{\omega}=\left ( -\ln \frac{1}{\omega}\right ) \sum_{i=1}^\omega\ p_i=\ln\omega . $$
not exactly . fusion of atoms in supernova nucleosynthesis is thought to be responsible for the various atoms that make up the periodic table . while there has not been one in our part of the galaxy for quite some time , plenty of supernova are occurring through out the universe right now . so , while you are made of old stuff , in terms of atoms , most of it probably is not as old as the big bang itself . “the nitrogen in our dna , the calcium in our teeth , the iron in our blood , the carbon in our apple pies were made in the interiors of collapsing stars . we are made of star stuff . ” -carl sagan
suppose you have a constant angle $\theta$ slope in the original frame ( we suppose that the transitions from horizontal movements to the slope is quasi-instantaneous ) . call $x$ and $x'$ the horizontal displacements in the original and moving frame . call $t$ the total time for going to $z=h$ to $z=0$ . then you have $x'= x- v_0 t$ with $ x= h \cot \theta$ , the angle of the slope in the moving frame is given by : $$h \cot \theta ' = h \cot \theta - v_o t \tag{1}$$ the coordinates of the normal force ( in the original and moving frames ) are $ \vec n = mg ( - \sin \theta , \cos \theta ) $ . the unit displacement vector , in the moving frame , is $\vec n = ( - \cos \theta ' , - \sin \theta' ) $ the total supplementary work is : $$w_{supp} = \vec n . \vec n \quad \dfrac{h}{\sin \theta'} = mgh\ , ( \sin\theta \cot \theta ' - cos \theta ) \tag{2}$$ using $ ( 1 ) $ , we get : $$w_{supp} = -mg v_0 t \tag{3}$$ the work due to the gravity force is : $w = ( mg \sin \theta' ) \dfrac{h}{\sin \theta'} = mgh$ so , finally , in the moving frame , we have : $$ mgh - mgv_0t = \frac{1}{2} m ( \delta v ) ^2\tag{4}$$ however , $gt$ is nothing else than $\delta v$ . the work , in the lhs of the above equation , does not depend on the slope , so we may imagine a quasi-instantaneous $90$ degrees turn from horizontal to vertical , then a quasi-vertical slope , followed by a quasi-instantaneous $90$ degrees turn from vertical to horizontal . during the quasi-instantaneous turns , the modulus of the speed is conserved ( because energy is conserved and no work is done ) . so , considering a vertical movement , it is obvious that $\delta v= gt$ so finally you have , skipping the overall $m$ factor : $$ gh - v_0 \delta v = \frac{1}{2} ( \delta v ) ^2\tag{5}$$
actually , what your professor is not telling you - what we always gloss over in intro quantum mechanics for simplicity , but i might as well give it away now because everything will make so much more sense once you get this - is that kets are not wavefunctions at all . forget that you ever learned about wavefunctions for a minute . kets are a form of notation for representing quantum states . ( any quantum states . technically this answers your question , but you may want to read on . ) basically , whatever sort of thing it is that specifies the behavior of a quantum system , we call it a quantum state , and write it as the ket $\lvert\psi\rangle$ , where $\psi$ is some label that we can choose for our convenience . ( so $\lvert 1\rangle$ , $\lvert 2\rangle$ , $\lvert\alpha\rangle$ , etc . , they are all just arbitrary labels , and you do not know anything about them except that they are different , $\lvert 1\rangle\neq\lvert 2\rangle$ and so on . ) then we assume that these abstract objects , the quantum states , have all the properties needed to make them members of a vector space : they can be added , multiplied by scalars , you can take inner products of them , and so on . thus we can meaningfully write things like $\frac{1}{\sqrt{2}}\lvert\text{bob}\rangle - \frac{1}{\sqrt{2}}\lvert\text{sally}\rangle$ , or $\langle\text{steve}\lvert\text{cookies}\rangle$ . now , since these kets represent states of quantum systems , we should be able to relate the properties of those quantum systems to the kets . for example , suppose you have some system , maybe an electron in a box . this electron has some state , which we will denote $\lvert\psi\rangle$ . it is reasonable to ask , what is the probability of finding the electron in a certain tiny volume $\mathrm{d}^3 v$ at position $\mathbf{a}$ ? this probability , which i will denote $p ( \mathbf{a} ) \ , \mathrm{d}^3 v$ , should be computable from the quantum state , so there should be some procedure by which we can start with $\lvert\psi\rangle$ and get $p ( \mathbf{a} ) \ , \mathrm{d}^3 v$ . of course , there is such a procedure . it requires the use of a particular ket , which i will denote $\lvert\mathbf{a}\rangle$ , and through arguments which i will not go into here you can determine that the ket $\lvert\mathbf{a}\rangle$ corresponds to the quantum state of a particle which is known to be at the position $\mathbf{a}$ . with that definition out of the way , the procedure is $$p ( \mathbf{a} ) \ , \mathrm{d}^3 v = \langle\psi\lvert\mathbf{a}\rangle\langle\mathbf{a}\rvert\psi\rangle\mathrm{d}^3 v$$ this quantity $\langle\mathbf{a}\rvert\psi\rangle$ is just a number - even though we may not really know what $\lvert\psi\rangle$ or $\lvert\mathbf{a}\rangle$ are , we know that their inner product is a number , because that is what an inner product does , it produces a number out of two elements of a vector space . you can pick a different position in the box , $\mathbf{b}$ , and go through the same procedure , obtaining $$p ( \mathbf{b} ) \ , \mathrm{d}^3 v = \langle\psi\lvert\mathbf{b}\rangle\langle\mathbf{b}\rvert\psi\rangle\mathrm{d}^3 v$$ where $\langle\mathbf{b}\rvert\psi\rangle$ is , again , a number - probably a different one . then you can go on and do this for all the other positions in the box . for every position $\mathbf{r}$ , there is some associated " localized " ket $\lvert\mathbf{r}\rangle$ , and then you can take this ket and combine it with $\lvert\psi\rangle$ to get the number $\langle\mathbf{r}\rvert\psi\rangle$ . essentially , what you are doing here is constructing a mapping of positions to numbers . this mapping is the wavefunction . $$\psi ( \mathbf{r} ) = \langle\mathbf{r}\rvert\psi\rangle$$ of course , your question is really about the reverse process : given a wavefunction , is there always necessarily a ket ( a quantum state ) that corresponds to it ? there is , and you can see it by making an analogy to regular vectors . if you have some vector $\vec{v} = a\hat{x} + b\hat{y} + c\hat{z}$ , you can extract the $x$ component by computing $\hat{x}\cdot\vec{v}$ . that will give you $a$ . then you can get back to the original vector by multiplying each component by its corresponding unit vector and adding them all up : $$ ( \hat{x}\cdot\vec{v} ) \hat{x} + ( \hat{y}\cdot\vec{v} ) \hat{y} + ( \hat{z}\cdot\vec{v} ) \hat{z} = a\hat{x} + b\hat{y} + c\hat{z} = \vec{v}$$ but extracting a component is exactly what we were doing with $\lvert\mathbf{a}\rangle$ and $\lvert\mathbf{b}\rangle$ and so on . see , the vector space in which $\lvert\psi\rangle$ lives is an infinite-dimensional vector space , with one basis vector associated with each point in real space . so something like $\langle\mathbf{a}\lvert\psi\rangle$ is the component of $\lvert\psi\rangle$ corresponding to the unit vector $\lvert\mathbf{a}\rangle$ . accordingly , if you want to reconstruct $\lvert\psi\rangle$ from the components , you have to multiply each component by its unit vector , $$\langle\mathbf{r}\lvert\psi\rangle\lvert\mathbf{r}\rangle$$ and add them up - or since there are an infinite number in this case , you integrate : $$\int\lvert\mathbf{r}\rangle\langle\mathbf{r}\lvert\psi\rangle\mathrm{d}^3 v = \lvert\psi\rangle$$ in this way , given any wavefunction $\psi ( \mathbf{r} ) $ and the basis states $\lvert\mathbf{r}\rangle$ , you can turn it into a quantum state . ( assuming there is no additional information encoded in the quantum state . that is something to think about later , when you learn about spin . )
i think a diagram or slightly clearer rephrasing of the question is in order , but i think you want to do $180^{\circ} - \theta$ where $\theta$ is the angle you have found .
does space have an elastic quality ? no it does not . 1 you are taking the " rubber sheet " analogy too far . it is only meant to help you accept the fact that objects curve spacetime and that their motion is affected by this curvature . if you go any further into general relativity than that , the analogy breaks down . in particular , the rubber sheet analogy has nothing to say about the expansion of spacetime . what it means for space to be expanding is simply that the distance between any two objects ( which are not bound together by some force ) grows with time . you can find further explanation of this point in another answer i have written and some of the other answers to that question . 1 to be fair : perhaps somebody has proposed a theory about elasticity of space , but if so , the idea has not caught on .
the feynman lectures only need a little amending , but it is a relatively small amount compared to any other textbook . the great advantage of the feynman lectures is that everything is worked out from scratch feynman 's way , so that it is taught with the maximum insight , something that you can only do after you sit down and redo the old calculations from scratch . this makes them very interesting , because you learn from feynman how the discovering gets done , the type of reasoning , the physical intuition , and so on . the original presentation also makes it that feynman says all sorts of things in a slightly different way than other books . this it is good to test your understanding , because if you only know something in a half-assed way , feynman sounds wrong . i remember that when i first read it a million years ago , a large fraction of the things he said sounded completely wrong . this original presentation is a very important component , it teaches you what originality sound like , and knowing how to be original is the most important thing . i think vol i is pretty much ok as an intro , although it should be supplemented at least with this stuff : computational integration : feynman does something marvellous at the start of volume i ( something unheard of in 1964 ) , he describes how to euler time-step a differential equation forward in time . nowadays , it is a simple thing to numerically integrate any mechanical problem , and experience with numerical integration is essential for students . the integration removes the student 's paralysis : when you are staring at an equation and do not know what to do . if you have a computer , you know exactly what to do ! integrating reveals many interesting qualitative things , and show you just how soon the analytical knowledge paistakingly acquired over 4 centuries craps out . for example , even if you did not know it , you can see the kam stability appear spontaneously in self-gravitating clusters at a surprisingly large number of particles . you might expect chaotic motion until you reach 2 particles , which then orbit in an ellipse . but clusters with random masses and velocities of some hundreds of particles eject out particles like crazy , until they get to one or two dozen particles , and then they settle down into a mess of orbits , but this mess must be integrable , because nothing else is ejected out anymore ! you discover many things like this from piddling around with particle simulations , and this is something which is missing from volume i , since computers were not available . it is not completely missing , however , and it is much worse elsewhere . the kepler problem : feynman has an interesting point of view regarding this which is published in the " lost lecture " book and audio-book . but i think the standard methods are better here , because the 17th century things feynman redoes are too specific to this one problem . this can be supplemented in any book on analytical mechanics . thermodynamics : the section on thermodynamics does everything through statistical mechanics and intuition . this begins with the density of the atmosphere , which motivates the boltzmann distribution , which is then used to derive all sorts of things , culminating in the clausius clayperon equation . this is a great boon when thinking about atoms , but it does not teach you the classical thermodynamics , which is really simple starting from modern stat-mech . the position is that the boltzmann distribution is all you need to know , and that is a little backwards from my perspective . the maximum entropy arguments are better--- they motivate the boltzmann distribution . the heat-engine he uses is based on rubber-bands too , and yet there is no discussion of why rubber bands are entropic , nor of free-energies in the rubber band or the dependence of stiffness on temperature . monte-carlo simulation : this is essential , but it obviously requires computers . with monte-carlo you can make snapshots of classical statistical systems quickly on a computer and build up intuition . you can make simulations of liquids , and see how the atoms knock around classically . you can simulate rubber-band polymers , and see the stiffness depend on temperature . all these things are clearly there in feynman 's head , but without a computer , it is hard to transmit it into any of the student 's heads . for volume ii , the most serious problem is that the foundations are off . feynman said he wanted to redo the classical textbook point of view on e and m , but he was not sure how to do it . the feynman lectures were written at a time just before modern gauge theory took off , and while they emphasize the vector potential a lot compared to other treatments of the time , they do not make the vector potential the main object . feynman wanted to redo volume ii to make it completely vector potential , but he did not get to do it . somebody else did a vector-potential based discussion of e and m based on this recommendation , but the results were not so great . the major things i do not like in vol ii : the derivation of the index of refraction is done by a complicated rescattering calculation which is based on a plum-pudding style electron oscillators . this is essentially just the forward-phase index-of-refraction argument feynman gives to motivate unitarity in the 1963 ghost paper in acta physica polonika . it is not so interesting or useful in my opinion in vol ii , but it is the most involved calculation in the series . no special functionology : while the subject is covered with a layer of 19th-century mildew , it is useful to know some special functions , especially bessel functions and spherical harmonics . feynman always chooses ultra special forms which give elementary functions , and he knows all the cases which are elementary , so he gets a lot of mileage out of this , but it is not general enough . the fluid section is a little thin--- you will learn how the basic equations work , but no major results . the treatment of fluid flow could have been supplemented with he4 flows , where the potential flow description is correct ( it is clear that this is feynman 's motivation for the strange treatment of the subject ) , but this is not explicit . numerical methods in field simulation : here if one wants to write an introductory textbook , one needs to be completely original , because the numerical methods people use today are not so good for field equations of any sort . vol iii is extremely good , because it is so brief . the introduction to quantum mechanics there gets you to a good intuitive understanding quickly , and this is the goal . it probably could use the following : a discussion of diffusion , and the relation between schrodinger operators and diffusion operators : this is obvious from the path integral , but it was also clear to schrodinger . it also allows you to quickly motivate the exact solutions to schrodinger 's equation , like the 1/r potential , something which feynman just gives you without motivation . a proper motivation can be given by using susy qm ( without calling it that , just a continued stochastic equation ) and trying out different ground state ansatzes . galilean invariance of the schrodinger equation : this part is not done in any book , i think only because dirac omitted it from his . it is essential to know how to boost wavefunctions . since feynman derives the schrodinger equation from a tight-binding model ( a lattice approximation ) , the galilean invariance is not obvious at all . since the lectures are introductory , everything in there just becomes second nature , so it does not matter that they are old . the old books should just be easier , because the old stuff is already floating in the air . if you find something in the feynman lectures which is not completely obvious , you should study it until it is obvious--- there is no barrier , the things are self-contained .
the wok is a design that gets the maximum efficient use of energy for cooking and needs a minimum of heat source . a flat bottom equivalent pan transfers the heat to the food from the bottom plane , and the walls are just containers radiating most heat away , since the food is at the bottom . a wok focuses the heat from the walls to the food being cooked ( think of a parabola and its focal point ) . the walls partake in the cooking process effectively giving more area contact to the food than the flat bottom one .
this may be the relevant paper that would answer your question : http://journals.aps.org/prb/abstract/10.1103/physrevb.57.8472
strictly speaking vacuum is the state of lowest energy . that means no matter or radiation ( photons or any other particles ) . note that space is not a perfect vacuum . also note that , technically , a gas of planets and comets etc . has a pressure ( there is usually little reason to care about it though ) . there is also radiation pressure due to the photons . people often use the term vacuum loosely to refer to anything less than atmospheric pressure . this is the sense people use when they say space is a vacuum . edit ( re the comments ) : yes , there is a minimum energy . imagine that you start with vacuum . there is nothing there by definition . now create some particle . this necessarily takes some energy ( at least $mc^2$ where $m$ is the mass of the particle ) , so the state with a particle in it has more energy . now the value of the vacuum energy is a subtle thing . without gravity only energy differences matter , so you can always set the vacuum energy to zero . but with gravity it is tricky , because all energy gravitates . indeed , physicists now believe that empty space has an energy density , now known as dark energy . now people will tell you a big song and dance about quantum fluctuations and zero point energy , but this is only one side of the coin , and only comes in when you try to actually calculate the vacuum energy from a more basic theory ( quantum field theory ) . the basic picture is really simple though : vacuum energy is just a number - some physical constant that we could go out and measure . now if you check very carefully all the laws we know then you will find that gravity is the only place the vacuum energy comes in , so for most purposes you can forget about vacuum energy . ( people also mention the casimir effect around this point , but that is another thing entirely . ) on the other question : whether true vacuum is achievable theoretically . well , it depends what you mean " theoretically . " if you mean " in the mind of a theoretical physicist " then sure , it is possible . ; ) but if you mean there is some way to build a box and make a perfect vacuum inside of it then no , you can not , because the box will always have some finite temperature and hence blackbody radiation will fill the cavity . you can make it arbitrarily close to true vacuum by cooling the box , but you could never actually reach it .
the normal force does decrease with angle . this does not mean that the coefficient of friction changes : we can , depending on the angle $\theta$ of the slope , split the gravitational force $f_g = mg$ acting upon a thing with mass $m$ resting on the slope into the normal force $f_n = mg \cos ( \theta ) $ and the force pointing down the slope , $f_s = mg\sin ( \theta ) $ . now , the coefficient of friction is a property of materials , and does not change with the angle - but it is the case that the friction force will decrease since it is $f_k = \mu_kf_n$ . the " greater propensity " of things to slide down steeper inclined slopes is due to the friction force decreasing , and due to the force pointing down the slope increasing with increasing angle .
i will mostly talk about the classical physics as this is complicated enough already ( i might mention something about quantum stuff at the end ) . so , let us first get all the relevant terms straight , so that we avoid any further confusion . in particular , we need to be precise about what we mean by invariance because already two different notions have been thrown into one bag . a symmetry group of a physical system is a group of transformations that leave the system invariant . e.g. the electric field of a point charge is invariant under rotations w.r.t. to that point . in other words , we want the group to act trivially . but that means that this immediatelly rules out any equations that carries tensorial indices ( i.e. . transforms in a non-trivial representation of the rotation group ) . for these equations , if you perform a rotation , the equation will change . of course , it will change in an easily describable manner and a different observer will agree . but the difference is crucial . e.g. in classsical quantum mechanics we require the equations to be scalar always ( which is reflected in the fact that hamiltonian transforms trivially under the symmetry group ) . carrying a group action is a broader term that includes tensorial equations we have left out in the previous bullet point . we only require that equations or states are being acted upon by a group . note that the group action need not have any relation to the symmetry . e.g. take the point charge and translate it . this will certainly produce a different system ( at least if there is some background so that we can actually distinguish points ) . a gauge group of a system is a set of transformations that leave the states invariant . what this means is that the actual states of the system are equivalence classes of orbits of the gauge group . explicitly , consider the equation ${{\rm d} \over {\rm d} x} f ( x ) = 0$ . this has a solution $g ( x ) \equiv c$ for any $c$ . but if we posit that the gauge group of the equation consists of the transformations $f ( x ) \mapsto f ( x ) + a$ then we identify all the constant solutions and are left with a single equivalence class of them -- this will be the physical state . this is what gauge groups do in general : they allow us to treat equivalence classes in terms of their constituents . obviously , gauge groups are completely redundant . the reason people work with gauge groups in the first place is that the description of the system may simplify after introduction of these additional parameters that " see " into the equivalence class . of course historical process went backwards : since gauge-theoretical formulation is simpler , this is whan people discovered first and they only noticed the presence of the gauge groups afterwards . now , having said this , let 's look at the electromagnetism ( first in the flat space ) . what symmetries are the maxwell equations invariant under ? one would like to say under lorentz group but this actually not the case . let 's look at this more closely . as alluded to previously the equation $$\partial_{\mu} f^{\mu \nu} = j^{\nu}$$ can not really be invariant since it carries a vector index . it transforms in the four-vector representation of the lorentz group , yes -- but it is certainly not invariant . contrast this with the minkowski space-time itself which is left invariant by the lorentz group . we also have ${\rm d} f = 0$ and therefore ( in a contractible space-time ) also $f = {\rm d} a$ which is obviously invariant w.r.t. to $a \mapsto a + {\rm d} \chi$ . in terms of the above discussion the equivalence class $a + {\rm d} \omega^0 ( {\mathbb r}^{1,3} ) $ is the physical state and the gauge transformation lets us distinguish between its constituents let 's move to a curved spacetime now . then we have $$\nabla_{\mu} f^{\mu \nu} = j^{\nu}$$ again , this is not invariant under ${\rm diff} ( m ) $ . but it transforms under an action of ${\rm diff} ( m ) $ . the only thing in sight that is invariant under ${\rm diff} ( m ) $ is $m$ itself ( by definition ) . in the very same way , gr is not invariant under ${\rm diff} ( m ) $ but only transforms under a certain action of it ( different than em though , since gr equations carry two indices ) . also , ${\rm diff} ( m ) $ can not possibly be a gauge group of any of these systems since it would imply that almost all possible field configurations get cramped to a single equivalence class possibly indexed by some topological invariant which can not be changed by a diffeomorphism . in other words , theory with ${\rm diff} ( m ) $ as a gauge group would need to be purely topological with no local degrees of freedom .
the easiest way to get the exact behavior is from thinking about light as a classical wave interacting with the atoms in the solid material . as long as you are far away from any of the resonant frequencies of the relevant atoms , this picture is not too bad . you can think each of the atoms as being like a little dipole , consisting of some positive and some negative charge that is driven back and forth by the off-resonant light field . being an assemblage of charges that are accelerating due to the driving field , these dipoles will radiate , producing waves at the same frequency as the driving field , but slightly out of phase with it ( because a dipole being driven at a frequency other than its resonance frequency will be slightly out of phase with the driving field ) . the total light field in the material will be the sum of the driving light field and the field produced by the oscillating dipoles . if you go through a little bit of math , you find that this gives you a beam in the same direction as the original beam-- the waves going out to the sides will mostly interfere destructively with each other-- with the same frequency but with a slight delay compared to the driving field . this delay registers as a slowing of the speed of the wave passing through the medium . the exact amount of the delay depends on the particulars of the material , such as the exact resonant frequencies of the atoms in question . as long as you are not too close to one of the resonant frequencies , this gives you a really good approximation of the effect ( and " too close " here is a pretty narrow range ) . it works well enough that most people who deal with this stuff stay with this kind of picture , rather than talking in terms of photons . the basic idea of treating the atoms like little dipoles is a variant of " huygens 's principle , " by the way , which is a general technique for thinking about how waves behave .
what you calculated there is the electric flux generated by a proton , measured at a distance that is greater the size of the proton : you have used e as the charge , which means that your surface d s encloses the whole of the proton ( at least within classical electrodynamics , where we can assume that the proton has some kind of spherical/definite shape ) . the number of field lines is not something that has much of a physical meaning . basically because there could be an infinite number of field lines through a given patch of surface d s . you would need to specify the distance between these field lines , so that you can divide a length ( say 10 m ) through the separation between each field line ( say 1 m ) therefore resulting in 10 field lines . this , however , assumed that the field lines are equally spaced . check this
one way to formulate the equations of motion of a charged particle as a geodesic equation is through the kaluza–klein theory . in it we add additional dimension ( just one , if we are only interested in the electromagnetism ) and write the 5d metric $$ ds^2 = ds^2 + \epsilon \phi^{2} ( dx^{4} + a_{\mu}dx^{\mu} ) ^2 , $$ where $ds^{2} = g_{\mu \nu} dx^{\mu} dx^{\nu}$ is the 4d ( curved ) metric , $\epsilon=+1$ or $-1$ is a sign choice for either space-like or time-like dimension , $a_\mu$ is identified with the 4-potential of electromagnetic field and $\phi$ is an additional scalar field . the geodesic equation written in this 5d metric is : \begin{multline} \frac{d^2 x^{\mu}}{d{\cal s}^2}+ {\gamma}^{\mu}_{\alpha \beta}\frac{dx^{\alpha}}{d{\cal s}}\frac{dx^{\beta}}{d{\cal s}}= n f^{\mu}_{\ ; \ ; \nu}\frac{dx^{\nu}}{d{\cal s}}+ \epsilon n^2 \frac{\phi^{ ; \mu}}{\phi^{3}} - a^{\mu}\frac{dn}{d{\cal s}}-\\- g^{\mu\lambda}\frac{dx^4}{d{\cal s}}\left ( n \frac{\partial{a_{\lambda}}}{\partial{x^4}}+\frac{\partial{g_{\lambda\nu}}}{\partial{x^4}}\frac{dx^{\nu}}{d{\cal s}}\right ) , \end{multline} and the same rewritten so that particle motion is parametrized through 4d proper interval $s$ , rather than 5d $s$: \begin{multline} \frac{d^2 x^{\mu}}{ds^2}+{\gamma}^{\mu}_{\alpha \beta}\frac{dx^{\alpha}}{ds}\frac{dx^{\beta}}{ds}=\\= \frac{n}{ ( 1-\epsilon{n^2}/{\phi^2} ) ^{1/2}}\left [ f^{\mu}_{\ ; \ ; \nu}\frac{dx^{\nu}}{ds} - \frac{a^{\mu}}{n}\frac{dn}{ds}- g^{\mu\lambda}\frac{\partial{a_{\lambda}}}{\partial{x^4}}\frac{dx^4}{ds} \right ] + \\ + \frac{\epsilon n^2}{ ( 1-\epsilon n^2/\phi^2 ) \phi^3}\left [ \phi^{ ; \mu} + \left ( \frac{\phi}{n}\frac{dn}{ds}- \frac{d\phi}{ds}\right ) \frac{dx^{\mu}}{ds}\right ] -\\-g^{\mu\lambda}\frac{\partial{g_{\lambda\nu}}}{\partial{x^4}}\frac{dx^{\nu}}{ds}\frac{dx^4}{ds} . \end{multline} here the $f_{\mu\nu}$ tensor is the usual em strength 4-tensor : $$ f_{\mu\nu} = a_{\nu , \mu}-a_{\mu , \nu} , $$ and $n$ is the ( covariant ) 4-speed component along the additional dimension : $$ n =u_4 = \epsilon {\phi}^2\left ( \frac{dx^4}{d{\cal s}} + a_{\mu}\frac{dx^{\mu}}{d{\cal s}}\right ) . $$ these equations are taken from the paper : ponce de leon , j . ( 2002 ) . equations of motion in kaluza-klein gravity reexamined . gravitation and cosmology , 8 , 272-284 . arxiv:gr-qc/0104008 . which in turn refers to the book : wesson , p . s . ( 2007 ) . space-time-matter : modern higher-dimensional cosmology ( vol . 3 ) . world scientific google books . we see in these equations many new terms absent in the equations of motion of a charge in a 4d curved spacetime . to eliminate these terms we impose constraints on the 5d metric by requiring independece of all metric component of the $x^4$ coordinate , and assuming the scalar $\phi$ is simply constant . then the quantity $n$ is an integral of motion and the geodesic equation now looks like this : $$ \frac{d^2 x^{\mu}}{ds^2}+{\gamma}^{\mu}_{\alpha \beta}\frac{dx^{\alpha}}{ds}\frac{dx^{\beta}}{ds}= \frac{n}{\left ( 1-\epsilon{n^2}/{\phi^2}\right ) ^{1/2}}\left [ f^{\mu}_{\ ; \ ; \nu}\frac{dx^{\nu}}{ds} \right ] , $$ which is exactly the equation of motion for the charge in curved space-time in the presence of em field . with the ( now ) constant factor $n ( 1-\epsilon{n^2}/{\phi^2} ) ^{-1/2}$ having the role of a charge to mass ratio $e/m$ . i have left out numerous questions arising from this simple treatment , for them you should look into the relevant books and papers , but for the purpose of casting equations of motion of a test charge as a geodesic equations the answers to them are not needed .
let $t_0$ and $p_0$ denote the temperature and pressure of the environment and let us put some system in this environment . then , using the gibbs equation , $\mathrm{d} u= t \mathrm{d} s - p \mathrm{d}v$ , it is easy to verify that : $$ t_0 \mathrm{d} s_0 = - \left ( \mathrm{d} u + p_0 \mathrm{d} v \right ) $$ furthermore , let $\mathrm{d} s$ denote the change in entropy of the system , such that the total change of entropy is : $$ \mathrm{d} s_{\text{total}} = \mathrm{d} s_0 + \mathrm{d} s \geq 0 $$ hence : $$ t_0 \mathrm{d} s_{\text{total}} = - ( \mathrm{d} u + p_0 \mathrm{d} v - t_0 \mathrm{d} s ) \geq 0 $$ or equivalently : $$ \mathrm{d} u + p_0 \mathrm{d} v - t_0 \mathrm{d} s \leq 0 \tag{1} $$ this inequality play an important role when systems are moving to equilibrium . in order to see this let us consider a system with constant entropy and volume , i.e. $\mathrm{d}s= \mathrm{d} v =0$ . then equation $ ( 1 ) $ becomes : $$ \mathrm{d} u \leq 0 $$ which implies that $u$ decreases and reaches a minimum value at equilibrium . in exactly the same way , you should be able to understand the other examples you have quoted . furthermore , if there are two fixed parameter , is it not true that the state of system is fixed ? ( assuming there is a state formula $f ( t , p , v ) =0 ) $ . the equation of state $f ( t , p , v ) =0 ) $ tells us that if we know , say $t$ and $p$ , then we also know $v$ . in other words , one of the three variables $t$ , $p$ or $v$ ( whichever one you choose ) can be expressed in terms of the other two . thus , the state of the system is completely determined by two out of the three quantities $t$ , $p$ and $v$ . i really do not understand what this has to do with the example you are referring to and why this is confusing you . if we do not change the volume and entropy , then we still have not exhausted the state variables .
there are a few standard textbooks on neutron star . for interior structure and nuclear physics side two books by glendenning are good . http://www-nsdth.lbl.gov/~nkg/description.html for more general relativity side shapiro and teukolsky has been a standard texk book for many years . http://www.amazon.com/black-holes-white-dwarfs-neutron/dp/0471873160 finally , if you seek for real rigor , a new book by friedman and stergioulas is must . http://www.amazon.com/rotating-relativistic-stergioulas-cambridge-monographs/dp/0521872545 there are several review papers including two in living review . http://relativity.livingreviews.org/articles/lrr-2003-3/ http://relativity.livingreviews.org/articles/lrr-2007-1/ several by lattimer and prakash are also good starting point . for example , http://arxiv.org/abs/astro-ph/0612440
how about diagnostic methods in modern medicine ? nuclear magnetic resonance ( nmr ) - it would not even make sense to talk about it without quantum mechanics , because it depends on the quantum mechanical concept of spin positron emission tomography - hey , the name says it all , not only do you apply quantum mechanics , but you have a direct application of antimatter x-ray scanning , scintigraphy and many , many more . . . nuclear medicine is full of direct applications of nuclear , particle and quantum physics . . . it is even common to find particle accelerators in oncology departments for cancer therapy ! and what is a better application to mention to a common layman than " curing cancer " ? i am sure you will find lots of examples from medicine on the internet : )
[ . . . ] $\delta^+ \rightarrow p + \pi^0$ , [ . . . ] $\delta^+ \rightarrow n + \pi^+$ , which process is favored : the proton and neutral pion or neutron and charged pion [ ? ] since the kinematics ( and corresponding " phase space " factors ) for the two final states are presumably as good as equal , the evaluation of the branching ratio $$\text{br} := \frac{\gamma [ \delta^+\rightarrow p+\pi^0 ] }{\gamma [ \delta^+\rightarrow n+\pi^+ ] }$$ simplifies to determining the ratio of " state constituent " transition probabilities $$\text{br} := \frac{\gamma [ \delta^+\rightarrow p+\pi^0 ] }{\gamma [ \delta^+\rightarrow n+\pi^+ ] } \simeq \frac{\left\lvert \langle p ; \pi^0 \mid \delta^+ \rangle \right\rvert^2}{\left\lvert \langle n ; \pi^+ \mid \delta^+ \rangle \right\rvert^2} . $$ analyzing ( or defining ) the initial state $\delta^+$ and the two distinct final states in terms of isospin leads to the expressions $$ \lvert \delta^+ \rangle \equiv \big\lvert \left ( 3/2 , 1/2\right ) _i \big\rangle , $$ where the first value represents the magnitude of $\mathbf i$ , and the second value represents the magnitude of $i_3$ , along with $$ \lvert p ; \pi^0 \rangle \equiv \big\lvert ( 1/2 , 1/2 ) _f ; ( 1 , 0 ) _f \big\rangle \equiv \sqrt{ \frac{2}{3} }~\big\lvert ( 3/2 , 1/2 ) _t \big\rangle - \sqrt{ \frac{1}{3} }~\big\lvert ( 1/2 , 1/2 ) _t \big\rangle , $$ and $$ \lvert n ; \pi^+ \rangle \equiv \big\lvert ( 1/2 , -1/2 ) _f ; ( 1 , 1 ) _f \big\rangle \equiv \sqrt{ \frac{1}{3} }~\big\lvert ( 3/2 , 1/2 ) _t \big\rangle + \sqrt{ \frac{2}{3} }~\big\lvert ( 1/2 , 1/2 ) _t \big\rangle , $$ where the coefficients of the linear combinations on the right-hand sides are clebsch-gordan coefficients ( specificly those values listed in table "$1/2 \otimes 1$" ) , all states are normalized , and the indices $f$ and $t$ are to distinguish final states and " state representations to evaluate transition probabilities" ; such that in particular the states $ ( 1/2 , 1/2 ) _f$ and $ ( 1/2 , 1/2 ) _t$ are ( meant to be ) distinct ; and both are distinct , and indeed disjoint , from the initial state $\lvert \delta^+ \rangle \equiv \lvert ( 3/2 , 1/2 ) _i \rangle$ . now identifying $$\big\lvert ( 3/2 , 1/2 ) _t \big\rangle \equiv \big\lvert ( 3/2 , 1/2 ) _i \big\rangle $$ we can evaluate \begin{align} \langle p ; \pi^0 \mid \delta^+ \rangle and \equiv \bigg\langle \sqrt{ \frac{2}{3} }~ ( 3/2 , 1/2 ) _t - \sqrt{ \frac{1}{3} }~ ( 1/2 , 1/2 ) _t \bigg\vert ( 3/2 , 1/2 ) _t \bigg\rangle \\ and = \bigg\langle \sqrt{ \frac{2}{3} }~ ( 3/2 , 1/2 ) _t \bigg\vert ( 3/2 , 1/2 ) _t \bigg\rangle \\ and = \sqrt{ \frac{2}{3} } \end{align} and \begin{align} \langle n ; \pi^+ \mid \delta^+ \rangle and \equiv \bigg\langle \sqrt{ \frac{1}{3} }~ ( 3/2 , 1/2 ) _t + \sqrt{ \frac{2}{3} }~ ( 1/2 , 1/2 ) _t \bigg\vert ( 3/2 , 1/2 ) _t \bigg\rangle \\ and = \bigg\langle \sqrt{ \frac{1}{3} }~ ( 3/2 , 1/2 ) _t \bigg\vert ( 3/2 , 1/2 ) _t \bigg\rangle \\ and = \sqrt{ \frac{1}{3} } \end{align} obtaining the sought branching ratio value as $$\text{br} := \frac{\gamma [ \delta^+\rightarrow p+\pi^0 ] }{\gamma [ \delta^+\rightarrow n+\pi^+ ] } \simeq \frac{ ( \sqrt{ 2/3 } ) ^2 }{ ( \sqrt{ 1/3 } ) ^2} = 2 . $$
actually , that first statement is not correct . the universe is not expanding due to dark energy . it is accelerating due to dark energy . the normal expansion , called metric expansion , is an effect of general relativity . when you get a homogeneous distribution of matter or radiation ( a perfect fluid , a uniform gas , radiation , a homogeneous distribution of galaxies in the case of the universe today ) , you can solve the einstein field equations for general relativity for an expanding universe , called the frw metric . in this metric , the distance in between bound objects ( i.e. . galaxies today ) increases over time . this does not require dark energy , just a universe filled with matter or radiation . a simple article about some misconceptions about the expanding universe is here , i recommend reading it : http://www.mso.anu.edu.au/~charley/papers/lineweaverdavissciam.pdf so , prior to the discovery of dark energy , the expanding universe was understood perfectly well . however , we assumed that this expansion was slowing down . however , a discovery in the later nineties that was awarded the 2011 nobel prize in physics showed that not only is the universe expanding , it is accelerating . so , this is the role of dark energy . many different candidates for dark energy have been proposed , but one is heavily favored , the cosmological constant . in einstein 's equations for general relativity , you can throw in an extra term , $\lambda$ , the would play the role of a negative pressure vacuum energy . this has the effect of accelerating the expansion of the universe . why are we confident dark energy is just a cosmological constant ? one of the defining features of a cosmological constant is its equation of state . the equation of state , $w$ , is given by $p \over \rho$ , where $p$ is the pressure it contributes , and $\rho$ is the energy density . a cosmological constant has $w=-1$ . the wmap seven year report recorded the value as $w=-1.1 ± 0.14$ . within the error margins , the cosmological constant fits very well . quantum field theory also predicts the existence of a vacuum energy , so it was hoped that this would match the value of the cosmological constant . however , the value calculated by qft was enormously higher . using the upper limit of the cosmological constant , the vacuum energy in a cubic meter of free space has been estimated to be 10^-9 joules . however , the qft prediction is a whopping 10^113 joules per cubic meter . this is the ' vacuum catastrophe ' . for a simple page from the usenet faq about the cosmological constant , see here : http://www.astro.ucla.edu/~wright/cosmo_constant.html for a very thorough description , see here : http://philsci-archive.pitt.edu/398/1/cosconstant.pdf so , because the cosmological constant works so well as a description of dark energy , and is supported by the evidence , we prefer that over a description such as quintessence , or something similar to the explanation you proposed . addition - regarding the quantum fluctuations : the very early universe was filled with an obscenely hot and dense plasma and a bath of radiation . the metric expansion of space cooled and redshifted the radiation , and broke up the plasma into a much less dense gas of hydrogen . this is the essential nature of the big bang model , which you should note has nothing to do with a ' bang ' . the model was been confirmed by observations , which you can read about here . however , there are a few problems - first is the flatness problem . we observe that the universe is very , very close to being spatially flat . since expansion would cause the universe to deviate away from flatness , it must have been even flatter at the time of the big bang . ridiculously flat . how did it get this way ? second is the horizon problem . we observe that the universe is homogeneous on large scales , that is , it is pretty much the same everywhere . this means that primordial plasma must also have been perfectly homogeneous , which is confirmed by observations of the cosmic microwave background . however , if the expansion of the universe was extremely rapid from time zero onward , how did this plasma come to equilibrium ? it certainly would not have the time to do this . and third is the monopole problem . grand unified theories , or guts , are theories that unify the electroweak interaction with the strong nuclear force . they have the unfortunate feature of predicting that hot temperatures of the early universe should have produced an abundance of heavy magnetic monopoles , which we certainly do not observe . fourth is the homogeneity problem - why are there no inhomogeneities besides galaxies ? what made the early plasma so ' smooth ' ? a model called inflation fixes all of these problems . inflation proposes that the very early universe underwent an enormous expansion , growing the universe by at least 60 $e$-folds . this expansion would be driven by the inflaton field . this field would reach an undesirable energy value , called a false vacuum . when it is in this false vacuum , it has the property that it exerts an enormous negative pressure ( somewhat similar to dark energy ) . this drives inflation . after a very short period of time , the inflaton field reaches it is true vacuum ( through normal quantum effects such as tunneling ) . when this happens , it decays into a bath of radiation , heating the universe so that the big bang model can go from there . so , how does this solve the problems of the big bang model ? well , the enormous expansion would eliminate any curvature , making the universe extremely flat . this solves the flatness problem . second , it would allow the universe to expand very slowly before inflation , allowing it to come to equilibrium . this solves the horizon problem . any monopoles produced in the early universe would have been spread out so that we would only see about one in the entire observable universe , so the monopole problem is solved . and finally , inflation would ' iron out ' any large scale inhomogeneities with the rapid expansion . so , this is where those quantum fluctuations come in - prior to inflation some regions of the primordial plasma would have become very slightly denser due to quantum fluctuations - when the universe inflates , the random changes in density that come from quantum mechanics will get magnified , and you end up with what is called a " scale free power spectrum . " it is like drawing a small line on a flat balloon . blow the balloon up , and the line will become very large . similarly , small density perturbations become primordial ' seeds ' . since these are due to random fluctuations , we would expect this to produce a universe that has an even distribution of galaxies , such as ours . from there , dark matter clumps around these seeds , which then draws in the rest of the matter to form proto-galaxies . from there , full galaxies develop .
suppose you have some linear algebra background . the most important thing you need to know is that the inner product has the same meaning of what you have learnt in linear algebra class . the inner product $$\left\langle \phi|\psi\right\rangle =\int\phi^{*} ( x ) \psi ( x ) dx$$ has the meaning related to a projection of one vector onto another vector ( for true projection , the wavefunctions needed to be normalized ) . it is similar to the projection of a three dimensional vector $\mathbf{\vec{v}}=a\hat{\mathbf{x}}+b\hat{\mathbf{y}}+c\hat{\mathbf{z}}$ onto another unit vector $\mathbf{\hat{x}}$ which gives you the results $\mathbf{\vec{v}}\cdot\mathbf{\hat{x}}=a$ . first , the inner product can give you the " length square " of the wavefunction : $$\left\langle \psi|\psi\right\rangle =\int\psi^{*} ( x ) \psi ( x ) dx =\int|\psi ( x ) |^2dx$$ similar to the $\mathbf{\vec{v}}\cdot\mathbf{\vec{v}}=a^{2}+b^{2}+c^{2}$ , so you can normalize your wavefunction by the condition $\left\langle \psi|\psi\right\rangle =1$ . second , it allows you to show that two wavefunctions are orthogonal to each other , given by the condition that the inner product evaluated to zero $\left\langle \phi|\psi\right\rangle =0$ which is the analog to the $\mathbf{\hat{x}}\cdot\mathbf{\hat{y}}=0$ . third , if we write the wavefunction $\psi ( x ) $ as a linear combination of orthonormal wavefunctions $\psi_{n} ( x ) $: $$\psi ( x ) =\sum_{n}c_{n}\psi_{n} ( x ) $$ similar to a general vector in linear algebra , then we will have the inner product $\left\langle \psi_{n}|\psi\right\rangle =c_{n}$ . the meaning of $c_{n}$ is the probability amplitude and it is a complex number in general . so the probability $p_{n}$ of the wavefunction $\psi$ having the component $\psi_{n}$ is given by $p_{n}=|c_{n}|^{2}=|\left\langle \psi_{n}|\psi\right\rangle |^{2}$ . the meaning here is very important when you learn how to preform measurement . lastly , you should add two wavefunctions amplitude together before you take the square , similar to adding the amplitude of two water waves . more precisely , if the new wavefunction is $\psi ( x ) = a [ \psi_{a} ( x ) +\psi_{b} ( x ) ] $ , then the probability density at position $x$ is $a^2|\psi_{a} ( x ) +\psi_{b} ( x ) |^{2}$ . note that $a$ is the normalization constant given by the condition $\left\langle \psi|\psi\right\rangle=1$ . it is where the quantum effect arise . dont take the square and then add them together .
there are several qualitative and quantitative differences between gravity and magnetism . when you attract ' neutral ' bits of metal with a magnet , or attach it to something like a plate of metal , what is happening is that individual atoms of the metal react to the magnetic force . in a ferromagnetic metal , one with a similar electronic structure to iron or nickel , the individual atoms work like nanoscopic magnets ; but they are very weak , and they are not lined up with one another , so that their fields cancel one another out over any macroscopic distance . but if you bring a " large " magnet ( such as a fridge magnet ) up to them , the field of the large magnet causes them to align with the field , so that they are pulled towards the magnet &mdash ; and the magnet is pulled towards them . this is why some metal objects are attracted to magnets . other metals , such as aluminum or silver , also react to magnets , but much more weakly ( and in some cases repulsively ) : the way that they react to magnetic fields is described as paramagnetism ( for materials which align very weakly with magnetic fields ) and diamagnetism ( for materials which align very weakly against magnetic fields ) . the very fact that different materials react differently to magnetic fields is something that sets magnetism apart from gravity . gravitation works equally with masses of any sort , and is always attractive ( as noted by nic ) ; magnetism can both attract and repel , and do so with different degrees of force , as between ferromagnetic , paramagnetic , and diamagnetic materials . but of course , quite famously , even a single object can be both attracted and repelled by magnetic forces : the north poles of two magnets repel each other , as do the south poles ; only opposite poles attract each other . ( this , of course , is the basis on which compasses work . ) the way that these forces operate over distance also varies . gravity very famously ( but only approximately ) obeys an inverse-square law ; the field far from a bar magnet , however , decreases like the inverse of the cube of the distance from the magnet . finally , moving electric charges produce magnetic forces ; whereas they do not cause any gravitational forces which could not be accounted for just by the fact that the charged particles have mass ( whether moving or at rest ) . so , on both the macroscopic level and on the level of individual atoms , the forces of gravity and magnetism act quite differently .
the classical equations of motion are not affected by changing the lagrangian $$l \qquad \longrightarrow \qquad l&#39 ; = l+ \frac{df}{dt}$$ by a total time derivative . put $f= -q_1 q_2$ . then $$l&#39 ; = -\frac{1}{2} ( q_1^2 + q_2^2 ) . $$ this lagrangian $l&#39 ; $ does not contain time derivatives , and thus there are no dynamics . the classical equations of motion are $$ q_1=0 \qquad \mathrm{and}\qquad q_2=0 , $$ in conflict with what is said in the original question formulation ( v1 ) .
gerard ' t hooft 's " quantum field theory for elementary particles . is quantum field theory a theory ? " ( phys . rept . 104 nos . 2-4 ( 1984 ) , 129-142 , author 's eprint ) is a beautifully written review . from the abstract , what i would like to point out is that renormalizability is just one step in an evolutionary process of quantum field theory . in order to illuminate this point of view i will present a survey of the evolution of quantum field theory into its present form . however we will not follow the historical development , but rather , for my convenience , the lines of logic . as is well known , that is quite something different . ' t hooft also has a longer introduction to the subject : the conceptual basis of quantum field theory . gerard ' t hooft . in philosophy of physics ( j . butterfield and j . earman , eds . , elsevier/north-holland : amsterdam , 2007 ) . author 's eprint . this reads more like a textbook geared at readers with fairly solid quantum mechanics and a good understanding of special relativity , and covers a rather wide range of topics , so it is a little more advanced .
indeed , nothing can get under the horizon . the stuff close to the event horizon does move outwards as the bh radius increases . even more with any bh deformations such as waves on its surface , the tidal deformations or the change of the rotation speed , all the oblects close enough to the horizon remain " sticked " to it and follow all the changes of the bh form . all objects close enough to a rotating bh horizon , rotate with it at the same speed . you may ask then , how a black hole can appear then and the horizon form . it is conjectured that they cannot , and the only possible black holes are the hypothetical primordial black holes that existed from the very beginning of the universe . the objects that can be very similar to black holes are called collapsars . they are virtually indistinguishable from actual black holes after a very short time of the formation . they consist only of matter outside the radius of the event horizon of a bh with the same mass . this matter is virtually frozen on the surface like with actual bh , due to high gravity level . such collapsars possibly can become bhs for a short time due to quantum fluctuations and thus emit hawking radiation . astrophysicists do not separate such collapsars from actual black holes and call all them bhs due to practical reasons because of their actual indistinguishability . here is a quote from one paper that supports such point of view : our primary result , that no event horizon forms in gravitational collapse as seen by an asymptotic observer is suggestive of the possibility of using the number of local event horizons to classify and divide hilbert space into superselection sectors , labeled by the number of local event horizons . our result suggests that no operator could increase the number of event horizons , but the possibility of reducing the number of pre-existing primordial event horizons is not so clear and would require that hawking radiation not cause any primordial black hole event horizons to evaporate completely . source
you have exact equations for the solution in the related question time it takes for temperature change . here i would add a few comments . it is actually easier if container is thick ! then suppose that all water is at same temperature $t$ and all the air in the frizer is at the same temperature $t_e$ . $t_e$ is constant . if that is so , you can use only fourier 's law to describe how heat $q$ leaves the container $$\frac{\text{d}q}{\text{d}t} = \frac{\lambda a}{d} ( t-t_e ) . $$ $d$ is thickness , $a$ area and $\lambda$ thermal conductivity of the container . knowing that water cools as heat is leaving the container $$\text{d}q = m c \text{d}t , $$ where $m$ is mass and $c$ is specific heat capacity of the liquid , you get rather simple differential equation $$m c \frac{\text{d}t}{\text{d}t} = \frac{\lambda a}{d} ( t-t_e ) , $$ $$\frac{\text{d}t}{ ( t-t_e ) } = \frac{\lambda a}{d m c } \text{d}t = k \text{d}t , $$ which has exponential solution : $$k t = \ln \left ( \frac{t-t_e}{t_0-t_e}\right ) , $$ $$t = t_e + ( t_0-t_e ) e^{-kt} . $$
no . there are two different types of an angular momentum . first is connected with the coordinate representation , so it can be interpreted from classical mechanics point of view . second is not connected with coordinate representation , but it exist in every particle of the free field ( i.e. . , is an own angular momentum ) which you have tested . they are the principal different types of an angular momentum . from the qm position , they both are the eigenvalues of the representations of 3-rotation generator , but first refers to reducible , and second - to irreducible representations . maybe it is more convenient for you to compare the spin and electrical charge . mainly we do not ask about origin of charge and do not interpret it as the result of other quantity . it is independ quantity , and it is existence leads to electromagnetical interaction . also , the existence of spin ( it is value ) leads to some spin interaction ( simplistically can imagine as result of fermi-dirac or bose-einstein statistics . also , we can make an analogy between the quantum spin of particle and an own angular momentum ( classical spin ) of the system of particles . first and second are not connected with motion the particle ( or system ) as whole .
yes , they look at how the point ( on the streamline ) moves as a result of the defined quantities ( pressure , density , velocity , etc ) . since bernoulli 's equation results from an equation of conservation of energy , you are assuming no loss of energy , which means no friction , which for fluids means no viscosity , which means inviscid flow . so the equation is $v^2/2+gy+p/\rho = const$ , which has no change in internal energy ( i.e. . $u_1=u_2$ ) . example on pg74 , however , has a change in internal energy , but that is because here the fluid is not inviscid . ok was going to leave this for you to puzzle through , but just for completeness : because of the immediate area change of the pipe , with the constant steady pressure $p_1&gt ; 0$ , there is turbulent flow ( high reynolds number , depicted in the picture by the swirls ) , corresponding to viscosity and hence not inviscid .
as in the comments , there is certainly something of a convention at work here and it is to do with the " co-incidence " that we live in three spatial dimensions . as in greg 's answer , torque is intimately linked with angular momentum through euler 's second law . that is , torque and angular momentum are about rotational motion . and rotations , in general , are characterized by the planes that they rotate together with the angles of rotation for each of these planes . in three dimensions , the plane of rotation can be defined by a single vector - namely the vector orthogonal to the plane . so we have the concept of the " axis " of rotation , but this is not general , its simply that a line happens to be the subspace of a three dimensional vector space that is orthogonal to the plane of rotation . in four and higher $n$ spatial dimensions , the concept of an axis is meaningless : not only does an axis not specify a plane ( the space orthogonal to a plane is of dimension $n-2$ ) , but also a general rotation rotates several planes ( up to and including the biggest whole number less than or equal to $n/2$ ) . so the " true " information specifying a three dimensional rotation is the " bivector " $a\wedge b$ , where $a , b$ are linearly independent vectors defining the plane , and a bivector is an abstract directed " plane " just like a " vector " is an abstract directed " line " . cross products in three dimensions are actually bivectors , not vectors , but we can get away with thinking of them as such in three dimensions . some further reading to help you out : the wikipedia pages plane of rotation , rotation matrix and orthogonal group ( rotation matrices form the group $so ( n ) $ , the group of orthogonal matrices with unit determinant ) .
you do not want $1/r$ ( although technically it means the same ) but rather the full curvature term : $\delta p=\sigma \kappa$ . in fact you will get a source term in the navier-stokes equations that looks like this : $$\sigma \kappa \delta ( n ) \mathbf{n} $$ where $\delta ( n ) $ is the dirac delta function that only has a value at the interface and $\mathbf{n}$ is the interface normal . the curvature $\kappa$ can be written as the divergence of the unit interface normal : $$\kappa=\nabla \cdot \mathbf{\frac{n}{|n|}} $$ apart from the source term you indeed also have boundary conditions on the interface which are basically the standard free slip condition and a jump for the normal stress coming again from the laplace pressure . there is a good explanation of these in the first part of the seminal work on fluid-fluid cfd by brackbill . if you are interested in the curvature itself , i think slides 22-28 of this course on wetting are probably also a good source to take a look at for more background .
more physically than a lot of the other answers here ( a lot of which amount to " the formalism of quantum mechanics has complex numbers , so quantum mechanics should have complex numbers ) , you can account for the complex nature of the wave function by writing it as $\psi ( x ) = |\psi ( x ) |e^{i \phi ( x ) }$ , where $i\phi$ is a complex phase factor . it turns out that this phase factor is not directly measurable , but has many measurable consequences , such as the double slit experiment and the aharonov-bohm effect . why are complex numbers essential for explaining these things ? because you need a representation that both does not induce time and space dependencies in the magnitude of $|\psi ( x ) |^{2}$ ( like multiplying by real phases would ) , and that does allow for interference effects like those cited above . the most natural way of doing this is to multiply the wave amplitude by a complex phase .
as you have discovered proper time , $\delta\tau$ , can be either real or imaginary . however , this means that it does not necessarily reflect something measurable with a clock . when it is imaginary , as in the case of a space-like relation of two events , then there is no single clock that can be present at both events . to do so would require having a velocity $v&gt ; c$ . this is , i suppose , a fancy way of saying that the two events are not causal ( event 1 does not cause event 2 and vice versa ) .
let us for simplicity consider $n$ point charges $q_1$ , $\ldots$ , $q_n$ , at positions $\vec{r}_1$ , $\ldots$ , $\vec{r}_n$ , in the electrostatic limit , with vacuum permittivity $\epsilon_0$ . now let us sketch one possible strategy to prove gauss ' law from coulomb 's law : deduce from coulomb 's law that the electric field at position $\vec{r}$ is $$\tag{1} \vec{e} ( \vec{r} ) ~=~ \sum_{i=1}^n\frac{q_i }{4\pi\epsilon_0}\frac{\vec{r}-\vec{r}_i}{|\vec{r}-\vec{r}_i|^3} . $$ deduce the charge density $$\tag{2} \rho ( \vec{r} ) ~=~\sum_{i=1}^n q_i\delta^3 ( \vec{r}-\vec{r}_i ) . $$ recall the following mathematical identity $$\tag{3}\vec{\nabla}\cdot \frac{\vec{r}}{|\vec{r}|^3}~=~4\pi\delta^3 ( \vec{r} ) . $$ ( this phys . se answer may be useful in proving eq . ( 3 ) , which may also be written as $\nabla^2\frac{1}{|\vec{r}|}=-4\pi\delta^3 ( \vec{r} ) $ ) . use eqs . ( 1 ) - ( 3 ) to prove gauss ' law in differential form $$\tag{4} \vec{\nabla}\cdot \vec{e}~=~\frac{\rho}{\epsilon_0} . $$ deduce gauss ' law in integral form via the divergence theorem .
the values of $e_{\infty}$ were probably calculated numerically . they explain in the paper that they rewrote the equation for the energy to contain only $a$ . so what will have done is input a range of realistic values for $a$ and plot the energy in that range . they found a minimum and calculated the value of $e$ in that minimum ( that is not too hard ) . the reason for writing $e_{\infty}$ will be because there is apparently an attractor point in configuration space to which the oscillon tends . this means that after a long time the oscillon will come infinitesimally close to that point . it will ' settle down ' there . the energy corresponding to that point is therefore a good approximation for the energy after a long time ( $t\rightarrow\infty$ ) . that is most likely why this energy is subscripted with an infinity symbol .
it means that the charge operator $q$ is a lie algebra generator for some lie group $g$ . the field $\phi\in v$ takes values/transform in a representation $v$ of the lie group . ( note that any lie group representation $v$ is also lie algebra representation of the corresponding lie algebra . ) the charge operator $\rho ( q ) $ in the representation $\rho : g \to gl ( v ) $ is proportional to the identity . the proportionality factor/eigenvalue is the actual charge of the field . to see an example of this , say the ( strong ) $u ( 1 ) $ hypercharge $y$ , see this answer .
well i ought to be studying for a physics exam , but i will consider answering this to be my studying . newton 's third law states that for every action , there is an equal an opposite reaction . in this case , the jetpack is ejecting water at high velocity toward the ground . this is generating a significant force downward . the resulting opposite force pushes upward , elevating the rider . the actual pump for the unit is located on a floating watercraft attached to but not elevated with the jetpack . this is how there is enough force for the unit to fly - otherwise , the mass of the pump would be so great that it would be difficult to maintain enough pressure to keep both the pump and rider aloft . i do not know the specifics of how it orients itself , though i imagine that by making small variations in the water pressure and orientation of each nozzle , one can cause an imbalance in the forces acting on the rider , casing them to move one way or the other .
yes , it does . when an object floats , its mass is not affected . it only affect the force experienced by it , as the water exerts a " buoyant force " on the object : basically , there is a pressure difference between the top and bottom surfaces , and this corresponds to a force difference , leading to a net upwards force : however , remember that the force exerted by the water on the object leads to an equal and opposite force exerted by the object on the water . since at equilibrium , $m_{obj}g=\text{buoyant force}$ , a force equal to the weight of the body is exerted on the water . when the beaker is weighed , this extra force is balanced by the normal force . here are some free body diagrams . bf is the buoyant force , t is the string tension . note that n is the weight that the weighing platform measures .
the space between atoms depends very much on the medium you are talking about . in solids the typical distance between atoms is about the same as the size of the atoms themselves . in everyday gases at room temperature and pressure the distance between molecules is many times their size , and in deep space you can get densities as low as one proton per cubic centimetre ! you can get a rough idea of the average separation $\ell$ between atoms by using $$ \ell \approx \left ( \frac{m}{\rho}\right ) ^{1/3} $$ where $m$ is the mass of an atom and $\rho$ is the ( mass ) density of the material . this can be compared to the size of an atom , which for all elements is about the same at $\approx 10^{-10} - 10^{-9}\ \mathrm{m}$ . space is full of fields like the electric and magnetic fields . you can think of certain types of " vibrations " of these fields as virtual particles , but the common view of modern physics is that the field picture is more fundamental . there are fields for all of the elementary particles , and the fields are constantly fluctuating due to quantum mechanics . you can think of temporary ripples in the fields as virtual particles , which are responsible for transmitting disturbances through space . real particles are quantised excitations ( or vibrations ) in a field which propagate long distances . matt strassler has gone to great lengths to explain this point of view in his popular articles . frederic brünner brings up an important point about virtual particles . physicists use an approximation called perturbation theory to do most of their calculations ( because the calculations are really hard to do without making approximations ) . virtual particles are a convenient way to organise these calculations , but you should not think of them as physical objects like real particles . in a sense virtual particles are misleading . what they really represent are rapid fluctuations of the fields ( what i called ripples before ) . at large distances these fluctuations do not matter except when they average out to a smooth classical field . for the interactions between atoms , and even most of the interactions between electrons and nuclei , the classical field is all you need .
from the point of view of people standing on the earth , no effect whatsoever . that is because the so-called " relativistic mass " is an effect of different frames of reference . ( it is also pretty trivial from the pov of someone at rest with respect to the sun , surpressed by factors of order $\frac{30000\text{ m/s}}{300000000\text{ m/s}} = 10^{-4}$ . ) for many purposes working scientists have almost entirely stopped using the phrases " relativistic mass " and " rest mass " , finding the former concept to be of little practical use .
more smaller triangles are better than a few big ones ( more stiff ) . also think about which triangle shape distributes the loads for evenly . how are you constrained dimensionally ? ideally you create a vertical structure that distributes the weight over $n$ columns of noodles . if you can not do this , then you find which configuration yields the more vertical orientations . also it is important to know if the noodles can rotate about their support making them a 2-force member , or are they fully loaded with the ends fixed in location and orientation ? think of stiffness here . how would you make the truss as stiff as possible ? would long flat triangles be better , or short tall ones ? i guess without more details we cannot qualify an answer .
i think that sunrise equation and declination of the sun provide enough information . you put the equation from the second link into the equation from the first link . you get hours by multiplying the positive solution $\omega_0$ by $2 \cdot \frac{24\text{h}}{2\pi}$ . if the equation from the first link has no solution ( $\tan\phi \cdot \tan\delta> 1$ ) , this means day is either $24\text{h}$ or $0\text{h}$ long . as far as i checked equations ' output , they seem to be consistent .
when you scatter an electron you change it is energy . so if it was not possible to change the energy of an electron you could not scatter it . this is basically what happens in superconductors . in a metal at room temperature the electrons have a continuous range of energies . this means if i want to change the energy of an electron by 0.001ev , or even 0.000000001ev there is no problem doing this . this is a big difference from an isolated atom , where the electrons occupy discrete separated energy levels . if you try to change the energy of the electron in a hydrogen atom by 0.001ev you can not do it . you have to supply enough energy to make it jump across the energy gap to the next energy level . in superconductors the correlation between the electrons effectively turns them into bosons and they all fall into the lowest energy state . however the correlation also opens a gap between the energy of this lowest state and the energy of the next state up . this is why defects in the solid can not scatter electrons , and why they conduct with no resistance . for an electron to scatter off a defect ( or anything else ) in the conductor you have to supply enough energy to jump across the gap between the lowest energy level and the next one up . however the energy available is not great enough for this , and this means the defects can not scatter the electrons and that is why they superconduct . the trouble is you are now going to ask for an intuitive description of why the electron correlations open a gap in the energy spectrum , and i can not think of any way to give you such a description . sorry :- (
you are right and the author is wrong . the problem of p=np is a pure mathematical problem , which has nothing to do with physics . even though quantum mechanics ( or whatever physical system ) can solve all problem in blink of eye , it still does not prove whether p=np or not . the key point is that all computations are based on physics , but not the reverse . in computer complexity theory , they treat these ( existing or imaginary ) superpower machine as an oracle machine , which can give an answer in a single computational step . this formulation allows them to analyze quantum computer . the claim of non-observable macroscopic quantum effects because of p ! =np is based on the following argument : to prove macroscopic quantum effects , we need to compare the physical system with the simulation results of schrodinger equation . so , if we can not simulate schrodinger equation efficiently , then we can not prove any quantum effect . as shown in the paper : this implies that in the case , in which the problem $\phi_\psi$ would be intractable , the deterministic quantum model of a macroscopic system ( built around the exact solutions to the system schrodinger equation ) would be without predictive content inasmuch as there would be no practical means to extract the prediction about the system future state from the schrodinger equation . in this manner , a schrodinger cat state – as a linear combination of the exact ( and orthogonalized ) solutions to the system schrodinger equation – would be predictively contentless and for this reason unavailable for inspection . the author clearly does not familiarize with quantum mechanics , nor the schrodinger equation . schrodinger equation is only a part of qm . he also does not understand the particle concept in the schrodinger equation . a particle is not an atom . this is a basic concept that most physics student should have understand after half dozen courses in qm . the interference of one c$_{60}$ molecule can be described by one particle wavefunction $\psi ( x ) $ . there is no need to solve a 60-particles wavefunction $\psi ( x_1 , . . . , x_{60} ) $ , which is already extreme hard to solve by current computers . if a schrodinger cat state exists , you can always perform a bell-state type measurement , even at the macroscopic level . there is no need to solve schrodinger equation with large number of variables in wavefunction $\psi ( x_1 , . . . , x_{10^{23}} ) $ to know the result , since the system should be effectively described by a two state system .
there are very good experimental limits on light neutrinos that have the same electroweak couplings as the neutrinos in the first 3 generations from the measured width of the $z$ boson . here light means $m_\nu &lt ; m_z/2$ . note this does not involve direct detection of neutrinos , it is an indirect measurement based on the calculation of the $z$ width given the number of light neutrinos . here 's the pdg citation : http://pdg.lbl.gov/2010/listings/rpp2010-list-number-neutrino-types.pdf there is also a cosmological bound on the number of neutrino generations coming from production of helium during big-bang nucleosynthesis . this is discussed in " the early universe " by kolb and turner although i am sure there are now more up to date reviews . this bound is around 3 or 4 . there is no direct relationship between quark and neutrino masses , although you can derive possible relations by embedding the standard model in various guts such as those based on $so ( 10 ) $ or $e_6$ . the most straightforward explanation in such models of why neutrinos are light is called the see-saw mechanism http://en.wikipedia.org/wiki/seesaw_mechanism and leads to neutrinos masses $m_\nu \sim m_q^2/m$ where $m$ is some large mass scale on the order of $10^{11} ~gev$ associated with the vacuum expectation value of some higgs field that plays a role in breaking the gut symmetry down to $su ( 3 ) \times su ( 2 ) \times u ( 1 ) $ . if the same mechanism is at play for additional generations one would expect the neutrinos to be lighter than $m_z$ even if the quarks are quite heavy . also , as you mentioned , if you try to make fourth or higher generations very heavy you have to increase the yukawa coupling to the point that you are outside the range of perturbation theory . these are rough theoretical explanations and the full story is much more complicated but the combination of the excellent experimental limits , cosmological bounds and theoretical expectations makes most people skeptical of further generations . sorry this was not mathier .
do you even know what you mean by " ignoring the uncertainties " ? i do not . you may neglect uncertainties sometimes . that does not so much have anything to do with a particular measurement of a quantity $x$ being exceptionally precise , but with putting that measurement in a calculation / comparing it with another measured quantity $y$ that has a much larger uncertainty . we can then argue that , using the actual uncertainty $\sigma_{\ ! x}$ of $x$ , the final result will not be notable different from the result we had get assuming $\sigma_{\ ! x}=0$ . but that does not mean we ignore $\sigma_{\ ! x}$: we might later on compare $x$ to another quantity $z$ that is known with even smaller uncertainty than $x$ is . to give an example in numbers , let $$\begin{aligned} x = and 37.5088 ( 46 ) \:\mathrm{m} , \\y= and 37.41 ( 30 ) \:\mathrm{m} , \\z= and 37.5067351 ( 93 ) \:\mathrm{m} \end{aligned}$$ these measuments clearly are all in agreement . we might have decided to neglect the uncertainty of $x$ when comparing it to $y$ , because $y$ is much more uncertain . but if we had decided to ignore it we would then have faced the comparison $$ 37.5088\:\mathrm{m}\pm0\ \overset{ ? }=\ 37.5067351 ( 93 ) \:\mathrm{m} $$ from which we would have had to conclude that $z$ disagrees with $x$ , with great significance !
here 's my two cents worth . why lie algebras ? first i am just going to talk about lie algebras . these capture almost all information about the underlying group . the only information omitted is the discrete symmetries of the theory . but in quantum mechanics we usually deal with these separately , so that is fine . the lorentz lie algebra it turns out that the lie algebra of the lorentz group is isomorphic to that of $sl ( 2 , \mathbb{c} ) $ . mathematically we write this ( using fraktur font for lie algebras ) $$\mathfrak{so} ( 3,1 ) \cong \mathfrak{sl} ( 2 , \mathbb{c} ) $$ this makes sense since $\mathfrak{sl} ( 2 , \mathbb{c} ) $ is non-compact , just like the lorentz group . representing the situation when we do quantum mechanics , we want our states to live in a vector space that forms a representation for our symmetry group . we live in a real world , so we should consider real representations of $\mathfrak{sl} ( 2 , \mathbb{c} ) $ . a bit of thought will convince you of the following . fact : real representations of a lie algebra are in one-to-one correspondence ( bijection ) with complex representations of its complexification . that sounds quite technical , but it is actually simple . it just says that we can have complex vector spaces for our quantum mechanical states ! that is , provided we use complex coefficients for our lie algebra $\mathfrak{sl} ( 2 , \mathbb{c} ) $ . when we complexify $\mathfrak{sl} ( 2 , \mathbb{c} ) $ we get a direct sum of two copies of it . mathematically we write $$\mathfrak{sl} ( 2 , \mathbb{c} ) _{\mathbb{c}} = \mathfrak{sl} ( 2 , \mathbb{c} ) \oplus \mathfrak{sl} ( 2 , \mathbb{c} ) $$ so where does $su ( 2 ) $ come in ? so we are looking for complex representations of $\mathfrak{sl} ( 2 , \mathbb{c} ) \oplus \mathfrak{sl} ( 2 , \mathbb{c} ) $ . but these just come from a tensor product of two representations of $\mathfrak{sl} ( 2 , \mathbb{c} ) $ . these are usually labelled by a pair of numbers , like so $$|\psi \rangle \textrm{ lives in the } ( i , j ) \textrm{ representation of } \mathfrak{sl} ( 2 , \mathbb{c} ) \oplus \mathfrak{sl} ( 2 , \mathbb{c} ) $$ so what are the possible representations of $\mathfrak{sl} ( 2 , \mathbb{c} ) $ ? here we can use our fact again . it turns out that $\mathfrak{sl} ( 2 , \mathbb{c} ) $ is the complexification of $\mathfrak{su} ( 2 ) $ . but we know that the real representations of $\mathfrak{su} ( 2 ) $ are the spin representations ! so really the numbers $i$ and $j$ label the angular momentum and spin of particles . from this perspective you can see that spin is a consequence of special relativity ! what about compactness ? this tortuous journey shows you that things are not really as simple as ryder makes out . you are absolutely right that $$\mathfrak{su} ( 2 ) \oplus \mathfrak{su} ( 2 ) \neq \mathfrak{so} ( 3,1 ) $$ since the lhs is compact but the rhs is not ! but my arguments above show that compactness is not a property that survives the complexification procedure . it is my " fact " above that ties everything together . interestingly in euclidean signature one does have that $$\mathfrak{su} ( 2 ) \oplus \mathfrak{su} ( 2 ) = \mathfrak{so} ( 4 ) $$ you may know that qft is closely related to statistical physics via wick rotation . so this observation demonstrates that ryder 's intuitive story is good , even if his mathematical claim is imprecise . let me know if you need any more help !
the pairs of lines are the same phase and at the same voltage - they are really just a single thick wire split into two thinner ones . it is easier to install two smaller wires to double the current capacity than a single thicker wire . it is easier to handle the lighter cable and you can stock just a single gauge of wire and handling equipment . it also provides some redundancy if one wire fails . there is an effect with ac electricity that the current mostly flows near the surface of the conductor . a number of thinner wires have more area-near-the-surface and so a larger effective cross section area than a single thick one . at the 50/60hz frequencies used by ac transmission this only affects wires more than a cm thick .
no . of course , to argue if a definition applies , we must first agree on a definition . wikipedia gives this one : a crystal or crystalline solid is a solid material whose constituent atoms , molecules , or ions are arranged in an orderly repeating pattern extending in all three spatial dimensions . humans are certainly solid-ish , and our constituent molecules are arranged in a somewhat orderly pattern in all three dimensions . however , i think we fail the ' repeating ' portion of this definition . if you want to use a more broad definition of a crystal , link to it . finally , supposing the answer was ' yes ' . what are the practical predictions which follow from this assertion ? we certainly do not diffract x-rays into a regular grid , for example .
massless particles do not always have zero chemical potential . suppose that you have a box full of photons and other particles , and it is possible for the photons to exchange energy with other particles , but the number of photons cannot change . then the system will reach a thermal equilibrium in which the photons are described by a bose-einstein distribution with ( in general ) a nonzero chemical potential . the reason this does not usually happen with photons is that the number of photons is often not conserved in situations like this . if there are photon-number-changing processes , then the equilibrium state for the photons will have zero chemical potential ( since otherwise entropy could go up by creating or destroying a photon ) . in summary , the rules are that the chemical potential must be zero if particle-number-changing interactions are possible , but not otherwise . that distinction often coincides with the massless or massive nature of the particles , but not always . by the way , there was a period of time in the early universe when we were in precisely this situation : photons could thermalize via compton scattering with electrons , but at the temperature and density at the time , photon-number-changing processes essentially did not occur . that means that the cosmic microwave background radiation today could have a nonzero chemical potential . people have tried to measure the chemical potential , but as it turns out it is consistent with zero to quite good precision . this makes sense , as long as the photons and electrons came into thermal equilibrium at an earlier epoch ( when photon-number-changing processes did occur ) , and nothing happened during the later epoch to mess up that equilibrium . various theories in which particle decays inject energy into the universe during the constant-photon-number epoch are ruled out by this observation .
there exists a variety of options for this task but let me stress first that this is an extremely complicated and difficult issue that is still subject of current research because analytical continuation is an ill posed problem ! 1 ) the ' analytical ' analytical continuation can be performed when the function $f ( \mathrm i\omega ) $ under consideration is a rational function of $\mathrm i\omega$ . so $$f ( \mathrm i\omega ) =\frac{1}{\mathrm i\omega}$$ can be continued to the complex plane $\mathrm i\omega \rightarrow z\in\mathbb c$ while $$f ( \mathrm i\omega ) =\frac{e^{\mathrm i\omega\beta}}{\mathrm i\omega}$$ is not a rational function of $\mathrm i\omega$ and making the replacement here is a mistake . instead one needs to evaluate the exponential first and find $e^{\mathrm i\omega\beta}=\pm 1$ depending on the statistics . 2 ) directly inferred from this replacement rule comes the expansion of a function in a finite laurent series $$f ( z ) =\sum^{m_2}_{n=m_1} a_n z^n , \ ; \ ; m_1 , m_2\in\mathbb z$$ where the coefficients may be calculated from the numerical values known at $m_2-m_1$ matsubara energies . 3 ) one of the oldest methods to do a numerical analytical continuation is the pade approximation . the function in question is expanded in a continued fraction $$f ( z ) =b_0+\frac{a_1z}{1-\frac{a_2z}{1-\frac{a_3z}{1- . . . }}} . $$ the coefficients can be computed from a pade table , see http://en.wikipedia.org/wiki/pad%c3%a9_table method 1 ) is exact and other than for almost trivial calculations of little practical value . 2 ) and 3 ) suffer from cutoff effects due to the limited amount of available matsubara points at which the function value might also have a numerical error as is the case for data from quantum monte carlo calculations . but in fact analytical continuation is very volatile towards cutoff and noise effects . this is where physical considerations have to be accounted for . to tackle the cutoff one can approximate the tail ( large $\mathrm i\omega$ or respectively $z$ expansion ) of the function with an analytical form that can often times be computed exactly from the many body problem or general physical necessities , e.g. the 1-particle green 's function of a fermionic system always has the form $\frac{1}{z}+\frac{a_1}{z^2}+ . . . $ . the tail can be used to compute an arbitrary number of expansion coefficients but keep in mind that the interesting low energy spectrum of your system is strongly influenced by small matsubara energies and less so by the tail so from computing a large number of coefficients from the tail one gains little to nothing . the treatment of statistical noise is even more delicate than the cutoff and the reason why a lot of people try to avoid calculating on the matsubara axis altogether . 4 ) a prominent method for noisy data is the maximum entropy method about which you can read more here http://arxiv.org/pdf/1001.4351v1.pdf where you will also find references to alternative techniques .
1 ) the frequency chosen in microwaves precisely corresponds to the vibrational states of water , fats and sugars according to " how stuff works " on microwaves . http://home.howstuffworks.com/microwave1.htm this frequency , has an energy , equal to , e = hf , where h is planck 's constant . this energy is related to the vibrational states http://en.wikipedia.org/wiki/molecular_vibration since most foods have water , fat , or sugars in them , and they absorb this frequency and start vibrating , then they collide with other molecules and make them move too . when a group of molecules or atoms start moving around , then its " hot " . that is what heat basically is . even if visible light is more energetic than microwaves , they do not correspond to the vibrational states of the molecules in the food , ( water , fats , and sugars ) . therefore , what happens is that the light is not effectively absorbed . it gets reflected , or passes right through . if you have some em wave that is extremely high in energy , it would probably ionize the molecules in your food , which would literally start breaking it apart . ( which you do not want ) 2 ) microwaves are dangerous to humans or living organisms because even if they are low in energy , corresponding to radio waves , there frequency corresponds to water , fats and sugars vibrational states . this is what we and living organisms are mostly made out of . therefore , if you are inside a microwave , or if you put a live animal in a microwave , it will vibrate all the water , fats and sugar molecules . you will immediately feel really hot really fast , and all other molecules would start moving . you will be so hot that the water content in your body would have enough energy to start boiling . and when your water content in your body starts boiling , your cells break down and you would probably die . this is why its dangerous .
tl ; dr they shift but only if you have non perfect system , phase difference is compensated in a perfect system . first how to get no shift . imagine it is an ideal transformer . you apply the induction law once and get the $b ( t ) $ . $$u ( t ) =\int_\ell e ( t ) \mathrm{d}\ell=-\frac{\mathrm{d}b ( t ) }{\mathrm{d}t} $$ now the $b ( t ) $ is shifted in relation to $u ( t ) $ because of the derivation , but when we do the calculation of $\hat u$ we shift it back : $$\hat u ( t ) =-\frac{\mathrm{d}b ( t ) }{\mathrm{d}t} $$ so two shifts in opposite directions give zero phase difference . perfect . ( you should now think : but the second conductor influences the first . . . in some way , so i think this may be wrong ! that is what i was thinking , then i got over it . explanation : you would use the same formula over and over again ( ping-pong ) always getting net zero shift . ) if you really want to have a shift , you can look at it as a non-ideal transformer ( if you like add capacitances ) . $$\hat u ( t ) =\hat l\frac{\mathrm{d}\hat i ( t ) }{\mathrm{d}t}+ m\frac{\mathrm{d} i ( t ) }{\mathrm{d}t}+r\hat i$$ $$u ( t ) =l\frac{\mathrm{d} i ( t ) }{\mathrm{d}t}+ m\frac{\mathrm{d}\hat i ( t ) }{\mathrm{d}t}+ri$$ for a constant frequency $\omega$ you can write : $$\hat u ( t ) =j\omega \hat l \hat i+ j\omega m i+r\hat i$$ $$u ( t ) =j\omega l i+j\omega m\hat i+ri$$ if you know how to solve simple circuits these equations should not be a problem for you . the question remains , what are the values of $\hat l , m$ and $l$ ? the derivation of these formulas is something i can not remember , but you will find it for sure somewhere . the resistances can be calculated easier . what would happen if the resistances were the same ? if you add the capacitances you will have a characteristic impedance rating . even more fun stuff to think about !
when the notions of electric and magnetic fields were conceptualized , they imagined that there was an invisible fluid being pushed around by charges , and they leveraged some of the equations and terminology of fluid mechanics . modern understanding of field have largely gotten rid of this picture , but some colorful langauge like " electric flux " remains . if you want to picture positive charge as " amount of fluid added to region per unit time " and negative charge as " amount of fluid removed from region per unit time " , you can , but this thinking only gets you so far . safer to just think of it as an abstract mathematical definition .
it appears that electromagnetism has some of preponderant role in the universe compared to other theories this is not true . it played an important historic role , but is in no way theoretically " unique " because the photon travels at speed c . indeed , the gluon also travels at speed c . if photons were found to be slightly massive , it would change a lot of things , but it would not change relativity . at this point we have a well constructed theory where all and only massless particles travel at speed c . in other words , the photon is only historically central to relativity , not conceptually . the space-time speed of light ( with upper-case ) is really more of a universal constant that is not necessarily related to the speed of the electromagnetic waves or of photons . you have pretty much answered your own question , because this is exactly right . historically , questions regarding inertial reference frames and electromagnetism led to the development of special relativity . within relativity , an important speed constant appears . if indeed the photon is massless , then this speed should be the same as the speed of light . even if the photon were to turn out to be massive , this in no way obligates us to rewrite our theory . it just means that the historical tool we used to build relativity was not exactly what we thought it was , but was close enough for the early development of the theory . our current theoretical understanding of minkowski space ( the space-time geometry described by special relativity ) is not one that depends on electromagnetism , in fact it is the other way around : in our current understanding of electromagnetism , we use our understanding of minkowski space to conclude that massless particles like photons should move at the speed of light .
[ this is now a long answer . in summary , generally you need a physical assumption , the clock postulate , which people tend to omit , but is necessary , and can not be argued for a priori . however sometimes special relativity plus a restricted version of the postulate suffices . mundane experience is sufficient to verify this restricted version . ] let $\lambda = t$ the time according to inertial $o$ , and let $\vec{x}'$ be the spatial position of $o'$ according to $o$ , while $t'$ is the time measured by $o'$ . if $o'$ is piecewise inertial , then along each piece , $$c^2 ( \delta t'/\delta t ) ^2 = c^2 - ( \delta\vec{x}'/\delta t ) ^2\qquad [ 1 ] $$ and what you are trying to justify is that , even if $o'$ is not piecewise inertial , $$c^2 ( dt'/dt ) ^2 = c^2 - ( d\vec{x}'/dt ) ^2\qquad [ 2 ] $$ so , the problem is , special relativity strictly speaking only makes claims about inertial observers . and if you do not make any assumptions whatsoever about the experience of accelerated observers , then i think you are just stuck , mathematically i do not think you can go from $ [ 1 ] $ to $ [ 2 ] $ . ( for example , we can not rule out that proper acceleration itself further contributes to time dilation . ) i suggest : the motion of $o'$ is smooth . [ a1 ] for every $\epsilon&gt ; 0$ , there is a $\delta&gt ; 0$ such that , if , from the point of view of a certain unaccelerated observer $a$ , the magnitude of the velocity of another observer $b$ never exceeds $c\delta$ betwen time $t_0$ and $t_1$ , then time lapse $\delta t_b$ on $b$ 's clock satisfies $ ( t_1-t_0 ) ( 1-\epsilon ) &lt ; \delta t_b &lt ; ( t_1-t_0 ) ( 1+\epsilon ) $ . [ a2 ] pick $\epsilon&gt ; 0$ , use [ a2 ] to get $\delta$ ; use [ a1 ] to break the motion of $o'$ into intervals small enough such that , from the frame of reference of an interial observer travelling between the endpoints of a piece , the velocity of $o'$ never exceeds $c\delta$ ; use [ a2 ] to make $ [ 2 ] $ true within $\epsilon$ . since this works for all $\epsilon&gt ; 0$ , [ 2 ] is simply true . now [ a1 ] might look suspect , since we have used a piecewise inertial observer , whose motion is obviously not smooth ! so we can not even assume anything about what this piecewise inertial observer experiences at the corners ! but that is okay , [ a2 ] only refers to the individual pieces and not the whole . use a family of ( truly ) inertial observers that meet at the appropriate points . as for [ a2 ] , it is a bit opaque , but what it says that if you are not moving too fast relative to an inertial observer , your experience of time is almost the same . this does not follow logically from anything in particular , it is just a physical assumption . but note that special relativity is so hard for many people to accept precisely because [ a2 ] is a fact of life , for reasonably small $\epsilon$ . to make it true for even smaller $\epsilon$ requires more than everyday experience , but it is still " common sense " , and presumably testable to quite small values . now , to believe it literally for arbitrarily small $\epsilon$ requires quite a leap , but do not take differential equations literally . ( added : ) aha ! i found the clock postulate for accelerated observers , and i do believe [ a2 ] is interchangeable with it . and yes , it is often omitted but cannot be derived from other assumptions . it has been tested . ( second addendum ) : even though they are interderivable , mine is better :- ) i have given the accuracy of [ 2 ] directly in terms of the accuracy of [ a2 ] . for example , we do not need the full clock postulate for the twin paradox ( which you mention as a motivating example in a comment ) : the proper acceleration of $o'$ is continuous and its magnitude is bounded by $a_{max}$ . [ a1' ] ( over any finite interval , [ a1 ] does imply [ a1' ] for some value of $a_{max}$ . and [ a1' ] is sufficient for the above argument . ) now , even with mundane accelerations , the twin paradox can produce a sizeable mismatch in ages within a human lifetime . ( besides , if they are not survivable accelerations , the travelling twin 's lifetime ends ! ) so , there is a usable $a_{max}$ for [ a1' ] . and , mundane experience alone proves [ a2 ] up to that $a_{max}$ and down to fairly small $\epsilon$ . so [ 2 ] holds sufficiently accurately to give the twin paradox . we only need special relativity plus a mundane restricted clock postulate . ( i realise you can bypass the whole acceleration question by altering the paradox so that there are three inertial observers who compare clocks as they pass . but then it is not the twin paradox anymore , duh ! )
the process used in this kind of source is the spontaneous parametric down conversion ( spdc , see , e.g. wikipedia for details ) . it is a nonlinear optical process in which from a photon with angular frequency $\omega_0$ you get two photons with frequencies $\omega_1$ , $\omega_2 = \omega_0-\omega_1$ . these photons are then phase matched and have correlated polarization ( either the same or opposite , we speak of type i spdc and type ii spdc , respectively ) .
most of the popular science tv programmes and magazine articles give entirely the wrong idea about how the higgs mechanism works . they tend to give the impression that there is a single higgs boson that ( a ) causes particles masses and ( b ) will be found around 125gev by the lhc . the mass is generated by the higgs field . see the wikipedia article on the higgs mechanism for details . to ( over ) simplify , the higgs field has four degrees of freedom , three of which interact with the w and z bosons and generate masses . the remaining degree of freedom is what we see as the 125gev higgs boson . in a sense , the higgs boson that the lhc is about to discover is just what is left over after the higgs field has done it is work . the higgs boson gets its mass from the higgs mechanism just like the w and z bosons : it is not the origin of the particle masses . the higgs boson does not have zero rest mass . a quick footnote : matt strassler 's blog has an excellent article about this . the higgs mass can be written as an interaction with the higgs field just like e.g. the w boson . however matt strassler makes the point that this is a coincidence rather than anything fundamental and unlike the w and z the higgs boson could have a non-zero mass even if the higgs field was zero everywhere .
basically , i think the idea that the universe is infinite comes from considerations of the large-scale curvature of spacetime . in particular , the flrw cosmological model predicts a certain critical density of matter and energy which would make spacetime " flat " ( in the sense that it would have the minkowski metric on large scales ) . if the actual density is greater than that density , then spacetime is " positively curved , " which implies that it is also bounded - that is , that there is a certain maximum distance between any two spacetime points . ( i do not know the details of how you get from positive curvature to being bounded ) however , if the actual density is not greater than that critical density , there is no bound , which means that for any distance $d$ , you could find two points in the universe that are at least that far away . i think that is what it means to be infinite . overall , the observations done to date , paired with current theoretical models , are inconclusive as to whether the actual density of matter and energy in the universe is greater than or less than ( or exactly equal to ) the critical density . now , if the universe is in fact infinite in this sense , it still could have had a big bang . the flrw metric includes a scale factor $a ( \tau ) $ which characterizes the relative scale of the universe at different times . specifically , the distance between two objects ( due only to the change in scale , i.e. ignore all interactions between the objects ) at different times $t_1$ and $t_2$ satisfies $$\frac{d ( t_1 ) }{a ( t_1 ) } = \frac{d ( t_2 ) }{a ( t_2 ) }$$ right now , it seems that the universe is expanding , so $a ( \tau ) $ is getting larger . but if you imagine running that expansion in reverse , eventually you had get back to a " time " where $a ( \tau ) = 0$ , and at that time all objects would be in the same position , no matter whether space was infinite or not . that is what we call the big bang .
so you want to know how much water a certain surface adsorbs . this is really dependent on the surface material/conditions . check adsorption and relative humidity on wikipedia . to where i have analyzed , it seems that there is about enough information in the two articles . i am not a specialist on the subject so i might be missing some important factor .
the first explanation is just a quick argument to avoid doing calculations proposed in the second explanation . as you are looking for ground state of the system , you want to minimize energy $\langle \psi | \hat{h}| \psi\rangle$ . now , from commutation relations between position and momentum operators $ [ \hat{q} , \hat{p} ] = i \hbar$ follows that $\delta q \delta p \geq \frac{\hbar}{2}$ , no matter for which state you calculate $\delta q$ and $\delta p$ . from this you can estimate kinetic energy to be of order $\frac{1}{ ( \delta q ) ^2}$ , while the coulomb energy should be of order $-\frac{1}{\delta q}$ . obviously as you shrink your candidate for ground state , sooner or later kinetic term will dominate and blow up your energy , meaning your ground state is stable against shrinking . all these " heisenberg " and " schroedinger " formulations , wave-particle duality and whatever are just out-of-date jargon , which brings nothing but confusion .
they do not always seem to be that shape . for some cuboids which are not designed to be moved regularly , see this picture from fort knox . but for bullion bars in the world market ( about 10.9-13.4 kg of gold ) which have the standard sort of trapezium cross-section , the top when casting is the wider area to get them out of the mould , so the wider area is defined as the " top " in the international specification ) and the fineness , hallmarks and serial numbers usually stamped on top ( as illustrated here ) . stacking them top face up makes these easier to read for the bars on top , as well as easier to move . pure gold bars may dent if dropped because of gold 's softness , requiring recasting , so some care is needed .
hints to the question ( v5 ) : op correctly imposes two conditions because of the delta function potential at $x=-a$ , but op should also impose the boundary condition $\psi ( x\ ! =\ ! 0 ) =0$ because of the infinite potential barrier at $x\geq 0$ . there is zero probability of transmission because of the infinite potential barrier at $x\geq 0$ . ( recall that transmission would imply that the particle could be found at $x\to \infty$ , which is impossible . ) hence there is a 100 percent probability of reflection , cf . the unitarity of the $s$-matrix . see also this phys . se answer . as op writes , away from the two obstacles , one has simply a free solution to the time-independent schrödinger equation , namely a linear combination of the two oscillatory exponentials $e^{\pm ikx}$ . this solution is non-normalizable over a non-compact interval $x\in ] -\infty , 0 ] $ . to make the wave function normalizable , let us truncate space for $x&lt ; -k$ , where $k&gt ; 0$ is a very large constant . so now $x\in [ -k , 0 ] $ . one may then define and calculate the probability $p ( -a \leq x\leq 0 ) $ of finding the particle between the two barriers via the usual probabilistic interpretation of the square of the wave function . if we now let the truncation parameter $k\to \infty$ , then we can deduce without calculation that this probability $p ( -a \leq x\leq 0 ) \to 0$ goes to zero .
you have forgotten to include the motion of the center of mass of the system . as the problem says , use the angle $\phi$ and the cartesian coordinates $\textit{of the cm}$ . the kinetic energy should instead be $t = \frac{1}{2} i \dot{\phi}^2 + m ( \dot{x}^2 + \dot{y}^2 ) $
yes . you have probably heard that string theory predicts the universe is 10 dimensional ( and m-theory predicts it is 11 dimensional ) while we only see 4 dimensions . however this is not because the number of dimensions has changed , but because 6 ( or 7 ) of the dimensions are rolled up into a very small circle . having said this , there have been suggestions that the universe started out as 2 dimensional ( 1 space and 1 time ) then the number of dimensions increased to 10/11/whatever as the universe evolved . however the idea comes from causal dynamical triangulation , and this is pretty speculative even by the standards of quantum garvity theories . there are even wilder suggestions that the spacetime dimension might be fractal .
imagine what happens when $\delta \phi$ keeps increasing to make a full rotation of $360^ \circ$ . then the angle of $p_2$ increases by $360^ \circ$ so that $p_2$ comes back to $p_1$ . also we know that after the full rotation $\vec{v}_1$ must be equal to $\vec{v}_2$ again . since $\vec{v}_2$ is going around in a circle at the same time $p_2$ does , its angle with $\vec{v}_1$ seems like it should be the same as the angle $p_2$ makes with $p_1$ . more rigourously , the direction of $\vec{v}_1$ is just the direction of $p_1$ rotated by $90^\circ$ . similarly the direction of $\vec{v}_2$ is just the direction of $p_2$ rotated by $90^\circ$ . then since the difference in angle between $p_1$ and $p_2$ is $\delta \phi$ , and $\vec{v}_1$ and $\vec{v}_2$ are essentially rigidly rotated copies of $p_1$ and $p_2$ , the angle between $\vec{v}_1$ and $\vec{v}_2$ must also be $\delta \phi$ .
yes , you are literally simulating the flow of heat through a material . the diffusion coefficient is basically just a local measure of thermal conductivity and heat capacity . rather than convolving your image with a gaussian kernel , you could use something like the crank-nicholson method and make some number of timesteps . you could also adjust $d$ to make it greater in parts of the image with greater noise . $\nabla$ is the gradient operator . it is a vector operator that looks like this in two dimensional cartesian coordinates : $$ \nabla= \hat{\mathbf{x}} \frac{\partial}{\partial x} + \hat{\mathbf{y}} \frac{\partial}{\partial y}$$ where $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}}$ are the unit vectors pointing in the $x$ and $y$ directions . in short , your intuitive understanding is correct .
i do not know the exact number but i want to support johannes ' claim that the percentage is way smaller by a calculation . most of the light arguably comes from the milky way - especially the strip that gave name to the galaxy . the diameter of the milky way is 100,000-120,000 light years so the median star 's distance is something like 50,000 light years away from us . that is approximately $3\times 10^{9}$ times longer a distance than those 500 seconds for the sun . one must square the distance ratio to get the light power ratio , about $10^{17}$ , between the sun and the typical milky way star . even when $10^{-17}$ is multiplied by the number of stars in the milky way , about $1-4\times 10^{11}$ stars , one gets $1-4$ parts per million of the light , also assuming that the sun is an average-size star . my estimate is 3 orders of magnitude greater than johannes ' but it is still vastly smaller than 0.5% . just to check , sirius is the brightest star in the sky . it is 25 times brighter than the sun but it is 9 light years away , which is $500,000$ times further than the sun . square it and divide 25 by it to get $10^{-10}$ . that is the fraction of the sunlight obtained from sirius . you see that it is much smaller than the result for the generic milky way stars above , so individual bright ( and mostly nearby ) stars are unlikely to topple the statistical estimate . the weakest point of the statistical estimate is that the sun is not quite the average star . one may also check the contribution from other galaxies . there are about $2\times 10^{11}$ galaxies in the universe . however , even if you decide that the average distance from us is 5 billion years only , shorter than half of the age of the universe , it is 100,000 times further than the average milky way star discussed above ( 50,000 light years ) . square it to get $10^{10}$ for the ratio . if you multiply $10^{-10}$ by $10^{11}$ , you actually conclude that the total light from other galaxies is about 10 times greater than the total light from the milky way . but that is probably an overestimate because much of the very distant galactic light is redshifted , absorbed , and the older galaxies may have a lower luminosity . at any rate , it is unlikely that they will drive us above 1/100,000 of the sunlight . finally , instead of trying even more distant stars , let me mention that there is also the moon in the sky . it is actually dominating or almost dominating the luminosity at night , except for the new moon or eclipses . in average , we get 1 milliwatt from the moonlight which is 1/300,000 of the sun 's 342 watts ( averaged over places , seasons , day cycles ) . that is about the same what i got for the total strip of stars in the milky way – 3 parts per million of the sun – but my estimate of the stars was probably an overestimate and i believe the moon is brighter than the milky way combined .
both formulas are equivalent , if you are in the electrostatic approximation and your dipole vector does not depend on the position $\mathbf{r}$ . let 's consider the expression $\mathbf{f}=\nabla_{\mathbf{r}} ( \mathbf{p} \cdot \mathbf{e} ) $ which can be easily obtained from the potential energy function $u=-\mathbf{p} \cdot \mathbf{e}$ and its relation with the force $\mathbf{f}=\nabla_\mathbf{r} u$ . now , recall the vector identity $\nabla_\mathbf{r} ( \mathbf{a}\cdot \mathbf{b} ) = ( \mathbf{a} \cdot \nabla_\mathbf{r} ) \mathbf{b}+ ( \mathbf{b} \cdot \nabla_\mathbf{r} ) \mathbf{a} + \mathbf{a} \times ( \nabla_\mathbf{r} \times \mathbf{b} ) + \mathbf{b} \times ( \nabla_\mathbf{r} \times \mathbf{a} ) $ for $\mathbf{a}=\mathbf{a} ( \mathbf{r} ) $ and $\mathbf{b}=\mathbf{b} ( \mathbf{r} ) $ two arbitrary vectors . for $\mathbf{p}=\mathbf{a} \neq \mathbf{p} ( \mathbf{r} ) $ [ independent of the position ] and $\mathbf{b}=\mathbf{e} ( \mathbf{r}$ ) we have $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}+ ( \mathbf{e} \cdot \nabla_\mathbf{r} ) \mathbf{p} + \mathbf{p} \times ( \nabla_\mathbf{r} \times \mathbf{e} ) + \mathbf{e} \times ( \nabla_\mathbf{r} \times \mathbf{p} ) $ as the dipole vector does not depend on the position we can drop the second and the fourth terms . in the electrostatic approximation , faraday 's law reads $\partial_t \mathbf{b}=\mathbf{0}\leftrightarrow \nabla_\mathbf{r} \times \mathbf{e} ( \mathbf{r} ) =\mathbf{0} $ [ this is known as ''carn 's law'' ] so that the electric field is irrotational and the curl vanishes . then we can drop the third term and $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}$ so that your definitions agree .
javier , try looking at what is an intuitive picture of the motion of nucleons ? to start . the short answer is that it is as reasonable to say that they are identical as it is to say that the configuration of electrons in multiple atoms of a single element are identical . that is , there is a set of position ( or momentum 1 ) distributions to which they conform . 1 the position and momentum distributions turn out to be linked to each other by fourier transformations , so information required to specify one is the same as that required to specify the other . nuclear physicist mostly concern themselves with the momentum distributions .
if alice observes that bob travels 1000 lightyears in 1000 years , then bob 's speed is c according to alice and thus , is c in all frames of reference which means that bob does not have a frame of reference at all . let 's try adjusting the numbers a bit . let alice observe that bob , moving with a constant speed , travels 999 lightyears in 1000 years . according to alice , bob has a speed of 0.999 c . bob knows when he has travelled 999 lightyears according to alice because alice has helpfully put up a marker for each lightyear ( according to her ) . how much time elapses on bob 's wristwatch between the event that the 0 lightyear marker flashes by and the event that the 999 lightyear marker flashes by ? according to sr , the elapsed time ( in years ) according to bob is $$\tau = 999\sqrt{1 - ( 0.999 ) ^2} = 44.7$$ but remember , according to bob , it is alice and the lightyear markers that are moving . and , according to bob , the distance ( in lightyears ) between alice 's lightyear markers is $$d = 1\sqrt{1 - ( 0.999 ) ^2} = . 0447 $$ the point is this : we must be careful in our thinking about distance travelled and elapsed time since these quantities are reference frame dependent . now , if we introduce acceleration into the thought experiment , alice and bob are no longer are equivalent . imagine that alice and bob are both on earth , bob instantly accelerates to 0.999 c relative to and away from earth , travels for 44.7 years according to his wristwatch , and then instantly decelerates to zero speed relative to earth . bob and alice will both agree that he is now 999 lightyears from earth , that bob has aged 44.7 years and alice has aged 1000 years .
the average/expected value of a product is in general not the same as the product of expected values . ( the " mean value " function is linear though : a sum of mean values is equal to the mean value of the sum . ) the product of the sum and sum of product are related by the covariance : $cov ( x , y ) = &lt ; xy&gt ; - &lt ; x&gt ; &lt ; y&gt ; $ , as you stated yourself . i hope this helps . source : http://en.wikipedia.org/wiki/expected_value#non-multiplicativity
heat consists of random vibrations in a material . if a magnetic field connects two objects , then it creates a mechanical coupling between the two objects . such a coupling will couple vibrations , therefore heat will be conducted by the magnetic field . in practice , the effect will be very small , but given enough time , heat will be conducted by the magnetic field . the same applies to suspension by a combination electric and magnetic field . this is not entirely incompatible with some of the other ( wrong ) answers . for example , since the suspended object is not at absolute zero , it is not possible for the magnetic field suspending it to be entirely static . as an example , let 's consider a superconducting metal container of gas suspended in a magnetic field : now suppose that the gas is at some non-zero temperature . then there is a non-zero probability that the gas will concentrate onto one side : in order for this to happen , assuming that the center of mass of the container remained constant , the metal part of the container had to have shifted in the opposite direction : assuming that the levitation is stable , a motion of the superconducting container must be opposed by the magnetic field . thus the above motion is opposed by a force from the magnetic field . however , for every action there is an opposite and equal reaction ( newton 's 3rd law ) , therefore there will be a corresponding opposing force on the suspension : therefore thermal motion in the suspended container will induce thermal motion in the base , hence there will be conduction of heat .
note : this is basically item 3 in jkel 's answer . if you move at an appreciable fraction of the speed of light , then your shadow can appear to be " trailing " you , although it will always be " attached " to your feet if you are on flat ground . suppose a person is moving in the direction shown below and that there are plane waves coming in from an angle . the left image is a top view , and the person is moving toward the right of the screen . the right image , where the person is moving into the screen , shows better the angle that the light is making relative to the horizontal . now think about the light rays that hit the person 's head , body , and feet all at the same time , as shown in the diagram below . the dashed lines show the path light would travel if the person were not there . but the light does get blocked . since light has a finite speed , it will take a longer amount of time for the shadow of the person 's head to appear on the ground compared to the shadow from the person 's feet . now all of this is for light that strikes the person at the same time . so far , so good . okay , by similar reasoning , the shadows on the ground at any given time are created by light striking the person at different times . that is , light that is missing and creating the shadow of a person 's head must have hit the head earlier than light that is missing and creating the shadow of the person 's feet . in other words , at any given instant in time , the shadow of the person 's head is behind where you might expect , since that missing light struck the person 's head at an earlier time . if you put it all together , you get something like this at any instant in time : the shadow will come out from the feet , but the head will be " behind " where you might expect it to be if the person was stationary .
you are almost there . i am assuming your square well $v_0 ( r ) $ is nonzero and negative only on some interval $-r_0 &lt ; r &lt ; +r_0$ about the origin . in that case , for positive $a , b$ you already have that the isosinglet well is deeper than the isotriplet well . remember that the finite square well has a finite number of bound states , each with energy $-|v_0| &lt ; e &lt ; 0$ . find the width and depth of a well with a single bound state ( for fun , with the correct binding energy , 2.2 mev ) . next find the minimum $b$ so that a same-radius well , shallower by $\frac{3}{4}v_0b$ , has zero bound states . tada : a bound isosinglet with no excited states , and an unbound isotriplet .
yes -- because refraction influences the apparent distance to the horizon , it also has an effect on the curvature . to visualize this , it might help to think in extreme cases , for example in the case where due to refraction the horizon is at an apparent distance of only 1 meter . in this case , the curvature of the horizon would be extreme ( it would be a circle of radius 1 meter around you ) . in reality the curvature effect is much smaller of course , and i doubt that it is visible . ( as an aside : one result of atmospheric refraction that is observable is a phenomena called ' the green flash ' . because refraction is colour-dependent , the red and yellow part of the sun could already have set , while a small part of the ' green sun ' is still above the horizon . this can be observed by the naked eye , preferably when the horizon is sharp . )
there are two thermodynamic aspects to laser cooling that are worth mentioning . the first , as others have noted , has to do with the frequency of the light that is absorbed and emitted . in doppler cooling , the laser is tuned slightly below the frequency that the laser wants to absorb . an atom moving toward the laser sees that light shifted slightly up in frequency , and is thus more likely to absorb it than an atom at rest or moving away from the laser is . when it absorbs the light , it also picks up the momentum associated with that photon , which is directed opposite the momentum of the atom ( since the atom is headed in the opposite direction from the light ) , and thus slows down . when the atom spontaneously emits a photon a short time later , dropping back to the ground state , the emitted photon has exactly the resonant frequency of the atomic transition ( in the atom frame ) . that means that its frequency is a little higher ( in the lab frame ) than the frequency of the absorbed light . higher frequency means higher energy , so the photon has absorbed a low-energy photon and emitted a high-energy photon . the difference in energy between the two has to come from somewhere , and it comes out of the kinetic energy of the moving atom . ( you might reasonably ask what happens with the momentum of the emitted photon ; the emission process also gives the atom a kick in the direction opposite the emitted photon 's direction . if this happens to be exactly the same as the direction of the laser , the resulting kick sends the atom back to the same initial velocity ; however , the direction of spontaneous emission is random , and as a result , it tends to average out over many repeated absorption-emission cycles . on average , the atom loses one photon 's worth of momentum with each cycle , and a corresponding amount of kinetic energy . ) the other aspect , which you did not ask about , but is worth mentioning , is the entropy . on the surface , it might seem that even though energy is conserved , there would be a thermodynamics problem here because the system moves from a higher entropy ( lots of fast-moving atoms ) to a lower entropy ( lots of slow-moving atoms ) . the difference is made up in the entropy of the light field . initially , you have a single monochromatic beam of photons , all with the same frequency , all headed in the same direction , which is about as low-entropy as it gets . after the cooling , those photons have been scattered out into all different directions , and with a range of frequencies , so the entropy of the light is much greater , and more than makes up for the decrease in the entropy of the atoms . that is probably way more detail than you wanted/needed , but there you go .
what you have worked out so far is the left hand side of the answer . you have written that the sum of the accelerations is zero , but that is not correct because the system is being driven by the roller moving across the sinusoidal surface . parameterize in terms of time by writing $x=ut$ , then rewrite your $y_0 ( x ) $ equation as $y_0 ( t ) $ . now that you have the vertical position of the roller as a function of time , you can find its acceleration by taking the derivative twice with respect to time . the result should look comforting . this new acceleration term goes with the force that drives the system .
your calculations are correct . the reason we do not feel it is because the 1 atmosphere of pressure will be applied to all surfaces of our body , including the soles of our feet . in fact the interior of our body is also pushing out against our skin with 1 atmosphere of pressure so there is no net force being applied in either direction at the skin 's surface . the only way you could feel this pressure would be if part of your body was in contact with a vacuum while the rest of your body remained in air at 1 atmosphere of pressure . imagine that you were standing on an opening to a vacuum chamber that was almost , but slightly less than the size of the soles of your feet . if there was a perfectly air tight fit between the vacuum opening and your feet , you would indeed feel that pressure as that much force on your feet just as you calculated . as a simpler experiment , just put your hand over the hose of a vacuum cleaner - that is only a very partial vacuum but the force you need to exert to remove your hand is due to the 1 atmosphere of pressure around us all pushing your hand onto the partial vacuum . in fact if you measured that force needed to pull your hand off of the vacuum cleaner hose , you could compute what the air pressure in the vacuum hose was .
the partial pressure of the water in the solution does , indeed , decrease . the total pressure of water plus solute , i.e. the pressure of the solution as a whole , stays the same . is this what you are asking ? how can we see this ? pressure is force per area . since we did not change the area , and because forces from different atoms/molecules in the solution are additive , the partial pressures are additive . this is called dalton 's law . strictly speaking , this is only true , if there are no internal forces between the components of a mixture , so it holds reasonably well for many gas mixtures at low pressure and high enough temperature , but it does not hold for concentrated ionic solutions , for which we also have to calculate strong interactions between the solvent atoms/molecules and the solute ions .
i ) the closest cosmetic resemblance between the nambu-goto action and the polyakov action is achieved if we write them as $$\tag{1} s_{ng}~=~ -\frac{t_0}{c} \int d^2{\rm vol} ~\det ( m ) ^{\frac{1}{2}} , $$ and $$\tag{2} s_{p}~=~ -\frac{t_0}{c}\int d^2{\rm vol}~ \frac{{\rm tr} ( m ) }{2} , $$ respectively . here $h_{ab}$ is an auxiliary world-sheet ( ws ) metric of lorentzian signature $ ( - , + ) $ , i.e. minus in the temporal ws direction ; $$\tag{3} d^2{\rm vol}~:=~\sqrt{-h}~d\tau \wedge d\sigma$$ is a diffeomorphism-invariant ws volume-form ( an area actually ) ; $$\tag{4} m^{a}{}_{c}~:=~ ( h^{-1} ) ^{ab}\gamma_{bc} $$ is a mixed tensor ; and $$\tag{5} \gamma_{ab}~:=~ ( x^{\ast}g ) _{ab}~:=~\partial_a x^{\mu} ~\partial_b x^{\nu}~ g_{\mu\nu} ( x ) $$ is the induced ws metric via pull-back of the target space ( ts ) metric $g_{\mu\nu}$ with lorentzian signature $ ( - , + , \ldots , + ) $ . note that the nambu-goto action ( 1 ) does actually not depend on the auxiliary ws metric $h_{ab}$ at all , while the polyakov action ( 2 ) does . ii ) as is well-known , varying the polyakov action ( 2 ) wrt . the ws metric $h_{ab}$ leads to that the $2\times 2$ matrix $$\tag{6} m^{a}{}_{b}~\approx~\frac{{\rm tr} ( m ) }{2} \delta^a_b~\propto~\delta^a_b $$ must be proportional to the $2\times 2$ unit matrix on-shell . this implies that $$\tag{7} \det ( m ) ^{\frac{1}{2}} ~\approx~ \frac{{\rm tr} ( m ) }{2} , $$ so that the two actions ( 1 ) and ( 2 ) coincide on-shell , see e.g. the wikipedia page . ( here the $\approx$ symbol means equality modulo eom . ) iii ) now , let us imagine that we only know the nambu-goto action ( 1 ) and not the polyakov action ( 2 ) . the the only diffeomorphism-invariant combinations of the matrix $m^{a}{}_{b}$ are the determinant $\det ( m ) $ , the trace ${\rm tr} ( m ) $ , and functions thereof . if furthermore the ts metric $g_{\mu\nu}$ is dimensionful , and we demand that the action is linear in that dimension , this leads us to consider action terms of the form $$\tag{8} s~=~ -\frac{t_0}{c}\int d^2{\rm vol}~ \det ( m ) ^{\frac{p}{2}} \left ( \frac{{\rm tr} ( m ) }{2}\right ) ^{1-p} , $$ where $p\in \mathbb{r}$ is a real power . alternatively , weyl invariance leads us to consider the action ( 8 ) . obviously , the polyakov action ( 2 ) ( corresponding to $p=0$ ) is not far away if we would like simple integer powers in our action .
there is a fair amount of background mathematics to this question , so it will be a while before the punch line . in quantum mechanics , we are not working with numbers to represent the state of a system . instead we use vectors . for the purpose of a simple introduction , you can think of a vector as a list of several numbers . therefore , a number itself is a vector if we let the list length be one . if the list length is two , then $ ( . 6 , . 8 ) $ is an example vector . the operators are not things like plus , minus , multiply , divide . instead , they are functions ; they take in one vector and put out another vector . multiplication is not an operator , but multiplication by two is . an operator acts on a vector . for example , if the operator " multiply by two " acts on the vector $ ( . 6 , . 8 ) $ , we get $ ( 1.2 , 1.6 ) $ . commutativity is a property of two operators considered together . we cannot say " operator $a$ is non-commutative " , because we are not comparing it to anything . instead , we can say " operator $a$ and operator $b$ do not commute " . this means that the order you apply them matters . for example , let operator $a$ be " switch the two numbers in the list " and operator $b$ be " subtract the first one from the second " . to see whether these operators commute , we take the general vector $ ( a , b ) $ and apply the operators in different orders . as an example of notation , if we apply operator $a$ to $ ( a , b ) $ , we get $ ( b , a ) $ . this can be written $a ( a , b ) = ( b , a ) $ . $$ba ( a , b ) = ( b , a-b ) $$ $$ab ( a , b ) = ( b-a , a ) $$ when we apply the operators in the different orders , we get a different result . hence , they do not commute . the commutator of the operators is defined by $$\textrm{commutator} ( a , b ) = [ a , b ] = ab - ba$$ this is a new operator . its output for a given input vector is defined by taking the input vector , acting on it with $b$ , then acting on the result with $a$ , then going back to the original vector and doing the same in opposite order , then subtracting the second result from the first . if we apply this composite operator ( to wit : the commutator ) to $ ( a , b ) $ , we get ( by subtraction using the two earlier results ) $$ ( ab - ba ) ( a , b ) = ( -a , b ) $$ so the commutator of $a$ and $b$ is the operator that multiplies the first entry by minus one . an eigenvector of an operator is a vector that is unchanged when acted on by that operator , except that the vector may be multiplied by a constant . everything is an eigenvector of the operator " multiply by two " . the eigenvectors of the switch operator $a$ are $\alpha ( 1,1 ) $ and $\beta ( 1 , -1 ) $ , with $\alpha$ and $\beta$ any numbers . for $ ( 1,1 ) $ , switching the entries does nothing , so the vector is unchanged . for $ ( 1 , -1 ) $ , switching the entries multiplies by negative one . on the other hand if we switch the entries in $ ( . 6 , . 8 ) $ to get $ ( . 8 , . 6 ) $ , the new vector and the old one are not multiples of each other , so this is not an eigenvector . the number that the eigenvector is multiplied by when acted on by the operator is called its eigenvalue . the eigenvalue of $ ( 1 , -1 ) $ is $-1$ , at least when we are talking about the switching operator . in quantum mechanics , there is uncertainty for a state that is not an eigenvector , and certainty for a state that is an eigenvector . the eigenvalue is the result of the physical measurement of the operator . for example , if the energy operator acts on a state ( vector ) with no uncertainty in the energy , we must find that that state is an eigenvector , and that its eigenvalue is the energy of the state . on the other hand , if we make an energy measurement when the system is not in an eigenvector state , we could get different possible results , and it is impossible to predict which one it will be . we will get an eigenvalue , but it is the eigenvalue of some other state , since our state is not an eigenvector and does not even have an eigenvalue . which eigenvalue we get is up to chance , although the probabilities can be calculated . the uncertainty principle states roughly that non-commuting operators cannot both have zero uncertainty at the same time because there cannot be a vector that is an eigenvector of both operators . ( actually , we will see in a moment that is is not precisely correct , but it gets the gist of it . really , operators whose commutators have a zero-dimensional null space cannot have a simultaneous eigenvector . ) the only eigenvector of the subtraction operator $b$ is $\gamma ( 0,1 ) $ . meanwhile , the only eigenvectors of the switch operator $a$ are $\alpha ( 1,1 ) $ and $\beta ( 1 , -1 ) $ . there are no vectors that are eigenvectors of both $a$ and $b$ at the same time ( except the trivial $ ( 0,0 ) $ ) , so if $a$ and $b$ represented physical observables , we could not be certain of them both $a$ and $b$ at the same time . ( $a$ and $b$ are not actually physical observables in qm , i just chose them as simple examples . ) we would like to see that this works in general - any time two operators do not commute ( with certain restrictions ) , they do not have any simultaneous eigenvectors . we can prove it by contradiction . suppose $ ( a , b ) $ is an eigenvector of $a$ and $b$ . then $a ( a , b ) = \lambda_a ( a , b ) $ , with $\lambda_a$ the eigenvalue . a similar equation holds for $b$ . $$ab ( a , b ) = \lambda_a\lambda_b ( a , b ) $$ $$ba ( a , b ) = \lambda_b\lambda_a ( a , b ) $$ because $\lambda_a$ and $\lambda_b$ are just numbers being multiplied , they commute , and the two values are the same . thus $$ ( ab-ba ) ( a , b ) = ( 0,0 ) $$ so the commutator of $a$ and $b$ gives zero when it acts on their simultaneous eigenvector . many commutators can not give zero when they act on a non-zero vector , though . ( this is what it means to have a zero-dimensional null space , mentioned earlier . ) for example , our switch and subtract operators had a commutator that simply multiplied the first number by minus one . such a commutator can not give zero when it acts on anything that is not zero already , so our example $a$ and $b$ can not have a simultaneous eigenvector , so they can not be certain at the same time , so there is an " uncertainty principle " for them . if the commutator had been the zero operator , which turns everything into zero , then there is no problem . $ ( a , b ) $ can be whatever it wants and still satisfy the above equation . if the commutator had been something that turns some vectors into the zero vector , those vectors would be candidates for zero-uncertainty states , but i can not think of any examples of this situation in real physics . in quantum mechanics , the most famous example of the uncertainty principle is for the position and momentum operators . their commutator is the identity - the operator that does nothing to states . ( actually it is the identity times $i \hbar$ . ) this clearly can not turn anything into zero , so position and momentum cannot both be certain at the same time . however , since their commutator multiplies by $\hbar$ , a very small number compared to everyday things , the commutator can be considered to be almost zero for large , energetic objects . therefore position and momentum can both be very nearly certain for everyday things . on the other hand , the angular momentum and energy operators commute , so it is possible for both of these to be certain . the most mathematically accessible non-commuting operators are the spin operators , represented by the pauli spin matrices . these deal with vectors with only two entries . they are slightly more complicated than the $a$ and $b$ operators i described , but they do not require a complete course in the mathematics of quantum mechanics to explore . in fact , the uncertainty principle says more than i have written here - i left parts out for simplicity . the uncertainty of a state can be quantified via the standard deviation of the probability distribution for various eigenvalues . the full uncertainty principle is usually stated $$\delta a \delta b \geq \frac{1}{2}\mid \langle [ a , b ] \rangle \mid$$ where $\delta a$ is the uncertainty in the result of a measurement in the observable associated with the operator $a$ and the brackets indicate finding an expectation value . if you would like some details on this , i wrote some notes a while ago that you can access here .
i think the problem here is that you are being vague about the limits special relativity impose . let 's get this clarified by being a bit more precise . the velocity of any particle is of course limited by the speed of light c . however , the theory of special relativity does not imply any limit on energy . in fact , as energy of a massive particle tends towards infinity , its velocity tends toward the speed of light . specifically , $$e = \text{rest mass energy} + \text{kinetic energy} = \gamma mc^2$$ where $\gamma = 1/\sqrt{1- ( u/c ) ^2}$ . clearly , for any energy and thus any gamma , $u$ is still bounded from above by $c$ . we know that microscopic ( internal ) energy relates to macroscopic temperature by a constant factor ( on the order of the boltzmann constant ) , hence temperature of particles , like energy , has no real limit .
you are correct : everything is in motion ( or not ) based on the reference frame . motion is a relative concept , so you are never " moving " but only " moving with respect to something " . find a good basic primer here : http://en.wikipedia.org/wiki/principle_of_relativity
as you point out , the minkowski metric $\eta = \mathrm{diag} ( -1 , +1 , \dots , +1 ) $ in $d+1$ dimensions possesses a global lorentz symmetry . a highbrow way of saying this is that the ( global ) isometry group of the metric is the lorentz group . well , translations are also isometries of minkowski , so the full isometry group is the poincare group . the general notion of isometry that applies to arbitrary spacetimes is defined as follows . let $ ( m , g ) $ be a semi-riemannian manifold , then any diffeomorphism $f:m\to m$ ( coordinate transformation essentially ) that leaves the metric invariant is called an isometry of this manifold . a closely related notion that is often useful in relation to isometries is that of killing vectors . intuitively a killing vector of a metric generates an " infinitesimal " isometry of a given metric . intuitively this means that they change very little under the action of the transformations generated by the killing vectors . isometries and killing vectors are a big reason for which group theory is relevant in gr . killing vectors often satisfy vector field commutator relations that form a lie algebra of some lie group . addendum ( may 28 , 2013 ) . remarks on symmetric spaces and physics . one can show that in $d$ dimensions , a metric can posses at most $d ( d+1 ) /2$ independent killing vectors . any metric that has this maximum number of killing vectors is said to be maximally symmetric . example . consider $4$-dimensional minkowski space $\mathbb r^{3,1}$ . the isometry group of this space , the poincare group , has dimension $10$ since there are $4$ translations , $3$ rotations , and $3$ boosts . on the other hand , in this case we have $d=4$ so that the maximum number of independent killing vectors is $4 ( 4+1 ) /2 = 10$ . it turns out , in fact , that each rotation , translation , and boost gives rise to an independent killing vector field , so that minkowski is maximally symmetric . one can in fact show that there are ( up to isometry ) precisely three distinct maximally symmetric spacetimes : $\mathrm{ads}_{d+1} , \mathbb r^{d , 1} , \mathrm{ds}_{d+1}$ called anti de-sitter space , minkowski space , and de-sitter space respectively , and that these spacetimes all have constant negative , zero and positive curvature respectively . the isometry groups of these spacetimes are well-studied , and these spacetimes form the backbone of a lot of physics . in particular , the whole edifice of $\mathrm{ads}/\mathrm{cft}$ relies on the fact that $\mathrm{ads}$ has a special isometry group that is related to the conformal group of minkowski space .
frank white 's viscous fluid flow book contains a good list of these " exact " solutions . i am not sure if it is complete though . i have provided links to a few of the solutions . steady flow between a fixed and moving plate axially moving concentric cylinders flow between rotating concentric cylinders hagan-poiseuille flow combined couette-poiseuille flow between plates noncircular ducts -- fully developed flow starting flow in a circular pipe pipe flow due to an oscillating pressure gradient suddenly accelerating plate oscillating plate/oscillating freestream steady couette flow where the moving wall suddenly stops unsteady couette flow between a fixed and an oscillating plate radial outflow from a porous cylinder radial outflow between two circular plates combined poiseuille and couette flow in a tube or annulus gravity-driven thin fluid films decay of a line oseen-lamb vortex the taylor vortex profile uniform suction on a plane flow between plates with bottom injection and top suction start up of wind driven surface water the ekman spiral plane stagnation flow axisymmetric stagnation flow flow near an infinite rotating disk jeffrey-hamel flow in a wedge-shaped region stokes ' solution for an immersed sphere -- creeping flow creeping flow past a fluid sphere blasius boundary layer falkner-skan-cooke boundary layer compressible self-similar boundary layer free-shear flows plane laminar wake -- far field plane laminar jet flat-plate with uniform wall-suction
as we cannot resolve arbitrarily small time intervals , what is ''really'' the case cannot be decided . but in classical and quantum mechanics ( i.e. . , in most of physics ) , time is treated as continuous . physics would become very awkward if expressed in terms of a discrete time . edit : if time appear discrete ( or continuous ) at some level , it could still be continuous ( or discrete ) at higher resolution . this is due to general reasons that have nothing to do with time per se . i explain it by analogy : for example , line spectra look discrete , but upon higher resolution one sees that they have a line width with a physical meaning . thus one cannot definitely resolve the question with finitely many observations of finite accuracy , no matter how contrived the experiment .
if you look at the graphs for the sine and cosine functions , and know about the relation between the two : $\sin ( x ) = \cos{\left ( \frac{\pi}{2}-x\right ) }$ you should be able to understand what happened . the expressions are not completely equivalent , but both are solutions to the wave equation .
i guess lubos motl 's comment really refers to the terminology used in my post . if i try to insist on what i meant by " fermionic string " , the string formed by $s=s_{rns}-s_p$ , the massless free dirac action $s=\iint\limits_{s} i\hbar\gamma^\mu\partial_\mu\psi \mbox{ d}^2\xi$ , then i guess it would simply mean that the theory is inconsistent . the only way i can see that this is so , is that the " fermionic string " again is an inconsistent string theory . i think i get why this is so . if there are no fields $x^\mu$ in the action , then the string worldsheet can not get embedded into spacetime at all ( ! ) . this theory would then not exist . so the answer boils down to " the ( purely ) fermionic string is not studied because it is not even a consistent theory , since the string worldsheet would not be embedded into spacetime . "
the figure should explain the notions ( except for $t$ , but never mind on it ) . $m$ is your mass , $m$ is the mass of the block . $$t \cos \alpha = \frac{1}{2} m g$$ $$t h \sin \alpha = m g b$$ thus the answer for the minimum mass required for the concrete block is $$m = \frac{1}{2} m \frac{h}{b} \tan ( \alpha ) $$ but this will not do since it the low boundary , so multiply it by 5-10 and bury the bucket . to community : maybe we have a policy on such questions ?
i have experienced something similar . i am not 100% sure if it is the same phenomenon you are describing , but i suspect so . it happens if you use coffee that is ground too finely , and does not have to do with boiling . what happens is that the fine coffee grains block all the holes in the mesh . this means that the water is under more pressure than usual , since it can no longer pass through the plunger . because of this the water ends up escaping by forcing a small part of the mesh away from the side of the carafe and squirting out at high velocity . the reason for the high speed is just that it is passing through a small aperture - it is the same effect as when you put your finger over a hose . to prevent this from happening , you could try a coarser grind , or if you already use coarse-ground coffee , try pressing even more gently on the plunger . if you meet resistance then try lifting the plunger slightly before continuing - this should redistribute the grounds slightly and hopefully unblock the holes in the mesh . from a physics point of view it is worth mentioning that boiling would be a very unlikely response to compressing hot coffee . it is possible for a liquid to be in a " superheated " state , where it is above its boiling point yet remains liquid . when water is in such a state it can indeed boil very suddenly . but this state can only be reached if there are no nucleation sites available to allow steam bubbles to form , and the coffee grounds would probably provide excellent nucleation sites , so if the water were superheated it would boil as soon as you poured it onto the grounds . ( this would probably produce very bitter coffee . ) liquids can also suddenly boil if their boiling point decreases below their temperature - but pressing the plunger increases the pressure , which increases rather than decreases the boiling point . this is true for all liquids ( by le chatelier 's principle ) , so we would never expect boiling to result from an increase in pressure .
as far as where you put things like the $2 \pi$ and the $\hbar$ in the fourier transform or inverse fourier transform , it does not really matter . what really matters is that the operations are the inverse of each other . for example : $\phi ( p ) = \frac{1}{\sqrt{2 \pi \hbar}} \int^{\infty}_{-\infty} dx \hspace{2mm} \psi ( x ) e^{-i \frac{p}{\hbar} x} $ $ =\frac{1}{2 \pi \hbar} \int^{\infty}_{-\infty}dx \int^{\infty}_{-\infty}dq \hspace{2mm}\phi ( q ) e^{ i \frac{q}{\hbar} x } e^{-i \frac{p}{\hbar} x}$ $=\frac{1}{2 \pi \hbar} \int^{\infty}_{-\infty} dx \int^{\infty}_{-\infty}dq \hspace{2mm}\phi ( q ) e^{ i \frac{ ( q-p ) }{\hbar} x } $ $ =\frac{1}{2 \pi \hbar} \int^{\infty}_{-\infty} dq\hspace{2mm} \phi ( q ) 2 \pi \hbar\delta ( q - p ) $ $= \int^{\infty}_{-\infty} dq \hspace{2mm}\phi ( q ) \delta ( q - p ) $ $ = \phi ( p ) $ this is what matters . now if you really want to `derive ' the second set of relations from the first , simply set $p\rightarrow \frac{p}{\hbar}$ and $a ( k ) \rightarrow a ( k ) \sqrt{\hbar}$ . this gives $f ( x ) = \frac{1}{\sqrt{2 \pi}} \int^{\infty}_{-\infty}a ( k ) \sqrt{\hbar} e^{i\frac{k}{\hbar} x} \frac{dk}{\hbar} =\frac{1}{\sqrt{2 \pi \hbar}} \int^{\infty}_{-\infty}a ( k ) e^{i\frac{k}{\hbar} x} dk $ and $a ( k ) \sqrt{\hbar} = \frac{1}{\sqrt{2 \pi}} \int^{\infty}_{-\infty}f ( x ) e^{i \frac{k }{\hbar}x} dx \rightarrow a ( k ) = \frac{1}{\sqrt{2 \pi\hbar}} \int^{\infty}_{-\infty}f ( x ) e^{i \frac{k }{\hbar}x} dx$
imagine a rock on a rope . as you rotate the rope faster and faster , you need to pull stronger and stronger to provide centripetal force that keeps the stone on the orbit . the increasing tension in the rope would eventually break the it . the very same thing would happen with bar ( just replace the rock with the bar 's center of mass ) . and naturally , all of this would happen at speeds far below the speed of light . even if you imagined that there exists a material that could sustain the tension at relativistic speeds you had need to take into account that signal can not travel faster than at the speed of light . this means that the bar can not be rigid . it would bend and the far end would trail around . so it is hard to even talk about rotation at these speeds . one thing that is certain is that strange things would happen . but to describe this fully you had need a relativistic model of solid matter . people often propose arguments similar to yours to show special relativity fails . in reality what fails is our intuition about materials , which is completely classical .
the answer to all questions is no . in fact , even the right reaction to the first sentence - that the planck scale is a " discrete measure " - is no . the planck length is a particular value of distance which is as important as $2\pi$ times the distance or any other multiple . the fact that we can speak about the planck scale does not mean that the distance becomes discrete in any way . we may also talk about the radius of the earth which does not mean that all distances have to be its multiples . in quantum gravity , geometry with the usual rules does not work if the ( proper ) distances are thought of as being shorter than the planck scale . but this invalidity of classical geometry does not mean that anything about the geometry has to become discrete ( although it is a favorite meme promoted by popular books ) . there are lots of other effects that make the sharp , point-based geometry we know invalid - and indeed , we know that in the real world , the geometry collapses near the planck scale because of other reasons than discreteness . quantum mechanics got its name because according to its rules , some quantities such as energy of bound states or the angular momentum can only take " quantized " or discrete values ( eigenvalues ) . but despite the name , that does not mean that all observables in quantum mechanics have to possess a discrete spectrum . do positions or distances possess a discrete spectrum ? the proposition that distances or durations become discrete near the planck scale is a scientific hypothesis and it is one that may be - and , in fact , has been - experimentally falsified . for example , these discrete theories inevitably predict that the time needed for photons to get from very distant places of the universe to the earth will measurably depend on the photons ' energy . the fermi satellite has showed that the delay is zero within dozens of milliseconds http://motls.blogspot.com/2009/08/fermi-kills-all-lorentz-violating.html which proves that the violations of the lorentz symmetry ( special relativity ) of the magnitude that one would inevitably get from the violations of the continuity of spacetime have to be much smaller than what a generic discrete theory predicts . in fact , the argument used by the fermi satellite only employs the most straightforward way to impose upper bounds on the lorentz violation . using the so-called birefringence , http://arxiv.org/abs/1102.2784 one may improve the bounds by 14 orders of magnitude ! this safely kills any imaginable theory that violates the lorentz symmetry - or even continuity of the spacetime - at the planck scale . in some sense , the birefringence method applied to gamma ray bursts allows one to " see " the continuity of spacetime at distances that are 14 orders of magnitude shorter than the planck length . it does not mean that all physics at those " distances " works just like in large flat space . it does not . but it surely does mean that some physics - such as the existence of photons with arbitrarily short wavelengths - has to work just like it does at long distances . and it safely rules out all hypotheses that the spacetime may be built out of discrete , lego-like or any qualitatively similar building blocks .
if you consider a typical metal the highest energy band ( i.e. . the conduction band ) is partially filled . the conduction band is effectively continuous , so thermal energy can excite electrons within this band leaving holes lower in the band . at absolute zero there is no thermal energy , so electrons fill the band starting from the bottom and there is a sharp cutoff at the highest occupied energy level . this energy defines the fermi energy . at finite temperatures there is no sharply defined most energetic electron because thermal energy is continuously exciting electrons within the band . the best you can do is define the energy level with a 50% probability of occupation , and this is the fermi level .
the answer is a definite maybe ! ignoring air resistance the velocity a falling object hits the ground can be calculated using the appropriate suvat equation or by equating potential energy lost with kinetic energy gained . however this only tells you the speed the object hits the ground , and how hard it hits the ground depends on how fast it decelerates . if you can measure the force as a function of time during the collision you can calculate the total impulse , and since this is equal to the momentum change you can calculate the original momentum and hence the original velocity . just calculating the peak force does not help because it could be a hard slow moving object decelerating suddenly or a soft fast moving object decelerating slowly . i have not gone into specifics , since this looks rather like a homework question , but this should direct you to the physics you need to answer your problem .
in this question i showed that the power received from a light source moving away from the receiver at speed $\beta=v/c$ is $$ p = \frac{p_0}{\gamma^2 ( 1+\beta ) ^2} = p_0 \frac{1-\beta}{1+\beta} $$ taking into account redshift , time dilation , and the effect of the changing travel distance for the photons . your question is essentially the same , except that you have the source and receiver approaching ; in that case the sign of $\beta$ changes and you get a small increase in the received power .
let $y_1 , y_2$ $2$ complex vectors and let $&lt ; , &gt ; $ be a complex inner product defined by $&lt ; y_1 , y_2&gt ; = \vec y_1^* . \vec y_2$ . let $\vec a$ and $\vec b$ the real and imaginary part of $\vec y$ : $\vec y = \vec a + i \vec b$ then : $$&lt ; y_1 , y_2&gt ; = ( \vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 ) + i ( \vec a_1 . \vec b_2 - \vec b_1 . \vec a_2 ) = u ( y_1 , y_2 ) + iv ( y_1 , y_2 ) $$ the cauchy-schwartz inequality gives : $$&lt ; y_1 , y_1&gt ; &lt ; y_2 , y_2&gt ; ~~\ge ~~|&lt ; y_1 , y_2&gt ; |^2$$ we note that : $&lt ; y_1 , y_1&gt ; = u ( y_1 , y_1 ) $ , so we have : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{u^2 ( y_1 , y_2 ) + v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }$$ now , fixing a particular $y_1$ , we limit the set of $y_2$ to those which respect $u ( y_1 , y_2 ) =0$ . so , we have now : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ now , take explicitely $y_2$ defined by $ \vec a_2 = - \vec b_1 , \vec b_2 =\vec a_1 , $ , we see that $\vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 = 0$ , that is $u ( y_1 , y_2 ) = 0$ , so this choice is coherent with our previous hyphothesis . morevoer , we have $v ( y_1 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , and $u ( y_2 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , so we have , for this particular $y_2$ . $$u ( y_1 , y_1 ) ~~= ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ so , we see , that the inequality $ ( 1 ) $ is effectively saturated by our choice of this particular $y_2$
if you assume a radiator ( sun , lightbulb ) of constant power ( e . g . 60 w ) , you can increase its surface in any way you like ( assuming the material only diffracts the radiation and does not absorb it ) , but it will always emit the same power , which is ( e . g . ) 60w . this is , simply , conservation of energy . it is not a matter of temperature . if you are , on the other hand , talking about black body radiation , wikipedia states that the emitted radiation is proportional to the surface . double the surface , and you will double the radiation .
the statement that bosonic strings came first and fermionic strings came later is not exactly correct as history . fermionic strings came almost simultaneously , when ramond discovered the two dimensional super-conformal algebra in 1971 . ramond style string theories did not have space-time supersymmetry ( or rather , they did , but the gso projection which was required to extract the physical spectrum was not discovered until 1976 , and the proof that this projection actually leaves a sensible theory did not come until the green-schwarz formulation was developed in the early 1980s ) . the neveu schwarz paper analyzes bosonic oscillations of a fermionic string , and was motivated by exploring all consistent bootstraps to find something that would work for the light mesons . the problems at the time was that a bosonic tachyon was interpreted as the experimentally known instability of the pion vacuum , so it was considered essential for a good theory . the neveu-schwarz sector , without the gso projection , contains such a tachyon . now we know that this means that the theory is sick , but back then , it was considered a good sign . the ramond fermions were then interpreted as bare baryons , to be dressed with the pion condensation , and this interpretation is also wrong , since baryons have a three-quark symmetry structure . the neveu-schwarz sector was interpreted as mesons , but they also had a tachyon ( which is gso odd and vanishes ) , and nothing looks like the qcd spectrum , not with the crude tools available then . the inconsistency of ramond-neveu-schwarz strings was expressed most simply by edward witten in the early eighties : the closed string sector of a fermionic string contains massless spin-3/2 particles , so it must couple to some space-time supercurrent in order to make sense . the graviton and spin 3/2 gravitinos must make a sensible supergravity theory . the development of supergravity was initiated to a large degree by string theory , since scherk immediately began to investigate supergravities after gso . he probably understood even then that the low-energy limit of superstrings would have to be some sort of supergravity . so it is most fair to say that the development of superstrings and of bosonic strings went hand in hand , but the full perturbation series for the bosonic string was completed earlier , while a full perturbation theory of the fermionic string had to wait until the early eighties .
when you snap your fingers there are multiple sound waves , but the speed of sound is so fast you can not distinguish individual waves . the frequency of sound waves is around 100hz to 10khz so each wave completes one oscillation in 0.01 to 0.0001 seconds . what you are hearing when you snap your fingers is the envelope i.e. the overall amplitude of the sound waves . when you throw a stone into still water you get an expanding ring of waves moving out , so at the centre it is still then as you move outwards you pass through the waves and beyond them the water is still again . hearing the finger snap is the same as being struck by the expanding ring of ripples .
fermionic spin 3/2 fields , much like bosonic fields with spin 1 and higher , contain negative-norm polarizations . roughly speaking , a spin 3/2 field is $r_{\mu a}$ where $\mu$ is a vector index and $a$ is a spinor index . if $\mu$ is chosen to be 0 , the timelike direction , one gets components of the spintensor $r$ that creates negative-norm excitations . this is not allowed to be a part of the physical spectrum because probabilities can not be negative . it follows that there must be a gauge symmetry that removes the $r_{0a}$ components - a spinor of them . the generator of this symmetry clearly has to transform as a spinor , too . there must be a spinor worth of gauge symmetry generators . the generators are fermionic because the original field $r$ is also fermionic , by the spin-statistics relation . it follows that the conserved spinor generators are local supersymmetry generators and their anticommutator inevitably includes a vector-like bosonic symmetry which has to be the energy-momentum density . this completes the proof that in any consistent theory , spin 3/2 fields have to be gravitinos . the number of " minimal spinors " - the size of the gravitinos - has to be equal to the number of supercharges spinors which counts how much the local supersymmetry algebra is extended . in particular , it can not be linked to another quantity such as the dimension of a yang-mills group . so while both 3/2 and 1/2 differ by 1/2 from $j=1$ , more detailed physical considerations show that it is inevitable for the superpartner of a gauge boson , a gaugino , to have spin equal to 1/2 and not 3/2 . similarly , one can show that the superpartner of the graviton can not have spin 5/2 because that would require too many conserved spin-3/2 fermionic generators which would make the s-matrix essentially trivial , in analogy with the coleman-mandula theorem . gravitinos can only have spin 3/2 , not 5/2 .
i think it is important to not loose yourself in calculations . the method in your book probably starts from saying that considering an object $\delta q$ that has the following general form : $\delta q = c_vdt + hdv$ say , then it is an exact differential iif $\left ( \frac{\partial c_v}{\partial v}\right ) _t = \left ( \frac{\partial h}{\partial t}\right ) _v$ this is also equivalent to saying that $c_v$ and $h$ can be thought of as partial derivatives of the same state function with respect to $t$ and $v$ respectively . now , from the first law of thermodynamics , we know that $c_v \equiv \left ( \frac{\partial u}{\partial t}\right ) _v$ hence the point is then to figure out if $h$ is the partial derivative of $u$ with respect to $v$ at fixed $t$ and obviously it is not since the latter would be $h-p$ where $p$ is the thermodynamic pressure which is not a null function . that ends the story i believe . . . but please tell me if you do not trust this argument , i can easily be fooled myself by circular arguments .
decibels measure a ratio of power on a logarithmic scale . the decibel measure of a ratio between powers $p_1$ and $p_2$ is given by $10\log_{10} ( p_1/p_2 ) $ . dbm is an expression of power relative to 1 mw . dbw is similarly the ratio between some amount of power and 1 w . remember , db measure a ratio . dbm or dbw actually specify an amount of power . 100mw --> +10 db --> -2 db --> point ' x ' --> -2 db --> -2 db --> point ' y ' 100 mw is +20 dbm , because $10\log_{10} ( \dfrac{100\ \mathrm{mw}}{1\ \mathrm{mw}} ) $ is 20 . 20 + 10 is 30 . 30 - 2 is 28 . 28 - 2 is 26 . 26 - 2 is 24 . so after all these gains and losses you have +24 dbm . 24 dbm is 251.19 mw because $10^{\frac{24}{10}}$ is 251.19 . i was able to come up with an answer of ~251 mw by repeatedly using the power 2 = power 1 x 10^ ( gain or loss / 10 ) formula . it looks like you got the right answer , but i am not sure why you had to repeatedly use a complicated formula . after you convert the input power to dbm , it is just additions and subtractions to work out the output power . then convert back to mw if you want the answer in mw . that is the advantage of working on a log scale : multiplications and divisions turn into additions and subtractions .
there do not exist materials made of antimatter , so even though they would behave completely symmetrically to the corresponding matter materials , the fact is irrelevant . dark matter reacts only with gravity , and x-rays are electromagnetic waves . to all intents and purposes , as far as possibility of measurements , dark matter is transparent to x-rays , since the gravitational interaction of x-rays is miniscule . what is your definition of transparency ? for example flesh is transparent while bones and metal are not for an x-ray photograph . hard x-rays can penetrate some solids and liquids , and all uncompressed gases , and their most common use is to image the inside of objects in diagnostic radiography and crystallography . there will always be some interaction of x-rays through matter , transparency has to be defined .
yes it is certainly possible to construct a universal clock . if you are at rest with respect to the average matter in the universe ( basically this means being at rest with respect to the cosmic microwave background ) , and not in a gravitational field , then you are a comoving observer . every comoving observer will agree on the time since the big bang ( 13.798 $\pm$ 0.037 billion years ) , and they will all agree on the rate that time passes i.e. the length of the second ( defined using your caesium standard ) . so we can use the number of seconds since the big bang as a universal clock that applies everywhere in the universe . this is not a particularly practical clock , since we have little hope of every knowing the age of the universe to an accuracy comparable to one second . still , in principle it could be done . if you get in a spaceship and fly around the universe then your local clock will get out of sync with the universal clocks due to relativistic time dilation , but you can in principle calculate the loss of sync and correct your clock as you go . likewise if you are in a gravitational field your local clock will run slower than the universal clock , but again this can in principle be corrected .
the human eye has three types of colour receptors , called cones , that respond to red , green and blue light . your brain interprets the colour based on the signals from these cones . for example suppose you are looking at red light . only the " red " cones will generate a signal and your brain interprets this as red . suppose now you are looking at a mixture of red and green light . this time the red and green cones generate signals while the blue cones do not , and your brain interprets this as yellow . if there is no light entering the eye none of the cones generate a signal and your eye interprets this as black . some colours are pure , that is they consist of light with only a single wavelength , but most colours are mixtures of light with lots of different wavelengths . there are lots of ways scientists measure colour e.g. the rgb system , but actually this turns out to be a surprisingly complicated thing to do because many measurement schemes can not measure all possible colours . have a look at http://en.wikipedia.org/wiki/colour for more info . finally , to take you example of a red apple : the apple is red because it reflects red light but absorbs other colours . if i shine white light from a torch onto the apple only the red light is reflected and enters my eye , so i see the apple as red . suppose i shine pure blue light ( wavelength about 450nm ) onto the apple , what will i see . well the apple will absorb the blue light so no light enters my eye , and the apple looks black .
you just made some math mistakes . you made a mistake when you did $q = h\int_a kr$ . you got $q = h\pi k r^2$ , but you should have gotten $q = \frac{2}{3} h\pi k r^3$ . notice how this second expression has units of charge while the first one does not . another mistake you make is that you say $\frac{1}{r} \frac{\partial re ( r ) }{\partial r} = \frac{1}{r} ( \frac{\partial e ( r ) }{\partial r} + r\frac{\partial e ( r ) }{\partial r} ) $ . this is not proper application of the product rule . if correct these mistake you will get the right answer unless there are other mistakes i did not find .
they accrete gas from a disk , fed by either a wind or roche lobe overflow from their companion . almost all known millisecond pulsars are in binary systems , but i think some in globular clusters may have been disrupted by three-body encounters , so appear to be isolated .
i think you are missing that the source is actually referring to he observable universe explicitly - just somewhat indirect : it is not obvious as it is talking about " visible/observable universe " , but about " total mass of the visible matter " and " mass density of visible matter " . as far as i can see , any references to mass etc in the universe are covered by this . so it is basically talking about the visible matter in the whole universe ; but that is just the same as the whole matter in the visible universe , and all we care about here is the matter .
all stars in our galaxy are in stable elliptical orbits around the galactic centre . but they are not all moving in the same direction with the same speed . . . meaning there is a random maxwellian distribution of velocities among the stars . what this effectively means is ( like the animation that crazy buddy posted ) although there is a net effective attraction between the stars and the centre of the galaxy , whenever two stars get close to each other , they exert a sort of " gravitational drag " force , which acts like a frictional force and ends in a net slowing down of the interacting bodies . when the star then slows down , it does not have the speed to maintain it is current orbit , so it will move into a nearer orbit closer to the centre of the galaxy . this is the general mechanism of how stars in a galaxy are " collapsing " into the centre . there is no need to worry about falling into any ( hypothesised ) super massive black holes though . any of these processes take a really long time , and by the time we are close enough to the centre of the galaxy to worry about it , our sun will be well into it is red giant stage and the earth would already have been consumed by it . : ) p . s - to clarify my first paragraph , assuming there are no other stars orbiting the centre of the galaxy , then we will never spiral into it . that effect is only caused by the presence of other stars in the galaxy . edit : @johnrennie pointed out that because of conservation of momentum , the lighter stars will tend to gain energy and the heavier ones loose energy in dynamical friction interactions . this will tend to push the heavier stars closer to the centre and the lighter stars further out . his answer to this same question points this out .
you can see this as an example of the relativistic doppler shift ( for equations , see eg : http://en.wikipedia.org/wiki/relativistic_doppler_effect ) . the hands of the clock are moving with some angular frequency and you are moving with a velocity v towards the clock . it follows that the frequency you are seeing will be higher , thus the clocks hands will move faster . conceptually this makes sense . suppose a picture of the clock is emitted each second . since you are moving towards the clock , you will pick up one of those pictures more often than once per second , thus making the clock seem to run faster .
so you were on the right track with integrating over r and over t . here 's how you could do it : the acceleration at any radius , r ( if we assume earth is a point mass ) is : $$a=-{gm\over r^2}$$ the minus sign is because the acceleration is anti-radial . then you can do the following : $$\lim_{\delta t\rightarrow 0}~-{gm\over r^2}\delta t~=~\delta v$$ $$thus$$ $$\delta r~=~\lim_{\delta t\rightarrow0}-{gm\over r^2}\delta t^2$$ $$then$$ $$r^2\delta r~=~-gm\delta t^2$$ i dropped the limit in the last part because it is implied . if we now use $\lim_{\delta t\rightarrow0}\delta t=dt$ and integrate : $$\int_{r_o}^{r_f}r^2dr~=~\iint_0^t-gmd^2t$$ $r_o$ is any initial radius and $r_f$ is any final radius ( although because this derivation assumed a zero initial velocity , if $r_f&gt ; r_o$ it all breaks down ) . and after some razzmatazz algebra : $$r_f ( t ) ~=~\sqrt [ 3 ] {r_o^3-{3\over2}gmt^2}$$ and if you want to check it , type this into wolfram , differentiate it twice , plug in the radius of earth , its mass , and $t=0$ and you will find it says the acceleration is $-9.8{m\over s^2}$
first of all you should not be using the classic ( incompressible ) bernoulli equation for this kind of problem as you are clearly in the compressible flow regime . you should be using the isentropic flow relations because they are more physically accurate ( $\gamma=1.4$ ) : $\frac{p_t}{p}= [ 1+\frac{\gamma -1}{2}m^2 ] ^\frac{\gamma}{\gamma-1}$ ; $\frac{t_t}{t}=1+\frac{\gamma -1}{2}m^2$ ; $\frac{\rho_t}{\rho}= [ 1+\frac{\gamma -1}{2}m^2 ] ^\frac{1}{\gamma-1}$ plus we will employ the ever-useful adiabatic relation ( which is true even for non-isentropic flows ) : $t_t=t+\frac{v^2}{2c_p} \rightarrow \boxed{t=t_t-\frac{v^2}{2c_p}}$ since you already know the engine massflow rate , the rest of the problem is straightforward . applying ideal gas law and the definition of mach number to the basic massflow equation , we have . . . $\dot{m}_1=\rho_1 v_1 a_1= ( \frac{p_1}{rt_1} ) ( m_1\sqrt{\gamma rt_1} ) a_1=p_1a_1m_1\sqrt{\frac{\gamma}{rt_1}}$ this last bit is the key , since if you know $\dot{m}_1$ , $a_1$ , $m_1$ , and $t_1$ you will easily be able to find $p_1$ . and in fact , this really just boils down to finding $t_1$ because you already know the inlet velocity and $m_1=\frac{v_1}{\sqrt{\gamma rt_1}}$ . the key insight here is that the stagnation properties ( aircraft reference frame ) do not change from the freestream ( $\infty$ ) to the inlet lip ( $1$ ) . thus , $t_t=t_\infty+\frac{v_\infty^2}{2c_p}=t_1+\frac{v_1^2}{2c_p}$ , which implies that $\boxed{t_1=t_\infty+\frac{v_\infty^2-v_1^2}{2c_p}}$ , with $c_p=\frac{\gamma r}{\gamma-1}$ since we know the ambient static temperature , aircraft flight speed ( 245m/s ) and the inlet flow velocity ( 215m/s ) we can solve for the inlet static temperature . everything else follows from this information and you should be able to find everything you need for the rest of the problem . because the flow is decelerating into the engine during cruise , we would expect the inlet static temperature to be slightly higher than the freestream ( and it is ) . another way to compute the answer is to again leverage the fact that the stagnation properties do not change as the flow isentropically decelerates into the engine , except that now we will equate the stagnation pressures instead of temperatures : $p_t=p_\infty [ 1+\frac{\gamma -1}{2}m_\infty^2 ] ^\frac{\gamma}{\gamma-1}=p_1 [ 1+\frac{\gamma -1}{2}m_1^2 ] ^\frac{\gamma}{\gamma-1}$ or $\boxed{p_1=p_\infty\left [ \frac{2+ ( \gamma -1 ) m_\infty^2}{2+ ( \gamma -1 ) m_1^2}\right ] ^\frac{\gamma}{\gamma-1}}$ where $m_\infty=\frac{v_\infty}{\sqrt{\gamma rt_\infty}}$ and $m_1=\frac{v_1}{\sqrt{\gamma rt_1}}$ good luck ! ! ! p.s. your diagram is wrong . the two velocities should be pointing in the same direction . the freestream is approaching the aircraft at 245m/s , and the air actually enters the engine at 215m/s ( it slows down ) . both velocities are relative to the aircraft reference frame and pointed in the same direction . also , your engine massflow is wrong because it assumes $\rho_1=\rho_\infty$ , which is incorrect . you need to use the isentropic relations here as well in a way analogous to the method outlined above .
i guess so - i mean , as far as i know , there is no law of physics that strictly prohibits those " exotic " states from being realized . as long as the state exists and can be reached by some path from the " center " of the state space where the likely states are , there should be a nonzero ( not even infinitesimal , really ) probability of accessing it . but for a typical system , that probability is really , really , really small . so small that it is impossible to intuitively comprehend just how unlikely such an event is . the thing is , a lot of people are not used to dealing with even moderately large or small numbers . if you confront them with a probability like $10^{-10^{23}}$ , they often fail to put the smallness of that value in perspective , and instead focus on the fact that it is not strictly equal to zero . from there they may start coming up with all sorts of nonsensical ideas about walking through walls and spontaneous combustion ( the weird kind ) and the like . so physicists usually find it easier to just say the probability is zero - and in fact , for any purpose other than a rigorous mathematical proof , it might as well be . ( sorry about the rant , i know most people are actually relatively sensible about these things , but it bothers me that the crazy ones seem to get all the attention despite being wrong . )
the heat capacity of an einstein solid is given by \begin{equation} c = nk \left ( \frac{\epsilon}{kt}\right ) ^{2} \frac{e^{\epsilon/kt}}{ ( e^{\epsilon/kt}-1 ) ^{2}} , \end{equation} where $n$ is the number of degrees of freedom . so the value of the energy quantum $\epsilon$ , or more precisely the ratio $x\equiv\epsilon/kt$ matters ! the above equation tends to the equipartition value $nk$ as $x \rightarrow 0$ , and the half maximum corresponds the value of $x$ satisfying \begin{equation} x^{2}\frac{e^{x}}{ ( e^{x}-1 ) ^{2}} =\frac{1}{2} . \end{equation} it turns out that the solution is $x_{\rm{half}} = 2.98287\ldots \approx 3$ , and this is the origin of the relation $kt/\epsilon\approx 1/3$ given to you .
when integrating in ( 2d ) polar coordinates you need to use a surface element : $$da = r\ ; dr\ ; d\theta$$ the reason for this can be seen geometrically : the surface element has the same shape as one of the spaces between two red and two blue lines ( a sort of curved rectangle ) . in the infinitesimal limit the area of one such segment is just its length multiplied by its width . the length is easy , it is $dr$ , and is always the same ( notice that the length of a blue segment between the evenly spaced red lines is always the same ) . the width is a bit more subtle . first , you might notice that the " inner width " and " outer width " are different . we do not need to worry about this because in the infinitesimal limit , they approach the same length . but the length of the arc between two evenly spaced blue lines clearly increases as the radial coordinate increases . it should be easy to convince yourself that the length of an arc that spans angle $theta$ is $r\theta$ ( of course $theta$ in radians ) . it follows that the width of the surface element is $r\ ; d\theta$ ( the angle shrinks to an infinitesimal , but the radial coordinate does not - it simply takes the value of the radius wherever we place our surface element ) . so why does not your integral using a line work ? well , this treats surface elements at all distances from the origin as having the same size ( i am speaking very loosely here , since infinitesimals do not really have a size ) . but looking at the diagram , clearly surface elements close to the origin need to be smaller than ones further away , otherwise you end up " overcounting " area near $r=0$ and under-counting area further out . put another way , take a bunch of long thin rectangles of the same size cut out of paper and try to arrange them into an approximate segment of a circle without overlapping them . you should find it is impossible . even as you make them infinitesimally thin this fails . you need to use little wedge shapes pieces .
your situation is unfortunate , but this will become very typical as you get further in school . i will do my best to explain this problem fully : to begin with , in this problem , two interference patterns are formed , each pattern unique to one of the wavelengths provided to you . it is important to understand here that the fringe patterns might overlap , but do $\textbf{not}$ $\textit{interfere}$ with one another ( in terms of wave front interference ) . consequently , it can be concluded that the bright fringes of one wavelength will eventually share locations with the dark fringes of the other wavelength . when this occurs , no fringes will be visible , as there will be no dark bands to differentiate a between a single fringe , and the fringes adjacent to it . in order to make the transition between $\textbf{periods of fringe absence / appearance}$ the mirror 's change in position must produce an $\textbf{integer number}$ of fringe shifts for each wavelength , and the number of shifts for the shorter wavelength must be one more than the number for the longer wavelength . $\textbf{this next bit is imperative for understanding the michelson interferometer:}$ the light travels the length of the apparatus $\textbf{twice}$ , so the change in the position of the mirror must also be accounted for $\textbf{twice}$ . thus , even though a fringe shift possesses a wavelength value of $\lambda$ , we will denote it as $\cfrac{\lambda}{2}$ and the change in position of the mirror $ ( \bigtriangleup x ) $ will become $2 ( \bigtriangleup x ) $ to wrap this problem up , let 's make $\epsilon_n$ the number of fringes produced by a given wavelength . $\epsilon_1$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_a}$ and $\epsilon_2$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_b}$ , therefore $\epsilon_2$ = ( $\epsilon_1 + 1$ ) from here we can see that $\cfrac{2 ( \bigtriangleup x ) }{\lambda_b}$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_a} + 1$ , meaning that $\bigtriangleup x = \cfrac{\lambda_a\lambda_b}{2 ( \lambda_a-\lambda_b ) }$ i will let you plug in your values , you should get a number much less than a meter , but much greater than a nanometer . this was not an easy problem for what i am assuming is an a level physics course . let me know if you have questions . good luck .
this is a common question that applies to chemical reactions , batteries , and other common forms of energy conversion . the short answer is yes , but of course the change is negligible . in a chemical reaction you have a set of reactants and a set of products . if you were to take the mass of the reactants and sum them up , you would find them to be more than the sum of the mass of the products . this mass difference time $c^2$ is the energy of the reaction . again , this is by an amount that is so small that it is unmeasurable . the mass change from of nuclear reactor fuel after burning it at a large heat rate for 4 years is barely measurable itself , if you are talking about putting the fuel on a scale and measuring it . now come the qualifiers . if you put air and wood in a completely closed container and let it burn what would happen to the total mass of the container ? it would stay the same . the temperature would increase , and as the molecules move faster the mass increases ( basic relativity ) . if you preformed a chemical reaction , then cooled the products , you could try to measure a mass difference that is theoretically predicted . but even assuming you have a scale that is precise enough , you will have to look into other things like tidal forces . experimentally measuring these mass changes would be a futile effort .
if i were thinking about it from my own perspective , i would see it as the need to balance energy . we already knew from relativity that mass carried an intrinsic energy with it , and so if i measure something and energy is missing , either i ( 1 ) did not account for some lossy mechanism or ( 2 ) did not measure all the energy coming out . from here , you are stuck with either a particle that carries energy but has no mass ( the photon , which i can measure ) or something that has mass but has kinetic energy . as you observed , this particle would have to carry no charge because of charge conservation but also because charged massive particles are easy to detect if they exist and do not decay rapidly . assuming you are hunting for photons at a known wavelength and did not detect them , the only choice left to you is a massive chargeless particle . energy conservation and charge conservation are the two most fundamental laws of physics , and we have never observed them violated in fundamental processes .
space actually has energy , vacuum energy . it has been shown by various experiments and can be explained by quantum mechanics . so more space means more energy . although it probably can not be used to do work , it does act to increase expansion . check out the wikipedia page for more info . although galaxies are massive , they are far away and thus the resulting acceleration towards each other is weak . if the space between galaxies is expanding at a faster rate than their mutual atraction ; galaxies will move apart . imagine a football player running from one endzone toward the other as fast as he could . however the distance between them endzones doubles every 4 seconds . the endzones are not moving ; the space between them is growing . there is no way the player could hope to reach his goal . in a short time he would not be able to see the other end . the scary thing is this not only goes for the field but everything . the player himself would be ripped apart as his own body parts moved farther and farther apart . if space was expanding fast enough the attractive forces keeping black holes or even atoms together would not be enough . this what is commonly referred to as the big rip . luckily the expansion is slow enough that only " distant " galaxies are moving away from each other . gravitational forces in a solar system or a galaxy are more than enough to withstand expansion . the larger the scale of the system the more expansion has to play . when we talk about expansion we are not talking about objects moving away because of their great velocities against a static backdrop . what you are describing is like marbles on a grid . the marbles get farther away due to their velocities relative to the grid pointing in opposite directions . instead expansion is like the grid growing in scale . this effects the distances between the marbles independently of their velocities ; which is why we observe all far away objects as moving away from us . if it was not for expansion we would expect a more random distribution . it is only at smaller scales where expansion is less of a factor that we can see objects moving toward us such as andromeda 's galaxy . we are not in any sense moving away from the center of the universe . the idea of the big bang and inflation is that not only every thing , matter and energy ; but everywhere was contained in the big bang .
i was wondering if there is an intuitive way to understand the motion of a body influenced by two other massive bodies ( say the moon being influenced by the earth and the sun . no , intuition is not of real use in a three body gravitational problem , more so in many body . in 1887 , mathematicians ernst bruns [ 4 ] and henri poincaré showed that there is no general analytical solution for the three-body problem given by algebraic expressions and integrals . the motion of three bodies is generally non-repeating , except in special cases . [ 5 ] actually the motions of the planets are studied as possible dynamic chaos . planetaria solve the many body problem with numerical approximations .
the wikipedia article that anna mentioned is an excellent description of holography and i am not going to try and compete with it , but since i am guessing that you are not a physicist the following might help make things clearer . when you " see " things , you see them because they enter your eye , get focussed by the lens and hit the retina . in addition you see 3d because the image recorded by your left and right eyes are slightly different , and the brain can reconstruct a 3d image from the differences . so if you are looking at a mouse ( to use the example from the wikipedia article ) it is the light reflected off the mouse that the eye uses to " see " the mouse . suppose you could come up with some clever trick to remove the mouse but still send the light to your eyes as if it had come from the mouse . your brain could not tell the difference because your eyes are still receiving the same light as when the mouse was there . this is what a hologram does . a hologram is a pattern of light and dark areas . when you shine a laser onto a hologram the light and dark areas scatter the light by a process called diffraction . the clever bit is that the light is scattered in exactly the same way as if there were a mouse there , so your brain sees light that looks as if it has come from a mouse , so you see a mouse . it appears in 3d because the hologram scatters light differently depending on the angle you are looking at it , so your left and right eye receive differently scattered light just as they would from a real mouse . you might think it would be tremndously difficult to make a hologram to scatter light in just the right way to make it appear as a mouse , but actually you make a hologram from a real mouse i.e. it is just a type of photograph . it is hard to make multicoloured holograms because to " see " the hologram you have to shine a laser on it , and lasers are just a single colour . you could use three lasers , e.g. a red , green and blue laser , but annoyingly the hologram scatters different coloured light in different ways and your multicoloured hologram would be very blurred . i hope this helps - to get any further you will need to work through the wikipedia article , and also understand what diffraction is .
what we call " laws of physics " have an evolutionary path . it really started with newton and the falling apple and slowly it evolved into complete mathematical models of experimental observations , called theories . from the observations and the theories conservation laws emerged . these laws are strictly obeyed within the framework that they have been validated . take the thermodynamic law " entropy remains the same or increases in closed systems " . the region of validity of the law was transformed when the atomic nature of matter became understood , and statistical mechanics became the underlying framework . there , from an absolute law it became an estimate of probability outcomes , which to all intents and purposes recreates the law for macroscopic systems . if the universe could emerge from nothing , what about physical laws ? as our observations and experiments advance , new mathematical frameworks appear which transmute the laws of the overlying frameworks : conservation of energy became the relativistic four vector energy which blended mass in the mixture . conservation of relativistically defined energy and momentum also became fuzzy instead of absolute due to the heisenberg uncertainty principle of quantum mechanics . so when we come to cosmology where there exist theoretical models of solutions of general relativity it is not surprising that apparent inconsistencies with conservation laws developed for different frameworks . at the moment there does not exist a theory of everything which quantizes gravity and includes the other three forces , weak , strong , electromagnetic that has been validated through all relevant observations , even though string theory offers such possibilities . it is therefore premature to be definitive of how the known conservation laws validated by our laboratory experiments will evolve in a cosmological setting . something and nothing have to be mathematically defined within the appropriate theoretical models .
only insofar that the maximum writing speed is limited by the need to unambiguously encode the disk 's surface . if you tried to raise the speed of writing , the bit error rate increases : theoretically as a continuous , monotonic function of writing speed but practically a point is reached where the ber increases abruptly , so the ber as a function of speed tends to look like a step function . the writing data corresponds physically to changing the optical properties of the disk so this takes a nonzero time , and the more time one spends doing it , the more well formed the symbols are . as the ber increases , the error correction procedures , namely the computing of the nearest valid codeword to a corrupted one in coding space and , more importantly , repeat reads if error correction fails , use up more time , so the read gets markedly slower for a disk with a few errors , then fails altogether . this phenomenon is very like the practical example where it gets harder and harder to read someone 's hadnwriting as the writer gets more and more hurried and thus shapes their letters more and more ambiguously .
first of the sum of the forces on the bar have to be zero because the center of gravity does not move ( assuming symmetry and such ) . the only thing you can calculate is the torque requited to maintain the constant speed . i can give an example with a rectangular bar . take the bar and split it into infinitesimal slices . each slice has face area of ${\rm d}a = h\ , {\rm d}r$ where $r$ is the distance from the center and varies from $-l/2$ to $l/2$ , and $h$ is the height of the bar . the liquid-solid force for each slice is ${\rm d}f =\frac{1}{2} \rho\ ; c_d v^2 {\rm d}a$ where $\rho$ is fluid density , $c_d$ is the coefficient of drag and $v$ is the velocity of the bar slice , which is equal to $ v = \omega\ ; r $ . note that the force has to flip signs when $r$ flips signs and thus we have to add a ${\rm sign} ( r ) $ term and integrate over $r$ to get the total force $$ f = \int_{-l/2}^{l/2}\ ; {\rm sign} ( r ) \frac{1}{2}\rho\ ; h\ ; c_d\ ; ( \omega\ , r ) ^2\ ; {\rm d}r = 0 $$ and to total torque $$ m = \int_{-l/2}^{l/2}\ ; {r\ ; \rm sign} ( r ) \frac{1}{2}\rho\ ; h\ ; c_d\ ; ( \omega\ , r ) ^2\ ; {\rm d}r = \frac{\rho c_d h \omega^2 l^4}{64} $$ the main assumption here is that the $c_d$ does not depend on the flow velocity $v=\omega\ , r$ which it does . a more extensive analysis would require finding the reynolds number and the fanning friction factor and deriving the pressure distribution along the bar based on the flow characteristics .
i greatly sympathize with your question . it is indeed a very misleading analogy given in popular accounts . i assure you that curvature or in general , general relativity ( gr ) describe gravity , they do not assume it . as you appear to be uninitiated i shall try to give you some basic hints about how gravity is described by gr . in the absence of matter/energy the spacetime ( space and time according to the relativity theories are so intimately related with each other it makes more sense to combine them in a 4 dimensional object called space-time ) is flat like a table top . this resembles closely with ( not completely ) euclidean geometry of plane surfaces . we call this spacetime , minkowski space . in this space the shortest distance between any two points are straight lines . however as soon as there is some matter/energy the geometry of the surrounding spacetime is affected . it no longer remains minkowski space , it becomes a ( pseudo ) riemannian manifold . by this i mean the geometry is no longer like geometries of a plane surface but rather like geometries of a curved surface . in this curved spacetime the shortest distance between any two points are not straight lines in general , rather they are curved lines . it is not very hard to understand . our earth is a curved surface and the shortest distance between any two points are great circles rather than straight lines . similarly the shortest distance between any two points in the 4 dimensional spacetime are curved lines . an object like sun makes the geometry of spacetime curved in such a way that the shortest distance between any two points are curved . this is called a geodesic . a particle follows this curved geometry by moving along this geodesic . einstein 's equations are mathematical descriptions of the relation of the geometry to the matter/energy . this is how gravity is described in general relativity .
when one reaches energies of gev and experimentally measures interactions that happen below 1 fermi , the nuclear dimensions , the question of size of an elementary particle becomes separated from the concept of its dimensions . the elementary particles of the standard model have dimensions 0 . this certainty comes because the theoretical standard model fits very well practically all available particle data , and the zero dimension of its constituent particles is one of the basic blocs in the computations . but , at the level below fermi , elementary zero dimensional particles have complicated interactions , described by form factors , which create a size for them . in particular for the photon the size is seen as a photon structure function , which changes with the interaction exchange energies . it gives an effective size to the photons , not a fixed one , but one depending on the energy of the probing particle . high energy photons can transform in quantum mechanics to lepton and quark pairs , the latter fragmented subsequently to jets of hadrons , i.e. protons , pions etc . at high energies e the lifetime t of such quantum fluctuations of mass m becomes nearly macroscopic : t ≈ e/m2 ; this amounts to flight lengths as large as 1,000 nanometers for electron pairs in a 100 gev photon beam , and still 10 fermi , i.e. the tenfold radius of a proton , for light hadrons . form factors are an experimental measurement of these complicated virtual diagrams that exist whenever one elementary particle interacts with another . so an elementary particle , including the photon , has a size measurement due to the particular interaction possibilities when scattering or being scattered by other elementary particles . a variable size measurement which depends on the energy transfers of the collisions . it is still of dimension zero . if you have difficulty visualizing this , think of a spark gap , where an electric field is applied . the higher the field the larger the spark seen , the larger the spatial dimensions it occupies , nevertheless the geometry of the gap is fixed . in the same way the geometrical dimensions of the particle are zero , but depending on the energy of interaction , it has a size .
c is a priori not the speed of light . it is the speed of massless particles . the way it comes about is as follows : you construct the lorentz-transformations as the symmetry transformations of minkowski space . the group has one parameter , that is c . you have to fix it by physical means . you can look at the dynamics of massive particles and massless particles and find that massive particles will approach c asymptotically only at infinite energy , and massless particles always move with c . since to our best knowledge photons are massless , c is also the speed of light . also , that was historically einstein 's motivation , which is why it is usually motivated in textbooks this way . however , should it turn out one day that photons do have very tiny masses , then c will still be there , it will just no longer be called the speed of light .
you are of course right . experimentally established , known laws of physics - especially the equivalence principle - are enough to be certain that antimatter has the same gravitational properties - including universal attraction - as ordinary matter . http://motls.blogspot.com/2010/09/can-antimatters-gravity-be-repulsive.html the justification of further experiments by " tests of antimatter 's gravity " is partly based on ignorance and partly on deliberate deception to get funding .
dear john , note that 23.85 å is equal to 2.385 nm , while the observed 4.3 nm is approximately two times larger . there is a simple error in your calculation that exactly fixes the factor of two . note that the actual calculation you should have done has a radius proportional to $1/m$ and the correct $m$ that you should substitute is the reduced mass of the two-body problem governing the relative position of the two particles . http://en.wikipedia.org/wiki/reduced_mass the reduced mass is $m_1 m_2 / ( m_1+m_2 ) $ . now , the important point is that an exciton is not a bound state of the effective electron and a superheavy nucleus : instead , it is a bound state of an effective electron and an effective hole - a larger counterpart of the positronium ( an electron-positron bound state ) . http://en.wikipedia.org/wiki/exciton assuming that both the electron and hole masses are equal , 0.26 $m_0$ , the reduced ( and still also effective ) mass is 0.26/2 $m_0$ = 0.13 $m_0$ , and the resulting $a$ is twice as big as your result , 4.77 nm - assuming that your arithmetics is right . the deviation from 4.3 nm is not too large but i can only handwave if i were trying to pinpoint the most important source of the discrepancy . it could be a different effective mass of the hole ; finite-size effects caused by the fact that the silicon atoms were not quite uniformly distributed inside the exciton , and so on . update oh , in fact , i noticed that your properties table does include a special figure of the effective hole 's mass and it differs from the electron mass : 0.38 $m_0$ . so the reduced mass is $$\frac{0.38\times 0.26}{0.38+0.26} m_0 = \frac{0.0988}{0.64} m_0 = 0.154 m_0 $$ and the calculated radius is $$ \frac{11.7}{0.154} \times 0.53\ &#197 ; = 40.3\ &#197 ; . $$ well , this is 7 percent too small , much like the previous one was 7 percent too big . ; - ) hydrogen atoms with composite heavy fermions concerning your second question , as you clearly realize , the calculated radius of the " atom " with such " heavy electrons " would be much smaller than the ordinary atom . this also proves that the assumptions of such a calculation fail : the heavy fermions ( in condensed matter physics ) are the result of the collective action of many atoms on the electron and its mass . so the large mass of the heavy fermions is only appropriate for questions about physics at long distances - much longer than the ordinary atom . if you look at very short distances - a would-be tiny atom with the heavy fermion - you cannot use the long-distance or low-energy effective approximations of condensed matter physics . you have to return to the more fundamental , short-distance or high-energy description which sees electrons again . at any rate , you will find out that there can be no supertiny atoms created out of the effective particles such as heavy fermions . the validity of all such phenomenological effective theories - such as those with heavy fermions - is limited to phenomena at distances longer than a certain specific cutoff and highly sub-atomic distances surely violate this condition , so one must use a more accurate theory than this effective theory , and in those more effective theories , most of the fancy emergent condensed matter objects disappear . non-relativistic effective theories just a disclaimer for particle physicists : in this condensed matter setup , we are talking about non-relativistic theories so the maximum allowed energy $e$ of quasiparticles does not have to be $pc$ where $p$ is the maximum allowed momentum in the effective theory . in other words , we can not assume $v/c=o ( 1 ) $ . quite on the contrary , the validity of such effective theories in condensed matter physics typically depends on the velocities ' being much smaller than the speed of light , too . so the mass of the heavy fermions is much greater than $m_0$ which would make $m_e c^2$ much greater than $m_0 c^2$ ; however , the latter is not a relevant formula for energy in non-relativistic theories . instead , $p^2/2m_e$ , which is ( for heavy fermions ) much smaller than the kinetic energy of electrons , is relevant . the maximum allowed $p$ of these quasiparticles is much larger than $\hbar/r_{\rm bohr}$ - the de broglie wavelength must be longer than the bohr radius . that makes $p^2/2m_e$ really tiny relatively to the hydrogen ionization energy .
yes it is called the absolute ceiling . this is the highest altitude at which an aircraft can sustain level flight . when a plane reaches this height the thrust of the engines at full power is equal to the total drag at minimum drag speed . this occurs where the maximum thrust available equals the minimum thrust required , so the altitude where the maximum sustained ( with no decreasing airspeed ) rate of climb is zero . the service ceiling is lower and has some safety margin built in . this is the height where the rate of climb is not zero so the craft still has some maneuverability . most commercial jetliners have a service ( or certificated ) ceiling of about 42,000 feet and some business jets about 51,000 feet . many military jets are able to fly substanitally higher but it is classified . the sr71 in 1976 published a world record 85,135 feet , however i am sure they and some others can go higher . all propeller based aircraft have much lower ceilings and they vary widely with design of the craft . the turboprop aircraft with the highest altitude ceiling is the lockheed p-3 orion , which has a maximum cruise altitude of 55,000 ft . there is a trade off point because as the air gets less dense the plane can travel faster . so many planes climb to the 20,000+ height for less drag but still enough pressure for efficient thrust .
your last sentence answers your question . we observe lorentz symmetry in the laws of nature . therefore , we demand that the building blocks of our theory transform in definite representations of the lorentz ( or rather poincaré ) group . would you allow fields that are not representations of the lorentz group , it would become extremely hard to construct a theory that looks lorentz invariant .
let me start by saying nothing is known about any possible substructure of the electron . there have been many experiments done to try to determine this , and so far all results are consistent with the electron being a point particle . the best reference i can find is this 1988 paper by hans dehmelt ( which i unfortunately can not access right now ) which sets an upper bound on the radius of $10^{-22}\text{ m}$ . the canonical reference for this sort of thing is the particle data group 's list of searches for lepton and quark compositeness . what they actually list in that reference is not exactly a bound on the electron 's size in any sense , but rather the bounds on the energy scales at which it might be possible to detect any substructure that may exist within the electron . currently , the minimum is on the order of $10\text{ tev}$ , which means that for any process occurring up to roughly that energy scale ( i.e. . everything on earth except high-energy cosmic rays ) , an electron is effectively a point . this corresponds to a length scale on the order of $10^{-20}\text{ m}$ , so it is not as strong a bound as the dehmelt result . now , most physicists ( who care about such things ) probably suspect that the electron can not really be a point particle , precisely because of this problem with infinite mass density and the analogous problem with infinite charge density . for example , if we take our current theories at face value and assume that general relativity extends down to microscopic scales , an point-particle electron would actually be a black hole with a radius of $10^{-57}\text{ m}$ . however , as the wikipedia article explains , the electron 's charge is larger than the theoretical allowed maximum charge of a black hole of that mass . this would mean that either the electron would be a very exotic naked singularity ( which would be theoretically problematic ) , or general relativity has to break at some point before you get down to that scale . it is commonly believed that the latter is true , which is why so many people are occupied by searching for a quantum theory of gravity . however , as i have mentioned , we do know that whatever spatial extent the electron may have cannot be larger than $10^{-22}\text{ m}$ , and we are still two orders of magnitude away from probing that with the most powerful particle accelerator in the world . so for at least the foreseeable future , the electron will effectively be a point .
the purpose of the morley-michelson experiment was to detect the motion of the lab relatively to the inertial system of the luminiferous aether , i.e. the " aether wind " . the theory that the electromagnetic waves were waves of a composite medium – analogously to sound 's being made of waves in the air – predicted that the speed of light should change to $c-v$ and $c+v$ if we move relatively to the preferred frame by the speed $v$ ( in the direction of light or against it , respectively ) . so none of your entries 1,2 in the first list describe the situation correctly . the intent was exactly the opposite ( not that it matters too much ) . the conclusion is 1 in the second list of yours , aether indeed does not exist ( or does not pick a preferred frame ) and the electromagnetic waves are waves that do not require any medium and that violate the rules for the additional of velocities ( the speed of light is always $c$ , not $c\pm v$ , regardless of the speed of the source or the detector ) , except that you should erase " indeed " because no one had expected that result , not even einstein who was 8 at that time in 1887 . the mm experiment may be viewed as the primary experimental support for special relativity . however , it is also another historical fact that it has not played a key role for einstein while developing special relativity – einstein 's reasoning was entirely theoretical , he did not refer to the mm experiment , and the only historical evidence that he was actually aware of it was einstein 's reference to a paper by lorentz that did mention the mm experiment .
if you believe general relativity , and specifically if you believe the assumptions used to derive the flrw model are justified , then the expansion of the universe is solely the result of the expansion of spacetime . those " ifs " may seem a bit excessive , but it is important to emphasise that gr is a mathematical model that seems to work when we compare it with experiment . we may discover new things that modify the model . for example the flrw metric did not include dark energy , though if dark energy can be described by a cosmological constant the flrw metric does include it . as the wiki article mentions , most physicists believe the flrw metric is a good description of the universe . to answer your second question requires a bit of background . if you take two non-interacting particles some fixed distance apart and sit back and wait for some significant fraction of a hubble time you will see the particles moving apart . for the sorts of distances we see every day this effect is tiny ; it is only significant at intergalactic distances . because the expansion is so small at small distances , it can be counteracted by even the tiniest of forces . this means that if our two test particles are interacting , the interaction will probably completely swamp the expansion . this is why the expansion of the universe is not making you expand . the forces between the atoms in your body are hugely greater than the expansion effect . even on galactic scales this is the case . galaxies do not expand because the gravitational forces between the stars in them overcomes the expansion . it is not until we get to the scale of galaxy clusters that expansion wins . the last part of your question is hard to answer without things getting exceedingly technical . in gr you choose some convenient set of co-ordinates to state the metric . the flrw metric uses co-moving co-ordinates . by definition , in these co-ordinates the time co-ordinate is the same as the proper time , which is an invarient , because the proper time is the time experienced by a freely falling observer . that means that time is not curved . however there is no special distinction between space and time co-ordinates , and there will be other co-ordinate systems in which time is curved .
well at the most fundamental level , the index of refraction of a material is defined as $ n = \sqrt {\epsilon \mu} $ where $\epsilon $ is the electric permittivity and $\mu$ is the magnetic permeability of the material . this arises from the solution of maxwell 's equations in a medium . also arising from the wave equation , which can be derived from maxwell 's equations , is that the index of refraction is the speed of light in vacuum , $c$ divided by the speed of light in the material $c_m$ . $ n ={ c \over c_m} $ worth noting is that since $c_m$ is always less than $c$ , the index of refraction is always greater than 1 . now the phase velocity for an electromagnetic wave of angular frequency $\omega$ is given by $v_p = {\omega \over k}$ where $k= {2\pi \over\lambda_m}$ is the magnitude of the wave vector and $\lambda_m$ is the wavelength in the medium . so after a little algebra , we find that the wave vector and index of refraction are related by $k ={ n\omega \over c}$ where does all this come into play in refraction and snell 's law ? well , it is the wave vector that comes into play in satisfying the boundary conditions on the electric ( and magnetic ) fields at the interface between two media . to see this , let 's look at the simple case of a plane wave of monochromatic light incident on the interface between two media with indices of refraction $n_1$ and $n_2$: in this simple case , considering the boundary conditions on the electric field at the interface is sufficient to derive snell 's law . the boundary condition is given by equation ( 1 ) in the diagram , namely that the incident and reflected electric fields minus the transmitted electric field must be zero , or equivalently that the total electric field at the interface must be continuous . since we defined y=0 as the plane of the interface , this boundary condition must hold for any value of x . this leads after a little algebra to equation ( 2 ) in the diagram . this equation depends only on the angles of incidence $\phi_{inc}$ and refraction $\phi_{tr}$ , the wave vector magnitudes in both media $k_1$ and $k_2$ , and the transmission and reflection coefficients $t$ and $r$ ( the fraction of energy that is transmitted into the new medium and reflected into the old medium respectively ) . by symmetry , $\phi_{inc} = \phi_{refl}$ . because the left side of equation 2 is independent of angle , so must the right side be . this leads to the term in the exponential being zero , which leads directly to snell 's law , using the relation between wave vector and index of refraction shown above . the group velocity never comes into play in the boundary conditions of refraction . it does come into play in the propagation of energy in the media ( as opposed to the fields ) , but that is another question .
the coulomb energy goes like $z ( z-1 ) $ , so it is typically a very small effect for light nuclei . the coulomb energy would be responsible for the difference in binding energy between 3he and 3h . in general , even-even nuclei are always more bound than odd nuclei . this is due to pairing . apart from pairing , one can usually predict nuclear binding energies quite well at a quantitative level by considering them to be the sum of a liquid-drop energy and a shell correction ( strutinsky 1968 ) . the shell correction for 4 he is large , because it is doubly magic . ( i do not actually know if the strutinsky works well for such light nuclei , but it is definitely good enough to give a correct qualitative explanation . ) “shells” in deformed nuclei . v . m . strutinsky , nucl . phys . a 122 no . 1 ( 1968 ) pp . 1-33 . see also : curvature correction in the strutinsky 's method . p . salamon , a . t . kruppa . j . phys . g : nucl . part . phys . 37 no . 10 ( 2010 ) 105106 . arxiv:1004.0079 . ( a variation on the technique , describes the technique itself . )
this is a gravitational phenomenon known as tidal lock . it is closely related to the phenomenon of tides on earth , hence the name . tidal locking is an effect caused by the gravitational gradient from the near side to the far side of the moon . ( that is , the continuous variation of the gravitational field strength across the moon . ) the end result is that the moon rotates around its own axis with the same period as which it rotates around the earth , causing the face of one hemisphere always to point towards the earth .
if we are considering " rigid " objects ( ones where the deformation of the material simply is not large enough to be relevant to our problem ) , then no , there is no decrease . you can think of each of those ten elements having the same force from one to the next . $p$ pulls on #10 , #10 pulls on #9 , and so on . between each element is the same force . also , the location of the force makes no difference for linear translations or accelerations . you could pull the end of the rod , the middle , or anywhere else . ( it does matter where the force is applied if you are interested in the torque or rotation of the object ) . the location would affect your diagram though . if instead of pulling on the end of the rod you pulled on the middle , then the wall would feel the same force , but the end of the rod would not . only the elements from the pull point to the wall would have your force $p$ . now gravity is different . there , you are correct that each individual piece is being pulled slightly . however , newton did the math to show that in many cases , we do not have to worry about the complexity . instead , the result is the same as if we consider a simple force acting on the object through the center of gravity . in cases where we cannot generalize , then you might want to actually sum up all of the infinitesimal forces ( via calculus perhaps ) . i was reading your first question differently . as in " if you pull on the end , what do the forces between the elements look like " . i think now you were asking " is pulling on the end with p really just something pulling on each element with a fraction of the force " . i would say that it is not the same , but both would yield similar reactions on a rigid body . since you can just do a vector summation of the forces , then one force of $p$ , or 10 forces each with one-tenth the magnitude are equivalent . but that second way of seeing it is not " more accurate " . once you move beyond rigid bodies and care about the internal stress , then where the force is applied matters . when the force is applied at the end , then each linear element pulls on the next with the same force . but if the force is applied in the middle , only the elements from there to the wall have the forces between them . if the force is distributed over all elements ( as gravity would do ) , then the forces between each element are different .
would hot hydrogen ( in the same sense as hot air ) be able to lift even more mass ? yes . though i suppose the fire danger goes up , and you certainly can not use a propane burner to warm it . . . would a higher or lower density of hydrogen in a ballon lift more ? lower density always means higher buoyancy . if you could have a balloon which had nothing in it ( it was a vacuum inside ) would that lift more than a hydrogen balloon ? yes , and this has been proposed in various ways in science fiction literature . the engineering challenge is finding a away to confine the vacuum that is as light as a gas bag so that you do not loose the advantage to extra weight . in general a volume $v$ of material of density $\rho$ immersed in a fluid of density $\rho_f$ experiences a buoyant force of $$ f_b = gv\rho_f $$ and a weight of $$w = -gv\rho $$ so the available lifting force is $$ f_l = gv ( \rho_f - \rho ) . $$ where the object is floating at the surface of a liquid the buoyant force is modified to reflect the volume of liquid displaced $f_b = g v_d \rho_f$ where $v_d$ is enough to cover the weight of the floating object .
spin is a technical term specifically referring to intrinsic angular momentum of particles . it means a very specific thing in quantum/particle physics . ( physicists often borrow loosely related everyday words and give them a very precise physical/mathematical definition . ) since truly fundamental particles ( e . g . electrons ) are point entities , i.e. have no true size in space , it does not make sense to consider them ' spinning ' in the common sense , yet they still possess their own angular momenta . note however , that like many quantum states ( fundamental variables of systems in quantum mechanics , ) spin is quantised ; i.e. it can only take one of a set of discrete values . specifically , the allowed values of the spin quantum number s are non-negative multiples of 1/2 . the actual spin momentum ( denoted S ) is a multiple of planck 's constant , and is given by $s = \sqrt{s ( s + 1 ) }$ . when it comes to composite particles ( e . g . nuclei , atoms ) , spin is actually fairly easy to deal with . like normal ( orbital ) angular momentum , it adds up linearly . hence a proton , made of three constituent quarks , has overall spin 1/2 . if you are curious as to how this ( initially rather strange ) concept of spin was discovered , i suggest reading about the stern-gerlach experiment of the 1920s . it was later put into the theoretical framework of quantum mechanics by schrodinger and pauli .
there would , of course , be no " impact " since the black hole will not interact with the earth the same way that a solid object would . however , the gravitational effects from the black hole would be catastrophic . since the black hole is about as massive as the moon , it would significantly deform the earth as it passes through , causing mega-earthquakes , mega-tsunamis , and mega-volcanoes . ( the actual accretion rate of matter into the black hole would be negligible , as mentioned in the other question ) per alan 's comment : some of the matter that gets pulled near the black hole will be turned into extremely powerful radiation ( in a relativistic jet along its axis of rotation ) which will surely do additional damage .
you have probably learned about a quantity called " impulse " - try using that to solve the problem . let $j$ be impulse , defined for constant forces as $j = f t$ where $f$ is the force applied and $t$ is the time for which the force is applied . since $f=ma$ , we can substitute this to get : $$\begin{aligned} j and = ft \\ j and = mat \\ j and = m ( at ) \\ j and = m \delta v \\ j and = \delta ( mv ) \\ j and = \delta p \end{aligned}$$ that is , $f t$ is equal to the change in momentum of the cart . since both carts have the same force applied to them for the same amount of time , the momentum change of both carts must be equal . for part ( b ) , consider conservation of energy . the force does a certain work on the cart , which causes the cart to gain kinetic energy . to find the work done on each cart , simply find the increase in kinetic energy of each one . try this part by yourself , and comment if you need further assistance .
i am not sure if this is exactly what you are looking for or perhaps you already know what i am about to say . there is a geometric notion of a twistor spinor ( or conformal killing spinor ) : one which is in the kernel of the penrose operator ( see below ) . then one defines the twistor space as the projectivisation of the space of twistor spinors . doing this for minkowski spacetime recovers the usual twistor space . let $ ( m , g ) $ be a riemannian spin manifold . ( when i say riemannian i include also the case of a metric with indefinite signature . ) let $s$ denote the complex spinor bundle . the spin connection defines a map $$ \nabla : \gamma ( s ) \to \omega^1 ( s ) $$ from spinor fields to one-forms with values in $s$ . now $\omega^1 ( s ) = \gamma ( t^*m \otimes s ) $ and clifford action of one-forms on spinors gives a map $$ \omega^1 ( s ) \to \gamma ( s ) $$ the composition of the previous two maps is the dirac operator . the penrose operator is in some sense the complement of the dirac operator $d$ . the kernel of the clifford map $t^*m \otimes s \to s$ defines a subbundle $w$ , say , of $t^*m \otimes s$ . composing the covariant derivative with the projection $\omega^1 ( s ) \to \gamma ( w ) $ defines the penrose operator $p : \gamma ( s ) \to \gamma ( w ) $: explicitly , $$ p_x \psi = \nabla_x \psi + \frac1n x \cdot d\psi $$ for all vector fields $x$ and spinor fields $\psi$ , and where $n = \dim m$ . ( my clifford algebra conventions are $x^2 = - |x|^2$ . ) notice that the " gamma trace " of the penrose operator vanishes . there is a sizeable literature on twistor spinors mostly in riemannian and lorentzian signatures . this is the work of helga baum and collaborators in berlin . a search for " twistor spinors " in mathscinet should give you many links . one important property of the twistor spinor equation is that it is conformally invariant , whence the twistor spinors of conformally related riemannian spin manifolds correspond in a simple way . since you mention maximally symmetric lorentzian manifolds , this observation might be of use because such spaces are conformally flat , hence you can write down the twistor spinors simply by rescaling the twistor spinors in minkowski spacetime . in riemannian signature ( hence for round spheres and hyperbolic spaces ) this is described in the 1990 humboldt university seminarberichte twistor and killing spinors on riemannian manifolds by baum , friedrich , grunewald and kath , later published by teubner .
assuming $m_e$ means the mass of an electron , a photon with that much energy would be a gamma ray . the mass of an electron is about $ . 51 mev/c^2$ , and a photon with an energy of $ . 51mev$ is called a gamma ray .
here 's coulomb 's law : if you scale everything by $\lambda$ , you get $\frac{1}{\lambda^2}$ in the denominator , but you must also introduce a jacobian for the integral . for a volume charge , the jacobian is $\lambda^3$ , so you are on the surface of a ball of charge and you make the ball bigger ( with the same charge density ) , the e-field increases proportionately . for a surface charge , though , the jacobian is $\lambda^2$ , which cancels the $\frac{1}{\lambda^2}$ in the denominator of coulomb 's law . thus , a 2-d distribution of charge is " scale invariant " . since scaling an infinite sheet leaves it unchanged , scaling changes only the distance from the sheet , and we see that the e-field is independent of distance .
friction is a force resisting relative movement of objects or fluids caused by the interactions taking place at the contact between the objects or fluids . hence , by definition there cannot be friction between an object and magnetic field . this does not mean though that there cannot be a force resisting movement of a metal object relative to magnetic field . you would not call any such forces friction , though . one kind of breaking force appears when a metal object moves relative to magnetic field and is caused by induction which produces eddy currents which in turn generate opposing magnetic field which resists the movement of the metal object . this is how eddy current brake works .
yes , the expansion of space itself is allowed to exceed the speed-of-light limit because the speed-of-light limit only applies to regions where special relativity – a description of the spacetime as a flat geometry – applies . in the context of cosmology , especially a very fast expansion , special relativity does not apply because the curvature of the spacetime is large and essential . the expansion of space makes the relative speed between two places/galaxies scale like $v=hd$ where $h$ is the hubble constant and $d$ is the distance . when this $v$ exceeds $c$ , it means that the two places/galaxies are " behind the horizons of one another " so they can not observer each other anytime soon . but they are still allowed to exist . in quantum gravity i.e. string theory , there may exist limits on the acceleration of the expansion but the relevant maximum acceleration is extreme – planckian – and does not invalidate any process we know , not even those in cosmic inflation .
note : $r_{ab}= ( -i-3j+3k ) m$ . so , $$ v_{ab}=- ( 2i+3j+4k ) \cdot ( -i-3j+3k ) $$ or , $v_{ab}= - ( -2-9+12 ) $ volt ( s ) or , $v_{ab}= -1$ volt .
everywhere along the wall velocity is zero . so if you move in the tangential direction ( along the wall ) , the difference $\delta\vec{v}_t$ is zero . however , if you are moving away from the wall , velocity is no longer zero , so $\delta v_n$ is not zero .
yes , in fact one of the comments made to a question mentions this . if you stick to newtonian gravity it is not obvious how a photon acts as a source of gravity , but then photons are inherently relativistic so it is not surprising a non-relativistic approximation does not describe them well . if you use general relativity instead you will find that photons make a contribution to the stress energy tensor , and therefore to the curvature of space . see the wikipedia article on em stress energy tensor for info on the photon contribution to the stress energy tensor , though i do not think that is a terribly well written article .
it is easy to get mixed up between time and the flow of time , and i think you have done this in your question . take time first : since 1905 we describe any event as a spacetime point and label it with four co-ordinates ( $t$ , $x$ , $y$ , $z$ ) . saying that time does not exist means we can ignore the time co-ordinate and label everything by just it is spatial co-ordinates ( $x$ , $y$ , $z$ ) , which is contradiction with observations . the time co-ordinate obviously exists and be used to distinguish events that happen at the same place but at different times . now consider the flow of time : actually this is a tough concept , and relativity makes it tougher . we all think we know what we mean by the flow of time because we experience time passing . to take your example of movement , we describe this as the change of position with time , $d\vec{r}/dt$ , where we regard time as somehow fundamental . i am guessing that this is what you are questioning i.e. whether the flow of time is somehow fundamental . i do not think there is a good answer to this . to talk about the flow of time you had have to ask what it was flowing relative to . in relativity we can define the flow of co-ordinate time relative to proper time , $dt/d\tau$ , and indeed you find that this is variable depending on the observer and in some circumstances ( e . g . at black hole event horizons ) co-ordinate time can stop altogether . but then you had have to ask whether proper time was flowing . you could argue that proper time is just a way of parameterising a trajectory and does not flow in the way we think time flows . at this point i am kind of stuck for anything further to say . if i interpret your question correctly then you do have a point that just because we observe change of position with time ( i.e. . movement ) this does not necessarily mean time is flowing in the way we think . however i am not sure this conclusion is terribly useful , and possibly it is just semantics .
what the definition needs to capture is that a black hole is not ( 1 ) a naked singularity , or ( 2 ) a big bang ( or big crunch ) singularity . we also want the definition to be convenient to work with so that , for example , it is possible to prove no-hair theorems . since we want to exclude naked singularities , it is natural that we require an event horizon . event horizons are by their nature observer-dependent things . for example , if we have a naked singularity , we can always hide its nakedness by picking an observer who is far away from it and accelerating continuously away from it . such an accelerated observer always has an event horizon , even in minkowski space . this example shows that it makes a difference what observer we pick . actually , we can not have a material observer at null infinity , since timelike infinity , not null infinity , is the elephants ' graveyard for material observers . however , the choice of null infinity is the appropriate one because a black hole is supposed to be something that light can not escape from . of course the actual universe is not asymptotically flat , but that does not matter . in practice , all we care about is that the black hole is surrounded by enough empty space so that the notion of light escaping from it is well defined for all practical purposes . there are other possible ways of defining a black hole , e.g. , http://arxiv.org/abs/gr-qc/0508107 .
look at around 0:34 in the video . i want to point out something relevant to the question here . the end of the tube is narrowed . that is , in technical terms , a nozzle . nozzles are extremely common in engineering and they work as a form of mechanical leverage just like a lever . i should also note that the straw itself is already a form of nozzle and allows ( i think ) a more potent blow than would otherwise be possible with the lips . a human mouth has limitations . the most accurate way to frame this would be to say that one 's mouth can only produce a given flow rate , $\dot{m}$ , at a certain pressure above atmosphere , $\delta p$ . combined , these give an energy rate , or power , that can be produced by the mouth . the comment by steve melvin does apply - that the balls can not move faster than the fluid that is passing by it . however , the small outlet of the straw he uses is a way to make a tradeoff , getting high fluid velocity by sacrificing volume of flow . this would , in fact , be rather more difficult to do as accurately and gracefully with mechanical forces . this type of easy conversion ability of forms of fluid mechanical work is a major reason that hydraulics is such a useful science .
i actually solved the problem . the key idea is to use the fact that the metric depends on the internal manifold ( the flag ) only through the maurer-cartan forms and hence the scalar curvature cannot depend on the position in the internal manifold . one can then expand the elements of $su ( n ) $ near the origin . keep the metric exact in terms of the lambdas and to second order in the flag directions . one can then perform explicit calculations of the scalar curvature . in case someone needs this in the future , this is the result for the scalar curvature : $r=-4 ( p-1 ) \sum\limits_{i\neq j}\frac{1}{ ( \vec\lambda_i-\vec\lambda_j ) ^2}-3\sum\limits_{i\neq j\neq k}\frac{ ( \vec\lambda_i-\vec\lambda_j ) . ( \vec\lambda_i-\vec\lambda_k ) }{ ( \vec\lambda_i-\vec\lambda_j ) ^2 ( \vec\lambda_i-\vec\lambda_k ) ^2}$ , where $p$ is the number of commuting matrices and $\vec\lambda_i$ are the eigenvalues .
q1: the important thing to know is that there are several distinct concepts of entropy in statistical physics and mathematics and there is no " the entropy " ( life is hard ) . only in thermodynamics , the word entropy has clear meaning by itself , because there it is the clausius entropy . to answer your question , in short : it can be shown that for macroscopic systems ( large number of particles ) , in quasi-static processes that isolated system undergoes , the quantity $\ln v$ behaves as the clausius entropy in thermodynamics , that is , it does not change as the process proceeds . so it is sometimes called entropy too ( i do not think boltzmann entropy is a good name for it , since it is not clear whether boltzmann thought this to be " the entropy" ; it is said he never wrote in his papers and was first written down by max planck ) . it would be better to call it , say , phase-volume-entropy or so :- ) . the volume of the phase space could be that of the region of phase space accessible to the system , or to the region corresponding to lower energy than the system has . for ordinary macroscopic systems , these give the same value of entropy . q2: i do not know , but there seems to be a connection . q3: they are largely the same , the difference is that the gibbs formula with probabilities is meant for states of a macroscopic physical system ; for such system in thermodynamic equilibrium , the gibbs formula with the boltzmann exponential probabilities gives value that is practically equal to the value of the phase-space-entropy ( for phase space that is consistent with the macroscopic variables of the equilibrium state ) . the shannon expression describes something very different , a degree of uncertainty of the actual value of some variable ( say , one character of a message ) . there is a connection ; the maximum possible value of the shannon expression given fixed average energy and temperature is almost equal to the phase-volume-entropy for phase space region that would be assigned were the system isolated and in state with the same values of macroscopic quantities ( energy , volume , . . . ) this is basis of the information-theoretical approach to statistical physics ( see works of edwin jaynes on statistical physics http://bayes.wustl.edu/etj/node1.html ) .
a damped harmonic oscillator with a sinusoidal driving force is represented by the equation $$\ddot{x} + \gamma\dot{x} + \omega_0^2x = \frac{f_d \sin ( \omega_d t ) }{m}$$ where $\gamma = b/m$ ( $b$ is the damping coefficient , $b=f/v$ ) and $\omega_0^2 = k/m$ is the resonant frequency of the oscillator . the particular solution to this equation can be determined by taking the imaginary part of the solution to $$\ddot{x} + \gamma\dot{x} + \omega_0^2x = \frac{f_d}{m}e^{i\omega_d t}$$ if you assume* the solution takes the form $$x ( t ) = a e^{i ( \omega_d t + \phi ) }$$ and plug that in , you get $$-a \omega_d^2 + \omega_0^2 a = \frac{f_d}{m}\cos ( \phi ) $$ and $$\gamma\omega_d a = \frac{f_d}{m}\sin ( \phi ) $$ solving for the phase difference gives $$\tan\phi = \frac{\gamma\omega_d}{\omega_0^2 - \omega_d^2}$$ this depends on the frequency of the driving force and the resonant frequency of the oscillator , but not on the amplitude of the driving force . you can express this in terms of the dimensionless variable $x = \omega_d / \omega_0$ as $$\tan\phi = \frac{\gamma}{\omega_0}\frac{x}{1 - x^2}$$ and if you graph it , ( graph generated by wolfram alpha ) you will see how the response of the oscillator jumps from leading to lagging when $\omega_d = \omega_0$ ( at $x=1$ ) , that is , when the driving and resonant frequencies are equal . *the same solution can be obtained from fourier decomposition without making this assumption .
i take this question to mean : why does the laguerre-gaussian ( lg ) modes have an $e^{i\ell\phi}$ dependance on the azimuthal coordinate $\phi$ ? why is $\ell$ required to be an integer ? question 1 the lg modes are solutions to the paraxial wave equation in cylindrical coordinates . this means that we get solutions that reflect this symmetry . in particular the solutions should only trivially change if you make the change $$\phi\to\phi+\delta\phi . $$ if we define a rotation operator $r_{\delta\phi}$ such that this operator acting on any function $f ( \phi ) $ gives $$r_{\delta\phi}f ( \phi ) \equiv f ( \phi+\delta\phi ) $$ cylindrical symmetric solutions will be the eigenfunctions of $r_{\delta\phi}$ , i.e. $$r_{\delta\phi}f ( \phi ) =\lambda f ( \phi ) , $$ where $\lambda$ is a constant . the solution to this equation is of the form $$f_\ell ( \phi ) \sim e^{i\ell\phi} , $$ i.e. $$r_{\delta\phi}f_\ell ( \phi ) =e^{i\ell\delta\phi}e^{i\ell\phi}=\lambda_\ell e^{i\ell\phi} . $$ therefore cylindrically symmetric solutions such as the lg modes will be of the from $$lg ( r , \phi ) = f ( r ) e^{i\ell\phi} . $$ question 2 the reason $\ell$ has to be an integer ( i.e. . quantized ) is because $\phi$ is periodic . what this means is that $\phi$ and $\phi+2\pi$ are the exact same point , therefore all functions of $\phi$ must meet the requirement $$f ( \phi+2\pi ) =f ( \phi ) . $$ if our function is $e^{i\ell\phi}$ , as we saw in part 1 , then this means $$e^{i\ell ( \phi+2\pi ) }=e^{i\ell\phi}\to e^{i\ell 2\pi} =1 , $$ which is only true if $\ell$ is an integer .
i am not a professional , but i will try to answer anyway . meteor showers occur when the earth passes through the orbit of a comet ( or , in at least one case , an asteroid ) . over time , the debris spreads over the entire orbit of the comet . a shower can last for several days , which is an indication of how wide the debris stream is . assuming a duration of 1 day , and assuming the earth 's orbit is roughly at right angles to the debris stream , that gives a width of very roughly 2.5 million kilometers ( and a length of several hundred million kilometers ) . the earth is only about 12,735 kilometers in diameter . say the comet 's orbit is 1 billion kilometers long ( that is probably shorter than average ) . then multiplying the length of the orbit by the area of a circle 2.5 million kilometers across gives the volume of the stream , and multiplying 2.5 million kilometers by the area of a circle 12,735 kilometers in diameter gives the volume of the stream through which the earth passes ( the hole it punches in the stream ) . the ratio is about 15 million . other factors : earth 's gravity will pull in some debris that would not otherwise have hit it , making its effective diameter a bit bigger ( thanks to ghoppe 's comment ) . the density of the stream is not uniform . there are bound to be clumps of greater density . there is probably also a systematic change of density with distance from the sun . the width of the stream probably varies as well . i have no idea of the details the moon ( and its gravity well ) will also sweep up some debris -- but the moon 's effective area is a small fraction of earth 's . but the blatant errors in my assumptions undoubtedly swamp any such effects , and i am only looking for a rough estimate . so yes , the earth 's passage through a meteor stream will effectively punch a hole in it , but it is a very small hole relative to the size of the entire stream . it could have a significant effect over millions of years . i am making a lot of simplifying assumptions here , but the conclusion seems about right if i have gotten the result within one or two orders of magnitude . reference : http://www.amsmeteors.org/meteor-showers/meteor-faq/#5, plus some of my own extremely rough back-of-the-envelope calculations .
you should always find an answer that is a formula , and then only apply significant figures once you get to the one final step of substituting your numbers back into the problem in place of variables . avoid multiple intermediate steps of substituting numbers at all costs . not only will this save your pencil a lot of work , but it will also cause your answer to be more accurate , as rounding errors can pile up , even when using a calculator .
the last part of your question is the easiest to answer , so i will get to that first . the best book on the fundamentals of optical design is " modern optical engineering " by warren j . smith . it is not specific to aspheric optics , but does cover them in addition to the rest of geometrical optics and lens design . it is probably the single most common reference book among optical engineers . now , the rest of your question is a bit complicated , and needs a little bit of background , so bear with me for a moment . as has been mentioned , even an ideal lens will produce a focal spot of some minimum size , determined by the ratio of the lens focal length to its aperture ( this quantity is called the " f-number , or $f/\#$" ) and the wavelength of the light . this is what optical engineers call the diffraction limited spot size . for a circular aperture , the diameter of the diffraction limited spot size will be $$2.44 \times \lambda \times f/\#$$ where $\lambda$ is the wavelength . so as the $f/\#$ decreases ( as the lens gets " faster" ) the diffraction limited spot will become smaller . however , any aberrations in the lens will also become more significant ! this means that a very slow lens ( one with a long focal length , relative to its aperture ) can produce a diffraction limited spot even though it may have some aberration relative to an ideal lens , while a very fast lens will need to have a slightly aspheric shape to achieve diffraction limited performance . this is important to understand because it means that , in some cases , a spherical lens can indeed focus light as close to a point as is physically possible , even though a sphere is not the ideal shape . so what is that ideal shape ? well again , it depends on a few things . for both lenses and mirrors , the ideal shape will change depending on the distance from the object plane to the lens , and from the lens to the image plane . in the case you have asked about , where the incoming light is collimated , optical engineers would say that the object plane is at infinity . in this case , as some other people have pointed out , the ideal shape for a mirror is indeed a parabola . however , for a lens this is not the case . as it turns out , the ideal shape for a lens to focus a collimated beam of light to a point is to have the first surface of the lens ( the one the light hits first ) be elliptical , and the back surface be hyperbolic . lens designers usually specify the shape of a lens surface with the following equation : $$z = \frac{c r^2}{1 + \sqrt{1- ( 1+\kappa ) c^2 r^2}}$$ where $z$ is the " sag " of the lens surface , or its departure from a plane tangent to the lens surface at the center of the lens , $r$ is the radial distance from the center of the lens , $c$ is the curvature of the lens ( the reciprocal of its radius of curvature ) and $\kappa$ is called the " conic constant . " it is the value of $\kappa$ which determines what sort of conic section describes the surface : $\kappa &gt ; 0$ oblate ellipse $\kappa = 0$ sphere $0 &gt ; \kappa &gt ; -1$ prolate ellipse $\kappa = -1$ parabola $-1 &gt ; \kappa$ hyperbola on a related note , it is more than just the conic constant that can be adjusted to control aberrations . even with purely spherical surfaces , the relative curvature of the front and back lens surface can be varied , while keeping the effective focal length constant . adjusting this is more common than adding aspeheric surfaces to a lens , because aspheric surfaces are expensive to manufacture . many optical supply companies even offer off-the-shelf optics with an ideal bending ratio for a given application . these are often sold as " best form " lenses .
this is an eigenvalue problem . let 's assume your bogoliubov transformation is of the form : $ ( a_k , b_k ) ^t=x ( c_k , d_k ) ^t$ . what this transformation do is let your hamiltonian become : $h_k=w_1c_k^\dagger c_k+w_2 d_k^\dagger d_k$ , with the anti-commute relation holds for new field operators $c_k$ and $d_k$ . now you can check that $x$ is just the matrix where its columns are just the normalized eigenvectors of your original matrix .
an atom in isolation offers a potential well , and electrons form bound states in the well . the energy of those bound states can be calculated exactly in the case of a single-electron ( hydrogen-like ) atoms or by variational computational methods for more complicated cases . now when you put several atoms together in a tight and regular array , they offer a combined well resulting from the sum of all their potentials . that combined well might look ( in cartoon form ) something like this : here the red lines represent electron energy level that are still contained by the locally stronger effect of their " own " nucleus , but the blue line represents those slightly higher energy levels that see the combined potential as a single large energy well ( with some high-frequency detailed structure ) . in a conductor , non-conduction electrons fill all the red ( local ) energy levels and the remaining electrons must ( because of pauli exclusion ) then occupy the blue ( non-localized ) conduction levels .
if you place a camera you will not see any interference pattern . so , the answer is yes . the camera will cause the wavefunction to " collapse " . but i do not like the term " wavefunction collapse " , because wavefunction is not really any physical object . what the camera will basically do is cause an abrupt change in the state of the particle . here is the defintion of measurement from landau 's book by measurement , in quantum mechanics , we understand any process of interaction between classical and quantum objects , occurring apart from and independently of any observer . the importance of the concept of measurement in quantum mechanics was elucidated by n . bohr . we have defined " apparatus " as a physical object which is governed , with sufficient accuracy , by classical mechanics . such , for instance , is a body of large enough mass . however , it must not be supposed that apparatus is necessarily macroscopic . under certain conditions , the part of apparatus may also be taken by an object which is microscopic , since the idea of " with sufficient accuracy " depends on the actual problem proposed .
in physics " nothing " is generally taken to be the lowest energy state of a theory . we would not normally use the word " nothing " but instead describe the lowest energy state as the " vacuum " . i can not think of an intuitive way to describe the qm vacuum because all the obvious analogies have " something " instead of nothing " nothing " , so i will do my best but you may still find the idea hard to grasp . that is not just you - everybody finds it hard to grasp . start with the classical description of an electric field ( maxwell 's equations ) . it is not too hard to image an electric field as a field filling space . you can even feel the field : for example if you put your hand near an old style tv screen you can feel the static electricity . you can imagine turning down the electric field until it disappears completely , in which case you are left with the vacuum i.e. nothing . now imagine the same field , but this time we are using the quantum description of the field ( quantum electrodynamics instead of maxell 's equations ) . at the classical level the field is approximately the same as the description maxwell 's equations give , but now we have fluctuations in the field due to the energy-time uncertainty principle . just as before , imagine turning down the electric field until it disappears . unlike the classical description , the ( average ) electric field may disappear but the fluctuations do not . this means the quantum vacuum is different from the classical vacuum because it contains the fluctuations even after you have turned the field down to zero . the key point is that when i say " turn the field down " i mean reduce the energy to the lowest it will go i.e. you can not make the energy of the electric field any lower . by definition this is what we call the " vacuum " even though it is not empty ( i.e. . it contains the fluctuations ) . it is not possible to make the vacuum any emptier because the fluctuations are always present and you can not remove them .
the thing that is going wrong with your manual calculation is you are taking the velocity to be constant in every interval i.e. , you are taking velocity to be $10m/s$ from $0$ to $1s$ , $9m/s$ from $1s$ to $2s$ and so on which is incorrect . the velocity is continuously decreasing . you may calculate like this : at $t=0 , v=10m/s , a=-1m/s^2$ which means from $t=0$ to $t=1$ , car has travelled a distance , $$s=10\times1+\frac{1}{2} ( -1 ) \times 1^2$$$$=10-\frac{1}{2}$$ and the velocity has become $9m/s$ at $t=1s . $ so , from $t=1$ to $t=2$ , car has travelled a distance , $$s=9\times1+\frac{1}{2} ( -1 ) \times 1^2$$$$=9-\frac{1}{2}$$ and so on . $$10-\frac{1}{2}+9-\frac{1}{2}+ . . . . . . $$$$=55-5=50$$
if $\psi_1 ( x_1 ) \psi_1 ( x_2 ) $ is antisymmetric ( and i understand this is impossible , since the ground state is not degenerate ) the ground state is degenerate , since both particles have the same $n$ ( principal ) quantum number and thus the same energy . in general , for $n$ particles , the symmetric and antisymmetric wavefunction may be constructed as \begin{align}\psi_{_s} and \equiv\sqrt{\frac{n_1 ! \cdots{n}_k ! }{n ! }}\sum_p\hat{p}\ , \phi_{n_1} ( \zeta_1 ) \phi_{n_2} ( \zeta_2 ) \ldots\phi_{n_n} ( \zeta_n ) \\ [ 0.1in ] \psi_{_a} and \equiv\sqrt{\frac{n_1 ! \cdots{n}_k ! }{n ! }}\begin{vmatrix}\phi_{n_1} ( \zeta_1 ) and \cdots and \phi_{n_1} ( \zeta_n ) \\\vdots and and \vdots\\\phi_{n_n} ( \zeta_1 ) and \cdots and \phi_{n_n} ( \zeta_n ) \end{vmatrix}\end{align} respectively , where $\zeta_i$ are the internal degrees of freedom and $n_i$ is the degeneracy of the $i$-th set of degenerated particles ( for the antisymmetric part , most usually $n_1 ! \cdots{n}_k ! =1$ ) . in your case ( given that you can always write the wavefunction as a product of the spatial and spin parts ) , $$\psi_{_a}=\begin{vmatrix}\psi_1 ( x_1 ) and \psi_1 ( x_2 ) \\\psi_1 ( x_1 ) and \psi_1 ( x_2 ) \end{vmatrix}=0$$ which is why the spatial antisymmetric part is impossible for the ground state . for fermions this is a natural consequence of the pauli exclusion principle , since you would allow the possibility of two particles being in the same state , given that the spin part would be symmetric . now , for the first excited level there is no restriction about considering both the symmetric and antisymmetric parts , in fact you must consider them both . just as when you must consider the three posibilities from the triplet spin state $$\chi_{_t}=\begin{cases}\chi_\alpha\\\chi_\beta\\\chi_+\end{cases}$$ you may consider both solutions ( 4 in total , as you say ) , $$\psi=\begin{cases}\frac{1}{\sqrt{2}}\left [ \psi_1 ( x_1 ) \psi_2 ( x_2 ) +\psi_1 ( x_2 ) \psi_2 ( x_1 ) \right ] \chi_-\\\frac{1}{\sqrt{2}}\left [ \psi_1 ( x_1 ) \psi_2 ( x_2 ) -\psi_1 ( x_2 ) \psi_2 ( x_1 ) \right ] \chi_{_t}\end{cases}$$ the thing is that this is a set of possible solutions , just as what you found out for the spin part with the triplet state , the particles may have this or that state , usually when dealing with fermions the only restriction to take care of is pauli exclusion principle . you may thus consider them all to construct the total wavefunction . now , the thing trimok says , note that , mathematically , you can have a total antisymmetric wave function , without having a specific symmetry in the spatial part or in the spin part , for instance : $$\psi_1 ( x_1 ) \psi_2 ( x_2 ) \alpha ( s_1 ) \beta ( s_2 ) - \psi_2 ( x_1 ) \psi_1 ( x_2 ) \beta ( s_1 ) \alpha ( s_2 ) $$ could be misleading . this can be seen if you construct the first excited state from the slater determinant ( the general expression for $\psi_{_a}$ ) , say , $n_1=1$ , $n_2=2$ , $\alpha ( 1 ) $ , $\beta ( 2 ) $ , i.e. $$\mathcal{s}_1\equiv\frac{1}{\sqrt{2}}\begin{vmatrix}\psi_1 ( x_1 ) \alpha ( 1 ) and \psi_1 ( x_2 ) \alpha ( 2 ) \\\psi_2 ( x_1 ) \beta ( 1 ) and \psi_2 ( x_2 ) \beta ( 2 ) \end{vmatrix}$$ which is the expression given by trimok , but the thing , again , is that you must consider all possible solutions for this state , meaning that for $n_1=1$ , $n_2=2$ , you can have \begin{align}\alpha ( 1 ) , and \ , \alpha ( 2 ) \\\beta ( 1 ) , and \ , \beta ( 2 ) \\\beta ( 1 ) , and \ , \alpha ( 2 ) \\\alpha ( 1 ) , and \ , \beta ( 2 ) \end{align} you can interchange $n_1 , \ , n_2$ also if you please ( there is no new information ) . for the first , the slater determinant pops out the antisymmetric spatial solution times $\chi_\alpha$ , the second , the antisymmetric spatial part times $\chi_\beta$ , but you may take the third , $$\mathcal{s}_2\equiv\frac{1}{2}\begin{vmatrix}\psi_1 ( x_1 ) \beta ( 1 ) and \psi_1 ( x_2 ) \beta ( 2 ) \\\psi_2 ( x_1 ) \alpha ( 1 ) and \psi_2 ( x_2 ) \alpha ( 2 ) \end{vmatrix}$$ and the fourth to build the antisymmetric part times $\chi_+$ as $\mathcal{s}_1+\mathcal{s}_2$ and to build the symmetric part times $\chi_-$ as $\mathcal{s}_1-\mathcal{s}_2$ . here both must be taken in count because of the indistinguishability of particles , considering $\mathcal{s}_1$ or $\mathcal{s}_2$ alone is just insufficient . as i showed first , all this is taken care of if you just factor the spatial and spin parts of the wavefunction and treat each one apart , as you were doing .
choose $\alpha$ as the generalized coordinate , so $y_\text{com}=\frac{1}{2}l \sin\alpha$ , and $x_b=l\cos\alpha$ . then $\delta y_\text{com}=\frac{1}{2}l \cos\alpha \ , \delta\alpha$ , and $\delta x_b=-l\sin\alpha \ , \delta\alpha$ . substitute into the equation .
as a summary of what other answers have already stated , in essence : capacitance is a function of the geometry of the capacitor ( directly proportional to the overlap area of the plates and inversely proportional to it is separation ) and of the relative permittivity of the dielectric employed . once this constructive parameters have been fixed , it is capacitance gets uniquely defined , and so it is constant when the capacitor gets charged and discharged ( assuming linear response , as it is the norm in standard circuits ) . the relation between voltage between the plates of the capacitor , the accumulated charge and it is capacitance is just the definition of the latter . that capacitance is a property of the system which is a function only of geometry and dielectric 's permittivity , is a fact that can be deduced from this definition . so , once you have picked up a capacitance ( by fixing the parameters involved as explained above ) maximization of stored energy in the electric field generated gets down to increasing voltage between the plates as much as possible . nevertheless , increasing the electric field 's strenght has a limit ( the dieletric strength ) , which corresponds to the breakdown voltage of the dielectric , so that is the maximum voltage safely and technically attainable . so : $w_{max}=\displaystyle\frac{1}{2}cv^{2}_{max}\ ; \\v_{max}=e_{max}\cdot d\ ; \\ c=\displaystyle\varepsilon_{0}\varepsilon_{r}\frac{a}{d}\longrightarrow w_{max}=\displaystyle\frac{1}{2}\varepsilon_{0}\varepsilon_{r}e_{max}^{2}\cdot a\cdot d=\displaystyle\frac{1}{2}\varepsilon_{0}\varepsilon_{r}e_{max}^{2}\cdot vol_{\ dielectric}$ corollary : the amount of energy stored in a fully charged capacitor is just obliquely related to it is capacitance . nevertheless , even though the energy of the electric field is directly proportional to the volume of dielectric between the plates ( the product of the plate 's area and their separation ) , for a given amount of dielectric material , the preferred geometry implies a large area and as little separation as possible , because that arrangement allows more compact designs . that is why a larger capacitances leads to larger energy densities in practical applications .
since $$\cos\left ( x-\frac{\pi}{2}\right ) =\sin x , $$ using $\cos$ or $\sin$ does not matter , it depends on the choice of initial conditions . in addition , in general , there will be a initial phase $\phi$ , so sinusoidal wave is written like $$ y ( x , t ) =a \cos ( kx-\omega t+\phi ) . $$
for a plane wave , all cartesian field component space and time dependencies are $exp ( i\ , \vec{k}\ , \vec{x}-i\ , \omega\ , t ) $: time derivatives $f\mapsto \mathrm{d}_t f$ become $f\mapsto\ , -i\ , \omega\ , f$ and vector curls $\vec{f}\mapsto\nabla\times\vec{f}$ become $\vec{f}\mapsto i\ , \vec{k}\times\vec{f}$ . so now we recallampère 's law in time-harmonic form : $$\begin{array}{lclcl} and \nabla\times h and = and \vec{j}+\partial_t\vec{d}\\\rightarrow and \vec{e} and = and \frac{i}{\sigma -i\ , \omega\ , \epsilon}\vec{k}\times\vec{h}\end{array}\tag{1}$$ so you see from this that $\vec{e} , \ , \vec{k} , \ , \vec{h}$ , in that order , form a right handed triple of mutually orthogonal vectors . on the incidence side of the boundary , assumed normal to $\vec{k}$ , there are two such triples : an incident wave with electric field $\vec{e}^+$ and a reflected wave $\vec{e}^-$ ( this one has wavevector $-\vec{k}$ . on the transmission side there is only one triple with electric field $e^t$ . now we simply equate electric fields at the boundary ( they are all in the same direction , so we only need signed scalars henceforth ) , since tangential $\vec{e}$ must be continuous across it : $$e^+ + e^- = e^t\tag{2}$$ and use ( 1 ) to equate the magnetic fields at the boundary so as to fulfill the boundary condition that $\vec{h}$ must be continuous across the boundary . we note that the magnetic fields too are all in the same direction ( orthogonal to the $\vec{e}$s ) , so signed scalars work here too : $$\frac{-1}{\omega\ , \epsilon_0} ( e^+ - e^- ) = \frac{i}{\sigma -i\ , \omega\ , \epsilon}\ , e^t\tag{3}$$ note the minus sign on the lhs between the two $e$s : $e^+ - e^-$: this is because $\vec{k}$ is in the opposite direction for the reflected wave . you should now be able to find $e^-$ and $e^t$ in terms of $e^+$ from ( 2 ) and ( 3 ) and then use maxwell 's relationship $\epsilon = n^2$ to simplify .
if the light is bouncing ( off the mirrors ) in the same direction as b 's spaceship is moving , a would see exactly what b does : a single beam of light bouncing off each mirror alternately , retracing its own path over and over again . remember that the mirrors are moving . so when the light beam travels from the rear mirror to the forward mirror , observer a would actually see a light beam having to catch up to a receding mirror . similarly , when the light beam travels from the forward mirror to the rear mirror , observer a would see the mirror catching up to the light . this means that according to observer a , the light beam travels further each time it goes forward than when it goes backward , as this image shows : even the two halves of the light beam 's trip are not the same length according to a , so clearly a and b have to measure different intervals for at least one of those halves ( and in fact both ) . quantitatively , suppose the relative speed of a and b is $v$ and the distance between the mirrors ( as seen by a ) is $\delta x_a$ . on the forward trip of the light beam , as observed by a , the position of the light beam is described by $x_\text{light} = ct$ and the position of the forward mirror is described by $x_\text{mirror} = \delta x_a + vt$ . the time it takes for the light to reach the mirror is obtained by setting these equal to each other : $$\delta t_\text{forward} = \frac{\delta x_a}{c - v}$$ on the backward trip of the light beam , observer a sees $x_\text{light} = -ct$ and $x_\text{mirror} = -\delta x_a + vt$ , so $$\delta t_\text{backward} = \frac{\delta x_a}{c + v}$$ adding it up , you get a total round-trip time of $$\delta t_{a , \text{total}} = \frac{2c\delta x_a}{c^2 - v^2}$$ now , suppose you want to find the relationship between $\delta x_a$ and $\delta x_b$ , the proper distance ( i.e. . as seen by b ) between the mirrors . hopefully it should be clear that if you look at this from b 's perspective , you get $$\delta t_{b , \text{total}} = \frac{2\delta x_b}{c}$$ if you believe the time dilation formula ( and it sounds like you do ) , you can write $$\delta t_a = \frac{\delta t_b}{\sqrt{1 - \frac{v^2}{c^2}}}$$ and now combining the last three equations leads you to $$\delta x_a = \delta x_b\sqrt{1-\frac{v^2}{c^2}}$$ basically , time dilation is able to account for part of the factor of $\bigl ( 1 - \frac{v^2}{c^2}\bigr ) $ difference between $\delta t_{a , \text{total}}$ and $\delta t_{b , \text{total}}$ , but not all of it . we have to attribute the rest to length contraction . since the only constant in all of this is the speed of light , the only way acceptable to all observers is to measure a distance using c as a yardstick . so . . . imagine that a sees a beam of light start at the ' rear ' of b 's spaceship and make its way forward to the ' front ' of the spaceship . actually , that is not the best way to go about measuring distances , for exactly the reason i described above . as you saw , if you time a light beam traveling from the back of the spaceship to the front ( or from a rear mirror to a forward mirror ) , the time you will actually measure is $\delta t = \frac{\delta x}{c - v}$ , not $\delta t = \frac{\delta x}{c}$ as you thought . ( of course , in a reference frame where the distance being measured is at rest , this works fine since $v = 0$ . ) the easiest and recommended way to measure the distance of a moving object is by sitting still at a point and recording the times when the front and back of the object pass you . once you have the time difference , you can determine the length of the object in your reference frame by $\delta x = v\delta t$ , where $v$ is the object 's speed relative to you . since boosts between different reference frames " mix " time and space , it is easiest to keep your spatial coordinate fixed when you are measuring time , and vice-versa .
as the other answers ( and dmckee 's comments ) note , yes , if you take the square root of a dimensional quantity then you need to take the square root of the units too : $$ \sqrt{4\ ; {\rm kg}} = 2\ ; {\rm kg}^{\frac12} $$ and no , i can not think of any meaningful physical interpretation for the unit ${\rm kg}^{\frac12}$ either . however , in the comments you say that you were " told to plot a graph of distance against square root of mass . " what that means is simply that you should scale the mass axis non-linearly , presumably in order to more clearly show the relationship between the two quantities . for labeling the mass axis , you basically have two choices : label the axis $\sqrt m$ , with equally spaced ticks at , say , $1\ ; {\rm kg}^{\frac12} , 2\ ; {\rm kg}^{\frac12} , 3\ ; {\rm kg}^{\frac12} , 4\ ; {\rm kg}^{\frac12} , \dotsc$ , or label the axis $m$ , with equally spaced ticks at $1\ ; {\rm kg} , 4\ ; {\rm kg} , 9\ ; {\rm kg} , 16\ ; {\rm kg} , \dotsc$ . while , technically , both of these are valid , i would strongly recommend the latter option . just compare these two plots and see which one you find easier to read : $\hspace{60px}$ alas , not all plotting software necessarily supports such axis labeling , or at least does not make it easy , which is why you sometimes see plots with funny units like ${\rm kg}^{\frac12}$ .
evaporation is a different process to boiling . the first is a surface effect that happens at any time , while the latter is a bulk transformation that only happens when the conditions are correct . technically the water is not turning into a gas , but random movement of the surface molecules allows some of them enough energy to escape from the surface into the air . the rate at which they leave the surface depends on a number of factors - for instance the temperature of both air and water , the humidity of the air , and the size of the surface exposed . when the bridge is ' steaming': the wood is marginally warmer than the air ( due to the sun shine ) , the air is very humid ( it has just been raining ) and the water is spread out to expose a very large surface area . in fact , since the air is cooler and almost saturated with water , the molecules of water are almost immediately condensing into micro-droplets in the air - which is why you can see them . btw - steam is completely transparent . if you can see it then it is water vapour . consider a kettle boiling - the white plume only occurs a short distance above the spout . below that it is steam , above it has cooled into vapour .
i will assume in this answer that " drag " means tension . you are asked to find the tension in the chain as it is rotating . this is independent of the link size , so long as the links are not a significant fraction of the circumference . if you have a hoop of mass density per unit length $\rho$ and circumference c ( so that $\rho c = m$ where m is the total mass ) , rotating with rotational velocity $\omega$ , the centripetal force on a segment of length l is the mass times the rotational velocity squared times the radius , or $$ f_c = \rho l w^2 {c\over 2\pi} $$ if the chain is at tension t , the two endpoints of the segment pull in with a total force of $$ {tl\over c} $$ setting the two forces equal , the l drops out ( as it must ) and gives the tension : $$ t = ( \rho c ) \omega^2 {c\over 2\pi} = m \omega^2 {c\over 2\pi} $$ or $\omega= 30 {1\over s}$ , $m= . 4 \mathrm{kg}$ , $c = 1.2 m$ , this is about 68n .
once the battery is disconnected , the charge on the capacitor plates is stuck where it is and has no path to go anywhere else . since the charge remains on the plates , there is an electric field between the plates . and because there is a electric field between the plates there must be a voltage difference between them . we know the voltage was equal to the battery voltage when the battery was connected . and since it does not change when the battery is disconnected , it must still be equal to the battery voltage afterwards .
the answer is no . the simplest proof is just the principle of relativity : the laws of physics are the same in all reference frames . so you can look at that 1-kg mass in a reference frame that is moving along with it . in that frame , it is just the same 1-kg mass it always was ; it is not a black hole .
believe it or not , this was actually a theory held back in the 1990 's ! astronomers back then thought that grbs were the direct result of anti-matter-matter collisions ( between anti-matter comets and matter comets ) that were taking place in the oort cloud . this 1996 article by chuck dermer ( paywall ) , titled gamma-ray bursts from comet-antimatter comet collisions in the oort cloud , discusses the details of how it could be possible . unfortunately , that theory has since been thrown out the window for a few reasons , but the most important one was the connection of supernovae and grbs ( arxiv link ) . the current thinking is that short-duration grbs are caused two neutron stars or two black holes that are merging ( spiraling around each other ) while long-duration grbs are caused by hypernovae ( super-luminous supernovae ) that produce black holes ( which causes the explosion to go outwards in the commonly-shown jet emission , rather than a spherical explosion ) .
suppose you throw the ball upwards at some speed $v$ . then the time it spends in the air is simply : $$ t_{\text{air}} = 2 \frac{v}{g} $$ where $g$ is the acceleration due to gravity . when you catch the ball you have it in your hand for a time $t_{\text{hand}}$ and during this time you have to apply enough acceleration to it to slow the ball from it is descent velocity of $v$ downwards and throw it back up with a velocity $v$ upwards : $$ t_{\text{hand}} = 2 \frac{v}{a - g} $$ note that i have written the acceleration as $a - g$ because you have to apply at least an acceleration of $g$ to stop the ball accelerating downwards . the acceleration $a$ you have to apply is $g$ plus the extra acceleration to accelerate the ball upwards . you want the time in the hand to be as long as possible so you can use as little acceleration as possible . however $t_{\text{hand}}$ can not be greater than $t_{\text{air}}$ otherwise there would be some time during which you were holding both balls . if you want to make sure you are only ever holding one ball at a time the best you can do is make $t_{\text{hand}}$ = $t_{\text{air}}$ . if we substitute the expressions for $t_{\text{hand}}$ and $t_{\text{air}}$ from above and set them equal we get : $$ 2 \frac{v}{g} = 2 \frac{v}{a - g} $$ which simplifies to : $$ a = 2g $$ so while you are holding one 3kg ball you are applying an acceleration of $2g$ to it , and therefore the force you are applying to the ball is $2 \times 3 = 6$ kg . in other words the force on the bridge when you are juggling the two balls ( with the minimum possible force ) is exactly the same as if you just walked across the bridge holding the two balls , and you are likely to get wet !
i would recommend you take a look at the k and j magnetics website , particularly the magnet properties calculator : http://www.kjmagnetics.com/calculator.asp your question has too much ambiguity for a real answer , but i think that perhaps that page will address what you are wondering about .
no , it is not . your system will go through the same point twice in every oscillation , once moving in each direction , and the friction force will be reversed in each pass , so your approach does not work . what you need to consider is the velocity , not the displacement , so $$ma=-kx - \mathrm{sign} ( v ) f_{\mathrm{fric}} . $$ this is not all that helpful in actually figuring out the motion , and to solve that equation you will have to break it down into several parts . also , if static and dynamic friction are different , your mass will stop at its maximum elongation , and you will then have static friction again . this is what causes stick-slip vibrations .
it is just so fast you do not notice it . you will not see the effect of the travel time in something like turning on a light , because your eyes are not fast enough to register the delay , but if you do even moderately precise experiments involving signal transmission and look at it on an oscilloscope , you will find that the travel time is easily measurable . the speed of signal propagation is close to that of light , or about a foot per nanosecond . ( it is worth noting that this is not the speed of electrons moving through the wires , which is dramatically slower . the signal is a disturbance that propagates more rapidly than the drift velocity of electrons in a conductor . )
this diagram shows the difference between closing the tap and pinchng the end of the hose : in both cases you are reducing the area the water has to flow through , and this increases the water velocity in the constriction . the upper diagram shows what happens when you close the tap . closing the tap increases the velocity of the water at the constriction , but as soon as the water is past the constriction is slows down again and it emerges from the end of the hosepipe with a relatively low velocity . the lower diagram shows what happens when you pinch the end of the pipe . the constriction increases the velocity of the water but because the constriction is right at the end the water does not have a chance to slow down again so it leaves the end of the pipe with a relatively high velocity .
try explicitly calcultaing $l_z^{\dagger}$ from the differential representation you correctly gave for $l_z$ . remember that the derivative is by definition an antihermitian operator ( ie $\partial_x^{\dagger}=-\partial_x$ ) which relation you find between $l_z$ and $l_z^{\dagger}$ ? how is this useful to solve your problem ?
you have said : if , for instance , the relative motion observed between two frames of reference is that of uniform acceleration , how can we determine which frame is the unaccelerated system ? it is obviously not possible . and another part of this very question is also : how can we call the occupied frame of reference as being inertial regardless of whether other frames of reference are accelerating with respect to the occupied frame of reference ? both these questions have been answered below . why would it not be possible ? if you are in a reference frame which is accelerating at all , then you will experience pseudo-forces ( forces whose source is not determined in that frame ) . that will tell you that your frame is accelerating . moreover , if the relative motion between two frames is that of uniform acceleration , then both are accelerating ! you do not have to determine which is accelerating ! the presence of acceleration ( uniform or not ) for any reference frame , guarantees that you will experience pseudo-force if you are in it . for example , if you throw a ball from a height , it seems to hit the ground after travelling a path perpendicular to ground . but the actual trajectory is not so . as the ball falls it is deflected due to coriolis force , which is a pseudo-force . so technically the earth is not an inertial frame of reference in any way since we can never point to a source who caused this coriolis force ! you have said : resnick states that the frame of reference he occupies is an unaccelerated one . with respect to what ? if accelerated motion were to be observed with respect to other frames of reference , how are we to determine that we occupy an inertial frame of reference at all ? according to resnick he occupies an inertial frame that means , in his frame , newton 's first law holds true . obviously you need a reference object . when we say a car travels at 75m/s then we actualy mean it travels 75m/s with respect to , say , a stationary tree . but it would travel at 50m/s with respect to another car travelling with 25m/s . so you need a reference object .
the key to this is the physical principle that the quantity you are asking about ( delay between noise and noise cancelling ) carries dimensional information ( i.e. . it is a time ) and therefore it has to depend on the specific situation . the simplest case is trying to cancel out a pure note , with a sinusoidal waveform , then the delay can be as long as you want : you just wait a whole number of periods and it does not change anything . ( the delay precision , though , has to be quite high ! the absolute delay can be as large as you want , but its precision must be much smaller than the pure note 's period ( 1/its frequency ) for the noise cancelling to work . ) a pure note , however , is not really a physical thing . all sounds have a finite duration , and therefore a more physical model is a finite waveform such as ( image source ) here it is clear that you can not wait forever , or the " noise cancelling " will just be an echo . the relevant timescale your delay must act on is that in which the pulse is changing . this is given by the input 's bandwidth : you can represent your noise as a ( fourier ) superposition of pure notes drawn over some interval of frequencies $ [ 0 , \nu_\text{max} ] $ ; the shortest timescales over which the waveform can change are of the order of $1/\nu_\text{max}$ . this is the maximum delay for the noise-cancelling system to be effective . in practice , it works the other way . such systems have a fixed delay , which determines which noise bandwidths they are effective against . designers try to make their headphones effective against most usual sources of noise , but this can not always be managed . some systems ( including the headphones i am currently wearing ) include a choice of bandwidths - mine says " low " and " wide " , best for trains and airplanes or viceversa - to adapt to different noise sources . the bottom line , then , is an emphatic yes to your second question . the delay should be shorter than 1/ ( highest noise frequency ) for whichever source you are dealing with .
when we ask " how strong is this force ? " what we mean in this context is " how much stuff do i need to get a significant amount of force ? " richard feynman summarized this the best in comparing the strength of gravity - which is generated by the entire mass of the earth - versus a relatively tiny amount of electric charge : and all matter is a mixture of positive protons and negative electrons which are attracting and repelling with this great force . so perfect is the balance however , that when you stand near someone else you do not feel any force at all . if there were even a little bit of unbalance you would know it . if you were standing at arm 's length from someone and each of you had one percent more electrons than protons , the repelling force would be incredible . how great ? enough to lift the empire state building ? no ! to lift mount everest ? no ! the repulsion would be enough to lift a " weight " equal to that of the entire earth ! another way to think about it is this : a proton has both charge and mass . if i hold another proton a centimeter away , how strong is the gravitational attraction ? it is about $10^{-57}$ newtons . how strong is the electric repulsion ? it is about $10^{-24}$ newtons . how much stronger is the electric force than the gravitational ? we find that it is $10^{33}$ times stronger , as in 1,000,000,000,000,000,000,000,000,000,000,000 times more powerful !
that is a really good question . you are right that measuring the tranverse velocity is a very difficult measurement , mostly due to andromeda 's distance from the sun . the problem can be tackled in two ways : directly , and indirectly . direct measurements mean actually tracking a positional change between andromeda and even more distant objects assumed to be essentially at rest , like quasars . the recent discovery of water masers mentioned above should make this possible ; a transverse velocity of ~100 km/s is an angular shift on the order of 10 microarcseconds per year . this is much smaller than is possible with optical telescopes ; the extreme baselines of radio telescopes like the very long baseline array , however , do make direct measurements feasible . these observations are currently taking place , and we should have a published measurement within a couple of years . indirect measurements of andromeda 's transverse velocity use a few different techniques . the loeb et al . ( 2005 ) paper made their estimate based on the fact that m33 , a neighboring galaxy to andromeda , shows no sign that its stellar population has been disturbed by passing nearby andromeda . this constrains the possible range of directions and speeds of andromeda 's velocity . they combine this with data on m33 's orbit , plus simulations of how close the galaxies would have to be to show an effect , and estimate both a direction ( mostly eastward ) and speed ( $100 \pm 20$ km/s ) of andromeda 's proper motion . a second indirect method was published by van der marel and guhathakurta in 2008 ; they used information on the orbits of satellite galaxies orbiting m31 to estimate the center of mass ( or barycentre ) of our local group . since the position and velocity of the local group barycentre depend partially on m31 's orbit , they also estimated a transverse velocity . their result is -78 km/s w , -38 km/s n . the upcoming direct measurement of m31 's proper motion should answer which ( if either ) of these other estimates are correct . in addition , we are looking forward to answering several interesting questions regarding both the past and future of our local group of galaxies . stay tuned !
i will sketch one of the eternal inflation variants : " false-vacuum driven eternal inflation " . the idea is that you start with a spacetime manifold , on which is everywhere defined some scalar field . the scalar field ensures that whole region undergoes inflation . to ensure some sort of stability , the value of the field is chosen to be at a local minimum of the potential - it is not at the global minimum and hence tends to be called a false vacuum . now , due to some sort of perturbation , or due to quantum mechanical tunneling , the scalar field ends up , at least at some location , having a value at the other side of the potential barrier . one tunneling mechanism is described by the coleman de luccia instanton . this point with the new tunneled-to $\phi$ value is thought of as the origination point of a small bubble containing a different phase inside the inflating spacetime . it is sometimes called a nucleation point in analogy with bubble nucleation , in which bubbles form on particles suspended in a liquid . the potential is now able to roll down the hill and release energy inside the bubble . the bubble wall expands and the bubble contents continue to expand , but at an ever decreasing rate . anything inside this bubble , i.e. in the forward light cone of the nucleation point has the new $\phi$ value , and hence physics in there sees a different vacuum . the spacetime inside this bubble has a " natural " foliation by 3-dimenional hypersurfaces of constant $\phi$ . as $\phi$ decreases , these surfaces become spatially flat , like our observable universe . these bubbles ( "pocket universes" ) originate in regions where inflation actually ends . so in this soft of model , the orgin of our observed universe , is the end of inflation . outside of the bubble in question , other bubbles are continually nucleating . bubbles may even merge . of course this sort of model is only as good as the predictions it makes for our observable universe . there has been some discussion of the possible observability of bubble nucleation in our past . see here for example . so , getting back to the question , if you want to attach the term " multiverse " to this scenario , i would assume it refers to the ensemble of pocket universes , embedded in an inflating false vacuum spacetime which , at least in this toy model , obeys the usual laws of gr and quantum mechanics .
no , any ellipse is a stable orbit , as shown by johannes kepler . a circle happens to be one kind of ellipse , and it is not any more likely or preferable than any other ellipse . and since there are so many more non-circular ellipses ( infinitely many ) , it is simply highly unlikely for two bodies to orbit each other in a perfect circle .
power - your wifi router puts out about 0.1 - 1.0 w , your microwave oven puts out 1000w . it would take a lot of wifi routers to cook a turkey - more than you think because the antennea on the router is designed to spread the power evenly around the room rather than concentrate it on the center of the oven . there is a danger of being ' cooked ' from being close to very high power transmitters such as some warship 's radar while they are operating . ps . it is the same reason your laser pointer can not be used to cut steel plates ( or james bond ) in half !
if the container full of air is spinning around you , the drag will eventually set you spinning as well , regardless of the rotational speed or the air density . low air density just means that it will take much longer . eventually the air and you will share the same rotation , so that as you speed up , the air and the container will slow down . only in ( complete ) vacuum will you never start spinning . but there is no such thing as a complete vacuum , there are always at least some atoms or molecules around . when the density gets too low , quantum effect will start to take over , as individual particles push you one way or the other .
ok david asked me to bring the rain . here we go . indeed it is very feesible and very efficient to use an electromagnetic accelerator to launch something into orbit , but first a look at our alternative : space elevator : we do not have the tech rockets : you spend most of the energy carrying the fuel , and the machinery is complicated , dangerous , and it cannot be reused ( no orbital launch vehicle has been 100% reusable . spaceshipone is suborbital , more on the distinction in a moment ) . look at the sls that nasa is developing , the specs are not much better than the saturn v and that was 50 years ago . the reason is that rocket fuel is the exact same - there is only so much energy you can squeeze out of these reactions . if there is a breakthrough in rocket fuel that is one thing but as there has been none and there is none on the horizon , rockets as an orbital launch vehicle are dead end techs which we have hit the pinnacle of . cannons : acceleration by a pressure wave is limited to the speed of sound in the medium , so you cannot use any explosive as you will be limited by this ( gunpowder is around $2\text{ km/s}$ , this is why battleship cannons have not increased in range over the last 100 years ) . using a different medium you can achieve up to 11km/s velocity using hydrogen . this is the regime of ' light gas guns ' and a company wants to use this to launch things into orbit . this requires high accelerations ( something ridiculous like thousands of $\mathrm{m/s^2}$ ) which restricts you to very hardened electronics and material supply such as fuel and water . maglev : another company is planning on this ( http://www.startram.com/ ) but if you look at their proposal it requires superconducting loops running something like 200ma generating a magnetic field that will destroy all communications in several states , i find this unlikely to be constructed . electromagnetic accelerator ( railgun ) : this is going to be awesome ! there is no requirement on high accelerations ( a railgun can operate at lower accelerations ) and no limit on upper speed . the tech for this exists and there have been papers out on it , here are two of them . http://www.westphalianarms.com/low-cost_launch_2.pdf http://www.westphalianarms.com/ieee.em.pdf some quick distinctions , there is suborbital and orbital launch . suborbital can achieve quite large altitudes which are well into space , sounding rockets can go up to 400miles and space starts at 60miles . the difference is if you have enough tangential velocity to achieve orbit . for $1\text{ kg}$ at $200\text{ km}$ from earth the energy to lift it to that height is $0.5 m g h = 1\text{ mj}$ , but the tangential velocity required to stay in orbit is $m v^2 / r = g m m / r^2$ yielding a $ke = 0.5 m v^2 = 0.5 g m m / r = 30\text{ mj}$ , so you need a lot more kinetic energy tangentially . to do anything useful you need to be orbital , so you do not want to aim your gun up you want it at some gentle angle going up a mountian or something . the papers i cited all have the railgun going up a mountian and about a mile long and launching water and cargo . that is because to achieve the $6\text{ km/s}+$ you need for orbital velocity you need to accelerate the object from a standstill over the length of your track . the shorter the track the higher the acceleration . you will need about 100 miles of track to drop the accelerations to within survival tolerances nasa has . why would you want to do this ? you just need to maintain the power systems and the rails , which are on the ground so you can have crews on it the whole time . the entire thing is reusable , and can be reused many times a day . you can also just have a standard size of object it launches and it opens a massive market of spacecraft producers , small companies that can not pay 20 million for a launch can now afford the 500,000 for a launch . the electric costs of a railgun launch drops to about 3\$/kg , which means all the money from the launch goes to maintenance and capital costs and once the gun is paid down prices can drop dramatically . it is the only way that humanity has the tech for that can launch large quantities of object and in the end it is all about mass launched . noone has considered having a long railgun that is miles long because it sounds crazy right off the bat , so most proposals are for small high-acceleration railguns as in the papers above . the issue is that this limits what they can launch and as soon as you do that noone is very much interested . why is a long railgun crazy ? in reality it is not , the raw materials ( aluminum rails , concrete tube , flywheels , and vacuum pumps ) are all known and cheap . if they could make a railroad of iron 2000miles in the 1800s why can not we do 150miles of aluminum in the 2000s ? the question is of money and willpower , someone needs to show that this will work and not just write papers about this but get out there and do it if we ever have a hope of getting off this rock as a species and not just as the 600 or so that have gone already . also the large companies and space agencies now are not going to risk billions into a new project while there is technology which has been perfected and proven for the last 80 years that they could use . there are a lot of engineering challenges , some of which i and others have been working on in our spare time and have solved , some which are still open problems . i and several other scientists who are finishing/have recently finished their phds plan on pursuing this course ( jeff ross and josh at solcorporation . com , the website is not up yet because i finished my phd 5 days ago but it is coming ) . conclusions yes it is possible , the tech is here , it is economic and feesible to launch anything from cargo to people . it has not gotten a lot of attention because all the big boys use rockets already , and noone has proposed a railgun that can launch more than cargo . but it has caught the attention of some young scientists who are going to gun for this , so sit back and check the news in a few years .
i think it could be simply that the dirac operator is invariant under isometries , so if $\phi$ is an isometry and $\psi$ a solution to $$d\psi = 0 , $$ then $\phi^* \psi$ is also a solution , where $\phi^*$is pullback . then it would be similar to how harmonic functions $f$ on the sphere -- $\nabla^2 f = 0$ -- come in representations of the rotation group , the $y^l_m$ . in more detail if $\phi$ is a diffemorphism , that in coordinates takes the form $y^\mu = y^\mu ( x^\nu ) $ ( not a tensor expression ) , and $v^\mu$ is a vector field , then we can define a vector field $$ ( \phi_* v^\mu ) ( \phi ( p ) ) = \frac{\partial y^\mu}{\partial x^\nu} v^\nu ( p ) $$ called the pushforward of $v^\mu$ . naturally we can pushforward any tensor , in particular the metric . by definition $\phi$ is an isometry if $$ ( \phi_* g_{\mu\nu} ) ( \phi ( p ) ) = g_{\mu\nu} ( \phi ( p ) ) . $$ this means that if we have any tetrad ( also known as a vierbein or a frame ) , that is a set of vector fields $e_a^\mu$ such that $e_{a\mu} e^\mu_b = \eta_{ab}$ for some symmetric matrix $\eta_{ab}$ with signature $+---$ , it is pushed forward to another tetrad . i let $\eta_{ab}$ be general because in spinor problems it is more natural to use a null tetrad $$\eta_{ab} = \begin{pmatrix} 0 and 1 and 0 and 0 \\ 1 and 0 and 0 and 0 \\ 0 and 0 and 0 and -1 \\ 0 and 0 and -1 and 0\end{pmatrix} . $$ since $\eta_{ab}$ has zeros on the diagonal all the tetrad vectors are null . it is well known ( see for example spinors and space-time or the newman-penrose paper ) that to every null tetrad corresponds exactly two bases for two-spinors , called dyads , say $ ( o^a , \iota^a ) $ and $ ( -o^a , \iota^a ) $ . thus at least for isometries connected to the identity , the pushforward of tetrads lifts to a pushforward of dyads . ( however when the isometry group is not simply connected this might not be continuous globally , but i think it does not matter here , since we can consider isometries close to the identity , which will take us to lie algebra representations , and then we integrate them , and discard the representations that require passing to the simply connected cover . ) since we can pushforward dyads we can pushforward two-spinors ( by linearity ) , since we can pushforward two-spinors we can pushforward dirac spinors . $\newcommand{\dslash}{\ ! \not d}$ in particular for a dirac spinor $\psi$ , $\dslash\psi$ is of course also a dirac spinor , so $$\beta = \phi_* ( \dslash\psi ) = \phi_* ( \dslash \phi^* \phi_* \psi ) $$ makes sense , where $\phi^*$ as the inverse of $\phi_*$ so the second equality is just inserting the identity . now $\phi_* \dslash \phi^*$ defines a differential operator , it is the transformed dirac operator under the isometry $\phi$ . but since the dirac operator is defined by the metric and $\phi$ preserves the metric , this must be just the dirac operator again . ( you can probably make this argument more convincing . ) thus we have established that $$\beta = \dslash ( \phi_*\psi ) . $$ in particular if $\beta = 0$ , so that $\psi$ is a zero mode for the dirac operator , then $\tilde{\psi} = \phi_* \psi$ is also a solution . thus the isometry group ( or at least its lie algebra ) acts on zero modes .
it is because the atoms are arranged in a long chain that interacts mostly with itself , and very little ( at least for the electrons of interest ) with atoms in other chains . a better term might be " quasi-1d " since of course the atoms themselves are 3d , but 1d does convey the key idea that the parts of interest are interacting along a single dimension of space . quantum mechanics does very odd things when you insist that the wavelike properties of matter be limited to lines , planes , or for that matter points ( quantum does , atoms ) . you can see one reason by thinking about waves in tunnels : they do not dissipate ! a blast deep within a tunnel has nearly the same force when it exits it does when it happens deep in the tunnel , which is why explosive trucks are banned from long tunnels . in quantum mechanics your waves are further constrained by the need to arrive at a resonant , repeating pattern , which somewhat ironically is called a " stationary " solution since whatever it is does not appear to be moving when examined from our classical perspective . for long chains , that means that any long waves ( e . g . , conduction electrons in a metal ) must stabilize into solutions that are topologically similar to ordinary skip ropes , ones that can have one , two , three , or many more loops . for a semiconductor such as cdse you have more complicated electron configurations and energy levels , but you still maintain that need to settle into nicely resonant solutions . that in turn can lead to really interesting electronic and optical behaviors , which is why the fields of 1d and 2d ( and 0d , quantum dots ) have had and continue to have a lot of interesting materials research going on in them .
without seeing the quote/context i can only imagine that it means something like : if you take , say , a cube moving at close to c in the z direction , then ( in the frame in which it is moving ) its z extent gets lorentz contracted to virtually zero , so it is effectively now a square in the xy plane and has only the degrees of freedom that a square in the xy plane has .
you may just not bother to use a test function , here . this problem is so easy you can work it all just using the properties of the commutator . $$ [ xp_y , x ] =x [ p_y , x ] + [ x , x ] p_y$$ now $ [ p_y , x ] $ vanishes because of the fundamental commutation relation between $p_i$ and $x_i$ which is $$ [ p_i , x_j ] = -i\hbar \delta_{ij}$$ on the other hand $ [ x , x ] =0$ because anything commmutes with itself .
we start by mentioning a couple of standard formulas $$\tag{1} \psi ( x ) ~=~\langle x | \psi \rangle , $$ and $$\tag{2} \langle x | y \rangle ~=~\delta ( x-y ) . $$ the canonical commutation relation ( ccr ) is $$\tag{3} [ \hat{x} , \hat{p} ] ~=~i\hbar{\bf 1} . $$ the standard schrödinger position representation reads $$\tag{4}\hat{x}~=~x , \qquad \hat{p}~=~-i\hbar\frac{\partial}{\partial x} . $$ we may conjugate the standard schrödinger position representation ( 4 ) by an unitary operator $\hat{u}=e^{-if ( \hat{x} ) }$ , where $f:\mathbb{r}\to\mathbb{r}$ is a given differentiable function . in this way we obtain an unitary equivalent position representation $$\tag{5}\hat{x}~=~x , \qquad \hat{p} ~=~-i\hbar e^{-if ( x ) }\frac{\partial}{\partial x}e^{if ( x ) } ~=~-i\hbar\frac{\partial}{\partial x}+ \hbar f^{\prime} ( x ) , $$ of the ccr ( 3 ) . the standard schrödinger position representation ( 4 ) corresponds to $f\equiv {\rm const}$ . for a general irreducible representation of the ccr ( 3 ) , see the stone-von neumann theorem . the representation ( 5 ) implies $$\tag{6} \langle x | \hat{p} |\psi \rangle~=~ ( \hat {p} \psi ) ( x ) ~=~-i\hbar e^{-if ( x ) } ( e^{if}\psi ) ^{\prime} ( x ) ~=~-i\hbar\psi^{\prime} ( x ) + \hbar f^{\prime} ( x ) \psi ( x ) . $$ from ( 6 ) we conclude that the momentum matrix elements reads $$\tag{7} \langle x | \hat{p} |y \rangle~=~-i\hbar\delta^{\prime} ( x-y ) + \hbar f^{\prime} ( x ) \delta ( x-y ) $$ in the representation ( 5 ) . finally , here and here are two other phys . se posts that also discuss ambiguities in $x\leftrightarrow p$ overlaps .
in the standard model , fermion number is not conserved . lepton number is conserved , because of an accidental symmetry . one cannot write down a renormalizable , gauge and lorentz invariant operator that violates lepton number conservation in the standard model . a majorana neutrino would violate lepton number conservation by two units . to see this , consider , e.g. neutrinoless double beta decay . you can draw feynman diagrams with two $w^-e^-v_e$ vertices in which two incoming $w$-bosons each decay into an electron and an electron-neutrino . the two electron-neutrinos annihilate ( possible because they are majorana particles ) , leaving a final state of two elecrons , violating lepton number conservation by $2$ units . you can see that majorana neutrinos violate lepton number conservation by $2$ units from the mass term . the mass term , $$ \mathcal{l} = \frac12 m \psi^t c^{-1}\psi , $$ is not invariant under the $u ( 1 ) $ lepton number symmetry , $\psi\to\exp ( il\theta ) \psi$ . it picks up a phase of twice the lepton number of the neutrino , i.e. $\delta l=2$ rather than $\delta l=0$ . a majorana neutrino cannot be charged under a $u ( 1 ) $ symmetry . because there is not a lepton number $u ( 1 ) $ symmetry , there is no conserved noether charge corresponding to lepton number .
lorentz came with a nice model for light matter interaction that describes dispersion quite effectively . if we assume that an electron oscillates around some equilibrium position and is driven by an external electric field $\mathbf{e}$ ( i.e. . , light ) , its movement can be described by the equation $$ m\frac{\mathrm{d}^2\mathbf{x}}{\mathrm{d}t^2}+m\gamma\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}+k\mathbf{x} = e\mathbf{e} . $$ the first and third terms on the lhs describe a classical harmonic oscillator , the second term adds damping , and the rhs gives the driving force . if we assume that the incoming light is monochromatic , $\mathbf{e} = \mathbf{e}_0e^{-i\omega t}$ and we assume a similar response $\xi$ , we get $$ \xi = \frac{e}{m}\mathbf{e}_0\frac{e^{-i\omega t}}{\omega^2-\omega^2-i\gamma\omega} , $$ where $\omega^2 = k/m$ . now we can play with this a bit , using the fact that for dielectric polarization we have $\mathbf{p} = \epsilon_0\chi\mathbf{e} = ne\xi$ and for index of refraction we have $n^2 = 1+\chi$ to find out that $$ n^2 = 1+\frac{ne^2}{\epsilon_0 m}\frac{\omega^2-\omega^2+i\gamma\omega}{ ( \omega^2-\omega^2 ) ^2+\gamma^2\omega^2} . $$ clearly , the refractive index is frequency dependent . moreover , this dependence comes from the friction in the electron movement ; if we assumed that there is no damping of the electron movement , $\gamma = 0$ , there would be no frequency dependence . there is another possible approach to this , using impulse method , that assumes that the dielectric polarization is given by convolution $$ \mathbf{p} ( t ) = \epsilon_0\int_{-\infty}^t\chi ( t-t' ) \mathbf{e} ( t' ) \mathrm{d}t ' . $$ using fourier transform , we have $\mathbf{p} ( \omega ) = \epsilon_0\chi ( \omega ) \mathbf{e} ( \omega ) $ . if the susceptibility $\chi$ is given by a dirac-$\delta$-function , its fourier transform is constant and does not depend on frequency . in reality , however , the medium has a finite response time and the susceptibility has a finite width . therefore , its fourier transform is not a constant but depends on frequency .
what is a physical theory/model ? a given physical theory is typically mathematically modeled by some set $\mathscr o$ of mathematical objects , and some rules that tell us how these objects correspond to a physical system and allow us to predict what will happen to that system . for example , many systems in classical mechanics can be described by a pair $ ( \mathcal c , l ) $ where $\mathcal c$ is the configuration space of the system ( often a manifold ) , and $l$ is a function of paths on that configuration space . this model is then accompanied by rules like " the elements of $\mathcal c$ correspond to the possible positions of the system " and " given an initial configuration of the system and it is initial velocity , the euler-lagrange equations for $l$ determine the configuration and velocity of the system for later times . " what is a symmetry ? if we think of physics as being a collection of such models , we can define a symmetry of a system as a transformation on the set $\mathscr o$ of objects in the model such that the transformed set $\mathscr o'$ of objects yields the same physics . note , i am deliberately using the somewhat vague phrase " yields the same physics " here because what that phrase means depends on the context . in short : a symmetry is transformation of a model that does not change the physics it predicts . for example , for the model $ ( \mathcal c , l ) $ above , one symmetry would be a transformation that maps the lagrangian $l$ to a new lagrangian $l'$ on the same configuration space such that the set of solutions to the euler-lagrange equations for $l$ equals the set of solutions to the euler-lagrange equations for $l'$ . even in this case , it is interesting to note that $l$ need not be invariant under the transformation for this to be the case . in fact , one can show that it is sufficient for $l'$ to differ from $l$ by a total time derivative . this brings up an important point ; a symmetry does not necessarily need to be an invariance of a given mathematical object . there exist symmetries of physical systems that change the mathematical objects that describe the system but that nonetheless leave the physics unchanged . another example to emphasize this point is that in classical electrodynamics , one can make describe the model in terms of potentials $\phi , \mathbf a$ instead of in terms of the fields $\mathbf e$ and $\mathbf b$ . in this case , any gauge transformation of the potentials will lead to the same physics because it will not change the fields . so if we were to model the system with potentials , then we see that there exist transformations of the objects in the model that change them but that nonetheless lead to the same physics . how do groups relate to all of this ? often times , the transformations of a model that one considers form actions of groups . a group action is a kind of mathematical object that associates a transformation on a given set with each element of the group in such a way that the group structure is preserved . take , for example , the system $ ( \mathcal c , l ) $ from above . suppose that $\mathcal c$ is the configuration space of a particle moving in a central force potential , and $l$ is the appropriate lagrangian . one can define an action $\phi$ of the group of $g= \mathrm{so} ( 3 ) $ of the set of rotations $r$ one the space of admissible paths $\mathbf x ( t ) $ in configuration space as follows : \begin{align} ( \phi ( r ) \mathbf x ) ( t ) = r\mathbf x ( t ) . \end{align} then one can show that the lagrangian $l$ of the system is invariant under this group action . therefore , the new lagrangian yields the same equations of motion and therefore the same physical predictions . often times the objects describing a given model involve a vector space . for example , the state space of a quantum system is a special kind of vector space called a hilbert space . in such cases , it is often useful to consider a certain kind of group action called a group representation . this leads one to study an enormous and beautiful subject called the representation theory of groups . are groups the end of the story ? definitely not . it is possible for symmetries to be generated by other kinds of mathematical objects . a common example is that of symmetries that are generated by representations of a certain kind of mathematical object called a lie algebra . in this case , as in the case of groups , one can then study the representation theory of lie algebras which is , itself , also an huge , rich field of mathematics . even this is not the end of the story . there are all sorts of models that admit symmetries generated by more exotic sorts of objects like in the context of supersymmetry where one considers objects called graded lie algebras . most of the mathematics of this stuff falls , generally , under the name of representation theory .
obvouusly the highest number of splashes will be when the impact happens at right angle . this is because at such impact all the kinetic energy of the drop at one moment goes into the forces directed into different directions and tears the drop apart . conversely the least splashes will be when the drop impacts at narrow angle . in this case the drop continues sliding over the surface and keeps its integrity while kinetic energy slowly transforms into heat due to friction . the surface tension keeps the drop intact because all parts of the drop keep the same movement direction . as such i would recommend a form which meets the drop in the most probable impact place with a sloppy angle . evidently , such form should not be symmetric ( because all even functions have zero derivative at zero ) . the only variant of yours that has such property is the fourth . additionally it changes the drop 's velocity vector to the right direction so that any splaches that can occur will go to the right where it has a border that covers the greatest angle of possible splashes path out than all other variants . the slope 's curvature should be such that the amount of energy converted into heat was uniform along the drop 's path and the pressure never exceed the surface tension . so when the drop has the highest speed , the angle between the forces acting on the drop ( inertia and gravity combined ) and the surface should be the smallest , but as long as the drop slows , the angle slows the angle should rise so to keep the component which is perpendicular to the surface constant . the fourth picture roughly satisfies this criterion . thus the fourth variant in the best .
the speed of sound in a liquid is given by : $$ v = \sqrt{\frac{k}{\rho}} $$ where $k$ is the bulk modulus and $\rho$ is the density . the bulk modulus of mercury is $2.85 \times 10^{10}$ pa and the density is $13534$ kg/m$^3$ , so the equation gives $v = 1451$ m/sec . the speed of sound in solids is given by : $$ v = \sqrt{\frac{k + \tfrac{4}{3}g}{\rho}} $$ where k and g are the bulk modulus and shear modulus respectively . the bulk modulus of iron is $1.7 \times 10^{11}$ pa , the shear modulus is $8.2 \times 10^{10}$ pa and the density is $7874$ kg/m$^3$ , so the equation gives $v = 5956$ m/sec . you give a slightly different figure for the speed of sound in iron , but the speed does depend on the shape and the figure you give , $5130$ m/sec , is the speed in a long thin rod . there are more details in the wikipedia article i have linked .
the answer is no . the pole would bend/wobble and the effect at the other end would still be delayed . the reason is that the force which binds the atoms of the pole together - the electro-magnetic force - needs to be transmitted from one end of the pole to the other . the transmitter of the em-force is light , and thus the signal cannot travel faster than the speed of light ; instead the pole will bend , because the close end will have moved , and the far end will not yet have received intelligence of the move . edit : a simpler reason . in order to move the whole pole , you need to move every atom of the pole . you might like to think of atoms as next door neighbours if one of them decides to move , he sends out a messenger to all his closest neighbours telling them he is moving . then they all decide to move as well , so they each send out messengers to to their closest neighbours to let them know they are moving ; and so it continues , until the message to move has travelled all the way to the end . no atom will move until he has received the message to do so , and the message will not travel any faster than all the messengers can run ; and the messengers can not run faster than the speed of light . /b2s
i might be mistaken , but : even though the molecule is inside the event horizon ( relative to a distant observer ) , from the point of view of the molecule the event horizon is still ahead of it , and has not yet been reached . the inner atom would still be able to communicate with the outer atom well after they have both crossed the event horizon from our point of view . it is only when the molecule is much closer to the singularity ( i.e. . about to be spaghettified ) , that the inner atom will disappear into the event horizon relative to the outer atom ; any bond between them will be broken , and any charge of the inner atom will be added to the charge of the black hole .
he just means that an object which has a three-dimensional structure with no symmetries cannot be turned from a left-handed version to a right-handed version using rotations alone . you can not rotate a left-hand glove to be a right-hand glove . a vector can be inverted by rotating it , but this does not invert a general rigid body , because a vector only has one axis , not three . the proof of the statement that rotations cannot invert is by the continuity of the determinant function . the determinant of a rotation is always 1 , and of a reflection-rotation is -1 . the determinant cannot smoothly go from 1 to -1 .
if the moving mass is small enough , you can do this using the geodesic equation , $$\frac{\mathrm{d}^2x^\lambda}{\mathrm{d}t^2} + \gamma^{\lambda}_{\mu\nu}\frac{\mathrm{d}x^\mu}{\mathrm{d}t}\frac{\mathrm{d}x^\nu}{\mathrm{d}t} = 0$$ this is essentially the general relativistic equivalent of newton 's second law : it is a differential equation that governs how a test particle 's position changes in time . the connection coefficients $\gamma^{\lambda}_{\mu\nu}$ ( also called christoffel symbols ) can be calculated from the metric . so if the metric is known , even if it is time-dependent , you can calculate the connection coefficients in your desired reference frame , plug them in , and find a solution to the geodesic equation that tells you how the particle will move . there may not be an analytic solution , but in all but the most extreme cases you can either solve the equation numerically , or make some approximation that might make it analytically solvable . if the moving particle is not small enough to be considered a test particle , then the situation becomes more complicated because the metric , and thus the connection coefficients , will depend on the motion of the test particle itself . so you wind up with a coupled system of three equations : the geodesic equation for the moving particle , the geodesic equation for the other particle , and the einstein equation $$g^{\mu\nu} = 8\pi t^{\mu\nu}$$ which tells you how the metric changes in response to the motion of the two particles . in this case it is highly unlikely that you could find an analytic solution , but you could potentially still use a numeric differential equation solver , at least for some range of time . ( all the equations are nonlinear so it is likely that your solution would lose accuracy quickly . )
take the left side of eqn . ( 2 ) : $-\phi_{t , t , } ( x , t ) +\delta\phi ( x , t ) $ since this should be a non linear wave equation , $\phi_{t , t}$ means the second derivation in time of $\phi$ . now we make a variable change . first , set $\zeta^i/\varepsilon= x^i$ ( eqn . ( 6 ) ) . we put that into $\delta=\sum\frac{\partial^2}{\partial x_i^2}$ , which yields $\delta=\varepsilon^2\sum\frac{\partial^2}{\partial \zeta_i^2}\equiv\tilde{\delta}$ . now make the second variable change : $\tau/\omega ( \varepsilon ) =t$ ( eqn . ( 7 ) ) . compute the second derivation in time as needed : $\frac{\partial^2}{\partial t^2}= [ \omega ( \epsilon ) ] ^2\frac{\partial^2}{\partial \tau^2}$ . our function will change from $\phi ( x , t ) \to \phi ( \zeta , \tau ) $ . plug the transformed operators into the left side as well and we get : $-\frac{\partial^2}{\partial t^2}\phi ( x , t ) +\delta\phi ( x , t ) =- [ \omega ( \epsilon ) ] ^2\frac{\partial^2}{\partial \tau^2}\phi ( \zeta , \tau ) +\epsilon^2\tilde{\delta}\phi ( \zeta , \tau ) $ this is now the left side of eqn . ( 9 ) . they only renamed the operators again and left away the dependencies : $\frac{\partial^2}{\partial \tau^2}\phi ( \zeta , \tau ) =\ddot{\phi} \qquad \tilde{\delta}=\delta$ i hope this makes it clear for you . edit : additional question ok . how to gain eqns . ( 10-12 ) . write down the $\varepsilon$-expansions for $\omega$ and $\phi$ up to third order in $\varepsilon$ ( eqns . ( 5 ) and ( 8 ) ) : $\omega^2=1+\sum\varepsilon^k \omega_k=1+\varepsilon\omega_1+\varepsilon^2\omega_2+\varepsilon^3\omega_3+\dots$ $\phi=\sum\varepsilon^k \phi_k=\varepsilon\phi_1+\varepsilon^2\phi_2+\varepsilon^3\phi_3+\dots$ take the derivatives on $\phi$ which you need according to eqn . ( 9 ) and put everything together : $- [ 1+\varepsilon\omega_1+\varepsilon^2\omega_2+\varepsilon^3\omega_3 ] [ \varepsilon\ddot\phi_1+\varepsilon^2\ddot\phi_2+\varepsilon^3\ddot\phi_3 ] = \varepsilon\phi_1+\varepsilon^2\phi_2+\varepsilon^3\phi_3 + \sum g_k\phi^k$ i will show you what is the principle and derive eqn . ( 10 ) . you will have to do the rest by your own . we now search for the first order solution . that means we will only recognize that part of the equation where we find the first order of $\varepsilon$ and we will throw everything else away . means , for first order only write down what has a $\varepsilon^1$ , for the second order you will have to write down everything with a $\varepsilon^2$ . on the right side you will get only $\varepsilon \phi_1$ since everything else is higher order in $\epsilon$ except the sum , which is zero order . on the left side you will maintain only $-\varepsilon\ddot\phi_1$ since everything else is of higher order . so we ended up with $-\varepsilon\ddot\phi_1=\varepsilon \phi_1 \leftrightarrow \ddot\phi_1+\phi_1=0$ which is eqn . ( 10 ) .
the induction cooker works by passing a rapidly changing magnetic field through the thing to be heated . that rapidly changing magnetic field creates an emf ( electro motive force ) field . ( faraday 's law ) . this is like a voltage . current will flow , proportional to the conductivity . no conductivity , no current . low conductivity , low current . high conductivity , high current . heat will be produced according to i^2 r . so there will be a current in the salt water , and it will be heated . but not very much . its conductivity is much less than copper or iron , so it will not have a very large current or amount of heating . as for iron or other magnetic materials , they might be part of the design , required to concentrate the magnetic field , but in general , induction heating is from currents induced in a conductor . many weeks later : those darn induction cookers are advertising everywhere . sure enough , their writeup says you need some magnetic properties to the cookware . their explanation as to why , something like the first line in jj fleck 's answer , is garbled and not exactly correct . the addendum of fleck 's answer has it spot on , and with references too .
if $p^{a}$ are finite-dimensional matrices , then i found that your algebra actually implies that $p^{a}=0$ . i think that it is a consequence of the fact that $su\left ( n\right ) $ is a simple group , i.e. , there is no a normal subgroup in $su ( n ) $ . if we assume that $p^{a}$ are hermitian then your identities : $$ \left [ p^{a} , p^{b}\right ] =0 , \qquad\left [ q^{a} , p^{b}\right ] =if^{abc}p^{c} , \qquad\qquad ( 1 ) $$ are the algebra of an invariant subgroup . the formal proof that $p^{a}=0$ is as follows . if $p$ satisfies the algebra ( 1 ) then a linear independent subset of $p$ also satisfies eq . ( 1 ) , therefore without loss of generality we can assume that all $p^{a}$ are linear independent . let me now split $p$ into the hermitian and anti-hermitian parts : $$ p=\frac{p+p^{\dagger}}{2}+i\frac{p-p^{\dagger}}{2i}=r+ii , $$ where $r$ and $i$ are hermitian matrices . taking into account that all $f^{abc}$ are real and $$ \left [ q^{a} , p^{b}\right ] =if^{abc}p^{c}\quad\longrightarrow\qquad\left [ q^{a} , p^{\dagger b}\right ] =if^{abc}p^{\dagger c} , $$ one can conclude that : $$ \left [ q^{a} , r^{b}\right ] =if^{abc}r^{c} , \qquad\left [ q^{a} , i^{b}\right ] =if^{abc}i^{c} . $$ therefore $r^{a}$ and $i^{a}$ are hermitian traceless matrices thus they can be expressed as linear combinations of $q$ . thus , we find that matrices $p^{a}$ are linear combinations of $q$: $$ \qquad\qquad\qquad\qquad\qquad p^{a}=m^{ab}q^{b} , \qquad\qquad\qquad\qquad\qquad ( 2 ) $$ where $m^{ab}$ is some $\left ( n^{2}-1\right ) \times\left ( n^{2}-1\right ) $ compex-valued matrix . again from eq . ( 1 ) we obtain : $$ m^{ad}\left [ q^{d} , p^{b}\right ] =\left [ p^{a} , p^{b}\right ] =im^{ad}% f^{dbc}p^{c}=0 . $$ since all $p^{e}$ are linear independent then $m^{ad}f^{dbc}=0$ , thus $0=m^{ad^{\prime}}f^{d^{\prime}bc}f^{dbc}=c_{a}m^{ad}=0$ , hence $p^{a}=0 . $ there is another way to show the same . let 's consider the commutator $\left [ q^{a} , p^{b}\right ] $ and use the relation ( 2 ) : $$ \left [ q^{a} , p^{b}\right ] =m^{bc}\left [ q^{a} , q^{c}\right ] =m^{bc} if^{ace}q^{e}=if^{abc}p^{c}=if^{abc}m^{ce}q^{e} $$ comparing the coefficient of $q^{e}$ , we find : $$ m^{bc}f^{ace}=f^{abc}m^{ce}\quad\longrightarrow\qquad m^{bc}\left ( c^{a}\right ) _{ce}=\left ( c^{a}\right ) _{bc}m^{ce}\quad\longrightarrow \qquad\left [ m , c^{a}\right ] =0 , $$ where $\left ( c^{a}\right ) _{ce}=-if^{ace}$ are the generators of irreducible adjoined representation . therefore , according to schur 's lemma $m$ is a scalar matrix , i.e. , $m^{ab}=\lambda\delta^{ab}$ . if one requires that $p$ should be commutative then $\lambda=0 . $ the question is what about the case where $p^{a}$ are not finite-dimensional matrices . but in this case i do not know how to define the trace .
as wikipedia will inform you , a dipole magnet in a magnetic field $\mathbf b$ will be subject to a torque $$\mathbf \tau=\mathbf m\times\mathbf b , $$ where the magnetic dipole $\mathbf m$ points from the magnet 's south pole to its north pole . thus the equilibrium positions are parallel and antiparallel to the field , so the situation will look like your first picture . note also that there are two orientations for the magnet , of which one will be stable and one will be unstable . to discriminate between the two , you need to choose the configuration that minimizes the energy , $$u=-\mathbf m\cdot\mathbf b , $$ so $\mathbf m$ will be parallel to the magnetic field . ( to find out in which direction that goes , see this answer .
a high vacuum will also break down under a sufficiently high dc field . under an increasing dc voltage , small projections on the negative electrode ( called " asperities" ) begin to experience a localized electric field that is sufficient to initiate field emission , often accompanied by intense localized heating , thermoelectric emission , or even explosive emission of electrons and ejection of cathode material into the gap . field emission is a function of the cathode material 's work function - i.e. , how easy electrons can be stripped from the cathode . each of these events generates a microplasma at the site of the asperity , and electrically , each event shows up as a small spike of current through the gap . under somewhat higher voltages , one of these events may culminate in a spark that completely bridges the gap . if driven from a low impedance source , the spark may evolve into a sustained vacuum arc . vacuum arcs often cause significant heating and surface melting of the anode as well as the cathode . a high voltage vacuum gap ( as in a transmitting vacuum tube cathode-anode , vacuum capacitor , or vacuum switch ) can be " conditioned " to operate at higher voltages by using an adjustable hv supply , current limiting resistor , and a small capacitor . any asperities are melted or vaporized by the controlled energy in the hv capacitor , but damaging arcing is prevented by the resistor . the high voltage is slowly increased until the gap can operate at maximum voltage without further breakdown . this process is also called " spot knocking " . although the mechanisms are different , both air gaps and vacuum gaps may show pre-breakdown current pulses . and , once spark breakdown or arcing occurs , the negative resistance vi characteristics are also similar for gaseous and vacuum sparks and arcs . however , when current flow is temporarily stopped ( as during a current zero-crossing in an ac circuit ) , a vacuum arc will usually recover its dielectric strength more quickly than an arc within a gas . a good discussion can be seen here : http://cartan.e-moka.net/content/download/409/2311/file/ptv.pdf
yes it fluctuates but it is a very small fluctuation . note that unstable particles have a decay rate or width $\gamma$ that is related to its lifetime $\tau$ by $$ \gamma=\frac{\hbar}{\tau} $$ when you measure the mass/energy of such particles in experiments you always get a lorentzian or breit-wigner distribution like this from which you can measure the width and calculate both the mass ( with an uncertainty ) and the lifetime ( from the width measurement ) . note that this is for all unstable particles , even fundamental ones , then only stable particles as the electron have a perfectly defined mass . the issue with the electron is that it is lifetime is very very long ( otherwise aggregated matter would not exist ) , in fact proton decay has never been observed , though theoretical decay modes exist in some models . so it is considered to be effectively stable and both experiments and theoretical calculation set limits for its lifetime . but it still has a lifetime in principle so its mass is not perfectly defined , the fluctuation is really small though .
you claim that according to the principle of relativity the light will not strike the detector in the reference frame of a non-moving observer if the detector is on the floor of the train , then this is false . both the observer on the train , and the observer outside of the train ( who is stationary with respect to the earth let 's say ) , will see the light strike the detector . the person outside of the train will simply see the light travel along a " diagonal " path as illustrated in the image below . the left-hand image is what the train observer sees , and the right-hand imagine is what someone on the outside of the train sees . the math . in the train , which we call frame $s'$ , the spacetime trajectory of the light between when it leaves the ceiling at parameter value $\lambda = 0$ and when it hits the detector on the floor at parameter value $\lambda = d/c$ is given by is given by the following parameterized curve \begin{align} t' ( \lambda ) = \lambda , \qquad x' ( \lambda ) = 0 , \qquad y' ( \lambda ) = d-c\lambda \end{align} where i have taken the $y'$-axis perpendicular to the floor , the $x'$-axis parallel to the direction of motion of the train , and we ignore motion in $z'$ . notice that since $t' ( \lambda ) = \lambda$ the parameter $\lambda$ here is really just time according to a train observer . to determine what the person outside of the train , which we call frame $s$ sees , we apply the lorentz transformation : \begin{align} \begin{pmatrix} ct ( \lambda ) \\ x ( \lambda ) \\ y ( \lambda ) \\ \end{pmatrix} and = \begin{pmatrix} \gamma and \gamma\beta and 0 \\ \gamma\beta and \gamma and 0 \\ 0 and 0 and 1 \\ \end{pmatrix} \begin{pmatrix} ct' ( \lambda ) \\ x' ( \lambda ) \\ y' ( \lambda ) \\ \end{pmatrix} = \begin{pmatrix} c\gamma\lambda \\ c\gamma\beta\lambda \\ d-c\lambda \\ \end{pmatrix} \end{align} where we have assumed that the train moves along the positive $x$-axis with speed $v = \beta c$ . in other words , the path of the light in the frame $s$ is \begin{align} t ( \lambda ) = \gamma\lambda , \qquad x ( \lambda ) = \gamma \beta c\lambda , \qquad x ( \lambda ) = d-c\lambda \end{align} for convenience , we reparameterize this curve by defining $\mu = \gamma\lambda$ to give \begin{align} t ( \mu ) = \mu , \qquad x ( \mu ) = v\mu , \qquad y ( \mu ) = d- c\mu/\gamma \end{align} where we used $\beta c = v$ to simplify the $x$ component . the parameter $\mu$ is just the time as measured by an $s$ observer since $t ( \mu ) = \mu$ . notice , that the $x$-component of the trajectory now is nonzero , and in fact the light has velocity $v$ in the same direction as the train ! as a final check , let 's make sure that the speed of the light is $c$ in both frames . in the train frame we have \begin{align} v'_\mathrm{light} = \sqrt{\frac{dx'}{d\lambda}^2 + \frac{dy'}{d\lambda}^2} = \sqrt{0+c^2} = c \end{align} and in the frame outside the train we have \begin{align} v_\mathrm{light} and = \sqrt{\frac{dx}{d\mu}^2 + \frac{dy}{d\mu}^2} = \sqrt{v^2 + \frac{c^2}{\gamma^2}} = c\sqrt{\beta^2+\frac{1}{\gamma^2}} = c\sqrt{\beta^2 +1-\beta^2}=c \end{align}
in the video lecture mentioned in the question , the guy always use $t_{sur}$ in the inequality tds> du+pdv . in deriving the equation the " surrounding " ( also see " the principles of chemical equilibrium " by denbigh , p . 82 , the term " thermostat " is used instead ) is included as a bigger isolated system . let 's say the entropy of these bigger system is si . therefore $ds_{i}≥0$ , it can be replaced by ds+dssur≥0 where s is the entropy of the " inner " system and $s_{sur}$ is the entropy of the " surrounding " . as heat is transferred the surrounding loses entropy so it becomes ds-dq/tsur≥0 and therefore $t_{sur}ds≥du+pdv$ . in the case that there is no chemical reaction , tds=du+pdv is always true as state equation . the t here refers to the " inner " system 's temperature . there is no contradiction .
a closed form solution ( in terms of more elementary function than hankel functions ) does not exist . however , typically one is only interested in the regime where $k\rho \gg1$ , i.e. , the asymptotic region far away from the source . there one can use the asymptotic form of the hankel functions $$h^{1/2}_m ( x ) \sim \sqrt{\frac2{\pi x}} e^{\pm ( ix -i \frac\pi2 m - i\frac\pi4 ) } . $$ thus , the accumulated phase is given by $$\phi_\text{acc} ( \rho_1 , \rho_2 ) = \pm k ( \rho_2 - \rho_1 ) , $$ i.e. , the same as for a plane wave .
there is some discussion about this in the question how does a photon experience space and time ? . you will commonly hear it said that photons do not experience time , but this is somewhat misleading . observers moving at different velocities have different coordinate systems , and these systems are related by the lorentz transformation . if you apply the transformations to an object moving at very nearly the speed of light you will find that time slows down for that object , and the closer the object gets to the speed of light the slower its time gets . this is the origin for the claim that at the speed of light time stops , and therefore photons do not age . the trouble is that at the speed of light the lorentz transformation becomes singular and cannot be used . an object moving at $c$ does not have a rest frame so you cannot make statements about the behaviour of time and space in its rest frame . the claim that time stops for photons is meaningless as they have no rest frame to experience time in . incidentally this applies to all massless particles and not just light . as a photon moves it traces out a world line i.e. a series of the spacetime points that it passes through . at those points it can interact with some other object at the same point e.g. your eye . this worldline will look different in different inertial frames , but if two objects occupy the same spacetime point in one frame they occupy the same spacetime point in all frames .
you can project a real image onto a screen or wall , and everybody in the room can look at it . a virtual image can only be seen by looking into the optics and can not be projected . as a concrete example , you can project a view of the other side of the room using a convex lens , and can not do so with a concave lens . i will steal some image from wikipedia to help here : first consider the line optics of real images ( from http://en.wikipedia.org/wiki/real_image ) : notice that the lines that converge to form the image point are all drawn solid . this means that there are actual rays , composed of photon originating at the source objects . if you put a screen in the focal plane , light reflected from the object will converge on the screen and you will get a luminous image ( as in a cinema or a overhead projector ) . next examine the situation for virtual images ( from http://en.wikipedia.org/wiki/virtual_image ) : notice here that the image is formed by a one or more dashed lines ( possibly with some solid lines ) . the dashed lines are draw off the back of solid lines and represent the apparent path of light rays from the image to the optical surface , but no light from the object ever moves along those paths . this light energy from the object is dispersed , not collected and can not be projected onto a screen . there is still a " image " there , because those dispersed rays all appear to be coming from the image . thus , a suitable detector ( like your eye ) can " see " the image , but it can not be projected onto a screen .
if we ignore that coating a kevlar sheet with a high temperature superconductor is with current technology impossible one thing will happen : it will rip the superconductor apart . the lorentz for such a high current at small distances is enormous . here is a demonstration what happens : 5000 amps through a copper bar .
entanglement is being presented as an " active link " only because most people - including authors of popular ( and sometimes even unpopular , using the very words of sidney coleman ) books and articles - do not understand quantum mechanics . and they do not understand quantum mechanics because they do not want to believe that it is fundamentally correct : they always want to imagine that there is some classical physics beneath all the observations . but there is none . you are absolutely correct that there is nothing active about the connection between the entangled particles . entanglement is just a correlation - one that can potentially affect all combinations of quantities ( that are expressed as operators , so the room for the size and types of correlations is greater than in classical physics ) . in all cases in the real world , however , the correlation between the particles originated from their common origin - some proximity that existed in the past . people often say that there is something " active " because they imagine that there exists a real process known as the " collapse of the wave function " . the measurement of one particle in the pair " causes " the wave function to collapse , which " actively " influences the other particle , too . the first observer who measures the first particle manages to " collapse " the other particle , too . this picture is , of course , flawed . the wave function is not a real wave . it is just a collection of numbers whose only ability is to predict the probability of a phenomenon that may happen at some point in the future . the wave function remembers all the correlations - because for every combination of measurements of the entangled particles , quantum mechanics predicts some probability . but all these probabilities exist a moment before the measurement , too . when things are measured , one of the outcomes is just realized . to simplify our reasoning , we may forget about the possibilities that will no longer happen because we already know what happened with the first particle . but this step , in which the original overall probabilities for the second particle were replaced by the conditional probabilities that take the known outcome involving the first particle into account , is just a change of our knowledge - not a remote influence of one particle on the other . no information may ever be answered faster than light using entangled particles . quantum field theory makes it easy to prove that the information cannot spread over spacelike separations - faster than light . an important fact in this reasoning is that the results of the correlated measurements are still random - we can not force the other particle to be measured " up " or " down " ( and transmit information in this way ) because we do not have this control even over our own particle ( not even in principle : there are no hidden variables , the outcome is genuinely random according to the qm-predicted probabilities ) . i recommend late sidney coleman 's excellent lecture quantum mechanics in your face who discussed this and other conceptual issues of quantum mechanics and the question why people keep on saying silly things about it : http://motls.blogspot.com/2010/11/sidney-coleman-quantum-mechanics-in.html
the answer by @nowigettolearnwhataheadis is correct . it is worth learning the language used therein to help with your future studies . but as a primer , here 's a simplified explanation . start with your charge distribution and a " guess " for the direction of the electric field . as you can see , i made the guess have a component upward . we will see shortly why this leads to a contradiction . now do a " symmetry operation , " which is a fancy phrase for " do something that leaves something else unchanged . in this case , i am going to reflect everything about a horizontal line . i mean everything . the " top " of the sheet became the " bottom . " this is just arbitrary labeling so you can tell i flipped the charge distribution . the electric field is flipped too . ( imagine looking at everything in a mirror , and you will realize why things are flipped the way they are . ) hopefully everything is okay so far . but now compare the original situation with the new inverted one . you have exactly the same charge distribution . you can not tell that i flipped it , except for my arbitrary labeling . but if you have the same charge distribution , you ought to also have the same electric field . as you can see , this is not the case , which means i made a mistake somewhere . the only direction for the electric field that does not lead to this contradiction is perpendicular to the sheet of charge .
but it is a taylor expansion : $$ l ( x_0+x ) =l ( x_0 ) +\frac{\partial l}{\partial x} ( x_0 ) \cdot x $$ now set $x_0=v'^2$ , $x=2 \vec v \cdot \vec \epsilon + \epsilon^2$ . then neglecting powers of second order in epsilon leads to the desired result .
there is a more precise sense in which the question is ill-posed ( at least mathematically ) ; namely , it is a fundamental assertion of relativity ( special and general ) that the time ' measured ' ( counted , experienced , observed . . . ) by an observer between two events occurring on her worldline is the length of her worldline-segment joining the two events ( that is how we connect the physical notion of ( personal ) time with the mathematics of the theory ) . the way she determines motion depends on this notion of time . equivalently , proper time is measured by the arc-length parameter of the observer . now , since null curves have zero length ( hence no arc-length parameter ) the concept of proper time is not defined for null observers . hence neither is ( proper ) relative motion ( i.e. `from the photon 's perspective' ) . also , the relation you describe between timelike and null ( instantaneous ) observers is not reflexive at all ( whereas it is for the timelike ones , via the `lorentz boosts' ) : no isometry of minkowski space can take a timelike vector to a null one . although the question does not make sense , in this strict sense , mathematically , perhaps there are other physical or mathematical tricks for interpreting it ?
it seems like you know what the answer is , but you just do not know how to prove it . you are right though . to make things simpler , just view things in the center of mass frame . then the total momentum is zero , and , like you said , the total angular momentum is just the sum of the orbital momentum of each planet plus the sum of the spin angular momentum of each planet . after the collision , the linear velocity of the final planet must be zero by conservation of momentum . thus the final planet will have no orbital angular momentum . however , we know that the angular momentum must be conserved , so the planet must be spinning about sum axis , and you must have $\vec{l} = i \vec{\omega}$ . since you know what $\vec{l}$ is and you presumably know $i$ , you know what $\omega$ must be . so basically you had it right . to get the released energy , you need to take into account both the kinetic and potential energy . i will call the sum of these two the total energy . then the total energy of the final state will be less than the total energy of the initial state . so the energy released ( the energy that will cause the final planet to be hot ) will be the intial total energy minus the final total energy .
you should think of this by timestepping newton 's laws--- if you know the positions and velocity and one instant , you know the force , and the force determines the acceleration . this allows you to determine the velocity and an infinitesimal time in the future by $$ v ( t+dt ) = v ( t ) + dt f/m $$ $$ x ( t+dt ) = x ( t ) + dt v $$ you then find the position and velocity at the next time step , and you find the new force , and continue forever . this is an algorithm to solve newton 's laws , and all that ll are saying is that newton 's laws are known from experience with objects , they are inducted from observations .
i will select quotes from the wiki article on structure formation , bold mine : the very early universe in this stage , some mechanism , such as cosmic inflation , is responsible for establishing the initial conditions of the universe : homogeneity , isotropy and flatness . 3 [ 6 ] cosmic inflation also would have amplified minute quantum fluctuations ( pre-inflation ) into slight overdensities ( post-inflation ) . these acted as seeds around which the dark matter could begin to gravitationally congregate , even as the normal , baryonic matter was still in thermal equilibrium — far too hot to allow gravity to get any purchase on it . the basic component of galaxy formation is the force of gravity , as we know it in its newtonian form . but if the universe had remained homogenous there would not have been preferred locations in the expanding space that would feel the gravitational attraction more than other locations , so that aggregates of denser matter could evolve . it is the **quantum fluctuations ** that generate spots of inhomogeneity where gravity forces would be stronger locally in that space than in contingent ones . finally , at about 400,000 years after the ' bang ' , it is cool enough for the nuclei to capture their electrons , forming neutral-charge atoms . as the charged particles pair up , the photons no longer interact with them , they are free to propagate , and we currently detect those photons as the cosmic microwave background radiation ( cmb ) , because they fill the universe . the cmb is a snapshot of that time and it does show the inhomogeneities hypothesized by the proposal of quantum fluctuations . in those clusters nuclei have already formed and the higher gravitational fields in the clusters will slowly coalesce into galaxies and clusters of galaxies by the gravitational force , which though very weak is inexorable and acts at large distances . this outline is the most accepted model of how the structure of galaxies and clusters of galaxies has formed though the theoretical calculations which have to include general relativity can still be open to research . after all we still do not have a consistent unified model of a quantized gravity in a unified manner with the other three forces . that is how galaxies are seeded , by the primordial quantum mechanical fluctuations and the consequent coagulation of matter within them , in time . now accretion is a formation proposal used for single stars and planets , within galaxies . n astrophysics , accretion is the growth of a massive object by gravitationally attracting more matter , typically gaseous matter in an accretion disk . 1 accretion disks are common around smaller stars or stellar remnants in a close binary or black holes in the centers of spiral galaxies . some dynamics in the disk are necessary to allow orbiting gas to lose angular momentum and fall onto the central massive object . occasionally , this can result in stellar surface fusion so you are applying a wrong model to galaxy formation .
to be a little pedantic , nobody has yet done precision spectroscopy of antihydrogen , though the recent success in trapping it at cern ( all over the news this week , paper here ) is an early step toward that . it is possible that there are small differences in the spectrum of antihydrogen and hydrogen , though these differences can not be all that large , or they would be reflected in the interactions of antiprotons and positrons with ordinary matter in ways that would've shown up in other experiments . as i understand it ( and i am not an astronomer ) the primary evidence for a lack of vast amounts of antimatter out there in the universe is a lack of radiation from the annihiliation . we are very confident that our local neighborhood is matter , not antimatter , which means that if there were an anti-galaxy somewhere , there would also need to be a boundary region between the normal matter and antimatter areas . at that boundary region , there would be a constant stream of particle-antiparticle annihilations , which produce gamma rays of a very particular energy . we do not see any such region when we look out at the universe , though , which strongly suggests that there are not any anti-galaxies running around out there .
the ghost propagator connected with your lagrangian has the opposite sign to the standard one . this makes the sum of both propagators well-defined . how can we prove that this is the [ free ] propagator ? the free propagator is the green function of the free equation of motion ( with the suitable boundary conditions given by the $i\epsilon$ terms ) , so applying the klein-gordon operator to the free propagator you must get the dirac delta modulo a $i$ factor which depends on the convention one is using . added : regularizing an integral is to replace a sick-defined integral by a well defined one . this process entails the introduction of a dimension-full parameter ( and in some cases like in dimensional regularization a dimension-less parameter as well ) . there are several ways to do this and one usually —but not always— chooses the most symmetric one according to the problem in question . however , none of the known methods to deal with ultraviolet divergences in relativistic qft is physical , that is , none of them corresponds to a physical effect . some of them improve the behaviour of the integrand for high ( ultraviolet ) momenta : imposing a sharp cut-off ( step function in the integrand ) or a smother one like a gaussian $e^{-p^2/m^2}$ . likewise one can replace the propagator $\frac{1}{k^2+m^2}$ with : $$\frac{1}{k^2+m^2}\frac{m^2}{k^2+m^2}$$ that is equal to ( $m&gt ; &gt ; m$ ) $$\frac{1}{k^2+m^2}-\frac{1}{k^2+m^2}$$ so that one can think of the new term as something equivalent to add a very massive scalar particle with the wrong sign in the kinetic term ( and therefore something unphysical ) . or maybe one prefers to think of it as adding a very massive scalar particle with the wrong statistics in order to get a minus sign in each closed loop . . . each interpretation may be more convenient depending on the particular diagram one is regulating , but the interpretations are unnecessary because they are not physical . the significant is to define an expression that was undefined . at least , until somebody finds a physical regularization . some people think that quantum gravity ( through a violation of lorentz invariance , for example ) may provide a physical regulator for the uv divergencies of qft . we do not know yet if quantum gravity is able to give us that gift .
your question has to do with addition of velocities in special relativity . for objects moving at low speeds , your intuition is correct : say the bus move at speed $v$ relative to earth , and you run at speed $u$ on the bus , then the combined speed is simply $u+v$ . but , when objects start to move fast , this is not quite the way things work . the reason is that time measurements start depending on the observer as well , so the way you measure time is just a bit different from the way it is measured on the bus , or on earth . taking this into account , your speed compared to the earth will be $\frac{u+v}{1+ uv/c^2}$ . where $c$ is the speed of light . this formula is derived from special relativity . some comments on this formula provide direct answer to your question : if both speeds are small compared with the speed of light , they approximately add up as your intuition tells you . if one of the speeds is the speed of light $c$ , you can see that adding any other speed to it does not in fact change it : the speed of light is the same in all reference frames . if you add up any two speeds below $c$ , you end up still below the speed of light . so , any material object which has a mass ( unlike light , which does not ) , moves at a speed less than $c$ . adding to it according to the correct rule makes it closer to the speed of light , but you can never exceed it , or in fact not even reach it . i would recommend wheeler and taylor 's " spacetime physics " to read about this . unlike the reputation of the subject it is actually pretty intuitive ( i learned that formula in high school ) .
you are correct that standing on your head is the same as a g-force of -1 g . that is what einsteins equivalence principle tells us . your friend is correct that tolerance for negative gs is lower , but it is around -3 g not - . 5 g . tolerance for positive vertical gs is around 5 gs without g suits or training . tolerance for horizontal gs varies from 12 to 17 gs depending on whether the acceleration is " eyeballs out " or " eyeballs in " .
in real life , the current can not jump instantaneously because there is always some finite inductance in a circuit . however , this is just a typical idealized textbook problem where the inductance is assumed identically zero , so the current can jump instantaneously according to the assumptions of the problem . note the current also jumps in their solution for the capacative case .
they are proportional so essentially the same , but $\omega_\lambda$ is a convenient dimensionless number . straight out of weinberg 's newer cosmology book : $$ \lambda = 8 \pi g \rho_v , $$ where $\rho_v$ is the vacuum energy density , and $$ \rho_{v0} = \frac{3 h_0^2 \omega_\lambda}{8\pi g} . $$ putting them together $\lambda = 3 h_0^2 \omega_{\lambda0}$ . note the subscript $0$ represents the present day value . $h_0$ is the present day hubble rate . since $\lambda$ is a constant you can infer $\omega_\lambda \sim h^{-2}$ . $\omega_\lambda$ asymptotically approaches one as the vacuum dominates ever more and $h$ goes to the de sitter value $h_{ds} = \sqrt{\lambda / 3}$ . in an earlier stage you have to work out $h$ using the friedmann equation .
the key idea of how lcds work is that a peculiar substance , a " liquid crystal " , is placed between two linear polarizer filters . the filters are crossed , so normally a backlight behind the lcd is not seen , appearing black . when voltage is applied to part of the device , such as one segment of a 7-segment digit , or one row and column defining one pixel in an iphone display , the liquid crystal twists the light polarized by the back filter , allowing it to shine through the front filter . with color filters to make rgb images , imperfect polarization depending on wavelength , and interference effects due to reflections between thin optical layers , it is no surprise you had see shimmering colors and murky darkness or other optical weirdness when viewing the device through a linear polarizer . optical engineers balance different imperfections in ways to cancel out or at least to appear not imperfect . the display is , of course , designed to look good to normal human eyes without polarizers . see an explanation and illustration at http://electronics.howstuffworks.com/lcd2.htm or at http://lcp.elis.ugent.be/tutorials/lc/lc3 or at http://www.dciincorporated.com/products/overview.html or google for words like " lcd polarized construction " , " how liquid crystal display works " , " physics lcd display optics " , etc .
first , you set up your second integral wrong . the integrand should be $$r\cdot ( r^2\cos^2\theta+z^2 ) $$ since $\sqrt{r^2\cos^2\theta+z^2}$ is the distance of a point from the y-axis . second , you have used $h$ in two ways . in your picture $h$ measures the vertical distance from the top of the hemisphere whereas in your first iterated integral ( the $dr$ integral ) , $h$ is used in the way $z$ is used in cylindrical coordinates . then you use it as you have defined in your picture again in the next iterated integral . for the sake of clarity you should write your two integrals like $$\int_0^{2\pi} \int_{r-h}^r \int_{0}^{\sqrt{r^2-z^2}} r^3 dr dz d\theta$$ and $$\int_0^{2\pi} \int_{r-h}^r \int_{0}^{\sqrt{r^2-z^2}} r ( r^2 \cos^2\theta+z^2 ) dr dz d\theta$$ where standard notation for cylindrical coordinates has been used i.e. $ ( r , \theta , z ) $ .
it has nothing to do with any capacitance . it is all about skin resistance . the resistivity inside your body is much lower than that of the skin . as a result , your measurement is really showing you the sum of two resistances thru the skin . the main reason skin has higher resistance than the body internally is because the skin is dry . however , the skin can get moister by sweating and external blocking of evaporation . when you grip the probes , evaporation is blocked in the immediate vicinity , so moisture builds up . your body may also produce a bit more sweat when you grip hard . try licking your fingers before touching the probes and you will see the resistance reduced substantially . you can also reduce the resistance by gripping harder . that increases the contact area and also makes better contact .
it is ${\rm j}\cdot {\rm s}^{-1}$ , you might add " per unit $\cos\theta$" as well , but the latter has no standardized symbol because it is unphysical because the physical dimension of angles and their cosines is " one": they are dimensionless . after all , this is true even for radians and steradians . you may have added ${\rm sr}^{-1}$ to improve the readability and purpose of the quantity $i ( \phi , \theta ) $ but from purely dimensional considerations , $1\ , {\rm sr}\equiv 1$ so you may omit the powers of ${\rm sr}$ as you like , too . when we talk about the angular frequency $\omega$ , we do not say that the unit is ${\rm rad}\cdot {\rm s}^{-1}$ , radians per second , although this is also " more comprehensible " a way to write the unit . we just call the unit ${\rm s}^{-1}$ . the same holds for any powers of steradians . there is a natural unit of angle and solid angle , namely radian and steradian , respectively , and they are dimensionally as well as numerically ( as long as one uses the natural units for geometry ) equal to one . it is only the dimensionful units – those for which there is no " preferred , culturally independent unit " – for which we must be careful to add the units we use . angles and solid angles are dimensionless and do not need any units with new names .
as noted above in comments , i am not competely sure i understand the question . but anyway , i will give it a shot . the answer is model-dependent . the standard cosmological model at the moment is the lambda-cdm model . this model has various parameters . depending on these parameters , the spatial curvature can be positive , negative , or zero . observation puts ( model-dependent ) bounds on the spatial curvature : what is the curvature of the universe ? given these bounds , we can put ( model-dependent ) bounds on the size of the universe : size of the universe . we then find that the universe is much larger than our own observable region . therefore the ( model-dependent ) answer to the question ( as i construe it ) is yes : there are other regions of the universe in which observers would have observable regions that do not intersect our observable region ( and probably never will , given the acceleration of expansion ) .
the set-up you are describing is essentially an optical cavity , and you are asking what is the longest lifetime which has been achieved in such a cavity . in this paper ( also described here ) , s . kuhr et . al . describe a supraconducting cavity with a 130 ms lifetime . it is essentially 2 curved mirrors face to face . it works in microwaves ( 51 ghz ) , which has a long wavelength ( 6 mm ) , and this makes the manufacturing of mirror smooth at this scale much easier . this cavity is one of the key-element of this lab cqed ( cavity quantum electrodynamics ) experiments . i do not know if it is the best one , but i am pretty sure it is more or less the state of the art in this domain .
it is just the boltzmann distribution , with the energy being given by the chemical potential .
no , you also need an initial condition for the field , and a boundary condition for the field . a plane wave solution to the maxwell equations has the same 4-current as vacuum , after all .
there are two ways to approach your question . the first is to explain what brian greene means , and the second is to point out that the " particles being swallowed " explanation is a metaphor and is not actually how the calculation is done . i will attempt both , but i am outside my comfort zone so if others can expand or correct what follows please jump in ! when a pair of virtual particles are produced there is not a negative energy particle and a positive energy particle . instead the pair form an entangled system where it is impossible to distinguish between them . this entangled system can interact with the black hole and split , and the interaction guarantees that the emerging particle will be the positive one . nb " positive " and " negative " does not mean " particle " and " anti-particle " ( for what it does mean see below ) , and the black hole will radiate equal numbers of particles and anti-particles . now onto the second bit , and i approach this with trepidation . when you quantise a field you get positive frequency and negative frequency parts . you can sort of think of these as representing particles and anti-particles . how the positive and negative frequencies are defined depends on your choice of vacuum , and in quantum field theory the vacuum is unambiguously defined . the problem is that in a curved spacetime , like the region near a black hole , the vacuum changes . that means observers far from the black hole see the vacuum as different from observers near the black hole , and the two observers see different numbers of particles ( and antiparticles ) . a vaccum near the event horizon looks like excess particles to observers far away , and this is the source of the radiation . see the wikipedia article on the bogoliubov transformation for more information , though i must admit i found this article largely incomprehensible . exactly the same maths gives the unruh effect , i.e. the production of particles in an accelerated frame . the fact that the unruh effect also produces particles shows that a black hole is not necessary for the radiation , so it can not simply be virtual particles being swallowed .
this correlation is known as titius-bode 's law , which is often stated as \begin{equation} d=0.4 + 0.3 \cdot 2^n \end{equation} where d represents planet 's mean distance from the sun in astronomical units and n = -∞ , 0 , 1 , 2 . . . for mercury , venus , earth , mars , asteroid belt , jupiter and so on . the rule is not satisfied exactly with neptune 's orbit ( n =7 ) constituting a significant departure from it : according to the law neptune 's mean distance ought to be 38.8 aus , but is in reality just 30 aus ( disagreement of close to 30% with all other planets agreeing to less than 6% ) . in fact , this departure is what has historically led to diminishing importance of the law . see also the table and chart in wikipedia . it is currently thought that if the law is not a pure coincidence then it is a consequence of orbital instabilities and the mechanism through which solar system was formed . it is been shown that rotational and scale invariance of a protoplanetary disk leads to density maxima in the disk appearing periodically in variable \begin{equation} x = \ln \frac{r_n}{r_0} \end{equation} which leads to geometric series for planetary distances similar to that expressed in titius-bode 's law . see this and this paper for details . note that the requirements of rotational and scale invariance are very general . as the nebula from which protoplanetary disk is formed collapses under its own gravity , its rotation increases due to the law of the conservation of angular momentum . this eventually leads to the protoplanetary disk 's rotational symmetry . also , gravity does not have intrinsic length scale , so the nebula is highly likely to possess scale invariance . these two requirements are so general that even if the titius-bode 's law is real it is not at all useful to select between solar system 's formation models . i do not know of an advanced book specifically on solar system formation , but there is a very good book by a.e. roy on orbital mechanics which certainly would qualify as a book for the mathematically inclined which in addition to chapters on orbital mechanics , rocket dynamics , interplanetary trajectory design includes few solar system formation and many-body stellar systems . so depending on how broad your interests are you may enjoy it .
no . or at least , not without special caveats . edit after comments by mark beadles , david zaslavsky , and ron maimon , i should clarify that you cannot have a plain window that lets all light through in one direction but reflects all light coming the other direction . thus , you cannot have a window that does not absorb radiation at all and also has the prescribed one-direction property . imagine the glass separates two rooms which are completely isolated systems , except that they interact by sending electromagnetic radiation through the glass . we will say that light can only pass from left to right . further suppose the electromagnetic radiation in the rooms is a simple black-body spectrum , so each room has a well-defined temperature . imagine the temperature is higher on the left . because light only passes from left to right , heat will be transferred from the hotter room on the left to the colder room on the right with no other effect . this violates the second law of thermodynamics .
it is not quite clear what you mean by " telescope lens " - do you mean the system of lenses that make up a telescope ? if so , there are two basic types . the actual lenses in your telescope are probably more complicated and correct for all kinds of aberrations , but they work like this . the keplerian telescope ( top one in the diagram ) consists of two positive lenses , with different focal distances , with their foci at the same point , in between the lenses . imagine your eye on the left . two parallel rays will converge to a point at the focus of the right-hand lens , and since this point is also at the focus of the left-hand lens , they will become parallel again , but inverted . of course you are usually not looking at parallel rays with your telescope , but picturing it this way is what helped me to understand why we see a magnified image . you might think at first glance from the diagram that this makes the image smaller ; but what it does is take parallel rays traveling at different angles to the optical axis , and make them parallel again on the other side of the telescope , but traveling at a larger angle . this increases the apparent size of the object . ( google docs is not very good for drawing detailed diagrams - you could take a look at the more complicated ones on the wikipedia page on telescopes . ) the galilean telescope ( bottom one in the diagram ) consists of a negative and a positive lens , again with their foci at the same point . this time the point is on the outside of the telescope , at where your eye is . the positive lens focuses the parallel rays to that point , and the negative lens takes the converging rays and makes them parallel again . this time , the image is not inverted . then the second part of your question is about magnification . the focal lengths of the lenses are the only factors that influence the magnification : it is equal to $$m=-\frac{f_2}{f_1}$$ the minus sign seems counter-intuitive , but think about it - we fill in a negative focal length for the negative lens in the galilean telescope , so the magnification comes out positive . for the keplerian telescope , the magnification comes out negative - this indicates that the image is magnified , but also inverted .
the term closest to what you are asking for is " free energy " . free energy is the amount of energy that can be used to do work . it always decreases in spontaneous processes . see work content , thermodynamic free energy . it usually includes an entropy term , but we cannot say that it is entropy . then again , you may have been looking for entropy after all . entropy is a measure of randomness , and it is always " used up " ( by used up i mean that it increases ) . but to relate it with work and energy , one has to use free energy anyways .
start with the switch closed . you are told that all the resistors have the same value - let 's call this $r$ . you can use the usual rules for calculating the total resistance to calculate $r_{total}$ . now the battery voltage is $v = ir_{total}$ and you know $i$ so you can calculate $v$ in terms of $r$ . with the switch open you calculate $r_{total}$ again ( obviously it is higher than with the switch closed ) , and you calculated $v$ above , so now you can calculate $i$ .
after a little bit more research , i verified this is the exact method employed to determine whether a given metric is feasible to create with a non-exotic energy-momentum tensor , $ t_{\mu\nu} $ . resources : general relativity lecture notes and the arxiv research paper , passing the einstein-rosen bridge , by m . o . katanaev . the research paper was an excellent read , and i definitely recommend giving it a read !
if you take an isolated spherically symmetric object then the spacetime curvature around it is described by the schwarzschild metric . the bending of the rubber sheet is meant to be an analogy for this curvature , but bear in mind it is just an analogy and is in many ways a poor representation of what actually happens . anyhow , the schwarzschild metric only applies if : the planet/star is the only object in the universe the universe is time independant i.e. the planet/star has existed forever and will continue to exist forever obviously neither of these conditions are true for real stars in the real universe , but in most cases we expect the schwarzschild metric to be a good approximation . you specifically ask about the effect of the expansion of the universe . a convenient model for an expanding universe is the de sitter universe . like the schwarzschild metric this is only an approximation to the real universe as the de sitter universe contains no matter , but again it is not a bad approximation to our universe and actually over the next few billion years it will become a better and better approximation as the expansion becomes dominated by dark energy . so your question can be rephrased as how is the schwarzschild metric changed in a de sitter universe ? and actually we have an exact answer to that . the spacetime curvature due to a planet/star in a de sitter universe is described by the de sitter–schwarzschild metric . however as long as you are close to the star planet there is little difference from the schwarzschild metric . all this is inevitably a bit technical , and i would guess it is probably a deeper explanation than you wanted . but there is not really any way to simplify it except to say that the expansion of the universe does change the curvature around an astronomical body , but not very much .
is the 4-dimensional spacetime of general-relativity discrete or continuous ? in the usual definition of general relativity , spacetime is continuous . however , general relativity is a classical theory and does not take quantum effects into account . such effects are expected to show up at very short distances , where your question is relevant . are there experimental evidences of continuity/discreteness ? all the experimental evidence points to continuous space , down to the shortest distances at which we have been able to measure . we do not know what happens at shorter distances . we also do not have any direct experimental evidence that gravity is a quantum theory , with the same caveat . on the other hand , we are quite confident that a complete theory of nature must include quantum gravity and not just classical gravity . and , we have an educated guess of the distance scale at which quantum effects should become measurable : this is the planck length , roughly $10^{-33}$ cm . this is much much shorter than the shortest distance at which we can carry out experiments , so at least we are not surprised that we did not see any such effects so far . before proceeding , one more caveat . there is an interesting and quite recent astrophysical experiment that showed that lorentz symmetry holds even below the planck length . if lorentz symmetry is broken , it generally means that photons with different energies will travel at different velocities . at the experiment , they managed to detect a pair of photons that were created at almost the same time but had very different energies . they reached the detector almost simultaneously , which means their velocities were similar . because the photons travelled an enormous distance before reaching us , they must have had almost the same velocity . so we know that at least lorentz symmetry holds at very short distances , and it seems difficult to reconcile this experimental fact with a discrete spacetime . so at least naively it seems that this is evidence against discreteness . is the spacetime continuous or discrete ? at long distances spacetime can certainly be thought of as continuous . at short distances , the short answer is : we do not know . string theory is the only consistent theory of quantum gravity we know of , where we can actually compute things with some confidence . ( you will probably hear some opinions that contradict this statement , mentioning loop quantum gravity , causal sets , etc . , which are not related to string theory , but what i said is the common view in the community of high-energy theorists . ) string theory is giving us some strong hints that perhaps spacetime at short distances is not continuous or discrete , but something else that we do not understand yet . so the situation is that even theoretically , without talking about actual experiments that check the theory , we do not know what spacetime is like at short distances . perhaps this is why you do not see this question mentioned a lot . my personal guess is that spacetime at short distances is neither continuous nor discrete , but has a different nature that may require new mathematical tools to describe . or better , what if we consider additional dimensions like string theory hypothesizes ? are those compact additional dimensions discrete or continuous ? adding extra dimensions does not change any of the above .
i think there is not one general method to calculate this just because its too complex . if there was one f1 engineers would have an easier job . nevertheless you can guess $\mu$ 's dependence from the data , as you pointed out , if you plot it . i just did it here http://genflux.chartle.net/embed?index=47502 and seems an exponential decay which , makes sense . as you increase the velocity your tires will lack of adherence due to less contact . so my first guess is that you can write $$\mu = e^{-b |\vec v|} + a \quad ; \quad a &gt ; 0 , \quad b&gt ; 0 $$ where $a$ and $b$ will depend on the data to be fit ( your tires and conditions ) .
hwlau is correct about the book but the answer actually is not that long so i think i can try to mention some basic points . path integral one approach to quantum theory called path integral tells you that you have to sum probability amplitudes ( i will assume that you have at least some idea of what probability amplitude is ; qed can not really be explained without this minimal level of knowledge ) ) over all possible paths that the particle can take . now for photons probability amplitude of a given path is $\exp ( i k l ) $ where $k$ is some constant and $l$ is a length of the path ( note that this is very simplified picture but i do not want to get too technical so this is fine for now ) . the basic point is that you can imagine that amplitude as a unit vector in the complex plane . so when doing a path integral you are adding lots of short arrows ( this terminology is of course due to feynman ) . in general for any given trajectory i can find many shorter and longer paths so this will give us a nonconstructive interference ( you will be adding lots of arrows that point in random directions ) . but there can exist some special paths which are either longest or shortest ( in other words , extremal ) and these will give you constructive interference . this is called fermat 's principle . fermat 's principle so much for the preparation and now to answer your question . we will proceed in two steps . first we will give classical answer using fermat 's principle and then we will need to address other issues that will arise . let 's illustrate this first on a problem of light traveling between points $a$ and $b$ in free space . you can find lots of paths between them but if it will not be the shortest one it will not actually contribute to the path integral for the reasons given above . the only one that will is the shortest one so this recovers the fact that light travels in straight lines . the same answer can be recovered for reflection . for refraction you will have to take into account that the constant $k$ mentioned above depends on the index of refraction ( at least classically ; we will explain how it arises from microscopic principles later ) . but again you can arrive at snell 's law using just fermat 's principle . qed now to address actual microscopic questions . first , index of refraction arises because light travels slower in materials . and what about reflection ? well , we are actually getting to the roots of the qed so it is about time we introduced interactions . amazingly , there is actually only one interaction : electron absorbs photon . this interaction again gets a probability amplitude and you have to take this into account when computing the path integral . so let 's see what we can say about a photon that goes from $a$ then hits a mirror and then goes to $b$ . we already know that the photon travels in straight lines both between $a$ and the mirror and between mirror and $b$ . what can happen in between ? well , the complete picture is of course complicated : photon can get absorbed by an electron then it will be re-emitted ( note that even if we are talking about the photon here , the emitted photon is actually distinct from the original one ; but it does not matter that much ) then it can travel for some time inside the material get absorbed by another electron , re-emitted again and finally fly back to $b$ . to make the picture simpler we will just consider the case that the material is a 100% real mirror ( if it were e.g. glass you would actually get multiple reflections from all of the layers inside the material , most of which would destructively interfere and you had be left with reflections from front and back surface of the glass ; obviously , i would have to make this already long answer twice longer :- ) ) . for mirrors there is only one major contribution and that is that the photon gets scattered ( absorbed and re-emitted ) directly on the surface layer of electrons of the mirror and then flies back . quiz question : and what about process that the photon flies to the mirror and then changes its mind and flies back to $b$ without interacting with any electrons ; this is surely a possible trajectory we have to take into account . is this an important contribution to the path integral or not ?
i also know that l and s commute , but i am unsure why . i have heard that it is simply because they act on difference variables , but i do not understand exactly what this means . is there a way to show this explicitly ? suppose we have two hilbert spaces $h_1$ and $h_2$ , an operator $a_1$ acting on $h_1$ , and an operator $a_2$ acting on $h_2$ . let $h = h_1 \otimes h_2$ . then we can define $a_1$ and $a_2$ on $h$ by defining \begin{align} a_1 ( |a_1\rangle \otimes |a_2\rangle ) and = a_1|a_1\rangle \otimes |a_2\rangle \\ a_2 ( |a_1\rangle \otimes |a_2\rangle ) and = |a_1\rangle \otimes a_2|a_2\rangle \end{align} where $|a_1\rangle \in h_1 , |a_2\rangle \in h_2$ ; and extending linearly to all of $h$ . then \begin{align} a_1 \circ a_2 ( |a_1\rangle \otimes |a_2\rangle ) and = a_1 ( |a_1\rangle \otimes a_2|a_2\rangle ) \\ and = a_1|a_1\rangle \otimes a_2|a_2\rangle \\ and = a_2 ( a_1|a_1\rangle \otimes |a_2\rangle ) \\ and = a_2 \circ a_1 ( |a_1\rangle \otimes |a_2\rangle ) \end{align} so the commutator vanishes on all pure tensors , and hence on all of $h$ . this is precisely the situation we have with the operators $l$ and $s$ . in general , the wave function of a particle lives in a tensor product space . the spatial part of the wave function lives in one space , that of square-integrable functions on $\mathbb{r}^3$ . the spin part , on the other hand , lives in a spinor space , i.e. , some representation of $su ( 2 ) $ . $l$ acts on the spatial part , whereas $s$ acts on the spin part . what are the remaining commutation relations between $j$ , $j^2$ , $l$ , $l^2$ , $s$ , and $s^2$ ? you should be able to work these out on your own , using the commutation and anti-commutation relations you already know , and properties of commutators and anti-commutators . for example , $$ [ j_i , l_j ] = [ l_i + s_i , l_j ] = [ l_i , l_j ] + [ s_i , l_j ] = i\hbar\epsilon_{ijk} l_k$$ likewise : \begin{align} [ j_i^2 , l_j ] and = j_i [ j_i , l_j ] + [ j_i , l_j ] j_i \\ and = j_i ( i\hbar \epsilon_{ijk}l_k ) + ( i\hbar\epsilon_{ijk}l_k ) j_i \\ and = i\hbar\epsilon_{ijk}\{j_i , l_k\} \\ and = i\hbar\epsilon_{ijk}\{l_i + s_i , l_k\} \\ and = i\hbar\epsilon_{ijk} ( \{l_i , l_k\} + \{s_i , l_k\} ) \\ and = i\hbar\epsilon_{ijk} ( 2\delta_{ik} i + 2s_i l_k ) \\ and = 2i\hbar\epsilon_{ijk} s_i l_k \end{align} where we have used the fact that $l_i$ and $s_j$ commute , the linearity of $ [ , ] $ and $\{ , \}$ , and the identity $$ [ ab , c ] = a [ b , c ] + [ a , c ] b$$
it is not possible to communicate faster than light using entangled states . all you get out of entanglement is a correlation between the values of two measurements . ; the entanglement does not allow you to influence the value measured at another location in a non-causal way . in other words , the correlation only becomes evident after combining the results from the measurements afterwards , for which you need classical information transfer . for example , consider the thought experiment described on the wikipedia page for the epr paradox : a neutral pion decays into an electron and a positron , emitting them in opposite directions and with opposite spins . however , the actual value of the spin is undetermined , so with respect to a spin measurement along a chosen axis , the electron and positron are in the state $$\frac{1}{\sqrt{2}}\left ( |e^+ \uparrow\rangle|e^- \downarrow\rangle + |e^+ \downarrow\rangle|e^- \uparrow\rangle\right ) $$ suppose you measure the spin of the positron along this chosen axis . if you measure $\uparrow$ , then the state will collapse to $|e^+ \uparrow\rangle|e^- \downarrow\rangle$ , which determines that the spin of the electron must be $\downarrow$ ; and vice versa . so if you and the other person ( who is measuring the electron spin ) get together and compare measurements afterwards , you will always find that you have made opposite measurements for the spins . but there is no way to control which value you measure for the spin of the positron , which is what you had need to do to send information . as long as the other person does not know what the result of your measurement is , he can not attach any informational value to either result for his measurement .
not an expert here but i think it went something like this : her research established the techniques and methods that other physicists like rutherford needed to probe the internal structure of the atom . the concept of an atom ( i believe ) was postulated at the time but not really taken seriously . certainly there was no real concept of an atom 's internal structure . the energy contained in the radioactivity had to come from somewhere . finding out where it came from through experimental investigation of the radioactivity was really the start of atomic physics and paved the way for the development of modern phyiscs . the subsequent research changed the way we think about matter .
here comes a simple minded answer by an experimentalist . the luminiferous ether was discarded because it violated special relativity . it presupposed a fixed reference frame of the ether against which everything moved . in special relativity there exists no absolute frame of reference , and special relativity has been vindicated many times experimentally . the higgs field , as also the vacuum sea in general , comes from quantum field theory formulations of the interactions of elementary particles . all quantum field theories are consistent with special relativity , and thus the higgs field is also consistent with special relativity . it therefore cannot play the role of the luminiferous ether , and also the same holds for the vacuum sea , which is seething with virtual pairs of particle/antiparticle . at the level of your studies this should suffice .
the expression $ ( \hbar g/c^3 ) ^{1/2}$ is the unique product of powers of $\hbar , g , c$ , three most universal dimensionful constants , that has the unit of length . because the constants $\hbar , g , c$ describe the fundamental processes of quantum mechanics , gravity , and special relativity , respectively , the length scale obtained in this way expresses the typical length scale of processes that depend on relativistic quantum gravity . the formula and the value were already known to max planck more than 100 years ago , that is why they are called planck units . unless there are very large or strangely warped extra dimensions in our spacetime , the planck length is the minimum length scale that may be assigned the usual physical and geometric interpretation . ( and even if there are subtleties coming from large or warped extra dimensions , the minimum length scale that makes sense – which could be different from $10^{-35}$ meters , however – may still be called a higher-dimensional planck length and is calculated by analogous formulae which must , however , use the relevant newton 's constant that applies to a higher-dimensional world . ) the planck length 's special role may be expressed by many related definitions , for example : the planck length is the radius of the smallest black hole that ( marginally ) obeys the laws of general relativity . note that if the black hole radius is $r= ( \hbar g/c^3 ) ^{1/2}$ , the black hole mass is obtained from $r=2gm/c^2$ i.e. $m=c^2/g\cdot ( \hbar g/c^3 ) ^{1/2} = ( \hbar c/g ) ^{1/2}$ which is the same thing as the compton wavelength $\lambda = h/mc = hg/c^3 ( \hbar g/c^3 ) ^{-1/2}$ of the same object , up to numerical factors such as $2$ and $\pi$ . the time it takes for such a black hole to evaporate by the hawking radiation is also equal to the planck time i.e. planck length divided by the speed of light . smaller ( lighter ) black holes do not behave as black holes at all ; they are elementary particles ( and the lifetime shorter than the planck time is a sign that you can not trust general relativity for such supertiny objects ) . larger black holes than the planck length increasingly behave as long-lived black holes that we know from astrophysics . the planck length is the distance at which the quantum uncertainty of the distance becomes of order 100 percent , up to a coefficient of order one . this may be calculated by various approximate calculations rooted in quantum field theory – expectation values of $ ( \delta x ) ^2$ coming from quantum fluctuations of the metric tensor ; higher-derivative corrections to the einstein-hilbert action ; nonlocal phenomena , and so on . the unusual corrections to the geometry , including nonlocal phenomena , become so strong at distances that are formally shorter than the planck length that it does not make sense to consider any shorter distances . the usual rules of geometry would break down over there . the planck length or so is also the shortest distance scale that can be probed by accelerators , even in principle . if one were increasing the energy of protons at the lhc and picked a collider of the radius comparable to the universe , the wavelength of the protons would be getting shorter inversely proportionally to the protons ' energy . however , once the protons ' center-of-mass energy reaches the planck scale , one starts to produce the " minimal black holes " mentioned above . a subsequent increase of the energy will end up with larger black holes that have a worse resolution , not better . so the planck length is the minimum distance one may probe . it is important to mention that we are talking about the internal architecture of particles and objects . many other quantities that have units of length may be much shorter than the planck length . for example , the photon 's wavelength may obviously be arbitrarily short : any photon may always be boosted , as special relativity guarantees , so that its wavelength gets even shorter . lots of things ( insights from thousands of papers by some of the world 's best physicists ) are known about the planck scale physics , especially some qualitative features of it , regardless of the experimental inaccessibility of that realm .
the quick way to answer this is to point out that the riemann curvature tensor is co-ordinate invarient . this means the increase in the relativistic mass does not change the curvature i.e. if $\gamma$ is 2 that does not mean the gravitational force is twice as strong . the same argument prevents bodies collapsing into black holes at speeds near the speed of light . the confusion arises because people tend to consider gravity as being a function of mass , but life is more complicated than that . the einstein equation states : $$g_{\alpha\beta} = 8\pi t_{\alpha\beta}$$ where $g_{\alpha\beta}$ is the einstein tensor that describes the curvature and $t_{\alpha\beta}$ is the stress-energy tensor . so the curvature is not determined by mass , it is determined by the stress-energy tensor . as normally written , the stress-energy tensor is a 4 x 4 matrix with 10 independant entries ( only 10 because it is symmetric ) and the mass/energy density only contributes to the $t_{00}$ component . the $t_{0\beta}$ components ( and by symmetry the $t_{\alpha 0}$ components ) are momentum densities , so for the moving bodies in your example these are not zero and will affect the curvature . i must admit i have not attempted to calculate what the stress energy tensor looks like for systems like the one you describe , and how the momentum components balance out the mass/energy component in your system . i think this would be a hard calculation .
your teacher is right , but only in the context of the course he is teaching you . by definition , statics deals with systems in equilibrium or that can reach equilibrium . in other cases , like thermodynamics , you can have systems that can only approach equilibrium asymptotically -- that is , they can be very close to equilibrium but never quite reach it .
there are a few decent rules of thumb for para- and diamagnetism . a system is paramagnetic if it has a net magnetic moment because it has electrons of like ( parallel ) spins . these are often called triplet ( or higher ) states . in atoms and molecules , they occur when the highest occupied atomic/molecular orbital is not full ( degeneracy > 2 * # of valence electrons ) . in this case , hund 's rules suggest that the electrons lower their energy by aligning their spins . in contrast , a diamagnet has no magnetic moment because all electrons are paired . nearly all free atoms are paramagnetic because nearly all atoms have unpaired spins . the exceptions are the the last column of the s , p , d , and f block ( 2 , 12 , and 18 ) . ( any that i am missing ? ) for instance , that is an important property for the stern-gerlach experiments and magnetic trapping . most molecules , however , have fully paired spins . first off , most molecules have an even number of spins , except for free radicals , which are relatively unstable . to figure out if the molecule has a net magnetic moment ( paramagnetic ) or not ( diamagnetic ) , you need to look at its molecular orbitals . the classical example is oxygen , which has a half-full ( or half-empty ) $\pi_{2p}^\ast$ orbitals , and nitrogen , which has a full $\pi_{2p}^\ast$ orbital . see : http://www.mpcfaculty.net/mark_bishop/molecular_orbital_theory.htm . for crystals and solid-state materials , the question is more challenging , but it ends up coming down to the same question : is there a net magnetic moment because of unpaired spins , in which case it is a paramagnetic ? or is there no net magnetic moment because all spins are paired , in which case it is a diamagnet ? of course , in solid-state , there is a third situation , a ferromagnet . this is rather difficult to predict in real systems and is a major field of research . some model systems ( model system : a much simplefr mathematical model of a system ) are solvable and give hints of what to look for . for instance , free spins in a lattice create a paramagnet by the argument above : the crystal has a net magnetic moment . you expect , in a magnetic field , the spin of one electron creates a magnetic field that can effect its neighbors . since the system is paramagnetic , you might expect that the neighbors align with their local magnetic field , which is induced by their neighbors , and the whole crystal polarizes itself , creating a ferromagnet . this explanation is a mean-field ising model . it gives a good intuition even though it is too simple to describe any real system .
let us define the inertial mass , gravitational mass and rest mass of a particle . inertial mass : - to every particle in nature we can associate a real number with it so that the value of the number gives the measure of inertia ( the amount of resistance of the particle to accelerate for a definite force applied on it ) of the particle . using newton 's laws of motion , $m_i = f/a$ gravitational mass - ( this is defined using newton 's law of universal gravitation i.e. the gravitational force between any two particle a definite distance apart is proportional the product of the gravitational masses of the two particles . ) to every particle in nature we can associate a real number with it so that the value of the number gives the measure of the response of the particle to the gravitational force . $f = \frac{gm_{g1}m_{g2}}{r^2}$ all experiments carried out till date have shown that $m_g = m_i$ this is the reason why the acceleration due to gravity is independent of the inertial or gravitational mass of the particle . $m_ia = \frac{gm_{g1}m_{g2}}{r^2}$ if $m_{g1} = m_i$ then $a = \frac{gm_{g2}}{r^2}$ that is acceleration due to gravity of the particle is independent of its inertial or gravitational mass . rest mass - this is simply called the mass and is defined as the inertial mass of a particle as measured by an observer , with respect to whom , the particle is at rest . there was an obsolete term called relativistic mass which is the inertial mass as measured by an observer , with respect to whom , the particle is at motion . the relation between the rest mass and the relativistic mass is given as $m = \frac{m_0}{\sqrt{1-v^2/c^2}}$ where $v$ is the speed of the particle and $c$ is the speed of light , $m$ is the relativistic mass and $m_0$ is the rest mass .
this is going to be essentially the same in content as jerry schirmer 's response , but i thought you might like to hear it in more mathematical terms . the velocity function $v$ is defined as $$ v ( t ) = \dot{ x} ( t ) $$ let 's take the domain of the position function to be the open interval $ ( t_1 , t_2 ) $ and suppose that it has the property that given any point $x_0$ in the range of $x$ , there is a unique point $t_0$ in its domain $ ( t_1 , t_2 ) $ such that $x ( t_0 ) = x_0$ . then there exists a function $x^{-1}$ ( the inverse of $x$ ) defined on the range of $x$ satisfying $$ x^{-1} ( x ( t ) ) = t $$ now we define a function $\bar v$ on the range of $x$ by $$ \bar v ( x ) = v ( x^{-1} ( x ) ) $$ it is common to abuse notation here and use $v$ in place of $\bar v$ for this function , but let 's keep things notationaly rigorous . then on one hand the chain rule gives $$ \frac{d}{dt}\bar v ( x ( t ) ) = \frac{d\bar v}{dx} ( x ( t ) ) \ , \dot x ( t ) = \frac{d\bar v}{dx} ( x ( t ) ) \ , v ( t ) $$ while on the other hand we use the definition of $\bar v$ to write $$ \frac{d}{dt}\bar v ( x ( t ) ) = \frac{d}{dt} v ( x^{-1} ( x ( t ) ) ) = \frac{dv}{dt} ( t ) = a ( t ) $$ and combining these observations gives the identity you wanted $$ a ( t ) = \frac{d\bar v}{dx} ( x ( t ) ) \ , v ( t ) $$ notice that if we indulge in the usual abuse of notation , then we can simply write this as $$ a = v \frac{dv}{dx} $$
the paper you quote covers a similar case , which was solved previously by s.t. ma ( phys . rev . 69 no . 11-12 ( 1946 ) , p . 668 ) , but deals with the scattering problem on the tail of the exponential - hence the complex energies . what follows is partly inspired by that paper but is quite distinct from it . the tricky part is not getting scared by the bessel functions , but that is why we have the theory of special functions . for one , the exponential potential $e^{|x|}$ you ask for is an even function , which means that the corresponding eigenfunctions on $ ( -\infty , \infty ) $ will be either even or odd . therefore , they can be treated as eigenfunctions of the simpler potential $e^x$ on $ ( 0 , \infty ) $ , with boundary condition $\psi' ( 0 ) =0$ or $\psi ( 0 ) =0$ , respectively . since you ask about the latter condition , there is no point in keeping the absolute value . the problem , then , is the eigenvalue problem $$-\frac{d^2}{dx^2}\psi+a e^x \psi=e\psi\text{ under }\psi ( 0 ) =0=\psi ( \infty ) . \tag{problem}$$ ( a word on dimensions : to get the equation down to this form , we have had to set $\hbar , m$ and the length scale of the exponential to 1 , by taking appropriate units of time , mass and length respectively . this means that there is no more dimensional freedom , and the hamiltonian has one free parameter , $a$ , which will affect not only the scale of the spectrum ( which you might expect as $a$ is a scaling on the potential ) but also its structure . ) amore et al . treat this as a boundary-value problem in $\mathbb c$ and using a change to a complex variable . this complicates the issue more than is really necessary and for simplicity i will use only real variables , though this comes at the cost of dealing with modified bessel functions instead of standard ones . the initial step is to change variable to $z=2\sqrt{a}e^{x/2}$ , so that $ae^x=z^2/4$ and derivatives transform as $$ \frac {\partial }{\partial x}=\frac {\partial z}{\partial x}\frac {\partial }{\partial z}=\frac {z }{2}\frac {\partial }{\partial z} \text{ so } \frac {\partial^2 }{\partial x^2} =\frac14\left ( z^2\frac {\partial^2 }{\partial z^2}+z\frac {\partial }{\partial z} \right ) . $$ the final equation is thus $$ \left [ z^2\frac {\partial^2 }{\partial z^2}+z\frac {\partial }{\partial z}- ( z^2+\nu^2 ) \right ] \psi=0 \tag{equation} $$ where $\nu=i\sqrt{4e}$ . ( yes . some complexness is inevitable . no fear , it will eventually not matter . ) this equation is bessel 's equation in modified form with index $\nu$ . this is exactly the same as bessel 's equation for more normal situations ; the index is complex but that is all . two linearly independent solutions are the modified bessel functions of the first and second kind , $i_\nu ( z ) $ and $k_\nu ( z ) $ , so the general solution of $ ( \text{equation} ) $ looks like $$ \psi ( z ) =ai_\nu ( z ) +bk_{\nu} ( z ) . $$ we then only need to impose the boundary conditions $\psi|_{z\rightarrow \infty}=\psi|_{z=2\sqrt{a}}=0$: the condition at infinity requires that we set the coefficient of $i_\nu$ to zero , since the first kind function always explodes . we could have done this from the start : $k_\nu$ is , by definition , the exponentially decaying solution , while $i_\nu$ grows exponentially . the condition at $x=0$ then simply requires that $k_\nu ( 2\sqrt{a} ) =0$ . in terms of energies , then , $$ \boxed{k_{2i\sqrt e} ( 2\sqrt{a} ) =0 , } $$ and this is your quantization condition . as it happens , $k_\nu ( z ) $ is real for real $z$ and purely imaginary $\nu$ . one way to prove this is via this integral representation : $$ k_{\nu} ( x ) =\sec ( {\nu\pi}/{2} ) \int_{0}^{\infty}\cos ( x \sinh\nolimits t ) \cosh ( \nu t ) dt , $$ which is the analogue of bessel 's first integral for $k_\nu$ . i must confess , though , that my intuition is not as good here and i can not really point to the deep reason for that . since $k_\nu$ is real here , for whatever reason , we can ask for its zeros . as with all bessel zeros there is no chance of an elementary formula for them , but they can be found quite easily using numerical methods ( for properties of the zeros , see this dlmf reference ) . for a taster , here are some graphs , in log-linear scale ( so zeros show up as downward , log-like peaks ) , of $k_{2i\sqrt{e}} ( 2\sqrt{a} ) $ as a function of $e$ , for a few different values of $a$ . while there is not all that much to say about the energies from this , it is clear that there are a countable infinity of them , that they are bigger than $a$ , and that their spacing increases with increasing $a$ and $n$ ( why ? ) - but that is really all you had really want to know ! just for completeness : the eigenfunctions themselves , then , are of the form $$\psi_n ( x ) =c_nk_{2i\sqrt{e_n}}\left ( 2\sqrt{a}e^{x/2} \right ) . $$ it is interesting to note that the dependence in $n$ comes through the index instead of a coefficient before $x$ . this is partly to ensure the very strict decay $\psi\sim e^{-\exp ( x/2 ) }$ , which is required by the very hard exponential wall of the potential . for some information on how these bessel functions behave , try the functions of imaginary order subsection in the dlmf ; particularly important results are asymptotics on $k_{i|\nu|}$ at large $x$ and for the oscillatory region . the latter is $$ {k}_{{i\nu}}\left ( z\right ) =-\left ( \frac{\pi}{% \nu\mathop{\sinh}\left ( \pi\nu\right ) }\right ) ^{{\frac{1}{2}}}% \mathop{\sin}\left ( \nu\mathop{\ln}\left ( \tfrac{1}{2}% z\right ) -\gamma_{\nu}\right ) +\mathop{o}\left ( z^{2}\right ) , $$ so the asymptotic for the wavefunction is of the form $\psi ( x ) \sim\sin\left ( \sqrt{e_n}x\right ) $ , as it should be . ( note , though , that this holds little physics beyond the standard : the information on the potential 's variation is encoded in the change of the instantaneous frequency as in e.g. these formulas , and would require beefier maths . )
you have a few longer answers which were already updated , but here is a concise statement of the situation in mid-2014: an independent measurement by the icarus collaboration , also using neutrinos traveling from cern to gran sasso but using independent detector and timing hardware , found detection times " compatible with the simultaneous arrival of all events with equal speed , the one of light . " in an edited press release ( and probably in the peer-reviewed literature as well ) , all four of the neutrino experiments at gran sasso report results consistent with relativity . the mumblings that begin a few months after the initial report , that a loose cable caused a timing chain error , have been accepted by the experimenters . frédéric grosshans links to a nice discussion by matt strassler which includes this image : you can clearly see that the timing offset was introduced in mid-2008 and not corrected until the end of 2011 . it is important to remember the scale of the problem here . in vacuum , the speed of light is one foot per nanosecond . in copper/poly coaxial cable it is slower , about six inches per nanosecond , and in optical fiber it is comparable . a bad cable connector can take a beautiful digital logic signal and reflect part of it back to the emitter , in a time-dependent way , turning the received signal into an analog mess with a complicated shape . and a cable can go bad if somebody hits it the wrong way with their butt while they are working in the electronics room . ( i actually had something similar happen to me on an experiment : i had an analog signal splitter " upstairs " that sent a signal echo back to my detectors " downstairs " , and a runty little echoed pulse came back upstairs after about a microsecond and got processed like another event . i wound up spending several thousand dollars on signal terminators to swallow the echo downstairs . it was an unusual configuration and needed unusual termination hardware and i must have answered the question " but could not you just " a hundred times . ) gran sasso is an underground facility for low-background experiments — the detectors can not see gps satellites directly , because there is a mountain in the way , and their access to the surface is via a tunnel whose main purpose is to carry traffic for a major italian motorway . i am quite impressed that they had ~100 ns timing resolution between the two laboratories ; the " discovery " came about because they were trying to do ten times better than that . as an experimentalist i do not begrudge the opera guys their error at all . i am sure they spent an entire year shitting pineapples because they could not identify the problem . when they finally did release their result , they had the courage to report it at face value . the community was properly incredulous and the wide interest prompted a large number of other checks they could make . independent measurements were performed . an explanation was found . science at its best .
everything is affected by gravity . gravity warps of space-time according to the einstein field equations , and traveling on " geodesics " ( shortest path curves ) on that curved surface is how gravity is manifested . thinking as though there is some sort of euclidean space underneath the non-euclidean space-time in which neutrinos can take a more direct " straight line " between the points is not at all supported . everything we know of travels on this curved spacetime . also , speaking more newtonianly , neutrinos do seem to have mass in the more classic sense .
consider the way the earth is illuminated by the sun : light travels in straight lines , so on the earth we only get the light that happens to be travelling in our direction , and of course this is a tiny fraction of all the light emitted by the sun . now suppose we could place a gigantic lens in between the sun and the earth to focus the sun 's light onto the earth : the lens captures more of the sun 's light than the earth does , because it is closer to the sun and bigger than the earth . so if it focusses all this light onto the earth then the brightness of the sun 's light on the earth is going to be much greater than normal . so it will increase the illumination of the earth - i am guessing that this is what you mean by illuminating a planet . this works because the angle $\alpha$ subtended by the lens is much greater than the angle subtended by the earth . this allows the lens to capture lots of light and focus all that light on the earth . but we could not create a gravitational lens that big in the solar system bcause the mass needed would be vastly greater than the mass of the sun and all the planets combined . even the sun , which is by far the heaviest object in the solar system can only deflect light by about 0.0005° i.e. the angle $\alpha$ in the diagram above would only be 0.0005° . this is about a factor of 5 less than the angle subtended by the earth , so such a lens would increase the illumination of the earth by only a small amount . and this is the problem with your plan for illuminating a planet . you need lenses capable of capturing light over a wide angle $\alpha$ , but any lens that would fit in between the sun and the planet would not capture enough light to make much difference . there are much bigger lenses , like galaxy clusters , but they are millions of light years away so they are not much use .
the gravitational force exerted by a black hole is strong because the force is proportional to the mass and the black hole as a large mass . in the same way , the gravitational field of the earth may beat a weak enough magnet - because the earth is large and the magnet is small and we are comparing apples and oranges . but fundamentally speaking , among particles with masses that correspond to elementary particles , gravity is the weakest force . for example , the gravitational force between two electrons is $10^{43}$ times weaker than the electrostatic one . the weak nuclear force is as strong as the electromagnetic one at very short distances - but exponentially drops at distances much longer than the w-boson wavelength . the strong nuclear force is the strongest among the four forces . it seems that the weakness of gravity relatively to all other forces , when evaluated at the level of elementary particles , is a general principle that has to hold in any consistent quantum theory of gravity , see http://arxiv.org/abs/hep-th/0601001
there is no flaw in your argument . a computer heats the room just as effectively as an electric heater of the same power and you could use the computer to do something useful ( bitcoin mining ? ) while it is heating your room . there are some practical considerations , though i think these have been sufficiently discussed in the comments . computers would make for noisy and bulky heaters , though anyone who has had to refridgerate a server room will tell you they can be very effective heaters . if you want to be really , really , pedantic the computer may not be quite as effective as a heater because an infinitesimally small amount of it is power may go into the data on its disk drive . see is a hard drive heavier when it is full ? for a discussion of this , but note that i mention this for its amusement value rather tjhan because i seriously believe it is an issue .
zimmerman , vega , poisson , and haas published a paper on exactly that topic just this february . having read this paper , it is not definitive proof that such overcharging cannot occur , but it is a strong argument for that position . if you do not have access to physical review , then you can get a copy of the the paper from the arxiv . these articles referenced in the above 2013 paper are also highly relevant : 1 2 3 . in particular , 3 is written as if to decapitate the idea of overcharging a black hole ; it contains the following : " . . . we conclude that there is no supporting evidence that indicates the violation of the cosmic censorship in the proposed overcharging process . " i checked all other recent papers with e . poisson as coauthor . these ones are related , but not on exactly your question : 4 5 6 7 . i did this lit survey using the isi web of knowledge ; you can get access to it via a research institute or university network , if you have access to one . free versions of all of the papers linked above can be found on the arxiv : 1 2 3 4 5 6 7 .
when you solve a problem like this , you are using a system of reference ( actually you use one in all problems , but here it is very explicit ) . in this case , the easiest one is y in the vertical and x in the horizontal . almost all the forces are already in one of these 2 directions . namely , you have all the weights pointing downwards , so in the -y direction and all normal forces pointing upwards , so +y direction . but you have a force ( the one applied to body a ) that is in neither directions purely . it is in a crooked direction , so it has a component in the y-axis and another in the x-axis . that is why you use sine and cosine . you are calculating how much of this force is in the vertical direction ( projection in the y-axis ) and how much is in the horizontal direction ( projection in the x-axis ) . be very careful when asking sin or cos , because they depend on the angle you choose to represent the vector . in this case , the exercise says it is an angle with the horizontal , so a cosine will give you the horizontal component ( adjacent to the angle ) , while sine will give you the y-component ( opposite to the angle ) . but as it was said before , the best you can do is to draw a diagram and realize which component the sine or cosine of that angle will give you . as it was also mentioned , limiting cases help a lot ! ! for instance , in your particular case , you could ask : " if the angle were 0 with the horizontal , what do i expect ? " well , if it was 0 , that means it is completely in the horizontal direction , therefore you expect a cosine there , since cosine ( 0 ) = 1 , so all your vector is in the horizontal component . while for the y-component you get sin ( 0 ) = 0 , which is also expected since you have no projection there at all . the case of 90 degrees is also very good , since you can follow the same reasoning and realize that the roles change . you expect all of the force in the y-axis and nothing on x . and you get that from sin ( 90 ) = 1 and cos ( 0 ) = 0 . but again , do not memorize , this will always be sine and this always cosine , since it is very easy to get tricked by doing this . one can give you the angle with respect to the vertical axis and sine and cosine change places .
there is a definine velocity and momentum , we just do not know it . nope . there is no definite velocity--this was the older interpretation . the particle has all ( possible ) velocities at once ; it is in a wavefunction , a superposition of all of these states . this can actually be verified by stuff like the double-slit experiment with one photon--we cannot explain single-photon-fringes unless we accept the fact that the photon is in " both slits at once " . so , it is not a knowledge limit . the particle really has no definite position/whatever . is not that equivalent to saying because we have not seen star x , it does not exist ? it is limiting the definition of the universe to the limits of our observation ! no , it is equivalent to saying " because we have not gotten any evidence of star x , it may or may not exist --it is existence is not definite " technically , an undetected object does exist as a wavefunction . though it gets slightly philosophical and boils down to " if a tree falls in a forest and no one is around to hear it , does it make a sound ? "
if i was on a bus at 60 km/h , and i started walking on the bus at a steady pace of 5 km/h , then i would technically be moving at 65 km/h , right ? not exactly right . you would be correct if the galilean transformation correctly described the relationship between moving frames of reference but , it does not . instead , the empirical evidence is that the lorentz transformation must be used and , by that transformation , your speed with respect to the ground would be slightly less than 65 km/h . according to the lorentz velocity addition formula , your speed with respect to the ground is given by : $$\dfrac{60 + 5}{1 + \dfrac{60 \cdot 5}{c^2}} = \dfrac{65}{1 + 3.333 \cdot 10^{-15}} \text{km}/\text h \approx 64.9999999999998\ \text{km}/\text h$$ sure , that is only very slightly less than 65 km/h but this is important to your main question because , when we calculate the speed of the light relative to the ground we get : $$\dfrac{60 + c}{1 + \dfrac{60 \cdot c}{c^2}} = c$$ the speed of light , relative to the ground remains c !
you are supposed to solve as follows , the mechanical energy followed by reaching from the top to the bottom point is given by : gpe = 60×10× ( 223-50 ) { g is taken as 10 for simplicity } now the question says that 30% of this energy got wasted while the cords were being stretched , so 70% of gpe is the required answer . the mistake you were doing was that you had taken the change in gpe for the complete 223 m , however the person does not reach the bottom point . also 0.7 × 60 × 10 × 173 = 72660 j .
there is spontaneous fission , a rare decay mode for some superheavy nuclei like uranium and plutonium . there is also induced fission , where some interaction with the environment ( typically neutron capture ) causes a nucleus to split . both fissions typically produce a couple of free neutrons , which may be captured on other fissionable nuclei , or may be captured on some nearby neutron absorber ( a " control rod " or a " neutron poison" ) , or may escape from the reaction volume to infinity . the " critical mass " is the amount of fissionable material needed so that each fission is likely to trigger , on average , one other fission . it is not a property of a single nucleus , but of the fuel assembly , its geometry , and its environment .
the historical formulation of the sm involved one higgs doublet and only renormalizable couplings , the latter being due to the focus at the time on achieving a renormalizable formulation of the weak interactions . with these restrictions neutrinos are massless and do not oscillate . to get neutrino masses you need to extend this framework either by adding non-renormalizable dimension 5 operators , which one would naturally expect to be there in the framework of effective field theory , or you have to add renormalizable couplings involving new fields , typically including sm singlet weyl fermions ( i.e. . right-handed neutrinos ) and a sm singlet higgs field . how much of an extension of the sm this really involves is subjective . there were many theoretical papers speculating on such extensions before the actual discovery of neutrino oscillations .
here 's a video of physicist richard feynman discussing this question . imagine a blue dot and a red dot . they are in front of you , and the blue dot is on the right . behind them is a mirror , and you can see their image in the mirror . the image of the blue dot is still on the right in the mirror . what is different is that in the mirror , there is also a reflection of you . from that reflection 's point of view , the blue dot is on the left . what the mirror really does is flip the order of things in the direction perpendicular to its surface . going on a line from behind you to in front of you , the order in real space is your back your front dots mirror the order in the image space is mirror dots your front your back although left and right are not reversed , the blue dot , which in reality is lined up with your right eye , is lined up with your left eye in the image . the key is that you are roughly left/right symmetric . the eye the blue dot is lined up with is still your right eye , even in the image . imagine instead that two-face was looking in the mirror . ( this is a fictional character whose left and right side of his face look different . his image on wikipedia looks like this : ) if two-face looked in the mirror , he would instantly see that it was not himself looking back ! if he had an identical twin and looked right at the identical twin , the " normal " sides of their face would be opposite each other . two-face 's good side is the right . when he looked at his twin , the twin 's good side would be to the original two-face 's left . instead , the mirror two-face 's good side is also to the right . here is an illustration : so two-face would not be confused by the dots . if the blue dot is lined up with two-face 's good side , it is still lined up with his good side in the mirror . here it is with the dots : two-face would recognize that left and right have not been flipped so much as forward and backward , creating a different version of himself that cannot be rotated around to fit on top the original .
your confusion comes from the difference between special and general relativity . in special relativity , the space-time manifold is assumed to carry the structure of 4-dimensional minkowski space , which has the nice property that it is canonically identified with its own tangent space at the origin ( since it is a vector space ) . so in special relativity you can speak of a space-time event as a 4-vector , and you can also speak of global lorentz transformations ( by doing the local lorentz transformation in the tangent space and propagating the transformation to the whole space-time using the canonical identification ) . in general relativity , however , the space-time is allowed to be an arbitrary lorentzian manifold ( what we do is break the global lorentz symmetry of minkowski space and require it to only hold infinitesimally , i.e. on the tangent space ) , and you do not have a canonical way of identifying the entire space-time with the tangent space of a fixed point . therefore you cannot speak of a space-time event ( now just a point in your space-time manifold ) as a 4-vector ! edit let me try to make the difference between an affine and non-affine space more apparent . let us start by considering a two dimensional manifold . in fact , we will just compare the usual flat plane and the sphere . on the plane , we can fix an arbitrary point and call it the origin . starting from this origin , pick a direction , and call it the $x$ direction . consider the following two paths . first you go 1 unit in the $x$ direction . turn left for 90 degrees . travel another unit . first you face the $x$ direction . now turn left for 90 degrees , travel one unit . turn right now for 90 degrees and travel another unit . these two operations take you to the same place . on the sphere , however , you can again fix an arbitrary point and call it the " origin " , and a direction which we call "$x$" . now consider first you go a quarter the way around the sphere in the $x$ direction . turn left for 90 degrees , travel another quarter of the circumference . first you face $x$ direction , turn left for 90 degrees , travel a quarter of the circumference . now turn right and travel a quarter of the circumference . ( try to trace this out on a tennis ball or on an orange . ) you do not end up at the same place ! now , going back to the plane , if instead you start by going " face $x$ . turn left for 45 degrees . travel for $\sqrt{2}$ units . " you will again end up at the same place as the previous two tries . but on the sphere , if you start by " face $x$ . turn left for 45 degrees . travel for $\sqrt{2}$ times a quarter circumference . " you will end up somewhere else entirely again compared to the previous two tries . what does this mean ? on a flat plane , from the above demonstration , after fixing an origin and a direction $x$ , we can uniquely describe every point on the plane as " certain units in the $x$ direction , and then certain units to the left of that " . and it does not matter at all in which orders you zigzag to the destination . this identifies the plane with a vector space . that is , you can say that " the displacement from point $p$ to point $q$ is the vector $\vec{v}$ , and the displacement from point $q$ to point $r$ is the vector $\vec{w}$ , so the displacement from the point $p$ to point $r$ is the sum of vectors $\vec{v} + \vec{w} = \vec{w} + \vec{v}$" . so after fixing a point as the origin , we can describe all other points as a vector displacement relative to this origin , and add and subtract the vectors as appropriate . on the surface of the sphere ( a curved manifold ) , however , the above example shows that the intuition we learned from studying classical mechanics about using vector addition for the " displacement vector " cannot hold for the positions of points . so you should not think about space-time events in general relativity ( which is a point on some possibly curved manifold ) , as vectors relative to a fixed origin . ( because with vectors you would be tempted to add them and so forth . )
the beam of white light is refracted by the prism , but the different wavelengths in the light are refracted by different angles . that means when the light falls on the screen the different wavelengths are spread out over a region of the screen . this is what is meant by dispersion . the dispersion is linear if the different wavelengths of light are spread out evenly across the screen . so the wavelength at the point i have shown as $d$ would be given by : $$ \lambda_d = \lambda_{red} - \frac{d \left ( \lambda_{red} - \lambda_{blue}\right ) }{l} $$ in general the dispersion will not be linear , but for glass across the range of red to blue light it is not a bad approximation . the reason this matters is when you try to calculate the coherence length from the size of the slit . the coherence length is given by : $$ l = \frac{2\space ln ( 2 ) }{\pi n} \frac{\lambda^2}{\delta \lambda} $$ because the slit has a finite width the image is casts on the screen has a finite width and therefore covers a range of frequencies . going back to my diagram above , suppose the image of the slit on the screen has a width of $w$ , then if the dispersion can be assumed linear the range of frequencies is simply : $$ \delta \lambda = \frac{w}{l} \left ( \lambda_{red} - \lambda_{blue} \right ) $$ put this value for $\delta \lambda$ back into the equation for the coherence length to get the effect of the slit width on coherence length .
it seems to me that you have more of a conceptual issue than a mathematical one . to hopefully remedy this , let me remind you of a couple of facts . given an electric field $\mathbf e$ , an electric potential $v$ for $\mathbf e$ is any scalar function $v$ for which \begin{align} \mathbf e = -\nabla v \end{align} it follows that if $v$ is such a potential , then we can integrate both sides along a curve $c$ to obtain \begin{align} \int_c\mathbf e\cdot d\boldsymbol \ell = -\int_c \nabla v\cdot d\boldsymbol \ell \end{align} if $c$ is a curve with endpoints $\mathbf a$ and $\mathbf b$ , then the gradient theorem tells us that the right hand side can be evaluated in terms of the values of $v$ at these endpoints alone ; \begin{align} -\int_c \nabla v\cdot d\boldsymbol \ell = v ( \mathbf a ) - v ( \mathbf b ) \end{align} we now have the freedom to choose a reference point at which we decide what the value of the potential is ( this comes from the fact that in step 1 , the condition that the field be the gradient of the potential does not uniquely specify the potential ) at some chosen reference point $\mathbf b = \mathbf x_0$ , and then combining steps 2 and 3 allows us to compute the value of the potential at any other point $\mathbf a = \mathbf x$ . in other words , combining these remarks with steps 2 and 3 we obtain \begin{align} v ( \mathbf x ) = v ( \mathbf x_0 ) + \int_c \mathbf e\cdot d\boldsymbol \ell \end{align} where $c$ is any path from $\mathbf x$ to $\mathbf x_0$ . in short , the electric potential is computed by choosing its value at a certain reference point , and then performing a line integral along any path to another point at which you want to determine its value . in this way , you can obtain the functional form of $v$ at any point $\mathbf x$ you like .
a computer model for soda bottle oscillations : " the bottelator " describes a model for this . that is a pay site , but you can find the text here . the model used in the paper is that for a bubble to form it must nucleate then grow to a size big enough to rise to the surface . this process takes a finite time . when the cap is loosened the decrease in pressure causes a burst of nucleation , but after a few seconds the bubbles bursting on the surface raise the pressure again and this inhibits nucleation . as the gas escapes through the loosened cap the pressure falls again and nucleation restarts . the process repeats until the amount of gas being released by the bubbles is not longer enough to inhibit nucleation . note that the paper does not go into the physics of the nucleation and growth process . it just assumes that this is the basis of the oscillation . however the assumption seems reasonable . suppose you take a bottle of soda and momentarily loosen then tighten the caps . you will see a burst of fizzing . loosen then tighten the cap again and you will get another burst , and so on . in this case you are causing the pressure to oscillate by manually controlling the gas flow , but you can see how you had get a similar effect if the gas flow though a slightly loosened cap was just right . it would be interesting to measure the pressure above a saturated co$_2$ solution as a function of gas flow through the cap . i do not have the kit to do this , but it would not be a difficult experiment .
in raman it should say " optical phonon " instead of vibrational oscillation . i am not sure if you can call the vibrations in a molecule phonons , since phonons are the vibrations of the lattice in a solid ( even when the mathematical description , in first approximation are the same ) .
suppose you have a satellite of mass $m$ at a distance $r$ . if we assume the satellite is small enough to behave as a point mass the moment of inertia of the satellite is : $$ i = m r^2 $$ so its kinetic energy is : $$ e = \tfrac{1}{2} i w^2 = \tfrac{1}{2} m r^2 \omega^2 \tag{1}$$ but for a body moving in a circle of radius $r$ at an angular velocity $\omega$ we have the identity ; $$ v = r\omega $$ where $v$ is the tangential velocity . if we substitute for $r\omega$ in equation ( 1 ) we get : $$ e = \tfrac{1}{2}m v^2 $$ which is of course just the usual equation for kinetic energy . so you can treat the energy of the satellite either as rotational kinetic energy or translational kinetic energy as you find convenient . this only works because the satellite is small enough compared to the earth that we can assume every point is moving at the same tangential velocity . if you have some large object pivoting around a point within it , e.g. a disk rotating about its centre , the tangential velocity of all the points in the object will vary with distance from the pivot . in that case it is simpler to assume the angular form for the kinetic energy .
consider the grand canonical ensemble , $$ \rho \sim \exp [ -\beta ( e-\mu n ) ] $$ in the exponent , the inverse temperature $\beta = 1/kt$ is the coefficient in front of one conserved quantity , the ( minus ) energy , while another coefficient , $\beta\mu$ , is in front of the number of particles $n$ . the chemical potential is therefore the coefficient in front of the number of particles - except for the extra $\beta$ . the number of particles has to be conserved as well if $\mu$ is non-zero . as long as the sign of $n$ is well-defined , the sign of the chemical potential is well-defined , too . now , for bosons , $\mu$ can not be positive because the distribution would be an exponentially increasing function of $n$ . note that in the grand canonical ensemble - which is really the ensemble in which $\mu$ is sharply defined - the dual variable to $\mu$ , namely $n$ , is not sharply defined . however , the probability for ever larger - infinite $n$ would be larger , so the distribution would be peaked at $n=\infty$ . such a distribution could not be well-defined . we want , in the thermodynamic limit , the grand canonical ensemble , while assuming a fixed $\mu$ , also generate a finite and almost well-defined $n$ , within an error margin that goes to zero in the thermodynamic limit . that could not happen for bosons and a positive $\mu$ . this catastrophe would be possible because $n=\sum_i n_i$ , a summation over microstates $i$ , and every $n_i$ can be an arbitrarily high integer for bosons . for fermions , the problem does not occur because $n_i=0$ or $1$ for each state $i$ . so for fermions , we can not argue that $\mu$ has to be positive . note that $e-\mu n$ in the exponent is the sum $\sum_i n_i ( e_i-\mu ) $ . for bosons , the problem occurred for states for which $e_i-\mu$ was negative i.e. $e_i$ was low enough . for fermions , however , the number of such states - and therefore the maximum number of fermions in them - is finite , so the divergence does not occur if the chemical potential is positive . for fermions , positive $\mu$ is ok . in fact , for fermions , both positive and negative $\mu$ is ok . also , it is easy to see that if both particles and antiparticles exist , $\mu$ of the antiparticle has to be minus $\mu$ of the particle because only the difference $n_{\rm particles} - n_{\rm antiparticles}$ is conserved ; this is true both for bosons and fermions . so if the potential for electrons is positive , the potential for positrons or holes ( which play the very same role ) has to be negative , and vice versa . nothing changes about the meaning of the chemical potential when one switches from classical physics to quantum physics : in fact , above , i was assuming that there are " discrete states " for the particles , just like in quantum physics - otherwise we would not be talking about bosons and fermions which are only relevant in the quantum setup . classical physics is a limit of quantum physics in which the number of states is infinite because $\hbar$ goes to zero , so a finite number of particles never ends up in " exactly the same state " . in some sense , ludwig boltzmann , while working in the context of classical statistical physics , was inherently using the thinking and intuition of quantum statistical physics - he was a truly ingenious " forefather " of quantum physics . in relativity , one has to be careful how we define the energy of a state . note that the physically meaningful combination that appears in the exponent is $e_i-\mu$ , so if one shifts $e_i$ e.g. by $mc^2$ , the latent energy , one has to shift $\mu$ in the same direction by the same amount . the notions of chemical potential obviously work in relativity , too . relativistic physics is not a " completely new type of physics " . it is just a type of the old physics that happens to respect a symmetry - the lorentz symmetry . again , in quantum field theory which combines both quantum mechanics and relativity , statistical physics including the notion of the chemical potential also works but one must be careful that particle-antiparticle pairs may be created with enough energy . that implies $\mu_{p}=-\mu_{np}$ , as i said . there can not be any general ban on a negative chemical potential of fermions : fermionic $\mu$ can have both signs . however , in the particular theory that allan wanted to describe , he could have had more detailed reasons why $\mu$ should have been positive for his fermions . i am afraid that this would be an entirely different , more specific question - one about superconductors . as stated , your question above was one about statistical physics and i tend to believe that the text above exhausts all universal facts about the sign of the chemical potential in statistical physics .
draw the circuit using ideal circuit elements : now , the series current is : $$i = \dfrac{\mathcal{e}}{r_{internal}+ r_{load}}$$ the voltage across the internal resistance is : $$v = \mathcal{e} \dfrac{r_{internal}}{r_{internal}+ r_{load}} $$ the power dissipated by the internal resistance can by found three equivalent ways : $$p = vi = \dfrac{v^2}{r_{internal}} = i^2 r_{internal} = \mathcal{e}^2 \dfrac{r_{internal}}{ ( r_{internal}+ r_{load} ) ^2}$$ clearly , setting $r_{load} = 0$ yields : $$i = \dfrac{\mathcal{e}}{r_{internal} + 0} = \dfrac{\mathcal{e}}{r_{internal}}$$ $$v = \mathcal{e} \dfrac{r_{internal}}{r_{internal + 0}} = \mathcal{e}$$ $$p = \dfrac{\mathcal{e}^2}{r_{internal}}$$ simply put , the entire emf appears across the internal resistance . zero volts appears across the source plus internal resistance due to the short circuit .
the human eye is actually quite similar to a camera . it also requires a finite exposure time to build up sufficient photon signal . if the source is moving over that time ( typically about 0.1 sec for human eye ) , it will be blurred out over the exposure .
indeed , without gravity , and thus without buoyancy , there is no preferred direction for the candle flame . with gravity , like you said , the products of the combustion are much lighter than the unburned air and go upwards . fresh air is convected from the surroundings at the bottom of the flame to react with the fuel . hence why a longer wick will burn better than a one that just peeks out : it is easier to bring in the fresh air ( and thus oxygen ) . without gravity , the immediate surrounding oxygen would burn initially but there is no convection to get the products out of the way and fresh oxygen close to the wick where the fuel vapor is . mass diffusion takes over , mixing the products with fresh oxygen coming from the outside but it is a lot slower than convection . since there is no preferential direction ( ie gravity field ) , the resulting flame is a weak , almost spherical flame . if you want to know more about this , you can google " flame balls " , which are not a candle flame without the wick : it is not a blob of fuel reacting with the surrounding oxygen , but instead how a combustible mixture of premixed fuel and air/oxygen reacts when ignited locally . the combustion products are inside the ball while the reactants diffuse to the flame . a good starting point is this old nasa page ( the picture above comes from there ) .
you are right - for isolated galaxies , there is no obvious way of discerning whether they are made of matter or antimatter , since we only observe the light from them . but if there are regions of matter and antimatter in the universe , we would expect to see huge amounts of radiation from annihilation at the edges of these regions . but we do not . you could also make the case that galaxies are well-separated in space , and there is not much interaction between them . but there are plenty of observed galaxy collisions even in our own small region of the universe , and even annihilation between dust and antidust in the intergalactic medium would ( probably ) be observable .
i quite like your characterization of the partial trace ! i think you perceive a conflict with the wikipedia definition because you are only taking part of the latter : given an operator $t\in l ( v\otimes w ) $ , the requirement that its partial trace obey $$\text{tr}_w ( t ) \in l ( v ) $$ simply says that the partial trace over $w$ be an operator on $v$ , but that does not say which operator . ( the specification of that is done in a more concrete , basis-dependent way . ) to obtain the first result that confuses you , $\langle k |r|l\rangle=\text{tr} ( |l\rangle\langle k|r ) $ for some operator $r$ , simply take the trace in the same orthogonal basis where $|k\rangle$ and $|l\rangle$ came from : $$ \text{tr} ( |l\rangle\langle k|r ) =\sum_j \langle j|l\rangle\langle k|r|j\rangle =\sum_j \langle k|r|j\rangle\langle j|l\rangle =\langle k|r|l\rangle . $$ now , if you take $r=r_b=\text{tr}_a ( r_{ab} ) $ , the matrix elements of this partial trace in the $b$ basis are , from the above , $$ \langle k|r_b|l\rangle=\text{tr}_b ( |l\rangle\langle k|r_b ) =\text{tr}_{ab} ( ( \mathbb i\otimes|l\rangle\langle k| ) r_{ab} ) , $$ where the second equality is simply the fundamental definition of the partial trace , as you formulated it . now , i can understand it if all this simply looks complicated and does not provide any insight into what is going on - though that simply means that you need to look more closely into what your fundamental definition is saying . say i have a bipartite system $a\leftrightarrow b$ , which may be initially entangled , and then i completely forget about the $a$ part of the system . thus , i need to trade my full ( possibly entangled ) density matrix $\rho_{ab}$ for one i can deal with locally : a density matrix $\rho_b$ which acts only on the $b$ side , which i can act on with operators in $l ( h_b ) $ , and which i can take the $b$ trace on . that is , i need to be able to speak of the object $$\text{tr} ( l_b\rho_b ) , $$ and that object embodies all i need in order to make predictions . however , in terms of the full system , the state is $\rho_{ab}$ , when i operate on it i am really using the operator $\mathbb i\otimes l_b$ , and when i take the trace i am really taking the full trace $\text{tr}_{ab}$ over the full space . since both viewpoints must match , these objects must obey $$ \text{tr} ( l_b\rho_b ) =\text{tr}_{ab} ( ( \mathbb i\otimes l_b ) \rho_{ab} ) , \tag{1} $$ and this equation is simply a requirement on the only free object we have , $\rho_b$ , which we call the partial trace $\rho_b:=\text{tr}_a ( \rho_{ab} ) $ . as it happens , requiring $\text{tr}_a$ to obey this for all $l_b\in l ( h_b ) $ and $\rho_{ab}\in l ( h_a\otimes h_b ) $* is enough to uniquely determine it , so that requirement can act as a definition ( though , of course , you can have simpler definitions based on explicit basis-dependent formulae ) . * note that i am taking $\rho_{ab}$ to be a general operator , instead of only a density matrix , since we want to be able to act on $\rho_{ab}$ using entangling or correlated measurements before we forget about $b$ . however , requiring ( 1 ) for all $l_b\in l ( h_b ) $ and only those $\rho_{ab}\in l ( h_a\otimes h_b ) $ such that $\rho_{ab}\geq 0$ and $\text{tr}_{ab} ( \rho_{ab} ) =1$ is enough to determine $\text{tr}_a$ uniquely by linearity , as any operator $r=r_{ab}$ can be decomposed into positive-definite , trace-one operators $r_k$ as $r=r_1 r_1+ir_2r_2-r_3r_3-ir_4r_4$ , with each $r_k\geq0$ , by taking positive and negative parts of its hermitian and antihermtian parts .
in the question you posted the link of , the mathematics is given by the chosen answer . in the second answer there , the importance of conservation of angular momentum is stressed . in general , when considering gravitational solutions one has to think of conservation laws . as an answer here stressed , friction in the remnants of atmosphere reduces the kinetic energy and also angular momentum of the satellites and they may eventually fall to the earth . ( conservation of energy ) . it is instructive to examine another instability , when angular momentum is changed , which happens with the tides in the earth/moon system . the effect of the gravitational interaction is to increase the energy of the moon , which goes into a higher orbit ; i.e. the complex gravitational interaction transfers kinetic energy to the moon . if you read the article you will see that this angular momentum change due to tides induced on the earth can also dominate the path of artificial satellites . the moral of the story is that interactions determine the stability of orbits in gravitational systems , which change following conservation laws .
i wrote an unpublished paper on this once , and presented the poster at an aas meeting . you can actually do some relatively simple model building with this if you look at schwarzschild-de sitter solutions of einstein 's equation . ${}^{1}$ once you have the exact solution , you can then look at circular orbits , and do the usual stability analysis that you use to derive the $r &gt ; 6m$ limit that you get in the schwarzschild solution . what you will find is that , in the schwarzschild-de sitter solution , you will get a cubic equation . one solution will give you a $ r &lt ; 0$ value , which is unphysical , but you will also get an innermost stable circular orbit and an outermost stable circular orbit . the latter represents matter getting pulled off of the central body by the cosmology . there are also exact solutions that involve universes with nonzero matter densities other than the cosmological constant with a central gravitating body . these solutions generically do not have a timelike killing vector nor globally stable orbits . i ran numerical simulations of these orbits and published them in my dissertation 's appendices . edit : stack exchange seems to be killing hotlinks , but search for my name on the arxiv if you are interested . ${}^{1}$edit 2: you can find the schwarzschild-de sitter solution by replacing $1 - \frac{2m}{r}$ everywhere with $1-\frac{2m}{r} - \frac{1}{3}\lambda r^{2}$ . it should be easy enough to prove that that solution satisfies einstein 's equation for vacuum plus cosmological constant .
let us start from classical physics . the fundamental invariance postulate in classical physics is that all physical laws describing an isolated physical system in an inertial reference frame are invariant in form under the action of galileian ( lie ) group . that group is made of $4$ ( lie ) subgroups . ( 1 ) spatial translations , invariance under space translations is also known as spatial homogeneity in a given inertial reference frame . it implies that the total momentum of the system is conserved along its the evolution . ( 2 ) spatial rotations , invariance under space translations is also known as spatial isotropy in a given inertial reference frame . it implies that the total angular momentum of the system is conserved along its the evolution . ( 3 ) temporal translations , invariance under time translations is also known as temporal homogeneity in a given inertial reference frame . it implies that the total energy of the system is conserved along its the evolution . ( 4 ) subgroup of pure galileian transformations ( $t\to t'=t$ , ${\bf x} \to {\bf x}' = {\bf x} + t{\bf v}$ ) . this last type of invariance also implies physical equivalence of all inertial reference frames . it implies the existence of a relation connecting the total momentum of the system and the motion of the center of mass . in special relativistic physics , passing to deal with poincaré group in place of galileo group , the situation is almost identical . it is certainly identical regarding ( 1 ) , ( 2 ) and ( 3 ) as the mentioned groups of transformations are also subgroups of poincaré group . the only relevant difference concerns ( 4 ) . here the subgroup of pure galileian transformation is replaced with the set of pure lorentz transformations : the boosts . invariance under boost implies again physical equivalence of all inertial reference frames . the associated conservation law is a theorem analogous to the classical one , with the fundamental difference that now the total mass of the system includes the contribution of energies of the parts of the system in accordance with $m=e/c^2$ .
black is the absense of color , which our brain visualizes to us in some way . i can not really say that i know how you percieve black . but it is a fact that when no photons reach the photoreceptor cells in your eye , your brain will get a distinct value from it , and that value will be different than when it receives red or blue or any other wavelength of light . the value that your brain reads from your photoreceptor cell will be different for black than for blue - but it does get a value . i guess that value would be 0 , but it might just as well be 1 or 42 - who knows ? :- ) so in that sense , i would say that black is indeed a color . in physics , it is a bit different . it makes no sense talking about colors in physics . in physics , you talk about wavelengths as a continous spectrum of wavelengths from very low frequency to very high frequency . either there is a wavelength , or there is no wavelength . humans have named certain ranges of these wavelengths as colors red , green , blue , violet etcetera - because it makes it easier to communicate . there is no range of wavelengths in the spectrum that has the name " black " - and in physics " black " means that there is no wave at all . at the same time , there is no wavelength that has the name " white " , because white is a combination of all color frequencies at the same time . our eyes give white another distinct value , in the same way as black . no object ( except perhaps black holes ) are completely black . warm objects radiate low frequency light , and can never be " black " . your photoreceptor cells might give your brain the same value as for black , for many differenct physical colors - but no objects are actually black . so the short version of the answer is : your brain probably observes black the same way it observes any other color - as a certain " value " read off a photoreceptor cell - and in that sense , it is a color . in physics , nothing is black .
yes , it is just a short-cut . the main point is that the complexified map $$\tag{a} \begin{pmatrix} \phi \\ \phi^{*} \end{pmatrix} ~=~ \begin{pmatrix} 1 and i\\ 1 and -i \end{pmatrix} \begin{pmatrix} \phi_1 \\ \phi_2 \end{pmatrix} $$ is a bijective map :$\mathbb{c}^2 \to \mathbb{c}^2 $ . notation in this answer : in this answer , let $\phi , \phi^{*}\in \mathbb{c}$ denote two independent complex fields . let $\overline{\phi}$ denote the complex conjugate of $\phi$ . i ) let us start with the beginning . imagine that we consider a field theory of a complex scalar field $\phi$ . we are given a lagrangian density $$\tag{b} {\cal l}~=~{\cal l} ( \phi , \overline{\phi} , \partial\phi , \partial\overline{\phi} ) $$ that is a polynomial in $\phi$ , $\overline{\phi}$ , and spacetime derivatives thereof . we can always decompose a complex field in real and imaginary parts $$\tag{c} \phi~\equiv~\phi_1+ i \phi_2 , $$ where $\phi_1 , \phi_2 \in \mathbb{r}$ . hence we can rewrite the lagrangian density ( b ) as a theory of two real fields $$\tag{d}{\cal l}~=~{\cal l} ( \phi_1 , \phi_2 , \partial\phi_1 , \partial\phi_2 ) . $$ ii ) we can continue in at least three ways : vary the action wrt . the two independent real variables $\phi_1 , \phi_2 \in \mathbb{r}$ . originally $\phi_1 , \phi_2 \in \mathbb{r}$ are of course two real fields . but we can complexify them , vary the action wrt . the two independent complex variables $\phi_1 , \phi_2 \in \mathbb{c}$ , if we at the end of the calculation impose the two real conditions $$\tag{e} {\rm im} ( \phi_1 ) ~=~0~=~{\rm im} ( \phi_2 ) . $$ or equivalently , we can replace the complex conjugate field $\overline{\phi}\to \phi^{*}$ in the lagrangian density ( b ) with an independent new complex variable $\phi^{*}$ , i.e. treat $\phi$ and $\phi^{*}$ as two independent complex variables , vary the action wrt . the two independent complex variables $\phi , \phi^{*} \in \mathbb{c}$ , if we at the end of the calculation impose the complex condition $$\tag{f} \phi^{*} ~=~ \overline{\phi} . $$ iii ) the euler-lagrange equations that we derive via the two methods ( 1 ) and ( 2 ) will obviously be exactly the same . the euler-lagrange equations that we derive via the two methods ( 2 ) and ( 3 ) will be just linear combinations of each other with coefficients given by the constant matrix from eq . ( a ) . iv ) we mention for completeness that the complexified theory [ i.e. . the theory we would get if we do not impose condition ( e ) , or equivalently , condition ( f ) ] is typically not unitary , and therefore ill-defined as a qft .
if the ship is accelerating in a vacuum under its own power , it has to be a rocket ( because momentum is conserved ) and has to lose mass ( because mass-energy is conserved ) . the theoretically most efficient type of rocket is a photon rocket , basically because light ( or anything that moves at $c$ ) gives you the most bang for your buck in terms of momentum per unit energy . as mentioned in the article , the maximum speed is a simple function of the ratio of the rocket 's initial and final rest mass . solving for that ratio gives $$\frac{m_i}{m_f} = \sqrt{\frac{1+v/c}{1-v/c}}$$ so if you wanted to accelerate to , say , 0.5c , the ratio would be $\sqrt{3}$ , and about $1 - 1/\sqrt{3} \approx 42\%$ of your ship 's total mass would have to be fuel . if you want to decelerate again at the other end of your trip , the same ratio applies again , which means the total mass ratio is now $3$ , and $1 - 1/3 = 2/3$ of your ship 's total mass has to be fuel . for an ideal photon rocket , the answer is independent of the rate at which you accelerate . if the ship 's initial mass is $10^6\ , \mathrm{kg}$ , it will weigh only $3\cdot10^5\ , \mathrm{kg}$ after the trip . the rest will be light with a total energy , in the initial and final rest frame , of $mc^2 \approx 6\cdot10^{22}\ , \mathrm{j} \approx 2\cdot10^{16}\ , \mathrm{kwh}$ . compare the world 's current yearly energy consumption of about $2\cdot10^{13}\ , \mathrm{kwh}$ . real rocket ships are vastly less efficient than this .
you have the answer . consider $2*\zeta*\omega_n = 4$ . $\zeta = 0.7$ . $\omega_n^2 = a$ what value of $\omega_n$ ( or $a$ in your case ) satisfies this ? roughly 8.18 is your answer .
the water velocity into a region with atmospheric pressure goes as the square root of the pressure difference by bernoulli 's law . so if you quadruple the pressure difference , you get twice the speed . this is not exact , because as the water fills up your container , unless it is coming from the very top , the building up water will produce a counterpressure . the law is that when fluid is dynamically moving , the kinetic energy of the water coming out per unit mass , which is half the square of the velocity , is the loss in potential energy of the top of the imaginary water column of height h per unit mass , which is gh . so the velocity at the outlet is : $$ v=\sqrt{2gh} = \sqrt{2p \over \rho}$$ exactly the same as if you dropped the water from the top of the imaginary column to the point where you let out the water , and the answer does not depend on whether you have the pressure produced by a column as i imagined it above . this answer conserves energy , since water is disappearing from the top , and appearing at the bottom moving with the same speed as if you dropped it from the top of the column .
the diamond must become quantum as a unit , and the wave function of the quantum diamond must then disperse sufficiently to extend outside the box . at that point the diamond as a whole unit has a probability of jumping outside the box . the first criterion is by far the most difficult , because it can only be achieved by keeping the diamond in total and absolute information isolation from the rest of the universe . that is . . . unlikely to say the least . if even a single photon or phonon " detects " its location , then from that point forward the diamond is classical in the sense that the photon has pinpointed its location . the second criterion is just abysmally slow . because even a small diamond has a lot of mass , its wave function disperses very , very slowly . both of these criteria can be expressed in terms of path integrals , which provide a precise way to quantify the issues i only described conceptually . addendum 2012-06-16 @ollyprice very reasonably asked for clarifications on : ( 1 ) what does total isolation from the universe mean ? , and ( 2 ) what does it mean that the diamond must be kept quantum as a unit ? the most concrete way i can think of to quantify isolation is that the diamond can neither emit or receive particles of matter or energy . keeping out particles of matter is the easier part of that , since it means you " just " need the most absolute vacuum every created , including removal of all high-energy particles such as cosmic rays . preventing energy exchanges is much , much harder . the vacuum keeps you from exchanging phonons ( sound quanta ) , so you get two ( matter and phonons ) for the price of one with that one . your suspension system would need to be phonon-free , however , at least if you do the experiment here on earth . that leaves mostly electromagnetic radiation . radio frequencies whose wavelengths are a lot larger than the distance you want the diamond to jump are not a big problem , although if you get enough of them you may start locating the diamond too well and thus " lose coherence " as they say these days ( it is the same idea ) . so , that leaves mostly the higher frequencies of very short microwaves through infrared and light . infrared is going to be the biggest issue , so you want both the diamond and the cavity in which you do the experiment to be cold , as close to absolute zero as possible , to prevent stray space-locating infrared photons from traveling in either direction . . . . and having said all that , i must also say : hmm ! that portfolio of preconditions is not quite as impossible as i had always assumed . so , someone could possibly do this for real in a high-end lab , using something like , say , a bacterium ( huge ! ) . . . or much better , something a lot smaller , such as a low-end nanodiamond . the suspension system for the earthbound version would be the trickiest part . hmm . maybe a single embedded electron charge ? no , much easier : a superconducting particle over a dish magnet . or maybe even better : a very small piece of pyrolytic carbon similarly suspended by a magnetic field , though i do not know for sure what would happen to that materials extreme form of diamagnetism at such low temperatures . that is a truly fascinating possibility ! does anyone here know if anyone has ever tried this ? i certainly do not know . but wow , what a fascinating possibility : a material object , albeit a very very very tiny one , quantum-jumping through a physical barrier ? now that would be a thesis paper ! on to question ( 2 ) , what makes the diamond a " unit " ? that one 's easy ! bonding . the covalent bonds of the diamond keep it internal components aligned strongly with each other , and so unable to deviate over time into less certain relationships . that is not to say you can not get some weird stuff going on internally , but it will not in general be capable of disrupting the diamond 's internal structure . notice the inverse relationship here between isolation and cohesion ( bonding ) . that is , anything that bonds two objects together physically ( phonon-mediated ) or via information keeps them from " going quantum " relative to each other . conversely , it is the lack of such bonding ( isolation ) that enables relative quantum behavior . when the universe as a whole is one of those two partners , " relative quantum behavior " becomes a pretty one-sided concept , since we in the universe just stay classical . however , for something like two very small objects isolated both from each other and from the universe , the concept of relative quantum behavior becomes a testable idea . that would be where each system sees the other as being the quantum one , whenever they finally do interact with each other . ( fair warning : i just now made up the phrase " relative quantum behavior" ; it is not standard . however , the fully quantum frameworks for studying systems such as positronium would necessarily contain an equivalent concept , since for example in positronium the electron and positron are necessarily equally " quantum " relative to each other , whereas in hydrogen it is easy to approximate the proton as being classical . but i do not know if the idea has ever been explored as a stand-alone concept , especially as it would apply to larger systems . ) finally , and even more interesting ( to me at least ) is the idea that might be able to encapsulate a " hot spot " within a sufficiently large , cold piece of matter . by recording over time , this internal observer could watch itself even as the diamond as a whole " goes quantum " and starts doing things like being in two places at once . the idea that a classical observer could nonetheless still be subject to quantum non-locality at a larger level just fascinates me , in part because it is so counter-intuitive to how we usually view quantum mechanics .
i am sure you have seen photographs of snowflakes up close . you will notice that there are hundreds of small crystals of ice . this is the crystal structure of ice . you do not see ice cubes with a crystal structure because they freeze too fast . the water does not have enough time to move into the crystal lattice when you freeze the water . this web site shows how the molecules line up in the crystals . yes , some ice is denser than water . if you put pressure on regular ice , and give it time to rearrange , the molecules will move into a new crystal lattice which results in the ice being more dense than water . in the first ice crystal , there are spaces between some of the molecules which is not there in the second crystal structure . with extreme pressure , you can have frozen water at 100 °c .
the fluid is incompressible and has no sources inside . this means that the continuity ( mass conservation ) equation is $$ \text{div}\ , \vec{v} = 0 . \qquad ( 1 ) $$ now we follow the standard procedure and represent $\vec{v}$ as follows : $$ \vec{v} = \text{rot}\ , \vec{a} . $$ the divergence of any curl is zero so equation ( 1 ) is satisfied by any smooth vector field $\vec{a} ( \vec{r} ) $ . for 2-dimensional flow we can assume $$ \vec{a} = \bigl ( 0 , 0 , \psi ( x , y ) \bigr ) $$ so that $$ v_x = \frac{\partial \psi}{\partial y} ; \quad v_y = -\frac{\partial \psi}{\partial x} . \qquad ( 2 ) $$ in fluid dynamics $\psi ( x , y ) $ is called stream function because the lines of constant $\psi$ are the streamlines . we have two known stream lines : $$ y = h ( x ) $$ and $$ y = 0 . $$ let 's select the stream function as follows : $$ \psi ( x , y ) = c\frac{y}{h ( x ) } . ( 3 ) $$ for the upper streamline we have $\psi=c$ and for the lower line $\psi=0$ . this is a strong assumption and the main point of the solution . the selection of $\psi$ is not definite here . formula ( 3 ) is intuitive , it gives streamlines that are similar to $h ( x ) $ but coming more straight while approaching to the bottom . now we can use ( 2 ) and ( 3 ) to find $\vec{v}$: $$ \vec{v} ( x , y ) = \left ( \frac{c}{h ( x ) } , \ ; cy\frac{h&#39 ; ( x ) }{h^2 ( x ) }\right ) \qquad ( 4 ) $$ where $c$ is some constant determined by the boundary conditions . the velocity field depends on the unknown function $h ( x ) $ . finding $h ( x ) $ function $h ( x ) $ can be found by applying the bernoulli equation to the top streamline . bernoulli equation for incompressible fluid is $$ \frac{v^2\bigl ( x , y ( x ) \bigr ) }{2} + \frac{p\bigl ( x , y ( x ) \bigr ) }{\rho} + gy ( x ) = \text{const} $$ where $y ( x ) $ is the streamline , $p ( x , y ) $ is the pressure , $\rho$ is the density of the fluid , $g$ is the gravitational acceleration . the upper streamline $y ( x ) =h ( x ) $ is in the equilibrium with the atmosphere air . this means that the pressure of the fluid is equal to the atmosphere pressure : $$ p\bigl ( x , y ( x ) \bigr ) = p_0 . $$ so $$ \frac{v^2\bigl ( x , h ( x ) \bigr ) }{2} + gh ( x ) = \text{const} - \frac{p_0}{\rho} = d \qquad ( 5 ) $$ substitution of ( 4 ) into ( 5 ) gives the differential equation for $h ( x ) $: $$ \frac{c^2}{2h^2}\left ( 1 + h&#39 ; ^2\right ) + gh = d $$ or $$ \frac{dh}{dx} = \sqrt{\frac{2h^2}{c^2} ( d-gh ) - 1} $$ $$ h ( x_1 ) = h_1 $$ this can be solved numerically if we know $c$ and $d$ . finding $c$ and $d$ the parameters $c$ and $d$ are determined by the boundary conditions . if we know the velocity at the point $ ( x_1 , h_1 ) $ then from ( 4 ) : $$ c = h_1 v_x ( x_1 , h_1 ) $$ and from ( 5 ) : $$ d = \frac{v^2 ( x_1 , h_1 ) }{2} + gh_1 $$ conclusion there are two weak points in this solution : the intuitive assumption ( 3 ) ; the undefined constants $c$ and $d$ . some boundary conditions can violate ( 3 ) and/or make calculation of $c$ and $d$ very difficult . alternative there is another way to select the stream function . if we suppose the flow to be potential the velocity field will have the following form : $$ \vec{v} = \nabla \varphi $$ where $\varphi ( x , y ) $ is the potential of the velocity vector field . then in addition to ( 2 ) we will have : $$ v_x = \frac{\partial \varphi}{\partial x} ; \quad v_y = \frac{\partial \varphi}{\partial y} . \qquad ( 6 ) $$ now we can introduce the complex potential of the flow : $$ w ( x+iy ) = \varphi ( x , y ) + i \psi ( x , y ) $$ the formulas ( 2 ) and ( 6 ) together are exactly the cauchy-riemann conditions for the function $w ( z ) $ . this means that $w ( z ) $ describes some conformal map . if we find a conformal map $w ( z ) $ that turns some rectangle into the blue area in the picture in the question for any $h ( x ) $ , then we find a potential flow ( flow with zero vorticity ) that solves the problem . some manipulations will still be required to find $h ( x ) $ . in fact any $w ( z ) $ always turns 2-dimensional potential flow with $$ \varphi ( x , y ) = x $$ $$ \psi ( x , y ) = y $$ and $$ \vec{v} = ( v_x , 0 ) $$ into something more interesting and still fitting the hydrodynamics equations . this works only for potential flows that are not always a good approximation . finding of $w ( z ) $ in this case is a mathematical problem and perhaps should be discussed somewhere else .
there is no problem in writing down a theory that contains massless charged particles . simple $\mathcal{l} = \partial_{\mu} \phi \partial^{\mu} \phi^*$ for a complex field $\phi$ will do the job . you might run into problems with renormalization but i do not want to get into that here ( mostly because there are better people here who can fill in the details if necessary ) . disregarding theory , those particles would be easy to observe assuming their high enough density . also , as you probably know , particles in standard model compulsorily decay ( sooner or later ) into lighter particles as long as conservation laws ( such as electric charge conservation law ) are satisfied . so assuming massless charged particles exist would immediately make all the charged matter ( in particular electrons ) unstable unless those new particles differed in some other quantum numbers . now , if you did not mention electric charge in particular , the answer would be simpler as we have massless ( color- ) charged gluons in our models . so it is definitely nothing strange to consider massless charged particles . it is up to you whether you consider electric charge more important than color charge . another take on this issue is that standard model particles ( and in particular charged ones ) were massless before electrosymmetric breaking ( at least disregarding other mechanisms of mass generation ) . so at some point in the past , this was actually quite common .
a kilogram-meter does not have an intuitative physical significance . laar 's product examples are meaningful for composite units like force or volts , but when dealing with products of basic units like kg , m , s , moles , amperes , physical meaning seems to only occur when there is a divisor like seconds , per your comments . so for example , kilogram-meter without a second divisor ( for momentum ) is intuitively meaningless .
if you are really dealing with n non-interacting particles , the total energy is just the sum of the individuals ' . you can see this by writing the hamiltonian of the system as \begin{equation} h=\sum_{i=1}^n\epsilon_i c_i^\dagger c_i \end{equation} where $\epsilon_i$ is the energy of state $i$ and $c_i$ is the annihilation operator of state $i$ . for both bosons and fermions , $ [ c_i^\dagger c_i , c_j^\dagger c_j ] =0$ for $i\neq j$ , then you can diagonalize the hamiltonian by diagonalizing each term in the summation separately , which simply means the total energy is just the sum of the individuals ' . however , if you are dealing with interacting quantum gases , the problem immediately becomes complicated .
let me add two references to points already mentioned in this discussion : today , there is no reason known why the electric charge has to be quantized . it is true that the quantization follows from the existence of magnetic monopoles and the consistency of the quantized electromagnetic field , which was shown first by dirac , you will find a very nice exposition of this in gregory l . naber : " topology , geometry and gauge fields . " ( 2 books , of the top off my head i do not know if the relevant part is in the first or the second one ) . afaik there is no reason to believe that magnetic monopoles do exist , there is no experimental evidence and there is no compelling theoretical argument using a well established framework like qft . there are of course more speculative ideas ( lubos mentioned those ) . afaik there is no reason why mass should or should not be quantized ( in qft models this is an assumption/axiom that is put in by hand , even the positivity of the energy-momentum operator is an axiom in aqft ) , but a mass gap is considered to be an essential feature of a full fledged rigorous theory of qcd , for reasons that are explained in the problem description of the millenium problem of the clay institute that you can find here : yang-mills and mass gap
i asked the purely mathematical question over here , and received the most complete answer . while i thought there would be a simple trick to seeing the hyperbolic relationship , it looks like you just have to go through the tedious algebra to have it pop out . user jjacquelin found that it can be rearranged to the form $$\frac{z^2}{ ( \frac{ ( 2n+1 ) \pi}{k} ) ^2}-\frac{y^2}{l^2- ( \frac{ ( 2n+1 ) \pi}{k} ) ^2}=\frac{1}{4}+\frac{a^2}{l^2- ( \frac{ ( 2n+1 ) \pi}{k} ) ^2}$$ to show how this looks , i set $l = 20 \space \text{cm}$ , $a= 2 \space \text{cm}$ , and an infrared light source of $\lambda = 1000 \space \text{nm}$ . plotting a modest amount of these produced the following image :
it most certainly exist outside secret labs : ) like gerben wrote , the fields are called molecular dynamics ( md ) and quantum chemistry which , as computers grow faster , will be essential tools of nanotechnology and medicine . molecular dynamics is currently implemented by making certain approximations in that electron motion is not explicitely modelled . in practice , empirical forcefields are matched to experimental data and molecules are essentially modelled by summing the forces on each atom and using $f=ma$ , then integrating the acceleration over time . it is far from perfect - you usually can not match all measurable physical properties of a medium like water at the same time with the same forcefields . you can not normally model shifts of covalent bonds either since that involves changing the molecular composition which breaks the forcefield definitions . on the other hand , the methods are relatively quick and you can easily simulate systems of hundreds of thousands of atoms over a timescale of at least nanoseconds ( microseconds if you have access to a supercomputer : ) . to interpret results you need some understanding of the deficiencies of the algorithms though . some interesting large simulations to date was of the protein factory , the ribosome , probably the evolutionary oldest complex part of life , and of a complete virus , the satellite tobacco mosaic virus . below is a snapshot of a simulation i did of ion-channels in a cell membrane , you see a drug molecule bound in white in the center : i would think that the most used ( free ) simulators are gromacs and namd , with somewhat differing strengths but both are probably ok to start with . usually paired with decent graphical tools to visualize what happens like pymol or vmd . it is not particularly difficult to learn about this by writing your own simple simulator as well , if you are so inclined . in either case , simulating a 2x2x2 nm box of argon atoms or water-molecules for example illustrates many of the concepts . quantum chemistry involves getting one step lower into the motion and shifts of the electrons . thus you can simulate the breaking and bonding of covalent bonds and you do not have to rely on empirical forcefields for every molecule/atom you need in your simulation . however it is a lot slower computationally obviously ( a factor of 100 at least ? i am not sure ) . you might use this to model in detail the mechanism of an active site in an enzyme for example . the reason these are important fields for nanotech and medicine is that the simulations are the equivalents of the act of compiling and testing a program in computer software design , when it comes to designing new drugs and nano-scale machinery or materials . essentially you want methods to test your ideas without manufacturing them first , and computer simulations take advantage of moore 's law which says that computing power doubles every 18 months . . .
here is a sketched proof of the inequality . the problem is to show that $$ \sum_n\langle \phi_n|e^{-\beta \hat{h}}|\phi_n\rangle ~\stackrel{ ? }{\geq}~ \sum_n e^{-\beta\langle \phi_n|\hat{h}|\phi_n\rangle} , \qquad\qquad ( 1 ) $$ where the hamiltonian $\hat{h}$ is a selfadjoint operator , and $|\phi_n\rangle $ denote orthonormal basis vectors in the hilbert space of states . the lhs . of eq . ( 1 ) is the partition function ${\rm tr} ( e^{-\beta \hat{h}} ) $ . by scaling $\hat{h}$ , we may assume that $\beta=-1$ . the inequality ( 1 ) would follow if we can show the inequality for each and every summand $$ \langle \phi_n|e^{ \hat{h}}|\phi_n\rangle ~\stackrel{ ? }{\geq}~ e^{ \langle \phi_n|\hat{h}|\phi_n\rangle} , \qquad\qquad ( 2 ) $$ or equivalently , in a simplified notation for fixed $n$ , $$ \langle e^{ \hat{h}}\rangle ~\stackrel{ ? }{\geq}~ e^{ \langle \hat{h} \rangle} . \qquad\qquad ( 3 ) $$ but eq . ( 3 ) is just jensen 's inequality for a convex function ( with the exponential function playing the role of the convex function ) .
yes , you have misunderstood something : there is no work being done . what makes a conservative force conservative is the fact that it can be written as the gradient of a scalar field , and thus we can assign a pe to every position the object occupies . and of course the pe in a gravitational field depends on the distance to the origin only . so there is a force acting on the object orbiting in a circle , but it is not doing any work , since there is no change in pe . when work is done there is a change in pe compensated by an equal but opposite change in ke . that does not happen with a circular orbit . bt it does , on the other hand , if the object moves in a straight line . it is the same situation as when you are carrying a heavy object : you have to do a lot of force to keep it from falling , but if you are moving on level terrain at constant speed , you are not doing any mechanical work , despite all the huffing and puffing . . .
re the units of $a$: $a$ has dimensions of $l^{-3/2}$ . the modulus of $\psi^2 ( \vec{x} , t ) $ is a probability density so it has units of probability per unit volume , i.e. it is dimension is $l^{-3}$ , so the dimensions of $\psi$ and therefore $a$ must be $l^{-3/2}$ . i do not think there is an si unit corresponding to $\sqrt{p/m^3}$ , but then it is hard to see when you would need such a unit . re the other question : the function : $$\psi ( \vec x , t ) =ae^{i ( \phi+\vec k\vec x-\omega t ) }$$ is indeed a solution to the schrodinger equation for a free particle , but you need to be careful about it is physical meaning . as mentioned above , the modulus of $\psi^2$ gives you the probablity of finding the particle per unit volume , but $\psi$ itself does not have a physical interpretation . you are quite correct that the equation can also be written : $$\psi ( \vec x , t ) =acos ( \phi+\vec k\vec x-\omega t ) + aisin ( \phi+\vec k\vec x-\omega t ) $$ but you can not attach any physical meaning to the two terms on the right . p = probability re the comment : i suppose the si unit would be the metre$^{3/2}$ , but the value of the wavefunction has no physical significance so no such unit has ever been defined . this is why the value has no physical significance : suppose $\phi$ is zero , then what is $\psi ( 0 , 0 ) $ . the answer is of course $a$ . but the phase $\phi$ has no absolute meaning , it is a phase relative to some reference point that can be freely selected . so suppose i use a reference point that is $\pi/2$ different to yours then i would calculate $\psi ( 0 , 0 ) $ to be $ia$ not zero . so you think $\psi ( 0 , 0 ) = a$ while i think $\psi ( 0 , 0 ) = ia$ . does this make any difference in the real world . well the probability deniity is $\psi^*\psi$ so you calculate the probablility density to be $a^2$ , and i calculate the probability density to be , erm , $a^2$ and we get the same answer even though we have different values for $\psi$ .
i agree with the answer of lubos ( and ja72 is right that there is also a positivity criterion ) . however , since you repeated the question again to ja72 , let me explain the answer of lubos more explicitly and with more details . suppose that in your system of harmonic oscillators the kinetic term is defined with a mass matrix $m$ , i.e. ( as in the answer of ja72 ) : $$k=\frac{1}{2} \dot{q}^t m \dot{q}$$ now define the following to matrices $m_s=\frac{1}{2} ( m+m^t ) $ and $m_a=\frac{1}{2} ( m-m^t ) $ . it is very easy to verify that $m=m_s+m_a$ , $m_s^t=m_s$ and $m_a^t=-m_a$ . because of these properties , $m_s$ is called the symmetric part of $m$ , while $m_a$ is called the antisymmetric part ( this is also the terms lubos used ) . now , let us notice that since $m_a$ is antisymmetric the identity $v^t m_a v=0$ holds for any vector $v$ . writing up the kinetic term again , we obtain $$k=\frac{1}{2} \dot{q}^t m \dot{q}= \frac{1}{2} \dot{q}^t m_s \dot{q} + \frac{1}{2} \dot{q}^t m_a \dot{q}= \frac{1}{2} \dot{q}^t m_s \dot{q}$$ hence $m_s$ defines exactly the same kinetic term as $m$ ! this means that without loss of generality we can assume that the mass matrix is symmetric . or in other words : whenever you write down a kinetic energy with a a non-symmetric mass matrix $m$ , you can just forget about the antisymmetric part , and write down the same kinetic energy with $m_s$ - it simply leads to the same system . i hope that this more detailed explanation helps in understanding the answer ( which was also previously given ) . do not hesitate to ask , if something is still not clear . best , zoltan
the depictions you are seeing are correct , the electric and magnetic fields both reach their amplitudes and zeroes in the same locations . rafael 's answer and certain comments on it are completely correct ; energy conservation does not require that the energy density be the same at every point on the electromagnetic wave . the points where there is no field do not carry any energy . but there is never a time when the fields go to zero everywhere . in fact , the wave always maintains the same shape of peaks and valleys ( for an ideal single-frequency wave in a perfect classical vacuum ) , so the same amount of energy is always there . it just moves . to add to rafael 's excellent answer , here 's an explicit example . consider a sinusoidal electromagnetic wave propagating in the $z$ direction . it will have an electric field given by $$\mathbf{e} ( \mathbf{r} , t ) = e_0\hat{\mathbf{x}}\sin ( kz - \omega t ) $$ take the curl of this and you get $$\nabla\times\mathbf{e} ( \mathbf{r} , t ) = \left ( \hat{\mathbf{z}}\frac{\partial}{\partial y} - \hat{\mathbf{y}}\frac{\partial}{\partial z}\right ) e_0\sin ( kz - \omega t ) = -e_0 k\hat{\mathbf{y}}\cos ( kz - \omega t ) $$ using one of maxwell 's equations , $\nabla\times\mathbf{e} = -\frac{\partial \mathbf{b}}{\partial t}$ , you get $$-\frac{\partial\mathbf{b} ( \mathbf{r} , t ) }{\partial t} = -e_0 k\hat{\mathbf{y}}\cos ( kz - \omega t ) $$ integrate this with respect to time to find the magnetic field , $$\mathbf{b} ( \mathbf{r} , t ) = -\frac{e_0 k}{\omega}\hat{\mathbf{y}}\sin ( kz - \omega t ) $$ comparing this with the expression for $\mathbf{e} ( \mathbf{r} , t ) $ , you find that $\mathbf{b}$ is directly proportional to $\mathbf{e}$ . when and where one is zero , the other will also be zero ; when and where one reaches its maximum/minimum , so does the other . for an electromagnetic wave in free space , conservation of energy is expressed by poynting 's theorem , $$\frac{\partial u}{\partial t} = -\nabla\cdot\mathbf{s}$$ the left side of this gives you the rate of change of energy density in time , where $$u = \frac{1}{2}\left ( \epsilon_0 e^2 + \frac{1}{\mu_0}b^2\right ) $$ and the right side tells you the electromagnetic energy flux density , in terms of the poynting vector , $$\mathbf{s} = \frac{1}{\mu_0}\mathbf{e}\times\mathbf{b}$$ poynting 's theorem just says that the rate at which the energy density at a point changes is the opposite of the rate at which energy density flows away from that point . if you plug in the explicit expressions for the wave in my example , after a bit of algebra you find $$\frac{\partial u}{\partial t} = -\omega e_0^2\left ( \epsilon_0 + \frac{k^2}{\mu_0\omega^2}\right ) \sin ( kz - \omega t ) \cos ( kz - \omega t ) = -\epsilon_0\omega e_0^2 \sin\bigl ( 2 ( kz - \omega t ) \bigr ) $$ ( using $c = \omega/k$ ) and $$\nabla\cdot\mathbf{s} = \frac{2}{\mu_0}\frac{k^2}{\omega}e^2 \sin ( kz - \omega t ) \cos ( kz - \omega t ) = \epsilon_0 \omega e_0^2 \sin\bigl ( 2 ( kz - \omega t ) \bigr ) $$ thus confirming that the equality in poynting 's theorem holds , and therefore that em energy is conserved . notice that the expressions for both sides of the equation include the factor $\sin\bigl ( 2 ( kz - \omega t ) \bigr ) $ - they are not constant . this mathematically shows you the structure of the energy in an em wave . it is not just a uniform " column of energy ; " the amount of energy contained in the wave varies sinusoidally from point to point ( $s$ tells you that ) , and as the wave passes a particular point in space , the amount of energy it has at that point varies sinusoidally in time ( $u$ tells you that ) . but those changes in energy with respect to space and time do not just come out of nowhere . they are precisely synchronized in the manner specified by poynting 's theorem , so that the changes in energy at a point are accounted for by the flux to and from neighboring points .
the answer is a bit lengthy , but can be arrived at using arguments about elastic strain energy . here is a very detailed explanation : limits of poisson&#39 ; s ratio in isotropic solid this was written at a graduate mechanical engineering level , so i will simplify it here . imagine that there exists a function $\psi$ that describes how much energy is contained in a solid per unit volume . this quantity is a function of material properties and deformation . for a linear elastic , isotropic solid , the material properties are young 's modulus ( e ) , and the poisson ratio ( $\nu$ ) . one of the assumptions of the theory of elasticity is that the elastic energy $\psi$ is a function that is strictly increasing for all conceivable deformations . the details of this assumption are in my other answer ( the link ) , but it turns out that $\nu$ can only be in the interval $$ -1 &lt ; \nu &lt ; \frac{1}{2} $$ i hope this clears up your question at an appropriate level . let me know in the comments if not !
the first generation of elementary particles are by observation not composite and therefore not seen to decay . they are shown in this table of the standard model of particle physics in column i . the standard model of elementary particles , with the three generations of matter , gauge bosons in the fourth column and the higgs boson in the fifth . all these particles interact with some of the existing forces , so there exist potentials between them forming bound states if the energetics of the interaction are appropriate . the quarks in various ways bind themselves either by twos ( mesons ) or by threes ( baryons ) with the strong force and also interact electromagnetically . when bound into a proton the electromagnetic interaction with an electron creates the bound state of the hydrogen atom . atoms create stable bound states into molecules . a resonance describes a state in these potentials that is unstable , i.e. it decays in time within our observational horizon . a hydrogen atom for example can have its electron kicked to a higher energy level and be in an excited state , until it decays back to the ground state . for low , non relativistic energies a resonance is a temporary excitation seen in scattering a nucleus with a neutron , for example . the term is extensively used in nuclear physics and technology . in relativistic particle physics the potential model has been superceded by the quantum field theoretical one because of its convenience in description and calculations . these are scattering experiments where the measurement is of the total cross section , as @innisfree explains in his/her answer . the e^+ e^- scattering at lep , an electron and a positron in a head on collision " bind " for a while at a specific center of mass energy and display the z particle . the cross section is a measure of how large the particles see each other , and at the z their probability of interaction is much higher than in the energies before or after . the z is then displayed as a resonance . the independence from scattering existence of resonances , giving them the status of particles , is seen in the products of scattering experiments when appropriate combinations of outgoing particles , pions and protons and electrons etc , are plotted against their invariant mass . for example here is the z0 in the products of the high energy collisions in the lhc , when looking at the invariant mass of ( mu+ mu- ) . the background has been subtracted here . the original is also in the link . a typical resonance plot whose width is constrained by the experimental errors and is not the true width that would give the decay time of the pair . resonances , when the experimental errors are smaller than the physical width , are described by a breit wigner function from which the intrinsic width can be extracted , which characterizes the interaction . when the experimental errors are large a gaussian statistical fit will give the mass to identify the resonance but no information on the decay .
you must also take the fact that while rod with mass attached falls , the triangle ( with the rod ) moves to the right ( conservation of momentum in x axis ) as that might be a pain to calculate , consider this : there is no force of the system of $rod + m + m$ in x direction as there is no friction . hence , the x component of centre of mass must not change .
what you are doing is an eigenvalue problem . eigenvectors are determined by the space you are looking at , and this is why you usually specify some boundary conditions . in your case just the requirement of absence of singularities should do the job ( i.e. . you want some subspace of $c [ -1,1 ] $ ) . this is the analytical viewpoint . the numerical viewpoint actually depends on your algorithm . first of all , if you really want " to solve the equation numerically " , i assume that you are playing the game of not knowing the answer . so you do not actually know that $l$ is integer beforehand . if i were solving the problem , i would put it on a lattice and then write it as a finite-dimensional eigenvalue problem . in deriving the finite difference equations i would use the fact that my solution should be finite at the endpoints of the interval . a way to do this is to introduce homogenious lattice at points ( lets call them so ) 0,1,2,3,4 . . . then integrate the equation from $i-1/2$ to $i+1/2$ and use middle-difference fromulae for derivatives ( you will need their values at $i\pm1/2$ , so the middle difference will return you back to your lattice ) and middle-rectangles formula for integrals ( it is important to use approximations of the proper order . i believe that i am telling you an algorithm of second-order presicion ) . then you will have to do something with the endpoints . for them do the integration from $0$ to $1/2$ and respectively on the other end . in doing so you will use the fact that $ ( 1-x^2 ) \frac{dp}{dx}$ is $0$ at the endpoints . and this picks up the appropriate space for your solutions . long story made short , i believe that at least for some calculational schemes the conditions should be that $ ( 1-x^2 ) \frac{dp}{dx}$ vanishes at the endpoints .
by symmetry , it'll look circular . just look at it from above , along the axis . you can synchronize clocks along the rim by a signal from the center . then have those rim clocks all emit signals simultaneously . in the rest frame , their signals will arrive at the same time as those of stationary clocks positioned around the " orbit " . or imagine encasing the whole thing in a hollow toroidal container at rest . like the proton bunches going around the lhc . no ambiguity about what the encasing torus looks like .
i would generally say that most physicists mean " speed of light in a vacuum " when they say " speed of light , " and therefore would say that the " speed of light is constant . " if it is in a field that often deals with light propagation in materials ( optics , condensed matter ) , people are usually pretty careful to say " speed of light in a vacuum " when they mean it . generally whenever some says " the speed of light is a constant , " most physicists will assume they mean speed of light in a vacuum .
intuitively , the ice cream and bowl will move towards a state of equal temperature ( second law of thermodynamics ) . when you stir the ice cream you are doing at least four things : you are ' encouraging ' the heat to become more uniformly distributed ( as you suspected ) , causing the ice cream to come into contact with regions of the bowl from which it has yet to absorb energy , causing the ice cream to adopt the same shape as the bowl and thus increase its contact area ( and therefore the rate of heat exchange ) , and adding energy by stirring it .
the many comments have covered the main points about the question , but i thought it would be worthwhile explaining how the behaviour is calculated . if we solve the einstein equation for a point mass we get the schwarzschild metric : $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2 + \left ( 1-\frac{2m}{r}\right ) ^{-1}dr^2 + r^2 d\omega^2$$ all equations look scary to non-nerds , but do not worry too much about the details . the key points are that the equation involves time , $dt$ , and the distance from the centre of the black hole , $dr$ , and it calculates a quantity called the line element , $ds$ . the time , $t$ , and radial distance , $r$ , are the physical quantities measured by an observer outside the black hole , i.e. that is you and me . they are exactly what you would think i.e. the time is what we measure with a stopwatch . the radial distance is what you had get by measuring the circumference of a circle round the black hole and dividing by $2\pi$ ( because the circumference of a circle is $2\pi r$ ) . the line interval , $ds$ , is a bit more abstract but for our purposes $ds$ is the time as measured by someone falling into the black hole . this is called the proper time and usually written as $\tau$ . you have probably heard that time slows down as you approach the speed of light , and you get a similar effect here ( as dmckee menioned in a comment ) . that means the time measured by someone falling into the black hole , i.e. the proper time $\tau$ , is not the same as the time $t$ that we measure when watching the black hole from the outside . the point of all this stuff is that you can use the metric to calculate how long it takes to fall from some distance , $r$ , outside the black hole to the event horizon . first of all let 's calculate this for the person falling into the black hole . this means we have to calculate the proper time , $\tau$ . you can find this in any book on gr , or by googling , and the result is : $$ \delta \tau = \frac{2m}{3} \left [ \left ( \frac{r}{2m} \right ) ^{3/2} \right ] ^r _{2m} $$ again , do not worry about the detail . a long as we know the mass of the black hole , $m$ , and the starting distance , $r$ , we just feed these into a calculator and it gives us $\delta\tau$ which is the time measured by the person falling into the black hole . the time obviously depends on how far away you start and how big the black hole is , but it is just a number of seconds . in fact if we use $r = 0$ in the expression about we can calculate how long it takes to fall through the event horizon and on to the singularity at the black hole . so , the point to note down at this stage is that the person falling into the black hole reaches the event horizon and inded the singularity in a finite time . the next step is to calculate the time measured by you and me sitting outside the black hole . this is a bit more involved , but we end up with an expression : $$ dt = \frac {- ( \epsilon + 2m ) ^{3/2}d\epsilon} { ( 2m ) ^{1/2}\epsilon} $$ where $\epsilon$ is the distance from the event horizon i.e. $\epsilon = r - 2m$ . integrating this to find the time to reach the event horizon is a bit messy , but if restrict ourselves to distances very near the event horizon we find : $$\delta t \propto ln ( \epsilon ) $$ but $\epsilon$ is the distance from the event horizon , so it is zero at the event horizon and $ln ( 0 ) $ is infinity . that means that the time you and i measure for our astronaut to reach the event horizon , $\delta t$ , is infinity . and this is why you get the apparently paradoxical result about falling into black holes . the time measured by the person falling in , $\delta\tau$ , is finite but the time measured by people outside the black hole , $\delta t$ , is infinite .
the lagrangian of the system is , $$l = \frac{1}{2}m\dot{x_1}^2 - \frac{1}{2}k_1 ( l_1 - l_0+x_1 ) ^2-\frac{1}{2}k_2 ( l_2-l_0+x_2-x_1 ) ^2+mgx_1$$ here , $x_1$ is the downward distance from the equilibrium position of the mass , $x_2$ is the downward distance from the midpoint of the driving oscillator , $k_1$ and $k_2$ are spring constants of the top and bottom springs respectively , $l_0$ is the unstretched length of both springs , $l_1$ and $l_2$ are stretched lengths at equilibrium of the top and bottom springs respectively . the condition for equilibrium , when the driving force is not applied , is $$k_1 ( l_1-l_0 ) = k_2 ( l_2-l_0 ) +mg$$ we can take $x_2=a\ cos ( \omega t ) $ for the driving oscillation and then the equation of motion for the mass from euler-lagrange equation would be , $$m\ddot{x_1}+ ( k_1+k_2 ) x_1-ak_2cos ( \omega t ) = 0$$ solving this equation , we can get the amplitude of the driven oscillation to be , $$a=\frac{ak_2}{k_1+k_2-m\omega^2}$$ as a result , the resonant frequency should be the same for both setups with springs interchanged as $\omega_r=\sqrt{ ( k_1+k_2 ) /m}$ . the proportion between the resonant frequencies in your setups should be $\omega_{r ( 5,5 ) }:\omega_{r ( 5,15 ) }:\omega_{r ( 15,15 ) }=1:\sqrt{2}:\sqrt{3}$ which is apparent in your results . however , based on theoretical calculations the resonant frequencies should be the same for setups with just the springs interchanged . now , in this calculation damping is ignored while in all real systems damping is inevitable . the presence of damping is also the reason you are not getting an infinite amplitude at resonance . for a damping factor $\gamma$ , the resonant frequency will be $$\omega_r = \sqrt{\frac{k_1+k_2}{m} - \frac{\gamma^2}{2}}$$ if the damping is not same for both the springs , then the resonant frequencies might not match within a certain limit . i would suggest you to check for any possible source of damping in this setup that might significantly affect the resonant frequency .
because there is a changing configuration of charges with time , a time-dependent dipole moment . the electron 's field is coming from a different center at different times , so that the field is oscillating in magnitude with a period the orbital period of the electron . when you have an oscillation of electric fields , it sets up oscillations of the entire electromagnetic field which goes out at the speed of light , and this is the radiation .
if the position of the c.g. is $\vec{r}_c$ and the location of the force application $\vec{r}_a$ then the euler-newton equations of motion for rigid body are : $$ \vec{f} = m\ , \vec{a}_c \\ ( \vec{r}_a-\vec{r}_c ) \times \vec{f} = i_c \vec{\alpha} + \vec{\omega}\times i_c \vec{\omega} $$ with c.g. velocity $\vec{v}_c = \dot{\vec{r}_c}$ , c.g. acceleration $\vec{a}_c = \ddot{\vec{r}_c}$ , $i_c$ the moment of inertia tensor about the c.g. in 2d when $ ( x , y ) $ is the location of the c.g. point c this becomes $$ \begin{vmatrix} f_x \\ f_y \\ 0 \end{vmatrix} = m \begin{vmatrix} \ddot{x} \\ \ddot{y} \\ 0 \end{vmatrix} \\ \begin{vmatrix} c_x \cos\theta \\ c_y \sin\theta \\ 0 \end{vmatrix} \times \begin{vmatrix} f_x \\ f_y \\ 0 \end{vmatrix} = \begin{vmatrix} i_x and and \\ and i_y and \\ and and i_z \end{vmatrix} \begin{vmatrix} 0 \\ 0 \\ \ddot{\theta} \end{vmatrix} + \begin{vmatrix} 0 \\ 0 \\ 0 \end{vmatrix} $$ where $ ( c_x , c_y ) $ is the position of point a from the c.g. when the body orientation is $\theta=0$ ( initially ) . by component then the equations are $$ \ddot{x} = f_x/m \\ \ddot{y} = f_y/m \\ \ddot{\theta} = \frac{-c_y \sin\theta f_x + c_x \cos\theta f_y}{i} $$ if the force is rotating with the body , and initially located at $ ( cx , 0 ) $ pointing in the + y direction then $$ \ddot{\theta} = \frac{c_x f_y}{i} $$
spin is one observable , and it is space is spanned by two vectors , as you point out . what about all the other possible observables ? each observable has it is own " private " ( as it were ) hilbert space . the state of the system as a whole is the direct product space of all those individual hilbert spaces . the total state vector contains all information about all possible observables . the story of what a direct product space is , and how measurements of a individual observable are considered in these spaces is a long-ish story , so i will wait to see if you have questions about that .
the higgs ether does not pick out a preferred velocity--- it is the same in all reference frames . because of this , it can not impart a resistance to velocity , since any velocity is symmetric with any other . there is no higgs drag . but it can impart a resistance to change in velocity , and this is a change mass . the most important thing is that it allows different helcities of fermions , which would be necessarily massless , to join up in pairs to make massive fermions .
1 . ) the differentiation operator acting will give rise to kronecker-deltas since $\frac{\partial x_a}{\partial x_b}=\delta_{ab}$ this will kill one summation . more specifially : $\frac{\partial u}{\partial x_a}=-1/2 \sum_{ij}b_{ij} ( \delta_{ai}x_j+\delta_{aj}x_i ) =-1/2 ( \sum_{j}b_{aj}x_j+\sum_{i}b_{ia}x_i ) =-\sum_{j}b_{aj}x_j$ . rename j to be i and you are done . 2 . ) the hessian matrix is a matrix of second-order partial derivatives ; hence it is symmetric w.r.t. its indices , i.e. $b_{ij}=b_{ji}$ .
it depends on the walls . if they are truly insulated , no heat will flow in or out . the water that freezes raises the temperature , while the water that evaporates lowers the temperature . the triple point of water is $0.01^\circ c , 611.73$ pascals . if the chamber is very large , relatively much of the water will evaporate and the temperature will be below the triple point . you will form an ice/vapor equilibrium . if the chamber is not much larger than the water volume , a little water will evaporate , more water will freeze , and you will be at the triple point . since the volume of water and ice are not very different , you can calculate how much vapor is there from the total mass of water . then you can calculate the amount of ice to make the temperature right . you will not be far wrong . if you want , you can iterate looking at the change in the volume of water/ice .
your question became clear after you posted the images . this corresponds to the difference in a turn as accomplished by a bike/motorcycle and a car/bus/truck . so let 's study this case first . during the turn within the accelerated reference frame there is an imaginary " centrifugal " force , which is directed outward the turn of course . otoh the force applied by the ground has a component toward the turn ( due to friction to prevent/reduce sliding ) . this creates a momentum that tends to lean the object outward the turn . this is indeed what happens with most 4-wheel vehicles . they lean outward the turn , this transfers the pressure from the inner wheels to the outers , which causes appropriate change in the normal force applied to the wheels . this in turn creates a momentum which tends to lean the object toward the turn . by such the vehicle is leaned to some angle , after which equilibrium is achieved . now let 's see what happens with 2-wheel vehicles ( like bike ) . since the normal force is applied in just 2 points , leaning outward the turn does not transfer the pressure , there is no change in the normal force , hence no momentum toward the turn is created . moreover , leaning outward the turn displaces the mass from the 2-wheel axis , hence the gravitational force creates even more momentum outward the turn . the bike would just fall . to accomplish the turn however , the biker leans toward the turn deliberately . displacing the mass causes gravitational force to create a momentum toward the turn . which is in equilibrium with the momentum of the " centrifugal " force . now let 's see what happens with vessels . as with vehicles , the " centrifugal " force is applied outward the turn , the force applied by the water has a component toward the turn ( due to the viscosity ) . hence the " centrifugal " force 's momentum is outward the turn . the difference is that there is a considerable part of the vessel under the water . moreover , the center of mass is not required to sit above the water level . another difference is that there are no discrete contact points with the water , instead water pressure is applied on all the underwater part of the vessel . when the vessel leans ( to either side ) its configuration changes : its center of mass is displaced , its underwater part is changed , the volume and shape of the water " pushed out " changes as well . if the center of mass of the vessel + " pushed out " water raises - there is a momentum that tends to return the vessel back to its original state , hence it is stable . theoretically all the vessels are stable when at rest ( otherwise they had turn around ) . however during the motion some vessels are raised ( like the small boat in your question ) and become unstable . such vessels definitely may not perform the turn unless deliberately leaned toward the turn . simply because there is nothing to compensate for the " centrifugal " momentum . otoh big vessels may remain stable even during the motion , with enough reserve to perform the turn as-is . so , the factors to consider are : vehicle configuration ( sunk level , mass distribution , shape of the underwater part ) during the motion . required centripetal acceleration to perform the turn ( velocity and radius ) . exact forces imposed by the water ( hydrodynamics ) . based on those one may see to which side the vessel leans during the turn . there is however another interesting moment . if the vessel is unstable it should lean toward the turn . but how does this actually happen ? bicycle rider leans intentionally , otherwise he had fall . he does it by displacing his own mass toward the turn , which is considerable wrt the mass of the bicycle . but is this the case with the motor boats ? i doubt if the mass of the rider is considerable wrt the mass of the boat . plus , if this was the case , unskilled riders would turn around frequently , and i personally never saw this . there may be two explanations of this : perhaps such boats are designed such that steering alone makes them lean toward the turn ( due to a specific shape of the underwater tail , some hydrodynamical trick ) . during improper turn the vessel leans outward the turn , than it sinks a little , and in this new configuration there is an adequate momentum . so that the vessel does not turn around , it just passes the turn with a lower speed .
to calculate the propagation speed , you need to specify the return current path in addition to the " forward " path . the reason is that the electromagnetic fields that determine the propagation characteristics fill the space between the two conductors . [ if you try to calculate the inductance of a single wire , you get an infinite result . ] the filler material between the conductors matters too : its electric polarizability ( quantified by the dielectric constant $\epsilon$ , which is typically 2-5 times the value for free space $\epsilon_0$ ) slows down the signal speed . typically the filler is magnetically neutral , so its susceptibility $\mu$ is the same as for free space . for a coaxial conductor , the wave speed formula ends up being very simple : $$ v = \frac{1}{\sqrt{\mu \epsilon}}$$ for a relative dielectric constant ( $\epsilon/\epsilon_0$ ) of 3 , one calculates a velocity of 58% of the speed of light . finally , your elastic ball analogy is good to zeroth order , but i do not think you can use it to think about propagation velocity . there are two independent ( but coupled ) fields ( electric and magnetic ) at play here . update : it turns out that the geometry of the conductors does not matter much ; the main determinant of the propagation velocity is the filler material properties . for parallel conductors of arbitrary ( but constant ) cross-section , the propagation velocity is : $$ v = \frac{c}{\sqrt{\mu_r \epsilon_r}} $$ here the relative permeability of the filler $\epsilon_r = \epsilon/\epsilon_0$ ( typically 2-5 ) and relative magnetic susceptibility $\mu_r = \mu/\mu_0$ ( usually 1 ) , while $c$ is light-speed . so the formula for the coaxial geometry turns out to be quite general ( note $c=1/\sqrt{\epsilon_0 \mu_0}$ ) . as jaime mentions in the comments below , there will be some additional " internal " inductance due to the magnetic fields within the conductors which will reduce the velocity ; that bit is geometry-dependent .
well , the particles will not always follow circular paths ( for instance , the particles in this video ) . but , if you apply a constant magnetic field across the chamber , charged particles moving in the field will be deflected according to the lorentz force law . the centripetal acceleration for a particle moving in a circle is $a=\frac{v^2}{r}$ , where $v$ is the particles tangential velocity and $r$ is the radius of the circle . plugging this into newton 's second law we get $$ m\frac{v^2}{r}=qvb $$ rearranging : $$ \big ( \frac{q}{m}\big ) \big ( \frac{b}{v}\big ) =\frac{1}{r} $$ or , in other words , the charge-to-mass ratio is inversely proportional to radius of the particles circular orbit by proportionality constant $\frac{b}{v}$ . edit : i can not comment yet otherwise i would answer your second question in the comments . no , $v$ cannot be eliminated because it is integral to the magnitude of the lorentz force . a light but slower-moving particle could have the same orbit as a heavier , faster particle assuming that they have the same charge , but you can calculate each particle 's momentum in a creation event by using conservation of momentum and conservation of electric charge . so , no , i do not think you can say which particle is lighter if all you know are the two radii . of course , things are different if you have a velocity selector .
first of all , choose the reference frame co-moving with the paddle and assume that this reference frame is inertial . this is the key to all ball-and-wall problems . of course , ignore gravity and air drag . now we have a spinning ball incident on a stationary surface with friction . let $\mathbf{v}$ be the vector of the ball 's velocity with respect to the paddle , and $\mathbf{v}_n$ and $\mathbf{v}_t$ be the normal and the tangential components of the ball 's velocity before impact . let $\boldsymbol{\omega}$ be the ball 's angular velocity before impact . all quantities in bold are vectors . then it gets as complicated as you wish . to make it as simple as possible , let us assume that the ball spends time $\tau$ in contact with the surface , and that all forces acting on it during contact are constant in time . further , assume that the friction coefficient , $\mu$ , does not depend on the relative velocity of the ball and the paddle . important : this assumption will break down if rotation of the ball ever reaches such speed that the ball will start rolling instead of sliding ; at this moment the tangential force vanishes , and such situation must be treated independently . a lot of assumptions to make it possible to tackle the problem analytically , huh ? but the good news is that it is all mechanics 101 from now on . if the ball is perfectly elastic , then there are no dissipative forces acting in the direction normal to the surface . this means that when the ball rebounds , its normal component of velocity will just be inverted : $$ \mathbf{v}_n'=-\mathbf{v}_n\tag{1} $$ this allows us to calculate the average normal force acting on the ball during contact ( just newton 's 2nd law ) : $$\left|f_n\right|=2m\frac{\left|v_n\right|}{\tau}$$ tangential force acting on the ball due to friction during contact is $$\mathbf{f}_t=\mu\left|f_n\right|\hat{\mathbf{e}}=2m\mu\frac{\left|v_n\right|}{\tau} , $$ where $\hat{\mathbf{e}}$ a unit vector in the direction of the friction force . it can be found as $$ \hat{\mathbf{e}}=\frac{\left ( \mathbf{r}\times\boldsymbol{\omega}\right ) -\mathbf{v}_t}{\left|\left ( \mathbf{r}\times\boldsymbol{\omega}\right ) -\mathbf{v}_t\right|} , $$ where $\mathbf{r}$ is the vector from the center of the ball to the point of contact , $\boldsymbol{\omega}$ is the ball 's angular velocity , and $\mathbf{v}_t$ is the tangential speed of the ball . here | | denote taking a vector 's modulus ( length ) . this force , acting $\tau$ seconds , will the change the ball 's tangential momentum by $$ \delta \mathbf{p}_t=\tau\mathbf{f}_t=2m\mu\left|v_n\right|\hat{\mathbf{e}} , $$ ( note that $\tau$ cancels out ! ) , so the ball 's tangential speed after impact will be $$\mathbf{v}_t'=\mathbf{v}_t+\hat{\mathbf{e}}\frac{\left|\delta\mathbf{p}_t\right|}{m} = \mathbf{v}_t + 2\mu\left|v_n\right|\hat{\mathbf{e}}\tag{2}$$ finally , the angular momentum picked up by the ball equals $$\delta\mathbf{l}=r\tau|f_t|\hat{\mathbf{f}}=2rm\mu\left|v_n\right|\hat{\mathbf{f}}$$ where $r$ is the radius of the ball ( also , $r=\left|\mathbf{r}\right|$ ) , and $\hat{\mathbf{f}}$ is a unit vector in the direction in which the angular momentum was picked up . to find $\hat{\mathbf{f}}$ , use $$\hat{\mathbf{f}}=\frac{\mathbf{r}\times\hat{\mathbf{e}}}{r} . $$ this change in $\mathbf{l}$ will make the ball 's angular velocity after impact $$\boldsymbol{\omega}'=\boldsymbol{\omega}+\hat{\mathbf{f}}\frac{|\delta l|}{i}=\boldsymbol{\omega} + \frac{|2rm\mu v_n|}{i}\hat{\mathbf{f}}\tag{3}$$ here $i= ( 2/5 ) mr^2$ is the moment of inertia of the ball ( assuming that it is a hollow uniform sphere ) . note that the mass $m$ cancels out . equations ( 1 ) , ( 2 ) and ( 3 ) give you the relationship between pre- and post-impact linear velocity $\mathbf{v}$ and angular velocity $\boldsymbol{\omega}$ . they are determined in the paddle 's reference frame , so to get lab ( room ) frame quantities , you need to transfer back to that frame . it is tedious to do by hand , but if you are developing a ping pong emulator in which you use a game console 's accelerometer to emulate a paddle , the computer will happily do this for you . let me re-iterate at this point that this calculation does not apply to cases when the ball is rolling ( or nearly rolling ) , as opposed to sliding . this situation will occur if $ [ \mathbf{r} \times \boldsymbol{\omega} ] = \mathbf{v}_t$ , or , equivalently , $\hat{\mathbf{e}}=0$ . if this situation occurs ( it will if initial conditions are close to it ) , the ball will start rolling , its tangential momentum and angular velocity will freeze , and it will just rebound . to treat this situation properly , you may want to choose a finite $\tau$ , a large integer $n$ , and break $\tau$ into smaller time steps $\delta\tau=\tau/n$ . then calculate all quantities during impact in $\delta\tau$ increments . once you detect the rolling condition $\hat{\mathbf{e}}=0$ , you make the ball rebound with the current $\mathbf{v}_t'$ and $\boldsymbol{\omega}'$ . i may have messed up the signs here and there , but , basically , it is all very simple . to top it off , here is a diagram ( you are correct , i am not an artist ! ) in my example , $\boldsymbol{\omega}$ is pointing toward us , but in the solution , its direction can be arbitrary . edit : i just realized that equations ( 2 ) and ( 3 ) are only correct if the ball is spinning in the direction it is flying ( i.e. . if vector $\hat{\mathbf{e}}$ is parallel or anti-parallel to vector $\mathbf{v}_t$ ) . so 2d pong is ok with equations ( 2 ) and ( 3 ) . however , in 3d problem , if the ball is spinning sideways , then during contact , $\boldsymbol{\omega}$ and $\mathbf{v}_t$ will change in length , and therefore unit vectors $\hat{\mathbf{e}}$ and $\hat{\mathbf{f}}$ will change direction . this means that to get the ball 's speed and spin , we need to integrate in time . it can be done analytically ( a nice problem to torture graduate students ) , and it is not difficult to do numerically . one should choose a time of contact duration $\tau$ a large integer $n$ , and small time step $\delta t=\tau/n$ . and then calculate $$\mathbf{v}_t\left ( t+\delta t\right ) =\mathbf{v}_t ( t ) +\hat{\mathbf{e}}\frac{\left|\mathbf{f}_t\right|}{m}\delta t$$ $$\boldsymbol{\omega}\left ( t+\delta t\right ) =\boldsymbol{\omega}+\hat{\mathbf{f}}\frac{\left|\mathbf{f}_t\right|r}{i}\delta t$$ and do it $n$ times , re-calculating the values of $\hat{\mathbf{e}}$ and $\hat{\mathbf{f}}$ every time step . a fringe benefit of this approach , it is easy to control the onset of rolling . if any component of vector $\hat{\mathbf{e}}$ changes sign ( i.e. . , goes through 0 ) at any time step , sliding stops and rolling begins . the answer should not depend or $\tau$ or $n$ . edit end
yes , i agree with david . if somehow , you were able to travel at the speed of light , it would seem that ' your time ' would not have progressed in comparison to your reference time once you returned to ' normal ' speeds . this can be modeled by the lorentz time dilation equation : $$t=\frac{t_0}{\sqrt{1 - ( v^2 / c^2 ) }}$$ when traveling at the speed of light ( $v=c$ ) , left under the radical you would have 0 . this answer would be undefined or infinity if you will ( let 's go with infinity ) . the reference time ( $t_0$ ) divided by infinity would be 0 ; therefore , you could infer that time is ' frozen ' to an object traveling at the speed of light .
many processes cannot be drawn on a p-v diagram because the pressure is not always defined . those processes that can be drawn are called " quasi-static " . however , you cannot look at a certain path and say whether it represents a reversible or irreversible process for sure . for example , imagine a vertical line on the p-v plot , corresponding to adding heat to a system of fixed volume . you can do this reversibly by only allowing the heat source temperature to be infinitesimally different from your system 's temperature at any time , or you can do it irreversibly by keeping your heat source at a finite temperature difference from your system , but only allowing the heat to enter very slowly , so that the system achieves equilibrium and the pressure is well-defined . the path the process takes is the same in both cases , but one process is reversible and the other is not .
one of the results of special relativity is that a particle moving at the speed of light does not experience time , and thus is unable to make any measurements . in particular , it cannot measure the velocity of another particle passing it . so , strictly speaking , your question is undefined . particle #1 does not have a " point of view , " so to speak . ( more precisely : it does not have a rest frame because there is no lorentz transformation that puts particle #1 at rest , so it makes no sense to talk about the speed it would measure in its rest frame . ) but suppose you had a different situation , where each particle was moving at $0.9999c$ instead , so that that issue i mentioned is not a problem . another result of special relativity is that the relative velocity between two particles is not just given by the difference between their two velocities . instead , the formula ( in one dimension ) is $$v_\text{rel} = \frac{v_1 - v_2}{1 - \frac{v_1v_2}{c^2}}$$ if you plug in $v_1 = 0.9999c$ and $v_2 = -0.9999c$ , you get $$v_\text{rel} = \frac{1.9998c}{1 + 0.9998} = 0.99999999c$$ which is still less than the speed of light .
the physical context would have been helpful . i am guessing that this is some sort of once ionized atom , whose electron is in an l=1 state , sitting inside an atomic crystal , which breaks the rotational symmetry by the crystal axes . first , the s is a decoupled electron spin , which is completely irrelevant . you can ignore the 2s+1 factors and all the misdirecting jawboning about spin in the question . this is a reformulation : you have a quantum mechanical system with angular momentum 1 . this means that there are 3 different rotated versions of the state , with $l_z=-1,0 , +1$ , which have exactly the same energy , they are degenerate , because the world is rotationally invariant . now you put this system inside a crystal that breaks the rotational symmetry , and the leading perturbation from the crystal between these three states and no other , is $$ a l_x^2 + b l_y^2 + cl_z^2 $$ the operators $l_x$ , $l_y$ and $l_z$ are specific 3 by 3 matrices acting on these three states . you probably were given their form from the raising and lowering operators for z-angular momentum : $l_x - il_y$ , $l_x + il_y$ . find the new ground state , show that all the " matrix elements " of each component of l vanish in this ground state . the first part is easy diagonalization of the 3 by 3 matrix given to you above . this is clearly a homework exercize , so you should do it yourself . it is straightforward . the second depends on the intended interpretation of the word " matrix elements": after comments by jojo , i learn that the " matrix elements " are just supposed to be the expected value of the $l_x$ , $l_y$ an $l_z$ between the ground state copies with different spin . this is a terrible abuse of the term " matrix elements " , because these matrix elements are just the expectation value of $l_x$ , $l_y$ and $l_z$ in the ground state , the spin is completely decoupled . so all you need to show is that the expected value of $l_x$ , $l_y$ and $l_z$ are zero . i misinterpreted the wording initially to mean something impossible , namely : show that the matrix elements between the ground states and the other $l_z$ states are all zero , for each of the three operators $l_x , l_y , l_z$ . there are no states of l=1 system for which the matrix elements of each component of l vanishes . such a state would be a rotationally invariant vector , which does not exist--- the only rotationally invariant objects are scalars . because the actual wording is potentially misleading , this problem is not very good . it is better to say explicitly : " show that all the matrix elements between the components of l between the different ground states is zero . "
first , you are right in that non-minkowski solutions to string theory , in which the gravitational field is macroscopic , it should be thought of as a condensate of a huge number of gravitons ( which are one of the spacetime particles associated to a degree of freedom of the string ) . ( aside : a point particle , corresponding to quantum field theory , has no internal degrees of freedom ; the different particles come simply from different labels attached to ponits . a string has many degrees of freedom , each of which corresponds to a particle in the spacetime interpretation of string theory , i.e. the effective field theory . ) to your question ( 1 ) : certainly there is no great organizing principle of string theory ( yet ) . one practical principle is that the 2-dimensional ( quantum ) field theory which describes the fluctuations of the string worldsheet should be conformal , i.e. independent of local scale invariance of the metric . this allows us to integrate over all metrics on riemann surfaces only up to diffeomorphisms and scalings , which is to say only up to a finite number of degrees of freedom . that is an integral we can do . ( were we able to integrate over all metrics in a way that is sensible within quantum field theory , we would already have been able to quantize gravity . ) now , scale invariance imposes constraints on the background spacetime fields used to construct the 2d action ( such as the metric , which determines the energy of the map from the worldsheet of the string ) . these constraints reduce to einstein 's equations . that is not a very fundamental derivation , but formulating string theory in a way which is independent of the starting point ( "background independence" ) is notoriously tricky . ( 2 ) : this goes under the name " strings in background fields , " and can be found in volume 1 of green , schwarz and witten .
as very well known , the root lattice $q$ of a semisimple lie group is a sub-lattice of its weight lattice $p$ because the roots are the weights of the adjoint representation . the congruence class of a weight ( which is the generalization of the triality in $su ( 3 ) $ ) is its representative in the coset $p/q$ . ( slansky calls it " congruency class" ) . in this construction the rules of conservation of triality are obvious . the center of the group is the character group $\mathcal{x} ( p/q ) $ , i.e. , the group of one dimensional representations . thus the triality can be viewed as a discrete charge of the center of the gauge group . 1 ) slansky gives a few examples of formulas of the congruence class of $su ( n ) $ , $e_6$ etc . as a linear combination of the weight components on the top of page 37 of his review . the following thesis by:lenka ha’kova’ contains the formulas for the full cartan classification on page $42$ . 2 ) the relation of the triality and the electric charge follows from the empirical " triality rule " based on the fact that all known baryons belong to ( the flavor ) representations corresponding to young tableaux whose number of boxes is a multiple of 3 , thus must belong to a multiple of 3 tensor products of the quark representation , thus must be of triality zero . the electric charges of these baryons are integer multiples of the electron 's charge $e$ , thus the charges of the constituent quarks must be multiples of $ \frac{1}{3} e$ . magnetic monopoles provide an explanation of the triality rule and the fractionalization of the quark charges . please see for example the explanation by preskill : ( based on the work of goddard nuyts and olive ) the explanation is based on : 1 ) the magnetic charge generator $m$ is a linear combination of the generators of the broken gauge group ( for example $su ( 5 ) $ ) . 2 ) the magnetic charge generator must satisfy a generalized dirac quantization condition $e^{2 i \pi m} = i$ 3 ) the center of the broken gauge group remains unbroken because it is a subgroup of unbroken u ( 1 ) subgroup . these three conditions force a normalization of the generators such that a triality zero state must be zero for integer charges which is exactly what is observed experimentally . it is worthwhile to mention that the set of all possible magnetic charges generate the magnetic dual of the gauge group which is the basis of the famous electric-magnetic duality . the answer to the third question is that if such a symmetry breaking existed then the triality 1 representation $\underline{\overline{6}}$ would be of electric charge 0 thus breaking the empirical triality rule .
it is not necessary to go into lot 's of details here e.g. whether it is electrons that move or ions , or if it is some energy barrier that prevents a current from flowing or the mobility of ions or whatever . all those details are completely irrelevant to the question . all you need to look at is resistance , capacitance , charge and current . if you are charged there is a current flowing from you to ground . the more current flows the faster you get discharged . if the resistance of your shoes + the resistance of the carpet you stand on is relatively low , a high current can flow and you get discharged in a fraction of a second . if the resistance is high it can take a while . when you rub your feet on a carpet , the carpet and also the underside of your shoes become charged . that charge can then flow through your shoes into your body . but if the carpet has a low resistance all the charge will flow away to ground before you get charged significantly . so to be able to receive a shock by walking over a carpet and touching a metal object , you need to have a carpet with a high resistance and also shoes that do not have too high a resistance or else the charge can not flow through the sole into you . when you touch a charged electroscope it will mostly discharge even if the resistance between you and ground is very high . that is because your capacitance is a lot higher . capacitance is simply the ability to store charge . the bigger an object is the more charge it can store . so when you touch it the charge will distribute between you and the electroscope but since you are much bigger most of the charge goes to you and the electroscope is left with very little .
yes there is , the curve is a a hypocycloid . see for instance : http://en.wikipedia.org/wiki/hypocycloid http://demonstrations.wolfram.com/spherewithtunnelbrachistochrone/ http://www.physics.unlv.edu/~maxham/gravitytrain.pdf
in short , we do not know . there are a few indications that time started at the big bang , or at least it had some form of discontinuity . this might be wrong though . according to general relativity , there is no such thing as an absolute time . time is always relative to an observer , without the universe there would be no corresponding concept of time . all observers within the universe would have their clocks " slowed down " the nearer they are to the big bang ( nearer in time ) . at the big bang point , their clock would stop . this said , we know that gr does not apply as-is all the way to the big bang . some cosmological theories like ccc predict a series of aeons and some form of cyclic universe . these predict a discontinuity ( ccc predicts a conformal scale change ) of time at the big bang , and at the end of the universe . as a side note : people tend to have a special fascination with time . for all we know though , time is only relatively special . from a cosmological point of view the discussion is whether space-time existed . we are pretty sure that it was very very small at some point .
the entropy change should be zero – and essentially is zero , in the correct theory that takes the indistinguishability into account – because the thin membrane does not materially change the system and carries a tiny entropy by itself . the first reason is enough : the removal of the membrane is a reversible process – one may add the membrane back – so the entropy has to be zero . an entropy can not increase during a reversible process because it would decrease when the process is reversed – and that would violate the second law of thermodynamics . in other words , the self-evident reversibility of the unphysical membrane means that $\delta s = \delta q/t$ where $\delta q$ is the heat flowing to the system – but it is clearly zero . the paradox is removed when the indinguishability of the particles is appreciated . the calculable entropy change is zero , as expected . in some sense , we are implicitly assuming that the molecules are indistinguishable everywhere above . if the molecules carried some passports , they could have a canadian and american passport in the volume $v_1 , v_2$ , respectively , which would be a very special state ( none of the molecules is abroad ) while the number of states would be increased because each molecule may be either in its own country/volume or abroad . this is indeed why the wrong classical calculation claims that the entropy would increase . however , this prediction may be extracted even if the initial total volume $v_1+v_2$ is actually perfectly mixed before the membrane is added .
both temperature and pressure variation with altitude are given here you can use the ideal gas law to get the volume with altitude from these .
i think you are really asking " how can light deliver an impulse to the sail " . the answer is that although light has no mass it does carry momentum . when light is reflected off the sail , conservation of momentum requires that the sail changes momentum by twice the momentum of the light . the extra kinetic energy of the sail comes from the red shift of the reflected light . this question has several answers that discuss the momentum of light in some detail .
moving at a constant velocity , no matter how close to the speed of light , has absolutely no effect on the person moving . in fact , it has no effect on the laws of physics . this is the fundamental tenet of special relativity - you cannot tell absolute motion , only relative motion between different things . the changes you are referring to are what someone moving at a different velocity would see from his or her perspective . if you are moving very fast with respect to me , i might say that your " mass " ( in the resistance to further acceleration sense ) is large , and that you appear squashed in the direction of your motion , but for you it is just a normal day and i am the one who appears to be having problems . now two catches . first , if two people start at rest with respect to one another ( e . g . sitting on the earth ) and you want to get one of them moving relativistically with respect to the other , there will necessarily be acceleration involved . human bodies cannot withstand acceleration much beyond the $9.8~\mathrm{m}/\mathrm{s}^2$ or so we feel from earth 's gravity . in fact , a fundamental tenet of general relativity is that acceleration is indistinguishable from gravity , so a large acceleration for a long time would be like living on a planet with a larger gravitational field at its surface , which would probably not agree with your body too well . second , if you are thinking of space travel , keep in mind that space is not empty . if you are traveling at near-light speeds compared to your surroundings ( e . g . the solar system ) , you are likely to encounter some piece of dust that , from your perspective , is heading toward you at nearly the speed of light . not a pleasant experience . furthermore , if you go fast enough relative to the cosmic microwave background radiation that permeates the universe , the stuff in front of you will appear blueshifted to the point of being rather harmful ( and nearly impossible to shield ) gamma rays .
dear asmaier , you should not view $\vec l = \vec x \times \vec p$ as a primary " definition " of the quantity but rather as a nontrivial result of a calculation . the angular momentum is defined as the quantity that is conserved because of the rotational symmetry - and this definition is completely general , whether the physical laws are quantum , relativistic , both , or nothing , and whether or not they are mechanics or field theory . to derive a conserved charge , one may follow the noether 's procedure that holds for any pairs of a symmetry and a conservation law : http://en.wikipedia.org/wiki/noether_charge in particular , the angular momentum has no problem to be evaluated in relativity - when the background is rotationally symmetric . the fact that you write $\vec l$ as a vector is just a bookkeeping device to remember the three components . more naturally , even outside relativity , you should imagine $$ l_{ij} = x_i p_j - x_j p_i $$ i.e. $l_{ij}$ is an antisymmetric tensor with two indices . such a tensor , or 2-form , may be mapped to a 3-vector via $l_{ij} = \epsilon_{ijk} l_k$ but it does not have to be . and in relativity , it should not . so in relativity , one may derive the angular momentum $l_{\mu\nu}$ which contains the 3 usual components $yz , zx , xy$ ( known as $x , y , z$ components of $\vec l$ ) as well as 3 extra components $tx , ty , tz$ associated with the lorentz boosts that know something about the conservation of the velocity of the center-of-mass . incidentally , the general $x\times p$ ansatz does not get any additional " gamma " or other corrections at high velocities . it is because you may imagine that it is the generator of rotations , and rotations are translations ( generated by $\vec p$ ) that linearly depend on the position $x$ . so the formula remains essentially unchanged . in typical curved backgrounds which still preserve the angular momentum , the other non-spatial components of the relativistic angular momentum tensor are usually not preserved because the background can not be lorentz-boost-symmetric at the same moment .
the energy in a static discharge is 1/2 * voltage * charge . for a 1mv vdg with a 1 meter sphere that is approximately 1/2 * 55µc * 1mv = 27.5j however that is too much to do experiments safely . you should stay far below 1j . also mutimeters do not like sparks or high voltages of several kv .
the equation $p = m ( 2t/m ) ^{1/2}$ is wrong . you got that by stipulating that $t=p^2 / 2m$ but that is only true for particles traveling in a constant potential . for a particle in one dimension , and working in the position basis , the momentum operator is $\hat{p} = -i\hbar ( d/dx ) $ and the energy operator ( hamiltonian ) is $\hat{h} = \hat{p}^2/2m = \frac{\hbar^2}{2m} ( d^2/dx^2 ) $ . when you respect that these are operators you get the right answer . if you put the particle in a square potential with $x \in [ 0 , l ] $ the wave functions wind up being sine waves like $$\psi_n ( x ) \propto \sin ( k_n x ) . $$ with $k_n=n\pi / l$ . the point is that when you put in the time dependence you get $$\psi_n ( x , t ) \propto\sin ( k_n x ) e^{-ie_nt/\hbar}$$ which is not a traveling wave . it is a standing wave which is moving neither left nor right . the expectation value of the momentum of this wave is zero even though the energy is not zero . to show this you just compute $$\langle \hat{p} ( t ) \rangle \propto \int\psi ( x , t ) \left ( -i\hbar\frac{d}{dx} \right ) \psi ( x , t ) dx=-i\hbar k_n \int\sin ( k_n x ) \cos ( k_n x ) dx=0 . $$ the energy , on the other hand , is $$\langle \hat{h} \rangle \propto \int\psi ( x , t ) \left ( -\frac{\hbar^2}{2m}\frac{d^2}{dx^2} \right ) \psi ( x , t ) dx=\frac{\hbar^2 k_n^2}{2m}\int\sin ( k_n x ) \sin ( k_n x ) dx \neq 0 . $$ it is only in the case of a constant potential , in which the wave functions are $$\psi ( x , t ) \propto e^{i ( k x - e t/\hbar ) } , $$ that you find $e=p^2/2m$: $$\langle \hat{p} ( t ) \rangle \propto -i\hbar \int e^{-ikx}\frac{d}{dx}e^{ikx}dx=\hbar k . $$ since the energy is $$e = \langle h \rangle = \left \langle \frac{\hat{p}^2}{2m} \right \rangle \propto -\frac{\hbar^2}{2m} \int e^{-ikx}\frac{d^2}{dx^2}e^{ikx}dx = \frac{\hbar^2 k^2}{2m} = \langle \hat{p}^2 \rangle / 2m \equiv p^2/2m . $$
there are several ways to look at these two effects . their mechanisms are very similar , but their experimental realizations are rather distinct from one another . here 's one of the physical models , a classical one . since you ask for physical intuition , i think the classical picture works best . quantum mechanical models use a different language and tool set . there are other classical models . it is helpful , ultimately , to understand all of the models . ( even in the classical model there is one bit of quantum mechanics that we can not get around . you will see it below . ) warm up - linear transmission : for practice , and to set the stage , lets first consider simple transmission of light through a solid . ( i will focus on solids as an example . ) a light wave of frequency $f$ is present in the crystal . the electrons in the crystal are initially " stationary " , by which i mean that the electron density is not changing with time . the electrons respond to the wave , and " slosh " in sync with the light , both in time and space , resulting in a polarization wave . ( there could be a phase shift between the light wave and the polarization wave . we will ignore that . ) the polarization wave acts as a source of a new electromagnetic wave at the same frequence $f$ . the new wave interferes with the original wave . this is the mechanism of linear transmission . it can explain the reduced wavelength of the light in the medium , and various other phenomena like reflection and refraction . it explains all the " linear " effects , where the frequency of the light does not change . second harmonic generation ( shg ) : things start the same as linear transmission . the em wave in the solid induces a polarization wave oscillating at $f$ . but now we can go one step further ( second order ) and recognize that the original wave is now in the presence not of a stationary electron density , but one that is oscillating at $f$ . the em wave interacts with the oscillating charge distribution , resulting in beating at the sum and difference frequencies , $2f$ and $0$ . the polarization wave at $2f$ generates an em wave at $2f$: the second harmonic . the $0f$ component also exists , but is usually of no interest . before moving on to the raman effect , we need an aside on the structure of solids ( same applies to molecules ) . a simple model of a solid ( or molecule ) has fixed ions producig a potential where the electrons live . a model one step closer to reality allows the ions to move , and we have phonons ( vibrations in a molecule ) , but we do not allow the electrons and phonons to interact : they are uncoupled . one step further allows electron-phonon interactions . in this model we can have infrared absorption : an incident em wave causes a polarization wave at $f$ as usual . but now , due to electron-phonon interaction , the sloshing electrons drag the ions along a little bit . if the frequency $f$ matches a vibration frequency ( and perhaps a few other conditions ) , that phonon ( vibration mode ) will be excited : energy moves from the em wave to the vibration mode . this happens at frequencies much lower than visible light frequecies . in the infrared . now the raman effect : the incident light at $f$ causes an electron polarization at $f$ . but we have allowed electron-phonon interactions . vibration modes are also causing an oscillating polarization , this one at the phonon ( vibration ) frequency , $\omega$ . as before , the electrons are experiencing two sources of oscillation , and the result is beat frequencies at $f-\omega$ and $f + \omega$ , and new em waves are generated at those frequencies . the former is called the stokes frequency , and is what is ususally observed in raman scattering . the second is called the anti-stokes frequency , and it can be observed , also . but now we have a problem . the stokes frequency is always observed . even if there was no vibrational excitation in the first place to cause the polarization at $\omega$ , light at $f-\omega$ is observed . how can that be ? this is where we have to reach out just slightly to quantum mechanics . each phonon ( vibration mode ) has a zero point energy . they vibrations are never truly at rest . there is always some vibration in every mode . it is the zero-point vibrations that couple to the electrons , causing a polarization at $\omega$ and ultimately new light at $f-\omega$ . zero-point fluctuations induce other effects . for example , spontaneous emission from an excited atom is induced by zero-point em field fluctuations . as zero-point fluctuations induce the conventional raman effect , it is more properly called the spontaneous raman effect . there is a lot more to the complete story . there is a stimulated raman effect , and other effects , and there is a discussion to be had concerning the coherence properties of the light . and , of course , there is the quantum mechanical picture .
actually i think " meaningful " or " physically meaningful " in many cases is as good as you are going to get , although the word can split up into finer meanings . if we think of mathematics as a language , then think of words that describe how well the description meets its intended purpose . does the mathematical description evoke the " right " ideas ? so words you might like to think about are : " well-formed " or " syntatically correct " , i.e. is the expression even " legal " as defined by the relevant axiom system : non-well-formed expressions might be $x\ , -$ or $y\ , \times$ ; " tautological " ( in the logical sense ) i.e. is the expression true in every possible interpretation : it may achieve tautology by being a theorem in a consistent axiom system for example . often the expressions will not be fully tautological , but at least it can be construed as such in the very restricted context of the problem or physical situation at hand . " well motivated " or " well supported " : there may be a physical argument as to why the proposed expression is meaningful in a given context . there may also be experimental motivation : if you used quaternion addition restricted to the $&lt ; i , \ , j , \ , k&gt ; $ subspace to do statics calculations with , experiment would back you up . " sound " : very often mathematicians make explicit definitions of objects and how they are to be manipulated , so the only meaningful expressions are those that flow from the particular definition / axiom system : this is very like " well formed " . as an example , a hilbert space is a complete inner product space / vector space equal to its topological dual ( two equivalent definitions ) , so arguments flowing from hilbert spaces behest forming inner products , formation of cauchy sequences , construction of linear functionals and testing them for continuity and so forth : my point is that the " meaningful " expressions are very tightly , explicitly and obviously defined at the outset . physicists often do similar things : define an " axiom system " to represent some physical phenomenon and then ferret out " theorems " in the system : in physics there is the further required step that we must test that our inferred theorems are in keeping with experimental observation , but in principle it is very like , at least in principle , the " axiom , definition , lemma , proposition . . . . " flow of a mathematics argument : the " meaningful " ways of putting statements together is decided upon at the beginning . the process that feynman described as ( 1 ) guess a theory ( i.e. . lay down your axioms and work out all their implications ) and ( 2 ) test whether it models reality and ( 3 ) if you are wrong , as you almost always are in physics , go back to step ( 1 ) and iterate again ! keep in mind that mathematics is very much a language , and the kind if ideas you are groping for are not unlike the kinds of words a teacher , child developmental clinician or behaviouralist might use to describe the language they witness as the baby grows , acquires his or her mother tongue , firstly as " non-legal " ( at least in our restricted logical way of thinking ) sequences and babbles , then as simple sentences , then as accurate descriptions of the immediate world around him or her leading onto complicated stories and allegories wherein many concepts are grasped and wielded at once and accurately woven together to evoke precise ideas in the minds of the other social animals that make up the child 's social world . there are many different shades of meaning for " meaningful " .
archimedes ' principle tells us that the upthrust on a body immersed in a fluid is equal to the weight of the fluid displaced , where the weight is the force given by $f = ma$ i.e. the mass of fluid displaced , $m$ , multiplied by the acceleration , $a$ , experienced by the fluid . in this context there is no difference between gravitational acceleration and inertial acceleration - this is one example of einstein 's equivalence principle - so : $$ a = a_{gravity} + a_{inertial} $$ and the upthrust is therefore : $$ f = m ( a_{gravity} + a_{inertial} ) = v \rho ( a_{gravity} + a_{inertial} ) $$ as you said in your question .
two lenses placed such that the focal points of each overlap . i will give you an image to help . . . the blue dot is the focal point of both lenses . the red and green paths are simple enough , and the brown shows how the angle of incoming light is flipped , so it looks like the light is coming in from the top , not the bottom . btw : i used what looks like to convex lenses , this could also be two flats . so even something as simple as a glass ball given the right index could do this . edit : reading this a second time over , makes me think you might want something different than two glass spheres . this would make things on the left appear to be on the right and things on your right , left . i think a glass cylinder lens would be more like it . that way the ray 's horizontal angle remains unchanged . are we trying to do this ? if so , you need only use two of these . like this . basically take two of those prisms you are using and sandwich them with aluminum or some other reflective material .
a potential well in the schrodinger equation will produce energy levels similar to the energy levels of the hydrogen atom a nucleus with two protons to be neutral will need two electrons . these will be accommodated in the lowest two energy levels : because of the spin statistics of electrons they cannot occupy the exact same energy level . a nucleus with three protons ( z=3 ) will need three electrons occupying the lowest three energy levels etc . the electrons may thermally or by photon interactions be kicked up to higher energy levels giving the characteristic spectral lines of the specific atom when returning to the ground state . electrons do not come in pairs . the fill up the levels so that the atoms are neutral .
i believe that is covered by this answer i posted some time ago to a related ( but not quite the same ) question . adapting it to your notation , $$t = \frac{1}{\sqrt{2g ( m + m ) }}\biggl ( \sqrt{r_0 r ( r_0 - r ) } + r_0^{3/2}\cos^{-1}\sqrt{\frac{r}{r_0}}\biggr ) $$ the same formula is given in the wikipedia article qmechanic mentioned in a comment .
the path integral over a " thick layer " of spacetime always produces the transition amplitudes $$ \langle {\rm final}| u | {\rm initial}\rangle $$ where $u$ is the appropriate unitary evolution operator . this is already true in non-relativistic quantum mechanics where the equivalence between feynman 's path integral approach and the operator formalism is being shown most explicitly . the only difference in quantum field theory is that there are infinitely many degrees of freedom . it is like having infinitely many components of $x_i$ where the discrete index $i$ becomes continuous and is renamed as a point in space , $ ( x , y , z ) $ , and $x$ is replaced and renamed by fields $\phi$ , so $x_i$ is replaced by $\phi ( x ) $ . it means that if we have a " thick layer " of spacetime given by time coordinate $t$ satisfying $$ t_0 &lt ; t &lt ; t_1 $$ then the path integral with boundary conditions $\phi_0$ and $\phi_1$ at the initial and final slice calculates the matrix element $$ \langle \phi_1| u | \phi_0 \rangle$$ in a full analogy with ( just an infinite-dimensional extension of ) non-relativistic quantum mechanics . just to be sure , the initial and final states above are really meant to be given by the wave functional $$ \psi [ \phi ( x , y , z ) ] = \delta [ \phi ( x , y , z ) - \phi_0 ( x , y , z ) ] $$ which holds for one of them ( initial/final ) while the other is obtained by replacing $0$ by $1$ . so far , everything is completely isomorphic to the case of non-relativistic quantum mechanics except that the number of independent degrees of freedom $\hat x$ , now called $\hat \phi$ , is higher . the only new thing in quantum field theory is that we also often need path integrals where the initial or final state is replaced by a semiinfinite line . when it is done so , we no longer specify a particular configuration on this initial slice or final slice because there is really no initial slice or final slice on this side ( or on both sides , if the path integral integrates over configurations in the whole spacetime ) we do not specify the boundary conditions ; in fact , when we follow the correct rules , the path integral immediately and automatically replaces the initial or final state ( replaced by the semi-infinite line or half-spacetime ) by the ground state $|0\rangle$ or $\langle 0|$ , whichever is appropriate . why is it so ? it is because in the operator formalism , such a path integral still contains the evolution operator $$ \exp ( \hat h \cdot \delta t / i\hbar ) $$ over an infinite period $\delta t$ . in fact , the $i\epsilon$ and related rules in the path integral – the way how we treat the poles in the complex energy/momentum plane – really guarantee that $$ \delta t = \infty ( 1+i \epsilon ) $$ where $\epsilon$ is an infinitesimal constant which is however greater than $1/\infty$ where $\infty$ is the positive real prefactor above . consequently , the exponential ( evolution operator ) above contains the factor of $$ \exp ( -\infty \epsilon \hat h ) $$ which is suppressing states in the relevant initial and/or final state according to their energy . because $\infty\epsilon$ is still infinite , all the excited states are suppressed much more than the ground state and only the ground state survives . that means that if we integrate in feynman 's path integral over all configurations in the whole spacetime , we automatically get the matrix elements in the vacuum $$ \langle 0 | ( \dots ) |0 \rangle . $$ similarly , if we integrate over configurations in a semi-infinite spacetime and specify the boundary condition for the fields ( resembling a classical field configuration ) at the boundary , we obtain matrix elements like $$ \langle 0 | ( \dots ) | \phi_0 \rangle $$ or the hermitian conjugates of them where $\phi_0$ is the boundary condition . if the inserted operators $ ( \dots ) $ are empty , just an identity operator , the expression above clearly reduces to $$ \langle 0 | \phi_0 \rangle \equiv \psi^* [ \phi_0 ] $$ where the last identity is nothing else than an infinite-dimensional generalization of $$ \langle \psi| x \rangle = \psi^* ( x ) $$ in non-relativistic quantum mechanics . we just have infinitely many $x$-like degrees of freedom in quantum field theory which is why wave functions are replaced by wave functionals .
the particle-hole symmetry is not exactly a property of the superconductors . it is rather a property of the free electron gas , with the fermi level -- separating all the empty states above from all the occupied below it -- cutting a parabola . the property to have the spectrum symmetry $\varepsilon_{\mathbf{k}}=\varepsilon_{-\mathbf{k}}$ ( see the quotation below for the notations meaning ) is mandatory to have a particle-hole symmetry also . i quote below the book by c . kittel , quantum theory of solids ( 1963 ) , chapter 5 . in treating the electron system it is particularly convenient to redefine the vacuum state as the filled fermi sea , rather than the state with no particles present . with the filled fermi sea as the vacuum we must provide separate fermion operators for processes which occur above or below the fermi level . the removal of an electron below the fermi level is described in the new scheme as the creation of a hole . we consider first a system of $n$ free non-interacting fermions having the hamiltonian $$h_{0}=\sum_{\mathbf{k}}\varepsilon_{\mathbf{k}}c_{\mathbf{k}}^{+}c_{\mathbf{k}}$$ where $\varepsilon_{\mathbf{k}}$ is the energy of a single particle having $\varepsilon_{\mathbf{k}}=\varepsilon_{-\mathbf{k}}$ . we agree to measure $\varepsilon_{\mathbf{k}}$ from the fermi level $\varepsilon_{f}$ . in the ground state [ . . . $\phi_{0}$ . . . ] all one-particle states are filled to the energy $\varepsilon_{f}$ and above $\varepsilon_{f}$ all states are empty . we regard the state $\phi_{0}$ as the vacuum of the problem : it is then convenient to repreent the annihilation of an electron in the fermi sea as the creation of a hole . thus we deal only with electrons ( for states $k &gt ; k_{f}$ ) and holes ( for states $k &lt ; k_{f}$ ) . the act of taking an electron from $\mathbf{k}'$ within the sea to $\mathbf{k}''$ outside the sea involves the creation of an electron-hole pair . the language of the theory has a similarity to positron theory , and there is a complete formal similarity between particles and holes . we introduce the electron operators $\alpha^{+}$ , $\alpha$ by $$\alpha_{\mathbf{k}}^{+}=c_{\mathbf{k}}^{+}\ ; ; \ ; \alpha_{\mathbf{k}}=c_{\mathbf{k}}\ ; \ ; \text{for}\ ; \ ; \varepsilon_{\mathbf{k}}&gt ; \varepsilon_{f}$$ and the hole operators $\beta^{+}$ , $\beta$ by $$\beta_{\mathbf{k}}^{+}=c_{-\mathbf{k}}\ ; ; \ ; \beta_{\mathbf{k}}=c_{-\mathbf{k}}^{+}\ ; \ ; \text{for}\ ; \ ; \varepsilon_{\mathbf{k}}&lt ; \varepsilon_{f}$$ the $-\mathbf{k}$ introduced for the holes is a convention which gives correctly the net change of wave-vector or momentum : the annihilation $c_{-\mathbf{k}}$ of an electron at $-\mathbf{k}$ leaves the fermi sea with a momentum $\mathbf{k}$ . thus $\beta_{\mathbf{k}}^{+}\equiv c_{-\mathbf{k}}$ creates a hole of momentum $\mathbf{k}$ . so it is essentially a matter of convention in defining the operators . obviously , it is no more strange than calling all the excitations of a solid a particle i believe . for superconductor , though , the particle-hole symmetry plays a more prominent role than just a redefinition of what is called quasi-electron and quasi-hole . since the phonons ( as in the bcs model ) couple electrons at $\mathbf{k}$ and $-\mathbf{k}$ , they couple electrons and holes in the prescription cited above . hence the crucial role played by the nambu spinor . this is nevertheless not an exact symmetry as lubos motl mentioned , but it can hardly be contradict . indeed , the ratio $\delta / \varepsilon_{f}$ is so small ( $10^{-3} -10^{-5}$ for bcs superconductors ) that the linearisation of the spectrum close to the fermi energy is a really good approximation , and so the relation $\varepsilon_{\mathbf{k}}=\varepsilon_{-\mathbf{k}}$ is well verified .
the short answer is : it is a fundamental property of nature . the very short answer is " quantum " the long answer : from the beginning of the 20th century , slowly but certainly nature revealed to us that when we go the very small dimensions its form is quantum . it started in the middle of the nineteenth century , with the table of elements which showed regularities that could not be explained except by an atomic model with equal electrons to the charge of the nucleus . there were efforts to understand why the electrons which were part of the atoms did not spiral down into the nucleus and disappear , with the bohr model . this introduced the idea of " quantum " . the energy the electrons were allowed to have in the possible orbits around the nucleus was postulated to be quantized . in a similar way that the vibrations on a string have specific frequencies allowed with wavelengths which are multiples of the length of the string , the electrons about the atoms could have only specific energies . then a plethora of experimental results led theorists to postulate quantum mechanics from a few " axioms " . starting with the schrodinger equation formal theoretical quantum mechanics took off and we never looked back because it fits perfectly all known experimental data in the microcosm , and not only . the uncertainty principle is a lynch pin in the mathematical formulation of quantum mechanics . a premiss is that all predictions of the qm theory are given as probability distributions , i.e. no observable can be predicted except as a probable value . in quantum mechanics to every physical observable there corresponds an operator which acts on the state functions under study . operators often are represented by differential forms and the algebra of operators holds . in quantum mechanics two operators can be commuting , that is they can be like real numbers a*b-b*a=0 , or not , the value can be different than 0 . this means that one is working in a larger set than the real numbers , complex numbers are needed . the heisenberg uncertainty principle for position and momentum as it appears in the fundamental postulates of quantum mechanics is a commutation relationship between conjugate variables , x and p , represented by their corresponding operators : $$ [ x , p_x ] =i\hbar$$ this relationship is very fundamental in the theory of quantum mechanics which describes very successfully matter as we have studied up to now , mathematically . if the hup were falsified it would falsify the foundations of qm . now on the subject of the electron and the nucleus . the quantum mechanical solutions that describe the orbitals of the hydrogen atom , for example , have non zero probabilities for the electron to find itself in the center of the nucleus , when the angular momentum is zero . so it is not clear to me how feynman could have used that hand waving argument you are describing in your question . after all we do have electron capture nuclear reactions . he is probably basing the argument on the very small volume the nucleus occupies with respect to the atomic orbitals which will give a very small probability of capture .
the answer to this question depends very much on what field you are studying . for instance , in many areas of physics , being time derivatives of position , most would take the velocity and acceleration equations and treat the whole system as a differential equation , then solve for distance as a function of time only . similarly , they would then differentiate the distance to get a velocity equation as a function of time only . however , in some areas of study like robotics and certain fields in engineering , velocity may not only vary with time , but it may vary differently according to specific position . thus , in those circumstances , velocity is made a function of time and position . also , because the velocity has a different time dependence at every position , the position function becomes dependent on the path traveled . this means that in cases where position/velocity/acceleration are discontinuous and/or path-dependent , both distance and velocity must be functions of one another . add version sometimes they are only functions of time , sometimes they are functions of time and each other . depends on the situation . edit it is true that in many cases where velocity is taken as a function of position that it can be written as just a function of time ; however , this can be very impractical . so , the fact remains that in those circumstances we do write them as functions of position and time . edit 2 velocity and distance can also be functions of more than just time . temperature and mass are just some examples . edit 3 to answer the new part of your question , no this does not imply that anything is constant . this just means that these three things are functions of time . however , you do not need to hold velocity constant to see how position changes with time . rather $v ( t ) $ should be the time derivative of $s ( t ) $ and similarly for velocity -> acceleration .
it is a quite common assumption . when the particle is at infinity ( or fairly distant from the origin ) , the potential is assumed to be zero . other examples can be found in numerous scattering problems .
take the velocity vector a two points separated by an infinitesimally small time interval . subtract the later vector from the earlier , and divide by the infinitesimally small time interval . the resulting vector is the instantaneous acceleration ( in the limit that the time interval goes to zero ) . the direction of that vector is the direction of the acceleration . in finding velocity graphically , we plot the $x$ component of the position vector and the $y$ component of position vector for each instant of time . we end up with a curve : the trajectory . the direction of the velocity is tangent to the curve at each point of time . the analog of that method to find the direction of acceleration is to plot $v_y$ and $v_x$ for each instant of time on graph paper . the direction of acceleration is tangent to that curve at each point of time . you can not find the direction of acceleration directly from the trajectory , although if you had some extra information , namely the value of time at each point on the trajectory , you could figure it out with some work .
the keywords here are rayleigh scattering . see also diffuse sky radiation . but much more simply , it has to do with the way that sunlight interacts with air molecules . blue light is scattered more than red light , so during the day when we look at parts of the sky that are away from the sun , we see more blue than red . during sunset or sunrise , most of the light from the sun comes towards the earth at a sharp angle , so now the blue light is mostly scattered away , and we see mostly red light .
blackbody radiation is characteristic of every object in thermodynamic equilibrium and black bodies at constant uniform temperature . at any temperature objects emit thermal radiation . em radiation is emitted because inside the object , due to thermal motion of particles charged particles/dipoles start to oscillate , electromagnetic radiation is emitted because of these vibrations . if the object is a black body at constant uniform temperature , the radiation is called blackbody radiation . the energy emitted by any object is always finite with certain distribution over the frequencies with peak at some frequency . we cannot naively expect the energy emitted with all the frequencies carrying equal weight . this is a phenomenon which happens and is observed . this is explained quantum mechanically , infact this led to the development of quantum mechanics . so a cavity with a small hole with em radiation inside it is appropriate to study mathematically and is a near perfect blackbody because the hole allows negligible radiation to enter the cavity so that it affects negligibly the thermal equilibrium condition and we can have a very near thermal equilibrium and observe blackbody radiation from it . rayleigh and jeans could not explain blackbody spectrum at higher frequencies , their law predicted infinte spectral radiance at infinite frequencies . planck gave the solution to the ultraviolet catastrophe ( infinite spectral radiance at infinite frequencies ) and explained the spectrum of blackbody radiation by assuming the energy of the oscillators inside the cavity to be series of discrete values but not continuous which eventually results in spectral radiance going to zero at higher and infinite frequencies with peak at some frequency . radiations emitted by ordinary objects can be approximated as blackbody radiation , they are nearly in thermal equilibrium . one of the importance is that to know the temperature of a star , the relation between the temperature and wavelength of the peak , called wien 's displacement law , evaluated from planck 's radiation formula , is used approximating the radiation to be blackbody radiation .
the inflating universe can for example be described by the flrw metric $$ d\tau = dt^2-a ( t ) ^2 ( dx^2 + dy^2 + dz^2 ) $$ the scale factor $a ( t ) $ , which describes the expansion , is obtained from the appropriate friedman equation which contains only the vacuum energy $\rho_0$ as source of gravity $$ \frac{\dot{a}}{a} = \sqrt\frac{8\pi g \rho_0}{3} = h $$ with the exponential solution $$ a = k e^{h t}$$ for inflation with this exponential expansion to occur , the the hubble constant $h$ and therefore $\rho_0$ has to be constant . during an inflationary phase it is assumed that the most important contrubution to the vacuum energy is due to the potential energy $v ( \phi ) $ of the scalar inflaton field $\phi$ which has the lagrangian $$ \frac{\dot{\phi}^2}{2} + \frac{ ( \nabla\phi ) ^2}{2} - v ( \phi ) $$ so an inflational period itself , for example the inflation of the early universe , can not be described by a phase transition ; the potential energy density of the inflaton field is assumed to be about constant , sitting in the left local minimum in the figure below . however the end of this early inflation which can be described by a jump to the lower value of the potential energy density ( and much slower inflation ) we observe today , corresponds to a phase transition during which the difference in the potential energy density of the inflaton field between the left and the right local minimum is released as some kind of latent heat ( reheating ) and converted to kinetic energy and finally to particles and seeds of the galaxies we observe today . more details about how the end of the early inflation can be described as a phase transition can be found for example in this paper . as said above the inflaton field is assumed to be a scaler field ; plenty of scalar fields occur for example for different physical reasons in string theory , some of them are described here or here . some people are trying to reconstruct the potential of the inflaton field from observations .
for spin measurements the original experiment was the stern-gerlach experiment in which you will see that a prior unpolarized beam will split up in two ( spin up and down ) orientations . see : http://en.wikipedia.org/wiki/stern%e2%80%93gerlach_experiment for helicity , a very ingenious and fascinating experiment is the famours goldhaber experiment that uses a very peculiar set of elements and also the mößbauer effect to measure the helicity of neutrinos . the helicity is the projection of the spin onto the momentum direction , and thus if you measure spin and momentum , you can compute helicity . link to original paper here : http://www.bnl.gov/nh50/ people often confuse helicity and chirality they are only the same in the case of massless particles . this is also the reason why we know that ( at least interacting ) neutrinos are always left-handed ( the famous $$su ( 2 ) _l$$ ) , at least if we assume that neutrinos are massless ( which is almost the case , but not strictly ) . in contrast to helicity for massive particles ( where you can always boost into a frame where you change the sign of the momentum direction and thus change helicity ) , chirality is a lorentz-invariant property of the particle .
opening my mechanical engineering book on the rotating rings section and i get $$ \sigma_t ( r ) = \rho \omega^2 \frac{3+\nu}{8} \left ( r_i^2 + r_o^2 + \frac{r_o^2 r_i^2}{r^2} - \frac{1+3\nu}{3+\nu} r^2 \right ) $$ $$ \sigma_r ( r ) = \rho \omega^2 \frac{3+\nu}{8} \left ( r_i^2 + r_o^2 - \frac{r_o^2 r_i^2}{r^2} - r^2 \right ) $$ inner edge von-mises stress $\sigma^2_{vm} = \sigma_r^2 + \sigma_t^2 - \sigma_r \sigma_t $ , $r=r_i$ , $\sigma_r ( r_i ) =0$ $$ \frac{\sigma_{vm}}{\rho \omega^2 r_o^2} = \frac{ ( 1-\nu ) r_i^2}{4 r_o^2} + \frac{\nu+3}{4} $$ outer edge von-mises stress $\sigma^2_{vm} = \sigma_r^2 + \sigma_t^2 - \sigma_r \sigma_t $ , $r=r_o$ , $\sigma_r ( r_o ) =0$ $$ \frac{\sigma_{vm}}{\rho \omega^2 r_o^2} = \frac{ ( 3+\nu ) r_i^2}{4 r_o^2} + \frac{1-\nu}{4} $$ with the inner edge usually having the higher stress . here $\nu=0.4$ is poisson 's ratio . the result i get from mathcad is 34,200 rpm . the difference being , a ) the overall density is higher because the cd contains metals also , and b ) being brittle there are bound to be micro-cracks already in place before spinning starts . assuming the $75\ , \rm mpa$ figure is correct the density has to be $\rho=2.3\ , \rm gm/cm^3$ for it crack at 25,000 rpm .
it is just a label . each element of the set $\left|j_1m_1\right&gt ; $ is a unique state vector . for example , $\left|1,1\right&gt ; $ and $\left|1 , -1\right&gt ; $ are two independent angular momentum state vectors , just like any other state vector . whenever you see the indices $j$ , $m$ , etc . , see them as referring to a set of state vectors each labelled with particular values and put some numbers into the $j$s and $m$s in an equation to get an idea of what one of the equations belonging to that set is like .
( i have a suggestion to make this question a cw . ) general physics : ( early undergrad and advanced high school ) problems in physics i.e. irodov - ( highly recommended ) problems in physics s s krotov - ( once again , highly recommended but out of print ) physics olympiad books - ( have not read but saw some olympiad problems back in the day ) physics by example ( like this book a lot , lower undergrad ) feynman 's tips on physics ( exercises to accompany the famous lectures ) general qualifying exam books : the following books are a part of a series dedicated to the qualifying exams in american universities and has a large compilation of problems of all levels . others in the series include mechanics , electromagnetism , quantum mechanics , thermodynamics , optics and solid state physics . unlike other compilation of exercises for qualifiers ( such as princeton or chicago problems , or the one mentioned below ) , they make no excuse for economy and include as many problems from all levels for each subtopic . another good book that i read recently for my exam is the two volume series : a guide to physics problems ( part 2 has some relatively easy but interesting problems . i have not gone through the first part , which is much much more challenging . )
what is a shock wave ? definition : a shock wave ( also called shock front or simply " shock" ) is a type of propagating disturbance . like an ordinary wave , it carries energy and can propagate through a medium ( solid , liquid , gas or plasma ) or in some cases in the absence of a material medium , through a field such as the electromagnetic field . this about covers it . your thought experiment speaks of an explosive , and it can create a shock wave only if a medium exists to be shocked : sudden almost instantaneous creation of vacuum in air/medium and immediate collapse of that as air/medium rushes to fill it . the answer to your question that would arrive at the surface with pretty much the same amount of energy that you had get in air . is that right ? is no , it is not right . to reply to the title question : would a high-explosive in a vacuum be less harmful ? one would have to know the number of shrapnel from the explosion , and also if there is dust from the explosive or gas for some reason . from conservation of energy the fragments in vacuum will have larger energy because they will not be pushing against a medium ( to create the shock wave ) and if they hit would do more damage . any dust and gas would also move faster but one would need a detailed model and objective for the explosion to know whether the damage would be the same as in non vacuum .
ok , the inside of the sphere is perfectly-reflecting , and there is an ideal optical diode to let light in but keep it inside . as you keep the light turned on , the photon density in the sphere goes up and up , of course . it " looks " brighter and brighter , but you do not see that because the light can not escape . after turning the light off , it stays bright , the photons just keep bouncing around . if you " stick your head in " to look , you see a bright uniform glow that quickly dies away because your head and eyes are absorbing all the photons . but do the photons bounce around forever ? no ! ! even a perfectly-reflective sphere will still interact with the light , because of radiation pressure . each time a photon bounces off a wall , the wall gets kicked backwards , gaining energy at the expense of the photon ( on average ) . light can not produce a smooth force , only a series of kicks with shot noise statistics , because one photon hits the wall at a time . these kicks eventually heat up the walls , and cool down the photons . ( from the photon 's point of view , the photon frequency is going down because of doppler-shifts during reflection off the moving walls . ) eventually everything equilibrates to a uniform temperature , hotter than the sphere started out . i do not know how long that would take . [ in any realistic circumstance this radiation pressure effect can be ignored , because it is much less important than the " reflection is not 100% perfect " effect . ]
if you are sitting outside the event horizon watching a clock fall in , you will never see the clock reach the event horizon . you will see the clock slow as it approaches the horizon and you will see it running slower and slower . however there is no sense in which time stops at the event horizon . you can wait as long as you want , and you will see the clock creep closer and closer , but time will continue for both you and the clock . now suppose you are holding the clock . assuming you can survive the tidal forces you will cross the point where the external observer thinks the event horizon is ( you would see no horizon there ) and you would hit the singularity in a finite time . the problem is that at the singularity the spacetime curvature becomes infinite and there is no way to calculate your path in spacetime past this point . this is known as geodesic incompleteness ( annoyingly wikipedia has no good article on this but google " geodesic incompleteness " for lots of info on the subject ) . it is because there is no way to calculate your trajectory past the singularity that it is said ( but not by me ! ) that spacetime stops there .
the term $g_{i} + g_{i'} \rightarrow g_{i'}$ in the manual can be used because the structure which is under analysis is a periodic multi layered structure . due to its periodicity the different indexes of summation can be grouped and coupled or reduced to a single summation index .
it did not leave " only a scratch" ; it appears to have melted about 40% of the surface of the planet , creating a basin that is still visible at least 3.9 billion years later . it was not big enough to shatter the planet . assuming it was 2000 kilometers in diameter , that is only about 2.5% of the volume of the planet . shortly after the impact , the effects would have been much more visible . it is likely the other 60% of the planet 's surface was strongly affected as well . the surface has been extensively modified since then by volcanism and other impacts . leaving 40% of the surface of the planet " one of the smoothest surfaces found in the solar system " is about as far from " only a scratch " as you could possibly get without destroying the planet . mars is big enough that after any impact that does not completely disrupt it , it is going to return to a very nearly spherical shape . edit : and since the mass of the impactor was presumably incorporated into mars , it would have made mars itself a bit larger ; if mars is now 7000 km in radius and the impactor was 2000 km in radius , then mars before the impact would have been about 6945 km in radius . the surface of mars is now effectively about 55 kilometers higher than it was before . ( or some of the mass may have been blown off into space ; it depends on the nature of the imact . ) the surface you see now is largely the result of that impact . and if you had seen it shortly after the impact , the appearance would have been dramatic . if the impactor had hit at a shallower angle , it is likely that mars would have ended up with a sizable moon .
i think the best answer to your question is simply " because that is all we can see when we do experiments . " that is , no matter how hard anyone tries or how much energy they toss into the processes , electrons and quarks show no signs of any appendages , surfaces , hair-like structures , bumps , volume , whatever . when you model them mathematically as points , the math works -- and when you do not , you start getting problems and mismatches . protons and neutrons in sharp contrast show every sign of having volume , and emphatically are no more point-like ( zero dimensional ) than atoms , although their volumes are hugely smaller ( grain of sand compared to a large football stadium ) . string theory postulates , well , stringy structure , but has no experimental evidence for it , and does not expect any direct evidence . the energy levels are too high to be accessible to any conceivable direct-detection experiment .
i think the main reason is practical , but it might be related to a theoretical reason . the main reason is that we almost never use the time-dependent schroedinger equation because if the state was not stationary , its rate of change would be , at the usual atomic scales , so fast that we could not measure it or study it empirically with laboratory-sized apparatus . similarly , what governs the observable properties of macroscopic bodies , such as their chemical bonds and colours , involves stationary states . if the states were not stationary , the body would not persist long enough for us to consider it as having a property . it is striking how little direct empirical support the time-dependent schroedinger equation has , and how little use it finds . we do not even use it to study scattering events ( which , admittedly , for a very brief time occur very rapidly ) . this might be related to a deeper theoretical reason one finds in statistical mechanics . in statistical mechanics , it is often pointed out that measurements made with laboratory-sized equipment necessarily involve a practically infinite time average such as $$\lim_{t\rightarrow\infty}\frac1t\int_0^t f ( t ) g ( t ) dt . $$ well , in quantum mechanics , measurement has something similar about it , in that it always involves amplification of something microscopic up to the macroscopic scale so we can observe it ( an observation made by many , including feynman ) , and the main way to do this seems to be to let the microscopic event trigger the change from a meta-stable state to a stable equilibrium state of the laboratory-sized apparatus ( h . s . green observation in quantum mechanics , nuovo cimento vol . 9 , pp . 880--889 , posted at http://www.chicuadro.es , and many others since ) . once again , this involves a long-time , stable , equilibrium as in statistical mechanics . but the relation to the practical reason is not completely clear . that said , in theory it is sometimes possible to rephrase the time-dependent schroedinger evolution equation into a space-evolution equation , even though no one ever does this since it has no earthly use . consider the klein--gordon equation ( which is the relativistic version of schroedinger 's equation ) , $$ ( {\partial\over \partial x}^2-{\partial\over \partial t}^2 + v ) \psi = 0 . $$ obviously , we can get isolate either $x$ or $t$ , and under certain conditions take the square root of the operator to get $$ {\partial\over \partial x} \psi = \sqrt{ ( {\partial\over \partial t}^2 - v ) }\psi . $$ under the usual physical assumptions of flat space--time and no field-theoretic effects , one could do this to isolate $t$ and get the time evolution because we assume that energy is always positive , so we can indeed take the square root ( all the eigenvalues of the hamiltonian are positive ) . this may not always be true when , as here , we try to isolate $x$ and get the space-evolution . === now , as to the question of why consider any evolution at all , why not just consider $\psi ( x , y , z , t ) $ in a relativistically timeless fashion , the main answer is that it wreaks havoc with the idea of measurement , observable . and the justification of the born interpretation . dirac tried to write a quantum mechanics textbook your way , but gave up even after the fifth chapter , where he remarks that the notion of observable in not relativistic , and for the rest of the book he proceeds non-relativistically ( until he get to the dirac equation at the end ) . the second edition abandons the attempt to be relativistic , is more traditional and uses the time evolution point of view from the start . he remarked , famously , the main change has been brought about by the use of the word «state» in a three-dimensional non-relativistic sense . it would seem at first sight a pity to build up the theory largely on the basis of nonrelativistic concepts . the use of the non-relativistic meaning of «state» , however , contributes so essentially to the possibilities of clear exposition as to lead one to suspect that the fundamental ideas of the present quantum mechanics are in need of serious alteration at just tbis point , and that an improved theory would agree more closely ' with the development here given than with a development which aims at preserving the relativistic meaning of «state» throughout . and in fact relativistic quantum mechanics , as opposed to field theory , is , like many-particle relativistic ( classical ) mechanics , not theoretically very well developed . there seem to be so many problems , people prefer to jump right to quantum field theory in spite of the divergences and need for renormalisation and everything . furthermore , relativistic qm is restricted to the low energy regime since with high energies , particle pair production is possible , yet the equations of qm hold the number of particles as fixed and do not allow for pair production .
yes - the coiled spring has a certain amount of ( potential ) energy . when it gives up the energy to the ball , you could say the ball does negative work on the spring , so it loses ( potential ) energy .
john 's answer is a good one , i just wanted to add some equations and addition thought . let me start here : heating is really only significant when you get a shock wave i.e. above the speed of sound . the question asks specifically about a $200^{\circ} c$ increase in temperature in the atmosphere . this qualifies as " significant " heating , and the hypothesis that this would only happen at supersonic speeds is valid , which i will show here . when something moves through a fluid , heating happens of both the object and the air . trivially , the total net heating is $f d$ , the drag force times the distance traveled . the problem is that we do not know what the breakdown is between the object and the air is . this dichotomy is rather odd , because consider that in steady-state movement all of the heating goes to the air . the object will heat up , and if it continues to move at the same speed ( falling at terminal velocity for instance ) , it is cooled by the air the exact same amount it is heated by the air . when considering the exact heating mechanisms , there is heating from boundary layer friction on the surface of the object and there are forms losses from eddies that ultimately are dissipated by viscous heating . after thinking about it , i must admit i think john 's suggestion is the most compelling - that the compression of the air itself is what matters most . since a $1 m$ ball in air is specified , this should be a fairly high reynolds number , and the skin friction should not matter quite as much as the heating due to stagnation on the leading edge . now , the exact amount of pressure increase at the stagnation point may not be exactly $1/2 \rho v^2$ , but it is close to that . detailed calculations for drag should give an accurate number , but i do not have those , so i will use that expression . we have air , at $1 atm$ , with the prior assumption the size of the sphere does not matter , i will say that air ambient is at $293 k$ , and the density is $1.3 kg/m^3$ . we will have to look at this as an adiabatic compression of a diatomic gas , giving : $$\frac{t_2}{t_1} = \left ( \frac{p_2}{p_1} \right ) ^{\frac{\gamma-1}{\gamma}}$$ diatomic gases have : $$\gamma=\frac{7}{5}$$ employ the stagnation pressure expression to get : $$\frac{p_2}{p_1} = \frac{p1+\frac{1}{2} \rho v^2}{p1} = 1+\frac{1}{2} \rho v^2 / p1 $$ put these together to get : $$\frac{t_2}{t_1} = \left ( 1+\frac{1}{2} \rho v^2 / p1 \right ) ^{2/7}$$ now , our requirement is that $t2/t1\approx ( 293+200 ) /293 \approx 1.7$ . i get this in the above expression by plugging in a velocity of about $2000 mph$ . at that point , however , there might be more complicated physics due to the supersonic flow . to elaborate , the compression process at supersonic speeds might dissipate more energy than an ideal adiabatic compression . i am not an expert in supersonic flow , and you can say the calculations here assumed subsonic flow , and the result illustrates that this is not a reasonable assumption . addition : the concorde could fly at about mach 2 . the ambient temperature is much lower than room temperature , but the heatup compared to ambient was about $182 k$ for the skin and $153 k$ for the nose . this is interesting because it points to boundary layer skin friction playing a bigger role than i suspected , but that is also wrapped up in the physics of the sonic wavefront which i have not particularly studied . you have to ask yourself , what pressure is the nose at and what pressure is the skin at . the flow separates ( going under or above the craft ) at some point , and that should be the highest pressure , but maybe it is not the highest temperature , and i can not really explain why . we have pretty much reached the limit of the back-of-the-envelope calculations . ( note : i messed up the $\gamma$ value at first and then changed it after a comment . this caused the value to go from 1000 mph to 2000 mph . this is actually much more consistent with the concorde example since it gets &lt ; 200 k heating at mach 2 . )
this heat or energy given by the fork is given to the next layer of air and in this way , this energy makes sound . this is the crux of the misunderstanding ; sound energy is not transmitted by heat , but rather by the concerted kinetic energy of the gas movement . it is adiabatic because the heat has nowhere to go other than the gas itself . it is not isothermal because during compression , the temperature increases , and vice versa . response to comment : from which energy is heat energy evolved during compression of gas ? acc . to you , the kinetic energy of the molecules that makes the sound comes from the fork . where does the heat energy go during expansion ? i think the heat energy comes from the work of the fork on the gas during compression . but the energy goes as kinetic energy of the molecule . confused yes , heat is evolved during the compression . during expansion , the heat energy is converted into potential energy of the expanded gas . energy cycles between heat and potential energy , similar to this slow motion video of a shotgun primer firing underwater . during the compression stages , the bubble heats up , and during the expansion stages , the bubble cools down . the energy is still there in both the expanded and contracted bubbles , just in different forms . however , in this case , the adiabatic assumption is slightly incorrect , and the bubble slowly loses energy after each oscillation , causing the bubble oscillations to be damped after about 5 oscillations . here is a simple derivation of the speed of sound ; the key fact is to note that the speed is determined by $dp/d\rho$ , which in some sense represents the " stiffness " of a gas . they use the adiabaticity of the process to show $c=\sqrt{\gamma rt}$ , but do not totally explain why adiabaticity is used . once you understand that page , read this one , which is a more complicated version which explains why the adiabaticity is used . in particular , newton assumed boyle 's law held in his derivation of wave propagation in gases , but this is actually false , as explained in the paragraph which starts with " the flaw in this reasoning is the assumption that changes in pressure of an acoustic disturbance are exactly proportional to the changes in density " . but basically , the idea is that heat has nowhere to go except to heat up the gas .
if both of the person 's hands were really at one end of the staff , it would not be possible . the reason is that , in order for the staff not to move , the net torque on it has to be zero . with only one plane of rotation , the torque due to a given force can be defined as $\tau = rf\sin\theta$ where $r$ is the distance from a chosen " center of rotation " to the point where the force is applied , and $f$ is the magnitude of the force . now , suppose you choose the point where the staff is being held as the center of rotation . in that case , the hands exert zero torque since $r=0$ for them , but the force of gravity acts on the center of the staff , 3 feet away , so it exerts a nonzero torque . thus the total net torque is nonzero , and the staff will drop . however , if you take into account that a person 's hands have some finite size , and thus they can not be holding the staff exactly at its end , then it is possible to hold it up . the force required will depend on where the hands are positioned , though . suppose $d$ is the width of the person 's hand , and suppose that the force exerted by each hand is applied exactly in the center of that hand . ( this last assumption is not true but it is probably good enough for a basic explanation ) then you have three forces being exerted on the staff : gravity acting through its center , the force from one hand acting at a distance $d/2$ from one end , and the force from the other hand acting at a distance $3d/2$ from that same end . in order for the staff to be stationary , all the forces have to cancel out : $-mg + f_1 + f_2 = 0$ and all the torques have to cancel out : $-mg\times\frac{l}{2} + f_1\times\frac{d}{2} + f_2\times\frac{3d}{2} = 0$ ( $l$ is the length of the staff ) solving this system of equations gives you $f_1 = \frac{mg}{2d} ( 3d-l ) $ and $f_2 = \frac{mg}{2d} ( l-d ) $ plugging in the given numbers with a reasonable guess for hand width , i get 770 n ( 170 lbf ) , directed downward , for the first force ( closer to the end of the staff ) , and 860 n ( 190 lbf ) , directed upward , for the second force . so we are basically talking about the ability to exert 200 lbf with each hand , although you could rely on the structural integrity of your arm to help you out . notice that , since one of your hands is pulling upward and the other is pushing downward , the net force is 90 n , or 20 lbf , upwards - exactly what is needed to support the weight of the staff .
well first to solve the problem , let us assume force $f$ is applied to the end of rope as shown . now as the tension is uniform in the rope around the pulley , the extension in spring 2 is $f/k_2$ . for the first spring the extension is that which is caused by $2f$ and is $2f/k_1$ now as the point where the force actually applied will go down , its downwards displacement would be $f/k_2+2*2f/k_1$ ( the factor of two is because if the spring one extends by $y$ the displacement of the point of force is $2y$ ) now just write extension as $x_t=f/k_2+4f/k_1$ , remove the force common and take the remaining part on the otherside , and that part is your equivalent $k$
i am not sure what effects the heliosheath is having on the spacecraft beyond interacting with the on-board instruments which are still relaying data back to earth . on effect though is that there is no longer a solar wind from behind the spacecraft as they have crossed over into the transition region between space dominated by the solar wind and the interstellar medium . as for their speed , from the offical voyager website : as of march 2010 , voyager 1 was at a distance of 16.9 billion kilometers ( ~ 113 au ) from the sun . voyager 2 was at a distance of 13.7 billion kilometers ( ~ 92 au ) . voyager 1 is escaping the solar system at a speed of about 3.6 au per year . voyager 2 is escaping the solar system at a speed of about 3.3 au per year . the voyagers do not really have any fuel left , except maybe for attitude control . ( this is achieved using small propulsion-system thrusters to incessantly nudge the spacecraft back and forth within a deadband of allowed attitude error . voyagers 1 and 2 have been doing that since 1977 , and have used up a little over half their 100 kg of propellant as of april 2006 [ 1 ] . ) effectively they are just coasting out away from the solar system with the final velocities ( declerated by the sun 's gravity ) that they had when they left their final planetary encounter back in the 80 's . i strongly recommend checking out the web site as it has lots of great information about the on-going voyager missions .
this is because the path integral ${\cal z}$ is an infinite-dimensional version of a grassmann-odd gaussian integral $$\int \ ! \mathrm{d}^n \bar{\theta} ~\mathrm{d}^n\theta ~e^{\sum_{i , j=1}^n\bar{\theta}_i ~m^i{}_j ~\theta^j}~\propto~\det ( m ) , $$ where the indices $i , j$ can be interpreted as dewitt 's condensed notation .
well - air does have " relative humidity " and this really affects the things that interact with it . for example - you will have a hard time cooling down by sweating when the relative humidity is very high , as the rate of evaporation that you can achieve ( and therefore heat rejection ) becomes quite low : this is why you end up " sweaty " on a hot muggy day . similarly , materials like nylon are very hygroscopic : when they are in humid air , they will absorb some of that moisture . this will affect their mechanical properties . an extreme example of this can be seen with sugar and salt - they will " melt " when exposed to humid air for a while . if we are talking about " pure air " , then we still see this effect . there are chemical reactions ( mostly with the oxygen in the air ) that can result in changes . for example , various metals will oxidize when left unprotected in the air : you could say they became " wet " . exposing them to a reducing environment ( e . g . hot hydrogen gas ) could " dry " them again , as the oxygen is reduced and the surface once again becomes metallic . i hope that i did not completely misunderstand your question . . .
for the layman interested in coordinates , go here and then click on the map link for any given day : http://curiosityrover.com/tracking/drivelog.html then you are in the " google mars " view , with the rover 's path displayed . of course , you could figure out something to get a list of coordinates in a standard map format . but if you are looking for something simple , i suggest just pointing your mouse at the object you want and reading the coordinates in the bottom right of that screen :
i can only recommend textbooks because that is what i have used , but here are some suggestions : gravity : an introduction to general relativity by james hartle is reasonably good as an introduction , although in order to make the content accessible , he does skip over a lot of mathematical detail . for your purposes , you might consider reading the first few chapters just to get the " big picture " if you find other books to be a bit too much at first . a first course in general relativity by bernard schutz is one that i have heard similar things about , but i have not read it myself . spacetime and geometry : an introduction to general relativity by sean carroll is one that i have used a bit , and which goes into a slightly higher level of mathematical detail than hartle . it introduces the basics of differential geometry and uses them to discuss the formulation of tensors , connections , and the metric ( and then of course it goes on into the theory itself and applications ) . it is based on these notes which are available for free . general relativity by robert m . wald is a classic , though i am a little embarrassed to admit that i have not read much of it . from what i know , though , there is certainly no shortage of mathematical detail , and it derives/explains certain principles in different ways from other books , so it can either be a good reference on its own ( if you are up for the detail ) or a good companion to whatever else you are reading . however it was published back in 1984 and thus does not cover a lot of recent developments , e.g. the accelerating expansion of the universe , cosmic censorship , various results in semiclassical gravity and numerical relativity , and so on . gravitation by charles misner , kip thorne , and john wheeler , is pretty much the authoritative reference on general relativity ( to the extent that one exists ) . it discusses many aspects and applications of the theory in far more mathematical and logical detail than any other book i have seen . ( consequently , it is very thick . ) i would recommend having a copy of this around as a reference to go to about specific topics , when you have questions about the explanations in other books , but it is not the kind of thing you had sit down and read large chunks of at once . it is also worth noting that this dates back to 1973 , so it is out of date in the same ways as wald 's book ( and more ) . gravitation and cosmology : principles and applications of the general theory of relativity by steven weinberg is another one that i have read a bit of . honestly i find it a bit hard to follow - just like some of weinberg 's other books , actually - since he gets into such detailed explanations , and it is easy to get bogged down in trying to understand the details and forget about the main point of the argument . still , this might be another one to go to if you are wondering about the details omitted by other books . this is not as comprehensive as the misner/thorne/wheeler book , though . a relativist 's toolkit : the mathematics of black-hole mechanics by eric poisson is a bit beyond the purely introductory level , but it does provide practical guidance on doing certain calculations which is missing from a lot of other books .
how does string theory differ from philosophy or religion ? as the field of mathematics , string theory differs from philosophy and religion not by its experimental verifiability but by its methods . it is mostly based on mathematical reasoning . it uses basically the same tools that are used in other areas of quantum physics . it has many points of contact with other areas of physics and mathematics . it is studied in physics and math departments . some of those working on string theories are also highly regarded in other areas of theoretical physics or mathematics . ( e . g . witten got the fields medal , the highest distinction in mathematics . ) some concepts first discussed in string theory found later use in particle physics . ( e . g . , ads/cft , hep-th/9905111 ; hep-ph/0702210 ) it may make one day testable predictions of previously unknown effects , and can then be checked for its validity .
i think you are exercising an incorrect picture of statistics here - mixing the inputs and outputs . you are recording the result of a measurement , and the spread of these measurement values ( we will say they are normally distributed ) is theoretically a consequence of all of the variation from all different sources . that is , every time you do it , the length of the string might be a little different , the air temperature might be a little different . of course , all of these are fairly small and i am just listing them for the sake of argument . the point is that the ultimate standard deviation of the measured value $\sigma$ should be the result of all individual sources ( we will index by $i$ ) , under the assumption that all sources of variation are also normally distributed . $$\sigma^2 = \sum_i^n{\sigma_i^2}$$ when we account for individual sources of variation in an experiment , we exercise some model that formalizes our expectation about the consistency of the experiment . your particular model is that the length of the string ( for instance ) changes very little trial after trial compared to the error introduced by your stopwatch timing . unless we introduce other errors , this is claiming $n=1$ , and if the standard deviation of your reaction timing contributes $0.1 s$ to the standard deviation of the measurement , then theoretically the measurement should have that standard deviation as well . if this conflicts with the statistics of the time you actually recorded , then the possible ways to account for this include : your reaction time is not as good as you thought it was there are other sources of experimental error i would favor the latter , although it could be a combination of both of them .
a quick google search for " aurora plasma temperature " brings up several interesting results , which seem fond of reporting temperatures in electron volts . that is entirely sensible , but probably not quite what you want . while we could do some math to convert those measurements to kelvin , rocket measurements of plasma densities and temperatures in visual aurora by a . g . mcnamara , however , conveniently reports temperatures in kelvin already . according to that source , the temperature of the electron plasma in an aurora ranges from 500 k to 1400 k . note , however , that the density , and therefore the total heat capacity , of the plasma is very low ; the altitude at which auroras occur is practically outer space . so , if you somehow managed to fly up there and stick your hand in one , it probably would not feel all that warm .
this argument is going to be challenging , because the theory of a single massive vector interacting like a photon with scalar and spinor matter is renormalizable . this is an accident of the u ( 1 ) gauge theory . it has a stueckelberg version of the higgs mechanism , where you take the limit that the higgs mass goes to infinity while the condensate charge goes to zero . the limit is described on wikipedia in the page on higgs mechanism , under affine higgs mechanism . this is an accidental property of massive electrodynamics . the reason the massive gauge field is nonrenormalizablei in general is that for a gauge group other than u ( 1 ) , the charge cannot be arbitrarily small , so there is no decoupling limit for the higgs . the standard argument against a massive vector is that the propagator has a kk/ m2 term in the numerator iwhich means that longitudinal components do not have a falling off propagator , which means that loops involving longitudinal vectors blow up as a power at high k . in gauge theories , gauge inavariance guarantees that longitudinal bosoms are not produced , but this requires that gauge invariance is broken spontaneously not by the lagrangian . the stueckelberg limit means that a mass term does not wreck renormalizability for u ( 1 ) .
actually , you are right : the hubble constant is not really constant . at least , it is not constant in time . the reason it is called a constant is that , when edwin hubble originally compared the recession velocities of galaxies with their distances in 1929 , there was no reason to expect any particular pattern . after all , just a few years prior , people had thought there were no other galaxies . but what hubble found was that , except for a small amount of random variation , the velocities of galaxies were proportional to their distances ; in other words , the ratio $v/d$ was roughly the same for all galaxies he observed . the value of this ratio came to be known as hubble 's constant , $h_0 \equiv \frac{v}{d}$ , because it was constant from one galaxy to the next , rather than varying randomly as one might have guessed at the time . of course , it was not long before people realized that if the recessional velocity of each galaxy was proportional to its distance , you could extrapolate back to some point in the past at which $d = 0$: all the galaxies would have started out in the same place . this gives you an effective age for the universe . if you use a simple linear extrapolation , from basic kinematics you get $$t = \frac{d}{v} = \frac{1}{h_0}$$ so the hubble " constant " is not constant in time , but rather is inversely related to the age of the universe . as the universe gets older , the hubble constant gets smaller , as you would expect . this happens because the distance $d$ to any given galaxy increases with time . however , the fact that the universe has an age does not create an absolute time . sure , different observers at different points in spacetime will measure different values for the age of the universe . and sure , you could define a time coordinate system by specifying that the time coordinate for any observer is the age of the universe as measured by that observer . this is called the comoving time , and it is a useful and sensible way to set up a coordinate system in time . but it is not the only possibility , and there is certainly nothing so special about it that it deserves to be called " absolute . " any observer who is moving with respect to the universe as a whole ( i.e. . relative to the hubble flow ) would not measure time at the same rate as this comoving time .
it was really difficult to find any mentioning of the " lunar twilight " in astronomical literature . but i managed to find one in an old book by g.v. rosenberg called " general picture of twilight phenomena " . the book is in russian , but there is not too much about the moon . so i will just translate all the relevant stuff . first of all there is this plot for the solar twilight ( i have translated it ) : where e is the illuminance of a horizontal surface in luxes . and lg -- is a logarithm base 10 . and then there is a small chapter : lunar twilight lunar twilight is similar to solar twilight , but it is much more bleak . the illumination from the moon varies from $10^{-9}$ ( new moon ) to $2\cdot 10^{-8}$ ( full moon ) from the illumination from the sun at the same point in the sky . therefore the illumination from the full moon high in the sky corresponds approximately to the middle of nautical twilight . and lunar twilight ends virtually when the moon gets under the horizon .
textbook solution the first thing to do is to define the center of mass and relative coordinates : $$ r ( t ) = {m_1 r_1 + m_2 r_2 \over m_1 + m_2} $$ $$ r ( t ) = r_2 - r_1 $$ you invert this to find $$ r_1= r - {m_2\over m} r$$ $$r_2=r+ {m_1\over m} r$$ the equation of motion for r is trivial , since center of mass is a conservation law : $$ {d^2 r \over dt^2 } = 0 $$ and it is solved by $$ r ( t ) = v_0 t + r_0 $$ where $v_0$ and $r_0$ are the initial center of mass velocity and position respectively ( which are calculated from the given initial conditions ) . the nontrivial equation is for the relative coordinate : $$ m {d^2 r \over dt^2} = - {m r\over |r|^3}$$ where $m=m_1 + m_2$ is the total mass , and m is the reduced mass : ${1\over m} ={1\over m_1} + {1\over m_2}$ the problem is reduced to solving the kepler motion in a 1/r potential . from now on , i will rescale time to make the mass parameter in the r equation 1 . you can choose the x axis to lie along the initial r , and the y-axis to lie along the component of the initial $\dot{r}$ perpendicular to the initial r . another way of saying this is that you rotate the coordinates to make the angular momentum vector $r\times p$ where $p=m\dot{r}$ to lie along the z-axis . this rotation reduces the problem to a plane , and the rotation matrix columns are given by the normalized initial r ( now along the x-axis ) , the component of the initial velocity perpendicular to r , normalized ( along the y-axis ) , and normalized l along the z-axis . you then use units to set the total over reduced mass to 1 , and use polar coordinates in the x-y plane of the motion , and note that the angular momentum is constant : $$ r^2 {d\theta\over dt} = l $$ this tells you that equal areas are swept out by the r-vector in equal times . the equation of motion for r ( t ) ( no longer a vector , now a scalar radial coordinate ) is : $$ {d^2 r \over dt^2} = {l^2 \over r^3} - {1 \over r^2} $$ then you change time out for $\theta$ , expressing everything in terms of $r ( \theta ) $ , which you can do using the equal area law , whenever the anguar momentum is nonzero ( if the initial angular momentum is zero , or very close to zero , this is a one-dimensional two-body problem which can be solved directly by more elementary means ) . the equation of motion for $r ( \theta ) $ simplifies when you make a coordinate transformation to $u={1\over r}$: $$ {d^2 u \over d\theta^2} = c - u $$ where c is some unimportant constant , and this is solved by $$u ( \theta ) = {1\over a} ( 1 + a \cos ( \theta -\theta_0 ) ) $$ where a is the semi-major axis of the ellipse ( if the orbit is an ellipse ) , $\theta_0$ determines the orientation in the x-y plane , and a is the eccentricity of the ellipse ( if a&lt ; 1 ) , or determines the angle of the hyperbola ( if a> 1 ) or tells you the orbit is a parabola ( a=1 ) . the only result you need is that $$ r ( \theta ) = {a\over 1+ a\cos ( \theta-\theta_0 ) } $$ this gives you the solution of r as a function of $\theta$ , which gives the shape of the orbit . this is where textbooks stop . finding $\theta$ as a function of $t$ but you then want the solution for $\theta$ as a function of time , to get the r and $\theta$ as functions of time . this is determined from the area law , conceptually : $$ r^2 {d\theta\over dt} = l$$ $$ {d\theta \over ( 1+ a \cos ( \theta-\theta_0 ) ) ^2 }= {l \over a^2} dt $$ and integrating this from time 0 to time t , tells you in principle what $\theta ( t ) $ is . the result can be written as : $$ f ( a , \theta ) = f ( a , \theta_0 ) + {l\over a^2} t $$ where $f ( a , \theta ) $ is the special function that gives you the area of a conic section of parameter a , in a wedge from the focus where one half-line is along the major axis , and the other half-line makes an angle $\theta$ with the first . this special function is not expressible in terms of elementary functions . this function is defined by the integral above , and you can calculate it numerically using any numerical integration method . finding this function , and inverting it , is the only difficult part of this problem . there are three limits which are necessary for perturbations : for a=0 , $ f ( 0 , \theta ) = \theta$ for a=1 , $ f ( 1 , \theta ) = {y\over 4} + {y^3\over 12} $ where $y=r\sin{\theta} = {\sin ( \theta ) \over 1+\cos ( \theta ) }$ for $a=\infty$ , $ f ( a , \theta ) \approx {1\over a} \tan ( \theta ) $ each of these are elementary degenerations : the first is the circle , the second is the parabola , and the third is a straight line . the important thing is that each of these degenerations gives you x ( t ) and y ( t ) which are simple , and further , you can perturb around each of these three limits in a nice way . in the following , the parameter t is rescaled to absorb ${l\over a^2}$ circle : $x ( t ) = \cos ( t ) $ $y ( t ) = \sin ( t ) $ parabola : $y ( t ) = ( \sqrt{1+36t^2} + 6t ) ^{1\over 3} - ( \sqrt{1+36t^2} - 6t ) ^{1\over 3} $ $x ( t ) = {1\over 2} - {y^2\over 2} $ line : $x ( t ) = {1\over a}$ , $y ( t ) = t$ the line and circle are obvious , the parabola is found by inverting the cubic for y as a function of t using the cubic equation . near the circle , time is periodic with the orbital period , which is the area inside the ellipse $aa^2$ divided by the area sweep-rate $l/2$ . so you have once-winding function from a circle to a circle , which can always be written as a fourier series with a linear term , which is found from the power series of the integrand in a , integrated term by term . near the straight-line hyperbola , you can similarly perturb in a series , and the only interesting degeneration is the parabola . near the parabola , the perturbation theory is a little more complicated .
i think that while temperature is a major factor for resistance , the equation you wrote is not correct . in a linear approximation , $$r=r_0 ( 1+\alpha ( t-t_0 ) ) $$ where $r_0$ is the resistance at temperature $t_0$ . exaclty all factors that affect resistance , i think they are many , but these are the major ones ( electrical applications ) .
we know that this field is an electromagnetic field because it is a function of space and time . from amperes law we know that the time rate of change of electric field ( derivative with respect to time ) generates a magnetic field , from faraday 's law we know that the time rate of change ( derivative with respect to time ) of magnetic field generates electric field . so every field generates the other , that is why it is an electromagnetic field . you do not need to bother with gauge , having the electric field you can obtain the expression of the magnetic field ( b ) for z > 0 and z &lt ; 0 . at point z = 0 ( where the current is situated ) you will find they have the same expression , the only difference is a minus sign . having the same magnetic field with different signs to left and right is known to be the magnetic field generated by infinite current sheet . the relation between magnetic field and the current flowing on the sheet is give by the equation in yellow here hope that helped
there are two important concepts here that explain the influence of gravity on light ( photons ) . the theory of special relativity , proved in 1905 ( or rather the 2nd paper of that year on the subject ) gives an equation for the relativistic energy of a particle ; $$e^2 = ( m_0 c^2 ) ^2 + p^2 c^2$$ where $m_0$ is the rest mass of the particle ( 0 in the case of a photon ) . hence this reduces to $e = pc$ . einstein also introduced the concept of relativistic mass ( and the related mass-energy equivalence ) in the same paper ; we can then write $$m c^2 = pc$$ where $m$ is the relativistic mass here , hence $$m = p/c$$ in other words , a photon does have relativistic mass proportional to its momentum . de broglie 's relation , an early result of quantum theory ( specifically wave-particle duality ) , states that $$\lambda = h / p$$ where $h$ is simply planck 's constant . this gives $$p = h / \lambda$$ hence combining the two results , we get $$m = e / c^2 = h / \lambda c$$ again , paying attention to the fact that $m$ is relativistic mass . and here we have it : photons have ' mass ' inversely proportional to their wavelength ! then simply by newton 's theory of gravity , they have gravitational influence . ( to dispel a potential source of confusion , einstein specifically proved that relativistic mass is an extension/generalisation of newtonian mass , so we should conceptually be able to treat the two the same . ) there are a few different ways of thinking about this phenomenon in any case , but i hope i have provided a fairly straightforward and apparent one . ( one could go into general relativity for a full explanation , but i find this the best overview . )
topological insulators are gapped states of free fermions with particle number conservation and time-reversal symmetry . according to the k-theory classification , there is no topological insulator in 1d . however , 1d interacting fermions with time-reversal symmetry do have non-trivial symmetry protected topological phases if the particle number is conserved only mod n . the result can be obtained from group cohomology theory arxiv:1106.4772 of chen , gu , liu , and wen .
here is the precise treatment for determining the eigenvectors of the full hamiltonian . you will probably find the two physics . se posts i link to at the end useful for understanding this stuff ( which basically boils down to understanding tensor products ) : let $\mathcal h_0$ denote the harmonic oscillator hilbert space and $\mathcal h_{1/2}$ denote the spin hilbert space then the total hilbert space of the system is their tensor product $\mathcal h = \mathcal h_0\otimes \mathcal h_{1/2}$ . the notation you are using here is really a shorthand for defining the total hamiltonian as an operator on $\mathcal h$ $$ \hat h = \hat h_0\otimes i_{1/2} + \mu bi_0\otimes \hat s_z $$ where $i_0$ is the identity operator in the harmonic oscillator hilbert space , and $i_{1/2}$ is the identity operator in the spin hilbert space . if we define $$ \hat h_0|n\rangle = e_n|n\rangle , \qquad \hat s_z|s_z\rangle = \hbar s_z|s_z\rangle $$ then the states $|n\rangle$ form a basis for $\mathcal h_0$ consisting of eigenvectors of $h_0$ and the states $|s_z\rangle$ form a basis for $\mathcal h_{1/2}$ consisting of eigenvectors of $\hat s_z$ . if we define $$ |n , s_z\rangle = |n\rangle\otimes |s_z\rangle $$ then it is a standard result on tensor products of hilbert spaces that the states $|n , s_z\rangle$ form a basis for the total hilbert space $\mathcal h = \mathcal h_0\otimes \mathcal h_{1/2}$ of the system . moreover , one can show ( which you have probably already essentially done by the comments above ) using these definitions that $$ \hat h|n , s_z\rangle = ( e_n+\hbar \mu bs_z ) |n , s_z\rangle $$ so that the states are a basis for the total hilbert space $\mathcal h$ consisting of eigenvectors of $\mathcal h$ . we have therefore identified that the " stationary states " you wrote down originally , if correctly mathematically interpreted , can be proven to be the eigenstates of the full hamiltonian . other useful posts : how to tackle &#39 ; dot&#39 ; product for spin matrices should it be obvious that independent quantum states are composed by taking the tensor product ? along with anything else you find about tensor products .
after reading @peter morgan 's answer , and giving it some more thought , i think this is actually simpler than it seems at first . for finite-dimensional spaces the trace of a commutator is indeed always zero . for infinite-dimensional spaces the trace is not always defined , since it takes the form of an infinite sum ( for countable dimension ) or an integral ( for continuous dimension ) which do not always converge . when the trace is defined , it obeys the same rules as in finite dimension , specifically the trace of a commutator is zero . for operators such as $x$ , $p$ and their products , the trace is simply not defined , so there is no sense in asking questions about it . when computing thermal averages , the factor $e^{-\beta h}$ makes sure the trace converges , since the energy is always bound from below ( otherwise the system is unphysical ) . i am sure the concepts mentioned by @peter morgan are important in this context ( boundedness , kms-condition ) , but i do not know anything about them , and i think the answer i just provided suffices for practical purposes .
how many hamsters do you need to power a 15 w light bulb ? i am going to treat this as a fermi problem . let 's give a hamster a typical mass $m$ of $\ 0.15\ kg$ . and let 's assume this hamster can climb $\approx 0.6\ m/s$ against a gravitational acceleration of $\ g\ =\ 10\ m/s^2$ . in doing so this hamster would generate $\ m g v\ =\ 1\ watt$ . this leads to the estimate of 15 hamsters being needed to do the job . as a check on this result , we can use kleiber 's law to upscale this estimate from the realm of rodents to that of humans . if we assume a typical human weight to equal that of 500 hamsters ( $\approx 75\ kg$ ) , kleiber 's law tells us we have to upscale the power by a factor $500^{3/4}\approx\ 100$ . this leads to an estimate of a human being capable of generating $\approx 100\ w$ . a very reasonable result : " adults of good average fitness average between 50 and 150 watts for an hour of vigorous exercise " [ from : wikipedia article on human power ] .
suppose you are holding an apple . you obviously know where the apple is , and as long as you do not drop the apple you can predict it is future position . but suppose you now drop the apple and you want to predict where it will be in one second , two seconds or even ten seconds if you are standing on a tall building . as soon as you drop the apple it starts accelerating downwards due to gravity . therefore to predict the position of the apple 1 second after it leaves your hand you have to know the acceleration . if you repeat the experiment on the moon you need to know that the acceleration on the moon is different , because of it is lower gravity , and if you feed this lower acceleration into your equation you will find the apple has moved a smaller distance after 1 second , as indeed you will know from watching videos of the lunar astronauts .
there are two essential facts that make a hole a hole : fact ( 1 ) the valence band is almost full of electrons ( unlike the conduction band which is almost empty ) ; fact ( 2 ) the dispersion relation near the valence band maximum curves in the opposite direction to a normal electron or a conduction-band electron . fact ( 2 ) is often omitted in simplistic explanations , but it is crucial , so i will elaborate . step 1: dispersion relation determines how electrons respond to forces ( via the concept of effective mass ) explanation : a dispersion relation is the relationship between wavevector ( k-vector ) and energy in a band , part of the band structure . remember , in quantum mechanics , the electrons are waves , and energy is the wave frequency . a localized electron is a wavepacket , and the motion of an electron is given by the formula for the group velocity of a wave . an electric field affects an electron by gradually shifting all the wavevectors in the wavepacket , and the electron moves because its wave group velocity changes . again , the way an electron responds to forces is entirely determined by its dispersion relation . a free electron has the dispersion relation $e=\frac{\hbar^2k^2}{2m}$ , where m is the ( real ) electron mass . in the conduction band , the dispersion relation is $e=\frac{\hbar^2k^2}{2m^*}$ ( $m^*$ is the " effective mass" ) , so the electron responds to forces as if it had the mass $m^*$ . step 2: electrons near the top of the valence band behave like they have negative mass . explanation : the dispersion relation near the top of the valence band is $e=\frac{\hbar^2k^2}{2m^*}$ with negative effective mass . so electrons near the top of the valence band behave like they have negative mass . when a force pulls the electrons to the right , these electrons actually move left ! ! i want to emphasize again that this is solely due to fact ( 2 ) above , not fact ( 1 ) . if you could somehow empty out the valence band and just put one electron near the valence band maximum ( an unstable situation of course ) , this electron would really move the " wrong way " in response to forces . step 3: what is a hole , and why does it carry positive charge ? explanation : here we are finally invoking fact ( 1 ) . a hole is a state without an electron in an otherwise-almost-full valence band . since a full valence band does not do anything ( can not carry current ) , we can calculate currents by starting with a full valence band and subtracting the motion of the electrons that would be in the hole state if it was not a hole . subtracting the current from a negative charge moving is the same as adding the current from a positive charge moving on the same path . step 4: a hole near the top of the valence band move the same way as an electron near the top of the valence band would move . explanation : this is blindingly obvious from the definition of a hole . but many people deny it anyway , with the " parking lot example " . in a parking lot , it is true , when a car moves right , an empty space moves left . but electrons are not in a parking lot . a better analogy is a bubble underwater in a river : the bubble moves the same direction as the water , not opposite . step 5: put it all together . from steps 2 and 4 , a hole responds to electromagnetic forces in the exact opposite direction that a normal electron would . but wait , that is the same response as it would have if it were a normal particle with positive charge . also , from step 3 , a hole in fact carries a positive charge . so to sum up , holes ( a ) carry a positive charge , and ( b ) respond to electric and magnetic fields as if they have a positive charge . that explains why we can completely treat them as real mobile positive charges in their response to both electric and magnetic fields . so it is no surprise that the hall effect can show the signs of mobile positive charges .
the primary authority on this is the wmap project ( though there are other observational cosmology projects springing up all the time ) . they analyze the cmb observations every two years , and the nine-year data was just published . the paper can be found here . basically , everyone assumes flatness because we know it is a very good approximation , because it simplifies the equations , and also because aesthetically being exactly flat is more appealing than being just close to flat . thus most of the analysis is done assuming the curvature parameter $\omega_k$ vanishes . toward the middle of the paper , in table 9 , they show the results of fits that allow curvature to vary . they report $\omega_k = -0.037^{+0.044}_{-0.042}$ . when combined with results from other projects and surveys , they find $\omega_k = -0.0027^{+0.0039}_{-0.0038}$ .
for hamiltonian operators of the form $$h = -\frac{\hbar^2}{2m} \delta + v ( x ) $$ $v$ must be real to assure that $h$ is at least symmetric and eigenvalues $e$ are real . therefore , if $0\neq \psi= \psi ( x ) $ is an eigenfunction , so that : $$-\frac{\hbar^2}{2m} \delta \psi + v ( x ) \psi = e\psi \tag{1}$$ taking the complex conjugate of both sides you have : $$-\frac{\hbar^2}{2m} \delta \overline{\psi} + v ( x ) \overline{\psi} = e\overline{\psi}\: . \tag{2}$$ ( 1 ) and ( 2 ) together imply that the real valued functions $\psi= \psi + \overline{\psi}$ and $\phi= i ( \psi - \overline{\psi} ) $ are eigenfunctions with the same eigenvalue $e$ . notice that the pair $\psi$ , $\phi$ encompass the same information as that of the pair $\psi$ and $\overline{\psi}$: they are linearly independent if and only if $\psi$ and $\overline{\psi}$ are and generate the same vector subspace ( of the eigenspace associated to $e$ ) . if an eigenspace with energy $e$ has dimension $\geq 2$ , you may have a nonvanishing flux of a generic eigenfunction with that energy .
no . the large scale geometry of the universe is described by the friedmann-lemaitre-robertson-walker metric . the geometry of the spacetime of a black hole ( in its simplest form ) is described by the schwartzschild metric . these are totally different solutions of the einstein field equations . for example , in the schwarzschild metric , the spacelike part of the spacetime is curved , in the flrw metric it is planar .
a description is here : he employed an innovative approach called the shadowgraph . a typical shadowgraph experiment is illustrated below . in this technique , light is passed through an airflow and reflected onto a screen or film plate . since shock waves create changes in the temperature and density of the airflow , the light waves are bent , or refracted , as they pass through the shock waves . these refracted light patterns create shadows that can be seen on the screen . once one has the " picture " recording it is a matter of triggering the exposure of the camera to the passing of the bullet . since he had also experimented with spark shock waves i guess the following experimental setup : the signal from the trigger wires ( the two vertical lines in the photo you show ) triggers a spark gap in a completely dark enclosure where the film is exposed and waiting . when the spark comes during the passing of the bullet the differences in the refraction index of the air from the shock wave are recorded . actually we used the technique of triggered sparking to record cosmic rays passing through a spark chamber back in the 1960 's . the film was continually exposed and was moved forward after the sparking/passing cosmic . the spark lasts nanoseconds only .
there are lots of questions about the twin paradox on this site , so it is probably not worth going over that material again . what is worth saying is that where people tend to get confused is by misunderstanding what an inertial frame is and how different inertial frames can be compared . we should simplify matters a bit and put twin b on a spaceship because orbital motion is a bit more complicated . the only time a and b can directly compare anything with each other is the moment that they pass i.e. the moment that they are in the same place . if a and b stay in their inertial frames they will never meet again and indeed will get further and further apart as time passes . the only way the twins will ever meet again is if they change inertial frames i.e. if one of them accelerates . in sr acceleration is absolute . by this i mean that velocity is relative i.e. you cannot tell whether a or b is the one moving , but it is always possible to tell which of the two is accelerating . the acceleration always introduces an assymetry so it is no surprise that when they meet again a and b will find their clocks differ . you can treat acceleration in special relativity . see for example my answer to how do i adjust the kinematic equations to avoid reaching speeds faster than light ? where i give ( some of ) the equations for understanding the motion of an accelerating rocket . if you do the calculation you will find that b sees a clock running fast while b is accelerating between inertial frames . see my answer to why isn&#39 ; t the symmetric twin paradox a paradox ? for more on this . the question of what carl sagan 's neutrino sees is quite a subtle one . suppose some particle interaction shortly after the big bang and 13.7 billion years away produced a neutrino and that neutrino has just passed you . for the neutrino only a few seconds has passed since the big bang . however when the neutrino passes you it sees you at your current age , 13.7 billion years , so what is going on ? the answer is that in the neutrino 's frame and your frame the big bang happened at different times . so the neutrino can see the 13.7 billion age of the big bang pass in a few seconds , but not because it sees the universe 's time running fast . it sees each successive bit of the universe as older because in each bit of the universe it passes through the big bang gets further and further back in time .
well you could replace the electrons with something else , such as muons . that is essentially what is proposed for this , although in extremely small quantities . muons in this case replace some electrons , but very very few compared to the total : http://en.wikipedia.org/wiki/muon-catalyzed_fusion obviously , this is fraught with difficulties because there is a good reason we live in the world defined by electrons . you would have to scale up this idea beyond the fusion proposal dramatically , and even if you did , the fact that it can induce fusion is a problem . . . well , a big problem . maybe you could replace the electrons with something other than muons , that is not as heavy . but probably not . if you are interested in how nuclei would interact with each other , you could consider a neutron star . that is rather like a big nucleus , but it is held together by gravity . it still does not have the electrons stripped out . as i understand , the electrons are still mixed in the mass . let 's say you just took ordinary matter and removed the electrons . let 's look at a tennis ball sized thing , with a diameter of $6.7 cm$ . the nuclei are now repelling each other . how much ? let 's calculate the energy , with the electric analog to the gravitational binding energy . $$ u = \frac{3gm^2}{5r} $$ here : $$ u = \frac{ 3 k q^2 }{5 r} $$ we can very simply find q from the nuclei alone . if we assume the ball is the density of water and solid ( unlike the tennis ball ) then we find it weighs about $1.25 kg$ . let 's assume it is carbon-ish in terms of the formula mass . divide by the formula mass , multiply by the electron charge ( which is the same as the proton charge ) , and i find about 10.1 million coulombs . here is the total calculation string in google , but i will not take the time to prettify it right now . ( 8.987551e9 n*m^2/c^2 ) 3 ( ( 1.0 g/cm^3 ) *4/3*pi* ( 6.7 cm ) ^3 * ( electron charge ) / ( 12 amu ) ) ^2/ ( 5*6.7 cm ) this comes out to $8.258 \times 10^{24} j$ . this is about 2.0 petatons of tnt equivalent . the tsar bomba was 100 megatons tnt equivalent . the kt extinction event was 100 million megatons of tnt , or 100 teratons of tnt . a ball stripped of electrons would have so much energy it would possibly destroy life on earth as we know it . that is how the nuclei would interact with each other .
the power going into an empty microwave will accumulate as an em field inside the cavity up to the point where the input power equals the rate of leakage . energy inside the cavity will leak out through the little holes in the metal screen in the door , through ohmic losses in the sided of the microwave , feedback into the power source , etc . the rate of power leakage from the cavity is given by $p_{leak} = 2\pi e f / q$ where $e$ is the energy stored in the cavity , $f$ is the resonant frequency of the cavity and q is the " quality factor " . in equilibrium then , $e = qp_{input}/{2\pi f}$ . for an resonant frequency of around 2.5 ghz , input power of 1kw , q of say 100 ( guess ) , the accumulated energy is only about a milli-joule . of course , the materials making up the microwave probably are not designed to dissipate 1kw of power over an extended time , so eventually the microwave may explode or something due to overheating of components .
there are a lot of different ways to get quantum field theories that look like the standard model in string theory . in some string theory models ( such as the heterotic models ) , every particle that the standard model treats as point-like ( electrons , quarks , etc ) is a single elementary string . but there are other more complicated models in which the standard model particles are not built out of strings at all , but instead realized as the low energy excitations of d-branes wrapped around various kinds of singularities . we do not know which ( if any ) of these models is actually correct , so we can not say with certainty that string theory predicts that an electron is made up of some number n of strings .
i am also surprised that the answerer apparently claims that the part before the equals sign , without taking the real part is equal to the part after the equals sign , with taking the real part . why is this ? the statement $$\tilde{\gamma} ( t ) = \int_{-\infty}^\infty e^{2\pi ift} g ( f ) \ , df=2\ , \mathrm{re}\left ( \int_{f_\text{min}}^{f_\text{max}} e^{2\pi ift}g ( f ) \ , df\right ) $$ is in fact correct , and follows from symmetry . the function $g ( f ) $ is symmetric with respect to inversion , ie $g ( -f ) =g ( f ) $ , since the power contained in negative frequencies should be the same as the power in positive frequencies . in practice it is implicit that your experimental data spectrum $g ( f ) $ is taken over a positive frequency range , ie $0&lt ; f_\text{min}&lt ; f_\text{max}$ . so $$\tilde{\gamma} ( t ) = \int_{-\infty}^\infty e^{2\pi ift} g ( f ) \ , df =\int_{-f_\text{max}}^{-f_\text{min}}e^{2\pi ift} g ( f ) \ , df+\int_{f_\text{min}}^{f_\text{max}}e^{2\pi ift} g ( f ) \ , df \\ =\int_{f_\text{min}}^{f_\text{max}}e^{-2\pi ift} g ( -f ) \ , df+\int_{f_\text{min}}^{f_\text{max}}e^{2\pi ift} g ( f ) \ , df \\ =2\ , \mathrm{re}\left ( \int_{f_\text{min}}^{f_\text{max}} e^{2\pi ift}g ( f ) \ , df\right ) . $$
yes , the statement holds true in general relativity as well . however , as we need to deal with tensors of higher and in particular mixed order , the rules of matrix multiplication ( which is where the idea of the representation via row- and column-vectors comes from ) are no longer sufficiently powerful : instead , the placement of the index determines if we are dealing with a contravariant ( upper index ) or a covariant ( lower index ) quantity . additionally , by convention an index which occurs in a product in both upper and lower position gets contracted , and equations must hold for all values of free indices . if the given metric is non-euclidean ( which is already true in special relativity ) , mapping between co- and contravariant quantities is more involved than simple transposition and the actual values of the components in a given basis can change , eg : $$ p^\mu = ( p^0 , +\vec p ) \\ p_\mu = ( p^0 , -\vec p ) $$ and in general : $$ p_\mu = g_{\mu\nu}p^\nu $$ where $g_{\mu\nu}$ denotes the metric tensor and a sum $\nu=1\dots n$ is implied .
i think you are mixing up two different things . namely : on the one hand , you can see qm as 0+1 ( one temporal dimension ) qft , where the position operators ( and their conjugate momenta ) in the heisenberg picture plays the role of the fields ( and their conjugate momenta ) in the qft . you can check , for instance , that spatial rotational symmetry in the quantum mechanical theory is translated to an internal symmetry in the qft . on the other hand , you can take the non-relativistic limit ( by the way , ugly name because galilean relativity is as relativistic as special relativity ) of the klein-gordon or dirac theory to get the schrödinger qft , where $\phi$ ( in your notation ) is a quantum field instead of a wave function . there is a chapter in srednicki 's book where this issue is raised in a simple and nice way . there , you can also read about spin-stastistic and the wave function of multi-particle states . let me add some equations that hopefully clarify ( i am using your notation and of course can be wrong factors , units , etc . ) : the quantum field is : $$\phi \sim \int d^3p \ , a_p e^{-i ( p^2/ ( 2m ) \cdot t - p \cdot x ) }$$ the hamiltonian is : $$h \sim i\int d^3x \left ( \phi^{\dagger}\partial_t \phi - \frac{1}{2m}\partial _i \phi ^{\dagger} \partial ^i \phi \right ) \ , \sim \int d^3p \ , \frac{p^2}{2m} \ , a^{\dagger}_p a_p$$ the evolution of the quantum field is given by : $$i\partial _t \phi \sim [ \phi , h ] \sim -\frac{\nabla ^2 \phi}{2m}$$ 1-particle states are given by : $$|1p\rangle \sim \int d^3p \ , \tilde f ( t , p ) \ , a^{\dagger}_p \ , |0\rangle $$ ( one can analogously define multi-particle states ) this state verifies the schrödinger equation : $$h \ , |1p\rangle=i\partial _t \ , |1p\rangle$$ iff $$i\partial _t \ , f ( t , x ) \sim -\frac{\nabla ^2 f ( t , x ) }{2m}$$ where $f ( t , x ) $ is the spatial fourier transformed of $\tilde f ( t , p ) $ . $f ( t , x ) $ is a wave function , while $\phi ( t , x ) $ is a quantum field . this is the free theory , one can add interaction in a similar way .
the theorem is called the noiseless coding theorem , and it is often proven in clunky ways in information theory books . the point of the theorem is to calculate the minimum number of bits per variable you need to encode the values of n identical random variables chosen from $1 . . . k$ whose probabilities of having a value $i$ between $1$ and $k$ is $p_i$ . the minimum number of bits you need on average per variable in the large n limit is defined to be the information in the random variable . it is the minimum number of bits of information per variable you need to record in a computer so as to remember the values of the n copies with perfect fidelity . if the variables are uniformly distributed , the answer is obvious : there are $k^n$ possiblities for n throws , and $2^{cn}$ possiblities for $cn$ bits , so $c=\log_2 ( k ) $ for large n . any less than cn bits , and you will not be able to encode the values of the random variables , because they are all equally likely . any more than this , you will have extra room . this is the information in a uniform random variable . for a general distribution , you can get the answer with a little bit of law of large numbers . if you have many copies of the random variable , the sum of the probabilities is equal to 1 , $$ p ( n_1 , n_2 , . . . , n_k ) = \prod_{j=1}^n p_{n_j}$$ this probability is dominated for large n by those configurations where the number of values of type i is equal to $np_i$ , since this is the mean number of the type i 's . so that the p value on any typical configuration is : $$ p ( n_1 , . . . , n_k ) = \prod_{i=1}^k p_i^{np_i} = e^{n\sum p_i \log ( p_i ) }$$ so for those possibilities where the probability is not extremely small , the probability is more or less constant and equal to the above value . the total number m ( n ) of these not-exceedingly unlikely possibilities is what is required to make the sum of probabilities equal to 1 . $$m ( n ) \propto e^{ - n \sum p_i \log ( p_i ) }$$ to encode which of the m ( n ) possiblities is realized in each n picks , you therefore need a number of bits b ( n ) which is enough to encode all these possibilities : $$2^{b ( n ) } \propto e^{ - n \sum p_i \log ( p_i ) }$$ which means that $${b ( n ) \over n} = - \sum p_i \log_2 ( p_i ) $$ and all subleading constants are washed out by the large n limit . this is the information , and the asymptotic equality above is the shannon noiseless coding theorem . to make it rigorous , all you need are some careful bounds on the large number estimates . replica coincidences there is another interpretation of the shannon entropy in terms of coincidences which is interesting . consider the probability that you pick two values of the random variable , and you get the same value twice : $$p_2 = \sum p_i^2$$ this is clearly an estimate of how many different values there are to select from . if you ask what is the probability that you get the same value k-times in k-throws , it is $$p_k = \sum p_i p_i^{k-1}$$ if you ask , what is the probability of a coincidence after $k=1+\epsilon$ throws , you get the shannon entropy . this is like the replica trick , so i think it is good to keep in mind . entropy from information to recover statistical mechanics from the shannon information , you are given : the values of the macroscopic conserved quantities ( or their thermodynamic conjugates ) , energy , momentum , angular momentum , charge , and particle number the macroscopic constraints ( or their thermodynaic conjugates ) volume , positions of macroscopic objects , etc . then the statistical distribution of the microscopic configuration is the maximum entropy distribution ( as little information known to you as possible ) on phase space satisfying the constraint that the quantities match the macroscopic quantities .
check out the following 3 articles and 2 books : regularization renormalization and dimensional analysis : dimensional regularization meets freshman e and m published in the american journal of physics ( can be found also on hep-ph , but slightly different with less references ) regularization , from murayama 's course of qft at berkeley a hint of renormalization a more general detailed , still introductory , treatment including renormalization would be the book renormalization methods : a guide for beginners a . zee 's book qft in a nutshell anyway , i hope that was useful revo
general relativity reduces to special relativity locally . what this means is that given an error tolerance $\varepsilon$ , you can find an extended region ( perhaps just a small one ) around any point in spacetime such that the laws of physics as tested only within that region match those of special relativity to within $\varepsilon$ . that means if you measure the speed of light , say with a ruler and stopwatch , doing the experiment right in front of you , then you will measure the speed of light to be the standard value . the same holds for anyone else anywhere else in the universe , as long as they also confine all their measurements to the same region . what confuses things is if you try to use a stopwatch in one place and a ruler in another . then the distance traveled by the photon divided by the time it takes to travel that distance can come out to be anything . meters at point $a$ are compatible with seconds at point $a$ , but not generally with seconds at point $b$ .
the car is behaving like a closed pipe , so you get a resonance set up . there is a wikipedia article here , but for once the wikipedia article is not that great , so there is another better article here . i imagine you ( like most of us ) will at some point have discovered you can make a sound by blowing across the top of an opened bottle , and it is the same thing happening in your car with the open window acting like the opening in the bottle . since your car is much bigger than a bottle the resonance frequency is uncomfortably low . when you open a second window you get an air current flowing through the car and this destroys the resonance . later : amazingly someone has actually published a paper about this . see http://www.me.iitb.ac.in/~fmfp/fmfp%20proc/am_13.pdf. some people have too much free time :- )
1 ) density matrix let us admit that the most general quantum description of a system is a " not normed " density matrix ( i mean a density matrix with a trace not necessarily equals to $1$ ) . the density matrix is a hermitian matrix , so in a $n$ dimensional space , it has $n$ real diagonal parameters and $\frac{n ( n-1 ) }{2}$ non-diagonal complex parameters , so the total is : $n + 2 * \frac{n ( n-1 ) }{2} = n^2$ real parameters . to describe a n-qubit , you have $n = 2^n$ . so you have $2^{2n}$ real parameters to describe the most general n-qbit density matrix . if you normalize the density matrix ( $tr ( \rho ) =1$ ) , you need only only $2^{2n}-1$ real parameters , so for $n=1$ , you will recover your $3$ parameters . 2 ) unitary transformations a unitary transformation ( $u ( n ) $ ) , in a $n$ dimensional space , requires $n^2$ real parameters , so in our case ( $n = 2^n$ ) , it requires $2^{2n}$ real parameters . however , mathematical states which differ only by a global phase correspond to the same physical state , so we may consider , that physically , we have only $2^{2n}-1$ real parameters , which correspond to special unitary transformations ( $su ( n ) $ ) . i must say i do not understand where you get your "$12$" . **update** 3 ) completely positive maps according to this paper , the dimension of completely positive maps seems to be $n^4-n^2$ . here , we have $n=2^n$ , so the dimension is $2^{4n}-2^{2n}$ . so for one qubit $ ( n=1 ; n=2 ) $ , we would have a dimension $2^4-2^2=16-4=12$ .
as you are a mathematician , i will just discuss the physics prerequwisites . newtonian mechanics . you seem to know this . lagrangian mechanics . unarguably the most elegant formulation of all of classical mechanics . hamiltonian mechanics . an uglier , but equally useful ( almost ) , formulation . newtonian gravity . the theory of gravity as an inverse square field with the " charge " as the mass . maxwellian electromagnetism . a lorentz invariant formulation of em . formulated way before lorentz invariance and lorentz symmetry were ever thought of . special relativity ( the minkowskian formulation , of course , is needed here . ) . of course . as you are a mathematician , you had probably like wald , r . m 's general relativity , a mathematically rigorous textbook . though i do not like it as it is too rigorous . i prefer ludvigsen 's general relativity : a geometric approach . but that is not a book a mathematician would like .
see for example topics in koopman-von neumann theory by d . mauro . this should be one of the most extensive overviews of kvn theory , it also contains some examples of applying this theory to some well known problems such as aharonov-bohm effect .
since an answer has already been posted using time dilation and $\gamma$ , here 's an alternative method employing the invariant interval . $$ ( c\tau ) ^2 = ( c \delta t ) ^2 - \delta x^2$$ as specified , the proper time for the pizza is $\tau = 120s$ the displacement of the pizza is $\delta x = 27 \cdot 10^6 km = 90$ light-seconds . solving for the elapsed coordinate time $\delta t$ yields $$\delta t = \sqrt{ \tau^2 + \frac{\delta x^2}{c^2}} = \sqrt{120^2 + 90^2} = 150s$$ thus , $$v = \frac{\delta x}{\delta t} = \frac{90}{150}c= \frac{3}{5}c$$
1 . kaluza-klein theory . this is similar to general relativity , but instead of three space dimensions plus time , there are four space dimensions plus time . the fourth dimension is cyclic , and satisfies some symmetry conditions . the electromagnetic potential appears as the components of the metric in the fourth space dimension . it is usually rejected on the grounds that we can not see the fourth space dimension , or that it is made too small to be seen . in fact , the symmetry conditions along this dimension make it indistinguishable , and moving along it is equivalent to a gauge transformation . so , this is the only evidence predicted by the theory , no matter how large we make the cyclic dimension . which leads us to 2 . gauge theory . as mentioned by dimension10 abhimanyu ps , electromagnetism can be described by a gauge theory whose gauge group is $u ( 1 ) $ ; the electromagnetic potential becomes a connection , and the electromagnetic field the curvature associated to the connection . it is in fact the symmetry group of the fourth dimension in kaluza-klein theory . for mathematicians , a gauge theory is described in terms of principal bundles , which , if the gauge group is $u ( 1 ) $ , are in fact 4+1 dimensional spaces , satisfying symmetry conditions like in the kaluza-klein theory . so , mathematically , they are equivalent , although there are variations of the kaluza-klein theory which cannot be described by a standard gauge theory . 3 . rainich-misner-wheeler theory . there is a way to obtain electromagnetism from geometry , in the 4d spacetime of general relativity . rainich was able to give in 1925 necessary and suficient conditions that spacetime is curved in a way which corresponds to the electromagnetic field . by einstein 's equation , the spacetime curvature is related to the field . so , rainich decided to see if one can obtain the electromagnetic field from the curvature , using einstein 's equation . he found some necessary and sufficient conditions for the ricci tensor , which are of algebraic and differential nature . this works for source free electromagnetism . there is an ambiguity , given by the hodge duality between the electric and the magnetic fields , for the source free maxwell equations . so , basically , the field is recovered up to a phase factor called complexion . the idea was rediscovered by misner and wheeler three decades later , who combined it with the wormholes of einstein and rosen . they interpreted the ends of the wormholes as pairs of electrically charged particles-antiparticles . the electromagnetic field , in this view , does not need a source , since the field lines go through the wormhole . while this idea may seem bizarre , it allowed to obtain " charge without charge " , and to fix the undetermined phase factor . this model of particles had some issues , for instance it could not explain the spin , and misner and wheeler abandoned it .
http://www.khanacademy.org/#physics scroll down till #145 , that is where optics starts . if you want just lenses , start with the ' virtual image ' one . if not for your online-only request , i would have suggested resnick-halliday-walker . imho , that is just about the best book for classical anything
i think it is a great question , and enjoyed it very much when i grappled with it myself . here 's a picture of some of the forces in this scenario . $^\dagger$ the ones that are the same colour as each other are pairs of equal magnitude , opposite direction forces from newton 's third law . ( w and r are of equal magnitude in opposite directions , but they are acting on the same object - that is newton 's first law in action . ) while $f_{matchbox}$ does press back on my finger with an equal magnitude to $f_{finger}$ , it is no match for $f_{muscles}$ ( even though i have not been to the gym in years ) . at the matchbox , the forward force from my finger overcomes the friction force from the table . each object has an imbalance of forces giving rise to acceleration leftwards . the point of the diagram is to make clear that the third law makes matched pairs of forces that act on different objects . equilibrium from newton 's first or second law is about the resultant force at a single object . $\dagger$ ( sorry that the finger does not actually touch the matchbox in the diagram . if it had , i would not have had space for the important safety notice on the matches . i would not want any children to be harmed because of a misplaced force arrow . come to think of it , the dagger on this footnote looks a bit sharp . )
first , you need to know the right ascension and declination of the sun on that day . then there are some angular relations connecting all of these . everything is available in the rasc observer 's handbook . another way would be to interpolate the table produced by this program provided by the us naval observatory : http://aa.usno.navy.mil/data/docs/altaz.php hope this helps !
using your definition of " falling , " heavier objects do fall faster , and here 's one way to justify it : consider the situation in the frame of reference of the center of mass of the two-body system ( cm of the earth and whatever you are dropping on it , for example ) . each object exerts a force on the other of $$f = \frac{g m_1 m_2}{r^2}$$ where $r = x_2 - x_1$ ( assuming $x_2 &gt ; x_1$ ) is the separation distance . so for object 1 , you have $$\frac{g m_1 m_2}{r^2} = m_1\ddot{x}_1$$ and for object 2 , $$\frac{g m_1 m_2}{r^2} = -m_2\ddot{x}_2$$ since object 2 is to the right , it gets pulled to the left , in the negative direction . canceling common factors and adding these up , you get $$\frac{g ( m_1 + m_2 ) }{r^2} = -\ddot{r}$$ so it is clear that when the total mass is larger , the magnitude of the acceleration is larger , meaning that it will take less time for the objects to come together . if you want to see this mathematically , multiply both sides of the equation by $\dot{r}\mathrm{d}t$ to get $$\frac{g ( m_1 + m_2 ) }{r^2}\mathrm{d}r = -\dot{r}\mathrm{d}\dot{r}$$ and integrate , $$g ( m_1 + m_2 ) \left ( \frac{1}{r} - \frac{1}{r_i}\right ) = \frac{\dot{r}^2 - \dot{r}_i^2}{2}$$ assuming $\dot{r}_i = 0$ ( the objects start from relative rest ) , you can rearrange this to $$\sqrt{2g ( m_1 + m_2 ) }\ \mathrm{d}t = -\sqrt{\frac{r_i r}{r_i - r}}\mathrm{d}r$$ where i have chosen the negative square root because $\dot{r} &lt ; 0$ , and integrate it again to find $$t = \frac{1}{\sqrt{2g ( m_1 + m_2 ) }}\biggl ( \sqrt{r_i r_f ( r_i - r_f ) } + r_i^{3/2}\cos^{-1}\sqrt{\frac{r_f}{r_i}}\biggr ) $$ where $r_f$ is the final center-to-center separation distance . notice that $t$ is inversely proportional to the total mass , so larger mass translates into a lower collision time . in the case of something like the earth and a bowling ball , one of the masses is much larger , $m_1 \gg m_2$ . so you can approximate the mass dependence of $t$ using a taylor series , $$\frac{1}{\sqrt{2g ( m_1 + m_2 ) }} = \frac{1}{\sqrt{2gm_1}}\biggl ( 1 - \frac{1}{2}\frac{m_2}{m_1} + \cdots\biggr ) $$ the leading term is completely independent of $m_2$ ( mass of the bowling ball or whatever ) , and this is why we can say , to a leading order approximation , that all objects fall at the same rate on the earth 's surface . for typical objects that might be dropped , the first correction term has a magnitude of a few kilograms divided by the mass of the earth , which works out to $10^{-24}$ . so the inaccuracy introduced by ignoring the motion of the earth is roughly one part in a trillion trillion , far beyond the sensitivity of any measuring device that exists ( or can even be imagined ) today .
in the 19th century , the physicists young and helmholtz proposed a trichromatic theory of color , in which the eye was modeled as three filters with overlapping ranges . this is essentially a physical model of the pigments in the eye , and it predicts the response of the nerve cells at the retina . helmholtz did related work on sound and timbre . ca . 1950 , hering , hurvich , and jameson proposed significant modifications to the trichromatic theory , called opponent processing . this models a later stage in the processing of the signals , after the retinal response but before the more sophisticated stages of processing in the brain . both the trichromatic model and opponent processing are needed in order to describe certain phenomena in human color perception . the complete theory can be modeled by two functions depending on wavelength . i will call these $rg ( \lambda ) $ and $by ( \lambda ) $ . these functions are drawn here . they both oscillate between positive and negative values . for any given pure wavelength $\lambda$ , the net result of pigment-filtering plus the later neurological processing produces these two numbers , which can be thought of as the final signals that go on to later processing in the brain . i am calling them $rg$ and $by$ for the following reasons . let 's pretend , for the sake of simplicity , that these functions oscillated between -1 and +1 . then the pair $ ( rg , by ) = ( 1,0 ) $ produces the sensation of red , ( -1,0 ) is green , ( 0,1 ) is blue , and ( 0 , -1 ) is yellow . there is various psychological evidence for this model , e.g. , no color is perceived as reddish-green or yellowish-blue . roughly speaking , what seems to be happening is that the eye-brain system is taking differences between signal levels of different cone cells . this sort of makes sense because , for example , the red and green pigments have response curves that overlap a lot , so if you want to place a pure-wavelength color on the spectrum , the difference between them is more a more direct measure of what you want to know than the individual signals . the $rg$ function actually has two different peaks , one at the red end of the spectrum and one , surprisingly , at the blue end . this implies that by mixing blue and red , you can produce an $ ( rg , by ) $ pair similar to what you would have gotten with monochromatic violet . if you look at other sources , e.g. , this one ( figure 3.3 ) , they seem to agree on the secondary short-wavelength peak of the $rg$ function , but the details of how the two functions are drawn at the short wavelengths are different and seem to make for a less convincing explanation of the observed perceptual similarity between violet and a red-blue mixture . i do not know if there is a valid reductionist explanation of the short-wavelength peak of the $rg$ function . like a lot of things produced by evolution , it may basically be an accident that got frozen in . however , it is possible that it serves the evolutionary purpose of helping us to distinguish different shades of blue and violet . if the $rg$ function was simply zero over the whole short-wavelength end of the spectrum , then the $by$ function would be the only information we had get for those wavelengths . but the $by$ function has a maximum , simply because the eye 's sensitivity to light fades out as you get into the uv . near this maximum , the ability of the $by$ function to discriminate between colors becomes zero . in the york university graph , it appears that the short-wavelength extrema of the $rg$ and $by$ functions are offset from one another , which would allow some color discrimination in this region . the physical information being preserved by the $by$ function would then be the difference in response between the blue and green cones . but the briggs graphs do not appear to show any such offset of the extrema , so it is possible that the explanation i am giving is a bogus " just-so story . " there may be a good analogy here with sound . the sound spectrum is linear , but there is a psychological phenomenon of octave identification , which makes the spectrum " wrap around , " so that frequencies $f$ and $2f$ are perceptually similar and can often be mistaken for one another even by trained musicians . similarly , the predictive power of the " color wheel " model shows that to some approximation we can think of the trichromatic/opponent process model as resulting in a wrapping around of the visible segment of the em spectrum into a circle . but in both cases , the wrap-around is only an approximation . in terms of pitch , $f$ and $2f$ are perceptually similar but not indistinguishable . for color , we have the 1976 cieluv color color diagram , which is a modification of the 1931 diagram meant to represent at least somewhat accurately the degree of perceptual similarity between different points based on the distance between them . the monochromatic spectrum constitutes part of the outer boundary of this diagram , and is more of a " v " than a circle ; there is quite a large gap between monochromatic violet and monochromatic red . it is trivially true that any such diagram has a boundary that is a closed curve . if the diagram is not constrained to give any accurate depiction of the sizes of the perceptual differences between colors , then it can be distorted arbitrarily , and we can arbitrarily define it such that its boundary is a circle . in this sense , the success of the color wheel model is guaranteed , and it follows from nothing more than the fact that humans are trichromats , so that the color space is three-dimensional , and controlling for luminance produces a two-dimensional space . but this fails to explain why there is some degree of perceptual similarity between the red and violet ends of the monochromatic spectrum ; for that you need the opponent processing model . there is also a slight variation in the absorbance of the pigment in the red cones at the blue end of the spectrum . i do not think this is sufficient to explain the perceptual similarity between violet and red , or the even closer similarity between violet and a mixture of red and blue light , i.e. , i do not think you can explain these facts using only the trichromatic theory without opponent processing . the classic direct measurements of the filter curves of cone-cell pigments were done with cone cells from carp by tomita ca . 1965 , but afaik the only direct measurement using human cone cells was bowmaker 1981 . bowmaker 's red-cell absorbance curve has a very slight rise at short wavelengths , but it is not very pronounced at all . you will see various other curves on the internet , often without any attribution or explanation of where they came from , and some of these show a much more pronounced bump rather than bowmaker 's slight rise . possibly some of these are from people using the cie 1931 curves , which were never intended to be physical models of the actual human cone-cell pigments . it should be clear , however , that the red and green pigments ' curves must have some variation near the violet end of the spectrum . if they did not , then the dimensionality of the color space would be reduced there , and the human eye would be unable to distinguish different wavelengths in this region , which is contrary to fact . bowmaker , " visual pigments and colour vision in man and monkeys , " j r soc med . 1981 may ; 74 ( 5 ) : 348 , freely accessible at http://www.ncbi.nlm.nih.gov/pmc/articles/pmc1438839/
electric potential is a potential energy just like gravitational potential energy or indeed any other form of potential energy . specifically , moving one coulomb of charge through an electrical potential of one volt produces ( or requires ) 1 joule of energy . from your question i guess you are basically happy with this , so the question is really how this energy is dissipated i.e. what happens to that 1 joule of energy ? when you apply a voltage to the conductor you produce a force on the conduction electrons so they accelerate - the potential energy is turned into kinetic energy of the electrons . however conductors are made up from a crystal lattice of atoms/molecules that is randomly vibrating due to thermal energy , and there is a probability that the moving electrons will scatter off this lattice and transfer energy to it . so the electron is slowed down and the magnitude of the lattice vibrations is increased . increased lattice vibrations mean the conductor is hotter , so the kinetic energy of the electrons has been transferred into thermal energy in the conductor . and that is what happens to the 1 joule of energy . it is transferred to the conductor and ends up as heat . some related issues you might want to look into further : when you cool a conductor you reduce the magnitude of the lattice vibrations and you make it less likely the electron will scatter off the lattice . that is why resistance ( usually ) decreases with decreasing temperature . the superconducting transition prevents electrons from scattering off the lattice , so they can not transfer energy to it and that is why superconductors have a resistance of zero .
the increased ' resistance ' of an underinflated tyre is due to mechanical deformation , friction is independent of area as suggested . the simplest explanation for me is that : as area increases the applied force per unit area decreases , but there is more contact surface to resist motion . added as per zass ' suggestion below : $$\rm{friction}= \rm{material\ coefficient} \times \rm{pressure} \times \rm{contact area}$$ where the material coefficient is a measure of the ' grippiness ' of the material , the pressure applied to the surface and the area of the surfaces in contact . so we can see the area in the pressure term cancels with the third term . this is not to be confused with traction , where spreading the motive force over a larger area can help .
well , the information does not have to escape from inside the horizon , because it is not inside . the information is on the horizon . one way to see that is from the fact that from the perspective of an observer outside the horizon of a black hole , nothing ever crosses the horizon . it asymptotically gets to the horizon in infinite time ( as it is measured from the perspective of an observer at infinity ) . an other way to see that is the fact that from the boundary conditions on the horizon you can get all the information you need to describe the space-time outside but that is something more technical . finally , since classical gr is a geometrical theory and not a quantum field theory* , gravitons is not the appropriate way to describe it . *to clarify this point , gr can admit a description in the framework of gauge theories like the theory of electromagnetism . but even though electromagnetism can admit a second quantization ( and be described as a qft ) , gr can not .
you are almost there . for the symmetry argument : first notice that faraday 's law , $\oint\textbf{e} \cdot d\textbf{l}=-\frac{d\phi}{dt}$ , looks the same as ampère 's law from electrostatics : $\oint\textbf{b}\cdot d\textbf{l}=\mu_0 i$ . now consider a current ( or a homogeneous current density ) pointing in the positive $x_3$-direction . what is the direction of the magnetic field such a current would produce ? indeed , using the right-hand rule ( or any of your favorite symmetry arguments ) it readily follows that the $\textbf{b}$-field encircles the current , i.e. it is symmetric around the $x_3$-axis and points anticlockwise . i trust that you are familiar with symmetries of magnetic fields produced by steady currents . now compare the form of faraday 's and ampère 's laws . because the laws look exactly the same , it is easy to see that the electric field due to a flux decrease in the $x_3$-direction will have the same symmetry as a magnetic field due to a current ( density ) in the $x_3$ -direction . hence here the $\bf{e}$-field will also be symmetric around the $x_3$-axis and will point in the azimuthal direction ! ( it'll point clockwise if the flux increases , but this will follow from the calculation . ) therefore , we can do the calculation in just the way you did , yielding $\textbf{e}=-\frac{r}{2}\frac{db}{dt}\hat{\phi}$ as you noted . ( here $\textbf{b} ( t ) =b ( t ) \hat{x}_3$ . ) note that in your calculation you had already assumed that $\textbf{e}$ is in the $\hat{\phi}$ direction when you said that $2\pi r |\vec{e}| = \int_{\gamma} \vec{e} d\vec{s}$ , since this assumes that $\bf{e}$ and $d\bf{s}$ are parallel . the last thing to notice is that your calculation holds when your contour $\gamma$ is in the circle $a$ where the flux changes . let $r$ be the radius of $a$ . if $\gamma$ is outside $a$ , then it encloses all of the flux , hence $\frac{d\phi}{dt}=\pi r^2\frac{db}{dt}$ so that for $r&gt ; r$ we get $\textbf{e}=-\frac{r^2}{2r}\frac{db}{dt}\hat{\phi}$ , which nicely vanishes as $r\rightarrow \infty$ . finally , notice that if we would calculate the magnetic field produced by a volume charge density $\textbf{j}=j_0\hat{x}_3$ with $j_0$ constant , and replaced in our answer $j_0$ by $-\frac{1}{\mu_0}\frac{db}{dt}$ , we had get exactly the electric field above .
in quantum field theories it is believed that anomalies in gauge symmetries ( in contrast to rigid symmetries ) cannot be coped with and must be canceled at the level of the elementary fields . may be the earliest work on the subject is : c . bouchiat , j . iliopoulos and p . meyer , “an anomaly free version of weinberg’s model” phys . lett . b38 , 519 ( 1972 ) . but certainly , one of the most famous ones is the gross-jakiw article : effect of anomalies on quasi-renormalizable theories phys . rev . d 6 , 477–493 ( 1972 ) they agrgued that the ' thooft-veltman perturbative proof of the renormalizability of gauge theories requires the anomalous currents not to be coupled to gauge fields . in the more modern brst quantization language , gauge anomalies give rise to anomalous terms in the slavnov-taylor identities which cannot be canceled by local counter-terms therefore ruin the combinatorial proof of perturbative renormalizability and of the decoupling of the gauge components and ghosts which results a non-unitary s-matrix . this phenomenon is independent of the quantum field theory dimension . in 3+1 dimension , the requirement of cancelation of the chirally coupled standard model leads to the correct particle content . in 1+1 dimensions , it leads to the dimension of string target space ( virasoro anomaly ) and the optional anomaly free gauged subgroups in the wznw models in two dimensions ( kac-moody anomaly ) . in certain dimensions there is the green-schwrz cancellation mechanism , but it is equivalent to an addition of a local term to the lagrangian . this mechanism is not appropriate to 3+1 dimensions , since this term is not renormalizable . the chiral anomaly can be corrected by adding a wess-zumino term to the lagrangian , but this term is not perturbatively renormalizable , thus does not solve the nonrenormalizability problem . however , the cancellation of anomalies does not mean that we should not seek for representations of the " anomalous " current algebras . at the contrary , in 1+1 dimensions the virasora and the kac-moody algebra do not have positive energy highest weight representations unless they are not anomalous . according to this prionciple , the spectra of each sector of the theory are determined by the anomaly of this sector , in spite of the fact that the total anomaly vanishes . in the late eighties , r.g. rajeev and especially juoko mickelsson started a project in which they seeked representations of anomalous non-centrally extended algebras . ( i.e. . , those present in 3+1 dimensions ) . the presence of gauge field depended abelian extensions ( the mickelsson-faddeev extension ) , made this problem hard to solve . one route they adopted was to consider an algebraic universal gauge theory ( in the same sense as universal classifying spaces ) . but a no-go result by doug pickrell stated that the universal anomalous algebra has no nontrivial unitary representations . this result is valid for the universal model but it is discouraging for the actual ( mickelsson-faddeev ) extended algebra . there are later works on the subject by juoko mickelsson himself and also by edwin langmann ; but the question remains open . update : regarding the question about the validity of an anomalously gauged theory as an effective gauge theory ( a la weinberg , let us consider the skyrme model for definiteness ) : both are strictly perturbatively non-renormalizable , but there is a big difference in their divergence behavior : an anomalously gauged theory requires , in each order in its loop expansion counter-terms , with arbitrarily high derivatives ( or arbitrarily high external momenta ) . in contrast , due to the chiral perturbation theory of an effective field theory each order of the loop expansion requires only counter-terms with one order higher ( by 2 ) in external momenta . chiral perturbation theory is unitary order by order in the momentum expansion . the bound on the powers of the momenta at each order makes it viable as a low energy effective theory . the current green functions respect non-anomalous ward identities , which control the divergences . in an anomalously gauged theory a truncation of the higher momentum power counter-terms would result s-matrix non-unitarity . this is the main reason , why it is widely accepted that an anomalously gauged theory requires additional fields to cancel the anomalies . ( this principle came into experimental demonstration in the prediction of the t-quark ) . in regards of drake 's remark ; there is the case of the chiral schwinger model in two dimensions ( 2d-qed with chiral coupling of the electron to the photon ) . this model is exactly solvable . in the exact solution , the gauge boson acquires a mass which is an indication consistent with drake 's remark that more " degrees of freedom are needed " . there is the " problem " that drake pointed out , of how the " number of degrees of freedom " jumps between the ( supposedly ) non-anomalous classical theory and the quantum theory . my , point of view is that the space of the grassmann variables describing the fermions ( semi- ) classically is not an appropriate phase space , although it is a symplectic supermanifold ( poisson brackets can be defined between grasmann variables ) . this difficulty was already encountered by : berezin and marinov , when they noticed that they cannot define a nontrivial grassmann algebra valued phase space distribution , so , they stated that grassmann variables acquire a meaning only after quantization . one way to define a fermionic phase space on which a density of states can be defined is to pick the " manifold of initial data " as in the bosonic case . in finite systems , these types of manifolds turn out to be coadjoint orbits , however i do not know of any work in which the anomaly is derived classically on " the manifold of initial data " of a fermionic field . i think that on these " honest " fermionic phase spaces , chiral anomalies are present classically ; however , i do not know of any work in this direction . i consider this a very interesting problem . a remark on the non-uniqueness of the mechanism of anomaly cancellation : we can cancel anomalies by adding a new family of fermions , various wess-zumino terms ( corresponding to different anomaly free subgroups ) , and may be masses to the gauge fields ( as in the schwinger model ) . this non-uniqueness , reflects the fact that when anomaly is present , the quantization is not unique , ( in other words the theory is not completely defined ) . this phenomenon is known in many cases in quantum mechanics ( inequivalent quantizations of a particle on a circle ) , and quantum field theory ( theta vacua ) . finally , my point of view is that the anomaly cancellation does not dismiss the need of finding " representations " to the anomalous current algebras in each sector . this principle works in 1+1 dimensions . it should work in any dimension because according to wigner quantum theory deals with representations of algebras . this is why i think that mickelsson 's project is important . may be for higher dimensions , more general representations than representations on hilbert spaces are needed due to the non-central extensions present in these dimensions . for me , this problem is very interesting .
a very brief answer : the fourier transform as used in quantum physics interprets a point in one set of three coordinates as a broad spectrum in another set of three coordinates , and vice-versa . this is the uncertainty relationship , since it means that when the object is point-like in one set of coordinates ( space , for example ) it become vast and diffuse in the other ( momentum , for example ) . to make that a bit more precise you need to stop thinking of fourier transforms as flat sinusoidal graphs , and instead think of them as having complex values -- that is , as having phase values in a plane ( two more axes really ) that is orthogonal to the other three . in this form , an object that is stretched out along one axis -- say x -- looks more like a stretched-out slinky if you add the two complex coordinates and move along x . the various continuity equations of physics simply say that the coils of any such slinky like to stay as smooth and unkinked as possible . ( the slinkies also rotate around their axis based on how massive the object is , but that is a different issue ) . if such a slinky is infinitely long in x and perfectly smooth at some frequency of coiling , it represents a particle that whose wave function is infinitely long in x , and whose probability of finding it at any one spot is essentially zero . that is position uncertainty , the worst possible case of it . but it is the frequency of coiling that is more important for uncertainty , because that frequency just happens to represent the momentum of the particle parallel to the x axis . if the coiling is perfectly regular , there is only one such frequency , and you could map that on frequency out ( radio-dial style ) into another triplet of axes labeled $p_x$ , $p_y$ , and $p_z$ . that is momentum space , the other side of the fourier coin . in that space , the particle has a very precise location indeed , that being the location of that particular frequency along the momentum axis $p_x$ . but what if you try it the other way around ? that is , what if you construct a similar helix using the same rules of adding a complex plane perpendicular to $p_x$ and creating a very long , very regular coil there . ( there is a beautiful symmetry in physics here , because it turns out that momentum space has continuity and smoothness rules very much like those of regular xyz space , despite some different meanings of them . ) well , pretty much exactly the same thing happens , only in reverse : the long , precise coil in momentum space also has a precise frequency , which also has an interpretation over in xyz space as a precise location . and again , having the coil stretched out in momentum space means that it is hard to find the particle there ; it could with equal likelihood be almost anywhere along the coil , meaning it could have almost any momentum . so , in this case , being " uncertain " about the location of the particle in momentum space means that it has a very precise location in regular space . the symmetry is simply gorgeous , and has many real implications in physics . metals , for example , are substances in which pairs of electrons exist more in momentum space than in regular space , causing some of them to have very high momenta ( energies ) and all of them to be distributed rather oddly over a metal crystal . the high energy ones make the metal reflective , the movement ( momenta ) of the electrons make it conductive , and the delocalization of the electrons combine with charge cancellation to create rather remarkable properties such as tensile strength ( it sticks together tightly ! ) and ductility ( it is easier to bend when half the charges are always in motion ) . but while the fourier part of this symmetry between the xyz and momentum spaces is exact , the meanings of frequencies in the two spaces is anything but . the biggest difference is that while the size of an object in xyz space has no huge energy implications , size in momentum space has huge implications , because higher momentum means higher energy . so , the longer a slinky ( more properly called a wave function ) becomes in momentum space , the higher in energy the particle becomes . and , since the length of the slinky in momentum space determines through the fourier transform how precisely it can be positioned in regular xyz space , extremely precise positions come at a cost -- a very high cost . the stanford linear accelerator , for example , has to accelerate electrons to extremely high velocities ( momentums ) because it is the only way to make electron locations precise enough to probe the innards of particles such as neutrons and protons . oddly , no such cost is accrued when particles become lost in regular xyz space . so , an electron wandering through the cosmos can in principle be represented by a very large slinky with a very precise frequency , if something ( e . g . diffraction ) encourages it to form that way . but even though the energy cost is low , such wave functions are unstable for a very different reason : any kind of information-imparting interaction between them and other matter causes the wave function to be irrelevant ( yes , i am trying to avoid getting into schools of quantum doctrine with that careful wording ) and for all practical purpose a vastly smaller wave function . just not too small , since only the energy of the interaction is available to alter the original diffuse form . and again , to bring all that back home : it is the fourier transform and its ability to say how well each frequency " matches " a slinky-form in either of the spaces that creates this interpretation back-and-forth between the two , and thereby creates quantum uncertainty .
bernoulli 's equation is frame-dependent as the following paper shows it in a nice way the bernoulli equation in a moving reference frame pdf here the essence of the argument is to realize that in a frame where the obstacles , around which the fluid moves , are not stationary , these surfaces do non-zero work . and one must account for this work done when using the bernoulli equation . a better way is to look at the generalized bernoulli equation as done here , which also covers viscous fluids .
less than g . since it generates energy in the conducting ring when passing through the ring , by conservation of energy , the magnet loses some kinetic energy ( there are forces acting on the magnet . see lenz 's law .
let me answer your first question : phase transition do not necessarily imply a symmetry breaking . this is clear in the example your are mentioning : the liquid-gas transition is characterized by a first order phase transition but there is no symmetry breaking . indeed , liquid and gas share the same symmetry ( translation and rotation invariance ) and may be continuously connected in the high temperature/pressure regime . in quantum systems at zero-temperature , one may also encounter transition in between quantum spin-liquid states for which there is also no symmetry breaking . yet another example is the case of the 2d xy model where there is a continuous phase transition but there is no symmetry breaking ( kosterlitz-thouless transition ) .
in a consistent theory of gravity , there can not exist any objects that can shield the gravitational field in the same way as conductors shield the electric field . it follows from the positive-energy theorems and/or energy conditions ( roughly saying that the energy density cannot be negative ) . to see why , just use the conductor to shield an ordinary electric field - which is what your problem reduces to temporarily for very low frequencies of the electromagnetic waves . the basic courses of electromagnetism allow one to calculate the electric field of a point-like charged source and a planar conductor : the electric field is identical to the original charge plus a " mirror charge " on the opposite side from the conductor 's boundary . importantly , the mirror charge has the opposite sign . in this way , one may guarantee that the electric field is transverse to the plane of the conductor . this fact makes the electromagnetic waves bounce off the mirror if you consider time-dependent fields . if you wanted to do an " analogous " thing for gravity , you would first have to decide what boundary conditions you want to be imposed for the gravitational field by the mysterious new object - what is your gravitational analogy of "$\mathbf e$ is orthogonal to the conductor " . the metric field has many more components . most of them will require you to consider a negative " mirror mass " - but the mass in a region can not be negative , otherwise the vacuum would be unstable ( one could produce regions of negative and positive energy in pairs out of vacuum , without violating any conservation laws ) . microscopically , one may also see why there can not be any counterpart of the conductor . the conductor allows to change the electric field discontinuously because it can support charges distributed over the boundary . the profile of the charge density goes like $\sigma\delta ( z ) $ if the conductor boundary sits at $z=0$ . however , you would need a similar singular mass distribution to construct a gravitational counterpart . if the distribution failed to be positively definite , it would violate the positive-energy theorem or energy conditions , if you wish . if it were positively definite , it would create a huge gravitational field . locally , the boundary of the gravitational " conductor " would have to look like an event horizon . but we know that the event horizons cannot shield the interior from the gravitational waves - the waves as well as everything else falls inside the black hole . moreover , the price for such " non-shielding " is that you have to be killed by the singularity in a finite proper time . one may probably write some formal solutions that have some properties but they can not really work when all the behavior of gravity and the related consistency conditions are taken into account . one can not fundamentally construct a " gravitational conductor " because gravity means that the space itself remains dynamical and this fact can not be undone . in particular , in a consistent theory of gravity - in string/m-theory - you will not find any objects generalizing conductors to gravity . by the way , there is one object that comes close to a " gravitational conductor " in m-theory - the hořava-witten domain wall , a possible boundary of the 11-dimensional spacetime of m-theory . it can be placed at $x_{10}=0$ and about $1/2$ of the components of the metric tensor become unphysical near this domain wall ( boundary of the world that carries the $e_8$ gauge multiplet ) because the domain wall also acts as an orientifold plane . but such an orientifold plane is not just some object ( like a conductor ) that is " inserted " into a pre-existing space that does not change . quite on the contrary , the character of the underlying spacetime is changed qualitatively : the world behind the orientifold plane is literally just a mirror copy of the world in front of the plane . so there is a lot of fundamentally incorrect thinking about gravity in all the answers that try to say " yes " . gravity is not another force that is inserted into a pre-existing geometrical space ; gravity is the curvature and dynamics of the spacetime itself . once we say that the space is dynamical , we can not find any objects that would " undo " this fundamental assumption of general relativity . there are also some more detailed confusions in the other solutions . first , a white hole corresponds to a time-reversed black hole with the same ( positive ) mass but it is an unphysical time reversal because it violates the second law of thermodynamics : entropy has to increase which means that the ( large entropy ) black hole can be formed , but it cannot be " unformed " . but a white hole , which is forbidden thermodynamically ( and microscopically , it corresponds to the same microstates as black hole microstates , they just never behave in any " white hole " way ) , is still something else than a negative-mass black hole . a negative mass black hole is not related to a positive-mass black hole by any " reflection " . it is a solution without horizons , with a naked singularity , and can not occur in a consistent theory of gravity because it would cause instability of the vacuum . ( also , naked singularities can not be " produced " by any generic evolution in 3+1 dimensions because of penrose 's cosmic censorship conjecture . ) so white holes and negative-mass black holes are forbidden for different reasons . however , even if you considered them , it would still fail to be enough to create a " gravitational conductor " which is nothing else than the denial of the fact that the geometry of the spacetime is dynamical , and one can not freely construct infinite , delta-function-like mass densities without completely changing the shape of spacetime .
this looks like an example of the tennis racket theorem . some axes of rotation for a rigid body are more stable than others . if the initial rotation axis does not correspond to one of the principal axes , a wobble can grow and cause the rotation axis to move to a principal axis . this is a result of euler 's equations of motion and the moments of inertia . the tennis racket theorem is a result in classical mechanics describing movement of a rigid body with three distinct angular momenta .
but then the speed of light is universal constant regardless of the motion of its frame of reference , so should not their relative velocity be $c$ ? what is their relative velocity and how ? edit upon further review ( special thanks to alfred 's comment ) , i think my original answer is incorrect . it turns out that the question of relative velocities of photons moving in the same direction is a meaningless question . the reason is as follows . for two objects A and B moving as such ,  v u -------&gt; A -------&gt; B ------------------ (ground)  the velocity of B in A 's frame is then $$ u'=\frac{u-v}{1-\frac{uv}{c^2}} $$ notice the denominator ? for $u=v=c$ , this is zero and we get 0/0 which is an undefined operation , hence the meaningless question . do objects moving at the speed of light obey law of addition of velocities ? not exactly . the galilean velocity addition , $s=u+v$ does not hold for large-velocity objects . we use the " composition law " , $$ s=\frac{u\pm v}{1\pm\frac{uv}{c^2}} $$ where $\pm$ depends on directions/frames . if $uv\ll c^2$ , then this does reduce to the galilean transformation .
when you write $\omega ( x , y ) $ this gives the impression that $\omega$ is a 2-form ! it is $d\omega$ that is a 2-form , if $\omega$ is a 1-form . the action of the 1-form $\omega =\omega_\mu dx^\mu$ on the vector field $x = x^\mu \partial_\mu$ is simply $$\omega ( x ) = \omega_\mu x^\mu . $$ ( since you use index notation , you probably know about relativity and this should be familiar : the contraction of a contravariant and a covariant vector is exactly a 1-form eating a vector field ! ) now you can always write $\omega = \omega_\mu dx^\mu$ . since $d ( dx ) = 0$ ( this is part of the definition of $d$ ) , it must be that $$d\omega = ( d\omega_\mu ) \wedge dx^\mu . $$ but on a function , $$d\omega_\mu = \frac{\partial \omega_\mu}{\partial x^\nu} dx^\nu . $$ ( this formula is what they should teach you in multivariate calculus but do not . . . ) now you can see that when you let $x$ act on $\omega ( y ) $ and $y$ on $\omega ( x ) $ , you will get derivatives also on the components $x^\nu$ and $y^\nu$ . but you will get that from $\omega ( [ x , y ] ) $ too , and they will cancel . you should be able to work this out yourself . [ there is actually a theorem that if you want to know if a tensor identity is true , it is sufficient to check it under the assumption that any lie brackets involved are 0 . can you think of why ? . ]
in the context of classical mechanics , your question is probably ill-formed . since point masses have no physical extent , the gravitational force increases without bound as r approached zero , which it will indeed do because they will only collide when they are superposed . infinite force on a finite mass implies infinite acceleration ( f = ma ) which implies infinite velocity , which is inconsistent with special relativity .
i am going to dare to give a very brief answer that is likely not what most folks would expect , but is deeply rooted in experiment : the speed of time is just the speed of a clock -- that is , of how fast some kind of a repeated cycle can be done . clocks thus only have meaning relative to each other . you can set one as a standard , then measure other by it , but you can never really define " the " time standard . that is actually a very einstein way of defining time -- which is to say , it is a very mach way of defining time , since einstein got much of his insistence on hyper-realism in defining physics quantities from mach . now , most likely you thought i was going to answer that there is some kind of velocity of an object along a time axis $t$ that has " length " in much the same fashion as x or y or z , not in terms of cycles . that is certainly what comes to mind for me , in fact ! while viewing $t$ as having ordinary xyz style length turns out to be an incredibly useful abstraction , it is difficult experimentally to make $t$ to behave fully like a length . the main reason is that the clock with its cycles keeps sticking in its nose and requiring that at some point , you sort of " borrow " a space-like axis from xyz space and use that to write out a sequence of clock cycles ( called proper time or $\tau$ ) on paper . as a result , it is not really $t$ you are drawing in those diagrams . you are instead borrowing a bit of ordinary space and mapping clock cycles onto it , making them seem like a length more through the way you represent order them than in how they actually work . fortunately , there is a different and more satisfying approach to the question of whether time has length , one that is suggested by special relativity , or sr . sr says in effect that xyz space and $t$ are interchangeable , and in a very specific way . so , even though there is always a need to write out some cycles in diagrams -- proper time happens ! -- you can argue that there is nonetheless a limit at which objects traveling closer and closer to the speed of light look more and more as if their time axis has been changed into a static length along some regular xyz direction of travel . so , by this take-it-to-the-limit kind of thinking , you can construct a more explicit concept of $t$ as an axis with xyz-style length . it also provides a pretty good answer to you question . since proper time comes to an almost complete stop as an object nears the speed of light , you can say that you have in effect " stolen " the velocity of that object or spaceship through time ( from your perspective or frame , not hers ! ) and converted it fully into a velocity through space ( from your perspective ) . so there is your answer : that " stolen " velocity along $t$ appears to correspond most closely with the velocity of light $c$ in ordinary space , since that is the real-space velocity at which proper time $\tau$ comes ( at the limit ) to a complete halt . this idea that objects " move " at the speed of light along the $t$ axis is in fact a very common assumption in relativity diagrams . it shows up for example whenever you see a light-cone diagram whose cone angle is $45^\circ$ . why $45^\circ$ ? because that is the angle you get if you assume that the " velocity " of light along the $t$ axis is identical to its velocity $c$ in ordinary xyz space . now , is there some slop in how that could be interpreted ? you bet there is ! the idea of a " velocity " in time is for example problematic in a number of ways -- just try to write it out as a derivative and you will see what i mean . but taking such a perspective at least in terms of how to think of the issue gives a really nice simplicity to the units involved , as well as that conceptual simplicity in how to think of it . more importantly , where such simplicity keeps popping up in the representations of something in physics , it is almost certainly reflecting some kind of deeper reality that really is there .
you can solve for $a$ and $b$ by using $r ( 0 ) = a$ , $\theta ( 0 ) = 0$ and evaluating $\dot{r}$ and $\dot{\theta}$ in your solution and using the initial conditions for velocity . you will need an equation for velocity as a vector in polar coordinates . furthermore , while you do not have $\theta ( t ) $ explicitly , it is a useful exercise to consider what happens to $u$ ( or $r$ ) as $\theta$ varies over the whole range $ ( 0 \dots 2\pi ) $ . if $u \to 0$ anywhere , then that would be an example of the particle escaping to infinity .
launching a space vehicle from a plane is an example of air launch and it does indeed provide some advantages like initial altitude boost and making greater part of the launch system reusable . air launch has recently been gaining popularity , see space ship one 's white knight carrier plane and recent paul allen 's plan for a new commercial spaceship . however , going from this to a single vehicle with dual propulsion system ( one for atmospheric flight and one for spaceflight ) has very significant disadvantages . one of the greatest challenges when it comes to launching into space is the huge amount of fuel one needs to use for every kilogram of useful mass . the reason for this is that launching into space is not only about leaving the atmosphere ( this is not far at all : just 100km above your head ) , but it is also about achieving orbital or escape velocity ( roughly 7-8km/s and 11.2 km/s respectively in the most common case ) . accelerating a vehicle to this velocity requires huge amount of energy and fuel . see tsiolkovsky rocket equation and this chart . this challenge is the reason why launchers employ the opposite approach : unlike planes they jettison every heavy piece of equipment that is no longer useful for the remaining flight ( e . g . first stages of a multistage rocket ) . carrying the atmospheric propulsion system with you all the way to space ( and all the way to orbital or escape velocity ) would make each launch extremely expensive if not impossible .
first of all , dark matter and dark energy , despite their naming , are two very different concepts . we do not really have any good reason to group them together , other than the fact that both represent things we do not understand . thus they are not necessarily backed by the same sets of evidence . why we believe these things exist as it happens though , some of the strongest evidence for both comes from the cosmic microwave background . basically , whatever " stuff " there is in the universe will have an effect on the temperature and polarization fluctuations in this radiation , which was emitted some few hundred thousand years after the big bang . the best all-sky map of this radiation is made by the wmap satellite , and every couple years they release several papers with the analysis of the data . for instance , here is the 2011 paper focusing on the cosmological parameters . they basically just feed all the data into an enormous statistical program to find the most likely values for a large set of parameters , including the amount of " normal " baryonic matter $\omega_\mathrm{b}$ , the amount of cold dark matter $\omega_\mathrm{c}$ , the amount of dark energy $\omega_\lambda$ , and the dark energy equation of state parameter $w$ . there is a lot that can be said as to what the effects of these parameters are , but ultimately you just cannot explain the cmb without having dark energy and dark matter . alternate theories for dark matter now " cold dark matter " ( the cdm of the $\lambda\text{cdm}$ model ) means massive particles interacting via gravity and the weak force but not via electromagnetism and that were non-relativistic even at the time of recombination when the cmb was released . just plain old " dark matter " refers to any gravitating mass that does not have much of an electromagnetic signal . the galaxy rotation curves you refer to were some of the first dark matter evidence , and indeed they could be explained by assuming a large number of quiescent black holes or star-less planets or dust that we just missed for one reason or another . these alternate theories could also , with enough manipulation , explain the bullet cluster , where the gravitational mass found with lensing maps is clearly not collocated with the baryonic mass in hot , x-ray emitting intracluster gas . however , microlensing surveys tend to rule out the first two , and we think we have a good handle on dust dynamics . something more exotic is called for . there is a diminishing community in support of mond - modified newtonian dynamics - which postulates long-range deviations from the inverse-square law in gravity . however , the bullet cluster , together with very precise solar system data , makes this theory difficult to get working . add to this the very nice " wimp miracle " ( no good wiki page there - sorry ) , which is suggestive of dark matter being new types of particles . " wimp " stands for " weakly interacting massive particle , " and the " miracle " is this : if you assume there is a species of particle $\text{x}$ whose only appreciable interaction is annihilation with its antiparticle $\overline{\text{x}}$ , with a cross section typical of weak interactions and a mass typical of , well , particles , you can easily calculate the abundance of these things in the present universe . they are in equilibrium with other species when the universe is energetic enough for them to be pair-created , and they " freeze out " when the universe cools enough . the end result is right around what we infer from other means . alternate theories for dark energy dark energy is a little trickier . there really is not a good explanation of " what " it even is - the name is more a catch-all for describing the observed accelerated expansion of the universe . you might believe it is a cosmological constant . in this case it is a nonzero scalar $\lambda$ in einstein 's equation $$ g_{\mu\nu} + \lambda g_{\mu\nu} = 8\pi t_{\mu\nu} , $$ where $g$ is the metric containing all the information about the curvature of spacetime , $g$ is a known function of $g$ and its first two derivatives , and $t$ is the stress-energy tensor containing all the information about matter , energy , and momentum in the universe . this is equivalent to saying there is some substance in the universe with equation of state $p = -\rho c^2$ ( $p$ is pressure , $\rho$ is mass density ) . [ for comparison , non-relativistic diffuse matter is well approximated by $p = 0$ , and relativistic matter has $p = \rho c^2/3$ . ] others are open to the idea that $w = p/ ( \rho c^2 ) $ is not exactly $-1$ for this new " stuff , " and so that can be a free parameter in your modeling . all the evidence points to $w$ being consistent with $-1$ , but the uncertainties are still somewhat large . you can get more exotic and note that the accelerated expansion phase the universe is currently undergoing is not entirely unlike the inflation many believe happened in the very early universe . inflation has all sorts of theories proposed for it , many of which are variations on " there is an abstract quantum field $\phi$ with these certain properties . . . " others take place in the realm of strings and branes . many of these theories can be boiled down to a $w$ that changes over time . future research these fields of study are very much alive and well . dark matter calls for both astrophysicists to study its role in large-scale dynamics and particle physicists to try to nail down its properties . the astrophysics side involves observers placing better constraints on the evidence so far , which has a lot to do with galaxy clusters and large-scale structure , as well as theorists to predict how dark matter influences e.g. galaxy formation , usually by using massive simulations . ( and those simulations , such as the millennium simulation , often lead to cool movies . ) the physics side involves experiments to try to detect the stuff directly ( there are literally dozens of these - far too many to list - and though there are some claimed detections , none are really accepted by the community at large ) . it also involves finding a place for such particles in an extended version of the standard model . dark energy is begging for physicists to come up with testable models that mesh with the rest of physics without seeming fine-tuned . it is still very much new , given that it was only last year 's nobel prize that recognized the teams that provided undeniable proof of its existence around the turn of the millennium . so by all means feel free to be inspired by these problems to work in some field . there is plenty to be done . i would even say the prospects are better than e.g. string theory alone , since we have solid , incontrovertible evidence saying there are gaping holes in our knowledge when it comes to dark matter and energy , and these holes are right where we can easily probe them via astronomy .
as far as our latest theories go , the speed of photons is going to be " c " only as they move with speed of light irrespective of your point of observations . the color factor depends on the original color that was emitted and then the application of dopplers effect for the shift in apparent wavelength and hence color due to velocity of approach of car , however in real life it is insignificant and whatever color is emitted is what we see .
you should simply multiply by two , to get $$f =2 \left ( \frac{\alpha}{\pi} \right ) ^{\frac{1}{2}} e^{-\alpha v^2} \text d v . $$ this is because your integral over speed will now be from zero to infinity , and you need to ' fold over ' the integral from minus infinity to zero . otherwise , there are no further geometrical factors - the integral $\int\text d\vec v=\int_{-\infty}^\infty \text dv$ is alreadyin the form you need it , and you just need to figure out what to do with negative velocities .
concerning the factor $\frac{1}{2}$: it seems that op in his classical reasoning only accounted for the coulomb potential energy $$\tag{1}\langle u\rangle ~=~-k_e e^2 \langle \frac{1}{r} \rangle ~=~-\frac{k_e e^2}{a_0} ~&lt ; ~0 . $$ here $k_e$ is coulomb 's constant and $a_0$ is the bohr radius . $^1$ however we should also take the kinetic energy $\langle t\rangle&gt ; 0$ into account ! we know from the virial theorem that the kinetic energy $$\tag{2}\langle t\rangle~=~-\frac{1}{2}\langle u\rangle~&gt ; ~0$$ is minus half the potential energy for the $1/r^2$ coulomb force . hence the total energy becomes half the coulomb potential energy : $$\tag{3} e ~=~ \langle t\rangle+ \langle u\rangle ~=~-\langle t\rangle ~=~\frac{1}{2}\langle u\rangle~&lt ; ~0 , $$ which is ( up to sign conventions ) the rydberg energy . -- $^1$ here op 's estimate is helped by the fact that the expectation value $$\tag{4}\langle\frac{1}{r}\rangle ~=~\frac{1}{a_0}$$ in the ground state happens to be the inverse bohr radius without any non-trivial dimensionless number appearing in eq . ( 4 ) ! [ note for comparison , that e.g. $\langle r\rangle =\frac{3}{2}a_0$ . ]
given the slew of comments to your question i will summarise them in an answer to make it simple to read . if we assume that you know the oscillations are regular and your time for ten of them is $t \pm \delta t$ , then the time for a single oscillation $\tau$ is : $$ \tau = \frac{t \pm \delta t}{10} = \frac{t}{10} \pm \frac{\delta t}{10} $$ so you divide your error by 10 and the error in the time for a single oscillation is $\pm 0.001$ seconds . carl 's point arises if you measure a single oscillation ten times in separate measurements . in that case if $t$ is now the sum of all ten times , and $\delta t$ is the error in any single measurement the time for a single oscillation is : $$ \tau = \frac{t}{10} \pm \frac{\delta t}{\sqrt{10}} $$
the quote is taken from just above eq . ( 1.32 ) in ref . 1: [ . . . ] if the internal forces are also conservative , then the mutual forces between the $i$th and $j$th particles , ${\bf f}_{ij}$ and ${\bf f}_{ji}$ , can be obtained from a potential function $v_{ij}$ . to satisfy the strong law of action and reaction , $v_{ij}$ can be a function only of the distance between the particles : $$v_{ij} ~=~ v_{ij} ( |{\bf r}_i-{\bf r}_j| ) . \tag{1.32}$$ the structure of internal forces among $n$ point particles can be quite rich in general , see e.g. this phys . se post . however the first sentence in the quote makes it clear that ref . 1 is additionally assuming : that the internal forces on one particle is a sum of forces from the other particles . thus it is enough to study the internal force ${\bf f}_{ij}$ from the $i$th particle on the $j$th particle . that ${\bf f}_{ij} ( {\bf r}_i , {\bf r}_j ) $ only depends on the two positions ${\bf r}_i$ and ${\bf r}_j$ of the $i$th and $j$th particles , respectively . that ${\bf f}_{ij} ( {\bf r}_i , {\bf r}_j ) $ is a conservative force , meaning that there exists a potential $v_{ij} = v_{ij} ( {\bf r}_i , {\bf r}_j ) $ such that $$ {\bf f}_{ij}~=~-{\bf \nabla}_{j} v_{ij} . $$ that the potential $$v_{ij} ( {\bf r}_i , {\bf r}_j ) ~=~v_{ji} ( {\bf r}_j , {\bf r}_i ) $$ is symmetric . the weak form of newton 's 3rd law then implies that $$ {\bf 0}~=-{\bf f}_{ij}-{\bf f}_{ji}~=~ {\bf \nabla}_{j} v_{ij}+{\bf \nabla}_{i} v_{ji}~=~ ( {\bf \nabla}_{i}+ {\bf \nabla}_{j} ) v_{ij} , $$ which in turn implies that the potential $v_{ij} = v_{ij} ( {\bf r}_{ij} ) $ only depends on the difference ${\bf r}_{ij}:={\bf r}_j-{\bf r}_i$ in positions . finally the strong form of newton 's 3rd law implies that $${\bf r}_{ij}~\parallel~ {\bf f}_{ij}~=~-{\bf \nabla}_{j} v_{ij} ( {\bf r}_{ij} ) , $$ which in turn implies that the potential $v_{ij} = v_{ij} ( |{\bf r}_{ij}| ) $ only depends on the distance $|{\bf r}_{ij}|$ . [ the last point follows from the fact that a scalar function $v:\mathbb{r}^3\to \mathbb{r}$ , with the property that the gradient $${\bf \nabla} v ( {\bf r} ) ~\parallel~ {\bf r}$$ is parallel to the position vector ${\bf r}$ , can only depend on the length $|{\bf r}|$ . can you see why ? hint : decompose the gradient in spherical coordinates . ] references : herbert goldstein , classical mechanics , chapter 1 .
the voltage itself does not do anything , it is the electric current that a high voltage can produce what kills you . a high current can disipate a lot of energy in your body ( human body has low conductivity ) , and literally burns you .
you know how the shape of atomic orbitals ( s , p , d ) depends on the angular momentum of the electron . this momentum can be preserved in photoelectrons , and from the shape of the electronic cloud hitting the detector you can determine not only the energy , but also the angular momentum of the electron . it gets more complicated for many-electron atoms and molecules , but the basic idea is the same .
i can not find any references to support this , but i would guess that because the linear molecule is more flexible than the branched molecules it has a higher entropy associated with the number of arrangements of the molecule . see http://cccbdb.nist.gov/config.asp for some comments on this , though this is just some site i found with google so caveat emptor .
for each action of body a on body b , there is a reaction of body b on body a . the two forces do not apply on the same body and thus will not cancel each other when looking at the movement of body b only .
i think david mc mohan 's sequence of demystified books could be about appropriate to smoothly approach string theory on a gentle but nevertheless technical level . however , if you are very serious and plan to do research , this does not replace studying the polchinski bible and many other " real " textbooks . . . the demystified books are best read in the following order : quantum mechanics relativity quantum field theory supersymmetry string theory before you read the string theory book , it is indispensable that you know some complex analysis too . i like these books because the layout is funny , and they contain nice small grained step by step derivations and good explanations of the most important ideas and concepts you need to know when delfing into a new subject for the first time . the purpose of these books is among other things to make reading " real textbooks " about each topic listed easier . in addition , to learn what should be studied in what order and find additional resources , gerard t'hooft gives some good advice about how to become a good theorist and waren siegel has devised a graduate curiculum for people who want to study high energy theory .
solid metals are crystals , not liquids . the way any crystal plastically deforms is by motion of crystal dislocations . every crystal obeys a stress-strain curve , where stresses up to a certain amount do not result in permanent deformation . higher stresses do result in permanent ( plastic ) deformation because dislocations move . if a metal is " soft " , all that means is the stress needed to start dislocations moving and plastically deform it is less than it is for other material . if the applied stress is lower than that number , it will not permanently deform . it is not like a highly viscous liquid .
when studying angular things - torque , angular velocity , angular momentum , etc . - physicists do a clever thing to avoid having to describe curves . you see , you might be tempted to draw a curved arrow for a torque , indicating that you are twisting something around in a circular-ish way . but then when you try to add two such arrows together , all of a sudden you realize your notation no longer has a natural , intuitive meaning . instead , we draw the arrow pointing perpendicular to the plain of the curve you are tempted to draw . more precisely in the case of torque , perpendicular to the plain defined by the radial vector and the force vector . note that this uniquely defines what plane your curved arrow must reside in , and , given the right-hand rule , clears up the ambiguity as to which way your curved arrow should point ( if your right-hand fingers curl in the direction of the curved arrow you want to draw , your thumb points in the direction of the straight arrow you should draw instead ) . it is then a simple matter to encode the magnitude of the torque/angular velocity/whatever in the length of this vector . the benefit is that you end up with straight arrows describing everything , and they add exactly as your torques should add - you have a genuine vector space , and are free to abstract away from all diagrams . and it is not even terribly counterintuitive - the torque vector is parallel to the axis around which you are applying torque . if you think about it long enough , you should be able to convince yourself that if you had to choose a single direction to define things , this is the least ambiguous .
note that the energy is per degree of freedom , so you do not use all the 14 tev ( nor are they actually running at that energy yet ) . so , how many degrees of freedom ? good question . each proton has three valence quarks , but these are generally agreed to make up a small portion of the mass ( and to carry a small portion of the momentum ) of the proton . the rest of the mass ( or momentum ) is carried by particles ( quarks and gluons mostly ) from the so-called " sea" ; these pop into and out-of existence owing to the uncertainty principle . worse , when the collision happens there is a great deal of energy available to put the virtual particles of the sea on to ( or nearly on to ) the mass shell , converting them into real particles , each equipped with their own swarm of ghostly hangers on . then many of these decay in a very short time . it is the average energy of this multitude of degrees of freedom which you are trying to measure/calculate , and it is non-trivial .
the atmosphere rotates along with the earth for the same reason you do . force is not needed to make something go . that is a basic law of physics - that a thing that is moving will just keep moving if there is no force on it . force is needed either to make something change its speed , or to make its motion point in a new direction . a force can do both or just one of these . most forces do both , but a force that pushes in the exactly the same direction you are already going only changes your speed , and does not change your direction . a force that pushes at a right angle to the direction you are already going only changes your direction , and does not add any speed . a force at "10 o'clock " , for example , will change both your speed and your direction . as you stand still on earth , you continue going the same speed , but your direction changes ; between day and night you move opposite directions . so the forces on you must be at a right angle to your direction of motion . indeed , they are . your motion is from west to east along the surface of the earth , and the force of gravity pulls you down towards the center of the earth - the force and your motion are at right angles . similarly for the atmosphere . it is moving along with the earth , and moving at a constant speed . it does not need anything to push it along with the earth . since only its direction of motion is changing , it only needs a force at a right angle to its motion , the same as you , and the force that does the job is again gravity . that is not the whole picture , because the amount that your direction of motion changes depends on how strong the right-angle force is . it turns out gravity is much too strong for how much our direction of motion changes as the earth spins . there must be some other force on us and on the atmosphere canceling out most of the gravity . there is . for me it is the force of the chair on my butt . for the atmosphere , it is the air pressure . so gravity does not " make the air rotate " . the air is already going , and gravity simply changes its direction to pull it in a circle . you may be wondering why the air does not just sit there and have the earth spin underneath it . one answer to that is that from our point of view that would mean incredibly strong wind all the time . that wind would run into stuff and eventually get slowed down to zero ( that is from our point of view - the air would " speed up " to our speed of rotation from a point of view out in space watching everything happen ) . even the air high up would eventually rotate with the earth because although it can not slam into mountains or buildings and get stopped from blowing , it can essentially " slam into " the air beneath it due to friction in the air . ( this is a little redundant with dmckee 's answer ; i was half way done when he beat me to the punch )
the change would transmit at the p wave velocity ( look up elastic waves ) . this is typically a few kilometers per second for most materials you would be likely to make a bar out of . you can maximize this speed by going for low density at high strength , something like titanium comes to mind . in any case the result would be many thousands of years .
because the forces act on different objects . if you write it as $f_{ab}=-f_{ba}$ , the first term is object $a$ acting on $b$ and vice versa for the second .
a way to do this is using regularization by substracting a continuous integral , , with the help of the euler-maclaurin formula : you can write : $$ \sum_{regularized} = ( \sum_{n=0}^{+\infty}f ( n ) - \int_0^{+\infty} f ( t ) \ , dt ) = \frac{1}{2} ( f ( \infty ) + f ( 0 ) ) + \sum_{k=1}^{+\infty} \frac{b_k}{k ! } ( f^{ ( k - 1 ) } ( \infty ) - f^{ ( k - 1 ) } ( 0 ) ) $$ where $b_k$ are the bernoulli numbers . with the function $f ( t ) = te^{-\epsilon t}$ , with $\epsilon &gt ; 0$ , you have $f^{ ( k ) } ( \infty ) = 0$ and $f ( 0 ) = 0$ , so with the limit $\epsilon \rightarrow 0$ , you will find : $$\sum_{regularized} = - \frac{b_1}{1 ! } f ( 0 ) - \frac{b_2}{2 ! } f ' ( 0 ) = - \frac{1}{12}$$ because $f ( 0 ) = 0$ and $b_2 = \frac{1}{6}$
to answer this we first need to be clear about why this wavefunction renormalization arises . for simplicity we focus on $\phi^4$ theory . for a free field we have , \begin{equation} \phi ( x ) \left| 0 \right\rangle = \int \frac{ d ^3 p }{ ( 2\pi ) ^3 } \frac{1}{ 2 e _{ {\mathbf{p}} } } e ^{ - i {\mathbf{p}} \cdot {\mathbf{x}} } \left| {\mathbf{p}} \right\rangle \end{equation} apart from the factor of $ \frac{1}{ 2e _{ {\mathbf{p}} }} $ this is just equal to $ \left| {\mathbf{x}} \right\rangle $ . thus we can intereprate $ \phi ( {\mathbf{x}} ) $ acting on the vacuum as a field producing an $ {\mathbf{x}} $ eigenstate . the equation above can be used to show that \begin{equation} \left\langle 0 \right| \phi ( {\mathbf{x}} ) \left| {\mathbf{p}} \right\rangle = e ^{ i {\mathbf{p}} \cdot {\mathbf{x}} } \end{equation} to continue to interpret $ \phi ( {\mathbf{x}} ) $ as the field that creates $ \left| {\mathbf{x}} \right\rangle $ eigenstates the renormalized field must obey this same relation . thus we impose the renormalization condition : \begin{equation} \left\langle 0 \right| \phi _r ( 0 ) \left| {\mathbf{p}} \right\rangle = 1 \end{equation} where $ \phi _r = \sqrt{ z } \left ( \phi - \left\langle \phi \right\rangle \right ) $ is the renormalized field . if the wavefunction does not change under renormalization then $z$ is just equal to $1$ . now $ z $ is not measurable for the same reason that the bare masses and couplings can not be measured . however , the renormalized field is measured in every process since if the condition above was not set to $1$ then we would be getting an extra factor for each external field line and another for each propagator in every diagram . for example if we just forgot about the wavefunction renormalization we would get an extra $\sqrt{z}$ for each external line and an extra $z$ for each propagator .
it can be instructive to see the applications of clifford algebra to areas outside of quantum mechanics to get a more geometric understanding of what spinors really are . i submit to you i can rotate a vector $a = a^1 \sigma_1 + a^2 \sigma_2 + a^3 \sigma_3$ in the xy plane using an expression of the following form : $$a ' = \psi a \psi^{-1}$$ where $\psi = \exp ( -\sigma_1 \sigma_2 \theta/2 ) = \cos \theta/2 - \sigma_1 \sigma_2 \sin \theta/2$ . it is typical in qm to assign matrix representations to $\sigma_i$ ( and hence , $a$ would be a matrix--a matrix that nonetheless represents a vector ) , but it is not necessary to do so . there are many such matrix representations that obey the basic requirements of the algebra , and we can talk about the results without choosing a representation . the object $\psi$ is a spinor . if i want to rotate $a'$ to $a''$ by another spinor $\phi$ , then it would be $$a'' = \phi a ' \phi^{-1} = \phi \psi a \psi^{-1} \phi^{-1}$$ i can equivalently say that $\psi \mapsto \psi ' = \phi \psi$ . this is the difference between spinors and vectors ( and hence other tensors ) . spinors transform in this one-sided way , while vectors transform in a two-sided way . this answers the difference between what spinors are and what tensors are ; the question of why the solutions to the dirac equation for the electron are spinors is probably best for someone better versed in qm than i .
by special relativity , the energy needed to accelerate a particle ( with mass ) grow super-quadratically when the speed is close to c , and is &infin ; when it is c . $$ e = \gamma mc^2 = \frac{mc^2}{\sqrt{1 - ( \text{“percent of speed of light”} ) ^2}} $$ since you can not supply infinite energy to the particle , it is not possible to get to 100% c . edit : suppose you have got an electron ( m = 9.1 &times ; 10 -31 kg ) to 99.99% of speed of light . this is equivalent to providing 36 mev of kinetic energy . now suppose you accelerate " a little more " by providing yet another 36 mev of energy . you will find this this only boosts the electron to 99.9975% c . say you accelerate " a lot more " by providing 36,000,000 mev instead of 36 mev . that will still make you reach 99.99999999999999% c instead of 100% . the energy increase explodes as you approach c , and your input will exhaust eventually no matter how large it is . the difference between 99.99% and 100% is infinite amount of energy .
this must have been asked before , but the only near duplicate i can find is how to deduce the theorem of addition of velocities ? and this is not an exact duplicate . the point is that relativistic velocities can not just be added . in your example let u be the plane 's speed , 0.8c , and v be your speed , 0.4c , then the speed a stationary observer sees , w , is given by : $$ w~=~\dfrac{u+v}{1+uv/c^2} $$ which is about 0.91c . the other passengers in the plane will see you running at 0.4c , but remember that the aeroplane 's time is running more slowly than a stationary observers time . so what looks like 0.4c to the passengers on the plane looks slower to me standing still on the ground .
for present consumers , they are not really useful . yes , we can not make much out of them because of their instability--but that does not mean we can in the future . we do not necessarily have to " make " stuff out of them for them to be useful--they can be part of a process as well . for example , we could have technology involving muons that are exchanged between object . ( just a random idea--here , if the exchange times are small enough , decay does not matter ) as a similar situation from the past , the concept of a plasma would have been considered unstable and " exotic " many years ago . i doubt anybody really thought it would be useful to consumers . but plasma displays have been in the market for years now ( they use tiny cells filled with a bit of plasma in the display ) . actually , the search for particles has a much larger impact here : searching for these particles help us develop theories of the underlying nature of the universe . science , as we know , consists of a tug-of-war between theory and experiment . sometimes , a theory is developed to explain some experimental results . other times , experimental results confirm the predictions of a theory . with any theory , one tries to start at a fundamental level--and particles are ( probably ) as fundamental as one can get . if a theory is consistent with experimental results on the existence of and interactions between fundamental particles , then we can apply it to bigger phenomena , and get a better understanding of how out universe operates . so , what use are these theories for consumers ? well , once we have a consistent theory of how the universe operates , technology may improve vastly . a lot of current technology is based on " new " theories . semiconductor technology ( which forms a huge part of our world now--you wouldn ; t be reading this if not for semiconductior technology sprouts from quantum mechanics . quantum mechanics , tries to explain stuff at the atomic level--at first glance that sounds useless for the consumer . but it is useful , as you probably know by now . similarly , a better understanding of how the universe operates can lead to improved technology for the consumer .
i am making this an answer because its too long to be a comment but its just an expansion of the things already stated in comments : non-normalizable states : the schroedinger equation has an infinity of solutions but almost all of them do not have a finite norm ( $\int|\psi ( x ) |^2dx$ is not finite ) . these are not phyiscally acceptable , since there would not be a probabilistic interpretation , amongst other issues . it is possible that when you discretize your system you are getting solutions that would have infinite norm in the limit $h\rightarrow 0$ and so are not physically acceptable . i strongly suspect this is the issue . to see if this is case look at the wavefunction associated with your bad eigenvalue ( this is the eigenvector associated with the eigenvalue ) . if in the limit $h\rightarrow0$ , your solution goes to increases faster than $\frac{1}{\sqrt{x}}$ as $x\rightarrow0$ than it is non-normalizable and unphysical . ' softening the singularity': the issue is that you potential goes to infinity at some point and that general numerical simulations do not like it when things go to infinity . especially having your potential go to negative infinity since the wavefunction wants to gather around the singularity . the way around this would be to replace your potential $\frac{1}{|x|}$ with say $\frac{1}{\sqrt{x^2+a^2}}$ where $a$ is some number you pick , and solve for the states . this new potential is the same as the old when $x\gg a$ but it does not have any singularity anymore , so it should not be a problem . to get back the real answer solve for the states with smaller and smaller values of $a$ until you see that if a limit is reached . it does not necessarily reach a limit and in this case you will still get the state that diverges .
this question has been asked before : the speed of gravity your friend is definitely wrong , but there is also a problem with your example . general relativity does not allow matter to be created or destroyed . ( technically the way to say this is that gr has local conservation of energy-momentum . ) if you put in an assumption that matter is destroyed , then the equations of general relativity are not self-consistent , so they do not predict anything . to fix up your example , it would be better to imagine that the sun was suddenly yanked out of the solar system at high speed . in that case , we would feel the change in gravity 8 minutes later , at the same time as the change in sunlight . general relativity predicts that disturbances in the gravitational field propagate as gravitational waves , and that low-amplitude gravitational waves travel at the speed of light . gravitational waves have never been detected directly , but the loss of energy from the hulse-taylor binary pulsar has been checked to high precision against gr 's predictions of the power emitted in the form of gravitational waves . therefore it is extremely unlikely that there is anything seriously wrong with general relativity 's description of gravitational waves . why does it make sense that low-amplitude waves propagate at c ? in newtonian gravity , gravitational effects are assumed to propagate at infinite speed , so that for example the lunar tides correspond at any time to the position of the moon at the same instant . this clearly can not be true in relativity , since simultaneity is not something that different observers even agree on . not only should the " speed of gravity " be finite , but it seems implausible that that it would be greater than c ; based on symmetry properties of spacetime , one can prove that there must be a maximum speed of cause and effect . [ ignatowsky , pal ] although the argument is only applicable to special relativity , i.e. , to a flat spacetime , it seems likely to apply to general relativity as well , at least for low-amplitude waves on a flat background . as early as 1913 , before einstein had even developed the full theory of general relativity , he had carried out calculations in the weak-field limit that showed that gravitational effects should propagate at c . this seems eminently reasonable , since ( a ) it is likely to be consistent with causality , and ( b ) g and c are the only constants with units that appear in the field equations , and the only velocity-scale that can be constructed from these two constants is c itself . high-amplitude gravitational waves need not propagate at c . for example , gr predicts that a gravitational-wave pulse propagating on a background of curved spacetime develops a trailing edge that propagates at less than c . [ mtw , p . 957 ] this effect is weak when the amplitude is small or the wavelength is short compared to the scale of the background curvature . w.v. ignatowsky , phys . zeits . 11 ( 1911 ) 972 palash b . pal , " nothing but relativity , " eur . j . phys . 24:315-319,2003 , http://arxiv.org/abs/physics/0302045v1 mtw - misner , thorne , and wheeler , gravitation
for future reference , if you even have only the most basic understanding of the physics behind it , you can guess that the fastest the mag field would drop off would be order $1\over r^2$ . and you can guess that it originates from the center of earth . since we are sitting pretty at ~6371km , adding another 100-1000 km would not even decrease it by half , let alone an order of magnitude .
as you correctly notice , this does not follow from strictly physical considerations , and it is mostly for convenience that we do this . essentially , this simplifies quite a bit the calculations of the coefficients of your state in a given basis . suppose , for example , that you have a basis $\{\chi_+ , \chi_-\}$ which is orthogonal but not necessarily normalized . then you can always write a given state vector $\chi$ as $$\chi=a\chi_++b\chi_- . $$ to find out the coefficients $a$ and $b$ , you take the inner product of $\chi$ with the basis states : $$⟨\chi_+ , \chi⟩=a⟨\chi_+ , \chi_+⟩$$ ( since $⟨\chi_+ , \chi_-⟩=0$ ) , and $$⟨\chi_- , \chi⟩=b⟨\chi_- , \chi_-⟩ . $$ if your states are not normalized , then you need to have the norms of the basis states in the denominator of these coefficients . however , if you set them to one , then the coefficients are exactly the overlap between the two vectors , which is a lot simpler to use : $$\chi=⟨\chi_+ , \chi⟩\chi_++⟨\chi_- , \chi⟩\chi_- . $$ additionally , this lets you directly interpret basic coefficients as direct products , which gives them a direct physical interpretation .
by applying gauss ' law one gets ( the surface integral over the sphere with $r&gt ; r$ ) : $$\oint_s \vec{e} ( r , \theta , \phi ) \cdot \hat{n} ( r , \theta , \phi ) \ , ds = \oint_s e ( r , \theta , \phi ) \ , ds = \iiint e ( r , \theta , \phi ) \ , r^2 \sin\theta \ , d\theta \ , d\phi = e4{\pi}r^2$$ the surface integral depends only on $r$ and is equal to the area of the sphere . $e$ also by spherical symmetry is idependent of $\theta$ and $\phi$ . spherical symmetry means that if a rotation is made to the system ( along $\theta$ or $\phi$ ) the field $e$ does not change , the rest follows . also since $$\iiint e ( r , \theta , \phi ) \ , r^2 \sin\theta \ , d\theta \ , d\phi = \frac{q_{enc}}{\epsilon_0}$$ the result is : $$e=\frac{q_{enc}}{\epsilon_04{\pi}r^2}$$
this is all about potential ; it is common that a particle movement is described by a following ode : $m\ddot{\vec{x}}=-\nabla v ( \vec{x} ) $ , where $v$ is some function ; usually one is interested in minima of $v$ ( they correspond to some stable equilibrium states ) . now , however complex $v$ generally is , its minima locally looks pretty much like some quadratic forms , and so the common assumption that $v ( x ) =ax^2$ . . . this makes the last equation simplify to : $\ddot{x}=-\omega^2x$ , with solution in harmonic oscillations . the common analogy of this is a ball in a paraboloid dish resembling potential shape ; it oscillates near the bottom .
i looked briefly at the blandford and thorne notes . the analogy between $\omega$ and b appears to be mainly illustrative and not to be extended too far . ( i do not see the reference to the analogy between e and $\omega \times \bf{u}$ there at first glance , but it does not seem to be apt at all . ) it seems to be intended to draw upon any previous intuition you might have about the relationship between the vector potential and the magnetic field to help you understand how fluid velocity and vorticity are related , and i think that is all . actually , i think a more apt analogy is to relate : 1 ) $\bf{u}$ to $\bf{b}$ , 2 ) $\omega$ to $\bf{j}$ , the current density , since $\nabla \times \bf{v} = \bf{\omega}$ and ( at least in magnetostatics ) $\nabla \times \bf{b} = \mu_\circ \bf{j}$ , and 3 ) the vector stream function $\bf{\psi}$ , where $\bf{v} = \nabla \times \bf{\psi}$ , to the magnetic vector potential $\bf{a}$ , with $\bf{b} = \nabla \times \bf{a}$ . this analogy is actually useful , because you can apply the tools of magnetostatics to the fluid flow , e.g. using the biot-savart law to write $\bf{v}$ in terms of $\bf{\omega}$ , and writing poisson 's equation to relate $\bf{\psi}$ to $\bf{\omega}$ , etc .
i am not very sure about this , but here is an attempt . well there is a connection you can try to make with the shannon entropy ! ! by apriori principle of microstates you can see that probability of each state is $$ p = \frac{1}{\omega} $$ in infromation theory given a set of events $ \{x_1 , . . . , x_n\} $ with probabilities $ \{p_1 , . . . , p_n\} $ shannon information for the $i^{th}$ event is defined by $$ i_i = -log_2p_i $$ from the definition you can see that lesser the probability of an event , greater is its information . this is the motivation for this definition . now the shannon entropy is defined as average information in the given set of events . for a probability distribution average of a quantity is defined by , $$ &lt ; q&gt ; = \sum\limits_{i=1}^n p_iq_i $$ so , the average information ( or entropy ) is given by $$ s_{shannon} =&lt ; i&gt ; = -\sum\limits_{i=1}^n p_ilog_2p_i $$ as an exercise you can also verify this average information is maximised when the value of $$ p_i = \frac{1}{n} \:\:\:\:\:\: \forall \: i = 1 , . . , n $$ which is the idea of apriori principle saying entropy is maximised . ( begin by setting $ \delta s = 0 $ ) the base 2 logarithm is a comfortable choice in the case of information theory . however we can translate this to the idea in statistical mechanics with natural logarithm and using the first equation where $ \omega $ is the total number of microstates ( volume of phase space divided by $ h $ - unit volume element of phase space ) . $$ s_{boltzmann} \propto ln \omega \implies s = k_b ln\omega $$ where $ k_b $ is factor involved conversion from log base 2 to natural logarithm . [ edit 1 ] detailed explanation is quite involved , but to give you first sight , microstate - is a particular set of values of ( p , q ) the momentum and position in the phase space . one set of ( p , q ) describes one physical state for the system . now , since the phase space is continuous , we can not do counting of the every point ( since that would sum infinite no . of them ) . from quantum mechanics , we have $$ \delta x\delta p \ge \hbar $$ from this we deduce the smallest area element in phase space ( ie x*p ) goes like h ( planck 's constant ) . after having discretised the space we can count the states , counting no . of smallest boxes ( i.e. . the bits of area h ) within the area - condition given by its energy in the proper mathematical language ( for a 2d phase space ) : $$ \omega = \int_{h\le e} \frac{dp dq}{h} $$ where e is energy of the system and h the hamiltonian . and the integration is over a 2-sphere . in a much simpler language to associate to your familiar information theory ( $ n \rightarrow \omega $ ) , which also gives you apriori principle for maximum entropy .
the obvious answer is hydrogen and helium plasma but the nuclear fusion can also create heavier elements . are these heavier elements a significant portion of the core ? as said in dmckee 's answer , no , the core of the sun is much too cool ( about ~15 000 000 k ) to burn any other than hydrogen into helium . the triple-alpha process , which converts helium into carbon , only kicks in somewhere around 80 000 000 k , depending on density . that said , the cno cycle does modify the internal abundances very slightly . that is , the cno cycle is a catalytic process , so the abundances of those elements are driven to the values at which the reaction proceeds at an equilibrium rate . glancing at a solar model i have lying around , this leads to about a 10% enhancement of the central nitrogen abundance , and a corresponding decrease of the carbon and nitrogen abundances . do the heavier elements " sink " to the " bottom " of the core , like iron has during planetary formation ? actually , yes , they do ! we refer to these processes as atomic diffusion . the one you are thinking of is known as gravitational settling . in short , yes , heavier elements " sink " towards the centre . this process takes a long time to make a meaningful difference : billions of years . for the metals , it is not important , but it is actually important for the helium abundances . there was a minor revolution in the mid-1990 's when this effect was included for the first time , and it led to a much better fit of solar models with respect to helioseismic observations . presumably , during the sun 's formation it would have accreted heavy elements made by previous generations of stars - does this just get added to the mix ? you are quite right : the sun 's initial composition reflected that of the nebula from which it was born , itself a product of whatever star ( s ) preceded it . the primordial mixture is expected to be fully mixed before a star starts burning hydrogen into helium . the reason is that the star ( or , at least , our models ) goes through a phase where the whole star is convective , so everything gets churned up and homogenized .
there is a nature article that describes the experiment and the results , http://www.nature.com/nature/journal/v473/n7348/full/nature10104.html , but that is behind a paywall . the experiment is described in some detail in " prospects for the measurement of the electron electric dipole moment using ybf " , http://arxiv.org/abs/1103.1566 ( i have only scanned the latter , but it looks to be quite informative ) . from the nature article , " in an atom or molecule with an unpaired valence electron , the interaction of the electron edm [ electric dipole moment ] with an applied electric field results in an energy difference between two states that differ only in their spin orientation . this energy difference is proportional to $d_e$ and changes sign when the direction of the field is reversed . a sensitive method of measuring this energy difference is to align the spin perpendicular to the field and measure its precession rate , which is proportional to the energy difference . an alternative description of the method is in terms of an interferometer . there is quantum interference between the two spin states , and the edm appears as an interferometer phase shift that changes sign when the electric field is reversed . " an electron is not either a sphere or not-a-sphere , but we can introduce more or fewer internal degrees of freedom into the quantum fields that are used to describe experiments results that we attribute to the electron field . introducing different degrees of freedom has consequences for the geometrical configurations of recorded experimental results . the nature article is explicit in saying that this is intended to distinguish between different speculative quantum field theories , " many extensions to the standard model naturally predict much larger values of $d_e$ that should be detectable " . this is an experimentalists ' article , however , so they link to a theory paper on the subject ( which i cannot access directly ) . if these fields give better descriptions than the standard model of particle physics , we expect to see different , less geometrically symmetrical statistics of events . many of the problems of reference here can be avoided if we talk about electron fields instead of about electrons . an " electron field " is less likely to be misrepresented as spherical or not spherical , but it can be associated with ( representation spaces of ) space-time symmetry groups ( which describe in a systematic way how something deviates from being symmetrical ) . care is needed because a quantum field is a more elaborate mathematical object than a classical field , but we can loosely think of a quantum field as a way to generate probabilities that the configuration of a classical field is one thing or another at any single time , while the details of quantum measurement are such that we can not talk about such probabilities at multiple times .
i probably would have calculated the angular velocity at the top of the circle . but the question is clearly ambiguous , and i do not think it was unreasonable of you to take " the angular velocity of . . . the swing " to mean the initial velocity of the swing , starting from the bottom . the question should have said " calculate the angular velocity at the top of the swing " , in order to make it clear what is being asked for . most likely the person setting the question did not actually consider that the angular velocity would be changing throughout the bucket 's motion . i have never marked a test , but if you started by calculating the angular velocity at the top of the swing and wrote down $\sqrt{g/l}$ then i probably would have given you the marks for that ( since it is the correct answer according to the answer scheme ) and just ignored the extra part .
the answer to your questions is very nuanced for the most part . i will start with the easy answers : light does not experience time , neither does it experience no time , the most accurate statement i could probably make off-hand is that it experiences null time . null time does not mean time has stopped , that would be zero time ; null time means null time , null is not a number ( isnan ( null ) ==true ) . photons have no frame of rest ( inertial or otherwise ) , as such it is impossible to discuss the time experienced by them because in order to do so you have to imagine a frame where they are at rest ( which is impossible ) . so i can not say that light experiences slower time when travelling through warped spacetime . but as for your first question , that is more complicated . in the comments ( now deleted ) i said it might be best for a black hole person to answer this than a gr person . the reason i said that is because black hole people probably answer this question so much that they have got great wording . now i stand by my opinions sometimes and this is one of those times . so i went and looked up a great answer by a black hole person that is much better than anything i could have come up with . but link-only answers are not fun , so here 's the tl ; dr version : special relativity does not say the speed of light is always constant ( even though , as i will get to , you can cheat such that it is ) . it says the speed of light is always constant when measured locally from an inertial frame of reference . an inertial frame of reference is a non-accelerating frame . the equivalence principle says that an accelerating frame is locally indistinguishable from a gravitational field . so that gives some room for the speed of light to not be measured consistently . if you are in free-fall , your acceleration due to gravity cancels out the gravitational field ( equivalent to accelerating the other direction ) such that you are now in an inertial frame . thus , if you measure the speed of light where you are , i guarantee you will get $c$ . however , as stated , this is only valid locally . say i am free . . . . . free-falling ( could not help myself , sorry ) and you are somewhere else and i measure the speed of light where you are , i will get a different value than $c$ because it is not local any more and the gravitational field still changes the passage of time around you . if you are in a gravity field and not free-falling , then you are not in an inertial frame and so will ( probably ) not measure the speed of light as exactly $c$ , not even locally . on earth , the gravity field is small so the effect of this makes the difference from $c$ extremely small . near a black hole , this effect becomes pronounced and very noticeable ( thus , black hole people are more accustomed to asking questions about it ) . however , there is a way to cheat to make the speed of light always appear constant locally . your measurement of the speed of light changes because your measurements of length and time change separately . you can get around this by measuring the speed of light with reference to measurements of length and time that are also changing . in the link i provided , the author uses lunar orbits and earth days as his length and time references . even though you would not measure these as the same number of kilometers or seconds in every frame , every observer in every frame will agree that they measure the local speed of light as about $12000$ lunar orbits/earth day . why is this cheating ? it is pretty much like saying the speed of light is 1 lightsecond/second or 1 lightyear/year . but so long as you can measure the length of the lunar orbit and the duration of an earth day , you will always have a constant number for the local speed of light .
it turns out that the vertex $x$ has to be contracted with the particle polarizations in the following manner , is $$x^{\mu \nu \alpha \beta} \epsilon_{1_\mu} \epsilon_{2_\nu} \epsilon_{3_\alpha} \epsilon_{4_\beta}$$ where $$x^{\mu \nu \alpha \beta} = 2 g^{\mu \nu} g^{\alpha \beta} - g^{\mu \alpha} g^{\nu \beta} - g^{\mu \beta} g^{\nu \alpha}$$ this results in : $$2 ( \epsilon_1 \cdot \epsilon_2 ) ( \epsilon_3 \cdot \epsilon_4 ) - ( \epsilon_1 \cdot \epsilon_3 ) ( \epsilon_2 \cdot \epsilon_4 ) - ( \epsilon_1 \cdot \epsilon_4 ) ( \epsilon_2 \cdot \epsilon_3 ) $$
polystyrene is used to hold coffee . most plastics will soften or melt around 100 °c ( the boiling point of water ) , although there are cups made out of plastic that will hold hot water . as you know , paper ( or wax lined paper ) cups allow the heat to transfer too quickly ( burnt fingers ) . ceramics transfer heat slowly and can be made to hold the liquid inside ( instead of seeping through ) .
a free quark is like the free end of a rubber band . if you want to make the ends of a rubber band free you have to pull them apart , however the farther apart you pull them the more energy you have to put in . if you wanted to make the ends of the rubber band truly free you had have to make the separation between them infinite , and that would require infinite energy . what actually happens is that the rubber band snaps and you get four ends instead of the two you started with . similarly , if you take two quarks and try and pull them apart the force between them is approximately independant of distance , so to pull them apart to infinity would take infinite energy . what actually happens is that at some distance the energy stored in the field between them gets high enough to create more quarks , and in stead of two separated quarks you get two pairs of quarks . this does not happen when you pull apart a proton and electron because the force between them falls according to the inverse square law . the difference between the electron/proton pair and a pair of quarks is that the force between the quarks does not fall according to the inverse square law . instead at sufficiently long distances it becomes roughly constant . i do not think this is fully understood ( it certainly is not fully understood by me :- ) , but it is thought to be because the lines of force in the quark-quark field represent virtual gluons , and gluons attract each other . this means the lines of force collect together to form a flux tube . by contrast the electron-proton force is transmitted by virtual photons and photons do not attract each other . finally , top quarks are usually produced as a top anti-top pair . it is possible to create a single top quark , but it is always paired with a quark of a different type so you are not creating a free quark .
when heated , air expands . now when you take the tupperware out of the microwave and it cools down , the air inside cools down again and contracts , and ' sucks ' the lid inwards . actually , that is not quite right - it is more precise to say the air outside presses the lid shut . the bodies of air outside and inside seek to equalize their pressure , and normally the tupperware would just shrink / contract until it is inside pressure is equal to the room pressure . but since the container is somewhat rigid , it can not shrink enough and there remains a pressure difference . this causes a force acting on the lid , keeping it shut . so : cooling down => volume shrinks cooling down , but forcing the volume to be constant => pressure goes down inequal pressure => force towards the lower pressure
to recover einstein 's equations ( sourceless ) in string theory , start with the following world sheet theory ( polchinski vol 1 eq 3.7.2 ) : $$ s = \frac{1}{4\pi \alpha'} \int_m d^2\sigma\ , g^{1/2} g^{ab}g_{\mu\nu} ( x ) \partial_ax^\mu \partial_bx^\nu $$ where $g$ is the worldsheet metric , $g$ is the spacetime metric , and $x$ are the string embedding coordinates . this is an action for strings moving in a curved spacetime . this theory is classically scale-invariant , but after quantization there is a weyl anomaly measured by the non-vanishing of the beta functional . in fact , one can show that to order $\alpha'$ , one has $$ \beta^g_{\mu\nu} = \alpha ' r^g_{\mu\nu} $$ where $r^g$ is the spacetime ricci tensor . notice that now , if we enforce scale-invariance at the qauntum level , then the beta function must vanish , and we reproduce the vacuum einstein equations ; $$ r_{\mu\nu} = 0 $$ so in summary , the einstein equations can be recovered in string theory by enforcing scale-invariance of a worldsheet theory at the quantum level !
because of scattering . blue light scatters more then other colours , because of its high frequency . for more information read http://www.sciencemadesimple.com/sky_blue.html
comment : a prof once said to me you should read the abstract , look at the pictures , then read the conclusion at the end , and then start reading the paper . it is only an overhead of minutes and you are slightly less lost and get an idea what the author thinks the value of the paper is . what i also like to do when taking notes is keeping in mind the search for what are appropriate lists for data . i.e. make it a task to find out what sort of collections of factoids would useful w.r.t. the task you set out to do . it helps forming an appropriate hierarchy of things on your head , which is different for every subject .
mass is one of fundamental attributes of a particle . these fundamental attributes are defined based on their interactions we observe in nature . there is no other way for us to assign a valued attribute to a particle . for example , charge is defined based on electromagnetic interaction . we observe the motion of particles under electromagnetic interaction and assign charge value based on this observation . similarly , we can assign attributes based on strong , weak , gravitational and inertial interactions . that is the thing . . based on gravitational interaction , we have got gravitational mass . and , based on inertial interactions , we have got inertial mass ( inertial interaction is fictional , but so is the gravitational interaction . . . . read on ) . general theory of relativity says that gravity is an inertial force . meaning , gravity is similar to what we feel at the time of breaking car . in decelerating car , your head accelerates forward despite there is no actual force to accelerate it . this illusion occurs because your reference frame ( the car ) is not inertial . . . it is in acceleration . in case of gravity , your car is spacetime . earth is actually moving straight in uniform speed . you see it revolving around sun due to curvature of spacetime . similarly , a freely falling body is actually in uniform speed ( due to which a freely falling body is inertial reference frame in general theory of relativity ) . as gravitational interaction is actually inertial interaction , the calculated mass by both means should be same . that is what mass equivalence is .
it can be said that the tangential speed of the moon in its orbit is represented by a vector that is constant in magnitude , but not so his direction . this variation of the vector direction ( always remains tangent to lunar orbit ) , is actually a change in velocity , and therefore acceleration . why the moon does not fall on the ground ? simplifying to a circular orbit , the centripetal force acting on the moon 's is the gravitational force , while by the movement itself , a centrifugal force that keeps the system in balance , is generated .
i did a report in school on the subject although that is almost 10 years past . the relation is - to a good precision - sinodial . you can simply use the longest and shortest day of the year to fit the sine $$ f ( x ) = a \cdot \sin ( \omega x - x_0 ) + b . $$ depending on your choice of scale , x will be in days or something else . this website does the calculation for you . you could just as easy look up the dates on your own and find $a$ , $x_0$ and $b$ for your location on your own ( $\omega$ will be $2 \pi/ ( 1 \mathrm{year} ) $ ) .
$p_i = \mid\psi_i\rangle\langle\psi_i\mid$ is the one-dimensional " projection " operator . by " one-dimensional " it means this projection operator projects $\psi$ onto a single dimension in hilbert space . firstly , any wavefunction $\psi$ can be written as a linear combination of orthogonal components . that is , $\psi = \sum a_i\mid\psi_i\rangle$ where $a_i$ is some coefficient . if there are $n$ such non-zero coefficients , $\psi$ can be thought of as a vector in $n$ dimensions , having components in each direction of length $a_i$ in this $n-dimensional$ hilbert space . $a_i$ is also the amplitude that the result $\psi_i$ will be obtained if the wave-function is measured in this basis . the probability is amplitude^2 . the projection measurement essentially " projects " the state $\psi$ onto one of these components . it is easiest to demonstrate why $p_i = \mid\psi_i\rangle\langle\psi_i\mid$ by applying it to the state $\psi$ . $p_i\psi = p_i\sum a_k\mid\psi_k\rangle = \mid\psi_i\rangle\langle\psi_i\mid\sum a_k\mid\psi_k\rangle = \mid\psi_i\rangle\sum a_k\langle\psi_i\mid\psi_k\rangle = a_i\mid\psi_i\rangle$ therefore , the operator $p_i$ acting on some arbitrary state $\psi$ , projects $\psi$ onto its i-th component vector . ( this is analogous to projecting a 2d vector onto say its x-component in euclidean geometry . for instance if a vector $v = ax + by$ , then the projection onto x-axis would yield $v_x = ax$ ) so since , $p_i\mid\psi\rangle=a_i\mid\psi_i\rangle$ we can easily show that $\langle\psi\mid p_i\mid\psi\rangle=\sum \langle\psi_k\mid a_k^*a_i\mid\psi_i\rangle = \sum a_k^*a_i\langle\psi_k\mid\psi_i\rangle = a_ia_i^* = \mid a_i\mid^2$ therefore we have shown that $\langle\psi\mid p_i\mid\psi\rangle$ gives the probability of the wavefunction being in the eigen-state $\psi_i$ . the next step is to show how the one in your book is also the probability . note your book 's use of $p_x$ is to represent probability and is not the projection operator . first consider the denominator $\langle\psi\mid\psi\rangle = \sum^j\sum^k\langle\psi_j\mid a_j^* a_k\mid\psi_k\rangle$ . the only terms that survive is when i=j . therefore we arrive at : $\langle\psi\mid\psi\rangle = \sum^j\langle\psi_j\mid a_j^* a_j\mid\psi_j\rangle = \sum^j a_j^*a_j = \sum^j \mid a_j\mid ^2 $ this is the total probability of any state which , if this is normalized , should be 1 . therefore $\langle\psi\mid\psi\rangle = 1$ for normalized states , otherwise it is the sum of all possible amplitudes^2 . next we consider the numerator . this is the dot product of the x-components of the state , which will yield $a_x^* a_x = \mid a_x \mid ^2$ therefore , numerator over denominator gives $\frac{\mid a_x \mid ^2}{\sum^j \mid a_j\mid ^2}$ . this is the probability for a particular state x to occur divided by the probability that any of the possible states will occur ( which should be 1 for normalized states ) .
the unified formula used in general relativity is $$d\tau=\sqrt{\sum_{\mu=0}^3\sum_{\nu=0}^3 g_{\mu\nu}dx^\mu dx^\nu} , $$ which by einstein 's notation ( summation over doubly repeating indices is implicit ) is also written as $$d\tau=\sqrt{g_{\mu\nu}dx^\mu dx^\nu} . $$ here $d\tau$ is the proper time " felt " or measured by particle moving in the spacetime , for some infinitesimal segment of the world line ; $dx^\mu= ( dx^0 , dx^1 , dx^2 , dx^3 ) = ( dt , dx , dy , dz ) $ are coordinate components of the displacement along that world line , and $g_{\mu\nu}$ ( 16 numbers ) is the metric tensor for the specific curved spacetime where the motion takes place . the metric tensor both shows the shape of the curved spacetime and the way the soordinate system is drawn onto it . for the longer part of the world line , you should integrate that formula over the world line , so it becomes $$\tau=\int_l\sqrt{g_{\mu\nu}dx^\mu dx^\nu} . $$ if we divide the formula by $dt$ , we get ( latin indices are summed only over the 1 . . . 3 range , so they imply only spatial coordinates ) : $$\frac{d\tau}{dt}=\sqrt{g_{00}+ ( g_{0i}+g_{i0} ) v^i+g_{ij}v^i v^j} . $$ $d\tau/dt$ is the time dilation factor with respect to the coordinate $t$ . for the flat spacetime and cartesian coordinates , $g_{00}=1$ , $g_{11}=g_{22}=g_{33}=-1$ and all other $g_{\mu\nu}$ 's are zero . for the spherical non-rotating gravitating body ( schwarzschild metric ) and polar coordinates , $$g_{tt}=1-\frac{2gm}{r} , \quad g_{rr}=\frac{1}{1-\frac{2gm}{r}} , \quad g_{\theta\theta}=r^2 , \quad g_{\varphi\varphi}=r^2\sin^2\theta , $$ and all other $g_{\mu\nu}$ 's are zero . besides that , gr considers very general spacetimes , so the general formula is the only one used everywhere .
do not think about annihilation as something exceptional . annihilation is just a type of interaction and there are many other possible interactions . now i do not use the word " interaction " in the sense of "4 fundamental interactions " , but in the sense of possible process in the quantum world where particles are destroyed and created . so the question should rather be - why there are interactions ? annihilation is simply one of the processes allowed by the interaction term in the lagrangian . see for example the interaction part of the qed lagrangian : notice that the term couples the dirac ( for instance electron ) field with the electromagnetic field . all possible processes follow from that : you can have electron emitting a photon , photon absorbed by electron , collison of 2 electrons creating a photon , collison of an electron with a positron creating 2 photons ( this is what we call annihilation ) , but such collision can also create heavier particles ( like muon and antimuon ) . annihilation is just the case where the quantum numbers cancel . but there is nothing really special about it . we should expect that there will be certain pairs of particles which have cancelling quantum numbers and other pairs that do not .
take a landscape . it can be modeled by a function f ( x , y , z ) . if all the derivatives , df/dx , df/dy , df/dz are zero , the landscape is flat to infinity and nothing interesting exists in the landscape . if one of the derivatives is different than zero , then we perceive a shape , and generally a landscape has a shape . as an example , suppose that we have a cone for this landscape , and there exists a funny " life " that exists in the distance from the center of the cone . all in one snapshot for us , birth is at the cone , middle age is some distance and death is where f is zero . in a similar manner we can think of time for each of us as starting at birth making a four dimensional shape and ending at death . another life form will see us as i explain in the example with the cone . thus time as a dimension for human perception is a df/dt . if nothing changed , there would be an uninteresting landscape . now we have developed means of studying what at first is a fourth dimension time axis , because all matter exists and has a df/dt in the four dimensional space . we have concluded from our observations that time has an arrow , i.e. one cannot " move " in the negative direction , from observing how nature behaves thermodynamically and microscopically . entropy always increases , and that defines an arrow of time independent of the human perception . the " motion " of time is the motion of our perception . when we look at a three dimensional landscape we can perceive it from zero to infinity . the landscape is not moving . with time we are at a specific time=t_0 sequentially and the four dimensional landscape opens to our perception in slices ultimately controlled by the rate of increase in entropy in our surroundings . the " force " is the usual statistical mechanics and quantum statistical mechanics that rules the nature of matter .
given your description , you clearly have non-exponential behaviour . however , there are two possible reasons for this behaviour : some materials in the system are nonlinear and do not follow newton 's law of cooling , which is that the heat flux at a given point is proportional to the temperature gradient vector . i should think this is the least likely of the two reasons ; general solutions of the heat equation do not have a simple exponential time dependence ; the general solution for materials that fulfil newton 's law of cooling ( i.e. heat flux proportional to temperature gradient ) is a superposition of functions of the form $f_n ( x , \ , y , \ , z ) \ , \exp ( -\alpha_n t ) $ , so you will only get a good fit to an exponential dependence when the boundary conditions / initial conditions are such that only one of these terms is dominant . see the discussion under the heading " solving the heat equation using fourier series " on the the heat equation wiki page to see how the superposition arises . as i said , the second is the most likely reason for your behaviour . the superosition weights are set by initial and boundary conditions . indeed a particular " prototypical " solution to the heat equation is the behaviour we see if we have a " hot spot " of very high temperature in a one-dimensional system , and we let this hot spot diffuse . our idealised description of this case is : $$\begin{cases} u_t ( x , t ) - k u_{xx} ( x , t ) = 0 and ( x , t ) \in \mathbf{r} \times ( 0 , \infty ) \\ u ( x , 0 ) =\delta ( x ) and \end{cases}$$ and its solution , with a highly non-exponential time dependence , is the heat kernel : $$\phi ( x , t ) =\frac{1}{\sqrt{4\pi kt}}\exp\left ( -\frac{x^2}{4kt}\right ) $$ whence we can build solutions to arbitrary initial temperature distributions in the bar by linear superposition . here , of course , $\delta$ stands for the dirac delta . again , see the discussion under the heading " fundamental solutions " on the the heat equation wiki page . so you likely need a more sophisticated model of your cooling thermometer . as an aside , i am surprised you got a curve with a simple dependence at all : i should not have thought " waving the thermometer around " to make it cool would set up particularly repeatable cooling conditions needed for proper experimental investigation .
if you try jumping on a trampoline , you will notice that when you jump up , the trampoline bends and stretches underneath you . it stretches some even if you stand still , but it stretches extra when you jump . the trampoline is elastic . when it is stretched , you can feel it pulling back towards its normal shape . thus , just before you jumped , the trampoline was extra stretched and exerted extra force on you , and this is why you went up in the air . a similar thing happens when you stand on other surfaces , except that the stretching is very minor and we do not usually observe it . when you jump , your legs exert extra force on the ground . that compresses the ground , making it act like a spring . being stretched further than normal , it exerts a larger-than-normal force on you , and you shoot up into the air . if you stood on a weak table or a thin sheet of ice , it might be strong enough to hold you as you stand still , but when you go to jump , the extra stretching might be too much and break it . it is not really necessary that the ground act like a spring ; this is just what i thought would be easiest to visualize . in fact you could push off water in a fairly similar way . all that matters is that the thing you push off has inertia , so that when you exert a force on it , it also exerts a force on you . i think this topic may be confusing for students because the ground is doing no work , yet still exerts a force . the ground does not have any energy to use to push you up , so it may seem counterintuitive that the ground is the source of the force . the very word " exert " in the phrase " exert a force " makes us think of actively pushing or pulling like a person does , not sitting there letting things happen to you as the ground does . this is just something you need to get used to . although in everyday language , we say " i forced the door open " or " i was forced against my will " and the word " force " refers to something that is moving or expending energy or exhibiting some sort of agency , that is not so in physics . the energy to jump comes from your legs ( and other parts of your body ) , but the force comes from the ground . a similar confusion often arises when students think about the force pushing forward a car driving at a constant speed on the road . because wind resistance is slowing the car down , something must be pushing it forward to cancel that . most students will say this is the " force of the engine " . but in fact the force that pushes the car forward is the force of friction from the road . the engine does provide the energy , but the road provides the force , as you can learn if the road is very slick with ice and unable to provide sufficient friction force to push the car forward . your question details ask whether the force is ultimately electromagnetic . yes , it is , but this is simply because almost every force we encounter in daily life besides that of gravity is electromagnetic . molecules have complicated forces between and within them due to electromagnetic interactions of their electrons and nuclei . however , understanding macroscopic forces precisely in terms of microscopic ones is generally very difficult .
to answer your question you need to be clear what co-ordinates you are using . if you use co-ordinates that are co-moving with the rock falling into the black hole then the rock will see the event horizon pass at the speed of light . external observers , using schwarzchild co-ordinates , will see the rock slow down as it approaches the horizon , and if you wait an infinite time you will see it stop . external observers obviously can not comment on the speed of the rock after it has passed the event horizon because it takes longer than an infinite time to get there . if you use the rock co-moving co-ordinates then you can ask what speed you hit the singularity and . . . actually i am not sure what the answer is . i will have to go away and think about it . incidentally http://jila.colorado.edu/~ajsh/insidebh/schw.html is a fun site describing what happens when you fall into a black hole .
spin can be measured as you say by the stern gerlach experiment for individual particles . spin for atoms can be found from measuring electromagnetic spectra of transitions between energy levels and fitting/assigning a solution of the potential problem which identifies the spin state for us . as the answer by dwin says there are specific energy levels that can be excited in nuclei and fitted with the appropriate model to identify the spin . in accelerator experiments the spin of resonances can be determined from their decay products , fitting their angular distributions . i have provided some links in the answer to a similar quiestion .
this is the question that has killed more brain cells in the history than any other did is time travel possible ? well it is possible at least a few millisecond 's in future . in an experiment performed in lhc ( large hedron collider ) scientist accelerated some particles to 99.99% of the spped of light which increased the life of the particles by few millisecond than their real life . so in a nutshell yes they traveled few millisecond in future ( bloddy time travellers i hate them ) so is problem for us to travel ? well there are ton 's of them so let us as much i know about them travelling in the past every human being or most of them ( do not know much about animal 's maybe they also have ) have a hardcore desire to travel back to the past and meet their loved one 's whom they have lost or waarn them about the calamities that are going to come . well there is a very pardoxing paradox in travelling to the past . here try to understand this . . neo the time traveler to the past ok let say neo have found a way to travel back to future and he is a mad scientist . so he makes a portal to go back in time . let 's say he just wan't to go only 5 min back in past and wan't to kill himself ( i told you he is mad ) . so he is now assembling gun part 's in front of the portal gate of his time machine . so it takes him about 5 min to do that . so now as he is ready to kill himself in the past he switches on the portal and set it to go 5 min in past . so portal is on and he see 's himself through the portal 5 min back assembling his gun ( ooo i look handsome ) . so no he takes his gun ( the present one ) and shoot 's the neo which he seeing through the portal in the past . ok so mission accomplished . but now here the paradox arises . neo in the present killed the neo who was assembling the gun in past but wait a minute if he was killed while assembling the gun , then how was he able to assemble the gun and switch on the portal and then kill himself ( o_o ) . this is paradox . like if you go in past tell the people that there is 9/11 going to happen so the people will get alert and stop it but if they stopped it from happening then how did you came to know that this is going to happen as it never happened cause they knew about it , but if it never happened then how did you came to know this happened and informed them in the past . . . . . . . . . see it just keeps on going on like a feedback happens in the poor sound system in a rock concert in a loop . to the future travelling in future is much more possible than going to past . but that will be no more useful other that you wan't to skip a long period of time like about 100 years in a time machine ( that is for observer 's out of time machine for them they will see that you are sitting in the magic box for hundred 's of year but for you inside it will be like everything is moving too fast outside and 100 of year will pass in the world yet only few min or sec or day 's depending on the machine for you in the time machine ) and then you come out of the time machine to live the future . now answer to your question ? i decide to check my future , and ( god-permitting ) i see that i have passed with flying colors . now , i " come back in time " and enjoy life , and do not study . so , it is just impossible that i succeed . so , where is the flaw ? ? this is imposible yes , it is totally impossible that you go in the future and see what you are doing and then come to past and fix all you flaws . why it is impossible ? hmm just think that you now sat in a time machine and you travel to the future to you exam hall to see how you are performing . now how are you even there in the future in the exam hall giving answer to the question 's . few day 's before you sat in the machine then who is living your life outside in normal speed as you were in the time machine all the time , you traveled to the future then how can you be there at that time . i know you are getting a little bit confused so try to look at it from a third person 's view ( that neighbor who always sneak 's at you ) . ok he saw that you wen't in the time machine to travel to the future . so now you are not there you are sitting in the time machine traveling in the future . now after 2 day 's there is your exam about which you wan't to know so the exam has started now but you are not in the examination hall as you are in the time machine travelling in future now after few minute you arrive their that the one who started travelling from the past to future to arrive at this time but you see you are not there your place is empty ass all the time for 2 day 's you were in the time machine ( try not to come in front of examiner or you will have to give the exam and you have not studying anything as you were busy in time traveling ) . but now you are two day 's in future and you only aged few min . so now you live longer at least 2 day 's haha hope you got what are the paradox of time travelling if you still do not understand then watch the documentary in the below given link on time travel that also in morgan freeman 's voice http://www.youtube.com/watch?v=tnkxi2va2ws
three eventual cases , depending on how much stuff ( of all kinds ) vs . inial expansion fills the universe . 1 ) too much stuff . big bang expansion slows , stops , reverses into the big crunch . 2 ) just the right amount of stuff . universal expansion asymptotically slows to a halt . a steady state universe is unstable to any perturbation , like a pencil standing on its sharpened point . 3 ) not enough stuff vs . total expansion . the universe expands forever into heat death . http://en.wikipedia.org/wiki/accelerating_universe http://www.preposterousuniverse.com/blog/2013/11/16/why-does-dark-energy-make-the-universe-accelerate/ http://www.eso.org/~bleibund/papers/epn/epn.html observation so far votes for ( 3 ) .
there is no working quantum theory of gravity , however one of the approaches physicists have played with is a sum over paths approach . in quantum electrodynamics you can describe the behaviour of e.g. an electron as a sum over all the possible paths that electron could take ( see the link for more info ) . in an analogous way you could describe the evolution of spacetime curvature as a sum over all possible spacetime curvatures . most of these paths through spacetime curvature will be well behaved , much like the well behaved spacetime we see around us . however some of the paths will contain esoteric structures like wormholes that allow closed time-like curves i.e. time travel . when you do a sum over paths you weight each path by a probability , and we expect the esoteric paths to have a very low probability so it is very unlikely you could encounter such a path and travel in time . however the probability is not zero . i am not familiar with this bit of hawking 's work , but from the book it appears he is taking a model spacetime that is relatively ( no pun intended ! ) easy to describe using a sum over paths , and using that as a model for a realistic spacetime . his calculation then gives the probability of a path that would take kip thorne back in time to meet his grandfather as 1 in $10^{10^{60}}$ . i would take this result with a pinch of salt ( as i am sure hawking would agree ) ! for a more detailed description see this article from the dept of applied mathematics and theoretical physics at cambridge . hawking was ( is ? ) a professor in this department .
this is not definitive , since the answer is always undefined , but let 's be cutesy . let 's let $u ' = a*c$ and $v = -a*c$ , where $a &lt ; 1$ then , $$\begin{align} u and = \lim_{a\rightarrow 1}\frac{v+u'}{1+ \frac{vu'}{c^2}}\\ and = \lim_{a\rightarrow 1}\frac{ac-ac}{1- \frac{a^{2}c^{2}}{c^2}}\\ and = \lim_{a\rightarrow 1}\frac{c-c}{-2a}\\ and = 0 \end{align}$$ now , the reason why this is not definitive is that you can take different limits , if you want . say , let $u ' = a^{2}c$ and $v = -ac$ then , $$\begin{align} u and = \lim_{a\rightarrow 1}\frac{v+u'}{1+ \frac{vu'}{c^2}}\\ and = \lim_{a\rightarrow 1}\frac{a^{2}c-ac}{1+ \frac{a^{3}c^{2}}{c^2}}\\ and = \lim_{a\rightarrow 1}\frac{c\left ( 2a -1\right ) }{3a^{2}}\\ and = \frac{c}{3} \end{align}$$ so , it is clear that , by taking the limit in different ways , you can get an arbitrary answer . it is not valid to choose an observer moving at the speed of light and then take velocities relative to that observer .
i had an extensive look around , and i turned up four conventions . this included a short poll of google , other questions on this and other sites , and multiple standards documents . ( i make no claim of exhaustiveness or infallibility , by the way . ) using $ [ q ] $ to denote commensurability as an equivalence relation . that is , if $q$ and $p$ have the same physical dimension $q$ , one might write $$ [ q ] = [ p ] = [ q ] , $$ but no bracketed quantity is ever shown equal to an unbracketed symbol . thus , if $v$ is a speed one might write $ [ v ] = [ l ] / [ t ] $ or $ [ v ] = [ l/t ] $ or $ [ v ] = [ l\ , t^{-1} ] $ or some equivalent construct . you can see $l$ and $t$ as denoting the dimension or just " some length " and " some time " . to see how you would work without evaluating braces , here is a proof that the fine structure constant is dimensionless : $$ [ \alpha ] =\left [ \frac{e^2/4\pi\epsilon_0}{\hbar c}\right ] =\frac{ [ f\ , r^2 ] }{ [ e/\omega ] [ r/t ] } =\frac{ [ f r ] [ \omega t ] }{ [ e ] }=\frac{ [ e ] }{ [ e ] } [ 1 ] = [ 1 ] , $$ so $\alpha$ and $1$ are commensurable . some examples are this , this , this , this , or this . using $ [ q ] $ to denote the dimensions of a quantity . thus if the physical quantity $q$ has dimension $q$ , one writes $$ [ q ] =q . $$ a velocity would then be written as $ [ v ] =l\ , t^{-1}$ or its equivalents . this seems to be the leading candidate on google , closely followed by the convention 1 . some examples are this , this , this , this , and this . this is my personal favourite , as i find that it permits the most flexibility without horribly formalizing the whole business ( though i will often skip the actual evaluation of the braces , essentially using convention 1 ) . using $ [ q ] $ to denote the units of a quantity . here if $q$ can be written as a multiple of some unit $\text q$ , you write $$ [ q ] =\text q . $$ this is contingent on what unit system you choose but different units for the same dimension are of course equivalent . when this approach is used , the notation $\{q\}=q/ [ q ] $ is sometimes used to denote the purely numerical value of the quantity . a speed would be written , for example , as $ [ v ] =\text m\ , \text s^{-1}$ . this use is endorsed by the nist guide to the si , section 7.1 , as well as the iso standard iso 80000 -1:2009 , section 3.20 . ( that document is very paywalled , but chapters 0-3 are available for free preview here google results seem relatively scarce , with this and this as examples , although that could simply be poor representation . ( there is also this document , which uses the notation $ [ \text w ] = [ \text v ] [ \text a ] $ , but i think this is quite uncommon as well as not very useful . ) using $\operatorname{dim} ( q ) $ to denote the dimensions of a quantity . this is the notation set as standard by the bureau international des poids et mesures in the si brochure ( 8th edition , chapter 1.3 , p . 105 ) . this also sets roman sans-serif as the standard for physical dimensions , so $\mathsf{q}$ would be the dimension of $q$ and you write $$\operatorname{dim} ( q ) =\mathsf q . $$ ( to typeset roman sans-serif in tex or mathjax , use \mathsf ; note that this is distinct from \operatorname , which is used for $\operatorname{dim}$ and would produce $\operatorname{q}$ through \operatorname{Q} . ) a real-world usage example is thus $\operatorname{dim} ( v ) =\mathsf l\ , \mathsf t^{-1}$ for a velocity . this use is set as standard by iso 80000-1:2009 , section 3.7 , and it is also endorsed by the nist guide to the si , section 7.14 . ( nist also reproduces the bipm text in p . 16 of the international system of units . ) examples of this online are this , this , this and this ; i note , though , that most examples i found are technical , while pedagogical examples tended to use conventions 1 and 2 . ( this also feels less common , but that is hard to judge . ) i also find it important to add that few academic journals impose standards in this area . as a working physicist in academia , the style guidance of one 's chosen journal is often the only style standard one is really obliged to follow . the style manuals of the american physical society , the institute of physics , reviews of modern physics , nature physics and several elsevier journals have no mention of which convention should be used in their publications . as was made clear in should we necessarily express the dimensions of a physical quantity within square brackets ? , the choice of what the symbol $ [ q ] $ means is entirely a matter of convention . the most important thing is that your usage is consistent . do not jump conventions within a document . if your work is closely allied to other resources ( e . g . textbooks ) that use a particular convention , it is best to stick to that , to avoid confusing your students . if you are presenting an exam , use the notations used in your course to avoid confusing your examiner , or - at the very least - define all non-standard notation you use . so , what convention should you use ? there is really no requirement to use any one of the above ( and you can even make up your own notation , as long as you define it appropriately and do not overdo it ) . this is really less of an issue than it looks , as there is actually rather rarely a need to use this notation in print except in pedagogic settings . ( that is not to say that professional physicists do not use it in practice : we do use it , often , in everyday life , but it is mostly informal work used on the side to keep calculations straight or as exploratory scaling arguments when starting work on a problem , for example . ) if your work is a commercial report , or similar document , and it could potentially have legal repercussions , then you should check whether there is a legal standard you should be using , which will then probably be conventions 3 and 4 . academically , you are typically free to choose the conventions you find most convenient as long as you use them properly and you avoid conflicts with other allied resources . if you are publishing in a journal or as part of a bigger work , you should check if they provide style guidance on this , though as i said journals rarely take stances on this . ( you should really be reading the style guidance anyway as part of your submission process , though . ) for your informal work , you should use whatever you are most comfortable with ! finally , if you have questions about the typesetting of these notations in latex , you should go to how should i typeset the physical dimensions of quantities ? on tex . se .
w.r. t your original question , yes , we would feel the gravitational effects for 10 years from now . changes in space-time propagate at the speed of light , so spacetime here would still be warped due to that star 's gravity even if it disappeared today .
suppose you are at a red light in your car . you apply newton 's second law on the street light . $$f=ma$$ $$f=0n , a=0ms^{-2}$$$$0n=0n$$ it works ! ! now the light turns green and you start accelerating . suppose your acceleration is $1ms^{-2}$ . according to you , you are at rest . do you see your nose moving ? apparently not . it means your body is at rest wrt you . so street light has acceleration $-1ms^{-2}$ wrt you . let 's apply newton 's second law . $$f=ma$$ clearly , there is no force acting on it . and the light , say , has mass=$50kg$ $$0n=-50n$$ nooooooooooooo . . . . . your mind just blew , right ? you see that you are unable to apply newton 's second law in an accelerating frame . let 's see how can we fix it . if we add $-50n$ on $lhs$ we will get the correct answer . hence , we define pseudo force as a correction term which enables us to apply newton 's second law in accelerating frames . it has no real existence , it is just a mathematical force . similarly , a centripetal force is needed to make you go in a circle . if you sit there , you have to apply a force outwards which we call centrifugal force , to use newton 's laws . centripetal force is a force which provides acceleration towards centre , say , tension while moving the object round with string . so if , you apply $f=ma$ from the revolving object , you have to add centrifugal force as the object is at rest wrt itself . you can explain what you experience while turning due to you inertia which resists you change in motion .
technically , there is one rule that does it all : the schroedinger equation for a system of n electrons and m nuclei . if you could solve that , you would know everything there is to know about your element . unfortunately , you cannot solve it exactly . there are some rules that will tell you which elements might want to form bonds with which other elements in what ratio . but even for " simple " things like the crystal structure , there is no simple rule : you will always have to do the math to find out that these elements form bcc-crystals and these other elements from fcc-crystals or whatever . finally : elements in the same column of the table tend to be have in a similar fashion , because they have the same number of chemically relevant valence electrons .
i have observed this as well , and experiment suggests it is because the dust is hydrophobic . if you splash a small amount of water gently onto the dusty surface you will see the water roll up to form beads that do not wet the surface . this is my rather crude attempt to illustrate what happens when you try and wet the dusty surface : the brown splodges are supposed to be the dust . because the water/dust interface has a high contact angle the water droplets cannot get to the surface underneath to wet it . hence it is difficult to wash the dust off the surface . you should find that if you use a detergent solution rather than just water the dust will wash off the surface . why the dust should be hydrophobic is not clear . apparently the rumour that dust is mostly skin cells is not true , which is a shame because it would neatly explain the hydrophobicity . skin is pretty hydrophobic , partly because of the sebum layer on it and partly because the dead keratinocytes have intrinsically hydrophobic cell walls . if anyone knows of definitive measurements of dust composition it would be interesting to see them ( googling finds many opinions about dust composition but little in the way of peer reviewed literature ! ) .
in general relativity , gravity effects anything with energy . while light does not have rest-mass , it still has energy---and is thus effected by gravity . if you think of gravity as a distortion in space-time ( a la general relativity ) , it does not matter what the secondary object is , as long as it exists , gravity effects it .
simple . you see that it is a quadratic equation in $t$ . $$ 0= -s + v_0t + \tfrac{1}{2}at^2 $$ solve for t . $$ t=\frac{-v_{0}\pm \sqrt{v_{0}^2-4 ( \frac{1}{2}a ) ( -s ) }}{a} $$ the fundamental theorem of algebra tells you should get exactly two solutions to the equation . chose the $t$ that is most likely to make sense
an interesting and horrifying possibility for your book could be that of a tiny black hole with negligible mass relative to the mass of the earth . it would silently sink into the ground , completely unnoticed . it would make damped oscillations around the center of the earth until eventually staying within the nucleus . there it would stay unnoticed , slowly growing like a parasite . the earth radius would start to diminish , and thus the crust would have to adapt by means of earthquakes - very weak at first , but of increasing frequency and strength . eventually , chains of volcanoes would appear along giant fault lines , heating and poisoning the atmosphere , bringing total obliteration . after that , the earth would continue shrinking until all that was left would be a tiny black hole with little more than one earth mass in the place where our planet was . and the moon would stay there as a horrified witness of the catastrophe . nothing on earth would be able to stop the process and save us . nothing except chuck norris . note : classical gr black holes are fully determined with just three values : mass , electric charge and angular momentum ( no-hair theorem ) . if the charge is large enough in the black hole of your novel , then the hero might be able to confine it by using strong magnetic fields .
the statement that the entropy increases because of collisions is incorrect . the conservation of phase space volume is a theorem of hamiltonian mechanics , and therefore applies to all known physical systems , regardless of whether they contain nonlinear forces , collisions or anything else . what actually happens is that although the phase space volume does not change as you integrate the trajectories forward , it does get distorted and squished and folded in on itself until the system becomes experimentally indistinguishable from one with a bigger phase space volume . the information that was originally in the particles ' velocity distribution ends up in subtle correlations between the particles ' motions , and if you ignore those correlations , that is when you get the maxwell distribution . the increase in entropy is not something that happens on the level of the system 's microscopic dynamics ; instead it occurs because some of the information we have about the system 's initial conditions becomes irrelevant for making future predictions , so we choose to ignore it . there is an excellent passage about this ( in a slightly different context ) in this paper by edwin jaynes , which gives a thorough criticism of the kind of textbook explanation that you mention . ( see sections 4 , 5 and 6 . ) it explains the issues involved in this much more eloquently than i can , so i highly recommend you give it a look .
photons do not have mass , but they do have energy and momentum . and since they can be absorbed or reflected , they can transfer their momentum to whatever it is that reflects or absorbs . the amount of energy is proportional to the frequency $\nu$ of the light : $e = h\ , \nu$ , where $h$ is planck 's constant . the momentum is $p = h\ , nu / c$ , in whatever direction the photon is traveling . the formula $m = m_0 / \sqrt{1-v^2/c^2}$ just does not work for particles traveling at the speed of light . that equation comes from the more general expression $e^2 = p^2\ , c^2 + m_0^2\ , c^4$ , where you make a substitution for $p$ that only works with massive particles . as for gravity , our best theory of gravity at the moment is general relativity . and the source of gravity is called the stress-energy-momentum tensor . basically , since photons do have energy and momentum , they act as a source for gravity . ( though usually such a tiny source that you can disregard them . )
photons always travel at $c$ ( not completely true , but a good simplification for this question 's purposes ) . common sense tells us that if person a running at velocity $v$ is chasing person b with velocity $u$ , the velocity of person b with respect to person a ( $w$ ) is : $$w=u-v$$ but our common sense is misleading , and this equation is only an approximation that works well at low velocities . special relativity tells us that the correct equation is actually : $$w=\frac{u-v}{1-uv/c^2}$$ so let 's say that someone 's running at velocity $v$ is chasing a photon traveling at $u=c$ with respect to the ground . the velocity of the photon with respect to the runner is : $$w=\frac{c-v}{1-cv/c^2}=\frac{c ( c-v ) }{c-v}=c$$ so the photon is still traveling at $c$ with respect to the runner , regardless of how fast he is running .
let there be given a $2n$-dimenional real symplectic manifold $ ( m , \omega ) $ with a globally defined real function $h:m\times [ t_i , t_f ] \to \mathbb{r}$ , which we will call the hamiltonian . the time evolution is governed by hamilton 's ( or equivalently liouville 's ) equations of motion . here $t\in [ t_i , t_f ] $ is time . on one hand , there is the notion of complete integrability , aka . liouville integrability , or sometimes just called integrability . this means that there exist $n$ independent globally defined real functions $$i_i , \qquad i\in\{1 , \ldots , n\} , $$ ( which we will call action variables ) , that pairwise poisson commute , $$ \{i_i , i_j\}_{pb}~=~0 , \qquad i , j\in\{1 , \ldots , n\} . $$ on the other hand , given a fixed point $x_{ ( 0 ) }\in m$ , under mild regularity assumptions , there always exists locally ( in a sufficiently small open darboux$^1$ neighborhood of $x_{ ( 0 ) }$ ) an $n$-parameter complete solution for hamilton 's principal function $$s ( q^1 , \ldots , q^n ; i_1 , \ldots , i_n ; t ) $$ to the hamilton-jacobi equation , where $$i_i , \qquad i\in\{1 , \ldots , n\} , $$ are integration constants . this leads to a local version of property 1 . the main point is that the global property 1 is rare , while the local property 2 is generic . -- $^1$ a darboux neighborhood here means a neighborhood where there exists a set of canonical coordinates aka . darboux coordinates $ ( q^1 , \ldots , q^n ; p_1 , \ldots , p_n ) $ , cf . darboux ' theorem .
it is because a fermion loop with $n$ vertices along the loop corresponds to , up to bosonic factors everywhere ( which never change the sign and mostly commute with everything else ) , $$ d = \langle \psi_1^\dagger \psi_1\cdot \psi_2^\dagger \psi_2 \cdots \psi_n^\dagger \psi_n \rangle $$ where each of the products $\psi^\dagger \psi$ comes from one vertex . note that vertices are what produces the $\psi$ factors because they appear in the interaction hamiltonian . however , in the feynman rules , we need to revisualize the diagram so that it is composed from propagators which are factors of the type $\psi_1\psi_2^\dagger$ and so on , geometrically corresponding to links between adjacent vertices . we can make this form manifest if we move $\psi_1^\dagger$ to the very end : $$ d = - \langle \psi_1\psi_2^\dagger\cdot \psi_2 \psi_3^\dagger \cdots \psi_n \psi_1^\dagger\rangle $$ in this form , we have a product of $n$ nice factors $\psi_i \psi_{i+1}^\dagger$ that may be attributed to propagators . however , i had to add a minus sign because we needed to permute grassmann-valued $\psi_1$ through $2n-1$ other fermionic operators and $2n-1$ is odd . therefore i had to correct the sign by $ ( -1 ) ^{2n-1}=-1$ for both expressions to be equal . i was a bit schematic so i did not indicate whether i used an operator formalism or the feynman path integral approach . the logic for the sign is the same in both versions . in the feynman path integral language , the $\psi$ objects in the demonstration above are grassmann numbers , not operators , so they strictly anticommute with each other .
zero strain does not always imply zero stress and visa versa . there are matterials that display stress-strain , $\sigma-\epsilon , $ hysteresis behaviour . in matterials like this , when you start loading them , they behave normally , i.e. increasing the stress increases the strain . however , when you start to unload them ( remove the load ) , instead of the stress becoming zero when the strain becomes zero , the matterial has some residual stress applied to it ! similary , if you repeat the cycle , although the stress becomes zero the strain retains a permanent value , i.e. the matterial remains permanently deformed ! these are very interesting elastic properties of such matterials . stress-strain hysteresis phenomena are very well known and are discussed extensively in literature .
well it has nothing to do with the higgs , but it is due to some deep facts in special relativity and quantum mechanics that are known about . unfortunately i do not know how to make the explanation really simple apart from relating some more basic facts . maybe this will help you , maybe not , but this is currently the most fundamental explanation known . it is hard to make this really compelling ( i.e. . , make it seem as inevitable as it is ) without the math : particles and forces are now understood to be the result of fields . quantum fields to be exact . a field is a mathematical object that takes a value at every point in space and at every moment of time . quantum fields are fields that carry energy and momentum and obey the rules of quantum mechanics . one consequence of quantum mechanics is that a quantum field carries energy in discrete " lumps " . we call these lumps particles . incidently this explains why all particles of the same type ( e . g . all electrons ) are identical : they are all lumps in the same field ( e . g . the electron field ) . the fields take values in different kinds of mathematical spaces that are classified by special relativity . the simplest is a scalar field . a scalar field is a simple number at every point in space and time . another possibility is a vector field : these assign to every point in space and time a vector ( an arrow with a magnitude and direction ) . there are more exotic possibilities too . the jargon term to classify them all is spin , which comes in units of one half . so you can have fields of spin $0 , \frac{1}{2} , 1 , \frac{3}{2} , 2 , \cdots$ . spin $0$ are the scalars and spin $1$ are the vectors . it turns out ( this is another consequence of relativity ) that particles with half integer spin ( $1/2 , 3/2 , \cdots$ ) obey the pauli exclusion principle . this means that no two identical particles with spin $1/2$ can occupy the same place . this means that these particles often behave like you expect classical particles to behave . we call these matter particles , and all the basic building blocks of the world ( electrons , quarks etc . ) are spin $1/2$ . on the other hand , integer spin particles obey bose-einstein statistics ( again a consequence of relativity ) . this means that these particles " like to be together , " and many of them can get together and build up large wavelike motions more analogous to classical fields than particles . these are the force fields ; the corresponding particles are the force carriers . examples : spin $0$ higgs , spin $1$ photons , weak force particles $w^\pm , z$ , and the strong force carriers the gluons , and spin $2$ the graviton , carrier of gravity . ( this fact and the previous one are called the spin-statistics theorem . ) now the interaction between two particles with " charges " $q_{1,2}$ goes like $\mp q_1 q_2$ for all the forces ( this is a consequence of quantum mechanics ) , but the sign is tricky to explain . because of special relativity , the interaction between a particle and a force carrier has to take a specific form depending on the spin of the force carrier ( this has to do with the way space and time are unified into a single thing called spacetime ) . for every unit of spin the force carrier has you have to bring in a minus sign ( this minus sign comes from a thing called the " metric " , which in relativity tells you how to compute distances in spacetime ; in particular it tells you how space and time are different and how they are similar ) . so for spin $0$ you get a $-$: like charges attract . for spin $1$ you get a $+$: like charges repel ! and for spin $2$ you get a $-$ again : like charges attract . now for gravity the " charge " is usually called mass , and all masses are positive . so you see gravity is universally attractive ! so ultimately this sign comes from the fact that photons carry one unit of spin and the fact that the interactions between photons and matter particles have to obey the rules of special relativity . notice the remarkable interplay of relativity and quantum mechanics at work . when put together these two principles are much more constraining than either of them individually ! indeed it is quite remarkable that they get along together at all . a poetic way to say it is the world is a delicate dance between these two partners . now why do atoms and molecules generally attract ? this is actually a more complicated question ! ; ) ( because many particles are involved . ) the force between atoms is the residual electrical force left over after the electrons and protons have nearly cancelled each other out . here 's how to think of it : the electrons in one atom are attracted to the nuclei of both atoms and at the same time repelled by the other electrons . so if the other electrons get pushed away a little bit there will be a slight imbalance of charge in the atom and after all the details are worked out this results in a net attractive force , called a dispersion force . there are various different kinds of dispersion forces ( london , van der waals , etc . ) depending on the details of the configuration of the atoms/molecules involved . but they are all basically due to residual electrostatic interactions . further reading : i recommend matt strassler 's pedagogical articles about particle physics and field theory . he does a great job at explaining things in an honest way with no or very little mathematics . the argument i went through above is covered in some capacity in just about every textbook on quantum field theory , but a particularly clear exposition along these lines ( with the math included ) is in zee 's quantum field theory in a nutshell . this is where i would recommend starting if you want to honestly learn this stuff , maths and all , but this is an advanced physics textbook ( despite being written in a wonderful , very accessible style ) so you need probably at least two years of an undergraduate physics major and a concerted effort to make headway in it .
the short answer is about less than a second . you can do a quick-and-dirty calculation by assuming that the mass of the polar ice caps ( roughly $10^{19}$ kg or about $\frac{1}{200,000}$ of earth 's mass ) . we expect then that this would slow earth 's rotation by that same amount ( because the mass flows from the poles to the equator ) : $$ \frac{1}{200,000}\cdot86000\ , {\rm s}\approx0.43\ , {\rm s} $$ you can do a more complicated solution involving rotational kinetic energy , $k=\frac12i\omega^2$ , but this will not change the answer significantly ( increases it to something like 2/3 a second ) than the quick and dirty solution .
charuhas got a nice point that gravity is always attractive because mass is always positive . there is a nice article in wiki for gravitational shielding . it has something clear . . . the term gravitational shielding refers to a hypothetical process of shielding an object from the influence of a gravitational field . such processes , if they existed , would have the effect of reducing the weight of an object . in case of electrostatic shielding , the charges redistribute themselves in an electric field , producing their own field which opposes the applied field . but in the case of gravity , you can not do that ( till now ) . whenever you place the massive object in the field , the objects attract towards each other and move towards the source of the field , which is not shielding but instead , attracting . though gravity is a long-range force , the field is much weaker than electrostatic attraction . i do not want to do much calculations , but a simple plugging shows that roughly , electric field is about $10^{40}$ times stronger than that of gravity . so , you can not achieve this effect in many possible ways . that maybe a reason why it is hypothetical . . .
this is a very nice question . there is indeed a simple way to see that a non-abelian theory will be of shorter range than an abelian one . the action of a gauge theory , generically contains terms of the form $ tr [ f f ] $ or $ tr [ f \star f ] $ , where $ f $ is the curvature or field strength of the gauge connection . for an abelian connection $a_\mu$ , the field strength is of the form : $$ f_{\mu\nu} = \partial_{ [ \mu}a_{\nu ] } $$ where the $ [ \dots ] $ represents antisymmetrization over the indices within the brackets . consequently the $f^2$ type terms in the action are of the form : $$ f^2 \sim ( \partial a ) ( \partial a ) $$ for a non-abelian connection $a_\mu^i$ , where $i$ is now an index in the lie algebra of some non-abelian group , we have : $$ f^i_{\mu\nu} = \partial_{ [ \mu}a^i_{\nu ] } + f^i_{jk} [ a^j_\mu , a^k_\nu ] $$ where $ f_{ijk} $ are the structure constants of the group in question . consequently the $ \mathcal{o} ( f^2 ) $ terms in the action now contain terms of the form : $$ ( \partial a ) a^2 \textrm{ and } a^4 $$ these are self-interaction terms which will , in general , endow the connection $a^i_\mu$ with a mass - in a suitable symmetry broken phase of the theory . and a massive gauge particle leads to a short range ( and/or confining ) interactions . that is the gist of it . there are likely other ways to approach the problem , but this is the one i am most familiar with . in response to some comments i would like to quote the following line from the jaffe-witten paper introducing the yang-mills problem as part of the clay math prize : " . . . one view of the mass gap in yang–mills theory suggests that it could arise from the quartic potential $ ( a \wedge a ) ^2$ in the action , where $ f = da + g a \wedge a $ , see [ 11 ] , and may be tied to curvature in the space of connections , see [ 44 ] . the reference [ 11 ] cited in the line above is a paper by feynman where he studies su ( 2 ) gauge theory in 2+1 dimensions and concludes the gauge invariance dictates the presence of a mass gap . one can argue about fixed points and phases and whatnot at different temperatures . but unless you have something that beats jaffe , witten and feynman i guess it is safe to conclude that @robert 's intuitive guess that the non-linear nature of non-abelian gauge theory is responsible for its short-range/massive/confining character is right on target .
ok , here 's a brief outline that is very math light ( and also somewhat oversimplified ) : the big idea is to compare the size of the lumpiness now to the size of the lumpiness back then when the cmb was generated , also called at the decoupling time . we measure the size of the lumpiness now by looking a galaxy superclusters and voids . we can then figure out the expansion rate of the universe , if we also measure the time and the distance back to the big bang ( decoupling ) . from einsteins gr we can convert the expansion rate , and also the sizes , distances and times into a curvature . ( we may also assume and use some other things such as approximate uniformity , and spherical symmetry . ) if you can truly explain all this , including the gr part , with a math-lite explanation , you are a much better man than i am , charlie brown .
i think of it this way : it takes exactly two vectors to define a plane . but a plane has a different set of symmetries than a vector does . if you have to shoehorn information about a plane into a vector , the closest thing to a unique vector is one normal to the surface . it so happens that the cross product is a relatively simple operation which produces this normal .
i find the phrase " acceleration need not be relative anything " to be awkward , but i can see where it comes from . for the moment restrict our consideration the galilean relativity ( just to keep the math simple ) . consider two frames of reference one ( $s$ ) in which the body is at rest and another ( $s'$ ) in which it moves with velocity $\vec{v&#39 ; _i} = \vec{u} = u \hat{z}$ . so we have the initial velocity of the body in frame $s$ as $v_i = 0$ , and $v&#39 ; = v + u \hat{z}$ now assume that the the body accelerates from time $t$ at acceleration $\vec{a} = a \hat{z}$ resulting in a velocity in frame $s$ of $\vec{v_f} = a t \hat{z}$ . compute the final velocity in frame $s&#39 ; $ as $v&#39 ; _f = v_f + u \hat{z} = ( u + a t ) \hat{z}$ , and from that the acceleration in the primed frame as $a&#39 ; = a$ . so the acceleration is the same in all frames ( you can check the cases for $a \not{\parallel} u$ yourself ) , and it is reasonable to say that accelerations are not relative anything . all of this is a consequence of the simple form of the transformation between frames : $$ \vec{x&#39 ; } = \vec{x_0} + \vec{u} t $$ $$ t&#39 ; = t $$ so what about einsteinian relativity ? here the transformation between frames is more complicated , and the math is much more complicated resulting in observers in different frames seeing different accelerations , but they will all agree on the acceleration as measured in the body 's own frame . in my opinion " the acceleration need not be relative " risks causing unnecessary confusion on these points . the magnitude and direction measured will measure in depend on the frame of the observer , which is often what is meant when people say " it is relative " .
the different tonality of a note in different instruments stems from the different mixes of amplitudes in the harmonic frequencies that the instrument provides . to be more concrete ( and keeping to a slightly simplified view ) , you play the a note ( 440 hz ) and then you have the harmonic frequencies 880 , 1320 , 1760 , . . . ( $440n$ where $n$ is the number of the harmonic ) . each of the frequencies will have an amplitude or " volume " contributing to the sound produced by an instrument . thus a particular set of amplitudes gives the instrument its tonality . this is highly related to the concept of fourier analysis used in many areas of physics .
should this have a homework tag ? if the radius of the cable is $r_0$ , the charge density $\rho = \alpha\delta ( r -r_0 ) $ . you need a constant of proportionality that makes the total charge per unit length be $\lambda$ . the total charge in length $l$ would be $q = \lambda l = \iiint \rho dx$ , where the integral is over length $l$ along the z axis . if you work out the integral , you should get an expression for $\alpha$ in the right units . then you need a relationship between $\lambda$ and $\sigma$ . that would be $\lambda l = \sigma 2\pi r_0l$
i seem to be answering a lot of questions about black holes at the moment ! in my answer to why is matter drawn into a black hole not condensed into a single point within the singularity ? i explained how matter falling into a black hole reaches the event horizon , and in my answer to why is a black hole black ? i showed how matter that reaches the event horizon can never escape it . so combining these two shows that once you have fallen into a black hole you can never escape . if you are prepared to restrict yourself to radially infalling matter then my answer to why is a black hole black ? shows that matter must hit the singularity because once inside the event horizon even light has a negative radial velocity i.e. even a light beam can not escape the singularity . however that is not the same as showing that once you have fallen into the black hole you are inevitably drawn into the singularity , because you might argue you could orbit the singularity . the proof that anyone falling into a black hole must hit the singularity was given by hawking and penrose . unfortunately my mission to make gr understandable hits a problem here because i do not understand the hawking penrose theorem so i certainly can not explain it to anyone else . to make life more interesting , if the black hole is electrically charged ( reissner–nordström metric ) or spinning ( kerr metric ) it is possible to find timelike paths that do not hit the metric but instead lead back out of the black hole into another region of spacetime . i even ranted on about this as well , see entering a black hole , jumping into another universe---with questions .
relativistic force is defined as $$\vec f = \frac {d} {dt} ( \gamma m_o \vec v ) = \frac {m_o\gamma^3} {c^2}\vec a\cdot\vec v + \gamma m_o\vec a$$ although generally different , this becomes the same as your expression when $\vec a$ is perpendicular to $\vec v$ giving $\vec a\cdot\vec v = 0$ .
looks like you are already familiar with the classical explanation but are still curious about the quantum version of it . 2 . phase difference between absorbed and emitted light yeah , this is essentially the lowest order contribution to the phase shift in the photon-electron scattering . here is the sloppy way to visualize it continuously ( this is basically the ' classical em wave scattering ' point of view ) : you can imagine that the " kinetic energy " ( -> frequency ) of the " photon " increases as it approaches the atom 's potential well and then it goes back to its normal frequency upon leaving the atom . this translates to a net increase in the phase ( $ ( n-1 ) \omega/c$ ) . " drift velocity " of photons ( they are not the same photons , they are re-emitted all the time ) by " drift velocity " do you mean a pinball-like , zigzag motion of the photon ? this will not contribute that much because it requires more scattering ( basically it is a higher order process ) . and also , i still do not really understand about the detail of the absorption-emission process . yes the absorption will still occur in all range of the frequency . the hamiltonian of the atom will be modified by the field ( by $- p \cdot e$ where p is the dipole moment of the atom and e is the electric field component of the light ) . this will give us the required energy level to absorb the photon momentarily , which will be re-emitted again by stimulated+spontaneous emission . edit : clarification , the term ' energy level ' is misleading , since the temporarily ' excited ' atom is not in an actual energy eigenstate . see the diagram here : http://en.wikipedia.org/wiki/raman_scattering
in an ideal conductor electrons are free to move . so when you apply an electric field to the conductor the electrons will feel a force $f=qe$ and start to move . this causes a charge separation which produces an electric field by itself . the net electric field is therefore a superposition of the external field and the field due to the charge separation . the electrons will continue to move until the net electric field inside the conductor is zero . note1: from this physical picture you can also infer that the charges will always accumulate on the surface of the conductor . note2: if you are wondering how the electrons know how to rearrange so that the net electric field is zero , just assume that the net electric field is non-zero . this causes a force $f=qe$ and the charge will separate along the electric field lines . this creates an electric field which is opposite to the external field that created the charge separation .
according to the generalized uncertainty principle , the planck length is , in principle , within a factor of order unity , the shortest measurable length – and no improvement in measurement instruments could change that . ( wikipedia ) $$1 \text{ planck length} = 1.61619926 \times {10}^{-35} m$$
actually , the quark and antiquark do annihilate with each other . it just takes some amount of time for them to do so . the actual time that it takes for any given pion is random , and follows an exponential distribution , but the average time it takes is $8.4\times 10^{-17}\ , \mathrm{s}$ according to wikipedia , which we call the lifetime of the neutral pion . what you have learned is a simplification , in fact ( it pretty much always is in physics ) . the actual state of a pion is a linear combination of the up state and the down state , $$\frac{1}{\sqrt{2}} ( u\bar{u} - d\bar{d} ) $$ this is how it is able to be its own antiparticle : there are not separate up and down versions of the neutral pion . each one is a combination of both flavors . the orthogonal linear combination , $$\frac{1}{\sqrt{2}} ( u\bar{u} + d\bar{d} ) $$ does not correspond to a real particle . ( in a sense it " contributes " to the $\eta$ and $\eta&#39 ; $ mesons , but i will not go into detail on that . )
the answer to your question depends on the context , but the basic unifying theme distinguishing different kinds of fields ( like vector fields , scalar fields , etc . ) is how these fields transform when they are acted on by lie groups ( and or lie algebras ) which falls under the mathematical subject of representation theory of lie groups and lie algebras . here are some examples to illustrate what i mean : consider a real field $v^\mu ( x ) $ defined on 4-dimensional minkowski space $\mathbb r^{3,1}$ . we say that this field is a lorentz 4-vector field provided when it is acted on by a lorentz transformation $\lambda = ( \lambda^{\mu}_{\phantom\mu\nu} ) $ ( an element of the lorentz group $\mathrm{so} ( 3,1 ) $ ) , it transforms as follows : $$ v^\mu ( x ) \to v'^\mu ( x ) = \lambda^{\mu}_{\phantom\mu\nu} v^\nu ( \lambda^{-1}x ) $$ now consider instead a field $\psi^a ( x ) $ defined on 4-dimensional minkowski space $\mathbb r^{3,1}$ . we say that such a field is a dirac spinor provided when it is acted on by lorentz transformations , it transforms as follows : $$ \psi^a ( x ) \to \psi'^a ( x ) = r_{\mathrm{dir}} ( \lambda ) ^a_{\phantom a b}\psi^a ( \lambda^{-1}x ) $$ where $r_{\mathrm{dir}}$ is called the dirac spinor representation of the lorentz group . there are also other kinds of spinors , all of whom transform according to different representations of different lie groups . hope that helps ! cheers !
the uncertainty principle , in the variance formulation , states that in any quantum state $|\rangle$ , the quantity $$\langle ( p-&lt ; p&gt ; ) ^2 \rangle \langle ( x-\langle x\rangle ) ^2\rangle \ge {\hbar^2 \over 4} $$ to understand why shifting p and x by their expected value and squaring gives the squared uncertainty , see this answer . the proof is by noting the following $$ |\langle \psi | \eta \rangle| \le \sqrt{ ||\psi||^2 ||\eta||^2}$$ this is the statement that the dot-product of two vectors is less than the product of their lengths . it is called the " cauchy schwartz inequality " . for the special case above , defining the operators $p= p-\langle p\rangle$ and $q=x-\langle x\rangle$ ( and squaring both sides ) , $$ ( \langle p q \rangle ) ^2 \le \langle pp\rangle\langle qq\rangle $$ where to see that the above is an instance of cauchy schwarz , take : $$ |\psi\rangle = p|\rangle$$ $$ |\eta\rangle = q|\rangle$$ while the product pq can be decomposed into a real and imaginary part $$ pq = {1\over 2} ( pq+qp ) + {1\over 2} ( pq-qp ) $$ the first part is imaginary , because if you take the hermitian conjugate , it changes sign . the second part is real ( this is ultimately because p and q are real , i.e. hermitian ) . the expected value of pq squared is the square of the imaginary and real parts separately $$ ( \langle p q \rangle ) ^2 = {1\over 4} ( \langle [ p , q ] \rangle ) ^2 + {1\over 4} ( \langle pq+qp ) \rangle ) ^2 $$ since both square things are positive , this means that the left hand side is bigger than one quarter the square of the commutator . the commutator is unchanged by the shifting , $$ [ p , q ] = [ p , x ] = \hbar $$ so that $$ \langle p^2 \rangle \langle q^2\rangle \ge ( \langle pq \rangle ) ^2 \ge {1\over 4} ( \langle [ p , q ] \rangle ) ^2 = {\hbar^2 \over 4} $$ the proof is usually given in one line , as directly above , where the cauchy schwarz step ( first inequality ) , the imaginary/real part decomposition ( second inequality ) and the shifted canonical commutation relations ( last equality ) are assumed internalized by the reader . this proof appears on wikipedia , it is used in all qm books , but perhaps this explanation is clearer .
energy is any quantity - a number with the appropriate units ( in the si system , joules ) - that is conserved as the result of the fact that the laws of physics do not depend on the time when phenomena occur , i.e. as a consequence of the time-translational symmetry . this definition , linked to emmy noether 's fundamental theorem , is the most universal among the accurate definitions of the concept of energy . what is the " something " ? one can say that is a number with units , a dimensionful quantity . i can not tell you that energy is a potato or another material object because it is not ( although , when stored in the gasoline or any " fixed " material , the amount of energy is proportional to the amount of the material ) . however , when i define something as a number , it is actually a much more accurate and rigorous definition than any definition that would include potatoes . numbers are much more well-defined and rigorous than potatoes which is why all of physics is based on mathematics and not on cooking of potatoes . centuries ago , before people appreciated the fundamental role of maths in physics , they believed e.g. that the heat - a form of energy - was a material called the phlogiston . but it is a long , long time ago when experiments were done to prove that such a picture was invalid . einstein 's $e=mc^2$ partly revived the idea - energy is equivalent to mass - but even the mass in this formula has to be viewed as a number rather than something that is made out of pieces that can be " touched " . energy has many forms - terms contributing to the total energy - that are more " concrete " than the concept of energy itself . but the very strength of the concept of energy is that it is universal and not concrete : one may convert energy from one form to another . this multiplicity of forms does not make the concept of energy ill-defined in any sense . because of energy 's relationship with time above , the abstract definition of energy - the hamiltonian - is a concept that knows all about the evolution of the physical system in time ( any physical system ) . this fact is particularly obvious in the case of quantum mechanics where the hamiltonian enters the schrödinger or heisenberg equations of motion , being put equal to a time-derivative of the state ( or operators ) . the total energy is conserved but it is useful because despite the conservation of the total number , the energy can have many forms , depending on the context . energy is useful and allows us to say something about the final state from the initial state even without solving the exact problem how the system looks like at any moment in between . work is just a process in which energy is transformed from one form ( e . g . energy stored in sugars and fats in muscles ) to another form ( furniture 's potential energy when it is being brought to the 8th floor on the staircase ) . that is when " work " is meant as a qualitative concept . when it is a quantitative concept , it is the amount of energy that was transformed from one form to another ; in practical applications , we usually mean that it was transformed from muscles or electrical grid or battery or another " storage " to a form of energy that is " useful " - but of course , these labels of being " useful " are not a part of physics , they are a part of the engineering or applications ( our subjective appraisals ) .
i sense that the mathematical derivation of the uncertainty principle may not be a sufficient explanation . . . : ) but if you are interested , look at the answer here : heisenberg uncertainty principle scientific proof think about how we look at something in macro space - we shine a light on it , such as with a microscope , and see what reflects back . in the case of a very small particle , this light could be as small as one photon - but one photon has enough momentum to push the object we are examining when they collide . so if we want to know where a particle is in space , we bounce a photon off of it - but this accelerates the particle , so now its momentum is " uncertain " ( or changed ) . likewise if we want to know what its momentum is - now its position has changed . whatever technique is used to measure a quantity ( photons are just an example ) , it necessarily has an impact on the subject of that measurement . thus it is impossible to measure these quantities simultaneously to a degree of precision which exceeds the uncertainty principle 's limitations .
with type ii supernovae , the photo-disintegration of iron produces helium and neutrons : $$ \gamma+\ , ^{56}{\rm fe}\leftrightarrow13^4{\rm he}+4{\rm n} $$ these helium atoms can then photo-disintegrate , producing protons and more neutrons : $$ \gamma+\ , ^{4}{\rm he}\leftrightarrow 2{\rm p}+2{\rm n} $$ the collapse of the star innitiates the process of neutronization ( generating neutrons from protons and electrons ) : $$ e^-+p\to n+\nu_e $$ ( note also that the electrons can bind to iron atoms to produce manganese and neutrinos : $e^-+\ , ^{56}{\rm fe}\to\ , ^{56}{\rm mn}+\nu_e$ , so there is lots of neutrinos made here ) . so clearly the driver of the energetic explosion are the neutrinos themselves . with type ia supernovae , however , the driver of the supernova energy is the decay of $\ , ^{56}{\rm ni}$: $$ \ , ^{56}{\rm ni}\to\ , ^{56}{\rm co}\to\ , ^{56}{\rm fe} $$ ( this reaction was recently confirmed with sn 2013j , which occurred in m82 ; prior to this discovery this was a purely theoretical result , albeit one that matched observed light curves ) . the decay of nickel-56 occurs via electron-capture : $$ \ , ^{56}{\rm ni}+e^-\to\ , ^{56}{\rm co}^*+\nu_e\to\ , ^{56}{\rm co}+\gamma $$ where the photons produced here are $\gamma$-rays . cobalt-56 then decays to iron-56 via : $$ \ , ^{56}{\rm co}\to\begin{cases} \ , ^{56}{\rm fe}+\gamma+\nu_e \\ \ , ^{56}{\rm fe}+\gamma+e^++\nu_e\end{cases} $$ so clearly there are neutrinos being produced ( and we should observe them , but detections of these events are unlikely as type ia 's are usually very far away from us ) , but it is not really the driver of the energetics as it is with type ii supernovae .
in hubble 's mind , the universe was a static frame of reference . you can define an absolute rest frame , no need for all this badabada relativity stuff . in this frame , galaxies are moving in straight lines at constant speeds . now , what i think is confusing you is the value of $h$ . as you have correctly derived , in this simplified model of the universe , $h=\frac{1}{t}$: the hubble parameter is changing over time ! we talk of the hubble constant because hubble 's original data reached only 2 mpc away , or 6.5 millions light years , that is a tiny fraction of the total age . so , even for the furthest galaxy he could observe , that was 6.5 million years old , the universe was pretty much as it is here today . you can see in this other question more details about the evolution of this parameters over time .
the most important thing to consider , which i do not see in your calculations at all , is the mass per surface area of the balloon . you are saying in the comments " the container is a magnetic field " , but unless you have a phyiscal container , how can you couple the payload to the bubble ? for the proposal to be reasonable , there needs to be a mechanism to couple the payload to the bubble of plasma , and a consideration of the mass of such a mechanism . also , there is no consideration in the proposal of the density of the plasma . what temperature and pressure will the plasma be , and what is the density of the plasma under such conditions ? in summary , the mass of the plasma and mass of the mechanism coupling a payload to the plasma need to be considered before you can even think about saying the balloon could lift a payload .
this concept has gotten consideration from nasa . in the nasa maglifter concept , a 300-600 miles per hour speed on a superconducting magnetic levitation track appoximately 2.5 miles long and going up a mountain to about 10,000 feet is proposed . the option of using a helium filled tunnel to reduce drag was also proposed .
classical mechanics is the study of second-order systems . the obvious geometric formulation is via semi-sprays , ie second-order vectorfields on the tangent bundle . however , that is not particularly useful as there is no natural way to derive a semi-spray from a function ( ie potential ) . lagrangian and hamiltonian mechanics are two solutions to that problem . while these formalisms are traditionally formulated on the tangent and cotangent bundles ( ie velocity and momentum phase space ) , they were further generalized : lagrangian mechanics led to the jet-bundle formulation of classical field theory , and hamiltonian mechanics to the poisson structure . the symplectic structure is a stripped-down version of the structure of the cotangent bundle - the part that turned out to be necessary for further results , most prominently probably phase space reduction via symmetries . it does not feature prominently in undergraduate mechanics lecture ( at least not the ones i attended ) because when working in canonical coordinates , it takes a particular simple form - basically the minus in hamilton 's equations , where it is used similarly to the metric tensor in relativity , ie to make a contravariant vector field from the covariant differential of the hamilton function . symplectic geometry also plays its role in thermodynamics : as i understand it , the gibbs-duhem relation basically tells us that we are dealing with a lagrangian submanifold of a symplectic space , which is the reason why the thermodynamical potentials are related via legendre transformations .
noether theorem is as valid in cm ( * ) as in qm ( ** ) . it deals with conservation laws and symmetries . in cm the variables are certain , in qm they may be uncertain . hup belongs to qm and gives a limitation on canonically conjugated variable uncertainties in a given state . if some variable in qm is uncertain , it does not mean its expectation value is not conserved . a superposition of free motions states $e^{ipr}$ is also a free motion state although the momentum , for example , may be uncertain . the dynamics of the momentum expectation value is determined with an external force , like in cm ( see the ehrenfest 's equations ) . no external force , no variation of the expectation value &lt ; p ( t ) > . so i do not see any relationship between hup and noether . ( * ) classical mechanics ( ** ) quantum mechanics
your question is not as straightforward as you might think , because if you drop an atom into a black hole it will fall freely and will not feel any force attracting it to the black hole . this sounds odd , but it is the same reason that astronauts in the international space station feel weightless even though they are being attracted by the earth 's gravity . a freely falling object does not feel any gravitational force . however an object falling into a black hole will feel tidal forces . this happens because one side of the atom is slightly nearer the black hole than the other side , so it is attracted more strongly . the end result is that the atom is stretched in the direction towards the black hole and squeezed in the direction at right angles to it is motion . this is known somewhat colourfully as spaghettification . so the question really is whether at some point the tidal forces get so strong that they tear the atom apart . presumably first the electrons would be torn off then the nucleus would be torn apart . well the key feature of a black hole is that classically ( i.e. . ignoring quantum theory ) the singularity at the centre of the black hole has zero size and the tidal forces at the singularity are infinite . this means that yes , at some point the tidal forces will get stronger than even the strong force and the atom will be completely torn apart . presumably even the protons and neutrons would be torn into separate quarks . but , and it is a big but , we almost certainly can not treat this classically because we expect quantum effects to become important that close to the singularity . exactly what will happen no-one knows because we do not have a theory of quantum gravity that can describe this situation . if string theory is the correct description at such small distances then presumably at around the string scale from the centre the atom would start behaving as a collection of seperate strings rather than the particles we are familiar with . response to comment : suppose you are hovering above a black hole and you let down a rope until it touches the event horizon . the rope will always be torn apart . this happens because all forces , electrostatic , strong and weak , propagate at the speed of light , and at the event horizon even the speed of light is not high enough to escape . so if this is the experiment you are thinking off , then once your charged particle has got sufficiently close to the event horizon you would not be able to pull it back out with a particle of the opposite charge . however this is not to do with the fundamental strength of the gravitational force . it is because one electron has a hard time resisting the gravitational force of an entire black hole . response to response to comment : if you are asking " is gravity stronger than the electromagnetic force " then you need to define what you mean . you are quite correct that at a black hole event horizon the electromagnetic force would not be enough to hold together an object that partially crossed the event horizon . in that sense gravity at the event horizon is stronger than the electromagnetic force , and indeed it is stronger than any force . but a physicist would not conclude gravity was stronger because you are not comparing like with like . to do a fair comparison you need to take two test particles with both charge and mass , put them some set distance apart and measure first the electrostatic force between them , then the gravitational force between them . whether you are near a black hole or not , this experiment will conclude that the gravitational force is much much weaker than the electrical force . if you conclude that gravity is stronger than electromagnetism at an event horizon all you are really saying is that the gravitational force due to the ( at least ) $10^{31}$ kilograms of a black hole is stronger than the electrostatic force from the $10^{-19}$ coulombs of charge on an electron . well , yes , that is not terribly surprising !
op wrote ( v1 ) : now if the ( poisson ) commutation of $\phi_m$ with $\gamma_m$ is non zero ( i.e. they both are second class ) does it terminate the ( poisson ) commutation chain there ? no , not necessarily . the rest of the $4\times 4$ matrix $\{\chi^i , \chi^j\}$ of the $4$ constraints still contains unspecified entries . finally , do not forget to check if there are tertiary constraints , quaternary constraints , etc .
the relation ( 4 ) literally switches the states , adds an overall complex conjugate , and removes a hermitian bar over the operator . ( actually , no one uses bars anymore to denote hermitian conjugates , they use daggers instead . and because stacked bars get ugly , i will use stars for complex conjugation of plain complex numbers . ) thus we have $$ \langle b \mid ( \alpha^\dagger ) ^\dagger \mid p \rangle = \left ( \langle p \mid \alpha^\dagger \mid b \rangle\right ) ^* = \left ( \langle b \mid \alpha \mid p \rangle^*\right ) ^* = \langle b \mid \alpha \mid p \rangle . $$ since this holds for any $\lvert b \rangle$ , $\lvert p \rangle$ , and $\alpha$ , this shows in a roundabout way that $\alpha^{\dagger\dagger} = \alpha$ for any $\alpha$ . i think what is confusing you is that you assumed such an obvious fact before dirac did , and you thought ( 4 ) meant " switch states , add an overall complex conjugate , and add a hermitian bar over the operator ( which may cancel with one already there ) . "
in general relativity the gravitational field is given by the metric tensor in space-time . the metric tensor is the solution to einstein’s field equations . it is a symmetric tensor with ten degrees of freedom . so fundamental excitation in quantum gravity must have spin two . in string theory the oscillations of the closed string includes a symmetric tensor state that can be identified with gravity as well as anti-symmetric tensor ( the kalb ramond field ) and a scalar field ( the dilaton ) . to get einstein’s gr field equations you have to find an effective low energy action that is fully compatible with the quantum conformal symmetry of the original closed string theory describing the massless bosonic excitations at the classical level . so the gr field equations are just the field equations obtained from the low energy massless bosonic action of closed string theory . this way you can even generate corrections to gr by going to higher level terms in the double curvature and string interaction expansion . this process of obtaining effective gravity equations is well described at an easily understandable level in maurizio gasperini’s string cosmology book . in that book is also explained how to describe strings in general curved backgrounds . about you third point . gravitons are usually described in the context of linear gr . in this description the metric tensor of space-time is separated in two parts the background metric and a fluctuating part that is then quantized . so you can visualize a graviton as a quantized oscillation describing the fluctuation of space time with respect to the background metric . so you can not say that the graviton is propagating in space time , the graviton is a quantized fluctuation of the space-time itself , not just propagating in it . in the context of string theory you can visualize the graviton as closed string states present in regions of fluctuating curvature of space-time . this is analogous to thinking about photons and electromagnetic waves . the em description is valid when the number of photons is high . you can say that a region in the wave with large amplitude is rich in photon states . gravitons closed string states and curvature can be seem the same way . a region of space where curvature is high is rich in closed string states that are a part of spacetime itself . this is a simplified and rough version of what gravity waves are in string theory .
be careful to distinguish correctly between lab frame and particle frame . in the particle frame $$\text{$\delta $t}=10^{-6}\sec$$ in the lab frame $$\text{$\delta $t}_l=\frac{\text{$\delta $x}}{v}$$ finally the relation between the time lapse in the particle frame and the lab frame is $$\text{$\delta $t}_l=\text{$\gamma \delta $t}=\frac{\text{$\delta $t}}{\sqrt{1-v^2/c^2}}=\frac{\text{$\delta $x}}{v}$$ solving for v yields $$v=\frac{c \text{$\delta $x}}{\sqrt{c^2 \text{$\delta $t}^2+\text{$\delta $x}^2}}$$
in your example , the relevant speed is not the speed of propagation of disturbances in the magnetic field , but rather the speed of the alignment of iron atoms . you are really asking " does magnetization of a wire/metal propagate at the speed of light ? " the answer is no ; it propagates at the speed at which each individual iron atom can align its polarity . if you are asking , " do changes in the magnetic field propagate at the speed of light ? " the answer is yes ; if a giant , huge , powerful magnet appeared one light year away out of nowhere , then it would take exactly one year for magnets on earth to feel its pull ( however small it may be ) . that is , it would take one year for the " magnetic force " to reach the earth .
the definition $m^2=p^\mu p_\mu$ is generally valid . by the correspondence principle , the definition of mass in relativity has to reduce to the newtonian one in the nonrelativistic limit , and the definition of mass in quantum mechanics has to reduce to the newtonian one in the classical limit . but in quantum mechanics , i think none of these definitions should be valid ( newton 's second law does not hold , gravity can not be described in qm and qm is not relativistic ) . newton 's second law holds in the classical limit of quantum mechanics , and that is enough to define the mass of any given object . for example , in the stewart-tolman effect , we are dealing with a large number of electrons , and large particle numbers are one way in which the classical limit can be obtained from quantum mechanics . once the mass of an electron has been established by such a classical technique , it is established in quantum mechanics . it is not really true that gravity can not be described in quantum mechanics . quantum mechanics does fine with gravitational fields , just not with spacetime curvature . ( by the equivalence principle , you can have a gravitational field in flat spacetime . ) for example , people have done neutron interferometry in a gravitational field ( colella 1975 ) and gotten exactly the results you had expect from freshman quantum mechanics , without having to do any quantum gravity . this means that even in a quantum mechanical context , we can verify that inertial and gravitational mass are equivalent . maybe in general relativity , i have read that there are problems defining energy in a localised point , that we can only evaluate the total energy actually even the total energy can be impossible to define in gr . but that does not stop you from , e.g. , defining the mass of an electron or a galaxy . there are conserved , scalar measures of mass that can be defined in any asymptotically flat spacetime . the real issue in relativity , even in sr , is that mass is not additive . also , in gr , it is not mass that is the source of gravitational fields , it is the stress-energy tensor . colella , overhauser , and werner , phys . rev . lett . 34 ( 1975 ) 1472
there is a spacecraft moving with velocity $v = 0.85c$ in a frame of reference where , helpfully , someone has put out ' mile ' posts exactly 1 light-second apart ( in that frame ) , along the direction of the spacecraft 's travel . in this frame of reference , the spacecraft just passes 85 mile posts in 100 seconds according to clocks at rest in this frame . however , according to the spacecraft , the mile posts are less than 1 light-second apart ( length contraction ) and , in fact , measure $$\sqrt{1 - ( 0.85 ) ^2} = 0.527 . . . $$ light-seconds apart . so , even though the spacecraft 's clock reads an elapsed time of just about 52.7 seconds ( time dilation ) when passing the "85" mile post , the distance covered , according to the spacecraft , is just $$d = 85 \cdot 0.527$$ light-seconds and thus , as must be the case , the spacecraft observer calculates a relative speed of $0.85c$ too .
image distortions you describe are because of the mirror being slightly bent . with sizes you mention ( very elongated ) the main reason would be the frame not rigid enough and deformed while being stored or transported . also when you leave them resting onto the wall they will bend more and this will make the distortion slightly worse , so hanging them onto wall would be advantageous , but is not likely to fix the problem completely . frame rigidness can be fixed by enforcing the frame - the first thing that comes to my mind is attaching some thin rigid hollow metal bars on the frame but if you are interested you can ask on diy se .
the lie algebras and $\mathfrak{so} ( 3 ) $ $\mathrm{su} ( 2 ) $ are isomorphic , but the lie groups $\mathrm{so} ( 3 ) $ and $\mathrm{su} ( 2 ) $ are not . in fact $\mathrm{su} ( 2 ) $ is the double cover of $\mathrm{so} ( 3 ) $ ; there is a 2-1 homomorphism from the former to the latter . how is this possible when every so ( 3 ) irrep can get raised to one of so ( 3 ) as described above ? the half-integer irreps of $\mathfrak{so} ( 3 ) $ do not have corresponding $\mathrm{so} ( 3 ) $ irreps , but they do have corresponding $\mathrm{su} ( 2 ) $ irreps . when you try to exponentiate the half-integer irreps , you do not get a representation of $\mathrm{so} ( 3 ) $ . an explanation of why we miss physically relevant stuff when we consider only $\mathrm{so} ( 3 ) $ and not its double cover is given here : idea of covering group
it is surprisingly hard to explain in simple terms why nothing , not even light , can escape from a black hole once it has passed the event horizon . i will try and explain with the minimum of maths , but it will be hard going . the first point to make is that nothing can travel faster than light , so if light can not escape then nothing can . so far so good . now , we normally describe the spacetime around a black hole using the scharwzschild metric : $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2 + \left ( 1-\frac{2m}{r}\right ) ^{-1}dr^2 + r^2 d\omega^2$$ but the trouble is that the schwarzschild time , $t$ , is not a good co-ordinate to use at the event horizon because there is infinite time dilation . you might want to look at my recent post why is matter drawn into a black hole not condensed into a single point within the singularity ? for some background on this . now , we are free to express the metric in any co-ordinates we want , because it is co-ordinate independant , and it turns out the best ( well , simplest anyway ! ) co-ordinates to use for this problem are the gullstrand–painlevé coordinates . in these co-ordinates $r$ is still the good old radial distance , but $t$ is now the time measured by an observer falling towards the black hole from infinity . this free falling co-ordinate system is known as the " rainfall " co-ordinates and we call the time $t_r$ to distinguish it from the schwarzschild time . anyhow , i am going to gloss over how we convert the schwarzschild metric to gullstrand–painlevé coordinates and just quote the result : $$ds^2 = \left ( 1-\frac{2m}{r}\right ) dt_r^2 - 2\sqrt{\frac{2m}{r}}dt_rdr - dr^2 -r^2d\theta^2 - r^2sin^2\theta d\phi^2$$ this looks utterly hideous , but we can simplify it a lot . we are going to consider the motion of light rays , and we know that for light rays $ds^2$ is always zero . also we are only going to consider light moving radially outwards so $d\theta$ and $d\phi$ are zero . so we are left with a much simpler equation : $$0 = \left ( 1-\frac{2m}{r}\right ) dt_r^2 - 2\sqrt{\frac{2m}{r}}dt_rdr - dr^2$$ you may think this is a funny definition of simple , but actually the equation is just a quadratic . i can make this clear by dividing through by $dt_r^2$ and rearranging slightly to give : $$ - \left ( \frac{dr}{dt_r}\right ) ^2 - 2\sqrt{\frac{2m}{r}}\frac{dr}{dt_r} + \left ( 1-\frac{2m}{r}\right ) = 0$$ and just using the equation for solving a quadratic gives : $$ \frac{dr}{dt_r} = -\sqrt{\frac{2m}{r}} \pm 1 $$ and we are there ! the quantity $dr/dt_r$ is the radial velocity ( in these slightly odd co-ordinates ) . there is a $\pm$ in the equation , as there is for all quadratics , and the -1 gives us the velocity of the inbound light beam while the +1 gives us the outbound velocity . if we are at the event horizon $r = 2m$ , so just substituting this into the equation above for the outbound light beam gives us : $$ \frac{dr}{dt_r} = 0 $$ tada ! at the event horizon the velocity of the outbound light beam is zero so light can not escape from the black hole . in fact for $r &lt ; 2m$ the outbound velocity is negative , so not only can light not escape but the best it can do is move towards the singularity .
if you need to produce the antimatter from ( e . g . electric ) energy – e.g. antiprotons may be produced from collisions of proton pairs at the lhc ( most of the initial , " invested " energy is the kinetic energy of the protons there ) – then you are obviously not getting an economic source of energy because you are converting energy to mass and back which would be ideally energy-neutral except that the efficiency can not be 100 percent so you are losing energy , after all . in particular , the lhc consumes as much energy as the nearby city of geneva and the energy you may get from the produced antimatter is surely not enough to pay the energy debt – it is lower by many and many orders of magnitude . the annihilation would of course be a tremendous source of energy if we found some " antimatter for free " and if we could harness it : one would get 1,000 times energy from a kilogram of fuel than from nuclear fission power plants . that is what $e=mc^2$ implies . this is of course a speculation . we know that there is no significant reservoir of antimatter left e.g. in the milky way because it would have already annihilated with the ordinary matter which is pretty much everywhere , in low densities that are enough . in fact , the antimatter and matter used to be almost balanced , 1 billion and 1 of matter 's mass versus 1 billion pieces of antimatter . matter won the bloody conflict so some baryons survived which is why we and the stars are here today . but that is a long time ago . we can not find any cheap antimatter around anymore . but that does not mean that the " antimatter fuel " is impossible in principle . it is exactly as possible in principle as the nuclear fuel and it would be 1,000 times more effective . an antimatter bomb designed to end the world war could weigh just grams etc . the situations are completely analogous . but by trying to produce the fuel that we can not find in our environment , you are missing the point . it is a similar " clever idea " as if you wanted to produce uranium for nuclear power plants if you did not have it to start with . that is pretty clearly economically unreasonable , is not it ? the case of antimatter is completely analogous . the fact that we do not have enough of this " fuel " around does not mean that it is not the most concentrated " fuel " that could exist . it is . getting $e=mc^2$ of energy from a unit of mass – by complete annihilation – is the maximum you can get out of it .
you are asking two separate questions . to take your second question first , the existance of seven extra spacelike dimensions is a requirement for the consistency of string theory and we have no experimental evidence that extra dimensions exist or that string theory is a good description of reality . so it is impossible to make any definitive comment about why we have not observed the seven extra dimensions . to get back to the question we can answer , modern physics treats spacetime as a four dimensional manifold equipped with a metric . four dimensions just means we need four numbers to specify the location of a point in spacetime . two points are identical if and only if all four numbers are the same . note that so far we have not said anything about time - only that there are four dimensions . the metric is what gives a notion of distance to our manifold . it allows us to calculate the distance between any two points in our four dimensional spacetime along a curve . to do the calculation we need to choose a coordinate system , that is we choose four vectors that we use to mark out spacetime . for example these vectors could be $t$ , $x$ , $y$ and $z$ , or polar coordinates $t$ , $r$ , $\phi$ and $\theta$ , or even a coordinate system like kruskal-szekeres $u$ , $v$ , $\phi$ and $\theta$ that does not correspond to anything humans can observe . the metric assigns a sign to each vector . the details are involved but basically one coordinate always has a negative sign while the other three have a positive sign . the vector with the negative sign is timelike while the ones with the positive signs are spacelike . nb i have used the word timelike not time because you can have coordinate systems where the timelike vector is not time is the sense we normally use the word . for example in kruskal-szekeres coordinates the ${\bf v}$ coordinate is the timelike one but is not simply time . the point of all this is that what we think of as time is not a unique way to define coordinates for spacetime and it is not special ( well not mathematically - it is obviously special for humans ) . so there is no reason to treat the time coordinate differently from the three space coordinates . that is why we consider it a dimension like the other three dimensions .
i do not know about the first part of your question , i think gas centrifuge specifications and capabilities may be classified which would make a reasonably exact estimate quite hard . the fat man nuclear bomb ( 20kt-yield , implosion design ) required 6.2kg of plutonium ( or , if the country is unwilling to take the extra step of converting its uranium to plutonium in nuclear reactors , about 13kg of highly enriched uranium would give off a similar yield ) . nuclear bombs can be designed to detonate based on any number of conditions ( specific atmospheric pressure , sudden acceleration , manual command , etc . ) and using a timer to trigger the bomb would be very simple . the timer would send an electric signal to the control circuits for the bomb , which would then trigger fast , synchronized pulses to special detonators ( often exploding-bridgewire detonators ) . the detonators would initiate a layer of conventional high explosives , which would send a shockwave to compress a core of uranium or plutonium until it reaches the critical density necessary to sustain a chain fission reaction , at which point it explodes violently , releasing a lot more energy ( several orders of magnitude ) than given off by the initial ( conventional he ) explosion .
''i am basically asking how to refute the line " if it cannot be tested experimentally then it is not science/physics " '' this is the argument of those believing that popper ( cf . http://en.wikipedia.org/wiki/karl_popper ) said the last word on the matter what science is . however , his views are not the only possible view on the matter ; see the section on ''criticism'' in the above wikipedia link . in particular , there is another influential view about what science is , by thomas kuhn , which is far less narrow than popper 's . as regards string theory , some answers to your question might be here : what differs string theory from philosophy or religion ?
yes , it is possible . for example kevin brown did here and here including this table . so for the xkcd problem the answer is $-\frac{1}{2}+\frac{4}{\pi} \approx 0.773$ .
my favourite is optics by hecht and zajac , though i am not sure it is still in print . this book was not one of those suggested in the two duplicate questions .
your example is an example of a microstate in your sample , which is a statistical ensemble and its entropy is defined by the number of microstates possible . where s is the entropy and k is the boltzman constant and ω is the total number of microstates . one of these microstates you show contributes to the ω as one state . it will immediately flow into another one as the kinematics unfreeze from your frozen picture . so , no it is not an example of an entropy decrease . to decrease entropy in an open system is possible . consider crystals drawn out of a liquid phase . there is energy given up and the crystal appears ordered from disorder . the entropy of liquid+ crystal , in a closed system , increases . live organisms continually decrease the entropy of their system , using chemical properties to release energy to the environment and increase the total entropy .
just wiki it . anyways i will give you a oneliner from wiki itself- absolute pressure is zero-referenced against a perfect vacuum , so it is equal to gauge pressure plus atmospheric pressure . gauge pressure is zero-referenced against ambient air pressure , so it is equal to absolute pressure minus atmospheric pressure . negative signs are usually omitted . differential pressure is the difference in pressure between two points .
a rotating fluid roughly observes a $v=r\omega$ law where $v$ is the velocity of a fluid element or anything moving with the fluid element , $r$ is the distance from the centre and $\omega$ is the roughly constant angular rotation rate . so if a large particle like a tea leaf residue is found in the fluid , it should rotate at a speed $v$ , given a particular distance . however , such large particles will be more suspect to frictional forces by the viscous tea that reduce the velocity of the particle very quickly . so in order to satisfy $v=r\omega$ , it has to collect at the centre of the cup once its velocity is completely reduced to zero . there is yet another reason why such a particle should float to the centre , but i suspect it has a much smaller contribution . in the rotating frame , there is a centrifugal force acting outward on the large particle . let 's ignore coriolis forces and look at the effect of only this centrifugal force . now locally , in a small region around this particle , this is analogous to a gravitational field acting along the surface of the fluid outward to the edge of the cup ( the equivalence principle ) . you had expect the particle to move towards the edge then , but remember that this tea leaf particle is less dense than tea ( it floats ) and so it should feel a force opposite in direction to the imaginary gravitational field ( the centrifugal force ) , just like a tea leaf particle in a stationary fluid should float upwards against a real vertical gravitational field . this sort of behaviour is counter-intuitive , but it is actually observed in centrifuges where the suspended particles are less dense than the fluid -- they collect in the center . if you want to check this for yourself without a centrifuge ( or if you suspect that friction has a greater role in this ) , take a helium balloon into a train and you will observe that when the train brakes , while you move forward because of the non-inertial force , the balloon actually inclines backwards ! ( if the braking is long enough for the air to settle and there are no air currents in the cabin -- agreed , it is not the easiest thing to test ) see here for a more detailed explanation of the balloon example : http://physics101.wordpress.com/2008/11/30/balloons-and-accelerations/
here are some intuitive arguments ; i believe they are valid but it is been a very long time since i thought about these things - so i am open to comments / improvements . the simple description of doppler cooling : because of the detuning , the probability of a photon absorption is greatest for an atom moving towards the light source - and in the process of absorbing the photon , the atom loses momentum in the direction it was going . when it re-emits the photon , it will be in a random direction ; therefore there will be no net impact on the momentum of the atoms . this is sufficient to see that the atoms " slow down " in the beam direction , but not enough to see that they lose energy in total . to see that the energy is lower , imagine that the atom had an initial momentum $2p$ where $p$ = momentum of the photon . after the absorption , the atom has momentum $p$ . now if we re-emit the photon at 90 degrees to the original direction , the net momentum is $\sqrt{2}p$ and thus the kinetic energy of the atom , which scales with momentum squared , is lower than it was . only if the photon is re-emitted in the original direction do you see no net change in energy . this does beg the question : what happened to the energy ? it appears that we did no work on the system - the same amount of energy came in ( one photon ) as went out ( same photon , barring tiny relativistic effects that are much smaller than the drop in kinetic energy of the atom ) . i do not think that the shift in the wavelength explains this . i suspect this is where entropy comes in : since this process actually decreases the entropy of the system , we must have done work on the system : $\delta u = -\delta s\cdot t$ . so energy needed to reduce the entropy was taken from the kinetic energy of the atom .
because spacetime includes both multiple points in space and multiple moments in time , you have to think of a particle as a line ( or a 1d curve ) through spacetime , not a point . the line is called the world line . it is made up of all the $ ( x , t ) $ points at which the particle exists : in other words , if you , as an external observer , measure the particle 's position $x$ ( using your own rulers ) at a bunch of different times $t$ ( using your own clock ) , and plot all those points on a graph , and connect them , you get a world line . you could say that the world line itself is fixed in spacetime . but that is not a particularly useful statement to make , because there is no separate time outside of spacetime , so it is not like the world line can actually move . besides , even if you did say the world line is fixed in spacetime , that applies equally well to massive particles and to photons . you can pick any two points on a world line and figure out how much time elapses , by your clock , between those two points . it is just $t_2 - t_1$ . you can also figure out how much time elapses , by some other observer 's clock , between those two points : it is $\frac{t_2 - t_1}{\sqrt{1 - v^2/c^2}}$ , where $v$ is the relative velocity between you and the other observer . and you can even try to apply this calculation treating the particle itself as the observer , which lets you figure out how much time passes for the particle between those two points . it works out to $\sqrt{ ( t_2 - t_1 ) ^2 - ( x_2 - x_1 ) ^2/c^2}$ . this works just fine for a massive particle . but for a photon , no matter which two points you pick , the answer you get is zero . this is why we say that photons do not experience time . i do not think it is accurate in general to say that photons are fixed in time , though . that might be a useful thing to say to make a particular point in a particular argument , but in most cases , it is probably more misleading than not .
try to read this paper accurate masses and radii of normal stars : modern results and applications . g . torres , j . andersen and a . gimenez . astron . astrophys . rev . 18 no . 1-2 , pp . 67-126 ( 2010 ) , arxiv:0908.2624 . you can find links to systems with orbits and accurate fundamental parameters there . some of them calculated on the base of third kepler 's law . see also : fourth catalog of interferometric measurements of binary stars , us naval observatory , 2012 , and the washington double star catalog , us naval observatory , 2012 . where you can request data for orbit plotting . maybe my question how to plot orbit of binary star and calculate its orbital elements ? at astronomy . se will help you too .
one of the best answers to your question is due to the painter rené magritte : http://www.wikipaintings.org/en/rene-magritte/the-treachery-of-images-this-is-not-a-pipe-1948 . it says : " this is not a pipe . " there are several ways for interpreting that statement , questionning the language , the image , or the role of representations . another answer is given by juliet : " what is in a name ? that which we call a rose by any other name would smell as sweet " . http://en.wikipedia.org/wiki/a_rose_by_any_other_name_would_smell_as_sweet the story does not say whether juliet actually smoked the pipe . the point is that notation , as well as language , is a pure matter of convention for communication with others , as well as with oneself . depending on established conventions and cultural commonalities , some choices may be better than others . it may also depend on the intended audience willingness to depart from common usage and make some effort to understand you . parents will do that with children . examiner may be less willing to do it with students who depart from well established notations or conventions . ( what is a " well established notation or convention " ? ) this say , the topic is scientifically very important , as science requires precision in statement . not just quantitative precision , but also precision in concepts . and sciences develops through communication between scientists . some of the progress in science is due to improved notation . the use of a symbol for zero , attributed to indian mathematicians is the most cited example . the development of positional numeration , such as the decimal system , depends on it . other progress is also due to evolution of concepts , of new views of existing knowledge , rather than new knowledge itself . but to keep your concepts clear , just watch out what is in the pipe .
partially coated ( silvered ) mirrors could do the job , if you have got some radiation of high intensity . for example , a laser . the intense beam comes out through the partial-mirror after a series of constructive reflections . wiki does not have a good article on one-way windows . but , googling it can throw a lot of results . these one-way mirrors are imaginary . say you are in a car and i am out . well , most of the cars today have these semi-reflective mirrors . if i try to peek into the car from quite a distance , i can not see you . but if i come closer enough ( or stick to the glass ) , i can see who is hiding inside . but , this should not happen with a perfect one-way mirror . i should not see you at all . but , you can see me completely . hence , we conclude that this is not practically allowable as it would violate second law of thermodynamics . ( perhaps , there is a hiding answer in - why does a window become a mirror at night ? )
not a complete answer , but hope it will help . have you heard about sklogwiki ? among others it has a nice article about lennard-jones model . the thing is that you are not dealing with argon , instead you are dealing with a model lennard-jones fluid . so it is wrong to suggest that phase transitions of the two perfectly coincide . molecular potential assumed to be the true potential of argon is referred to as aziz potential ( " a highly accurate interatomic potential for argon " , paywalled ) . i would suggest you to compare your results with ones already known for lj fluid , for example sklogwiki links to a work by jp hansen , l verlet " phase transitions of the lennard-jones system . " apart from this i have a couple of remarks . you mention " states above critical point " --- there is no gas or liquid above the critical point , it is a state known as " supercritical fluid " . it is normal you can not distinguish vapor from liquid there . the other thing i would be concerned of is the coexistence of vapor and liquid in the simulation . if it takes place everything is wrong . however i have never encountered this and i do not know how to diagnose such a nasty case .
think about the free body representation . if you have a mass on the inclined plane at some angle $\theta$ , the weight ( mg ) is in the -y direction . the normal force points normal to the surface , which means it will have both an x and a y component . through some simple trig , we can see that $f_{normal}=mg cos ( \theta ) $ . this is not completely mg , because the other component ( $mg sin ( \theta ) $ ) is the force responsible for the mass sliding down the inclined plane . so i am not completely sure what you mean by a normal force cancels a normal component of m g but there are different components to each .
there is not a simple answer to your question . the scaling will be different in different situations . let 's take your example of gravity . the acceleration is given by : $$ a = g \frac{m}{r^2} $$ so $a$ scales as mass$^1$ and distance$^{-2}$ . but consider some other quantity like the orbital period , which is given by : $$ t = 2\pi \sqrt{\frac{r^3}{gm}} $$ then $t$ scales as mass$^{-1/2}$ and distance$^{3/2}$ . the only way to work out what the scaling will be is to sit down with a piece of paper and write down the relevant equations for the property you are interested in .
first , there is no universal inequality that would say that materials have to be " paramagnets " . the opposite effects imply that materials may also be " diamagnets " which means that they react oppositely to the magnetic field . i think that atoms and molecules are the smallest objects whose response to the magnetic field may be viewed as the microscopic cause of diamagnetism and paramagnetism . it usually boils down to the electrons ' motion but one could not analyze the magnetic behavior for the electrons in isolation . second , one can not talk about the reaction to uniform weak-magnetic and strong-magnetic fields because there are not any . at least today , the electroweak symmetry is broken which means that the " weak-electromagnetic waves " are as short as the wavelength of the w-boson or z-boson which is $10^{-18}$ meters or so ; and they never quite move by the speed of light . no atom - and even no actual particle we know - may be squeezed to shorter distances or studied to shorter distances . this prevents us from talking about responses to uniform fields - simply because there are no uniform fields . analogously , the " strong electromagnetic fields " are non-uniform and variable because of a different reason in this case : the equations describing them are non-linear . there may be domains of size $10^{-15}$ meters or so where the " strong magnetic fields " may be aligned . see is the color gauge group spontaneously broken in qcd by the savvidy vacuum ? but these domains are still too small to be studied as a uniform field in which other particles may peacefully inserted and analyzed . the strong and weak nuclear interactions create such a frantically changing environment that they are almost universally studied and described in terms of particle collisions . there are no peaceful macroscopic uniform fields ; the latter is only possible for long-range forces such as electromagnetism and gravity .
tl ; dr teleportation plays an important role in the theoretical analysis of quantum cryptography . most of the security proofs work by reducing the considered quantum cryptography protocol , which may or may not be entanglement based , to an equivalent entanglement based protocol , which uses quantum teleportation . theoretician 's teleportation based qkd protocol let 's start with a " theoretician 's quantum key distribution ( qkd ) protocol " , which is involved , impractical , but the security of which is easy to prove . this protocol would would work as follow : alice and bob share many pairs of entangled particles they select randomly a subset of them and make local measurements to check that their particles are in a state close to maximally entangled state . ( if not , they abort ) alice then encodes a random string in one set of particles , for example in the rectilinear basis of the polarization of photons she teleports the particles to bob , which measures the particles and get the secret information ( the random string ) because of step 2 . alice and bob knows that the entangled state is close to a maximally entangled state , and because of the monogamy of entanglement , that this state cannot be correlated with anything else , in particular with anything under eve 's control . the global quantum state is said to be factorized ( i.e. . under the form $\left|\phi^+\right&gt ; _{ab}\otimes\left|\psi\right&gt ; _e$ ) , and eve is " factorized out " . this allows to easily show that the step 4 does not leak any information to eve , and that the protocol is secure . you can also consider using quantum error correction codes in step 1 to add some quantum computing in the mix . it is a theoretician 's protocol , after all ! reducing entanglement based qkd protocols to the previous one some qkd protocols are entanglement based . the most famous one is ekert 's e91 . they work as follow : alice and bob share many pairs of entangled particles they make random measurement on them they reveal a subset of measurement that the state is close to an maximally entangled state and abort otherwise . since the state is close to a maximally entangled state , the other measurements are perfectly correlated between alice and bob , and not correlated with anything else . eve has been factored out . the previous protocol works well , does not use teleportation , but has a big problem : as soon as the channel used in step 1 and/or the detectors used in step 2 are slightly imperfect , the state is slightly different from the maximally entangled state and alice and bob aborts . that prevents any use of it in any realistic scenario . the solution is to use some classical error correction ( information reconciliation in qkd parlance ) and privacy amplification ( usually based on universal hashing ) . these information theory based techniques allow us to distribute secret keys if the imperfections are not too big , but they rely on relatively complex data processing which is not easy to analyse theoretically . however , it can be shown that the whole protocol , including this classical post-processing , is equivalent to a teleportation based protocol where the step 1 . was made using the relevant quantum error correcting codes . this equivalence allows to prove the security of a practical qkd protocol by reducing to a teleportation based protocols , which is impractical but theoretically easier to analyse . what about entanglement-less qkd protocols ? most practical qkd protocols , and among them the 1st protocol , bb84 , do not use any entanglement . they are prepare and measure protocols ( p and m ) where alice choses randomly to send the state $\left|\psi_i\right&gt ; $ with probability $p_i$ , and bob performs a measurement . if alice choice is perfectly random ( and we assume it is ) , such a protocol is perfectly equivalent to a protocol where alice prepares the entangled state $\sum_i\sqrt{p_i}\left|i\right&gt ; _a\left|\psi_i\right&gt ; _{a'}$ , sends the $a'$ particle to bob and measures the $a$ particle in order to get $i$ . so we are back to the previous paragraph and the teleportation based protocol . why reducing this the simple protocol ( the last one ) to a more complex one ? the main reason we go through these equivalences , is that the teleportation protocol is easier to analyse . more specifically , we can show that no one but alice and bob can get any information in the process without having to know much about eve 's attack . in particular , we do not have to enumerate the set of attacks eve could perform and take the risk to forget one attack . in short , the teleportation based protocols are easy for theoreticians but a nightmare for experimentalist ( you need quantum computers to perform them with error correction ! ) , while the p and m protocol are easy ( or at least doable ) for experimentalist , but difficult to analyse directly for theoreticians . except , of course , when they are shown to be equivalent to a teleportation based protocol !
yes , you are right , although the first layer already works on the second layer during compression
since $p=ui$ , for q1 $100w=1v\cdot 0.1ma\cdot n\rightarrow n=10^6$ , so your answer is right . for single lemon we have $a=1v\cdot 0.1ma\cdot 3600 s=360mj$ . thus we can execute $a/q=360mj/ ( 0.18nj/instruction ) =2\cdot 10^9$ instructions .
when you say " anti-gravity material " , the closest thing i can think of is the hypothetical concept of negative mass : in theoretical physics , negative mass is a hypothetical concept of matter whose mass is of opposite sign to the mass of the normal matter . such matter would violate one or more energy conditions and show some strange properties such as being repelled rather than attracted by gravity . it is used in certain speculative theories , such as on the construction of wormholes . the closest known real representative of such exotic matter is a region of pseudo-negative pressure density produced by the casimir effect . but it gets more complicated because there are actually three different kinds of mass : gravitational mass , passive grativational mass , and inertial mass : thus objects with negative passive gravitational mass , but with positive inertial mass , would be expected to be repelled by positive active masses , and attracted to negative active masses . however , any difference between inertial and gravitational mass would violate the equivalence principle of general relativity . for an object where both the inertial and gravitational masses were negative and equal , we could cancel out mi and mp from the equation , and conclude that its acceleration a in the gravitational field from a body with positive active gravitational mass ( say , the planet earth ) would be no different from the acceleration of an object with positive passive gravitational and inertial mass ( so a small negative mass object would fall towards the earth at the same rate as any other object ) . in any case , there does not exist any such thing , to the extent of human knowledge .
using the resistors analogy seen in the image above , applying a cip is equivalent to setting the resistors in parallel for each of the spin orientations that is only true if the spacing between fm layers is large . in order for the current in plane configuration to work , the spacing must be less than the mean free path so that electrons traversing the device are scattered by adjacent fm layers . if the adjacent fm layers are anti-parallel , both spin up and spin down will spend some time in a high scattering region . if they are parallel , one spin direction will always see high scattering in the magnetic layers , while the other will always see low scattering . the latter is the low resistance channel that gives rise to the change in resistance of the device .
the answer is yes to both questions : yes , the screen does show one location for one particle and yes , the accumulated picture after repeating the experiment many , many times does show the interference pattern . there is a set of beautiful pictures and a video of the double slit experiment in one-particle-per-time mode that can be found here ( the experiment is with electron but conceptually there is no difference ) .
knowing only the force is not sufficient to calculate the original height . this is because in order to find this , one needs to calculate the velocity or momentum of the object at the instant before it collides with the ground . consider how the motion of the object is affected as it collides with the ground . as soon as the object makes contact , the force of the ground acting upward on the object causes a deceleration , which presumably occurs until the object reaches a velocity of $0$ . this situation is clearly dependent not only on the force applied but also the time that it was applied . presumably , then , if you have not only the force , but the time over which the force was applied you could find the velocity with which the object struck the ground . the way this works is by using the impulse , which is given by : $$\int_{t_i}^{t_f} f\cdot dt $$this expression is equivalent to the change in momentum of the object : $$\int_{t_i}^{t_f} f\cdot dt = p_f - p_i$$because the final momentum is $0$ we have : $$\int_{t_i}^{t_f} f\cdot dt = - p_i$$solving the integral gives us the momentum just before the object hit the ground . using this , one could find the corresponding velocity by using the fact that $p=mv$ . finally , all it would take is a kinematic equation to find the initial height : $$ ( v_f ) ^2- ( v_i ) ^2 = 2a\delta y $$where $\delta y$ is the desired height , $v_f$ is the quantity we previously found , and $v_i=0{m \over s}$ if the object begins falling from rest . its worth noting that if the force of the ground on the ball is given as an average force , meaning it is the constant equivalent to the real force versus time expression , the impulse integral simplifies simply to $\delta p =f_{avg}* \delta t$ .
the energy being used is lost internally , in your arm . your arm is not a solid , it has joints around which it is free to move . the way to hold them in place is to have your muscles act against gravity . now muscles are not " solids " either : they are made with filaments which can slide relative to one another , these filaments are connected by molecules called myosin , which use up energy to move along the filaments but detach at time intervals to let them slide . when you keep your arm in position , myosins hold the filaments in position , but when one of them detaches other myosins have to make up for the slight relaxation locally . they use up energy in doing so .
you can calculate the energy by assuming you charge the capacitor with a constant current . then the energy input to the capacitor is $\int_0^t{iv ( t' ) \mathrm{d}t'}$ since i is constant , you know that ( for a linear capacitor ) v ( t ) is a ramp function . so from geometry ( the area of a triangle formula ) the integral is $\frac{1}{2}itv ( t ) $ now , $it$ is the total charge that is been given to the capacitor and $v ( t ) $ is the final voltage after charging , so this is the same as the $\frac{1}{2}qv$ formula you are familiar with . physical explanation imagine you start with an uncharged capacitor . the first tiny element of charge you drive onto the plates takes practically no energy , because the voltage starts at 0 . as you add more charge to the plates the voltage increases and it takes more energy to add each additional element of charge . finally , the last element takes $v\delta{}q$ energy to push onto the plates . if all the charge took equal energy to push onto the plates , you would end up with your formula , $e=qv$ . but since the first bits of charge you put on took less energy , you end up with an overall average of just $e=\frac{1}{2}qv$ . to figure out why the prefactor is exactly 1/2 , you could either go to the integration formula , or use the geometric argument which is how i mentally calculated the integral anyway .
beyond the qotoolbox , which is showing its age but still very useful , i am not sure what to suggest . a graphical package does not exist ( as far as i am aware ) , and is not likely to unless you make it yourself since this not in high demand .
all you need is quantum mechanics , i.e. that nature in the microcosm is dual , sometimes it can manifest wave properties and sometimes particle properties . it depends on the measurement/experiment if the wave or the particle nature will manifest itself . electrons manifest this duality : in the two slit experiment their wave nature appears governed by the de broglie wavelength . photons do the same too , displaying the wavelength/frequency associated with the collective classical electromagnetic wave . the classical electromagnetic wave is built out of photons in a consistent way , and you could study this link if you are interested in this more complicated problem .
i suspect you have misunderstood what is meant by acceleration in the context of circular motion . acceleration is the rate of change of velocity , but remember that velocity is a vector so it has both direction and magnitude . you are probably thinking that acceleration means a change in the magnitude of the velocity , e.g. speeding up from 1 m/s to 2 m/s , but it can also mean a change in the direction . for a planet orbiting a sun , or a classical electron orbiting the nucleus , the direction of its motion is continually changing so it is accelerating even when the magnitude of its velocity stays constant . so when you say . . . i mean for example a planet would accelerate towards the sun , but because it will then have bigger velocity , it will escape a little bit the sun , and then it will accelerate towards the sun . . . this is not true because in a circular orbit only the acceleration is only changing the direction of motion . the planet stays at the same radial distance from the sun . incidentally , a planet orbiting a star , and therefore accelerating towards it , does radiate energy in the form of gravitational waves . however because gravity is a vastly weaker force than electromagnetism the rate of energy loss is vastly slower . there is more about this in the answers to the question uniform circular motion
i will try and answer in an intuitive way as best i can ( as you asked for on the crosslink ) . the relation between the true , or physical , surface area of a sphere with radius $r_{meas}$ and the surface area one expects from standard euclidean space is a measure ( as you say ) of the average curvature ( more precisely it is a measure of the scalar curvature $r$: http://en.wikipedia.org/wiki/scalar_curvature ) . in gr this curvature is generated by " mass-energy " ( again i am sure you know this ) , or the stress-energy tensor $t_{\mu \nu}$ . the above relation that feynman gives for a average mass density $\rho$ is valid if you are talking about cold matter or " dust " where in the average rest frame of the matter , there are no internal motions ( i.e. . pressures ) . if there are internal motions of the matter then there is kinetic energy in addition to rest mass energy which you should intuitively expect to also contribute to curvature . this additional energy plus the rest mass energy is what i think you mean by " active " mass . so it seems to me that penrose 's criticism is really that feynman did not use a realistic model of matter , since the original relation is true if the matter you are talking about is ultra cold .
the iau general assembly 2012 finished a few days ago . assuming resolution b2 ( pdf ) was passed , the astronomical unit has been frozen and the following values are exact by definition $$ 1\mathrm{a} = 365.25\mathrm{d} = 365.25 \cdot 86\ , 400\mathrm{s} = 31\ , 557\ , 600\mathrm{s} \\ 1\mathrm{ly} = 299\ , 792\ , 458 \mathrm{\frac ms} \cdot 1\mathrm{a} = 9\ , 460\ , 730\ , 472\ , 580\ , 800\mathrm{m} \\ 1\mathrm{au} = 149\ , 597\ , 870\ , 700 \mathrm{m} $$ this also gives the exact value $$ 1\mathrm{pc} = 1\mathrm{au} \cdot \cot 1'' = 1\mathrm{au} \cdot \cot \frac{\pi}{648\ , 000} \approx 30\ , 856\ , 775\ , 814\ , 671\ , 916\mathrm{m} $$ keep in mind that this makes both year and astronomical unit well-defined units which only approximate the ( time-dependant ) physical quantities they where derived from originally . the heliocentric gravitational constant $gm_s$ ( and thus the solar mass $m_s$ ) still needs to be determined observationally .
the effect you note - the curving off instead of converging to a point at infinity - is due to the imperfect alignment of the mirrors . while they may be nearly parallel , they will always be off by some small angle $\theta$ . this angle gets added up reflection on reflection : if the angle between the two mirrors is $\theta$ , then the angle between mirror 1 and its first image in mirror 2 will be $2\theta$ , its image inside that first image will be angled at $4\theta$ , and so on . this then causes further reflections to shift off to one side ( or up , or down ) until they are no longer visible . in the image below , each ' mirror ' is off from the vertical by $\theta=2^\circ$ , the image of the room in each successive mirror is out by an extra angle of $\theta$ , and this quickly accumulates . as an application , by placing your head at the top of one mirror and counting how many copies of the room are visible , you can estimate the tilt angle between the mirrors . say the room has width $l$ and the mirrors are a height $h$ , and you can see $n$ copies of the room . since the vertices in the image are part of a ( quasi ) regular polygon , the bottom of the $m$th reflection of the right mirror is at a height of $r ( \cos\theta-\cos ( 2m+1 ) \theta ) $ above the original , for $\tan\frac\theta2=\frac l{2r}$ , and when this passes $h$ you get $n=m$: $$l ( \cos\theta-\cos ( 2n+1 ) \theta ) =2h\tan\frac\theta2 , $$ or its small-$\theta$ version $$ 2n ( n+1 ) \theta=\frac{h}{l} . $$ why am i going into so much detail ? notice that the dependence on $n$ is quadratic whereas the equation is only linear in $\theta$ . this means that to see twice as many copies of the rooms , you need the alignment of the two mirrors to be four times as good . this reflects something known well to experimental phycisists : aligning optics is hard . hence the fact that mirrors casually mounted on walls are very rarely aligned well enough that you can see more than five to ten copies of the room .
there are two ways to think of the hilbert space as the space of sections of a line bundle . first , the exponentiated chern-simons action on a manifold $\sigma\times [ 0,1 ] $ is a section of the determinant line bundle $\mathcal{l}_\sigma$ on the space of flat connections on $\sigma$ . moreover , wilson loops ( which can be thought of as a 1d tft ) contribute $r_i$ each . so , the hilbert space ( before remembering gauge invariance ) is $\gamma ( \mathcal{l}_\sigma^k ) \otimes\bigotimes_i r_i$ . now , if $\sigma = s^2$ , which is simply-connected , the space of flat connections is a point , so $\gamma ( \mathcal{l}_\sigma^k ) =\mathbf{c}$ . finally , gauge invariance picks out the $g$-invariants in $\bigotimes_i r_i$ . note , that the hilbert space for a non-simply-connected $\sigma$ is nontrivial even without the punctures . another way to think of this hilbert space is to recall the 2d cft &lt ; -> 3d tft correspondence . the idea here is the following . correlation functions of a 2d cft live in a certain bundle over the moduli space of complex curves $m_{g , n}$ called the bundle of conformal blocks . the knizhnik-zamolodchikov equations on correlation functions correspond to a ( projectively ) flat connection on this bundle . so , a 2d cft associates global sections of this bundle to a topological surface $\sigma$ , this is the hilbert space in a 3d tft . in the case of the chern-simons theory , the associated 2d cft is the wess-zumino-witten model . a down-to-earth description can be found in s . elitzur , g . moore , a . schwimmer , n . seiberg , remarks on the canonical quantization of the chern-simons-witten theory , nucl phys b326 ( 1989 ) , 108 . mathematically , this correspondence is an equivalence between modular functors ( as defined by segal in the definition of conformal field theory ) and modular tensor categories which give rise to 3d tfts ( due to reshetikhin and turaev ) . all of that is discussed in an excellent book lectures on tensor categories and modular functors by bakalov and kirillov .
there is a logical flaw in the assumption : gravitational radiation is not the cause of gravitational attraction . the latter is due to curvature of spacetime outside the black hole . for a black hole to attract/exert redshift on something , nothing has to propagate from inside the event horizon . gravitational waves ( radiation ) merely represent moving disturbances in spacetime , they are not what causes the force of gravity , just like a ray of light does not pull you towards its source electromagnetically .
first of all , the wi-fi does not have a defrost setting , we have that going for us . jokes aside , i looked at the microwave and wi-fi that i have , and the first major difference is the power level . the microwave outputs $400w$ to $2kw$ where the for the wi-fi is just around $5w$ . also , the microwave runs on a single frequency in a very confined and shielded space . so all that power is focused in a small region of space . where as for the wi-fi , all that energy is radiated " symmetrically " . so your body will receive only a small fraction of its output . but if you where to surround yourself with hundreds of wi-fi 's my guess is that you will start to feel some discomfort .
the same wikipedia article everyone else is citing is a decent reference on this . basically , we do not know , and probably never will , because we can not put an object in an otherwise empty universe . suppose you could , though . so we have got a planet in an otherwise empty universe . to test the hypothesis of absolute rotation , you could do various experiments on the planet 's surface to measure the fictitious forces arising from being in a rotating reference frame . for example , you could set up foucault 's pendulum at various locations on the planet and measure the precession rate at each location . ( i think you had need at least 3 locations ) from those results you could determine the planet 's rotation axis and rotational velocity . the newtonian viewpoint holds that yes , rotation is relative to space . if this view is correct , and the isolated planet were rotating relative to space , you would see your pendulums ( pendula ? ) precessing at a nonzero rate , and you could solve for the planet 's rotational velocity . on the other hand , the einstein/mach viewpoint holds that rotation is not relative to space , but rather is defined relative to the matter in the universe . if this viewpoint is correct , you would never see any precession of the pendulums because the bulk of the matter in this experimental universe is the planet itself , so it basically defines the frame of zero rotation . in our universe , of course , there is a much larger distribution of matter to define a nonrotating rotational reference frame . mathematically , this results from a phenomenon in gr known as frame dragging . the newtonian/absolute view has the advantage of being kind of intuitive , but it does require that space defines some sort of absolute rotational reference frame . given that we know all linear motion is relative , it seems odd ( to me , and others ) that rotational motion could be absolute . in addition , if rotation could be absolute , for any nonzero rotational velocity , a large enough distribution of matter in the universe would require the outer objects to be moving at the speed of light relative to a nonrotating reference frame . this could conceivably be allowed , it would just mean that no matter could be boosted into that nonrotating reference frame , but again , it seems odd . the einstein/mach view has the advantage that it makes this " faster-than-light rotation " extremely unlikely as a consequence of the structure of the theory alone .
this quote is within the framework of general relativity only . it accepts that there exist " particles " but the only materialization of their existence as particles appears in the change of the functional form of the four independent variables-the coordinates of space and time . in such a mathematical model of nature continuity is built in . now the question is how well this theory/model agrees with experimental observation and astrophysical ones . in astrophysics it does very well , it has not been falsified . ( note a theoretical proposal cannot be proven true , it can be validated by observations and falsified if one observation disagrees with predictions ) . so if we consider matter as just clusters of galaxies , galaxies , stars , planets and even satellites one can give an a++ for the validity of the general relativity predictions and continuity of space time functions is one of them . this is not true when we enter the realm of atoms , molecules and elementary particles . at the micro level quantum mechanics reigns and all our experiments point out that they are discontinuous entities with mass and discrete energy levels and discrete interactions . three more interactions in addition to the gravitational one , which last general relativity models as space time formation . thus the quote is falsified by experimental data at the microscopic level . at the moment the three elementary interactions , strong , weak and electromagnetic are unified in a theory called the standard model . just today we learned that the nobel prize for physics was awarded to prof higgs and englert for predicting this completion of the standard model decades ago , with the discovery of the higgs by cern groups in 2012 . the goal of theoretical physics is to unite all forces , including gravitation in one theoretical framework that will give the standard model at microcosm and general relativity for the cosmos . many people are working on this . at the moment a dominant theory is string theory which can embed the standard model of particle physics which encapsulates all experimental data , and also quantize gravity , a theoretical problem that has been hard to tackle the last 60 or so years . it is work in progress and many theorists are working on this . the ideal theory then will have as an approximation for large masses and distances the quote above , while for the microscom will retain the observed discontinuities of the real masses of real particles in space time .
as @qmechanic points out , you really need to think about the angle between $x$ and $\dot{x}$ . in particular , since $x = r e_r$ , you can take the inner product with $\dot{x}$ and get a nonzero answer : $x \cdot \dot{x} = r\ , \dot{r}$ . this contribution needs to be removed when taking the exterior product $x \wedge \dot{x}$ , which is what you did in the first version when you eliminated $\lvert r e_r \wedge \dot{r} e_r \rvert$ . but setting $\sin\theta = 1$ does not accomplish this .
this is how you proceed , since $ = 0$ , bodies stick after collision total momentum remains conserved therefore : $m_1v_{1i} + m_2v_{2i} = ( m_1 + m_2 ) v_f$ solve for $v_f$ and then , $ i = m_1 v_f - m_1 v_1$ $ f = i/{dt}$ addendum : if $e\neq 0$ use $e = \frac{ \overrightarrow{v_{2f}} - \overrightarrow{v_{1f}}}{\overrightarrow{v_{1i}} - \overrightarrow{v_{2i}}}$ $e = \frac{\mbox{relative velocity after collision}}{\mbox{relative velocity before collision}}$ use this along with momentum conservation , the difficulty would then be determined by how many variable 's you can eliminate by using given values or otherwise . as this question mathematically proves , yes you can find momentum using that equation . to find the final velocities use : $ \delta v = i/m$ $ v_{1f} = v_{1i} + \delta v$
in an atom , the electron is not just spread out evenly and motionless around the nucleus . the electron is still moving , however it is moving in a very special way such that the wave that it forms around the nucleus keeps the shape of the orbital . in some sense the orbital is constantly rotating . to understand precisely what is happening you need to use quantum mechanical formalism . consider the hydrogen $1s$ state which is described by \begin{equation} \psi _{ 1,0,0} = r _1 ( r ) y _0 ^0 = r _{1,0} ( r ) \frac{1}{ \sqrt{ 4\pi } } \end{equation} where $ r _{1,0} \equiv 2 a _0 ^{ - 3/2} e ^{ - r / a _0 } $ is some function of only distance from the origin and is irrelevant for this discussion and the wavefunction is denoted by the quantum numbers , $n$ , $ \ell $ , and $ m $ , $ \psi _{ n , \ell , m } $ . the expectation value of momentum in the angular directions are both zero , \begin{equation} \int \ , d^3r \psi _{ 1,0,0 } ^\ast p _\phi \psi _{ 1,0,0 } = \int \ , d^3r \psi _{ 1,0,0 } ^\ast p _\theta \psi _{ 1,0,0 } = 0 \end{equation} where $ p _\phi \equiv - i \frac{1}{ r } \frac{ \partial }{ \partial \phi } $ and $ p _\theta \equiv \frac{1}{ r \sin \theta } \frac{ \partial }{ \partial \theta } $ . however this is not the case for the $ 2p _z $ state ( $ \ell = 1 , m = 1 $ ) for example . here we have , \begin{align} \left\langle p _\phi \right\rangle and = - i \int \ , d^3r \frac{1}{ r}\psi _{ 1,1,1} ^\ast \frac{ \partial }{ \partial \phi }\psi _{ 1,1,1} \\ and = - i \int d r r r _{2,1} ( r ) ^\ast r _{ 2,1} ( r ) \int d \phi ( - i ) \sqrt{ \frac{ 3 }{ 8\pi }} \int d \theta \sin ^3 \theta \\ and = - \left ( \int d r r _{2,1} ( r ) ^\ast r _{2,1} ( r ) \right ) \sqrt{ \frac{ 3 }{ 8\pi }} 2\pi \frac{ 4 }{ 3} \\ and \neq 0 \end{align} where $ r _{2 1} ( r ) \equiv \frac{1}{ \sqrt{3} } ( 2 a _0 ) ^{ - 3/2} \frac{ r }{ a _0 } e ^{ - r / 2 a _0 } $ ( again the particualr form is irrelevant for our discussion , the important point being that its integral is not zero ) . thus there is momentum moving in the $ \hat{\phi} $ direction . the electron is certainly spread out in a " dumbell " shape , but the " dumbell " is not staying still . its constantly rotating around in space instead . note that this is distinct from spin of and electron which does not involve any movement in real space , but is instead an intrinsic property of a particle .
because surfaces are not smooth on the atomic scale , when you touch two surfaces together only the high points ( asperities ) on the surfaces make contact so the real area of contact is much smaller than the apparent area of contact . i am guessing you know this , hence your comment in your second paragraph : and there would be more peaks the reason why the force is approximately independant of area is that as you increase the force you get elastic deformation of the asperities and the real area of contact per asperity increases . if you increase the area you increase the number of points of contact but you also decrease the force per point of contact so the asperities deform less and the real area of contact per asperity goes down . the real area per asperity is roughly proportional to pressure , i.e. $f/a$ , and the number of asperties in contact is proportional to area $a$ . so when you multiply these together you find the real area of contact , and hence the friction , is just proportional to the applied force . for relatively soft materials like rubber the approximation does not hold , and their frictional behaviour is a lot more complex .
this question reminds me of zeno 's paradoxes . it is assumed that the two mirror surfaces are absolutely parallel . in classical physics the electromagnetic waves that create the reflections are uniform and the energy loss due to the reflection ( depending on the material of the glass ) will be what will make the reflections fainter and fainter , but the process is continuous and the limit will be a limit in luminosity . in principle a totally reflecting material would have no limit , going to infinite reflections as time goes to infinity . {corrected from original statement that the wavefronts are instantaneous : maxwells equations obey special relativity i.e. the velocity c of light is finite} reality is quantum mechanical and also special relativity dependent . with special relativity in the problem it will take time to reach the next reflection , so even for a total reflector infinity will also be reached only at infinite time , during observation , though there will be an enormous number of reflections . quantum mechanically there can not be a totally reflecting mirror , even in a thought problem . there will always be a probability of absorption and thus a termination of the wavefront eventually , the images getting less and less defined until they become individual photons and finally totally absorbed . since quantum mechanics reigns , in reality no , there will not be an infinite amount of reflections .
the lift generated by magnetic field b on a superconductor of area s is : \begin{equation} f = \frac{b^2s}{2\mu_0} \end{equation} disregarding lateral forces and assuming superconducting cylinder ( or similar shape ) with area s at the top and bottom and height h , we need three forces to remain in the equilibrium : magnetic pressure on top , bottom and gravity force : \begin{equation} f_{b} - f_{t} = f_{g} \end{equation} denoting density of the superconductor as ρ , earth ' gravity as g and magnetic field at the top and bottom of the object as b t and b b , we have \begin{equation} \frac{1}{2\mu_0} ( b_{b}^2-b_{t}^2 ) =\rho gh \end{equation} assuming the vertical rate of change of magnetic field is nearly constant and denoting the average magnetic field as b , we have \begin{equation} -b\frac{db}{dz}=\mu_{0}\rho g \end{equation} compare with diamagnetic levitation ( superconductor 's magnetic susceptibility is -1 ) . now , earth magnetic field is between 25 to 65 μt . for the derivative i have found this survey from british columbia with upper point on the scale being 2.161 nt/m . assuming this to be the maximum for vertical derivative we get the required density of 1.1394e-08 kg/m 3 . for comparison air density at the sea level at 15c is around 1.275 kg/m 3 , so required density is 8 orders of magnitude smaller . even assuming a very high vertical derivative where b goes from its maximum 65 μt to 0 on 1 m of height results in density required of 0.00034272 kg/m 3 .
from inside the room i would view the light moving north at the speed of light that is true . from outside the room , observing from a stationary position , would the light exiting the window move at twice the speed of light ? or would it change speed ? no , it will still move at speed of light , thus theoretically outer observer will never see any light coming out of that window in this case , this is simple result of velocity-addition formula in special relativity , check the link .
as you may know , photons do not have mass . relating relativistic momentum and relativistic energy , we get : $e^2 = p^2c^2+ ( mc^2 ) ^2$ . where $e$ is energy , $p$ is momentum , $m$ is mass and $c$ is the speed of light . as mass is zero , $e=pc$ . now , we know that $e=hf$ . then we get the momentum for photon . note that there is a term called effective inertial mass . photon does have it .
the table you cite is a table for moist air . there is a convention for moist air entropy built from dry air entropy being 0 at t = 273.15k and p = 101325pa , and liquid water entropy being 0 at its triple point . see the page numbered 47 of this nist document : http://www.nist.gov/calibrations/upload/5241.pdf steam tables also use the standard that liquid water at the triple point has 0 entropy . another , more absolute , standard is to take the entropy of all pure crystalline substances to be zero at absolute zero . for aqueous ions , the entropy of aqueous h+ is usually taken to be 0 .
when one excites fluorescence with light ( e . g . blue light ) , the incoming photon pushes an electron into a higher state . generally one always excites vibrational states ( movement of the molecules ' atoms ) as well . therefore the emitted photon has lower energy ( e . g . green light ) . the lost energy ( internal conversion ) heats the solvent or lattice . the shift in frequency ( from blue to green ) is called stokes shift . people plot these energy levels in jablonski diagrams . i stole this from http://biophys.med.unideb.hu/old/pharmacy/fluorescence_print.pdf you can only excite as many electrons as there are in the material . this means that absorbing laser goggle are not safe for high power pulsed laser . the early photons of the pulse will saturate all the electrons in the goggles and the rest of the pulse can advance into the eyes and do damage . in a glow stick the excitation is done by chemical energy . hydrogen peroxide splits some organic molecule and one of the produced excited carbon dioxide molecules can transfer its energy to a fluorophore . i once saw an experiment where the chemicals from the glow stick were added to tea and the tea 's chlorophyll started glowing in red . the only reason i can think of right now , why anyone would want to store energy like this , is to be able to access it with the speed of light . in high power pulsed lasers special crystals are pumped into the higher state and the energy can suddenly be converted into photons . that way it is possible to create pulses with petawatts of power .
your assumption that pair production is ruled out , rules out* that two photons interact through higher-order processes . quantum electrodynamics tells us that two photons cannot couple directly . that leaves us with classical electromagnetism , which tells us that electromagnetic waves pass through each other without any interference . *edit . the photons can interact through higher-order processes . as pointed out in the comments ( and i hope i am getting this right ) , there is a ( quite small ) probability amplitude for two photons to get absorbed in , and two photons be emitted by , e.g. an fermion-antifermion virtual pair ( which is the leading contributor to the combined amplitude of all such processes ) . whether ( and this is my cop-out ) the emitted photons can be considered the same photons as the absorbed photons , i leave to the , certainly more knowledgeable , commentators .
great , important question . here 's the basic logic : we start with wigner 's theorem which tells us that a symmetry transformation on a quantum system can be written , up to phase , as either a unitary or anti-unitary operator on the hilbert space $\mathcal h$ of the system . it follows that if we want to represent a lie group $g$ of symmetries of a system via transformations on the hilbert space , then we must do so with a projective unitary representation of the lie group $g$ . the projective part comes from the fact that the transformations are unitary or anti-unitary " up to phase , " namely we represent such symmetries with a mapping $u:g\to \mathscr u ( \mathcal h ) $ such that for each $g_1 , g_2\in g $ , there exists a phase $c ( g_1 , g_2 ) $ such that \begin{align} u ( g_1g_2 ) = c ( g_1 , g_2 ) u ( g_1 ) u ( g_2 ) \end{align} where $\mathscr u ( \mathcal h ) $ is the group of unitary operators on $\mathcal h$ . in other words , a projective unitary representation is just an ordinary unitary representation with an extra phase factor that prevents it from being an honest homomorphism . working with projective representations is not as easy as working with ordinary representations since they have the pesky phase factor $c$ , so we try to look for ways of avoiding them . in some cases , this can be achieved by noting that the projective representations of a group $g$ are equivalent to the ordinary representations of $g'$ its universal covering group , and in this case , we therefore elect to examine the representations of the universal cover instead . in the case of $\mathrm{so} ( 3 ) $ , the group of rotations , we notice that its universal cover , which is often called $\mathrm{spin} ( 3 ) $ , is isomorphic to $\mathrm{su} ( 2 ) $ , and that the projective representations of $\mathrm{so} ( 3 ) $ match the ordinary representations of $\mathrm{su} ( 2 ) $ , so we elect to examine the ordinary representations of $\mathrm{su} ( 2 ) $ since it is more convenient . this is all very physically important . if we had only considered the ordinary representations of $\mathrm{so} ( 3 ) $ , then we would have missed the " half-integer spin " representations , namely those that arise when considering rotations on fermionic systems . so , we must be careful to consider projective representations , and this naturally leads to looking for the universal cover . note : the same sort of thing happens with the lorentz group in relativistic quantum theories . we consider projective representations of $\mathrm{so} ( 3,1 ) $ because wigner says we ought to , and this naturally leads us to consider its universal cover $\mathrm{sl} ( 2 , \mathbb c ) $ .
i tried to calculate the solution to your problem in your related question . if i did not miscalculate , the dispersion relation should be implicitly given by the boundary conditions at $y=0$ and $y=h$ . the solution was $$x{ ( t , y ) }=\sum_n e^{\{+ , -\}\mathrm{i}\omega_n t}\left [ a_n j_0 \left ( 2\omega_n \sqrt{\frac{t_0 + \lambda y}{\lambda g}} \right ) + b_n y_0 \left ( 2\omega_n \sqrt{\frac{t_0 + \lambda y}{\lambda g}} \right ) \right ] $$ but it is very unhandy . performing a coordinate transformation $$\rho = \frac{t_0 + \lambda y}{\lambda g}$$ we can drop the $y_0$ part of the solution since it is not limited at the origin and $j_0$ is already zero there . we thus find that the dispersion relation can be stated as $$2\omega_n \cdot \rho{ ( y = h ) } = \mathrm{&quot ; root\ of\ }j_0&quot ; $$ sincerely robert
i solved my problem numerically , using the diffusion equation $\frac{\partial v}{\partial t} = -k\nabla^2 v$ , with the following boundary conditions : voltage at point d is fixed at 1.0 voltage along the vertical line halfway between points a and d is fixed at 0.5 ( voltage at point a is 0.0 , use symmetry so we do not have to simulate the left half of the material ) the other three boundaries have the neumann condition where the gradient of potential has zero component perpendicular to the boundary . for a rectangular array , this just means setting the top and bottom rows and the right column equal to their nearest neighbors off the boundary . iterate a bunch of times until equilibrium is reached and $\frac{\partial v}{\partial t} = 0$ . then i graphed a streamplot and contour plot in matplotlib to show the current density and equipotentials : this seems like it would be a well-known problem , though , and i would be happier understanding an analytic solution that uses sums of the form $a_{mn} \sinh \frac{2m\pi x}{l} \cos \frac{2n\pi y}{w}$ or whatever .
the formulation you seek is gauge theory . it is not completely analogous to changing the metric of spacetime , but many similarities can be seen . in this , we take as our starting point a certain gauge group $g$ ( in the case of em , $\mathrm{u} ( 1 ) $ ) , which will induce symmetries of our theory , just as the lorentz group of special relativity is the symmetry of that theory ( but the lorentz group is emphatically not a gauge group for special relativity ! ) . then , we construct a so called $g$-principal bundle over our spacetime $\mathcal{m}$ , and take so-called associated vector bundles as the spaces where our fields now take value ( in addition to any spacetime indices they may possess ) ( just like vectors in gr take values in the ( co ) tangent spaces ) . now , we must learn with shock that the naive derivative of our fields do not transform as proper values in the associated bundles , just like naive derivatives of lorentz vectors are not vector either . and so we construct a covariant derivative , which yields the gauge field ( the vector potential ) as its connection form , just like we get the levi-civita connection in gr . ( the analogous thing in gr to the connection form coefficients would be the christoffel symbols , but remember that the analogy is not complete ) and from the connection that is the gauge field , we may calculate its curvature - which is the field strength tensor and will appear in the action of the theory , just like the curvature in gr appears in the einstein-hilbert action . the analogy goes quite a way , but i want to stress that it breaks down as soon as the metric is concerned . gauge theories do not change the metric , they take a spacetime with a given metric and build upon it . this is essentially why understanding gravity is so hard - because it is not obviously a gauge theory like all the other forces .
there is no unique way to answer this question . the problem is the following . you are asking if , at a specific proper time , a black hole of some radius exists . however , to do this you have to say something like " the black hole has a 10m radius at the same time that the observer 's proper time is 5sec " . such simultaneous events are not well defined in gr unless you make an arbitrary choice of a coordinate system with a time coordinate . nonetheless , if you make such a choice , it is true that the black hole will start with a radius of 0 inside of the infalling matter and then get larger . eventually it will reach a maximum radius and then get smaller through hawking radiation . so what will an observer literally see if she observes the star collapsing ? the answer is that the observer will see infalling matter slow down and thermalize . the matter will never appear to " fall in " as far as this observer is concerned . however , the particles/light emitted will be redshifted more and more ( and before long the only outgoing particles of reasonably not-low energy will be from hawking radiation ) .
if you stopped me on the street and asked me , i would reason it is probably because inside the conductor , the attractive forces from the nuclei are fairly uniform , so motion in one direction is as easy as motion in any other . on the surface , however , there are only forces on one side of the the electron , namely from the nuclei on the surface of the conductor . the air molecules do not attract the electrons strongly enough to pull them away because they are either neutral or relatively weak dipoles . as a result , electrons are effectively bound to the surface of the conductor unless a sufficiently high voltage is applied to overcome the strong attractions of the conductor 's nuclei . or so i would guess .
basically , the universe has a speed limit . no object can ever exceed the speed of light . now imagine you decide to prove einstein wrong by building a train capable of nearly reaching the speed of light , and then shooting a bullet forward in that train , so that the bullet will break the speed of light . in order to preserve this speed limit , time for you and your bullet , as determined by relatively moving observers , will be slower inside this train . whereas you will see the bullet shooting at normal speed , an observer outside the train , in the brief instant the train is in view , would see both you and the bullet moving in slow motion . hence , relative to the train , and relative to the outside observer , the bullet will never actually reach the speed of light . note that speed is the relationship between two quantities : distance and time ( i.e. . meters per second or miles per hour ) . hence , in order to maintain that speed limit , as an object moves faster through space , motion through time is slowed in the frame of reference of that object . the length of the object is , because of speed being the relationship between distance and time , also decreased , relative to other objects . the change in time and length also affect other quantities , such as mass , force , and energy . it is a little hard to believe , but in every single experiment ever used to test this theory , it has held true .
it was henri becquerel who discovered radioactivity . ernest rutherford proved that what was happening was the disintegration and transmutation of atoms . radioactivity was discovered in uranium , but the curies discovered a mineral that was more radiocative . they correctly deduced that it must contain a new element . they then used purely chemical methods to separate the various elements , keeping track of which residuum had kept the radioactivity . it is not my area , but i think that all of the methods were standard apart from using the radioactivity to trace . and there was an enormous amount of work to be done . they actually found two new elements , radium and polonium . in a sense they were lucky , because they could have been chasing a radioactive isotope of a known element . isotopes were only recognised later .
it seems that the question ( v1 ) is caused by the fact that there are two different notions of the commutator : one for group theory : $$\tag{1} [ a , b ] ~:=~ aba^{-1}b^{-1}$$ ( or sometimes $ [ a , b ] := a^{-1}b^{-1}ab$ , depending on convention ) , which is relatively seldom used in physics . one for rings/associative algebras : $$\tag{2} [ a , b ] :=ab-ba , $$ which is the definition usually used in physics . ( this latter definition ( 2 ) generalizes to a supercommutator in superalgebras . ) the identity $$\tag{*} [ a , b^n ] ~=~ nb^{n-1} [ a , b ] $$ holds in the latter sense ( 2 ) , if $ [ [ a , b ] , b ] =0$ . ( it is not necessary to demand $ [ a , [ a , b ] ] =0$ . ) more generally , for a sufficiently well-behaved function $f$ , we have $$\tag{**} [ a , f ( b ) ] ~=~ f^{\prime} ( b ) [ a , b ] , $$ if $ [ [ a , b ] , b ] =0$ . the group commutator ( 1 ) is dimensionless , which ( among other things ) makes the identity ( * ) unnatural to demand for group commutators .
i found an entry in a wormhole faq that seems to address your thought experiment : " is a wormhole whose mouths are arranged vertically in a gravitational field a source of unlimited energy ? no . the argument in favor of such a wormhole being an energy source is this : an object falls from the upper mouth , gains kinetic energy as it falls , enters the lower mouth , reemerges from the upper mouth with this newly acquired kinetic energy , and repeats the cycle to gain even more kinetic energy ad infinitum . the problem with this is that general relativity does not permit discontinuities in the metric – the descriptor of the geometry of spacetime . this means that the gravitational potential of an object at the lower mouth must continuously rise within the wormhole to match the potential it had at the upper mouth . in other words , this traversal of the wormhole is “uphill” and therefore requires work . this work precisely cancels the gain in kinetic energy . "
you said : photons , which behave as individual gaussian wave packets this is not strictly true . photons are not restricted to being represented by a gaussian wave packet , or really any type of wave packet allowed by maxwell 's equations . a plane wave of one particular frequency is a perfectly valid photon ( except that it would occupy an infinite amount of space ) . maxwell 's equations are the equivalent of the schrödinger equation for light , and the electric and magnetic fields are the equivalent of the wavefunction . any solution to maxwell 's equations is a valid photon wavefunction . short answer : you do not need a superposition of gaussian wave packets , the classical plane wave solution is fine on it is own . it is , however , possible to express the classical plane wave solution as a superposition of suitable wave packets .
it is mathematically possible to create some instances in which an object goes back in time relative to some observer . for example , simply going faster than light causes such an effect , but of course , speed of light is the limit for any massive object . while it is mathematically possible , there are many paradoxes caused by time travel to past , unless you accept existence of some sorts of multiverses , which does not really qualify as a time travel to past anyway , rather travel between said multiverses . mainly , and most importantly , time travel to past violates " causality " , one of the main principles the universe is thought to have , in which the cause precedes the result , e.g. your father is born before you are born . through time travel to past , it is possible for the " result " to eliminate its " cause " before it causes the result , that is , you killing your father before you were conceived . this is not the only paradox caused by time travel to past , however this is one of the main ones . also , i think it was hawking that suggested this , we see no time-travelers around us , pointing out to the fact that humanity will not achieve the technology to go back in time as far as it wants . however , it is possible that time travel to past may be discovered , only limited to taking travelers back to the creation of the machine , as some time travel machines work on this principle , namely wormholes . needless to say , wormholes require extremely exotic conditions to form and maintain , and therefore are very far from being created deliberately , if they even exist .
but , why is speed of light considered as a fundamental constant ? c is an invariant speed ; if something ( not necessarily light ) has speed c in one inertial reference frame ( irf ) , it has speed c in all irfs . this result follows from the lorentz transformation which , empirically , gives the correct coordinate transformation between relatively moving irfs . thus , c is a physical constant with units of speed . so it is c , the physical constant , that is constant . yes , light propagates with speed c in vacuum but there is nothing contradictory with light propagating with speed less than c in a medium . in other words , we do not denote the speed of light in a medium as c .
this is a very interesting question with a very interesting answer . the key lies in the reason for the stretchiness of the rubber band . rubber is made of polymers ( long chain molecules ) . when the elastic band is not stretched , these molecules are all tangled up with each other and have no particular direction to them , but when you stretch the elastic they all become lined up with one another , at least to some extent . the polymer molecules themselves are not stretched , they are just aligned differently . to a first approximation there is no difference in the energy of these two different ways of arranging the polymers , but there is a big difference in the entropy . this just means that there is a lot more different ways that the polymers can be arranged in a tangled up way than an aligned way . so when you release the elastic band , all the polymers are jiggling around at random due to thermal motion , and they tend to lose their alignment , so they go back towards the tangled state , and that is what makes the elastic contract . this is called an entropic force . now , i said earlier that there is not any difference in energy between the stretched ( aligned ) and un-stretched ( tangled ) states . but it takes energy to stretch the elastic -- you are doing work to pull the ends apart , against the entropic force that is trying to pull them back together . that energy does not go into stretching the individual polymer molecules , but it has to go somewhere , so it ends up as heat . some of this heat will stay in the elastic ( making the polymer molecules jiggle around a bit faster ) but some will be transferred to the surrounding air , or to your skin . the reverse happens when you let the elastic contract . the molecules are jiggling around at random and becoming more and more tangled , which makes them contract . but to contract they have to do work on whatever 's holding the ends of the elastic apart . that energy has to come from somewhere , so it comes from heat . at first this might seem to run against thermodynamics - normally you can not just cool something down without heating something else up . but remember that the state with the tangled molecules has a higher entropy than when they are aligned . so you are taking heat out of the air , which reduces its entropy , but this reduction in entropy is countered by the increase in entropy of the elastic itself , so the second law is safe . for further reading you can look into the ideal chain , which is an idealised mathematical model of this situation .
functional derivative : $f_d [ {\bf p} ] = \int \mathrm d\boldsymbol{r}\ f ( \boldsymbol{r} , {\bf p} ( \boldsymbol{r} ) , \nabla\cdot {\bf p} ( \boldsymbol{r} ) ) $ $\frac{\delta f_d}{\delta {\bf p} ( \boldsymbol{r} ) } := \frac{\partial f}{\partial {\bf p}} - \nabla \cdot \frac{\partial f}{\partial\nabla\cdot {\bf p}} , $ although something 's weird with your signs . your ${\bf p}$ also has several components , which makes it slightly more trickier - but you will figure it out .
a photon has zero mass , so you can not write it is kinetic energy as $\frac{1}{2}mc^2$ . you are thinking along the right lines . as the photon moves away from the earth it loses energy . however the loss of energy causes the photon to be red shifted i.e. it moves to a lower frequency . the photon energy is given by : $$e = h\nu$$ and using your notation : $$e_1 - e_2 = h\nu_1 - h\nu_2$$ you can use this to calculate the new frequency of the photon . this frequency change has been measured using the mossbauer effect . see http://en.wikipedia.org/wiki/pound%e2%80%93rebka_experiment for the details .
no . according to special relativity 's postulates , the speed of light in a vacuum is the same for all observers , regardless of the motion of the light source . you may want to read the relativistic doppler effect . it refers to the change in frequency of light due to motion of observer and/or source .
your confusion really just comes down to understanding the notation that is widely used for partial derivatives . for simplicity , i will restrict the discussion to a system with one coordinate degree of freedom $x$ . in this case , the lagrangian is a real valued function of two real variables which we suggestively label by the symbols $x$ and $\dot x$ . mathematically , we would write $l:u\to\mathbb r$ where $u\subset \mathbb r^2$ . let 's consider the simple example $$ l ( x , \dot x ) = ax^2+b\dot x^2 $$ when we write the expression $$ \frac{\partial l}{\partial \dot x} ( x , \dot x ) $$ this is an instruction to differentiate the function $l$ with respect to its second argument ( because we labeled the second argument $\dot x$ ) and then to evaluate the resulting function on the pair $ ( x , \dot x ) $ . but we just as well could have written $$ \partial_2l ( x , \dot x ) $$ to represent the same expression . both of these expressions simply mean that we imagine holding the first argument of the function constant , and we take the derivative of the resulting function with respect to what remains . in the case above , this therefore means that $$ \frac{\partial l}{\partial\dot x} ( x , \dot x ) = 2b\dot x $$ because $x$ labels the first argument , and taking a partial derivative with respect to the second argument means that we treat $x$ like a constant whose derivative is therefore $0$ . it it in this sense that the partial of $x^2$ with respect to $\dot x$ is zero . so to recap , when we are taking these derivatives , we just keep in mind that the symbols $x$ and $\dot x$ are just labels for the different arguments of the lagrangian . you might ask , however , " if $x$ and $\dot x$ are just labels , then what relation do they have to position and velocity ? " the answer is that after we have treated them as labels for the arguments of $l$ in order to take the appropriate derivatives , we then evaluate the resulting expressions on a $ ( x ( t ) , \dot x ( t ) ) $ , the position and velocity of a curve at time $t$ , to obtain equations of motion . for example , if you take the example of $l$ that i started with , we get $$ \frac{\partial l}{\partial x} ( x , \dot x ) = 2 ax , \qquad \frac{\partial l}{\partial \dot x} ( x , \dot x ) = 2b\dot x $$ now we evaluate these expressions on $ ( x ( t ) , \dot x ( t ) ) $ to obtain $$ \frac{\partial l}{\partial x} ( x ( t ) , \dot x ( t ) ) = 2 ax ( t ) , \qquad \frac{\partial l}{\partial \dot x} ( x ( t ) , \dot x ( t ) ) = 2b\dot x ( t ) $$ so that the euler-lagrange equations become $$ 0=\frac{d}{dt}\left [ \frac{\partial l}{\partial \dot x} ( x ( t ) , \dot x ( t ) ) \right ] - \frac{\partial l}{\partial x} ( x ( t ) , \dot x ( t ) ) =\frac{d}{dt} ( 2b\dot x ( t ) ) - 2ax ( t ) $$ which gives $$ b\ddot x ( t ) = a x ( t ) $$ once you understand all of this , you can ( and should ) dispense with the long-winded notation i used here for illustrative purposes , and you should make no error in using the abbreviated notation in your original post .
consider a gauge theory with gauge group $gl ( n , r ) $ . first of all , let me remind you the basics of gauge transformations : let $g$ be a gauge group , $g ( x ) ∈ g$ be an element of $g$ . then : $\psi ( x ) →g ( x ) \psi ( x ) $ $a_\alpha→g ( x ) a_\alpha g^{-1} ( x ) -\frac{∂g ( x ) }{∂x^{\alpha}}g^{-1} ( x ) $ is a gauge transformation , and covariant derivative defined as ${\nabla}_{\alpha}\psi = \frac{∂\psi}{∂x^{\alpha}}+a_\alpha \psi$ now consider coordinates $ ( x^1 , . . . , x^n ) $ in region $u$ . they define basis of vectors space $\frac{∂}{∂x^1} , . . . , \frac{∂}{∂x^n}$ . so , tangent vector fields in region $u$ can be considered as vector-valued functions : $\xi = ( \xi^1 , . . . , \xi^n ) $ . change of coordinates in $u$: $x^{\nu}→x^{{\nu^{\prime}}}=x^{{\nu^{\prime}}} ( x ) $ defines local transformation : $\xi^\nu→\xi^{\nu^\prime} = \frac{∂x^{\nu^\prime}}{∂x^\nu}\xi^\nu = g ( x ) \xi$ . here matrix $g ( x ) = ( \frac{∂x^{\nu^\prime}}{∂x^\nu} ) $ belongs to $gl ( n , r ) $ , and inverse matrix has the form $g^{-1} ( x ) = ( \frac{∂x^\nu}{∂x^{\nu^\prime}} ) $ . lie algebra of $gl ( n , r ) $ is formed by all matrices of degree $n$ , so the " gauge field " $a_\mu ( x ) $ is also matrix of degree $n$ . let us denote it is elements as follows : $ ( a_\mu ) ^{\nu}_{\lambda}=\gamma^{\nu}_{\lambda \mu}$ . covariant derivative of the vector $\xi$ reads as follows : $ ( \nabla_{\mu}\xi ) ^\nu=\frac{∂\xi^\nu}{∂x^\mu}+\gamma^{\nu}_{\lambda \mu}\xi^\lambda ↔ \nabla_\mu \xi=\frac{∂\xi}{∂x^\mu}+a_\mu \xi$ ( right side is in matrix form ! ) there is only one thing left to check , namely the form of gauge field transformation . using general rule of transforming gauge field we obtain : $\gamma^{\nu}_{\lambda \mu}→\gamma^{\nu^\prime}_{\lambda^\prime \mu}=\frac{∂x^{\nu^\prime}}{∂x^\nu}\gamma^{\nu}_{\lambda \mu}\frac{∂x^\lambda}{∂x^{\lambda^\prime}}+\frac{∂x^{\nu^\prime}}{∂x^\nu}\frac{∂}{∂x^\mu} ( \frac{∂x^\nu}{∂x^{\lambda^\prime}} ) $ . since $a_\mu$ is a covariant vector , then $a_{\mu^\prime}=\frac{∂x^\mu}{∂x^{\mu^\prime}}a_\mu$ . hence we obtain : $\gamma^{\nu^\prime}_{\lambda^\prime \mu^\prime}=\frac{∂x^\mu}{∂x^{\mu^\prime}}\frac{∂x^{\nu^\prime}}{∂x^\nu}\gamma^{\nu}_{\lambda \mu}\frac{∂x^\lambda}{∂x^{\lambda^\prime}}+\frac{∂x^{\nu^\prime}}{∂x^\nu}\frac{∂^2 x^\nu}{∂x^{\lambda^\prime}∂x^{\mu^\prime}}$ . q.e.d. and final remark : the commutator of two covariant derivatives leads to expression of the riemann tensor : $ ( f_{\mu\nu} ) ^\rho_\lambda = r^\rho_{\lambda , \mu\nu}$ edit : dear oaoa , i’m not a gr specialist , so what i have written below might be wrong . my first advise is as follows : do not read landau who is mixing up two fundamental concepts : connection and metric . instead i encourage you to read " space-time structure " by erwin schrödinger . in order to answer your question let us first separate the roles of connection and metrics . connection is used for parallel transport and enables to compare two vectors in different points . important consequence is that using connection one can introduce the curvature tensor ( that can further be contracted to curvature scalar ) . curvature appears when you transport vector along the closed curve and then compare with the initial vector . curvature scalar is then used to construct “field action” just like in all gauge theories . as shown in schrödinger’s book , connection can also be used to measure distance along geodesic line ( it worth noting that the expression for such “measure” is so much similar to the expression of feynman’s path integral action ! ) . but in general , connection cannot be used for measuring distances between arbitrary points . metric is introduced for measuring distances between arbitrary points and defining vector products . connection and metric are independent concepts . only additional condition of their consistency ( i.e. . when you require that vector product is invariant when both vectors are parallel transported ) allows to express connection via metric tensor . let’s get back to your question now . all that is written about $gl ( n , r ) $ above is related to connection only . in the first place , it allows expressing “field action” in terms of a scalar curvature . but what you are most interested in is probably not this , but conservation laws related to matter fields . in the theory with point particles functions $\xi$ ( or $\psi$ ) can be associated with vectors $\frac{dx^\nu}{ds}$ . i’m not sure , but consequent conservation law is probably energy-momentum conservation . i think it is the same in special relativity where space is flat and all connections are zero , but indirectly the conservation of energy-momentum in sr might be a consequence of “conservation” of null curvature by lorentz transformations ( please note that homogeneity of space-time means zero curvature ) . i know you expect to see some other conserved quantities similar to “electric charge” conservation in dirac theory of electron . but please note that in dirac theory the “global” conservation of “charge” is practically indistinguishable from conservation of energy-momentum . as for local theories – i do not know , concrete model need to be considered .
first , people who study physics are called " physicists " , not " physicians " . the latter are doctors who care about other people 's health . second , gravity is a fundamental force in nature , along with electromagnetism , the strong nuclear force , and the weak nuclear force . so all phenomena in the world – including computation – should be explained in terms of ( i.e. . should be reduced to ) these four fundamental forces . the fundamental forces themselves cannot be explained in terms of anything more basic because there is not anything that is more basic ; that is what the word " fundamental " means . the op is apparently trying to do the opposite thing , namely to explain fundamental forces in terms of non-fundamental processes such as computation . but that is not how nature or physics works . they work in the opposite way . computers are complicated systems with lots of elementary particles that mostly interact via electromagnetism . nature does not face any limitations of " chips with some number of transistors " because nature is not a chip with transistors . an apple or a planet is not a circuit with several transistors ( and gps receivers to measure their distances ) , either . nature is a system where the equations such as those describing gravity hold . it is not a free decision of the apple to fall down ; the external force is dragging it towards the earth " automatically " . the apple does not have to do anything . when all the appropriate corrections ( not only those of general relativity but also those of string theory etc . ) are included , the equations of physics exactly hold ( well , they only predict probabilities because the fundamental theory is a quantum theory ) , and even when it is difficult for us and our computers to calculate what will happen , nature has no problems with the laws because nature is simply not a " finite brain " similar to ours or a computer similar to one of those we possess . in this sense , when it comes to exact calculations of the outcomes of her own laws , nature is " omnipotent " and " omniscient " ( and yes , it is also " omnipresent" ) , like god . a closely analogous question , " how magnets work and what is the feeling between them ? " , was asked to richard feynman http://www.youtube.com/watch?v=wmfpe-dwulm feynman has spent much of the time by explaining the conceptual general issues that apply here as well . we must always " believe something " ( as an axiom ) if we want to explain something ; we reduce the questions we did not understand to those that we did . but we must start somewhere . and fundamental forces ( magnetism in his case , gravity in our case ) are simply more fundamental than computers and rubber bands .
no . your mass does not increase when you move at relativistic speeds . nothing changes at all . you feel exactly the same . although the world around you will appear distorted , nothing endogenous to your spaceship feels any difference . this is in fact the fundamental idea of relativity . see this wikipedia article for more detail . also check out galileo 's original description . sometimes , when people are trying to explain relativity , they say that your mass increases when you go faster . einstein said this , and you can find it in books as late as the feynman lectures , perhaps later . unfortunately , while it is possible to use this idea responsibly , it is subtle and not especially helpful , so that by now most physicists do not use it any longer . first , it is easily misunderstood by thinking that the changing mass is happening to the astronaut . not so . it is just an artifact of different observers in different frames of reference . it is a bit like velocity - different people on different planets might look at the spaceship and claim that it has different velocities . they could even say " the velocity of that spaceship increases when it is moving near the speed of light " , which sounds silly but is essentially the same thing as saying the mass increases . of course , from the astronaut 's point of view , he is just sitting around in his spaceship unmoving . similarly , from his own point of view his mass is just the normal mass . additionally , if we say that the mass increases , it leads to separate ideas for mass in the direction of motion and mass perpendicular to the motion . this is confusing and unnecessary , so the idea of increasing mass is best left alone . when you read a physics paper and it refers to a particle 's mass , it means the " rest mass " , the mass it has in its own reference frame . the old idea of changing relativistic mass is really just energy . people took the equation $e = mc^2$ , noticed that the energy of the astronaut increases as they move faster , and said that means the mass increases as well . a better way to do it is to write the new equation $e = \gamma m_0 c^2$ and say that as the astronaut moves faster and faster , the mass $m_0$ is constant , but it is multiplied by a lorentz factor called $\gamma$ equal to $\frac{1}{\sqrt{1-v^2/c^2}}$ . when you learn a little more about relativity , you can come back to this idea after reading about four-vectors . energy and momentum are parts of an object 's energy-momentum four-vector . mass squared is the norm of this four-vector , so mass is the same in every reference frame . you can read more about this here , but it is somewhat more advanced than the other articles i linked . finally , there is this article on mass in special relativity .
the key sentence is " from the frame of reference of the accelerator , the circumference must stay the same . " so from the accelerator frame , no length is changing , just accelerator 's time passes by more quickly by a factor of gamma than in the particle 's frame . this is what allows the particles to " travel extra distance " than would normally be possible in newtonian mechanics . so that is the explanation from the accelerator frame . from the particle 's frame of reference , time passes by normally , but the distance between each particle will be lorentz contracted . imagine a particle directly in front of it . since they are traveling in " almost " the same direction , there will not be a whole lot of length contraction since they are pretty close together . but now imagine a particle at 0 degrees and one at 90 degrees ( a quarter way around ) . one is traveling in the " x " direction and the other in the " y " direction . since the first particle traveling in the x-direction has no velocity in the y-direction , there will be significant length contraction , and actually the exact amount contracted would be the lorentz factor $\gamma$ . now two particles spaced on opposite sides are traveling towards each other , so length contraction is further exacerbated . since all particles are evenly spaced , on average there is a factor of $\gamma$ of length contraction , so the circumference in the particle 's frame appears to be $c/\gamma$ . so all in all , in the accelerator frame , the circumference is still normal , and the physics checks out since the particles are in a different time frame , allowing them to travel all the way around . in the particle 's frame of reference , the circumference appears shorter , so they are able to make it all the way around . since the radius is perpendicular to the motion of the particles this will not be lorentz contracted . this is known as the ehrenfest paradox .
i think the author has used the word " almost " to incorporate the possibility that $ \delta s $ can be equal to zero also . as the exact statement of second law of thermodynamics goes this way , an isolated system evolves in such a way that $ \delta s \geq 0 $ . this means that an isolated system should evolve in such a way that the multiplicity should remain same or increase . for example , consider a simple system with two macrostates $a$ and $b$ , with $4$ and $6$ microstates respectively . if we find the system in mactrostate $a$ , it can evolve and remain in same state or can move to macrostate $b$ . but , if we find the system initially in macrostate $b$ , it will remain in macrostate $b$ and never trasits to macrostate $a$ . to my knowledge , no one knows why nature follows this rule ! ! !
this is not really an answer , but i can not put images in a comment . i tried fitting your expression using the excel solver and got $q_0$ = 71.1 and $1/rc$ = 0.0966 . the resulting fit looked like :
1 ) yes , it basically will find a non-optimal solution . at every point , the top of the ray looks for the bigger potential gradient , the charge in the surrounding volume grows , polarizing surrounding material ( air , in this case ) until a bigger gradient shows up and the ray continues over that direction . this is why the lightining path looks like a jigsaw ; its basically a monte carlo search . not to say , that you can not apply this to find efficient solutions of the travelsman problem . this would require making a programmable configuration with weights related to capacitance so the electric arc finds the best path that also solves your particular adjacency matrix problem . this would probably be only hampered by the fact that the arc , in order to do any traversing , must burn its way across the capacitance barrier , so unless you find a cheap , repleaceable material that can also be configurable ( the weight needs to adjust to the adjacency matrix weights ) it will be cheaper to do this computation in a standard computer .
spontaneous processes such as neutron decay require that the final state is lower in energy than the initial state . in ( stable ) nuclei , this is not the case , because the energy you gain from the neutron decay is lower than the energy it costs you to have an additional proton in the core . for neutron decay in the nuclei to be energetically favorable , the energy gained by the decay must be larger than the energy cost of adding that proton . this generally happens in neutron-rich isotopes : an example is the $\beta^-$-decay of cesium : $$\phantom{cs}^{137}_{55} cs \rightarrow \phantom{ba}^{137}_{56}ba + e^- + \bar{\nu}_e$$ for a first impression of the energies involved , you can consult the semi-empirical bethe-weizsäcker formula which lets you plug in the number of protons and neutrons and tells you the binding energy of the nucleus . by comparing the energies of two nuclei related via the $\beta^-$-decay you can tell whether or not this process should be possible .
the the car behaves like a musical instruments ( e . g trumpet ) . if the turbulence reach the resonant frequency you hear the huge loud . changing the size of the open part of the window you have to change the velocity of the car in order to hear the " explosion " . it is like you are playing your car : )
a composite particle consisting of an even ( odd ) number of fermions behaves like a boson ( fermion ) . swapping $\require{mhchem}\ce{^3he}$ atoms involves swapping an odd number of fermions ( 3 nucleons and 2 electrons ) : an odd number of sign changes of the wave function corresponds to no sign change or bosonic symmetry . swapping $\ce{^4he}$ atoms involves swapping an even number of fermions ( 4 nucleons and 2 electrons ) : an even number of sign changes of the wave function corresponds to a sign change or fermionic behavior . the same applies to other atoms : $\ce h$ = 1 proton and 1 electron = 2 ( even ) fermions : bosonic , $\ce{d}$ = 2 nucleons and 1 electron = 3 ( odd ) fermions : fermionic , etc .
you can generate dirac neutrino masses through the higgs mechanism by introducing right handed neutrinos ( in the same way you generate masses for the upper quarks ) . since neutrino masses are at the sub-$ev$ scale , this means that the yukawa couplings have to be unnaturally small , of order $10^{-12}$ . people prefer to keep $\mathcal{o} ( 1 ) $ yukawa 's and blame the unusual smallness of neutrino masses on the fact that these are neutral particles . if neutrinos happen to coincide with anti-neutrinos , then lepton number is violated by two units ( global symmetries are not sacred ) . in this case it is natural to introduce a large majorana mass scale $\lambda$ which ends up suppressing the neutrino masses via see-saw mechanism as $\langle\phi\rangle^2/\lambda$ . people like this because this large scale turns out be quite close to the grand unification scale . an alternative way to generate small masses for neutrinos without introducing a large majorana scale or right handed neutrinos is through radiative corrections . you can assume that neutrino masses are zero at the tree level ( as in the sm ) and generate small non-zero masses at 1 or 2-loop level by introducing new heavy scalar fields . one nice example is the zee model , where the sm scalar sector is extended to include a second higgs doublet $\phi_2$ and a higgs singlet $\omega$ . the loop producing neutrino masses is given by these models also break lepton number , but generally at a much lower scale . ( e . g . at tev scale ) .
see ravi as the formula shows that gravitational force is directly proportional to the masses of both the bodies [ consider proton and electron ] their masses are way too small [ mass of electron is $9.10938291 × 10^{-31}$ kilograms ] [ mass of proton is $1.67262178 × 10^{-27}$ kilograms ] and the radius of the first p-orbital from the nucleus is $5.2917721092 ( 17 ) ×10^{−11}$ m calculation of these all terms will lead to a very small froce of order $10^{-47}$ units which will almost equal to zero so the force is negligible . so is your doubt clear now ? feel free to comment for any further doubts
to start with , i need to write a list of assumptions that are at play , and numerous disclaimers are needed . firstly , the ipcc scientists do not say this follows a $ln$ function at all . they say it follows whatever their computer models says it follows . this is only a first order solution . assumptions begin here : we are considering one gas we are only considering outgoing thermal radiation blockage from the earth we are only considering one absorption peak in the absorption spectrum of this gas this absorption peak is both sufficiently narrow and sufficiently close to the peak of outgoing radiation spectrum that the outgoing radiation as a function of wavelength at sea-level is effectively constant over the entire absorption peak the cross section as a function of energy around the peak follows an exponential decrease on both sides of the peak it is a valid approximation to say that for any given wavelength , either all or none of the outgoing radiation is absorbed , corresponding with the optically thin or thick characterizations . one more assumption may or may not be necessary , reflecting my own imperfect knowledge , which is that the exponential declines quickly relative to the width of the peak . also , i am referring to cross section , but i really mean the cross section specific to the thermal distribution of the co 2 molecules - just one of the finer details that i do not care much about . the following is my illustration of the assumption the model under discussion makes about the form of the effective cross section as a function of wavelength of light . the approximation above is a taylor series expansion of the effective cross section around $\lambda_1$ and $\lambda_2$ in a $ln$ scale . this is to say the following for points around the left edge . i will just keep to the left edge because the right part follows in the same way and is not worth typing . $$\ln ( \psi ( \lambda ) ) = \ln ( \psi ( \lambda_1 ) ) + \gamma \delta \lambda$$ $$ \psi ( \lambda ) = \psi ( \lambda_1 ) e^{ \gamma \delta \lambda }$$ the above includes an assumption about the form of the absorption peaks of the co 2 . if you were mathy , you might be able to get this from a gaussian or a broadened absorption peak , i just want to note it real quick b/c i will not be commenting on the validity . next , i need to write the equation for the optical thickness , $\tau$ as a function of energy , then mutilate the radiation forcing expression , $f$ into a piecewise function . also , $c$ is the concentration of carbon dioxide and $n_{air}$ is the volume density of gas molecules in the atmosphere . i will also write it again for the specific region around the left edge for which $\delta \lambda = \lambda - \lambda_1$ . $$\tau ( \lambda ) = c n_{air} \psi ( \lambda ) = c n_{air} \psi ( \lambda_1 ) e^{ \gamma \delta \lambda }$$ optically thick is defined by $\tau&gt ; 2.3$ . we will write the energy dependent flux coming off the ground as $\phi ( \lambda ) $ . by the previously stated assumption , $\phi ( \lambda ) \approx \phi$ . $f$ is proportional to the absorbed/reflected $\phi$ by some constant i do not care about , $\beta$ . $$f_{co2} = \int_0^{\infty} \beta \phi ( 1 - e^{\tau ( \lambda ) } ) d\lambda$$ $$ ( 1 - e^{\tau ( \lambda ) } ) \approx \begin{cases} 0 , and \mbox{if } \tau ( \lambda ) &lt ; 2.3 \\ 1 , and \mbox{if } \tau ( \lambda ) \ge 2.3 \end{cases}$$ $$f_{co2} = \beta \phi \left ( \lambda_2 - \lambda_1 \right ) $$ if you just gawked or gagged at that . . . all i can say is do not shoot the messenger . in fact , we do not even really need that $\beta$ there for the purposes of this question . anyway , i hope it is clear where we are going , and at this point i will assign $\lambda_1$ and $\lambda_2$ to be pre-industrial values . the physical effect humans have had on the greenhouse effect , by this model , is to broaden the range of wavelengths that co 2 in the atmosphere absorbs . to put this into math , i will say that $\lambda_1&#39 ; $ and $\lambda_2&#39 ; $ are the corresponding post-industrial values . also , i will use $c$ as post-industrial concentration , $c_0$ as pre-industrial , $\tau$ and $\tau&#39 ; $ , and $f$ and $f&#39 ; $ respectively . $$\delta f = f_{co2}&#39 ; -f_{co2} = \beta \phi \left ( ( \lambda_2&#39 ; - \lambda_2 ) + ( \lambda_1 - \lambda_1&#39 ; ) \right ) = \beta \phi \left ( \delta \lambda_2 + \delta \lambda_1 \right ) $$ now here is the beef , we can find $\delta \lambda_1$ from the prior taylor series expansion . remember , the edge of the absorption peak , the point that separates the optically thick and thin regions , comes from that magic 2.3 value . the driving force behind this is , of course , the change in concentration $c$ . for clarity i will note that $\delta \lambda_1 = \lambda_1-\lambda_1&#39 ; $ . you will probably be confused by the negative , it is just math trickery in order to account for the fact that we are on the left ( negative ) side of the peak , and thus get the right $c$ on top . post-industrial $$\tau&#39 ; ( \lambda ) = 2.3 = c n_{air} \psi ( \lambda_1 ) e^{ -\gamma \delta \lambda_1 }$$ pre-industrial $$\tau ( \lambda ) = 2.3 = c_0 n_{air} \psi ( \lambda_1 ) $$ the answer to my question follows with algebra now . just assume $\delta \lambda_2$ to be the same thing , not that the constant matters anyway . $$\delta \lambda_1 = \frac{1}{\gamma} \ln{\frac{c}{c_0}}$$ $$\delta f = \frac{ 2 \beta \phi}{\gamma} \ln{\frac{c}{c_0}}$$ finished . although i probably used more space than what i needed to . and just for the record , this does matter , since discussions with the ipcc scenarios involve values of $c/c_0$ of 2 to 3 . if you just used the decreasing exponential you will get a very different answer . the problem in a nutshell is a cross section that varies so starkly with wavelength . the $1-e^{-\tau}$ was always recognized to be mono-energetic . the $ln$ alternative is for the exact opposite scenario where it completely blacks out energy ranges and the mathematical form of the absorption peaks give rise to the taylor series used here . the reference for this work was posted by michael luciuk in the comments . http://www.princeton.edu/~lam/documents/lamaug07bs.pdf and people have , of course , pointed out that the real behavior does not exactly follow this model . here is some real data . i found it to look surprisingly logarithmic . source
